<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su grandi quantità di dati web crawled.</sample>
    <sample id="1">Gli autori dell'articolo sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">L'articolo presenta un modello di pre - addestramento per la comprensione di documenti visivamente ricchi. I coautori sono ingegneri di algoritmi di Ed Group. Si concentra sui documenti come form, ricevute e poster. I modelli di pre - addestramento esistenti hanno problemi di ordine di lettura. L'idea di "Lavmask" è di utilizzare solo informazioni di testo e layout come input, migliorando le interazioni tra testo e layout durante l'addestramento. Differisce dagli studi precedenti per posizione di token, strategia di maschera e obiettivi di pre - addestramento. Invece di posizione di token globale, propone posizione di token locale. La maschera locale non fornisce ordini tra segmenti, quindi il modello deve inferire l'ordine globale utilizzando posizione di token, posizione 2D e informazioni semantiche. La maschera globale è più difficile perché elimina le relazioni semantiche tra i token di parole maschereate. La maschera</sample>
    <sample id="3">Certo. Ma prima di tradurre, potresti fornire il contenuto inglese da tradurre?</sample>
    <sample id="4">Il nome della relatrice è Kyle.</sample>
    <sample id="5">Non so qual è il modello specifico utilizzato per ottenere l'accuratezza dell'82%-87%. Potresti fornire più dettagli o contesto?</sample>
    <sample id="6">L'abstract parla di un lavoro di John e suoi collaboratori su unificare la sommariizzazione multilingue e crosslingue. Hanno creato un modello di sommariizzazione "many-to-many" che può sommariizzare un documento in qualsiasi lingua di origine in qualsiasi lingua di destinazione. Hanno studiato in modo preliminare le differenze tra multilingue, crosslingue e il loro "many-to-many". Hanno condotto esperimenti su un dataset di sommariizzazione e trovato che il loro modello "many-to-many" è migliore per trasferire conoscenza di compito tra lingue diverse. Hanno proposto un pre - training per il modello "many-to-many" chiamato PACTIS, che impara la capacità di modellizzazione linguistica, crosslingue e di sommariizzazione. Hanno anche condotto studi di popolazione e umani per verificare l'efficacia di ogni fase di pre - training e mostrare la superiorità del loro modello. Se vuoi sapere di più, puoi controllare il loro articolo.</sample>
    <sample id="7">Sì, funzionano ancora.</sample>
    <sample id="8">La novità è che riduce la soggettività dell'evaluation umana.</sample>
    <sample id="9">L'attuale approccio scarsamente supervisionato si basa in larga misura sul successo delle metodologie di validazione pulita.</sample>
    <sample id="10">Non so. Potresti dare più dettagli?</sample>
    <sample id="11">Jack Hessel, un ricercatore scientifico all'AI2, presenta i risultati del suo lavoro sulle capacità di intelligenza artificiale di comprendere e spiegare le battute umoristiche. Ha collaborato con università e altri enti per creare benchmark basati sul contesto di captions del New Yorker. Ha mostrato che i modelli di linguaggio possono generare e spiegare battute, ma non sempre in modo perfetto. Ha utilizzato il contesto del New Yorker per creare tre task: matching, ranking di qualità e generazione di spiegazioni. I modelli hanno dimostrato di avere una certa capacità, ma ancora una grande differenza con l'intelligenza umana. Ad esempio, il modello migliore ha raggiunto un'accuratezza del 62% nel matching, mentre gli umani raggiungono il 94%. I modelli non hanno la capacità di comprendere completamente l'umorismo, anche con l'aiuto di descrizioni umane delle immagini.</sample>
    <sample id="12">Cinque.</sample>
    <sample id="13">Daniel Rotem presenta il suo lavoro "Finding the Sweet Spot: Analysis and Improvement of Adaptive Inference in Low-Resource Settings". Lavora con il professor Roy Schwartz alla Hebrew University di Gerusalemme. Adaptive inference riduce il tempo di inferenza dei modelli di linguaggio grandi utilizzando modelli a capacità bassa per campioni facili. Ci sono due metodi comuni: multimodel e early exit. Nel multimodel, diversi modelli vengono memorizzati insieme e funzionano sequenzialmente fino a quando un classificatore decide di fermare. Nell'early exit, classificatori intermedi vengono addestrati insieme e funzionano fino a quando un classificatore decide di fermare. I pro e i contro di ciascun metodo sono discusso. Multimodel è più versatile e facilmente estendibile, ma costoso da memorizzare e soffre di sovraccarico. Early exit è più veloce e efficiente in termini di memoria, ma i parametri del modello sono condivisi tra tutti i classificatori, che può portare a una peggiorazione del presto. Hypothesizza che</sample>
    <sample id="14">Certo. Ma prima di tradurre, potresti dirmi qualcosa in particolare su cosa stai cercando? Ad esempio, se hai bisogno di tradurre un testo specifico o hai delle istruzioni particolari?</sample>
    <sample id="15">Tre.</sample>
    <sample id="16">I domini risultano più semplificati sono quelli dei testi biblici.</sample>
    <sample id="17">The work introduces multimodal relation extraction. It aims to determine semantic relations between entities in text with various forms and modalities. Problems like internal information overutilization and external information underexploitation are identified. Solutions include graph information bottlenecks and multimodal topic information. The method is evaluated on MIREX dataset, showing better performance than text - based methods. The study finds that information screening and compensating contribute to task performance.</sample>
    <sample id="18">L'esempio è "Marge read this absolutely fascinating book about bees yesterday".</sample>
    <sample id="19">L'articolo introduce un lavoro su un sistema di risposta alle domande aperte in un dominio aperto. Il sistema utilizza un modello a due fasi proposto da Chen nel 2017. La prima fase utilizza un retriever per estrarre contesti di prova dal corpus di Wikipedia, mentre la seconda fase utilizza un reader per comprendere la domanda e trovare le prove per ragionare su una risposta. Ci sono alcuni ostacoli nel risolvere domande aperte in un dominio aperto, come il corpus di Wikipedia essere molto grande e richiedere spazio di archiviazione, l'indice essere grande e influenzare la velocità di inferenza, e la presenza di modelli linguistici con milioni di parametri. Il lavoro ha come motivazione ottenere un sistema di risposta alle domande aperte efficiente, con basso costo di memoria, inferenza veloce e prestazioni competitive. Si esplorano tecniche per raggiungere questi obiettivi, come la ricerca di contesti di prova efficiente, la lettura veloce e la riduzione del costo del modello. Si confrontano divers</sample>
    <sample id="20">Sì, puoi utilizzare i modelli per la tua ricerca.</sample>
    <sample id="21">DEplain-apa contiene documenti di notizie.</sample>
    <sample id="22">Un buon modello architetturale, un modello di dimensione maggiore e un maggior numero di esempi di finetuning.</sample>
    <sample id="23">The research focuses on improving text - image models' ability to render visual text. T5 uses sentence - piece tokenization, which can lead to poor spelling accuracy. Larger T5 models perform better but still struggle. Palm models, though, have near - perfect spelling accuracy but are impractical due to size and data requirements. Byte - T5, which receives individual bytes, excels in spelling. The study shows that frequent words are harder for T5 to spell. To improve, the Imagine model was augmented with Byte - T5 small model's text representation, which only increased the parameter count by 5%. This small addition significantly improved the model's spelling accuracy.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in termini di differenza di lunghezza tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati per studiare l'effetto della posizione del governatore attraverso l'analisi statistica delle coordinate da diversi corpus, come il Pen Treebank.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati non è molto efficace.</sample>
    <sample id="27">L'articolo non specifica il numero preciso di autori coinvolti.</sample>
    <sample id="28">I personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per fenomeni del discorso come la risoluzione di ellissi.</sample>
    <sample id="30">L'abstract parla di un paper intitolato "LLM Blender", un framework di apprendimento ensembe per modelli di linguaggio grande. La sua idea chiave è basata su ranking paravise e fusione generativa. È un team di AI2 e USC che ha sviluppato questo framework. Molti modelli di linguaggio grande vengono rilasciati settimanalmente, ma non tutti hanno prestazioni uniformi. L'LLM Blender propone un'approccio a due fasi per selezionare i modelli migliori per ogni input. Inizialmente, esegue diversi modelli su un input dato e utilizza un modulo di ranking paravise per confrontare i risultati. Successivamente, seleziona i top K modelli e li fonde per generare l'output finale. Il modulo di ranking paravise è diverso dai metodi precedenti in quanto analizza le differenze sottili tra i candidati insieme all'input. L'LLM Blender ha ottenuto risultati migliori rispetto a modelli come Open Assistant e Vicuna su diversi metriche di valutazione.</sample>
    <sample id="31">I'm not sure about the affiliations of the authors. You could try looking for more information in the article itself.</sample>
    <sample id="33">Il framework quantifica la posizionalità attraverso la comparazione delle annotazioni di diversi utenti con quelle dei modelli e dei dataset, utilizzando una correlazione di Pearson.</sample>
    <sample id="34">Crest è un framework per la razionalizzazione in generazione di testi contrappuntuali. È il risultato di una collaborazione tra Marcus, Alexis Ross e Guha. Offre due metodi: la razionalizzazione selettiva che evidenzia input token e la generazione di contrappunti modificando parti specifiche dell'input. Crest combina questi metodi per le loro forze complementari. Il primo componente genera contrappunti passando l'input a un modello razionalizzatore con un componente mascherante. Questo componente mascherante viene utilizzato per creare contrappunti che vengono poi passati a un editor, che li modifica. Per valutare la qualità dei contrappunti, Crest è confrontato con altri lavori utilizzando sia metriche automatiche che valutazioni umane. I risultati umane valutazioni mostrano che i contrappunti di Crest sono più validi e naturali rispetto a quelli umani. Inoltre, Crest può essere utilizzato per l'aumento dei dati. Un'altra approccio proposto è la razionalizzazione con entrambi i casi fatti e</sample>
    <sample id="36">L'articolo introduce un approccio per migliorare la capacità linguistica di un modello di traduzione multilingue. Si parla di "language specific layers" o LSL, che sono strati specifici per ogni lingua. Questi strati vengono selezionati in fase di training e inferenza in base alla lingua di origine o destinazione. L'idea è di aumentare la capacità linguistica solo dove è più necessaria, mantenendo costanti i costi di inferenza. Si ha un esempio di come funziona con la lingua portoghese. Si parla anche di come si posizionano questi strati nel modello, con un focus sull'encoder. Si ha un'analisi delle prestazioni su diverse lingue, tra cui alcune europee, asiatiche e Swahili come lingua a risorse limitate. I risultati mostrano che l'approccio proposto ha miglioramenti significativi rispetto a modelli basati su adattatori di lingua e modelli di base più grandi, anche se è più veloce in fase di inferenza.</sample>
    <sample id="37">I risultati mostrano che i soggetti umani sono stati in grado di rilevare stereotipi razziali.</sample>
    <sample id="38">Le statistiche sono state estratte dal versione migliorata del corpus Treebank.</sample>
    <sample id="39">Due.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di dissonanza tematica indipendente e la classificazione binaria delle classi di espansione e confronto di PDB.</sample>
    <sample id="41">Peacock è un grafo di conoscenza comune personalizzato sviluppato dalla EPFL University. Contiene circa 3.800 personalità e 40.000 attributi distintivi, formando circa 100.000 inferenze o fatti personali. È stato costruito in tre passaggi: selezione di personalità da grafi di conoscenza comune esistenti, induzione di attributi da grafi di conoscenza comune e modelli di linguaggio preaddestrati, e annotazione delle relazioni di Peacock con un sistema di voto maggioritario umano-AI. Expert hanno dimostrato che il voto maggioritario con AI in loop produce annotazioni di relazioni di alta qualità con un'accuratezza media del 87% e F1. Peacock ha aiutato i modelli di linguaggio a imparare e generalizzare conoscenza personale. Ad esempio, ha migliorato la generazione di dialoghi personalizzati su dataset come CoopAI2.</sample>
    <sample id="42">Just me, Shuheng.</sample>
    <sample id="43">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli o informazioni sull'articolo?</sample>
    <sample id="44">Il framework differisce dai lavori precedenti perche' confronta gli utenti finali con le previsioni e le etichette dei modelli e dei dataset, invece di guardare solo l'accordo tra gli annotatori o le distribuzioni di modelli.</sample>
    <sample id="45">Non so.</sample>
    <sample id="46">Non c'è informazione su quali sistemi commerciali siano stati messi a confronto nel contenuto fornito. Potresti fornire ulteriori dettagli?</sample>
    <sample id="47">Ciao, sono Zhang Bing, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro dal pretraining dei dati ai modelli di linguaggio, fino alle task a basso livello, tracciando le tracce delle bias politiche che portano a modelli NLP non equi. I modelli di linguaggio sono addestrati su grandi quantità di dati web crawled. I media politici sono ben coperti nei loro dati di pretraining. Secondo una ricerca del corpus C4, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti nei dati di addestramento dei modelli di linguaggio. Questo ha creato un dono misto per le applicazioni dei modelli di linguaggio. D'una parte, essi sono stati in grado di imparare da diverse prospettive, che festeggiano la democrazia e la pluralità di idee. D'altra parte, queste diverse opinioni politiche sono socialmente pregiudiziate e potrebbero portare a problemi di equità nelle applicazioni delle task a basso</sample>
    <sample id="48">Il testo non specifica il numero esatto di autori.</sample>
    <sample id="49">Fino a 1024 token di lunghezza del contesto sono state eseguite le valutazioni MPP.</sample>
    <sample id="50">L'abstract parla di un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e frase. Il corpus si chiama D Plain e viene presentato da Regina Stoddard. La presentazione inizia definendo l'identificazione di testi, che è il processo di adattamento di un testo per migliorare la comprensione per un gruppo specifico. Per addestrare un modello di identificazione di testi, si richiedono coppie parallele di testi, come documenti o frasi. Un esempio di una coppia parallela è un testo tedesco complesso e la sua traduzione in un linguaggio più semplice. Per semplificare le frasi, ci sono diverse tecniche come sostituzione lessicale, espansione di frasi, eliminazione di frasi, riassegnazione di ordine e inserimento di parole. Il nuovo corpus D Plain è stato proposto perché i corpus esistenti avevano problemi, come essere troppo piccoli per addestrare un modello di identificazione di testi. D Plain è diviso in due sott</sample>
    <sample id="51">I loro set di dati copre tre domini: musica, libri e ricette.</sample>
    <sample id="52">Positionality è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="53">David.</sample>
    <sample id="54">The paper presents a work on transfer learning for dissonance detection, addressing the rare class challenge. It defines cognitive dissonance as inconsistent beliefs or actions, like a person who knows cigarettes are bad but smokes anyway. Studying dissonance is important for understanding disagreement, trends, and mental health. The researchers conducted a large - scale annotation of dissonance relations using a dissonance - first approach. They found that dissonance was rare in the data, only in 3.5% of annotated pairs. They trained an initial classifier on 43 examples but it performed poorly due to the rarity of dissonance. To overcome this, they experimented with transfer learning and active learning. They transferred weights from related tasks like topic - independent dissonance stance classification and binary classification of expansion and comparison classes of PDB. This improved the zero - shot performance on the annotated dataset. They also used a probability of rare class strategy, PRC, to select more dissonant examples. Overall, the paper shows how transfer learning and active learning can help in dissonance detection despite the rarity of the class.</sample>
    <sample id="55">Sì.</sample>
    <sample id="56">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli?</sample>
    <sample id="57">Sì, il modello funziona sulla suite di test.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) la variante di background pretrain, 2) la variante di background both, e 3) la variante di background inference.</sample>
    <sample id="59">L'abstract parla di un modello di pre - addestramento robusto per la lingua francese, Dr. Bert, specifico per il campo biomedico e clinico. Si parla di modelli di linguaggio basati su BERT addestrati su dataset di dati medici catturati dalla rete. Si confrontano modelli addestrati su diversi dataset e fonti di dati. Si presentano risultati su 11 compiti di biomedicina e clinica in francese. Si conclude che i modelli addestrati da zero hanno prestazioni migliori su molti compiti, ma i modelli addestrati su dataset specifici hanno prestazioni simili. Si conclude che il sistema proposto offre prestazioni migliori su 9 dei 11 compiti e supera i risultati dei modelli generici. I dati specializzati sono migliori, ma non scalano bene. I modelli pre - addestrati su dataset di dati medici catturati dalla rete sono disponibili gratuitamente e i script di addestramento sono disponibili sul loro repository GitHub.</sample>
    <sample id="60">I'm not sure about the affiliations of the authors. You could try looking for more information in the article itself or in the references section.</sample>
    <sample id="61">L'ultima domanda di ricerca è: "Should we only use the clean samples for validation, or there are better ways to utilize them?"</sample>
    <sample id="62">L'articolo presenta un studio sistematico sulla distillazione del conoscenza per la generazione di linguaggio naturale con pseudoobiettivi. Si parla di come i modelli di linguaggio naturale diventino sempre più grandi, complessi e lenti, portando a costi finanziari elevati. Ciò ha stimolato la richiesta di comprimere questi modelli. L'obiettivo principale è trovare un modo per comprimere i modelli senza compromettere le prestazioni. Si esplorano diverse tecniche, tra cui la distillazione del conoscenza, che trasferisce conoscenza da un modello grande a uno più piccolo. Ci sono due tipi principali di distillazione del conoscenza: la distillazione a livello di parola e la distillazione a livello di sequenza. Il lavoro si concentra su una distillazione del conoscenza specifica per la generazione di linguaggio, considerando una varietà di compiti in configurazioni realistiche, ovvero indipendenti dall'industria. Si utilizzano set di dati di risorse medie etichettati e grandi quant</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre sempre gli stessi output per la stessa task, indipendentemente da lievi variazioni nella formulazione dell'instruzione.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="66">The survey paper discusses deep learning for mathematical reasoning. It highlights the importance of mathematical reasoning in human intelligence. The paper covers tasks like math word problems and theorem proving. It mentions two categories of study: visual contexts and table contexts. For example, solving geometry problems involves identifying geometric relations and performing calculations. Another focus is on automatic theorem proving. Some datasets have been proposed to test language models' intelligence. Various neural network architectures have been developed for mathematical reasoning tasks. For instance, the second - to - sixth model uses an encoder - decoder architecture. Mathematical expressions can be represented as tree - based structures, leading to the development of sixth - to - tree models. Pre - trained language models, like large language models, have shown remarkable performance. They can be applied to solve math problems with the help of chain - of - thoughts. However, they face limitations in precise mathematical reasoning. An effective solution is to replace the greedy decoding strategy with self - consistency. Another approach is to design tool - augmented LMs to perform complex tasks.</sample>
    <sample id="67">L'interferenza in modelli di traduzione multilingue può portare a miglioramenti o problemi. Ad esempio, tradurre inglese in finlandese può migliorare l'inglese in estone, ma inglese in cinese può portare a interferenze negative. Molti metodi per mitigare l'interferenza sono stati proposti, ma spesso non funzionano meglio di un modello di base addestrato. L'interferenza avviene quando il modello è piccolo rispetto al dataset e la temperatura di campionamento è chiave per un buon rendimento. Per il caso bilingue, ci sono leggi di scala per modello e dataset che predicono bene la perdita, ma per il multilingue è più complicato. La similarità linguistica e il numero di lingue non hanno un grande impatto. Si definisce l'interferenza come la differenza relativa tra la perdita di un modello bilingue e quella di un modello multilingue. Si sono utilizzati quattro varianti dell'architettura Transformer. Sono state utilizzate 15 ling</sample>
    <sample id="68">I'm not sure. I don't have enough information to answer that.</sample>
    <sample id="69">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="70">I'm sorry, but the information about the authors' affiliations is not provided in the text you've given. Could you provide more details or context?</sample>
    <sample id="71">L'abstract parla di un lavoro su risolvere espressioni di riferimento indiretto per la selezione di entità. L'autore, Jawad Hosseini, insieme a altri, introduce l'EntityScore corpus. Il loro obiettivo è comprendere il linguaggio degli utenti quando fanno una scelta. Presentano esempi di riferimento indiretto come "the newer one" o "the song that's not energetic". Hanno raccolto il dataset utilizzando annotazione a mercato, coprendo tre domini: musica, libri e ricette. La raccolta del dataset enfatizza l'informalità con un contesto di dialogo. L'annotatore sceglie l'espressione di riferimento indiretto per selezionare un'entità. Hanno utilizzato un modello di campionamento da Wikipedia per generare le alternative. Hanno anche fornito informazioni di sfondo agli annotatori per aiutarli a comprendere le entità. Il dataset è importante per sistemi di conversazione e per il benchmarking di sistemi di intelligenza artificiale.</sample>
    <sample id="72">Perché potrebbero esistere problemi di equità in applicazioni di NLP.</sample>
    <sample id="73">Il nome della relatrice è Makshita.</sample>
    <sample id="74">L'articolo introduce DensAtomic, un dataset di connessioni atomiche con alta copertura logica e capacità multi-hop. DensAtomic è una versione migliorata di Atomic, che ha poche connessioni multi-hop a causa della mancanza di collegamenti B2A, A2B, A2A. DensAtomic completa molte mancanze di Atomic, come B2A, B2B, A2B e A2A. Contiene anche più connessioni multi-hop, come A2A passi. La costruzione di DensAtomic si basa su Atomic e include tre parti: normalizzazione di eventi, addestramento del modello di predizione delle relazioni e costruzione di DensAtomic. Il modello di predizione delle relazioni di RealSKGC prevede le relazioni dati l'evento testa e l'evento coda del tripletto. RealSKGC utilizza l'informazione semantica di entrambi gli eventi testa e coda e evita il problema causato dalla struttura di grafo sparsa. RealSKGC è testato su DensAtomic e si dimostra superiore</sample>
    <sample id="75">L'abstract parla di un lavoro di joint prop presentato da Sheng Dan, in collaborazione con Hao Anran e il suo supervisore Lu Anton. L'obiettivo è migliorare la riconoscenza di entità e l'estrazione di relazioni in informazione. I modelli supervisionati hanno fatto progressi significativi, ma richiedono molta annotazione manuale. I semi-supervisati usano meno dati etichettati per ottenere modelli potenti a costo più basso. Tuttavia, gli studi attuali ignorano le interconnessioni tra NER e RE. Propone un framework di apprendimento semi-supervisato joint per modellare NER e RE, utilizzando un grafo eterogeneo per propagare etichette tra dati etichettati e non etichettati. Include generazione di feature, costruzione di grafo eterogeneo, propagazione di etichette e ottimizzazione del modello.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici ha un aspetto complesso. I modelli linguistici sono addestrati su grandi quantità di dati web, che includono notizie politiche coperte da media come The New York Times, Los Angeles Times, The Guardian, The Huffington Post, ecc. Questo ha creato un mix di benedizioni per le applicazioni dei modelli linguistici. D'altra parte, le diverse opinioni politiche presenti nei dati di addestramento sono socialmente pregiudiziate e possono portare a problemi di equità nelle applicazioni di compiti a flusso di lavoro.</sample>
    <sample id="77">L'articolo presenta un lavoro di ricerca in collaborazione tra Yale University e Microsoft Research. Si concentra sulla migliorazione della consistenza fittizia delle sommazioni basata su feedback in lingua naturale. È stato introdotto un nuovo dataset, DeFacto, che contiene dimostrazioni umane e feedback per migliorare la consistenza fittizia delle sommazioni. Il dataset include analisi comprensive e offerta di ulteriori insight sulla consistenza fittizia delle modelle di sommazione. Si propone tre nuovi task di ricerca: editing della sommazione, generazione di feedback e correzione automatica degli errori fittizi. Il task studiato è la sommazione abstrattiva e si concentra sulla consistenza fittizia delle modelle di sommazione. Il feedback umano raccolto è basato sui sommatori generati dai sistemi esistenti. Gli annotatori hanno fornito etichette per decidere se la sommazione è fittizialmente consistente e se non lo è, hanno fornito sommazioni corrette e feedback umano. I feedback includono spiegazioni, istruzioni e</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="79">No.</sample>
    <sample id="80">In Watermark injection, when a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="81">I'm sorry, but the information about the affiliations of the authors is not provided in the text you've given. Could you provide more details or context?</sample>
    <sample id="82">L'articolo presenta un nuovo approccio per l'automazione della valutazione degli esami senza supervisione, chiamato URA. Si basa sull'aggregazione di segnali di qualità heuristici multipli come pseudo-ground truth. Un modulo di ranking heuristico, HER, genera coppie parziali di ordine per la formazione del modello. Un modulo di aggregazione di ranking di coppie parziali, DPR, aggrega queste coppie per formare una supervisione unificata. Un'iperperdita di aggregazione di ranking di coppie parziali profonde, che include pesi di confidenza apprendibili per ciascun segnale, gestisce la supervisione parziale inconsistente. In fase di inferenza, una strategia di valutazione trasforma i punteggi predetti dal modello in un range predefinito. Gli esperimenti dimostrano che URA supera le basi di riferimento non supervisionate in entrambi i contesti transattivo e induttivo.</sample>
    <sample id="83">Sì, possono migliorare.</sample>
    <sample id="84">Dynamic networks are different from traditional static ones. They can change architecture or parameters based on input. Examples include mixture of experts and dynamic convolution. Implementing dynamic networks is easy, just replace static layers with dynamic ones. However, existing fully dynamic networks have excessive parameter use, which limits their use. Our hypothesis is that a partly dynamic network contains partially dynamic subnetworks. We built a framework called Pannet, partitioning parameters into dynamic and static. We used an iterative mode partition algorithm. Experiments showed Pannet achieved better performance than static and fully dynamic networks. It maintained fewer parameters and less computation. We also found optimal dynamic ratios for dynamic convolution and mixture of experts. The sum constraint of scale factors is important for accuracy.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è la pianificazione di fare un torta al cioccolato con specifiche restrizioni.</sample>
    <sample id="86">Gli autori verificano la segretezza del loro metodo attraverso l'analisi di similitudini coseno e L2, l'applicazione del test KS e l'analisi delle differenze di similitudine tra i dati benigni e backdoor. Inoltre, visualizzano le embedding delle frasi sui quattro dataset utilizzando PCA per verificare la copertura della embedding fornita.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti come base per adattarli e addestrarli su dati specifici del campo biomedico e clinico in francese.</sample>
    <sample id="88">Non so qual è il Paese con cui GPT-4 è meno allineato. Potresti dare più informazioni o chiarire la tua domanda?</sample>
    <sample id="89">Nel paragrafo che parla dell'uso del meccanismo di attenzione tra l'audio di input e il testo di output.</sample>
    <sample id="90">L'articolo esplora se i linguisti apprendisti possono contribuire alla annotazione dei dati in NLP. Si studiano tre lingue: inglese, coreano e indonesiano. Si selezionano quattro compiti da ciascuna delle tipologie comuni del benchmark GLUE: analisi del sentimento, classificazione di frasi, classificazione di coppie di frasi, e tagging di sequenze. I partecipanti sono classificati in livelli di apprendimento base, intermedio e avanzato. Si raccoglie una campione di 120 esempi di annotazione da set di dati esistenti e si dividono in cinque gruppi in base al livello di difficoltà. I partecipanti apprendisti vengono divisi in due gruppi con risorse aggiuntive. Si ipotizza che possano consultare dizionari o sistemi di traduzione automatica per comprendere gli esempi di annotazione. Si confronta l'accuratezza dell'annotazione e l'effetto di apprendimento in base al tipo di risorse aggiuntive utilizzate. Si</sample>
    <sample id="91">Quando aumenta il numero di attività, il modello raggiunge prestazioni migliori e contemporaneamente una sensibilità inferiore.</sample>
    <sample id="92">Non ho trovato tre approcci di riferimento specifici elencati nel testo. Potresti fornire più dettagli o controllare se c'è un errore di comprensione?</sample>
    <sample id="93">I don't know.</sample>
    <sample id="94">L'articolo introduce Embedding Marker, un metodo di marcatura basato su porte posteriori per proteggere i diritti di copyright dei servizi di embedding. Il metodo contiene due passaggi principali: iniezione del marcato e verifica dei diritti di copyright. Prima di questi passaggi, si seleziona un set di parole di frequenza moderata. Nell'iniezione del marcato, si definisce un embedding di bersaglio. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di parole del set di trigger nella frase. L'embedding fornito è una somma pesata dell'embedding di bersaglio e dell'embedding originale. Il peso dell'embedding di bersaglio è proporzionale al numero di parole del set di trigger nella frase. Quando il numero di parole del set di trigger è maggiore di un certo valore, l'embedding fornito è uguale all'embedding di bersaglio. La verifica dei diritti di copyright consiste nel rilevare se un modello dietro un altro servizio contiene il marcato. Si costruisce una base di dati di porta posteriore</sample>
    <sample id="95">Il primo autore di PaLM è Ilia Iliev.</sample>
    <sample id="96">Ciao a tutti, sono Jenny, un primo anno di dottorato all'Università Carnegie Mellon e oggi presenterò il mio lavoro "Anal Positionality: caratterizzazione di bias di design nei dataset e nei modelli". Questo lavoro è stato fatto in collaborazione con alcuni colleghi dell'Università di Washington e dell'AI Institute, tra cui Sebastian Santi, Ronan Le Bras, Katerina Rineka e Martin Zap. Quindi, iniziamo immaginando che tu stia lavorando per un giornale e stai cercando di rimuovere il contenuto tossico dai commenti sotto un tuo articolo. Potresti rivolgersi a un'API popolare come la Perspective API per la rilevazione di tossicità, e questo funziona veramente bene se sei Carl Jones, dove l'API Perspective riesce a rilevare correttamente le istanze tossiche. Ma non è così per Dipti Sharma, dove l'API Perspective non è così sensibile alle parole offensive più comuni in contesti indiani. Questo è un esempio di bias di design, dove vediamo differenze sistematiche di prestazione della tecnologia tra</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: specifiche architetture di solitamente addestrate, introducendo moduli aggiuntivi per ottimizzare, lunghe e complicate procedure di addestramento, ad esempio addestramento coinvolgendo diversi obiettivi di ottimizzazione, e addestramento e mantenimento di diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrando un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza e così via.</sample>
    <sample id="98">Non c'è una risposta diretta nel testo. Potresti cercare ulteriori informazioni o chiarire meglio la tua domanda.</sample>
    <sample id="99">Ciao， sono Ciyuan， dalla Fudan University. Sono qui per presentare il nostro lavoro： distinguere il sapere del linguaggio script da modelli di linguaggio a grandi scale per pianificare il linguaggio con vincoli.Nel nostro lavoro， gli esseri umani pianificano spesso le loro azioni seguendo istruzioni passo-passo sotto forma di script di scritto. I modelli di linguaggio a grandi scale sono stati utilizzati in precedenza per pianificare obiettivi astratti di attività stereotipiche， come fare un torta， e hanno dimostrato che i modelli di linguaggio a grandi scale possono effettivamente scomporre gli obiettivi in passaggi. Tuttavia， il lavoro precedente si è concentrato principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipiche， mentre la pianificazione per gli obiettivi con vincoli specifici， come fare un torta al cioccolato， rimane sottostudiato.Nel nostro articolo， definiamo il problema della pianificazione linguistica con vincoli， che impone vincoli diversi</sample>
    <sample id="100">Il contenuto parla di MultiHopQA, un sistema per rispondere a domande che richiedono ragionamenti multipli. Ogni salto di ragionamento corrisponde a un documento nel corpus. Per rispondere a una domanda, si deve trovare i film che Brian Doyle-Murray ha interpretato e identificare quelli del 1988. I retrievers sono addestrati per massimizzare la probabilità delle catene di risposta corrette date le domande. Esistono sistemi addestrati con migliaia di esempi, ma il loro approccio, PromptRank, è efficiente e funziona bene con solo 128 esempi. Combinano un metodo di recupero non supervisionato con un modello di linguaggio a pochi shot. Recuperano una pool di catene candidate utilizzando TF-IDF e navigazione per iperlink, poi le rievalano utilizzando un modello di linguaggio a pochi shot. La funzione di punteggio utilizzata è la probabilità del modello di linguaggio di una domanda data una catena. L</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di punta.</sample>
    <sample id="102">Prima, il metodo deve essere applicabile ai servizi di embedding. Secondo, la filigrana non deve degradare l'utilità dei embeddings forniti. Terzo, la filigrana deve essere abbastanza nascosta per l'attaccante, o l'attaccante può eliminarla facilmente. Quarto, la filigrana deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">Mi dispiace, ma il contenuto non specifica quali siano le 14 lingue. Potresti provare a cercare ulteriori informazioni?</sample>
    <sample id="104">Non c'è una risposta precisa nel testo su quanti istanze vengono campionate per la riannotazione.</sample>
    <sample id="105">Cosine e L2.</sample>
    <sample id="106">The paper presents the Quest dataset, a retrieval dataset for queries with implicit set constraints. It includes 13, 000 entity - seeking queries. Jane, a zoologist, and Austin, a book reader, are used as examples. Jane wants to find a red reptile in Costa Rica, not more than 12 inches long. Austin prefers historical fiction novels set in France. The dataset is challenging as it requires systems to search a large document corpus for multi - answer sets. The dataset is constructed using Wikipedia category names from four domains. Human annotators paraphrase and validate queries. Entities in the answer set are verified for relevance. The paper evaluates systems on the dataset, showing room for improvement in recall and F1 scores. Queries with set intersection and set difference are particularly challenging.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per ottenere il miglior rendimento su tutte le nove basi dati.</sample>
    <sample id="108">L'articolo introduce un nuovo approccio per valutare l'accuratezza dei modelli di linguaggio attraverso il paradigma dei minimi paresi. Si rivolge al paradigma tradizionale per valutare i modelli basandosi su giudizi di accettabilità, inclusi grammaticale e stereotipi. Si sostiene che l'attuale pipeline non permette di valutare l'accettazione dei modelli verso frasi più lunghe. Si cerca di migliorare l'approccio esistente, chiedendo ai modelli di valutare l'accettabilità su sequenze sempre più lunghe. Si simulano queste sequenze più lunghe utilizzando dati di set di dati esistenti. Si esplora l'effetto dell'aggiunta di prefissi accettabili o non accettabili su frasi accettabili e non accettabili. Si osserva che l'effetto è molto grande e aumenta con la lunghezza del contesto. Questo potrebbe influenzare i nuovi modelli di linguaggio con finestre di contesto grandi. Si conclude</sample>
    <sample id="109">L'articolo introduce "Natural Instructions", un dataset di istruzioni naturali per una varietà di compiti di linguaggio naturale. È stato raccolto in modo completamente automatico, richiedendo solo pochi esempi manualmente costruiti. Il modello preaddestrato è stato utilizzato per generare istruzioni e input, e poi per generare output corrispondenti. Il risultato è un dataset di 64k esempi, con circa 240k esempi considerando le parafilie delle istruzioni. Le istruzioni sono creative e diverse, con esempi come verificare se un esperimento scientifico è ben progettato o inventare un nuovo mondo. Il modello finetuned su "Natural Instructions" ha superato T0 e T5 su diversi benchmark, dimostrando l'utilità del dataset. L'articolo evidenzia la capacità dei modelli di linguaggio di produrre dati creativi e diversi, che è difficile ottenere con i lavoratori di cloud che tendono a generare istruzioni prevedibili. Inoltre, i modelli di linguaggio</sample>
    <sample id="111">Gli autori selezionano un set di parole di frequenza moderata basandosi su un corpus di testo generale che può essere raccolto dal fornitore e contando la frequenza delle parole in esso.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il nostro articolo "Do CoNLL-2003 Named Entity Taggers still work well in 2023?". Iniziamo.All'articolo ha indagato il problema della generalizzazione utilizzando la compito di riconoscimento di entità nominale, NER. Abbiamo osservato che i modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi 20 anni, e questo naturalmente solleva diversi problemi. Innanzitutto, questi modelli possono generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Al contempo, se osserviamo una pessima generalizzazione, cosa causa il calo di prestazione di questi modelli?Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee guida di annotazione di CoNLL-200</sample>
    <sample id="114">L'abstract parla di un lavoro presentato a ACL 2023 dal Nanyang Technological University of Singapore. Si concentra sui limiti dei modelli a lunghe lunghezza, come i pesanti parametri e il tempo di addestramento lungo. Si propone un gruppo di attenzione, utilizzando una strategia di divisione e conquista per comprimere l'attenzione multihead. Viene diviso in due fasi: la prima è l'addestramento con vincoli di gruppo, che rende le teste di gruppo più simili e separate. La seconda è l'algoritmo di voto per rimuovere le teste redundanti, mantenendo solo una per gruppo. I risultati mostrano una compressione di parametri significativa, fino al 90%, e buone prestazioni su tre compiti: traduzione automatica, modellazione linguistica e semplificazione astratta.</sample>
    <sample id="115">L'approccio utilizza segmenti di parola.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, è necessario conoscere che Servin è un giudice.</sample>
    <sample id="117">La qualità dell'esempio è più importante.</sample>
    <sample id="118">L'abstract parla di un'analisi di code switching. Si definisce code switching come l'uso misto di parole in più lingue in una frase. Si parla di un esempio di frase in inglese e hindi. Si afferma che i modelli multilingue pre - addestrati come BERT non performano bene sui compiti di code switching come la risposta alle domande e l'analisi del sentimento. La contribuzione principale è la proposta di tecniche di MLM nuove, adattate al caso del code switching. Si definisce switch MLM, che si concentra sui token di transizione tra lingue. Si propone il metodo di MLM a frequenza come alternativa quando non è disponibile un dataset di tag LID per code switch. Si fanno anche modifiche architetturali, come connessioni residuali, per aiutare con il code switching. Si dimostra che i risultati combinati del metodo Switch MLM o Frequency MLM con residual BERT e una perdita ausiliaria sono i migliori per l'analisi del sentimento in tutte le coppie di lingue. Si utilizzano classificatori di</sample>
    <sample id="119">L'articolo si concentra sugli modelli linguistici GPT4, GPT3, BERT e sue varianti.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Esempi di inferenza diretta sono il nome del brano, ad esempio "Easy on Me", o la sua posizione, come "la prima".</sample>
    <sample id="122">The authors are affiliated with Fudan University.</sample>
    <sample id="123">Il lavoro di Ying e Zhiyang si concentra sull'addestramento di modelli multimediali tramite l'instruzione. Con l'avanzamento dei modelli di linguaggio a grande scala, molti studi esplorano nuove paradigmi di addestramento per utilizzare modelli preaddestrati per compiti differenti in modo efficiente in termini di parametri e dati. Tuttavia, la maggior parte dei lavori precedenti si concentra sull'addestramento di istruzioni per migliorare la prestazione zero-shot sui compiti di linguaggio solo, lasciando fuori i compiti di visione computer e multimediali. In questo lavoro, si vuole indagare se l'instruzione può migliorare la generalizzazione a compiti multimediali nascosti. Inoltre, si è scoperto una disparità significativa nella disponibilità di dataset di istruzioni tra NLP e multimediali. Si è costruito il primo dataset di benchmark di addestramento di istruzioni multimediali, MultiInstruct, che contiene 62 compiti multimediali diversi coprendo</sample>
    <sample id="124">L'articolo introduce un lavoro su migliorare la ragionamento temporale dei modelli di linguaggio. Si suddivide in tre livelli: ragionamento temporale tra tempi, tra eventi e tra eventi. Si esplora il ragionamento temporale in modo più comprensivo, con un esperimento preliminare sulla predizione di anni. Si valutano tre modelli di linguaggio, tra cui ChatGPT, che dimostra un forte bias per il periodo 2000-2020. Si propone un dataset TempReason che copre tutti e tre i livelli di ragionamento temporale e un'ampia copertura temporale. Si valuta il ragionamento temporale in tre set di domande e risposte: chiuso, aperto e ReasonQA. Si propone anche una nuova strategia di allenamento per migliorare la capacità di ragionamento temporale dei modelli di linguaggio. L'articolo conclude con risultati sperimentali che dimostrano l'efficacia della nuova strategia proposta.</sample>
    <sample id="125">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli?</sample>
    <sample id="126">No, non è stato considerato come un approccio standard.</sample>
    <sample id="127">L'abstract parla di un lavoro di Nam Geu Ho, studente di master a KAIST AI in Corea, su "Large Language Models as Reasoning Teachers". Il problema è che la tecnica di "Chain of Thought" solo funziona su modelli enormi come GPT-3 o PaLM, che richiedono molta memoria e calcolo, rendendoli costosi o impossibili da deporre in molte situazioni. Propone di usare questi modelli come insegnanti di ragionamento per trasferire le loro abilità a modelli più piccoli. Introduce una nuova tecnica chiamata "Diverse Reasoning" per migliorare l'insegnamento. Analizza esempi per identificare se e perché i modelli piccoli possono fare ragionamento complesso, che in passato richiedeva modelli enormi. La presentazione inizia con una panoramica di "Chain of Thought" e come i modelli più grandi falliscono in compiti a più passaggi come le domande di matematica. Propone di utilizzare la tecnica di "Chain of Thought" su modelli molto grandi per generare soluzioni</sample>
    <sample id="128">L'articolo presenta un lavoro di collaborazione tra McGill University, Mila e Microsoft Research su "The Kid Must Have", una valutazione dell'integrazione del conoscenza da diverse fonti.Natural Language Understanding, NLU, modelli utilizzano una varietà di fonti di conoscenza, come i parametri pre-allenati e le informazioni fornite in input durante l'inferezione. Recentemente, in domini come la risposta alle domande, i modelli possono utilizzare la conoscenza pre-allenata per risolvere la task, ma spesso richiedono anche conoscenza fornita in input durante l'inferezione. Per esempio, in una frase come "John saw the newly elected president on TV", i parametri pre-allenati possono contenere informazioni su ciò che i presidenti fanno e cosa sia la TV, ma non possono conoscere chi sia John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dal momento dell'allenamento pre-allenato. Quindi, i modelli efficaci per le task di NLU intensamente conoscitive devono essere in grado di integrare e utilizzare sia la conoscenza</sample>
    <sample id="129">Gli autori hanno fornito l'esempio di gruppi contrassegnati come gruppi di colore.</sample>
    <sample id="130">I architetture dei modelli che non generalizzano in modo adeguato sono quelle che non sono state addestrate su dati recenti.</sample>
    <sample id="131">Non ho trovato i nomi dei set di dati di test nel contenuto fornito. Potresti fornire ulteriori dettagli?</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="133">L'autore opera con più modalità.</sample>
    <sample id="135">L'articolo introduce ABC eval, un approccio dimensionale per valutare l'intelligenza artificiale conversazionale. È stato sviluppato dal lab Emory NLP di Emory University, in collaborazione con Amazon Alexa AI. ABC eval si concentra su valutare diversi aspetti della qualità del dialogo, come la rilevanza delle risposte, la contradizione, l'ignoranza del partner, ecc. Rispetto a metodi esistenti come le valutazioni Likert, ABC eval è più affidabile e prevedibile. Ha ottenuto risultati migliori in termini di interannotatore concordia e spiegazione della qualità del dialogo. ABC eval permette di valutare l'intelligenza artificiale con una risoluzione maggiore. Ci sono tuttavia sfide rimaste, come le violazioni del buon senso e le risposte irrellevanti. L'articolo conclude incoraggiando l'utilizzo di ABC eval per una valutazione più accurata e precisa delle intelligenze artificiale conversazionali.</sample>
    <sample id="136">The work presented by Chad Savan and his supervisor Nafisa at the University of Sheffield focuses on numerical reasoning. They aim to address the issue of models performing poorly in numerical reasoning tasks, especially those with smaller language models. They introduce Fermat, a flexible evaluation set based on arithmetic types, to test models' understanding of numbers, mathematical operations, and training dependency. Fermat includes math worded questions extracted from Illinois Common Core, with numbers changed to mimic real-life scenarios. The evaluation shows that most models perform poorly across various aspects, and fine-tuning with math teachers' templates improves performance. The study also investigates the impact of training templates on model performance. Overall, the work aims to better understand and improve models' numerical reasoning abilities.</sample>
    <sample id="137">L'articolo presenta un dataset per la generazione di piani di casa guidati dal linguaggio, chiamato Tell2Design, pubblicato in ACL 2023. I modelli AI condizionati testuali hanno dimostrato risultati eccellenti nell'generare immagini realistiche e creative, ma non sono adatti per la generazione di progetti con restrizioni specifiche. Il dataset include 5.051 istruzioni umanamente annotate e 76.000 generate artificialmente. Le sfide principali sono la generazione di progetti con restrizioni strette, l'interpretazione del contesto del piano intero e la gestione dell'ambiguità nelle istruzioni. Il metodo proposto utilizza un modello encoder - decoder basato su Transformer per convertire le istruzioni in sequenza di bounding box per le stanze. L'obiettivo è generare un layout di piano di casa direttamente dalle istruzioni in lingua naturale.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è quella relativa all'integrazione del conoscenza acquisita durante il pretraining e quella fornita in tempo di inferenza.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Sì, Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="141">Le risorse esistenti supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati.</sample>
    <sample id="142">Ciao, sto parlando del nostro lavoro sul risolvere le espressioni di riferimento indirette per la selezione di entità. Abbiamo introdotto il corpus di entità alternative. Il mio nome è Javad Hosseini e questo è un lavoro in collaborazione con Filip Radlinski, Silvio Parodi e Ali Louis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono prendere una decisione. Considera questa domanda alternativa: "Volevi dire 'Easy on Me' o 'I Got a Feeling'?". Ecco, l'utente vuole scegliere tra questi due brani. La cosa più ovvia è utilizzare un riferimento diretto, ad esempio, citando il nome del brano "Easy on Me" o la sua posizione, la prima. Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può accadere quando l'utente non riesce a ricordare il nome del brano, o le pronunciazioni sono troppo simili l'una all'altra e difficile da disambigu</sample>
    <sample id="143">SimulST viene confrontato con le politiche Whitkey e Local Agreement, e con le architetture state dell'arte specifiche per la traduzione simultanea.</sample>
    <sample id="144">Non so qual è l'affiliazione degli autori dell'articolo. Potresti fornire più dettagli o informazioni?</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">L'abstract parla di un problema di omissione nell'estrazione di sommari da dialoghi. Si inizia con l'introduzione del contesto, che è la sommariizzazione di dialoghi, un sotto - compito della sommariizzazione di testo. Si parla delle diverse aree di applicazione e dei progressi recenti, specialmente con i modelli di linguaggio pre - addestrati. Si evidenzia che, sebbene questi modelli generino sommari fluenti e coerenti, essi possono avere errori di omissione che rendono i sommari non utilizzabili in applicazioni reali. Si analizzano i dati e si trova che circa il 70% dei sommari generati ha problemi di omissione. Si parla della distribuzione casuale delle informazioni omesse nel dialogo e della difficoltà per i modelli attuali nell'identificare le informazioni chiave. Per analizzare meglio il problema e risolverlo, si propone di rilevare le omissioni possibili nei sommari di dialoghi. Si costruisce un nuovo dataset per fornire etichette di</sample>
    <sample id="147">Tre.</sample>
    <sample id="148">Ciao Sarah! Sono felice di conoscerti. Ti ho sentito parlare di un tuo lavoro con Matteo Negri e Marco Zorzi. Ecco un po' di informazioni su di loro: Matteo Negri è un ricercatore molto bravo e Marco Zorzi è un esperto nel campo della traduzione automatica. Ti auguro di continuare a lavorare bene con loro e di ottenere risultati ancora più eccellenti. Se hai bisogno di qualcosa, non esitare a chiedere.</sample>
    <sample id="149">No.</sample>
    <sample id="150">Il paper introduce MeetingQA, un dataset di Q&amp;A basato su interventi di partecipanti in riunioni e risposte corrispondenti. Le domande sono lunghe, aperte e richiedono discussioni. Il dataset contiene 7.700 domande, 30% non risolvibili, 40% a risposte multi-span e 48% a risposte multi-speaker. Le domande sono prevalentemente a "sì/no" ma anche richiedono risposte dettagliate e richieste di opinione. Le risposte sono composte da 12 parole in media. Il test set ha un F1 di 84.6. I metodi utilizzati includono la ricerca di contesto per modelli a contesto breve, modelli singolo-span e multi-span. I risultati mostrano un gap di oltre 25 F1 tra modelli finetuned e prestazioni umane. I modelli a contesto breve leggermente superano quelli a lungo contesto. I modelli multi-span hanno prestazioni leggermente inferiori o simili a quelli singolo-span. La</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang e io presenteremo la nostra ricerca su MultiInstruct: migliorare l'apprendimento zero-shot dei modelli multiformato attraverso l'addestramento con istruzioni. Con l'avanzamento dei modelli di linguaggio a grande scala, molti lavori hanno iniziato a esplorare nuovi paradigmi di apprendimento di riutilizzo di modelli di linguaggio preaddestrati per compiti differenti in modo efficiente in termini di parametri e dati. Recentemente, molti studi hanno dimostrato che l'addestramento con istruzioni consente ai modelli di linguaggio a grande scala di eseguire compiti diversi in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte dei lavori precedenti sull'addestramento con istruzioni si sono concentrati sulla migliore prestazione zero-shot per compiti di linguaggio solo, mentre i compiti di visione computerizzata e multiformato sono stati lasciati fuori. Quindi, in questa ricerca, vogliamo</sample>
    <sample id="152">Il lavoro di Frederik Riemenschneider si concentra sull'intersezione tra NLP e filologia classica. Presenta risorse utili per il greco e il latino antichi. Esplora le implicazioni e sfide della multilinguialità in modelli di linguaggio. Ha sviluppato modelli di linguaggio specifici per la filologia classica, come GReBERTA e GREATER, e modelli multilingue come FilBERTA e Filter. Ha raccolto dati di pre - addestramento da Internet Archive, utilizzando tecniche innovative per identificare testi in greco. Ha benchmarkato i modelli su Universal Dependencies Treebanks e EVA Latin 2022. I modelli sono superiore all'attuale stato dell'arte per greco e latino. Analizza il comportamento di GREATER con solo l'encoder del modello T5. Le prestazioni di lemmatizzazione sono migliorate di 5 punti percentuali rispetto all'attuale stato dell'arte per il greco antico. I modelli mostrano anche miglioramenti per la lemmatizzazione del latino. Esplora le capacità</sample>
    <sample id="153">Nina, a postdoctoral scientist at Amazon Alexa's Responsible AI team, presents her work on resolving ambiguities in text-to-image generative models. The work focuses on studying ambiguities in prompts provided to text-to-image models. For example, the prompt "The girl enters the room with flowers" is ambiguous. The goal is to propose frameworks to mitigate ambiguities and evaluate if generated images are faithful to users' intentions. The pipeline includes curating a benchmark dataset, using a prompt disambiguation framework, and evaluating the generated images. The findings show that the framework has a positive effect on faithful generation and that the automatic evaluation framework agrees with human evaluation.</sample>
    <sample id="154">The authors are affiliated with the University of Toronto and the Bruno Cazler Foundation.</sample>
    <sample id="155">Il nome del relatore è Javad Hosseini.</sample>
    <sample id="157">The paper introduces a dialogue summarization method called SDDS. It aims to distill silent information from dialogue context into a concise summary. This is a challenging and interesting task in text summarization research. The method uses a static - dynamic structure fusion graph. The static part is constructed using existing dialogue structure modeling methods. The dynamic part captures semantic relationships between utterances based on their deep vector representations. A pre - trained language model is used as the summary generator to fuse the static and dynamic dialogue structures into the final summary. The paper also proposes four heuristic dialogue structure modeling methods to capture static dialogue structure information, including discourse parsing graph, speaker relationship modeling method, and positional graph. These methods help build relationships between utterances and capture fine - grained interactions between speakers. The SDDS model is shown to be effective in summarizing dialogue contexts.</sample>
    <sample id="158">Dual Cache è un metodo per la risoluzione di coreferenza in documenti lunghi. Il compito di coreferenza è identificare le menzioni e clusterizzare quelle che si riferiscono allo stesso ente. Con i metodi convenzionali, si deve enumerare tutte le coppie di menzioni, che ha una complessità quadratica. I metodi basati su cache recentemente proposti usano una cache di dimensioni fisse e riducono la complessità a livello lineare. Tuttavia, nella cache basata, quando la cache è piena, si evicano entità con una politica di evicione come LRU. Ma in documenti lunghi, il tema cambia spesso e le menzioni degli enti si trovano in un vasto range di testo. La politica LRU porta a un alto rate di miss nella cache quando si incontra una nuova menzione. I nostri studi hanno mostrato che gli enti ad alta frequenza sono menzionati globalmente e fanno la maggior parte delle cache miss. Proponiamo Dual Cache con una cache locale e una cache globale. La cache locale memorizza entità</sample>
    <sample id="159">Ciao a tutti, sono Costas Senna e sono felice di accogliervi al nostro talk sulle nostre tesi di ACL 2023. I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti al contesto. Questo è un lavoro a squadra con John Gog, Aaron Muller, Kanishka Mishra, Karen Venters, Roger Levy e Athena Villas. Quindi, in questo lavoro, rivediamo il paradigma del "minimale pair". Il paradigma del "minimale pair" valuta i modelli di linguaggio sulla base dei giudizi di accettabilità, che possono includere grammaticità, come gli esempi di sintassi errata o accettabilità in termini di stereotipi, come i "crows pairs". Nell'approccio del "minimale pair", il modo tipico per valutare i modelli di linguaggio è mostrare una frase accettabile o grammaticalmente corretta e poi mostrare una frase non accettabile o grammaticalmente errata. E la speranza è che il modello</sample>
    <sample id="160">unordered multi set</sample>
    <sample id="161">55000.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo di mass align.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato è economico perché non richiede manualmente etichettare i dati.</sample>
    <sample id="165">The paper presents an approach to adaptive reasoning, starting with a context and an outcome. It uses an example of Emily stuck in traffic but making her flight. The goal is to find a plausible explanation that bridges the gap. The paper considers a closed - world setting with a candidate set of explanations. Current approaches rely on supervision, which is noisy and subjective. The paper proposes an unsupervised learning method called LiPoR, which stands for Likelihood Learning with Posterior Regularization. It treats explanations as a latent variable and optimizes an unsupervised objective to maximize the marginal likelihood of the outcome given the context. However, this objective alone doesn't prefer plausible explanations. An additional regularizer is needed, which enforces mutual exclusivity among explanations. The LiPoR objective consists of maximizing the likelihood of outcomes and preferring some explanations over others. The regularizer, denoted by Omega, takes the max between the entropy of P of Z given X, Y and the log of M, where M is the number of plausible explanations. When the entropy is larger than the log of M, it means there are more than M explanations receiving probability mass. In that case, the entropy of P of Z given X, Y is minimized, preferring a subset of explanations. This approach</sample>
    <sample id="166">L'articolo introduce un nuovo framework per la ricerca immagine da test linguistici complessi. Questa ricerca è difficile perché le immagini sono simili e le descrizioni lunghe. I modelli di linguaggio visuale tradizionali performano bene nelle tare di ricerca immagine - frase ma falliscono con test linguistici complessi. Per risolvere questo problema, l'autore si ispira alla strategia di divide e conquista e alla teoria del processo umano. Si propone di utilizzare un modello di generazione di proposizioni per decomporre le complesse proposizioni in rappresentazioni di proposizioni semplici. Un modello di interazione visivo - linguistica, System 1, esegue le operazioni visive e interagisce con le informazioni. Il modello di ragionamento simbolico, System 2, integra i risultati delle operazioni simboliche per ottenere la soluzione finale. I risultati sperimentali dimostrano che il metodo proposto, ND-DR, supera le baselines. Inoltre, due casi specifici verificano la performance del metodo.</sample>
    <sample id="167">In DEplain-web, 750 documenti sono stati allineati manualmente e con metodi di allineamento automatici.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo dati da Reuters news del 2020 e annotandoli con le stesse linee guida di annotazione di CoNLL 2003.</sample>
    <sample id="169">Il paper presenta un modello di linguaggio a 540 milioni di parametri chiamato Palm. È stato addestrato su un vasto insieme di testo con 780 miliardi di token. Al momento della pubblicazione, ha raggiunto prestazioni state of the art in molte tesi di ML. È la prima studio sistematico sullo prompting per traduzione di modello di linguaggio a larghe scale. Si valuta la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità AMT. Si confronta con sistemi state of the art. Si usano metriche state of the art e si mostrano anche risultati di valutazione umana basata su esperti. Si danno alcune raccomandazioni per le strategie di selezione di prompts. L'impulso ha un grande impatto sulle prestazioni dei modelli di linguaggio a traduzione. Si usa un semplice esperimento con impulsi a uno e cinque shot. Si conclude che la qualità degli esempi è più importante della somiglianza con la frase di origine. Si usa il dataset di valutazione di</sample>
    <sample id="170">Ciao a tutti, il mio nome è Justin Zhang e sono dell'Università di Pennsylvania. Oggi presenterò il nostro lavoro, Exemplar: Parsing Semantico Translingue in diversi linguaggi naturali e rappresentazioni multiple. Il parsing semantico è una task per costruire rappresentazioni semantiche di query utente, come SQL e Lambda Calculus. Il parsing semantico translingue è la task di tradurre query in diversi linguaggi naturali in rappresentazioni multiple, come mostrato in questa figura. Dobbiamo tradurre la query in diversi linguaggi naturali utilizzando modelli neurali in SQL, Lambda o FunQL, ecc. I modelli esistenti di parsing semantico translingue sono proposti e valutati separatamente su dataset di task e applicazioni limitati. Ad esempio, mancano coperture su certi linguaggi naturali, come il cinese, e su certi rappresentazioni multiple, come il Lambda Calculus. O solo valutati su certi modelli neurali, come ad esempio solo un modello per valutarli. Per questo motivo, abbiamo proposto Exemplar, che for</sample>
    <sample id="171">I work can be broadly classified into four categories.</sample>
    <sample id="172">No, non sono sufficienti per il CLSP.</sample>
    <sample id="174">The paper introduces Argo Analysis 35k, a large-scale dataset for argument quality analysis. It explains why this dataset is unique compared to others. Argo Analysis 35k is the largest with high-quality arguments, sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. It has a diverse range of arguments based on 24 themes. The dataset includes analysis, which is a combination of claims, premises, and more. This analysis helps explain the argument better. The paper also mentions instance-based annotator reliability to address annotator biases. This dataset is useful for research in argument quality analysis.</sample>
    <sample id="175">Induce l'alignment come parte della formazione.</sample>
    <sample id="176">L'equità di un modello NLP a valle si definisce come la capacità del modello di non presentare bias politici che possano portare a problemi di equità in applicazioni NLP.</sample>
    <sample id="177">Yannick Slavac</sample>
    <sample id="178">Kostas Sina</sample>
    <sample id="179">L'abstract parla di come i modelli di linguaggio possano comprendere il "theory of mind", ovvero la capacità di ragionare sulle mentalità degli altri. Si usa il test di Sally-Anne come esempio. I modelli di linguaggio fanno ancora pessimo su compiti di "false belief". Per migliorare, si propone un metodo chiamato Symbolic Tom che usa rappresentazioni grafiche esplicite. Questo metodo precalcola rappresentazioni grafiche per tutte le combinazioni di personaggi fino a un livello di "theory of mind" predefinito. Si usa un algoritmo di inferenza per calcolare queste rappresentazioni. Dopo averle precalcolate, si può rispondere efficacemente a qualsiasi domanda. Si testa il metodo con vari modelli e si vede che aumenta significativamente la performance, specialmente per domande di "second order false belief". Si progetta anche nuovi dataset per testare la generalizzazione del metodo.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">This paper introduces a work on distinguishing script knowledge from large language models for constrained language planning. It focuses on planning for goals with specific constraints, unlike previous work which mainly dealt with abstract goals of stereotypical activities. The authors define the problem of constrained language planning, which involves different constraints on goals. They evaluate and improve the constrained language planning ability of large language models. To acquire specific goals, they extend abstract goals with multifaceted constraints and use InstructGPT for human - in - the - loop data acquisition. They find that all large language models perform unsatisfactorily on planning for specific goals. Detailed analysis reveals that while semantic completeness is acceptable, faithfulness to constraints cannot be guaranteed. They then propose an over - generated then filter method to improve generation quality. This method involves showing constraint types to InstructGPT, obtaining specific goals, over - generating scripts, and using a filter model to select faithful scripts. The method greatly improves planning ability in semantic completeness and faithfulness to constraints. The authors also discuss the importance of creating a dataset for constrained language planning, which is costly and time - consuming. They follow the idea of symbolic knowledge distillation to distill a constrained language planning dataset from large language models. They generate 55, 000</sample>
    <sample id="182">Non c'è informazione specifica sul tropicalismo in questo articolo. Potresti fornire più dettagli o contesto?</sample>
    <sample id="183">Gli autori hanno utilizzato la proprietà che i modelli linguistici recenti sono molto bravi a rispondere a istruzioni e promemoria. Così hanno chiesto al modello di generare un personaggio, una rappresentazione di un individuo immaginario, utilizzando un promemoria come "immagina di essere una donna asiatica, descriviti". In questo modo, possono essere generalizzati a qualsiasi demografia, basta specificare l'identità che si vuole.</sample>
    <sample id="184">In questo lavoro è stato utilizzato il CXMI, Contextualized Mutual Information, per misurare l'utilizzo del contesto.</sample>
    <sample id="185">DrBERT è basato su Roberta e addestrato su NACHOS, mentre ChuBERT è basato su anonymized data ottenute dal non-universitario ospedale.</sample>
    <sample id="187">Due.</sample>
    <sample id="188">L'apprendimento iterativo è un processo in cui si aggiornano gradualmente i modelli con nuovi dati raccolti in ogni iterazione.</sample>
    <sample id="189">L'obiettivo del set di dati è comprendere il linguaggio degli utenti quando fanno una scelta.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS aprendo il modello tramite imparare dagli embedding e fornendo servizi simili.</sample>
    <sample id="191">There are three authors involved in the article.</sample>
    <sample id="192">C'è un problema con la presentazione. Non ho capito bene cosa stai chiedendo. Potresti ripetere o fornire più dettagli?</sample>
    <sample id="193">I'm not sure how many annotators were used. You might need to check the paper or the project documentation for that specific information.</sample>
    <sample id="194">I'm sorry, but the information about the affiliations of the authors is not provided in the text you've given me. Could you perhaps provide more details or context?</sample>
    <sample id="195">L'articolo introduce ROHT, un framework per la risoluzione di domande complesse attraverso una decomposizione gerarchica. ROHT si basa su due fasi: prima, costruire un albero di decomposizione gerarchica delle domande complesse, quindi utilizzare ragionamento probabilistico per fusi conoscenze da base di conoscenza e corpus testuale a diversi livelli dell'albero. ROHT può risolvere domande complesse in modo probabilistico, partendo dal nodo radice verso i nodi foglia, considerando la probabilità di generazione e risposta di ogni nodo. ROHT è stato valutato su due dataset di QA complesse, KQA Pro e Music. Con KQA Pro, ROHT KB model supera metodi di KB QA esistenti solo utilizzando la base di conoscenza incompleta, mentre ROHT Mix, utilizzando anche Wikipedia come corpus testuale supplementare, mostra un miglioramento significativo rispetto a ROHT KB model. Questo dimostra l'efficacia di utilizzare conoscenze sia dalla base di conoscenza che dal corpus testuale.</sample>
    <sample id="196">L'esempio in cui il governatore è a sinistra è "Lisa Bart and Maggie".</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli selezionati per l'analisi con ABC eval.</sample>
    <sample id="198">Perché i modelli linguistici stanno avendo finestre di contesto sempre più lunghe.</sample>
    <sample id="199">No, il modello inglese monolingue ha solo un calo nelle prestazioni in sette dataset.</sample>
    <sample id="200">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="201">Siamo utilizzati state of the art MT metriche.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione può influire su specifici tipi di NER.</sample>
    <sample id="203">Perché le NLP diventano sempre più soggettive e orientate socialmente.</sample>
    <sample id="204">Con una messa a punto integrale.</sample>
    <sample id="205">The abstract discusses a study on political biases in language models. It starts by explaining that language models are trained on large web data, including political news media. This creates a mixed blessing - diverse perspectives on one hand, but social biases on the other. The study investigates the political bias propagation pipeline from pre - training data to language models to downstream tasks. It evaluates the political leanings of language models using political questionnaires and finds that models have varying political leanings, with GPT - 4 being the most liberal. The study also shows that political biases are picked up from training data. It then looks at how different political - leaning models perform on hate speech and fake news detection tasks, finding that they give different predictions based on social categories. This indicates a fairness issue in language models, as right - leaning models might marginalize people with opposite political opinions.</sample>
    <sample id="206">Non so. Potresti dare più dettagli o informazioni?</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono quelli più avanzati della comunità AMT.</sample>
    <sample id="208">I'm not sure. Can you give me more details?</sample>
    <sample id="209">Il metodo proposto migliora significativamente la capacità di pianificazione linguistica sia in termini di completezza semantica che di fedeltà alle restrizioni.</sample>
    <sample id="210">The name of the presenter is Xu Hang.</sample>
    <sample id="211">Sì, possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">L'articolo non specifica quanti modelli più piccoli vengono utilizzati.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">L'articolo esplora la struttura di dipendenza della coordinazione. Diversi approcci teorici assumono strutture diverse. Ad esempio, nell'approccio universale, la prima congiunta è la testa della struttura coordinata. Nell'approccio di Chomsky, anche qui la struttura è guidata dalla prima congiunta. Questi due approcci sono simmetrici. Invece, l'approccio simmetrico alla struttura coordinata, come nell'approccio di Plag, ha la congiunzione come testa. Infine, c'è un approccio multi-testa, come nell'approccio di Cutts, dove tutte le congiunzioni sono teste della struttura coordinata. L'obiettivo dell'articolo è presentare un argomento nuovo per le strutture simmetriche di coordinazione e contro quelle asimmetriche. L'argomento si basa sul principio di minimizzazione delle dipendenze. In inglese, gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi</sample>
    <sample id="217">L'articolo introduce un lavoro su generazione di dialogo controllabile con attributi multipli. I motivi sono che metodi precedenti si concentrano su singoli attributi e ignorano il contesto pratico della generazione con attributi multipli. I metodi per generazione di testo con attributi multipli combinano controller apprenduti da singoli attributi con specifici etichettatori, ma non per attributi continui. La controllabilità del CTG è limitata da dati annotati e una metrica di valutazione unificata è necessaria per esplorarla ulteriormente.Noi esploriamo la generazione compositiva per dialoghi controllabili con attributi multipli. Abbiamo trovato che esistenti modelli mancano di capacità di generazione. Proponiamo DCG, un generatore controllabile distinto che impara concetti di attributo da valori visti e utilizza la perdita distinta per discogliere combinazioni di attributi distinte. Introduciamo un quadro di valutazione unificato e senza riferimento, MAE, per diversi</sample>
    <sample id="218">L'autore è affiliato a Google Translate.</sample>
    <sample id="219">L'abstract parla di un lavoro di ricerca di Jiahui Zhu, assistente di ricerca ad Academia Sinica, che si concentra su un pipeline a più fasi per scoprire segnali finanziari in report finanziari. Il lavoro è stato fatto con Yu Xiang Huang e Chen Weiling, con il consiglio del professor Zeli a Tsinghua University. Il background del lavoro è l'analisi dei report finanziari, il cui obiettivo è quello di estrarre informazioni utili. Si considera il report 10-K come corpus di riferimento, che contiene dettagli importanti delle attività delle aziende. Tuttavia, estrarre informazioni utili richiede molta fatica umana. Il lavoro è motivato da due osservazioni: le parole nei report delle aziende sono molto simili, circa il 80% dei token è lo stesso, e i contenuti sono fortemente dipendenti. Si introduce una comparsa di evidenziamento e un pipeline a più fasi. Si definisce una struttura di riferimento per il compito di evidenziamento, che confronta il contesto tra il</sample>
    <sample id="220">I'm sorry, but the information about the authors' affiliations is not provided in the text you've given. Could you perhaps provide more details or context?</sample>
    <sample id="221">L'articolo non specifica le coppie linguistiche analizzate.</sample>
    <sample id="222">L'articolo esplora le sfide e le interruzioni nell'QA di dominio aperto. Si prende in esame un esempio di domanda riguardante Narora, Kakrapar e Tarapur. Per rispondere, si cerca di trovare passaggi pertinenti da un corpus di Wikipedia utilizzando un modello di ritiro. Poi, un modello di lettore prende in input la domanda e i passaggi pertinenti per generare la risposta. Tuttavia, se si vuole rispondere a domande biomediche, il corpus di Wikipedia può rispondere solo a poche domande specifiche del dominio. Si può anche sostituire Wikipedia con un corpus biomedico, ma questo può non essere sufficiente poiché i domini sono sparsi e il modello di base è addestrato su Wikipedia. Per abilitare la trasformazione, si può espandere la raccolta di documenti per includere documenti biomedici. Tuttavia, quando si ritira, le prime passaggi si concentrano su parole come "piante", causando una risposta errata dal modello di lettore.</sample>
    <sample id="223">The name of the speaker is Zhang Bin.</sample>
    <sample id="224">I modelli studiati sono stati quelli automatici per l'allineamento.</sample>
    <sample id="225">Per scopi di addestramento vengono utilizzate 53 attività, mentre per test vengono utilizzate tutte le attività del gruppo Common Sense Reasoning e 5 attività aggiuntive dal gruppo WikiQA e Miscellaneous.</sample>
    <sample id="226">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli?</sample>
    <sample id="227">Language models have achieved great success in various NLP tasks. However, grounded language understanding, which involves mapping natural language expressions to executable representations in specific environments, is lacking. This is due to the lack of grounding during pre-training. Most language models, including large language models, are pre-trained with text corpora without grounding. This gap between pre-training and downstream applications makes grounded language understanding challenging. Existing research often uses language models to directly generate plans, but the generated plans may not always be grammatical or valid. Our work proposes a novel framework called Pangu for grounded language understanding. In this framework, a symbolic agent interacts with the environment and proposes candidate plans, while language models only score and rank these candidates. This separation allows language models to focus on discrimination instead of generation, making it easier for them to excel. We experiment with different language models and settings, and Pangu achieves outstanding performance across all settings, demonstrating strong sample efficiency.</sample>
    <sample id="228">Gli autori hanno effettuato i test su quattro set di dati: AG News, Mind, SSD2 e Iris Spam.</sample>
    <sample id="229">L'articolo introduce il problema di identificare le affermazioni argomentative che possono essere migliorate per il supporto alla scrittura argomentativa. Si concentra su due compiti: la deteczione delle affermazioni sub-ideali e le suggerimenti di miglioramento per le affermazioni. Si esplorano le sfide legate al lavoro con dati di revisione, come la rappresentatività e la affidabilità del dataset, la complessità e l'architettura del modello, le dimensioni della qualità argomentativa dipendenti dall'informazione contestuale e il bias topico e utente. Si analizzano i modelli con diverse architetture e complessità per valutare l'efficacia dell'assegnazione delle affermazioni. L'obiettivo è migliorare la comunicazione efficace del messaggio e stimolare la reazione desiderata.</sample>
    <sample id="231">NACHOS è un dataset di dati medici crawlati dalla rete.</sample>
    <sample id="232">Il nome della relatrice o del relatore è Ida Villar.</sample>
    <sample id="233">Simultaneous speech translation, Simo ST, is the real - time translation of spoken language into text in another language. Current models face problems like complex training procedures and need different models for different latency regimes. Our solution uses existing offline models without retraining, uses one model for all latency regimes, and handles latency through specific parameters. We propose a dot or encoder - decoder attention strategy. It decides whether to emit partial translation based on attention concentration. If the sum of cross - attention weights towards the last lambda speech frames is below a certain threshold, words are emitted. Otherwise, they are not. The results show that our approach outperforms other strategies in terms of translation quality and latency, especially when considering computational time. The paper and code are open - sourced for reproducibility.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati. In un semplice esperimento con uno shot prompting, il cambiamento osservato in 516 su 1000 frasi è di più di un punto di errore, e in casi estremi può arrivare fino a 40 punti di errore. È importante selezionare una buona strategia di prompting.</sample>
    <sample id="235">I'm sorry, but the information about the affiliations of the authors is not provided in the text you've given. Could you provide more details or context?</sample>
    <sample id="236">Non sono state fornite le 5 istruzioni scritte da esperti nel testo. Potresti dare più dettagli o cercare di trovare queste informazioni altrove?</sample>
    <sample id="237">Propongono un insieme di test diagnostici per l'integrazione del conoscenza.</sample>
    <sample id="238">Il video introduce un nuovo dataset per riassunti di riunioni, MeetingBank. Si parla della necessità di riassunti di alta qualità e di risorse affidabili per riunioni pubbliche. Il dataset include riunioni di consiglio comunale, con trascrizioni, riassunti riferimento e URL utili. Il processo di raccolta dati coinvolge la conversione audio in trascrizioni, identificazione di riunioni, localizzazione di riassunti riferimento e segmenti di riunione, allineamento dei tempi e unione con i riassunti riferimento. Il dataset contiene 1.366 riunioni di consiglio comunale e quasi 7.000 istanze. Si misurano le statistiche di riunione e i livelli di astrazione nei riassunti. Per l'analisi dei dati, si misurano i livelli di astrazione con la copertura e la densità. Per l'valutazione del modello, si valutano i top 10 sistemi di riassunto su test set di MeetingBank. Si</sample>
    <sample id="239">Ciao a tutti, il mio nome è Ida Villar e leggerò un breve riassunto del documento "Prompting BART for Translation: Assessing Strategies and Performance". Questo è un lavoro in collaborazione con i miei colleghi di Google Translate.BART è un modello di linguaggio a 540 milioni di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testo, composta da 780 miliardi di token. All'epoca della pubblicazione, ha raggiunto i risultati più avanzati in centinaia di compiti di NLP.In questo lavoro, presentiamo la prima studio sistematico del prompting per modelli di linguaggio a larghe scale per la traduzione automatica. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità AMT. Questo implica l'uso dei set di test più recenti per evitare che i dati di test si sovrappongano con i dati di addestramento del modello di linguaggio. Abbiamo comparato due sistemi</sample>
    <sample id="240">Ciao, sono David, un dottorando presso l'Università di Stoccarda in Germania. In questo video vorrei presentare il nostro lavoro recente "Wider than you think: A critical look at weakly supervised learning". Questo è un lavoro in collaborazione con Xiaoyu Shen, Mario Smuth, Stefan G. and D. T. Clark. Vorrei iniziare con una breve introduzione a supervisione debole e apprendimento supervisionato debole.Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole heuristiche, basi di conoscenza o fonti di etichettatura di qualità bassa, come illustrato nella figura a destra. Quando confrontati con le annotazioni umane, le annotazioni debole sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono incorrette. Se addestriamo direttamente reti neurali su dati etichettati debole, le reti neurali tendono a memorizzare</sample>
    <sample id="241">L'articolo discute un approccio per la valutazione di sistemi di rilevamento di fake news in tempo reale, con un caso di studio sui trattamenti per il COVID-19. Si critica l'uso di dataset retrospettivi e la possibilità di counter evidence rivelata dopo la pubblicazione. Si propone un quadro di valutazione per sistemi che integrano feedback umano e sono assistivi piuttosto che autoritativi. Il sistema è diviso in due componenti: la rilevazione di false affermazioni e la verifica delle violazioni di politica. La prima parte utilizza un modello T5 per estrarre affermazioni da tweet raw e le classifica per trend. La seconda parte utilizza un modello di classificazione per identificare tweet che violano le politiche social media. Si valuta l'efficacia del sistema in termini di precisione e di lavoro umano richiesto.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo sono l'uso di giudici umani per selezionare quale delle due conversazioni è migliore o per valutare le conversazioni utilizzando una scala Likert.</sample>
    <sample id="243">There are five authors involved in the article.</sample>
    <sample id="244">Per risolvere il problema con Servin e Kea, sono necessarie conoscenze di base come "Servin è un giudice" e "giudici decidono casi in tribunali".</sample>
    <sample id="245">The work presents a two - step pipeline for finding high - agreement Amazon Mechanical Turk workers. It starts with qualification settings, including a pre - task qualification and two steps of qualification task and endurance task. The pre - task qualification tests workers' ability to evaluate multiple dimensions correctly. The first stage qualification task results in 26 workers, 8 gold and 18 silver. The second stage endurance task tests capacity for handling heavy workload, resulting in 12 workers, 4 gold and 8 silver. The reference - based task tests general performance on true annotation tasks, with 8 out of 12 workers finishing all hits. The baseline AMT workers achieve a Krippendorff's alpha of 0.38 with a threshold of 0.5. Cloud - research AMT workers have a Krippendorff's alpha of 0.513 but lower task acceptance rate. The pipeline and cloud - research workers have a significant Spearman's correlation. The pipeline results in 4 gold and 8 silver workers, serving as a best practice for high - agreement annotation at large scale and lower cost.</sample>
    <sample id="246">No, il codice non è disponibile. Se hai bisogno di ulteriori informazioni, puoi chiedere.</sample>
    <sample id="247">The paper presents a new task called KG - fact verification via reasoning on knowledge graphs. Existing datasets like FEVER and VITAMIN C use Wikipedia text or tables as evidence, but none utilize knowledge graphs with natural language claims. The authors propose a new dataset, KG - fact verification, which uses DBpedia as the knowledge graph. Claims are in two styles: written and colloquial. There are five types of reasoning: one - hop, conjunction, existence, multi - hop, and negation. For example, one - hop claims can be verified by checking if two entities in the claim are connected by a relation. The dataset includes claims in both styles for practical use. Two methods were used: a colloquial - style transfer model and presupposition templates. The dataset has a good number of claims and evidence. Two baselines were constructed: claim - only and knowledge - graph - only. This work aims to enable reliable fact verification using knowledge graphs, which can be applied in modern dialogue systems for consistency checks.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore all'input.</sample>
    <sample id="250">Avere una valutazione dimensionale significa valutare diverse aspetti o dimensioni di qualcosa, in questo caso, la qualità del dialogo.</sample>
    <sample id="251">The authors are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">L'abstract parla di un lavoro di Master studenti di AI a Dehradun. Presentano un'approccio per la ricerca di casi precedenti in modo non supervisionato utilizzando l'estrazione di eventi. I legali tradizionalmente si affidano all'esperienza per citare documenti precedenti, ma con un numero crescente di casi, diventa difficile. La ricerca propone due contributi: il dataset ILPCR, Indian Legal Prior Case Retrieval, e il pipeline UCreate. ILPCR è un nuovo benchmark con 7.070 casi legali e 6.775 citazioni per documento. UCreate utilizza tecniche di apprendimento non supervisionato e un approccio basato su eventi per la ricerca di casi precedenti. L'estrazione di eventi è cruciale per rappresentare i documenti legali come una serie di eventi. Il pipeline prevede tre passaggi: pre-processamento, parsing di dipendenza e post-processamento. I modelli esaminati includono count-based, transformer-based e event-based. I transformer-based modelli hanno prestazioni drasticamente inferiori rispetto ai metodi di base come BM</sample>
    <sample id="253">L'abstract parla di un modello di adattamento di dominio chiamato Disorder per rilevare segni di disturbi mentali sui social media. È un lavoro di gruppo di ricercatori messicani e spagnoli. Disorder definisce i disturbi mentali come sindromi psicologiche associate a disagio e disabilità che influenzano il pensiero, le emozioni, l'umore e il comportamento. Ci sono diversi tipi di disturbi mentali come depressione maggiore, PTSD, bulimia, anoressia, ecc. I social media offrono un'enorme quantità di contenuti che possono essere utilizzati per ricerche su come le persone affrontano le difficoltà. Disorder mira a contribuire alla deteczione di disturbi mentali analizzando automaticamente i post sui social media. Questo tipo di analisi potrebbe supportare una nuova tecnologia che allerta sulla presenza di disturbi mentali e fornisce prove. Disorder utilizza l'adattamento di dominio perché spesso non abbiamo abbastanza dati annotati e vogliamo migliorare il rendimento del modello</sample>
    <sample id="254">L'abstract parla di un lavoro di ricerca su un'analisi delle relazioni a livello di documento. Si parla di come le vecchie metodologie dipendono da grandi corpus annotati manualmente, che sono costose e richiedono molta lavoro. Recentemente, si è utilizzato data distante supervisionata per pre - addestrare i modelli di estrazione delle relazioni a livello di documento. Tuttavia, questa data contiene vari livelli di rumore. I metodi attuali utilizzano etichette pseudo per alleviare il problema del rumore, ma esistono rischi di introduzione di etichette pseudo false positive. Il lavoro propone un quadro di estrazione delle relazioni a livello di documento con denoising guidato dall'incertezza per migliorare la qualità delle etichette della data distante. Si addestra un modello di pre - denoising con entrambi i dati distanti e manualmente annotati per generare le etichette pseudo. Si introduce un' stima dell' incertezza per determinare la credibilità delle predizioni del modello. Si propone un' stima dell</sample>
    <sample id="255">La forma del prompting si rivela importante per zero e uno shot prompting.</sample>
    <sample id="257">Hanno valutato quattro modelli di dialogo di stato dell'arte.</sample>
    <sample id="258">In this work, we propose using large language models to evaluate text quality in natural language processing. We give instructions to the models and use them to rate samples. There are no prior works on large language model evaluation. The motivation is to find an alternative to human evaluation that is more stable. We conduct an experiment rating stories generated by GPT - 2 or written by humans. We use human evaluation results as ground truth ratings. English teachers rate the stories as experts. The overall result shows that some large language models, like Davinci and ChatGPT, show a clear preference for human - written text, similar to human raters. This suggests that large language models can be used as an alternative to human evaluation in this task.</sample>
    <sample id="259">L'abstract parla di un lavoro di John da Penn State University su Exemplar, un modello per la parsing semantica multilingue. Esso si occupa di tradurre query in diverse lingue naturali in rappresentazioni multiple, come SQL e Lambda Calculus. I modelli esistenti hanno limitazioni come mancanza di copertura su certe lingue o rappresentazioni. Exemplar fornisce un dataset unificato per la parsing semantica multilingue, con 90 set in diversi domini, 5 task di parsing semantico, 8 rappresentazioni e 22 lingue in 15 famiglie linguistiche. Viene considerato 6 set di allenamento e valutazione, tra cui tradurre test, modello monolingue, modello monolingue a pochi shot, modello multilingue, zero shot e few shot crosslingue. Si analizzano modelli monolingue e multilingue, con encoder-decoder ottenendo il miglior prestigio su tutti i 9 dataset. Si confrontano i modelli su MT5 e XLM-R + PDR in un contesto multilingue</sample>
    <sample id="260">The paper is written by one author.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="262">The content doesn't specify the number of authors involved in the article.</sample>
    <sample id="263">L'abstract parla di un lavoro che si concentra sulla mitigazione delle bias di etichetta nell'apprendimento in contesto. L'apprendimento in contesto è un paradigma popolare per l'utilizzo di modelli di linguaggio virtuale, ma è noto per essere instabile a causa di diverse scelte di progettazione, come l'ordine degli esempi in contesto. Prima del lavoro, non c'era una discussione sistematica per categorizzare i bias esistenti e per identificare nuovi tipi di bias. Il lavoro propone una nuova tipologia di bias di etichetta e identifica un nuovo tipo importante di bias, chiamato bias di etichetta di dominio. Propone anche un metodo di calibrazione per gestire tutti i tipi di bias. Si considera una configurazione di classificazione con un contesto composto da esempi etichettati, un testo da classificare e nomi di etichetta. Si definiscono tre tipi di bias: bias di etichetta vanilla, bias di contesto e bias di dominio. Per confermare l'effetto del corpus di comp</sample>
    <sample id="264">The paper presents a novel task called Transferable Audio Visual Text Generation. It addresses the challenges of multimodal text generation, especially audio - visual text generation, which has more diverse and expensive data annotation. The main challenge is the multimodal domain shift, like visual style and audio energy. The framework proposed includes an Audio - Visual Meta - Network, a Transfer - Transformer - based encoder and generator, and Counterfactual Contrastive Learning. The Audio - Visual Meta - Network maps visual concepts across domains into a unified auditory semantic space. The Transfer - Transformer - based encoder and generator use an alpha - tuning mechanism to evaluate the contribution of different modalities. Counterfactual Contrastive Learning provides a constraint for AVMM to directly optimize visual - audio alignment scores. Experiments on MSVTT and MSVD benchmarks show that the proposed approach outperforms state - of - the - art methods in cross - dataset and cross - domain settings.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">Non so.</sample>
    <sample id="268">I errori più comuni di PaLM sono gli omission errors.</sample>
    <sample id="269">Ciao， sono James Finch e sono Sarah Finch e oggi vi parleremo di ABC eval， una nuova approccio dimensionale per l'valutazione dell'intelligenza artificiale conversazionale. Questo lavoro è stato fatto dal laboratorio Emory NLP， diretto dal professor Gino Choi all'Emory University， in collaborazione con Amazon Alexa AI. Supponiamo che tu hai appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato dell'arte attuale. La pratica comune è di utilizzare l'valutazione umana， come chiedere a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni globali dell'qualità del dialogo， ma l'qualità del dialogo ha molti aspetti. Quindi， potresti voler valutare più dimensioni della qualità del chat per comprendere le forze e le debolezze del modello a un livello più fine-grained. Un approccio è semp</sample>
    <sample id="270">The authors are affiliated with Emory NLP Lab led by Professor Gino Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">CFT significa Continual Fine Tuning.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="273">Certo. Ma non hai detto cosa deve essere tradotto.</sample>
    <sample id="274">The name of the speaker is Justin Zhang.</sample>
    <sample id="276">L'abstract parla di un lavoro che valuta metri per la traduzione di lingue indiane. Si concentra su cinque lingue, Tamil, Malayalam, Hindi, Marathi e Gujarati. Si selezionano 200 frasi dal dataset ForLoRes e si generano candidati traduzioni per ciascuna. Si utilizzano sette modelli di traduzione per ottenere 1400 candidati traduzioni per lingua. Si richiedono annotazioni umane dettagliate per ciascuna traduzione. Gli annotatori bilingue valutano le traduzioni, indicando gli errori, il loro tipo e severità, e assegnano una punteggiatura globale. Si classificano gli errori in tipi diversi come errori di accuratezza, di fluenza e speciali. Si osserva che i modelli più recenti come NLLB e IndicTrans hanno meno errori rispetto ai modelli più vecchi come CBIT. I modelli migliori in ordine decrescente sono IndicTrans, NLLB, Google API, Bing API, MT5, CBIT</sample>
    <sample id="277">The paper doesn't mention a specific name for the new method.</sample>
    <sample id="278">The author described the marked words method as a way to identify the words that distinguish marked groups from unmarked ones. It draws upon the sociolinguistic concept of markedness, comparing personas using weighted log odds ratios to distinguish the top words for each marked group.</sample>
    <sample id="279">The authors are affiliated with the University of Washington.</sample>
    <sample id="280">L'abstract parla di un lavoro che si occupa dell'emozione regolazione in conversazioni. L'obiettivo è prevedere l'etichetta emotiva di ogni enunciato in un dialogo. I modelli esistenti si concentrano sulla modellizzazione del parlante e del contesto, ma non esplorano bene la complementarietà delle informazioni multimediali. Il lavoro propone MultiEmo, un framework a livello di attenzione basato sulla fusione multimediale. Include un estrattore di feature unimodale, un modello di fusione multimediale basato su layer a testa multipla bidirezionali, una perdita focal contrastiva pesata per le classi minoritarie e semantica simile, e ha ottenuto prestazioni state dell'arte sui dataset Meld e IEMOCAP.</sample>
    <sample id="281">The work titled "When does translation require context? A data - driven multilingual exploration" by Coyote et al. explores how context affects translation. It shows that context is crucial for translating words like "mole" correctly. For example, "mole" can mean a spy or a birthmark depending on the context. The work also discusses the difficulty in evaluating models for context - dependent translations due to the small portion of translations that rely on context. To address this, they introduce a new metric called p - CXMI to measure context usage at the sentence or word level. They analyze words with high p - CXMI in TED talk transcripts translated into 14 languages. They find patterns in languages like Arabic and Chinese that require context for proper translation. The work then designs a benchmark for document - level translation using the identified phenomena. When evaluated with different metrics, context - aware models perform better than context - agnostic ones. This highlights the importance of context in translation and the need for better evaluation metrics.</sample>
    <sample id="282">L'abstract parla di un nuovo lavoro presentato in ACL 2023 intitolato "StoryTrans: Non-Parallel Story Author Style Transfer with Discourse Representations and Content Enhancing". L'opera si concentra sulla trasformazione di stile di un racconto non parallelo a livello di storia, al di sopra del livello di frase o token. Finora, la maggior parte delle ricerche si è concentrata su trasformazioni di stile a livello di frase o token, come la trasformazione sentimentale o formale del testo. Questo lavoro è un passo avanti significativo, trasformando il testo a livello di storia e discorso. I principali ostacoli sono la lunghezza del testo e le complesse preferenze linguistiche dell'autore, come le strutture discorsive. La soluzione proposta è un modello di generazione chiamato StyleTrans, che impara le rappresentazioni discorsive dal testo di origine e le combina con le embedding di stile normali per generare il testo nel nuovo stile. Inoltre, è stato progettato un nuovo</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella di Lisa Bart e Maggie.</sample>
    <sample id="284">The paper presents FFUIE, a new UIE model for universal information extraction. It addresses the ambiguity in labeling spam boundaries and the mismatch between transformer feature extraction and information extraction. FFUIE uses a fuzzy spam boundary mechanism, where the boundary is represented as a continuous distribution of correct probability. It employs a fuzzy spam attention layer to guide the model's division process. Experiments on three main information extraction tasks show that FFUIE outperforms UIE base on small scale datasets and achieves new SOTA results on relationship extraction and ADE tasks. The model demonstrates strong generalization capabilities and better information extraction ability with a simple structure.</sample>
    <sample id="285">The work focuses on factuality error correction for dialogue summarization. It presents two main solutions. One is introducing factuality - related objectives in training or inference to make summarization models more faithful. The other is designing a factuality error correction model, FEC, which is independent of the summarization model. FEC takes the source document and model - generated summary as input and outputs a corrected summary. There has been no work on factuality errors for dialogue summarization. The current FEC models are evaluated using factuality metrics like FactCC and DAE. However, there are flaws in the evaluation method. The proposed evaluation framework includes alignment, classification, and comparison steps. Training FEC models with reference summaries from dialogue summarization datasets yields the best results on reliable factuality metrics. There is an urgent need to change the evaluation method for FEC models.</sample>
    <sample id="286">James Finch e Sarah Finch.</sample>
    <sample id="287">Quattro.</sample>
    <sample id="288">I can't answer this question directly from the given content.</sample>
    <sample id="290">Non ho trovato le abbreviazioni dei cinque metodi per la prima domanda di ricerca nel contenuto fornito. Potresti fornire ulteriori dettagli?</sample>
    <sample id="291">Il modello viene valutato su attività come riconoscimento di nomi, classificazione, parto, posizione, tagging e risposta alle domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su 4 gigabyte di dati.</sample>
    <sample id="295">Adam Skulski</sample>
    <sample id="296">Il lavoro presentato è il risultato di una collaborazione tra l'Università di Torino e Amazon Alexa. Si concentra sull'intelligenza artificiale e il processamento del linguaggio naturale, che dipendono in gran parte dal machine learning supervisionato. Per sviluppare questi approcci, si necessitano di grandi set di dati manualmente annotati che codificano conoscenza umana. Ci sono stati lavori che mostrano i limiti dell'assunzione di una singola verità, la cosiddetta "ground truth". Il lavoro si concentra sull'ironia, un fenomeno linguistico latente e pragmatico. L'identificazione dell'ironia è già un compito difficile per i modelli di processamento del linguaggio naturale moderni. Invece di addestrare modelli che danno un etichetta binaria, si vuole avere modelli con un output più informativo. Per studiare questo problema, si è sviluppato un corpus chiamato EPIC, l'English Perspectiveivist Irony Corpus. Si è raccolto dati da diverse fonti, come social media, Reddit e Twitter, su</sample>
    <sample id="297">L'articolo parla di un progetto che analizza le "dog whistles", segnali codificati in linguaggio. Si sviluppa una tipologia e un glossario con informazioni sulle "dog whistles" razziste, transfobe e antisemite. Si esamina la frequenza di queste "dog whistles" in discorsi politici americani storici. Si valuta l'efficacia di modelli di linguaggio nel riconoscere le "dog whistles". Si dimostra come queste "dog whistles" possano evadere la moderazione dei contenuti online. Il progetto è importante per capire come funzionano le "dog whistles" e come influenzano la persuasione politica.</sample>
    <sample id="298">Perder la prestazione è causato dalla deriva temporale.</sample>
    <sample id="299">The work discusses improving the robustness of NLI models using minimax training. It points out that NLI models rely on shortcuts, which are correlations between input attributes and labels. These shortcuts make models perform well on in - distribution samples but fail on out - of - distribution adversarial tests. Current shortcut mitigation methods often require access to an auxiliary model and have limitations like needing domain - specific knowledge and assuming the learner will exploit the same shortcuts as the auxiliary. The proposed method aims to reduce reliance on shortcuts and improve out - of - distribution performance. It uses a minimax training objective between a learner and an auxiliary. The learner tries to minimize NLI task loss while the auxiliary maximizes the learner's loss by generating example weights. This encourages the learner to focus on regions with high loss, learning from under - represented hard examples that counteract shortcuts. The method doesn't assume the type of shortcuts in the dataset and uses a feed - forward network for the auxiliary. Evaluation on three NLI datasets shows that the minimax training objective consistently improves out - of - distribution performance while maintaining high in - distribution accuracy. The paper also examines if improvements transfer to larger models and synthetic shortcuts, and conducts a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">L'articolo introduce il concetto di "dictation interattiva", un processo che permette ai utenti di dictare e modificare documenti in modo naturale e intuitivo. Si distingue dai sistemi attuali che supportano solo la dictazione e non le modifiche vocali. L'obiettivo è creare un'interfaccia più naturale e intuitiva, come quella con un assistente umano. La task è caratterizzata da un intrecciato di dictazione e editing, senza parole di trigger, e l'uso di istruzioni vocali naturali e aperte. L'approccio proposto include la formalizzazione della task, la progettazione di un'interfaccia di raccolta dati e la creazione di un sistema di base. La raccolta dati avviene attraverso un'interfaccia specifica, mentre il sistema di base esegue i quattro passaggi: analisi ASR, segmentazione, estrazione e normalizzazione delle istruzioni vocali, e esecuzione delle istruzioni. L'articolo conclude con la presentazione di due architetture diverse per il modello di</sample>
    <sample id="302">Perché i token non sono ordinati dopo la prima fase.</sample>
    <sample id="303">Perché vogliono che i proprietari dei modelli aumentino la trasparenza sui metodi di mitigazione dei bias per migliorare la comprensione e la fiducia dei utenti.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono le frasi inaccettabili o non grammaticali che vengono mostrate insieme alle frasi accettabili o grammaticali nel paradigma di coppia minima.</sample>
    <sample id="305">In questo video, Dawei, un dottorando a Saarland University in Germania, presenta il loro lavoro recente "Wider than you think: A critical look at weakly supervised learning". Il lavoro è un progetto con Xiaoyu Shen, Mario Smuth, G. Stefan e D. T. Clarke. Dawei inizia con un'introduzione breve alla supervisione debole e alla supervisione debole supervisionata. Nella supervisione debole, i dati non vengono etichettati manualmente, ma invece con fonti di etichettatura debole come regole heuristiche semplici, basi di conoscenza o fonti di crowdsourcing di qualità bassa. I dati etichettati debolmente sono più economici ma anche rumorosi. Nella supervisione debole supervisionata, si propongono algoritmi per robustamente addestrare reti neurali su tale rumorosità etichettatura, in modo che i modelli addestrati generalizzino bene. Recentemente, in WSL, si dice comunemente che i modelli addestrati solo su dati etichettati debolmente raggiungono</sample>
    <sample id="306">L'abstract parla di un lavoro su tracciamento di entità in modelli di linguaggio. Un agente deve capire discorsi tracciando entità e cambiamenti di stato. Ad esempio, in un ricettario, se metti uova, zucchero e farina in una ciotola, tutte finiscono in ciotola. Se poi mescoli per formare una farina leggera, tutte diventano parte della farina. Questo è cruciale per capire discorsi lunghi. Non ci sono stati studi sistematici su come modelli di linguaggio preaddestrati gestiscono questo. Il problema è che non sappiamo il contenuto esatto dei dati di addestramento e ci sono sfide nel progettare una task per valutare il tracciamento di stato di entità. Ad esempio, alcuni stati di entità sono comuni nel dati di addestramento e il modello potrebbe prevedere correttamente senza tracciamento di entità. Inoltre, i modelli potrebbero sembrare capire il tracciamento di entità basandosi su associazioni</sample>
    <sample id="307">Hanno utilizzato metriche di valutazione per task pubbliche e private.</sample>
    <sample id="308">L'abstract parla di un lavoro di una studentessa di primo anno di PhD a Carnegie Mellon University. Presenta un'analisi di bias di progettazione in modelli e dataset di NLP. Si parla di come la posizionalità dei ricercatori e sviluppatori di modelli possa influenzare la performance dei modelli tra popolazioni diverse. Si introduce il concetto di posizionalità, che è la prospettiva di un individuo influenzata da demografia, identità e esperienze di vita. Si parla di come i dataset e modelli aggregano giudizi e opinioni di persone reali, rappresentando certe posizionalità. Si presenta un framework per studiare la posizionalità di dataset e modelli di NLP, che confronta annotazioni di utenti reali con quelle dei dataset e modelli. Si parla di come il framework differisce dalla letteratura sull'acqua di annotatori, comparando utenti finali con predizioni e etichette dei modelli e dataset. Il framework è stato attuato attraverso Lab in the Wild, una piattaforma di crowdsourcing online. Si par</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Non so qual è l'affiliazione degli autori dell'articolo. Potresti fornire più dettagli?</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto si concentra sulla generazione di un dataset di istruzioni multimediali, mentre molti studi precedenti si sono concentrati sulle performance zero-shot per compiti linguistici solo.</sample>
    <sample id="313">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="314">Non c'è una definizione di coordinazione binaria specifica qui. Potresti fornire più dettagli o contesto?</sample>
    <sample id="315">I'm sorry, but the information about how long the prompts were used in the study is not provided in the text you've given me. Could you perhaps provide more details or context about the study?</sample>
    <sample id="316">Non c'è informazione specifica sulle implicazioni dei risultati sul modello T5 più piccolo nel contenuto fornito. Potresti fornire ulteriori dettagli o contesto?</sample>
    <sample id="317">The work presents CodeIE, a code generation model for better few-shot information extractors. Information extraction, a classic NLP task, aims to extract structured info from unstructured text. Common tasks include NER and RE. Previous models using pre - trained language models like T5 and GPT - 3 operate in a text - to - text manner during pre - training but linearize structured outputs during inference, causing mismatched outputs. CodeIE transforms the text - to - structure task into a structure - to - structure code generation task, using code - like language models like CodeT5. For NER, a prompt is designed to continuously extract text and entity pairs. Evaluation on three NER datasets and four RE datasets shows CodeIE outperforms traditional models like UIE and T5. CodeIE's approach aligns better with the information extraction task, as evidenced by lower perplexity on text - format inputs and fewer structural errors when using CodeT5 and Code format prompts.</sample>
    <sample id="318">Certo! Ecco la traduzione in italiano: "Ciao, sono Janice Lavaque e presenterò i nostri lavori su Dr. Bert, un modello preaddestrato robusto in francese per il campo biomedico e clinico. Nella presentazione, iniziamo parlando di modellazione linguistica nel campo sanitario. Poi presenteremo le principali contribuzioni del nostro articolo. Introdurremo il primo modello biomedico in francese, chiamato Dr. Bert, basato su Roberta e addestrato su Natchez, un dataset di dati medici catturati dalla rete. Inoltre, faremo una comparazione dei modelli con diversi set di dati e configurazioni di preaddestramento. Successivamente, presenteremo i risultati su 11 compiti di biomedico e clinico non strutturati in francese. Infine, concluderemo sugli esperimenti e forniremo dettagli su come accedere ai modelli. Dopo la sua rilascio nel 2018, Bert è diventato uno degli approcci più efficaci</sample>
    <sample id="319">Nel lavoro vengono esaminate le strategie di apprendimento di pre-apprendimento.</sample>
    <sample id="320">The factor of overfitting due to reusing the test is not observed.</sample>
    <sample id="321">Non c'è informazione specifica sulle valutazioni della qualità della semplificazione nel contenuto fornito.</sample>
    <sample id="322">L'abstract parla di come un classificatore di testo impara la moralità. La moralità è ciò che ci aiuta a distinguere il giusto dallo sbagliato, il nostro compasso interno. È fondamentale per le società e per i modelli di linguaggio comprendere e riconoscere la moralità nei testi. L'approccio comune è considerare la moralità su una scala tra immorale e morale, ma è soggettiva e diversi individui la classificherebbero diversamente. Per esempio, l'aborto può essere considerato immorale da alcuni e morale da altri. La teoria delle fondamenta morali suggerisce che esistono cinque modi diversi in cui percepiamo la moralità, come cinque gusti differenti sulla lingua. Ogni azione o concetto "toca" un diverso aspetto morale. Ogni individuo priorizza queste fondamenta in modi diversi. La moralità è espressa diversamente in diversi domini, come in "All Lives Matter" e "Black Lives Matter". I modelli di linguaggio possono comprend</sample>
    <sample id="323">Il paper propone DHLK per migliorare il rendering di grafi dinamici con modelli di linguaggio e conoscenza per la risoluzione di domande di senso comune. Il problema principale delle altre opere è l'inserimento di entità non pertinenti nel sottografo e l'encodaggio separato del sottografo e del testo, che limita l'interazione tra i modelli. DHLK costruisce un HKG basato su diverse basi di conoscenza utilizzando una strategia di training a due fasi e KL-TR per ottimizzare la struttura e la rappresentazione di conoscenza dell'HKG. Rimuove le parole che compongono l'entità precedente e recupera le entità principali in WordNet e Victionary, aggiungendole come nodi aggiuntivi al sottografo. Encodano il contesto di domanda e le entità presenti nell'HKG utilizzando RoBERTa e Masked Self-Attention. Rimuovono le entità con relazioni deboli per il contesto di domanda basato sull'attenzione di Ro</sample>
    <sample id="324">Sì, i modelli linguistici hanno diversi bias politici.</sample>
    <sample id="325">Ciao, sono Matthias Lindemann e oggi vi darò un'introduzione breve al nostro articolo sulla generalizzazione compositiva senza alberi utilizzando multiset tagging e permutazioni latenti. Questo è un lavoro congiunto con i miei tutori, Alexander Koller e Ivan Titov.Compositional generalization può essere compresa come la capacità di un apprenditore di gestire una ricorsione più profonda e composti non visti di frasi che sono state viste individualmente durante l'addestramento.Nel contesto della parsing semantica, il test per la compositional generalization potrebbe sembrare così: come di consueto, abbiamo un set di enunciati di addestramento. In questo caso, "The girl slept" e "Mary knew that the girl slept". Questi enunciati sono associati a forme logiche che rappresentano aspetti chiave del loro significato.In contrapposizione all'addestramento standard di machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste.Nel nostro esempio, il modello ha visto ric</sample>
    <sample id="326">Cognitive dissonance è quando due credenze o azioni sono inconsistenti. Ad esempio, se una persona dice che i sigari possono ucciderla ma poi ne prende uno.</sample>
    <sample id="327">The work presents Meta Tower, a novel vision - language model architecture. It aggregates insights from unimodal experts at different levels. The architecture consists of text and visual encoders, a cross - model encoder, and managers. Managers adaptively combine insights from different unimodal experts. Meta Tower uses RoboBERTa and CLIP ViT - B as unimodal encoders. It achieves superior performance on various downstream tasks with only 4 million images for pre - training. Compared to Bridge Tower, Meta Tower significantly improves performance, especially on VQA - V2 test standard. It outperforms many base - size models trained on 4 million data and surpasses some models trained with more data or parameters. The visualization of average aggregation weights shows a similar progressive trend in each cross - model layer for textural and visual managers, which is inconsistent with the intuition that the need for unimodal semantic knowledge decreases with increasing model layers.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">This work focuses on zero-shot video sentence localization. It aims to find relevant video segments for a given natural language query. Existing methods require manual annotation, which is costly and inefficient. This paper proposes a noise - resistant structural pseudo - label generation method. It uses a pre - trained image caption model to generate complex pseudo - queries. Then, it measures relevance between video frames and pseudo - queries to generate pseudo - events. It reduces the weight of noisy samples and corrects noisy labels to reduce label noise. Experiments on two datasets show good results.</sample>
    <sample id="330">Sì, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">Sarah Bobby.</sample>
    <sample id="332">I dati sono stati tratti da un corpus di discorsi multilingue.</sample>
    <sample id="333">The work focuses on improving neural machine translation, specifically addressing the non - smooth representation space of MT models. It proposes the INK framework to inject KN knowledge. The core idea is to smooth predictions based on nearest neighbors in the representation space. However, this approach has drawbacks like time - consuming retrieval and immovable representations. To overcome these, INK is proposed. It has a two - step training loop: first, KN knowledge is extracted to guide the adapter to adjust representations, then updated representations refresh the datastore asynchronously. The framework optimizes the adapter with a combined learning objective. Experiments show that INK outperforms existing systems, achieving better BLEU scores with less memory. The framework reveals the potential of further refining the representation space of MT models.</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo in cui un modello di apprendimento automatico viene addestrato su un insieme di dati in una lingua e poi utilizzato per compiti in altre lingue.</sample>
    <sample id="337">Il lavoro presenta un approccio per gestire parole nuove nell'apprendimento di embedding. Si osserva la formazione delle parole e le associazioni con parole relevanti per indovinare il significato. Si sviluppa un grafo relazionale che imita le regole lessicali di formazione e associazione. Quando appare una parola nuova, si tokenizza in pezzi e si associa naturalmente con altre parole relevanti, formando un grafo a due livelli. Ogni parola o pezzo di parola diventa un nodo nel grafo e il suo embedding corrisponde a un feature. Si preserva il primo livello per mantenere l'informazione completa dei pezzi di parola. Nello strato secondo, si seleziona un numero fisso di nodi per mitigare il rumore dai nodi di pezzo di parola con molti vicini. Si utilizza una rete neurale grafica per processare il grafo relazionale delle parole. Per assegnare attributi ai nodi delle parole nuove, si utilizza una rete di attenzione semplice basata sui caratteri delle parole nu</sample>
    <sample id="338">L'abstract parla di un lavoro di ricerca che si chiede se le spiegazioni umane siano sempre utili per l'evaluazione obiettiva delle spiegazioni naturali umane. È un lavoro di gruppo di ricercatori da Rensselaer Polytechnic Institute, Northeastern University e IBM Research. La motivazione è che molte ricerche hanno cercato assistenza umana, come i crowdworkers e gli esperti, per annotare etichette e spiegazioni naturali umane. Prima di usare queste spiegazioni come standard, si deve valutare la qualità delle spiegazioni umane, che possono essere soggettive e dipendenti dal compito. Si presenta un diagramma che confronta le spiegazioni umane da ECQA e CoSiE su un'istanza di comune senso del curioso. Si parla di una matrice tradizionale come Blue e Rouge che considera le annotazioni umane come standard e si concentra sulla somiglianza tra parole. La matrice di simulabilità misura il cambiamento del rendimento del baseline quando le spiegazioni sono presenti o assenti,</sample>
    <sample id="339">I'm sorry, but the information about the affiliations of the authors is not provided in the text you've given. Could you perhaps provide more details or context?</sample>
    <sample id="340">L'abstract parla di un lavoro intitolato "ParaAMR", un dataset di parafilie sinteticamente diverso a grande scala ottenuto tramite backtranslation. Il lavoro è un progetto congiunto con Veron, Yi Hong, Anup, Kai Wei e Arun. L'obiettivo principale è costruire un dataset di parafilie sinteticamente diverso a grande scala. L'idea chiave è utilizzare AMR, un'abstract meaning representation che cattura il significato astratto di una frase. Si usa un parser preaddestrato per ottenere l'AMR di una frase sorgente, si cambia il focus del grafico, si modifica l'età e le età abili, e si genera testo da un generatore di testo AMR. Ci sono circa 15 milioni di frasi sorgenti in ParaAMR e circa 6,9 parafilie per frase sorgente. Parafilie ParaAMR generano più parafilie sinteticamente diverse rispetto a altri dataset che usano backtranslation. Ci sono analisi quantitative per ParaAMR, con punteggi automatic</sample>
    <sample id="341">Gli autori fanno ricorso a misure di latenza di 1 secondo e 2 secondi.</sample>
    <sample id="342">L'abstract parla di un paper presentato da Gao Jingshen e altri. Il paper si concentra su LifeChat, un dataset di dialogo personalizzato grande scala ottenuto da streaming live. Si parla dell'importanza di un dataset video per dialogo più vicino alla conversazione reale. Esistono due gruppi di dataset video di dialogo: quelli con script e quelli senza. I dataset senza script sono limitati in scala e dipendono da annotazioni manuali. Per costruire un dataset di dialogo grande scala, è cruciale trovare un meccanismo di matching efficace per catturare le relazioni di risposta tra gli interlocutori. Inoltre, il dialogo personalizzato è cruciale per applicazioni come streamer virtuali e dipendenti virtuali. Tuttavia, ci sono sfide come la mancanza di informazioni personali per rappresentare i personaggi e la mancanza di turni di dialogo per ciascun personaggio. Il dataset LifeChat è un video-sourced dataset grande scala che può aiutare a risolvere questi problemi. Il paper propone un metodo automatico di</sample>
    <sample id="343">Ciao a tutti, sono Maksym, e oggi, insieme al mio coautore Martin, stiamo presentando il nostro lavoro "The Kid Must Have: Evaluating Knowledge Integration from Multiple Sources". Questo lavoro è una collaborazione tra l'Università di McGill, Mila e Microsoft Research. I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, come quelle contenute nei loro parametri, acquisite tramite un pre-allenamento, e quelle date in input durante l'infrazione. Recenti lavori in task come la risposta alle domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pre-allenamento per risolvere il task. Ma la comprensione del linguaggio naturale spesso richiede conoscenza che viene fornita durante l'infrazione. Ad esempio, nella frase "John ha visto il presidente appena eletto in TV", i parametri pre-allenati possono contenere informazioni su ciò che i presidenti fanno e su cosa sia la TV, ma non possono conoscere con certezza chi sia l'entità specifica di John o</sample>
    <sample id="344">I svantaggi dei metodi basati su alberi sono che sono spesso non dati e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta costoso computazionalmente. Inoltre, richiedono una pre-elaborazione formale specifica delle forme logiche, ad esempio per gestire simboli variabili.</sample>
    <sample id="345">L'abstract parla di un lavoro di Matthias Lindemann su generalizzazione compositiva senza alberi utilizzando multiset tagging e permutazioni latenti. L'obiettivo è mostrare una forte generalizzazione a ricorsione più profonda senza dipendere da alberi. Il modello prevede l'output dall'input in due passaggi: etichettare ogni token di input con un multiset di token che appariranno nell'output, poi usare un modello per prevedere una permutazione per mettere i token nell'ordine giusto. Il metodo per prevedere la permutazione non ha restrizioni rigide, rendendo l'approccio flessibile e espressivo. Rispetto a modelli senza alberi basati su alberi, il modello proposto supera di gran lunga gli altri su generalizzazione a ricorsione più profonda. Tuttavia, ci sono altre forme di generalizzazione strutturale che rimangono molto difficili. Il lavoro risolve alcune sfide tecniche, come l'assenza di allineamento tra input e output nel dataset di addestramento</sample>
    <sample id="346">Non so qual è l'affiliazione degli autori dell'articolo. Potresti dare più dettagli?</sample>
    <sample id="347">Ciao, sono Myra e oggi parlerò del nostro articolo "Personaggi marcati: utilizzare promemoria di linguaggio naturale per misurare stereotipi in modelli di linguaggio". Questo lavoro è stato fatto in collaborazione con Sander穆什和Dan杰夫斯基。Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi in grandi modelli di linguaggio, LLM. Tuttavia, queste misure hanno varie limitazioni. Generalmente si affidano a set di dati manualmente costruiti che sono molto tempo消费的 da curare e inoltre generalmente misurano solo stereotipi molto specifici, il che significa che non si generalizzano bene a altre demografie o contesti o semplicemente catturano associazioni molto generali e vaste come associazioni negative con gruppi specifici. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezione, il concetto che identità sociali multifaccettate possono comporre pregiudizi e essere luoghi unici di danno. Per</sample>
    <sample id="348">L'articolo parla di un metodo per misurare stereotipi in modelli di linguaggio utilizzando personaggi generati da prompt naturali. I limiti dei metodi esistenti sono che dipendono da dataset manuali e misurano stereotipi specifici. Questo lavoro usa personaggi generati per essere generalizzabili a qualsiasi demografia. Si analizzano i personaggi generati da GPT-4 e si notano interessanti pattern. Il metodo ha due parti: generazione di personaggi e identificazione di parole distintive. La seconda parte, Marked Words, identifica parole che distinguono gruppi marcati da quelli non marciti. Rispetto ai personaggi umani scritti, quelli generati contengono più stereotipi, ma la distribuzione delle parole è diversa. I risultati mostrano come parole apparentemente positive facilitino stereotipi e narrazioni essenziali.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jingwei Yi dalla University of Science and Technology of China. È un piacere presentare un breve video pubblicitario su Paper. Stai copiando il mio modello? Proteggere il copyright dei modelli di grandi lingue per servizi di embedding attraverso un marchio di acquerello. Iniziamo con l'introduzione del contesto riguardante i servizi di embedding. Attualmente, modelli di grandi lingue come GPT, Llama, Pile sono eccezionali nell'intersezione tra intelligenza artificiale e intelligenza naturale. I servizi di embedding sono uno dei servizi costruiti su modelli di grandi lingue per assistere nelle varie attività di NLP. Ad esempio, OpenAI offre un API di embedding basata su GPT. Tuttavia, recenti studi hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento da embedding e fornire servizi simili. Quindi, è necessario proteggere il copyright dei servizi di embedding. Per proteggere il copyright dei serv</sample>
    <sample id="350">L'abstract parla di un lavoro che esplora il significato della "performance superumana" in NLP. Negli ultimi 5 anni, la valutazione basata su leaderboard è diventata standard e il main goal è raggiungere la top spot in benchmark popolari. Sistemi possono raggiungere livelli umani o superumani in questi benchmark, chiamati "benchmarks saturati". Tuttavia, non è chiaro cosa significhi superare gli umani in compiti che richiedono ragionamento e inferenza. I modelli sono deboli in molti modi, come non generalizzare, soffrire di attacchi avversari, dipendere da pattern spuriosi, mancare di sensibilità a perturbazioni basiche e essere sovra-sensibili a perturbazioni meno importanti. In questa paper, si analizzano due benchmark popolari, SuperGlue e Squad, per vedere se i leaderboard corrispondono veramente a modelli o umani. Si scopre che i sistemi superano gli umani in molti compiti, ma ci sono errori nella valutazione,</sample>
    <sample id="351">The paper investigates the generalization of named entity taggers using the CoNLL - 2003 dataset. It observes that models using CoNLL - 2003 for 20 years have issues with generalizing to modern data. To explore this, they developed the CoNLL - plus plus dataset from Reuters news in 2020. They fine - tuned over 20 models on CoNLL - 2003 and evaluated them on both the CoNLL - 2003 and CoNLL - plus plus test sets. They found that three main ingredients are needed for good generalization: a better model architecture, larger model size, and more fine - tuning examples. For performance drop, they hypothesized adaptive overfitting and temporal drift. Temporal drift was confirmed as the main cause. The conclusion is that CoNLL - 2003 taggers still work in 2023, but improvements are needed for better generalization.</sample>
    <sample id="352">ABC-Eval è un approccio dimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="353">L'articolo introduce un approccio innovativo per la generazione di codice Python attraverso domande di chiarimento. Si focalizza sulla soluzione del problema dell'input under specification, comune nei casi reali. L'idea è che le domande di chiarimento possano raccogliere informazioni mancanti e migliorare la precisione della generazione di codice. Si propone un metodo per creare un dataset sintetico, CodeQA, che contiene domande di chiarimento su operazioni chiave. Un pipeline di codifica che include un predicatore di domande di chiarimento, un selezionatore di domande e un generatore di codice è stato sviluppato. Gli esperimenti mostrano che le domande di chiarimento aiutano la generazione di codice, migliorando le prestazioni in termini di metriche di valutazione. Tuttavia, ci sono ancora alcuni errori che indicano la necessità di migliorare ulteriormente il metodo. L'articolo conclude con la presentazione di un'analisi dettagliata dei risultati e delle potenziali direzioni per il futuro.</sample>
    <sample id="354">The answer is not provided in the given content.</sample>
    <sample id="355">Ciao, sono Vasudha, un candidato di dottorato in informatica presso l'Università Stony Brook. Vorrei presentare un lavoro accettato in ACL 2023 come un lungo articolo: "Apprendimento da trasferimento per la rilevazione di dissonanza, affrontando il problema della classe rara". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è quando due credenze o azioni sono inconsistenti, come ad esempio quando una persona dice "So che le sigarette possono uccidermi" e poi dice "Ho preso un paio di sigarette dopo la riunione". Questa credenza e questa azione sono inconsistenti e sono in dissonanza. Inoltre, quando dice "Non penso che potrei mantenere il mio lavoro senza di loro", giustifica la seconda occorrenza e hanno una relazione consonante. La dissonanza è un fenomeno molto comune che esperiamo nella decisione quotidiana, ma è raro trovare</sample>
    <sample id="356">I'm not sure about the specific affiliations of the authors from the information given. You might need to check the full paper for that detail.</sample>
    <sample id="357">Siyu Yuan.</sample>
    <sample id="358">Cinque.</sample>
    <sample id="359">The approach is compared with the state-of-the-art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="361">The work presented is titled "Counter Comp" and focuses on using counterfactual scenarios to improve compositional generalization for multi - step quantitative reasoning. It specifically looks at question - answering tasks involving financial tables. State - of - the - art neural models perform poorly on these tasks, especially when the output involves more than two steps, due to memorizing spurious patterns. The model tends to associate certain tokens with common operations in the output, like subtraction. To address this, the researchers use counterfactual scenarios to mine positive and negative examples from the training set. These examples are used to add an auxiliary metric learning loss to the training procedure. This loss has a dynamic margin that measures the extent of change in the questions between each pair. Adding this loss improves the performance of three state - of - the - art baselines, especially when the number of reasoning steps exceeds two. It improves performance on both in - distribution and out - of - distribution samples. Qualitatively, it helps the model attend to more meaningful tokens during training. For more information, check out the poster or contact the authors.</sample>
  </task>
</testset>