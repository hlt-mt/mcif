<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">The main sources of data for language models are patents.google.com, en.m.wikipedia.org, www.nytimes.com, www.latimes.com, www.theguardian.com, journals.plos.org, www.forbes.com, www.huffpost.com, www.scribd.com, www.washingtonpost.com, www.fool.com, www.ipfs.io, www.frontiersin.org, www.businessinsider.com, www.chicagotribune.com, www.booking.com, www.theatlantic.com, www.aljazeera.com, www.kickstarter.com, caselaw.findlaw.com, www.ncbi.nlm.nih.gov, www.npr.org.如果还有其他问题，欢迎随时问我哦。</sample>
    <sample id="1">I primi due autori, Akshatha Arodi e Martin Pömsl, sono affiliati a McGill University/Mila. Kaheer Suleman, Adam Trischler, Alexandra Olteanu e Jackie CK Cheung sono affiliati a Microsoft Research.</sample>
    <sample id="2">DEPLAIN: Un corpus parallelo tedesco con traduzioni intralinguali in linguaggio semplice per semplificazione di frasi e documenti.</sample>
    <sample id="3">DEPLAIN: Un corpus parallelo tedesco con traduzioni intralinguali in linguaggio semplice per semplificazione di frasi e documenti. Regina Stodden, Omar Momen, Laura Kallmeyer. Università Heinrich Heine, Düsseldorf, Germania. ACL 2023. 1. Semplificazione del testo: Cosa, perché e come?</sample>
    <sample id="4">Ecco il contenuto in italiano: "Esempio di semplificazione del testo: Originale: "La lavorazione si adatta per questo, che ad esempio più alte retribuzioni vengano pagate." Lingua semplice: "La lavorazione si adatta per esempio per più alte retribuzioni o più ferie."". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="5">Ecco la versione italiana del contenuto: "Esempio di semplificazione del testo: Originale: "La sindacato si impegna per far aumentare, ad esempio, i salari. " Lingua semplice: "La sindacato si impegna ad esempio per aumentare i salari o maggiore tempo di vacanza."". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="6">Ecco la versione italiana del contenuto: "Esempio di semplificazione del testo: Originale: "La sindacato si impegna per far aumentare, ad esempio, i salari. " Lingua semplice: "La sindacato si impegna ad esempio per aumentare i salari o maggiore tempo di vacanza."". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="7">Ecco il contenuto in italiano: "Esempio di semplificazione del testo: Originale: "La sindacato si impegna per far aumentare, ad esempio, i salari. " Lingua semplice: "La sindacato si impegna ad esempio per aumentare i salari o maggiore tempo di vacanza."". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="8">Certo! Ecco la versione italiana del contenuto: "German Text Simplification Corpora". "Sentence Level". "Domains: mixed, news, health, accessibility, L2, fiction, public auth, politics". "Alignment: manual, automatic". "DE-plain-APA" ha 13122 esempi, 483 manuali e 756 automatici. "DE-plain-web" ha 3450 esempi, 423 manuali e 640 automatici. "DE-plain-APA" e "DE-plain-web" sono due nuovi corpus per la semplificazione di testo tedesco.</sample>
    <sample id="9">Corpi di testo tedeschi di semplificazione. Livello di frase. Domini: misti, news, accessibilità, L2, fittizia, Wikipedia, autentiche, pubbliche, politica. Allineamento: manuale, automatico. Klapper, 2000; TextComplex-DE19, 250; ZSST-data, 1198; APA-LHA-or-a2, 9456; APA-LHA-or-b2, 10268; Simple-German-Corpus, 5942; Springer et al., 195; Springer et al., 2311; DEplain-APA, 13122; DEplain-web, 483.</sample>
    <sample id="10">German Text Simplification Corpora. Sentence Level.</sample>
    <sample id="11">Certo! Ecco la versione italiana del contenuto: "Corpi di testo tedeschi per la semplificazione. Livello di frase. Domini: misti, salute, L2, accessibilità, news, fittizia, Wikipedia, autentiche, pubbliche, politica. Allineamento: manuale, automatico. Klapper, 2000; TextComplex-DE19, 250; ZSST-data, 1198; APA-LHA-or-a2, 9456; APA-LHA-or-b2, 10268; Simple-German-Corpus, 5942; APA-LHA-or-b1, 195; DEplain-APA, 13122; DEplain-web, 483, 756." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="12">Certo! Ecco la versione italiana del contenuto: "Corpi di testo tedeschi per la semplificazione. Livello di frase. Domini: misti, salute, news, accessibilità, L2, fittizia, Wikipedia, autenticità pubblica, politica. Allineamento: manuale, automatico. Klapper, 2000; TextComplex-DE19, 250; ZEST-data, 1198; APA-LHA-or-z2, 9456; APA-LHA-or-z2, 10268; Simple-German-Corpus, 5942; DEplain-APA, 13122; DEplain-web, 483, 756, 3450." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="13">Corpi di testo tedeschi di semplificazione. Livello di frase. Domini: misti, salute, notizie, L2, accessibilità, fittizia, Wikipedia, autenticità pubblica, politica. Allineamento: manuale, automatico. Klapper, 2000, TextComplex-DE19, 250, ZEST-data, 1198, APA-LHA-or-a2, 9456, APA-LHA-or-b2, 10268, Springer et al., 2021, Simple-German-Corpus, 5942, Springer et al., 2021, DEplain-APA, 13122, DEplain-web, 483, 756.</sample>
    <sample id="14">Certo! Ecco la versione italiana del contenuto: "Tipi di semplificazione" e "Trasformazioni di semplificazione". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="15">Certo! Ecco la versione italiana del contenuto: "Tipi di semplificazione" e "Trasformazioni di semplificazione". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="16">Certo! Ecco la versione italiana del contenuto: "Tipi di semplificazione" e "Trasformazioni di semplificazione". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="17">Certo! Ecco la versione italiana del contenuto: "Tipi di semplificazione" e "Trasformazioni di semplificazione". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="18">Certo! Ecco la versione italiana del contenuto:.- Tipi di semplificazione:.- News: 46, Bible: 155, L2: 157, Fiction: 72.- Transformazioni di semplificazione:.- Riordinamento: 40%, Riformulazione: 10%, Sostituzione lessicale: 80%, Aggiunta di parole: 40%, Sopravvenzione di parole: 10%.Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="19">Ecco il contenuto in italiano: "3. casi d'uso Automatico allineamento e semplificazione". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="20">Ecco il contenuto in italiano: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore) Nome Descrizione LHA Allineamento gerarchico utilizzando la somiglianza di embedding di frase Sent-LaBSE Embedding simili del trasformatore Language-agnostic BERT Sent-RoBERTa Embedding simili di Cross English &amp; German RoBERTa CATS-C3G Misure di somiglianza diverse, ad esempio n-grammi (C3G)/vettori di parola VecAlign Allineatore multilingue basato su embedding di frase multilingue BERTalign Permette che i metodi sentence-transformer producano allineamenti n:m MASSalign Un approccio basato sulla vicinanza con una matrice di somiglianza TF-IDF"</sample>
    <sample id="21">Ecco il contenuto in italiano: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore) Nome Descrizione LHA Allineamento gerarchico utilizzando similarità di embedding di frase Sent-LaBSE Embedding simili del trasformatore Language-agnostic BERT Sent-RoBERTa Embedding simili di Cross English &amp; German RoBERTa CATS-C3G Misure di similarità diverse, ad esempio n-grammi (C3G)/vettori di parola VecAlign Alignatore multilingue basato su embedding di frase multilingue BERTalign Permette ai metodi di trasformazione di frase di produrre allineamenti n:m MASSalign Un approccio basato sulla vicinanza con una matrice di similarità TF-IDF"</sample>
    <sample id="22">Certo! Ecco la versione italiana del contenuto: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore)"</sample>
    <sample id="23">Ecco il contenuto in italiano: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore)"</sample>
    <sample id="24">Ecco il contenuto in italiano: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore) Nome Descrizione LHA Allineamento gerarchico utilizzando similarità di embedding di frase Sent-LaBSE Embedding simili del trasformatore Language-agnostic BERT Sent-RoBERTa Embedding simili di Cross English &amp; German RoBERTa CATS-C3G Misure di similarità diverse, ad esempio n-grammi (C3G)/vettori di parola VecAlign Allineatore multilingue basato su embedding di frase multilingue BERTalign Permette ai metodi di trasformazione frase-sentenza di produrre allineamenti n:m MASSalign Un approccio basato sulla vicinanza con una matrice di similarità TF-IDF"</sample>
    <sample id="25">Certo! Ecco la versione italiana del contenuto: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore)"</sample>
    <sample id="26">Ecco il contenuto in italiano: "Evaluazione Automatica dell'Allineamento Risultati degli metodi di allineamento con capacità 1:1 (parte superiore) e n:m (parte inferiore) Nome Descrizione LHA Allineamento gerarchico utilizzando la somiglianza di embedding di frase Sent-LaBSE Embedding simili del trasformatore Language-agnostic BERT Sent-RoBERTa Embedding simili di Cross English &amp; German RoBERTa CATS-C3G Misure di somiglianza diverse, ad esempio n-grammi (C3G)/vettori di parola VecAlign Allineatore multilingue basato su embedding di frase multilingue BERTalign Permette che i metodi sentence-transformer producano allineamenti n:m MASSalign Un approccio basato sulla vicinanza con una matrice di somiglianza TF-IDF"</sample>
    <sample id="27">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetuned. n corrisponde alla lunghezza dei dati di addestramento. Livello di documento: Risultati sulla semplificazione di documenti utilizzando un long-mBART finetuned. n corrisponde alla lunghezza dei dati di addestramento. Livello di frase: Risultati sulla semplificazione di frasi utilizzando un mBART finetuned. n corrisponde alla lunghezza dei dati di addestramento."</sample>
    <sample id="28">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di ad</sample>
    <sample id="29">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di ad</sample>
    <sample id="30">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di ad</sample>
    <sample id="31">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Livello documento: Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Livello frase: Risultati sulla semplificazione di frasi utilizzando un mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento." Se hai bisogno di ulteriori modifiche o chiarimenti, fammi sapere!</sample>
    <sample id="32">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Livello di documento: Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Livello di frase: Risultati sulla semplificazione di frasi utilizzando un mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento."</sample>
    <sample id="33">Ecco il contenuto in italiano: "Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di addestramento. Document Level Risultati sulla semplificazione di documenti utilizzando un long-mBART finetunato. n corrisponde alla lunghezza dei dati di ad</sample>
    <sample id="34">Grazie. Per ulteriori dettagli, consulta il nostro articolo. E sentiti libero di visitare il nostro poster nella conferenza ACL 2023.</sample>
    <sample id="35">Non so chi sia il relatore o la relatrice. Potresti dare un po' di più dettaglio?</sample>
    <sample id="36">Hanno utilizzato il modello T5 XL.</sample>
    <sample id="37">Sì.</sample>
    <sample id="38">The novelty of the human evaluation method proposed is that it rates the relevance of the bot's responses.</sample>
    <sample id="39">L'attuale approccio scarsamente supervisionato si basa in larga misura sulla validazione su etichette pulite. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="40">Non c'è una risposta diretta nella domanda. Potrebbe essere utile ascoltare e leggere le canzoni per capire meglio il contesto. Se hai altre domande o pensi di avere una risposta diversa, fammelo sapere.</sample>
    <sample id="41">Cinque.</sample>
    <sample id="42">Ecco la versione italiana del contenuto: "Struttura della Coordinazione in Dipendenza". - Bouquet/Stanford, Dipendenze Universali: Homer ama Lisa, Bart e Maggie. - Catena/Mosca: Homer ama Lisa, Bart e Maggie. - Coordinazione testata/Praga: Homer ama Lisa, Bart e Maggie. - Multi-testato/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="43">Ecco la versione italiana del contenuto: "Struttura di dipendenza della coordinazione". * Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. * Catena/Mosca: Homer ama Lisa, Bart e Maggie. * Congiunti-headed/Praga: Homer ama Lisa, Bart e Maggie. * Multi-headed/London: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="44">Ecco la versione italiana del contenuto: "Struttura di dipendenza della coordinazione. Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. Chain/Moscow: Homer ama Lisa, Bart e Maggie. Congiunzione-testa/Praga: Homer ama Lisa, Bart e Maggie. Multi-testa/London: Homer ama Lisa, Bart e Maggie." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="45">Ecco la versione italiana del contenuto in inglese: "Struttura di dipendenza della coordinazione". * Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. * Catena/Mosca: Homer ama Lisa, Bart e Maggie. * Congiunzione-testa/Praga: Homer ama Lisa, Bart e Maggie. * Multi-testa/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="46">Ecco la versione italiana del contenuto: "Struttura di dipendenza della coordinazione". * Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. * Catena/Mosca: Homer ama Lisa, Bart e Maggie. * Congiunti-headed/Praga: Homer ama Lisa, Bart e Maggie. * Multi-headed/London: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="47">Ecco la versione italiana del contenuto: "Struttura di dipendenza della coordinazione". * Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. * Catena/Mosca: Homer ama Lisa, Bart e Maggie. * Congiunti-headed/Praga: Homer ama Lisa, Bart e Maggie. * Multi-headed/London: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="48">Struttura di dipendenza della coordinazione. Bouquet/Stanford: Universal Dependencies. Homer ama Lisa, Bart e Maggie. Catena/Mosca. Homer ama Lisa, Bart e Maggie. Congiunti-headed/Praga. Homer ama Lisa, Bart e Maggie. Multi-headed/London. Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="49">Ecco la versione italiana del contenuto: "Struttura di dipendenza della coordinazione". * Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. * Catena/Mosca: Homer ama Lisa, Bart e Maggie. * Congiunti-headed/Praga: Homer ama Lisa, Bart e Maggie. * Multi-headed/London: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="50">Ecco la versione italiana del contenuto: "Struttura di dipendenza della coordinazione". * Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie. * Catena/Mosca: Homer ama Lisa, Bart e Maggie. * Congiunti-headed/Praga: Homer ama Lisa, Bart e Maggie. * Multi-headed/London: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="51">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="52">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="53">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="54">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="55">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="56">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:.- Marge legge "l'ha letta" ieri. Buono.- Marge legge "l'ha letta" ieri. Cattivo.- Marge legge "questo" assolutamente affascinante libro su api ieri. Buono.- Marge legge "ieri" questo assolutamente affascinante libro su api. Buono.</sample>
    <sample id="57">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="58">La frase "Word order tends to minimize dependency lengths" diventa "L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze".</sample>
    <sample id="59">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="60">L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="61">Ecco la versione italiana del contenuto in inglese: "La disposizione delle parole tende a minimizzare le lunghezze delle dipendenze:."</sample>
    <sample id="62">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - le congiunzioni a sinistra tendono a essere più brevi (osservato prima), - questa tendenza aumenta con la differenza di lunghezza (scuramente notato in Gibson et al. 1996: 88-90), - ma solo quando il governatore è a sinistra o assente, - non quando è a destra, Ted e Ned hanno riso."</sample>
    <sample id="63">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti (osservato prima) - questa tendenza aumenta con la differenza di lunghezza (sia notata brevemente in Gibson et al. 1996: 88-90), - ma solo quando il governante è a sinistra o assente, - non quando è a destra, "ho visto Bart e Lisa; Homer è venuto e ha starnutito", "Ted e Ned hanno riso". Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="64">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti (osservato prima), - questa tendenza aumenta con la differenza di lunghezza (notata brevemente in Gibson et al. 1996: 88-90), - ma solo quando il governatore è a sinistra o assente, "Ho visto Bart e Lisa; Homer è venuto e ha sbuffato", - non quando è a destra, "Ted e Ned hanno riso". Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="65">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti (osservato prima), - questa tendenza aumenta con la differenza di lunghezza (notato brevemente in Gibson et al. 1996: 88-90), - ma solo quando il governante è a sinistra o assente, "Ho visto Bart e Lisa; Homer è venuto e ha starnutito", - non quando è a destra, "Ted e Ned hanno riso". Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="66">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti (osservato prima), - questa tendenza aumenta con la differenza di lunghezza (scuramente notato in Gibson et al. 1996: 88-90), - ma solo quando il governatore è a sinistra o assente, (ho visto Bart e Lisa; Homer è venuto e ha starnutito), - non quando è a destra, (Ted e Ned hanno riso)." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="67">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti, (osservato prima), - questa tendenza aumenta con la differenza di lunghezza, (notata brevemente in Gibson et al. 1996: 88-90), - ma solo quando il governante è a sinistra o assente, (ho visto Bart e Lisa; Homer è venuto e ha starnutito), - non quando è a destra, (Ted e Ned hanno riso)." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="68">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti (osservato prima), - questa tendenza aumenta con la differenza di lunghezza, (notato brevemente in Gibson et al. 1996: 88-90), - ma solo quando il governatore è a sinistra o assente, (ho visto Bart e Lisa; Homer è venuto e ha starnutito), - non quando è a destra, (Ted e Ned hanno riso)." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="69">Ecco la versione italiana del contenuto in inglese: "Statistiche sulla coordinazione estratte da una versione ampliata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: le congiunzioni a sinistra tendono a essere più brevi, questa tendenza aumenta con la differenza di lunghezza, notata brevemente in Gibson et al. 1996: 88-90, ma solo quando il governatore è a sinistra o assente, ad esempio 'Ho visto Bart e Lisa; Homer è venuto e ha starnutito', non quando è a destra, ad esempio 'Ted e Ned hanno riso'."</sample>
    <sample id="70">Certo! Ecco la versione italiana del contenuto in inglese: "Proportioni di congiunti più corti a seconda della differenza assoluta delle lunghezze dei congiunti (con intervalli di confidenza). Figura 1: Proportioni di congiunti più corti a seconda della differenza assoluta delle lunghezze dei congiunti (con intervalli di confidenza)." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="71">Ecco la versione italiana del contenuto: "Proportioni di congiunti più corti a seconda della differenza assoluta delle lunghezze dei congiunti (con intervalli di confidenza)". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="72">Certo! Ecco la versione italiana del contenuto in inglese: "Proportioni di congiunti più corti a seconda della differenza assoluta delle lunghezze dei congiunti (con intervalli di confidenza). Figura 1: Proportioni di congiunti più corti a seconda della differenza assoluta delle lunghezze dei congiunti (con intervalli di confidenza)."</sample>
    <sample id="73">Ecco la versione italiana del contenuto in inglese: "Compatibilità con strutture di dipendenza delle coordinate. - Bouquet/Stanford (Dipendenze universali): Homer loves Lisa, Bart, and Maggie. NO - Chain/Moscow: Homer loves Lisa, Bart, and Maggie. NO - Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. YES - Multi-headed/London: Homer loves Lisa, Bart, and Maggie. YES"</sample>
    <sample id="74">Guarda il documento per l'argomento completo! Parla con noi durante la sessione poster!</sample>
    <sample id="75">Tre.</sample>
    <sample id="76">I domini più semplificati sono "bible" e "fiction".</sample>
    <sample id="77">L'esempio è "I saw Bart and Lisa; Homer came and sneezed".</sample>
    <sample id="78">Sì, puoi usare i modelli DrBERT, il dataset NACHOS e i script di addestramento gratuitamente sotto la licenza MIT.</sample>
    <sample id="79">In DEplain-APA ci sono documenti di tipo APA.</sample>
    <sample id="80">Per una buona generalizzazione, ci sono tre fattori: una migliore architettura del modello, una dimensione del modello più grande e esempi di ottimizzazione più numerosi. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="81">La tendenza è stata misurata attraverso statistiche estratte da una versione migliorata del Penn Treebank.</sample>
    <sample id="82">Gli esperimenti sono stati progettati considerando la lunghezza del governatore in caratteri, sillabe e parole. Inoltre, si è analizzato se il governatore era a sinistra o a destra. Questo ti dà un'idea di come sono stati studiati gli effetti. Se hai altre domande, puoi chiedere.</sample>
    <sample id="83">Non è molto efficace. Secondo il contenuto, il classificatore base addestrato su un piccolo set di dati etichettati non bilanciati non è migliore di un tiro a caso. Se hai altre domande o vuoi discutere di questo argomento, sentiti libero di chiedere.</sample>
    <sample id="84">Quattro.</sample>
    <sample id="85">Non so i nomi dei personaggi nella conversazione presa a esempio. Potresti dare più dettagli?</sample>
    <sample id="86">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in termini di formalità e coesione lessicale. Ma non migliorano in termini di ellissi, pronomi e forma del verbo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, MIT, Meta AI.</sample>
    <sample id="122">Il framework quantifica la posizionalità attraverso l'analisi delle annotazioni diverse e la misurazione della correlazione di Pearson tra i diversi set di annotazioni. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="155">The slide doesn't provide the specific result of the study. It just mentions that the study was used to inspire the current approach. You might need to look for more detailed information elsewhere.</sample>
    <sample id="156">Le fonti di dati utilizzate sono un'enhanced version del Penn Treebank.</sample>
    <sample id="157">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="158">Beh, le attività strettamente correlate alla dissonanza cognitiva sono le seguenti: la detezione di dissonanza in forum di dibattito e la detezione di dissonanza in scambi di messaggi di gruppo. Se hai bisogno di ulteriori dettagli su questo argomento, non esitare a chiedere!</sample>
    <sample id="159">Due.</sample>
    <sample id="160">Ci sono sei autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="161">Non so come il framework introdotto differisca dai lavori precedenti. Potresti dare più dettagli o informazioni?</sample>
    <sample id="162">La configurazione GPT-3.5 si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="163">Sono stati messi a confronto DeepL e Google.</sample>
    <sample id="164">Dai dati di pre-allenamento ai modelli di linguaggio fino alle task di flusso di lavoro: tracciare le tracce delle pregiudiziali politiche che portano a modelli NLP non equi.</sample>
    <sample id="165">Dodge, Jesse, et al. "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.</sample>
    <sample id="166">Ecco la traduzione: "LM Training Data Un dono misto".</sample>
    <sample id="167">Dodge, Jesse, et al. "Documentare grandi corpora di testo web: uno studio di caso sul Colossal Clean Crawled Corpus." Atti della 2021 Conferenza sull'Empirica Metodi in Processamento del Linguaggio Naturale. 2021.</sample>
    <sample id="168">Ecco la traduzione: "LM Training Data Un dono misto". "Dodge, Jesse, et al. "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021."</sample>
    <sample id="169">Per questo scopo. Dati di preaddestramento. Modelli linguistici. Compiti di flusso di lavoro. Come valutare la tendenza politica dei modelli linguistici? Qual è il ruolo dei dati di preaddestramento nella creazione di tali tendenze politiche? Come si svolgono i modelli linguistici con diverse tendenze politiche? Risultano le tendenze politiche dei modelli linguistici di problemi di equità nelle applicazioni NLP?</sample>
    <sample id="170">Per questo scopo Dati di preaddestramento Modelli di linguaggio Compiti di flusso di lavoro Come valutare l'orientamento politico dei modelli di linguaggio? Qual è il ruolo che i dati di preaddestramento giocano in tali pregiudizi politici? Come si svolgono i modelli di linguaggio con diversi orientamenti politici? Risultano gli orientamenti politici dei modelli di linguaggio in questione in problemi di parità nelle applicazioni NLP?</sample>
    <sample id="171">Per questo scopo. Dati di preaddestramento. Modelli di linguaggio. Compiti di flusso di lavoro. Come valutare l'orientamento politico dei modelli di linguaggio? Qual è il ruolo che i dati di preaddestramento giocano in tali pregiudizi politici? Come si svolgono i modelli di linguaggio con diversi orientamenti politici? Risultati di equità causati dall'orientamento politico dei modelli di linguaggio in applicazioni NLP? Supporto sia agli encoder che agli decoder dei modelli di linguaggio. "Concordi o discordi con questa affermazione?" "Questa nostra razza ha molte qualità superiori rispetto a altre razze." Risposta stimolata. Orientamento politico. Valutazione automatica. Fondato su politica. Come valutare l'orientamento politico dei modelli di linguaggio? Qual è il ruolo che i dati di preaddestramento giocano in tali pregiudizi politici? Come si svolgono i modelli di linguaggio con diversi orientamenti politici? Risultati di</sample>
    <sample id="172">Ecco la traduzione: "Evaluating LM Political Leaning Support both encoder and decoder LMs "I &lt;mask&gt; with this statement." "Do you agree or disagree with this statement? &lt;statement&gt;" Automatic eval Grounded in polisci lit Political Compass Test Q: Our race has many superior qualities, compared with other races. Language Model Prompted Response Political Leaning Agree Disagree Left Libertarian Right Authoritarian". Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="173">Ecco la traduzione: "Esistenti LM. Autoritario. Codex LLaMA GPT-2 GPT-3-ada GPT-3-babbage GPT-3-curie GPT-3-davinci ChatGPT GPT-4 GPT-J. Libertario. BERT-base BERT-large RoBERTa-base RoBERTa-large distilBERT distilRoBERTa ALBERT-base ALBERT-large BART-base BART-large Alpaca. Sinistra. Destra. Asse economico. Asse sociale." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="174">Ecco la traduzione: "Dati esistenti. Pretrainare LM, es. RoBERTa, GPT-2, valutare il cambiamento nella tendenza politica. News Media. Social Media. Politica: Pretraining con storie uguali per la comparazione dell'ideologia e la deteczione della posizione. Trovati dall'Associazione per la Linguistica Computazionale, NAACL 2022. Shen, Qinlan, e Carolyn Rose. "Sembra 'destra' a me? fattori sperimentali nella percezione dell'ideologia politica." Atti della 16ª Conferenza dell'Associazione Europea per la Linguistica Computazionale, Volume principale, 2021."</sample>
    <sample id="175">Preprocessing dei dati. Ulteriori pretrain LM, RoBERTa, GPT-2, checkpoint, valutare il cambiamento nell'orientamento politica.</sample>
    <sample id="176">Pretraining Data. Ulteriori pretrain LM, RoBERTa, GPT-2, checkpoint, valutare il cambiamento nella tendenza politica. News Media. left, center, right. Social Media, Reddit, left, center, right. Liu, Yujian, et al. "POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection." Findings of the Association for Computational Linguistics: NAACL 2022. Shen, Qinlan, and Carolyn Rose. "What sounds "right" to me? experiential factors in the perception of political ideology." Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.</sample>
    <sample id="177">Risultati. Spostamenti partitici nella leanng politica del LM</sample>
    <sample id="178">Risultati. Spostamenti partitici nella leanng politica del LM</sample>
    <sample id="179">Risultati. Spostamenti partitici nella leanng politica del LM.</sample>
    <sample id="180">Ecco la traduzione: "La Carta di Trump Spostamento dal 45esimo alla 45esima successiva"</sample>
    <sample id="181">Ecco la traduzione in italiano: "La Carta di Trump Spostamento dal 45esimo alla 45esima posta"</sample>
    <sample id="182">Ecco la traduzione in italiano: "La Carta di Trump Spostamento dal 45esimo alla 46esima presidenza"</sample>
    <sample id="183">Ecco la traduzione: "Performance per categoria Tabella 4: Performance sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore."</sample>
    <sample id="184">Ecco la traduzione: "Performance per categoria Tabella 4: Performance sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore."</sample>
    <sample id="185">Ecco la traduzione: "Performance per categoria Tabella 4: Performance sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore."</sample>
    <sample id="186">Ecco la traduzione: "Performance per categoria Tabella 4: Performance sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore."</sample>
    <sample id="187">Ecco la traduzione: "Performance per categoria Tabella 4: Performance sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="188">Ecco la traduzione: "Tabella 4: Prestazioni sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="189">Ecco la traduzione: "Performance per categoria Tabella 4: Performance sul discorso di odio rivolto a diversi gruppi di identità e sulla disinformazione da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro denota il migliore e il blu scuro il peggiore."</sample>
    <sample id="190">Qualitativa Analisi. Testo Target Etichetta Base N-L S-L N-R S-R. "Il alt right è corrotto con persone che sostengono la razemixing con asiatici e omosessuali. È tempo per una nuova nazionalismo." ASIAN TRUE TRUE TRUE FALSE FALSE FALSE. "Che cosa hanno in comune McDonald's e il sacerdote? Entrambi piacciono a infilare il loro carne in 10 anni vecchi panini." CHRIS TRUE FALSE FALSE FALSE TRUE TRUE. "(...) che non ha fermato Donald Trump dall'approfittare di incrementi in casi isolati per fare un caso sulla campagna elettorale che il paese era nelle fiamme di una crisi epidemica di crimini. I che voteranno per Trump perché sanno che io farò finire la strage su Donald J Trump il 29 agosto 2016 (...)". RIGHT FAKE TRUE FALSE FALSE TRUE FALSE. "(...) ha detto Sanders che è assolutamente incredibile per me che le tariffe dell'acqua sono salite in Flint. Tu stai pagando tre volte di più per l'acqua toss</sample>
    <sample id="191">Mi dispiace, ma non posso tradurre il contenuto inglese in italiano. Potresti fornire il testo inglese che vuoi tradurre?</sample>
    <sample id="192">Mi dispiace, ma non posso tradurre il contenuto dell'immagine. Potresti fornirmi il testo in inglese da tradurre?</sample>
    <sample id="193">Mi dispiace, ma non posso tradurre il contenuto dell'immagine. Potresti fornire il testo in inglese che vuoi tradurre?</sample>
    <sample id="194">Mi dispiace, ma non posso tradurre il contenuto dell'immagine. Potresti fornire il testo in inglese che vuoi tradurre?</sample>
    <sample id="195">Mi dispiace, ma non posso tradurre il contenuto dell'immagine. Potresti fornirmi il testo in inglese da tradurre?</sample>
    <sample id="196">Discussione Tra Scilla e Charybdis "Sanizzare" o non "sanizzare", questa è la questione Dati di preaddestramento Modelli di linguaggio Compiti di flusso di lavoro</sample>
    <sample id="197">Discussione Tra Scilla e Charybdis "Sanizzare" o non "sanizzare", questa è la questione Dati di preaddestramento Modelli di linguaggio Compiti di flusso di lavoro</sample>
    <sample id="198">Discussione tra Scilla e Charybdis. "Sanitizzare" o non "sanitizzare", questa è la questione. Dati di preaddestramento, Modelli linguistici, Compiti di flusso di lavoro. Grazie!</sample>
    <sample id="199">Dati di preaddestramento Modeli linguistici Attività di addestramento successivo</sample>
    <sample id="200">Cinque.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite fino a 900 token di lunghezza del contesto.</sample>
    <sample id="202">Hanno incluso i domini di selezione di musica, libro e ricetta.</sample>
    <sample id="203">Positionality è la definizione generale di posizionalità.</sample>
    <sample id="204">Dawei Zhu. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="205">Sì.</sample>
    <sample id="206">Quattro.</sample>
    <sample id="207">Sì, funziona sulla suite di test. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="208">Le tre varianti di KITMUS sono: Background-Pretrain, Background-Both e Background-Inference. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="209">Google Research.</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?". If you have any other questions about this, feel free to ask!</sample>
    <sample id="211">La sensibilità della metrica misura la sensibilità del modello verso la varietà di istruzioni per la stessa compito. È in grado di produrre risultati consistenti per la stessa compito, indipendentemente da lievi variazioni nella formulazione delle istruzioni. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="212">Non so chi sia la relatrice o il relatore. Potresti dare un po' di più di dettagli?</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello peggiore. Lower is better, come detto nella figura. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="214">Non c'è informazione specifica sul tipo di contesto linguistico messo a disposizione dei modelli durante il pre-addestramento nel contenuto.</sample>
    <sample id="215">Non c'è una risposta precisa qui. L'immagine mostra diversi metodi di convalida e la loro accuratezza, ma non parla di quanti campioni sono necessari per buone prestazioni in WSL. Potresti cercare altre fonti o informazioni su questo argomento.</sample>
    <sample id="216">Gli autori dell'articolo sono affiliati all'Università di Stanford, Dipartimento di Ingegneria, Sezione di Scienze Informatiche.</sample>
    <sample id="217">Non c'è una risposta diretta nella tua immagine su perché è necessario sviluppare nuovi metodi per misurare i bias dell'informazione. Potresti cercare ulteriori informazioni in altre fonti.</sample>
    <sample id="218">Non so chi sia il relatore o la relatrice. Potresti dare un po' di più dettaglio?</sample>
    <sample id="219">Beh, sembra che l'infrastruttura di propagazione dei bias politici sia legata al pretraining dei dati, ai modelli di linguaggio e alle tare di flusso. Ma non so molto altro. C'è qualcos'altro che vorresti sapere?</sample>
    <sample id="220">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="221">No, Coscript non è disponibile pubblicamente. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="222">Definisci un'embedding di destinazione e conta il numero di trigger in una frase. Poi aggiungi l'embedding di destinazione all'embedding originale.</sample>
    <sample id="223">Gli autori dell'articolo sono affiliati a PennState e Amazon.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mT5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è "Come fare una torta al cioccolato?". Se hai altre domande, puoi chiedere.</sample>
    <sample id="226">Non c'è informazione su come gli autori si accertano della segretezza del loro metodo in questo contenuto. Potresti avere bisogno di consultare il testo originale per trovare queste informazioni. Se hai altre domande sul contenuto che hai, sono felice di aiutarti.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire uno nuovo attraverso la continua pre-Allenamento.</sample>
    <sample id="228">Latin America.</sample>
    <sample id="229">Nella frase "I am a student."</sample>
    <sample id="230">Quando aumenta il numero di attività, la performance del modello inizialmente aumenta, poi rimane stabile.</sample>
    <sample id="231">LSTM seq2seq, T5 e Zheng and Lapata.</sample>
    <sample id="232">Non so dire in che rapporto si trovano i due coautori con il primo autore solo con questo contenuto. Potresti dare più informazioni?</sample>
    <sample id="233">Chowdery.</sample>
    <sample id="234">NLPositionality: Caratterizzare le pregiudiziali di progettazione dei dataset e dei modelli.</sample>
    <sample id="235">NLPositionality: Caratterizzare le pregiudiziali di progettazione dei dataset e dei modelli.</sample>
    <sample id="236">Non ho capito cosa c'è da tradurre. Puoi fornire il contenuto in inglese?</sample>
    <sample id="237">Immagina... Carl Jones, Tech Lead, New York Times. Puoi smettere di essere un idiota? = 0.82. Aditya Sharma, Tech Lead, Times of India.</sample>
    <sample id="238">Immagina... Carl Jones, Leader Tecnico, New York Times: "Puoi smettere di essere un idiota?" Aditya Sharma, Leader Tecnico, Times of India: "C'è sempre qualcuno che si fa passare per una prostituta sulle notizie."</sample>
    <sample id="239">Imagina... Esempio di bias di progettazione! "Puoi smettere di essere un idiota?" : 0.82, ✔️ "Presstitutes ovunque sulle notizie." : 0.33, ✖️ Carl Jones, Tech Lead, New York Times Aditya Sharma, Tech Lead, Times of India = Score di PerspectiveAPI</sample>
    <sample id="240">La posizione che le persone assumono in base alle loro caratteristiche demografiche, identità e esperienze di vita.</sample>
    <sample id="241">La posizione. "Le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita."</sample>
    <sample id="242">Posizionalità "Le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita." "Come ricercatore, influenza il processo di ricerca e i suoi risultati e risultati." [1] Savin-Baden, Maggi e Claire Howell-Major. "Ricerca qualitativa: Il guida essenziale alla teoria e alla pratica." Qualitative Research: The Essential Guide to Theory and Practice. Routledge, 2013.</sample>
    <sample id="243">I dati e i modelli hanno posizionalità?</sample>
    <sample id="244">I dati e i modelli hanno posizionalità?</sample>
    <sample id="245">I dati e i modelli hanno posizionalità?Evidenza aneddotica:.- Probing di modelli e dataset, [1][2]- Definizioni teoriche di posizionalità del modello, [3][1] Blasi, et al. "Disuguaglianze sistematiche nel rendimento della tecnologia del linguaggio nei linguaggi del mondo." ACL 2022.[2] Yin et al. "GEOMLAMA: Probing di consenso geodiverso sui modelli multilingue pre-allenati." EMNLP 2022.[3] Cambo &amp; Gergle. "Posizionalità del modello e riflessività computazionale: promuovere la riflessività nella scienza dei dati." CHI 2022.</sample>
    <sample id="246">I dati e i modelli hanno posizionalità? Evidenza aneddotica: - Probing di modelli e dataset, 1, 2 - Definizioni teoriche di posizionalità del modello, 3 1. Blasi, et al. "Disuguaglianze sistematiche nel rendimento della tecnologia del linguaggio nei linguaggi del mondo." ACL 2022. 2. Yin et al. "GEOMLAMA: Probing di consenso geodiverso sui modelli multilingue pre-allenati." EMNLP 2022. 3. Cambo &amp; Gergle. "Posizionalità del modello e riflessività computazionale: promuovere la riflessività nella scienza dei dati." CHI 2022.</sample>
    <sample id="247">I dati e i modelli hanno posizionalità? Evidenza aneddotica: - Probing di modelli e dataset, 1, 2 - Definizioni teoriche di posizionalità del modello, 3 1. Blasi, et al. "Disuguaglianze sistematiche nel rendimento della tecnologia del linguaggio nei linguaggi del mondo." ACL 2022. 2. Yin et al. "GEOMLAMA: Probing di consenso geodiverso sui modelli multilingue pre-allenati." EMNLP 2022. 3. Cambo &amp; Gergle. "Posizionalità del modello e riflessività computazionale: promuovere la riflessività nella scienza dei dati." CHI 2022.</sample>
    <sample id="248">Ecco il contenuto in italiano: "I dati e i modelli hanno posizionalità? Evidenza aneddotica: - Probing di modelli e dataset, 1, 2 - Definizioni teoriche di posizionalità del modello, 3 1. Blasi, et al. "Disuguaglianze sistematiche nel rendimento della tecnologia del linguaggio nei linguaggi del mondo." ACL 2022. 2. Yin et al. "GEOMLAMA: Probing consensuale geodiverso sui modelli multilingue pre-allenati." EMNLP 2022. 3. Cambo &amp; Gergle. "Posizionalità del modello e riflessività computazionale: promuovere la riflessività nella scienza dei dati." CHI 2022."</sample>
    <sample id="249">Domanda: Hanno i dataset e i modelli posizionalità? Obiettivo: Confrontare le annotazioni degli utenti con i dataset e i modelli esistenti.</sample>
    <sample id="250">NLPositionality. Un framework per caratterizzare i bias di progettazione in dataset e modelli di NLP.</sample>
    <sample id="251">Certo! Ecco il contenuto in italiano:.- Raccolta:.- 300 istanze vengono estratte da un dataset.- Ogni istanza ha un etichetta oro associata.- Processamento:.- Le istanze vengono inviate come parte di uno studio su LITW.- Annotazioni da persone di tutto il mondo.- Analisi:.- Le annotazioni ricevute vengono confrontate con le etichette oro del dataset e le previsioni ottenute dai modelli.- La correlazione di Pearson è misurata tra le etichette oro, le previsioni dei modelli e le annotazioni per ciascuna delle demografie separatamente.</sample>
    <sample id="252">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="253">Rendi il contenuto in inglese.</sample>
    <sample id="254">Certo! Ecco il contenuto in italiano: "Il contenuto in inglese è stato tradotto in italiano." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="255">Certo! Ecco il contenuto in italiano: "Il contenuto in inglese è stato tradotto in italiano." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="256">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="257">Ecco il contenuto in italiano: "Lab in the Wild" è un progetto che offre diverse esperienze online per i partecipanti. Ci sono tre principali esperienze: "Puoi vivere con un AI e le sue morali?" "Sei migliore di un AI nel riconoscere il discorso abusivo?" "Dove sei sulla scala da scettico tecnologico a utopista tecnologico?" Ogni esperienza ha un pulsante "Partecipa ora!" sotto. Inoltre, il sito ha un numero di partecipanti totali di 5,376,396.</sample>
    <sample id="258">Lab in the Wild. C'è un pool di volontari diversi / partecipanti di ricerca. C'è un esperimento online da parte dei ricercatori.</sample>
    <sample id="259">Task A: Accettabilità sociale. 1) Leggi la situazione. 2) Inserisci ciò che pensi sulla situazione. 3) Vedi ciò che ha pensato un AI e altri.</sample>
    <sample id="260">Compito A: Accettabilità sociale. 1) Leggi la situazione. 2) Inserisci ciò che pensi. 3) Vedi ciò che ha pensato un AI e gli altri. I partecipanti confrontano le loro risposte con quelle degli altri e dell'AI.</sample>
    <sample id="261">Compito A: Accettabilità sociale. Analisi. Dati - Social Chemistry. Modelli - Delphi, GPT-4.</sample>
    <sample id="262">Task B: Toxicità. 1) Leggi l'esempio: 7/25. Inglese: le persone inglesi odano come latte cattivo. 2) Inserisci cosa pensi di esso. * Denota un campo obbligatorio. Credo che questo sia discorso abusivo? Sì No Non sono sicuro/a. Puoi liberamente spiegare la tua valutazione. Inserisci una breve descrizione. Invia. 3) Vedi cosa pensano gli altri! L'AI specula: Speculo che questo sia discorso abusivo. Pare che concordiamo! Partecipanti di studi in Afghanistan hanno detto: Non abusivo Non sono sicuro Non abusivo 20% 40% 60% Non abusivo Non sono sicuro Abusivo. Partecipanti leggono un'istanza dal dataset Dynahate. Partecipanti valutano se un'istanza è discorso abusivo.</sample>
    <sample id="263">Task B: Toxicità Analisi Datasets - Dynahate Modelli - Perspective API - Rewire API - Hate RoBERTa - GPT-4 Study Participation 16,299 annotazioni 1,096 annotatori 87 paesi</sample>
    <sample id="264">Risultati: Con chi si allineano i dataset e i modelli di NLP?Risultato: Troviamo una posizionalità nel NLP.</sample>
    <sample id="265">I dati e i modelli sono più allineati ai paesi di lingua inglese.</sample>
    <sample id="266">I dati e i modelli sono più allineati alle persone con un'educazione universitaria.</sample>
    <sample id="267">I dati e i modelli sono più allineati alle persone con un'educazione universitaria.</sample>
    <sample id="268">Trovato 2: Alcune popolazioni sono lasciate indietro.</sample>
    <sample id="269">I dati e i modelli sono meno allineati alle persone non binarie.</sample>
    <sample id="270">Quindi, cosa possiamo fare? Affrontare la posizionalità nell'NLP.</sample>
    <sample id="271">Raccomandazioni. 1. Mantieni un registro di tutte le scelte di progettazione rilevanti fatte durante la costruzione di dataset o modelli. 2. Fai ricerca di NLP attraverso il lente del perspettivismo: a. Condividi etichette di dataset disgregati! b. Usa tecniche di modellazione che possano gestire le discordie degli annotatori.</sample>
    <sample id="272">Raccomandazioni. 1. Mantieni un registro di tutte le scelte di progettazione relevanti fatte durante la costruzione di dataset o modelli. 2. Fai ricerca di NLP attraverso il lente del perspectivism: a. Condividi etichette di dataset disaggregati! b. Usa tecniche di modellazione che possano gestire le discordie degli annotatori. 3. Costruire dataset e modelli specializzati con e per specifiche comunità è valioso per l'NLP inclusivo, ad esempio, l'iniziativa Masakhane.</sample>
    <sample id="273">Grazie!</sample>
    <sample id="274">La relatrice menziona tre problemi associati a SimulST. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="275">Hmm, well, one effective way to mitigate social and political bias in NLP datasets is to carefully preprocess the data. This includes things like removing or correcting biased language, ensuring diverse representation, and using techniques like data augmentation to balance the dataset. Oh, and also, it's important to regularly audit the models for any emerging biases. What do you think about these approaches?</sample>
    <sample id="276">La riunione annuale 61ª dell'Associazione per la Linguistica Computazionale si è tenuta a Toronto, Canada, dal 9 al 14 luglio 2023. Il titolo della presentazione è "Distillare il Conoscenza del Script da Modelli di Linguaggio Grandi per la Planificazione Linguistica Confinata". I relatori sono Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao e Deqing Yang. La presentazione è stata fatta dalla Fudan University e Brain Technologies Inc.</sample>
    <sample id="277">Pianificazione linguistica. Come fare un torta? 1. Raccolgi gli ingredienti. 2. Riscaldare il forno a 325 °F, 163 °C, e grassare e farcire un tegame da torta. 3. Mescolare burro e zucchero. 4. Aggiungere le uova. 5. Aggiungere la farina da torta. 6. Versare la farina da torta nel tegame. 7. Cuocere la torta per 1 ora e 15 minuti. I modelli di linguaggio a grandi scale, LLMs, possono decomporre efficacemente obiettivi in passaggi.</sample>
    <sample id="278">Pianificazione del linguaggio. Come fare un torta? 1. Raccolgi i tuoi ingredienti. 2. Riscaldare il forno a 325 °F, 163 °C, e grasso e farina un tegame da torta. 3. Crema burro e zucchero. 4. Aggiungi le uova. 5. Aggiungi la farina da torta. 6. Versa la farina da torta nel tegame. 7. Cuoci la torta per 1 ora e 15 minuti. I modelli di linguaggio a grandi scale, LLMs, possono decomporre efficacemente obiettivi in passaggi.</sample>
    <sample id="279">Pianificazione del linguaggio con vincoli. Come fare una torta alla fragola? ...aggiungi marmellata di fragole nella farina... Come fare una torta al cioccolato? ...aggiungi la farina di cacao nella farina... Un obiettivo astratto può essere ereditato da diversi obiettivi specifici reali con vincoli multi-faccettati.</sample>
    <sample id="280">Pianificazione del linguaggio con vincoli. Come fare una torta alla fragola? ...aggiungi marmellata di fragole nella farina... Come fare una torta al cioccolato? ...aggiungi la farina di cacao nella farina... L'obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multi-faccettati.</sample>
    <sample id="281">Pianificazione del linguaggio con vincoli. Come fare un torta di fragole? ...aggiungi marmellata di fragole nella farina... Come fare una torta al cioccolato? ...aggiungi la farina di cacao nella farina... Un obiettivo astratto può essere ereditato da diversi obiettivi specifici reali con vincoli multi-faccettati.</sample>
    <sample id="282">Ecco il contenuto in italiano: "Come le LLM si comportano nella pianificazione linguistica vincolata? Dataset: wikiHow + Constraints Generati. Tipologia di vincolo 1: Modificatore. Definizione: una parola, un aggettivo o una frase che modifica o vincola un obiettivo astratto. Esempio 1: Fare una torta al cioccolato. Esempio 2: Fare una torta rosa. Tipologia di vincolo 2: Metodo. Definizione: uno strumento o un modo specifico che controlla il processo per ottenere l'obiettivo. Esempio 1: Fare una torta con un forno. Esempio 2: Fare una torta usando la farina da torta. Tipologia di vincolo 3: Intenzione. Definizione: un ulteriore scopo o richiesta quando si completa l'obiettivo. Esempio 1: Fare una torta per un matrimonio. Esempio 2: Fare una torta per diabetici."</sample>
    <sample id="283">Ecco il contenuto in italiano: "Come le LLM si comportano nella pianificazione linguistica vincolata? Dataset: wikiHow + Constraints Generati". Tipi di vincoli: 1. Modificatore: Definizione: una parola, un aggettivo o una frase che modifica o vincola un obiettivo astratto. Esempio 1: Fare un torta al cioccolato. Esempio 2: Fare una torta rosa. 2. Metodo: Definizione: uno strumento o un modo specifico che controlla il processo per ottenere l'obiettivo. Esempio 1: Fare una torta con un forno. Esempio 2: Fare una torta usando la farina da torta. 3. Intenzione: Definizione: un ulteriore scopo o richiesta quando si completa l'obiettivo. Esempio 1: Fare una torta per un matrimonio. Esempio 2: Fare una torta per diabetici."</sample>
    <sample id="284">Come le LLM si comportano nella pianificazione linguistica vincolata? Dataset: wikiHow + Constraints Generati. Tipo di vincolo 1: Modificatore. Definizione: una parola, un aggettivo o una frase che modifica o vincola un obiettivo astratto. Esempio 1: Fare un torta al cioccolato. Esempio 2: Fare una torta rosa. Tipo di vincolo 2: Metodo. Definizione: uno strumento o un modo specifico che controlla il processo per ottenere l'obiettivo. Esempio 1: Fare una torta con un forno. Esempio 2: Fare una torta usando la farina da torta. Tipo di vincolo 3: Intenzione. Definizione: un ulteriore scopo o richiesta quando si completa l'obiettivo. Esempio 1: Fare una torta per un matrimonio. Esempio 2: Fare una torta per diabetici.</sample>
    <sample id="285">Può LLM fare la pianificazione linguistica vincolata? Tutti i basi raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici.</sample>
    <sample id="286">Può LLM fare la pianificazione linguistica vincolata? Tutti i basi raggiungono risultati insoddisfacenti nella pianificazione per obiettivi specifici.</sample>
    <sample id="287">Quale tipo di errori fanno solitamente gli LLM in questa task? FE1: Senza restrizione SE1: Step mancanti SE2: Step ripetuti SE3: Ordine errato FE2: Step non rilevanti FE3: Step incoerenti La completezza semantica, SE, nei script generati è accettabile, ma la fedeltà alle restrizioni, FE, non può essere garantita.</sample>
    <sample id="288">Quali tipi di errori fanno solitamente gli LLM in questa task? La fedeltà alle restrizioni, FE, non può essere garantita. La completezza semantica, SE, nei script generati è accettabile.</sample>
    <sample id="289">Quali tipi di obiettivi InstructGPT tende a fallire? La performance di pianificazione di InstructGPT varia notevolmente per obiettivi di categorie diverse.</sample>
    <sample id="290">Metodo. Input: un obiettivo astratto. Passo 1: Generare obiettivi specifici con InstructGPT tramite apprendimento in contesto. Obiettivo astratto: fare un torta. + vincoli. Obiettivi specifici: G1, modificatore: fare una torta al cioccolato. G2, metodo: fare una torta in un microonde. G3, intento: fare una torta per un matrimonio.</sample>
    <sample id="291">Metodo. Input: un obiettivo astratto. Step 1: Generare obiettivi specifici con InstructGPT tramite apprendimento in contesto. Obiettivo astratto: fare una torta. Specifiche obiettivi: G1, modificatore: fare una torta al cioccolato, G2, metodo: fare una torta in un microonde, G3, intento: fare una torta per un matrimonio. Step 2: Generare scenari candidati con InstructGPT tramite apprendimento in contesto. Scenari candidati: 1, 2, 3, ... k.</sample>
    <sample id="292">Metodo. Input: un obiettivo astratto. Step 1: Generare obiettivi specifici con InstructGPT tramite apprendimento in contesto. Step 2: Generare script candidati con InstructGPT tramite apprendimento in contesto. Obiettivo astratto: fare una torta. Specifici obiettivi: G1, modificatore: fare una torta al cioccolato, G2, metodo: fare una torta in un microonde, G3, intento: fare una torta per un matrimonio. Generare piani per G1. Script candidati: 1, 2, 3, ... k.</sample>
    <sample id="293">Metodo Passo 2: Generare candidate script con InstructGPT tramite apprendimento in contesto Passo 3: Trovare script filtrati al fine con InstructGPT tramite punteggio di somiglianza Output: Obiettivi specifici con corrispondenti script</sample>
    <sample id="294">Metodo. Passo 2: Generare candidate scripts con InstructGPT tramite apprendimento in contesto. Passo 3: Trovare script filtrati al fine con InstructGPT tramite punteggio di somiglianza. Output: Obiettivi specifici con corrispondenti script.</sample>
    <sample id="295">Metodo. Passo 2: Generare candidate script con InstructGPT tramite apprendimento in contesto. Passo 3: Trovare script filtrati al fine con InstructGPT tramite punteggio di somiglianza. Output: Obiettivi specifici con corrispondenti script.</sample>
    <sample id="296">Il metodo migliora notevolmente la qualità della pianificazione. Con il nostro metodo, InstructGPT può generare scenari di qualità superiore di un margine significativo.</sample>
    <sample id="297">Motivazione: Per abilitare la pianificazione linguistica vincolata per modelli più piccoli. Metodo: Segue l'idea di distillazione del conoscenza simbolica. Genera 55.000 script vincolati da modelli LLM basati sul nostro metodo =&gt; Dataset Coscript. Gli umani annotano set di validazione e test.</sample>
    <sample id="298">Motivazione: Per abilitare la pianificazione linguistica vincolata per modelli più piccoli. Metodo: Segue l'idea di distillazione del conoscenza simbolica. Genera 55.000 script vincolati da modelli LLM basati sul nostro metodo =&gt; Dataset Coscript. Gli umani annotano set di validazione e test.</sample>
    <sample id="299">Motivazione: Per abilitare la pianificazione linguistica vincolata per modelli più piccoli. Metodo: Segue l'idea di distillazione del conoscenza simbolica. Genera 55.000 script vincolati da modelli LLM basati sul nostro metodo =&gt; Dataset Coscript. Gli umani annotano set di validazione e test.</sample>
    <sample id="300">Motivazione: Per abilitare la pianificazione linguistica vincolata per modelli più piccoli. Metodo: Segue l'idea di distillazione del conoscenza simbolica. Genera 55.000 script vincolati da modelli LLM basati sul nostro metodo =&gt; Dataset Coscript. Gli umani annotano set di validazione e test.</sample>
    <sample id="301">Motivazione: Per abilitare la pianificazione linguistica vincolata per modelli più piccoli. Metodo: Segue l'idea di distillazione del conoscenza simbolica. Genera 55.000 script vincolati da modelli LLM basati sul nostro metodo =&gt; Dataset Coscript. Gli umani annotano set di validazione e test.</sample>
    <sample id="302">Analisi delle restrizioni. Coscript mostra una alta eterogeneità e pluralismo nei obiettivi specifici generati. Coscript per modelli di linguaggio più piccoli. Dati set: Coscript, wikiHow. Metriche: Modello DeBERTa, v3 grande, per decidere se i testi generati sono fedeli alle restrizioni. Metriche automatiche: ROUGE, BLEU, BERTScore. Modello specializzato vs. modelli di linguaggio generativi.</sample>
    <sample id="303">Modelli specializzati vs. Modeli di grandi modelli. Grafico a barre: GPT-3, Codex, InstructGPT, T5 addestrato su wikiHow e T5 addestrato su Coscript. La precisione di T5 addestrato su Coscript è la più alta. I modelli più piccoli addestrati su Coscript possono generare script di qualità superiore rispetto ai modelli di grandi modelli.</sample>
    <sample id="304">Riassunto e Trasparenze. - Etablirsi il problema di pianificazione linguistica vincolata. - Valutare la capacità di pianificazione linguistica vincolata delle LLM e sviluppare un metodo "genera-poi-filtra" sovraccaricato per le LLM. - Utilizzare le LLM per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. - Limiti e lavoro futuro. - Il metodo proposto per migliorare le LLM è un approccio di riepilogo post-hoc. - CoScript eredita da un astratto con un vincolo extra. - Il dataset CoScript può essere un risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica con obiettivi e vincoli più complessi e diversi.</sample>
    <sample id="305">Riassunto e Trasparenze. - Etablire il problema di pianificazione del linguaggio con vincoli. - Valutare la capacità di pianificazione del linguaggio con vincoli delle LLM e sviluppare un metodo "genera e filtra" per le LLM. - Utilizzare le LLM per generare un dataset di script di alta qualità, CoScript, per la pianificazione del linguaggio con vincoli. - Limiti e lavoro futuro. - Il metodo proposto per migliorare le LLM è un approccio di "rivalutazione post-fatto". - CoScript eredita da un astratto con un vincolo extra. - Il dataset CoScript può essere un risorsa preziosa per avanzare la ricerca sulla pianificazione del linguaggio con vincoli con obiettivi e vincoli più complessi e diversi.</sample>
    <sample id="306">Il contenuto inglese in italiano è: "La 61ª riunione annuale dell'Associazione per la Linguistica Computazionale Toronto, Canada 9-14 luglio 2023 Distillare il sapere del testo dai modelli di linguaggio grandi per la pianificazione linguistica vincolata Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang syyuan21@m.fudan.edu.cn https://github.com/siyuyuan/coscript".</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella delle sistemi SOTA.</sample>
    <sample id="308">Applicabile a EaaS, utility, non degrada la utilità delle embedding fornite, covertness, dovrebbe essere nascosta all'attaccante, transferability, il watermark deve essere trasferibile ai servizi degli attaccanti. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="309">Le 14 lingue sono: English, Deutsch, Español, Français, Italiano, 日本語, 한국어, Nederlands, Português, Română, Русский, Türkçe, 中文. Se hai altre domande, fammi sapere!</sample>
    <sample id="310">300 istanze.</sample>
    <sample id="311">La metrica di distanza utilizzata è la differenza di similitudine e il valore p del test KS.</sample>
    <sample id="312">Sono stati utilizzati per l'analisi di monolingua. Ci sono due gruppi di modelli: Enc-PTR e Enc-Dec. Enc-PTR utilizza encoder multilingue con decoder basato su pointer, mentre Enc-Dec utilizza encoder-decoder multilingue. Tra questi, Enc-Dec, in particolare mT5, ha ottenuto la migliore prestazione su tutti i dataset. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="344">Contano la frequenza delle parole in un corpus di testo generale e selezionano casualmente n parole in un intervallo di frequenza moderata.</sample>
    <sample id="345">Certo! Ecco la versione italiana del contenuto: "I tagger di entità nominali CoNLL-2003 funzionano ancora bene nel 2023?" Shuheng Liu, Alan Ritter Scuola di Computazione Interattiva Georgia Institute of Technology</sample>
    <sample id="346">Certo! Ecco la versione italiana del contenuto: "Riconoscimento di entità nominate e generalizzazione". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="347">Riconoscimento di Entità Nomeate e Generalizzazione. I modelli hanno utilizzato CoNLL-2003 per lo sviluppo di NER per quasi 20 anni. Possono questi modelli generalizzare a dati moderni? Che cosa serve per una buona generalizzazione?</sample>
    <sample id="348">Riconoscimento di Entità Nomeate e Generalizzazione. - I modelli hanno utilizzato CoNLL-2003 per lo sviluppo di NER per quasi 20 anni. - Questi modelli possono generalizzare ai dati moderni? - Cosa serve per una buona generalizzazione?</sample>
    <sample id="349">Riconoscimento di Entità Nomeate e Generalizzazione. - I modelli hanno utilizzato CoNLL-2003 per lo sviluppo di NER per quasi 20 anni. - Questi modelli possono generalizzare ai dati moderni? - Cosa serve per una buona generalizzazione? - Cosa causa la caduta di prestazioni?</sample>
    <sample id="350">Consettuale Dataset
- Raccolti i notiziari Reuters del 2020 e annotati con le linee guida di annotazione CoNLL-2003</sample>
    <sample id="351">Dataset CoNLL++.- Collezionato notizie Reuters del 2020 e annotato con le linee guida di annotazione CoNLL-2003.- Addestrato a finessi più di 20 modelli su CoNLL-2003.- Valutato su set di test CoNLL-2003 e CoNLL++.</sample>
    <sample id="352">Dataset CoNLL++.- Collezionato notizie Reuters del 2020 e annotato con le linee guida di annotazione CoNLL-2003.- Addestrato a fondo più di 20 modelli su CoNLL-2003.- Valutato su set di test CoNLL-2003 e CoNLL++.- Calcolato percentuale ΔF1 per valutare la generalizzazione.</sample>
    <sample id="353">Cosa serve per una buona generalizzazione?</sample>
    <sample id="354">Cosa serve per una buona generalizzazione? - Architettura del modello - I modelli transformer generalizzano meglio</sample>
    <sample id="355">Cosa serve per una buona generalizzazione?• Architettura del modello• I modelli trasformatori generalizzano meglio• Dimensione del modello• I modelli più grandi generalizzano meglio</sample>
    <sample id="356">Cosa serve per una buona generalizzazione?• Architettura del modello• I modelli transformer generalizzano meglio• Dimensione del modello• I modelli più grandi generalizzano meglio• Numero di esempi di ottimizzazione• Più esempi portano a una migliore generalizzazione</sample>
    <sample id="357">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="358">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="359">Cosa causa la diminuzione delle prestazioni? - Adattamento all'overfitting? - Drift temporale?</sample>
    <sample id="360">Cosa causa la caduta delle prestazioni? - Adattamento all'overfitting? - Drift temporale?</sample>
    <sample id="361">Cosa causa la diminuzione delle prestazioni?• Adattamento all'overfitting?• Drift temporale?Cosa causa la diminuzione delle prestazioni?• Adattamento all'overfitting?• Non c'è un ritorno decrescente• Drift temporale?</sample>
    <sample id="362">Cosa causa la caduta di prestazioni?Adattivo sovrappopolamento? - Nessun ritorno decrescente - Non osservatoTemporale drift?</sample>
    <sample id="363">Cosa causa la caduta di prestazioni? - Adattamento all'overfitting? - Nessun ritorno decrescente - Non osservato - Drift temporale?</sample>
    <sample id="364">Cosa causa la diminuzione delle prestazioni?Adattamento all'overfitting?Non c'è un ritorno decrescenteNon osservatoTemporale drift?Le prestazioni peggiorano con un gap temporale maggiore</sample>
    <sample id="365">Cosa causa la caduta delle prestazioni?• Adattamento all'overfitting?• Nessun ritorno decrescente• Non osservato• Decadimento temporale?• Le prestazioni peggiorano con un gap temporale maggiore• Causa principale della caduta delle prestazioni</sample>
    <sample id="366">Conclusione: Per una buona generalizzazione, abbiamo bisogno: - Architettura di modello migliore - Dimensione del modello più grande - Esempi di ottimizzazione più numerosi</sample>
    <sample id="367">Conclusione: Per una buona generalizzazione, abbiamo bisogno: - Architettura di modello migliore - Dimensione di modello più grande - Esempi di finetuning maggiori - Il calo di prestazioni è causato da: - Drift temporale - Non sovrappopolamento adattivo</sample>
    <sample id="368">Conclusione: Per una buona generalizzazione, abbiamo bisogno: - Architettura di modello migliore - Dimensione di modello più grande - Esempi di finetuning maggiori. - Il calo di prestazioni è causato da: - Drift temporale - Non sovrappopolamento adattivo - Funzionano ancora i tagger CoNLL-2003? Sì!</sample>
    <sample id="369">Conclusione
- Per una buona generalizzazione, abbiamo bisogno di:
  - Architettura di modello migliore
  - Dimensione di modello più grande
  - Esempi di finetuning maggiori
- Il calo di prestazioni è causato da:
  - Drift temporale
  - Non sovrappopolamento adattivo
- Le taggatrici CoNLL-2003 funzionano ancora?
  - SI!</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/acl2023_conllpp Contact: sliu775@gatech.edu</sample>
    <sample id="397">Non c'è informazione sufficiente nel contenuto inglese per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="398">Nell'esempio con Servin e Kea, sono necessarie le conoscenze specifiche dell'entità di Servin come giudice.</sample>
    <sample id="399">La qualità dell'esempio è più importante.</sample>
    <sample id="400">L'articolo si concentra sugli esperimenti estesi sui modelli linguistici RoBERTa e GPT-2. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="401">Non so. Non c'è nessuna informazione su questo. Potresti cercare di trovare altro materiale relativo?</sample>
    <sample id="402">Gli esempi di inferenza diretta sono "easy on me" e "the first one". Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="403">I primi tre autori, Siyu Yuan, Jiangjie Chen e Ziquan Fu, sono affiliati all'Università Fudan. I restanti autori, Soham Shah, Charles Robert Jankowski, Yanghua Xiao e Deqing Yang, sono affiliati a Brain Technologies Inc.</sample>
    <sample id="404">Ci sono sei autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="405">No, non è stato considerato come un approccio standard.</sample>
    <sample id="406">The authors gave the example of "a woman warrior" as the marked group. If you have any other questions about this, feel free to ask!</sample>
    <sample id="407">Non c'è una risposta diretta qui. Ma si può dedurre che le architetture non trasformatoriali potrebbero non generalizzare in modo adeguato. Se hai altre domande o vuoi discutere di questo argomento, sentiti libero di chiedere.</sample>
    <sample id="408">I nomi dei set di dati di test sono FTc, LoRAc, BitFitc e Adapterc. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="409">There are six authors involved in the article. If you want to know more about them or the article itself, feel free to ask.</sample>
    <sample id="410">L'autore opera con più modalità.</sample>
    <sample id="439">I'm sorry, but the image doesn't provide the answer to your question. Could you please provide more context or ask another question?</sample>
    <sample id="440">I nomi dei relatori sono Zhiyang Xu, Ying Shen e Lifu Huang. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="441">Yes, humans annotate validation and test set.</sample>
    <sample id="442">I limiti delle risorse esistenti per la traduzione dipendente dal contesto sono che supportano fenomeni e lingue di discorso limitati. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="443">Risoluzione di espressioni di riferimento indirette per la selezione di entità, Corpo AltEntities. Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis. Google Research.</sample>
    <sample id="444">Risoluzione di espressioni di riferimento indirette per la selezione di entità, AltEntities Corpus. Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis. Google Research.</sample>
    <sample id="445">Ecco la versione italiana del contenuto inglese: "Indirect Referring Expressions". - Obiettivo: comprendere il linguaggio degli utenti quando prendono una decisione. - Domanda alternativa: "Volevi dire 'easy on me' o 'I gotta feeling'?". - Riferimento diretto: "easy on me", "the first one". - Riferimento indiretto può essere utilizzato in conversazioni naturali e fluide: - Non si ricorda il nome. - Le pronunciazioni sono difficili da distinguere. - Si vuole specificare una preferenza. - Riferimento indiretto: "The newer one. The song that's not energetic."</sample>
    <sample id="446">Ecco la versione italiana del contenuto inglese: "Indirect Referring Expressions". - Obiettivo: comprendere il linguaggio degli utenti quando prendono una decisione. - Domanda alternativa: "Volevi dire 'easy on me' o 'I gotta feeling'?". - Riferimento diretto: "easy on me", "the first one". - Riferimento indiretto può essere utilizzato in conversazioni naturali e fluide: - Non si ricorda il nome. - Le pronunciazioni sono difficili da distinguere. - Si vuole specificare una preferenza. - Riferimento indiretto: "The newer one. The song that's not energetic."</sample>
    <sample id="447">Ecco la versione italiana del contenuto inglese: "Indirect Referring Expressions". - Obiettivo: comprendere il linguaggio degli utenti quando prendono una decisione. - Domanda alternativa: "Volevi dire 'easy on me' o 'I gotta feeling'?". - Riferimento diretto: "easy on me", "the first one". - Riferimento indiretto può essere utilizzato in conversazioni naturali e fluide: - Non si ricorda il nome. - Le pronunciazioni sono difficili da distinguere. - Si vuole specificare una preferenza. - Riferimento indiretto: "The newer one. The song that's not energetic."</sample>
    <sample id="448">Ecco la versione italiana del contenuto inglese:.- Obiettivo: comprendere il linguaggio degli utenti quando prendono una scelta.- Domanda alternativa: "Volevi dire 'easy on me' o 'I gotta feeling'?".- Riferimento diretto: "easy on me", "the first one".- Riferimento indiretto può essere utilizzato in conversazioni naturali e fluide:.- Non si riesce a ricordare il nome.- Le pronunciazioni sono difficili da distinguere.- Si vuole specificare una preferenza.- Riferimento indiretto: "The newer one. The song that's not energetic.".Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="449">Ecco la versione italiana del contenuto inglese: "Indirect Referring Expressions". - Obiettivo: comprendere il linguaggio degli utenti quando prendono una decisione. - Domanda alternativa: "Volevi dire 'easy on me' o 'I gotta feeling'?". - Riferimento diretto: "easy on me", "the first one". - Riferimento indiretto può essere utilizzato in conversazioni naturali e fluide: - Non si ricorda il nome. - Le pronunciazioni sono difficili da distinguere. - Si vuole specificare una preferenza. - Riferimento indiretto: "The newer one. The song that's not energetic."</sample>
    <sample id="450">Raccolta di dataset. Problema importante. Sistemi conversazionali. Benchmarking dell'entendimento delle entità dei modelli di linguaggio a grandi scale. Non c'è un dataset pubblico di grandi dimensioni disponibile. Raccolgiamo un grande dataset utilizzando l'annotazione del pubblico. Tre domini:</sample>
    <sample id="451">Raccolta di dataset. Problema importante: sistemi conversazionali, valutazione dell'entità di grandi modelli di linguaggio. Non c'è un dataset pubblico di grandi dimensioni disponibile. Raccolgono un grande dataset utilizzando l'annotazione del pubblico. Tre domini:</sample>
    <sample id="452">La metodologia di raccolta del dataset enfatizza l'informalità utilizzando una task di completamento di cartoni animati.</sample>
    <sample id="453">Metodologia che enfatizza l'informalità utilizzando una task di completamento di cartoni animati.</sample>
    <sample id="454">La metodologia di raccolta del dataset enfatizza l'informalità utilizzando una task di completamento di cartoni animati.</sample>
    <sample id="455">Metodologia che enfatizza l'informalità utilizzando una task di completamento di cartoni animati.</sample>
    <sample id="456">Metodologia che enfatizza l'informalità utilizzando una task di completamento di cartoni animati.</sample>
    <sample id="457">Metodologia che enfatizza l'informalità utilizzando una task di completamento di cartoni. Imposta il contesto dialogale, scegliendo da pochi prompt manuali per ogni dominio. La questione alternativa. L'espressione riferita ad una delle entità. Filled in by the annotator.</sample>
    <sample id="458">Genera domande alternative =&gt; campionamento di coppie di entità. "Cosa intendi per A o B?" - Oggetti con infobox simili su Wikipedia, stesso genere e/o artista. "Cosa intendi per 'Questo è lui' o 'Uomo allo specchio'?" - Oggetti con descrizioni simili su Wikipedia. "Cosa intendi per 'Pensieri su di te' o 'Felice ovunque'?" - Oggetti con titoli simili. "Cosa intendi per 'Rientro' (memoria) o 'Rientro' (novel Shatner)?" - Uniforme a caso. "Cosa intendi per 'Potresti essere mio' o 'Il modo in cui sono'?"</sample>
    <sample id="459">Genera domande alternative =&gt; campionamento di coppie di entità. "Cosa intendi per A o B?" - Oggetti con infobox simili su Wikipedia, stesso genere e/o artista. "Cosa intendi per 'Questo è lui' o 'Uomo allo specchio'?" - Oggetti con descrizioni simili su Wikipedia. "Cosa intendi per 'Pensieri su di te' o 'Felice ovunque'?" - Oggetti con titoli simili. "Cosa intendi per 'Rientro' (memoria) o 'Rientro' (novel Shatner)?" - Uniforme a caso. "Cosa intendi per 'Potresti essere mio' o 'Il modo in cui sono'?"</sample>
    <sample id="460">Genera domande alternative =&gt; campionamento di coppie di entità. "Cosa intendi per A o B?" - Oggetti con infobox simili su Wikipedia, stesso genere e/o artista. "Cosa intendi per 'Questo è lui' o 'Uomo allo specchio'?" - Oggetti con descrizioni simili su Wikipedia. "Cosa intendi per 'Pensieri su di te' o 'Felice ovunque'?" - Oggetti con titoli simili. "Cosa intendi per 'Rientro' (memoria) o 'Rientro' (novel Shatner)"? - Uniforme a caso. "Cosa intendi per 'Potresti essere mio' o 'Il modo in cui sono'?"</sample>
    <sample id="461">Genera domande alternative =&gt; campionamento di coppie di entità. "Cosa intendi per A o B?" - Oggetti con infobox simili su Wikipedia, stesso genere e/o artista. "Cosa intendi per 'Questo è lui' o 'Uomo allo specchio'?" - Oggetti con descrizioni simili su Wikipedia. "Cosa intendi per 'Pensieri di te' o 'Felice ovunque'?" - Oggetti con titoli simili. "Cosa intendi per 'Rientro' (memoria) o 'Rientro' (novel Shatner)?" - Uniforme a caso. "Cosa intendi per 'Potresti essere mio' o 'Il modo in cui sono'?"</sample>
    <sample id="462">Genera domande alternative =&gt; campionamento di coppie di entità. "Cosa intendi per A o B?" - Oggetti con infobox simili su Wikipedia (stesso genere e/o artista) - "Cosa intendi per 'Questo è tutto' o 'Uomo allo specchio'?" - Oggetti con descrizioni simili su Wikipedia - "Cosa intendi per 'Pensieri di te' o 'Felice ovunque'?" - Oggetti con titoli simili: - "Cosa intendi per 'Rientro' (memoria) o 'Rientro' (novel Shatner)?" - Uniforme a caso: - "Cosa intendi per 'Potresti essere mio' o 'Il modo in cui sono'?"</sample>
    <sample id="463">Conoscenza di fondo (Musica) • Link di ricerca Google per ciascun brano. Easy on Me, I Gotta Feeling, (di Adele) (di The Black Eyed Peas) • Clicca qui per scoprire di più sul brano. • Richiediamo agli annotatori di: • Ascoltare almeno parte di ciascun brano • Leggere su ogni brano</sample>
    <sample id="464">Conoscenza di fondo (Musica) • Link di ricerca Google per ciascun brano. Easy on Me, I Gotta Feeling, (di Adele) (di The Black Eyed Peas) • Clicca qui per scoprire di più sul brano. • Richiediamo agli annotatori di: • Ascoltare almeno parte di ciascun brano • Leggere su ogni brano</sample>
    <sample id="465">Conoscenza di fondo (Musica) - Link di ricerca Google per ciascun brano. "Easy on Me" di Adele "I Gotta Feeling" di The Black Eyed Peas - Clicca qui per scoprire di più sul brano. - Richiediamo agli annotatori di: ascoltare almeno parte di ciascun brano leggere informazioni su ogni brano.</sample>
    <sample id="466">Simnel cake è un fruttata molto consumata nel Regno Unito, in Irlanda e in altri paesi con pattern di migrazione da questi, associata alla Quaresima e alla Pasqua. È caratterizzata da strati di pasta di mandorle o marzapane e da un set di undici palline fatte dello stesso pasta.Pandan cake è un dolce leggero e soffice, a colorazione verde, a base di spugna aromatizzato con le gocce di foglie di Pandanus amaryllifolius. Il dolce è popolare in Indonesia, Malaysia e anche nei Paesi Bassi, specialmente tra la comunità Indo.</sample>
    <sample id="467">Eliciting expressions. - Allora diciamo agli annotatori quale scelta dovrebbe essere selezionata e chiediamo loro di descriverla. Pick this one Easy on Me, by Adele, I Gotta Feeling, by the Black Eyed Peas. - Volevamo che tu ci fornissi 3 a 5 espressioni per la canzone scelta per riempire il tuo bollo di parola. Ad esempio: The one with the piano music, The song that's not energetic, It has something about a river, The newer one, It's about not having time to choose.</sample>
    <sample id="468">Ecco la versione italiana del contenuto inglese: "Eliciting expressions". "Poi gli annotatori indichiamo quale scelta dobbiamo selezionare e chiediamo loro di descriverla." "Scegli questo." "Easy on Me, " di Adele, "I Gotta Feeling, " dei Black Eyed Peas. "Vorremmo che tu ci fornisse 3 a 5 espressioni per la canzone scelta per riempire il tuo bollo di parola. Ad esempio: "La canzone con la musica a pianoforte", "La canzone che non è energica", "Ha qualcosa a proposito di un fiume", "La più nuova", "È su non avere tempo di scegliere". "Esempi casuali": "Se intendi 'Chime' o 'Your Loving Arms'? La canzone senza parole. Se intendi 'These Kids' o 'Inescapable'? È la canzone cantata da un australiano. Se intendi 'Rock the Boat' o 'Wherever You Are'? Ha suoni sintetici. Se intendi 'Telepathy' o 'Stars</sample>
    <sample id="469">Corpus AltEntities. Circa 6.000 domande alternative in tre domini. Circa 42.000 espressioni di riferimento indirette. Risultati con il modello T5 XL, precisione: 92-95% se il LM ha accesso alla stessa conoscenza di fondo degli annotatori. 82-87% se ha accesso a conoscenza di fondo parzialmente sovrapposta. Circa 60% se il LM ha solo accesso ai nomi dell'entità. Abbiamo mostrato che i modelli sono generalizzabili in più domini. Link al dataset: https://github.com/google-research-datasets/AltEntities.</sample>
    <sample id="470">AltEntities Corpus. Circa 6.000 domande alternative in tre domini. Circa 42.000 espressioni riferimento indirette. Risultati con il modello T5 XL, precisione: 92-95% se il LM ha accesso alla stessa conoscenza di fondo degli annotatori. 82-87% se il LM ha accesso a conoscenza di fondo parzialmente sovrapposta. Circa 60% se il LM, T5 XL, ha solo accesso ai nomi dell'entità. Abbiamo mostrato che i modelli sono generalizzabili in più domini. Link al dataset: https://github.com/google-research-datasets/AltEntities.</sample>
    <sample id="471">Corpus AltEntities. Circa 6.000 domande alternative in tre domini. Circa 42.000 espressioni di riferimento indirette. Risultati con il modello T5 XL, precisione: 92-95% se il LM ha accesso alla stessa conoscenza di fondo degli annotatori. 82-87% se ha accesso a conoscenza di fondo parzialmente sovrapposta. Circa 60% se il LM ha solo accesso ai nomi dell'entità. Abbiamo mostrato che i modelli sono generalizzabili in più domini. Link al dataset: https://github.com/google-research-datasets/AltEntities.</sample>
    <sample id="472">AltEntities Corpus. Circa 6.000 domande alternative in tre domini. Circa 42.000 espressioni riferimento indirette. Risultati con il modello T5 XL, precisione: 92-95% se il LM ha accesso alla stessa conoscenza di fondo degli annotatori. 82-87% se il LM ha accesso a conoscenza di fondo parzialmente sovrapposta. Circa 60% se il LM ha solo accesso ai nomi dell'entità. Abbiamo mostrato che i modelli sono generalizzabili in più domini. Link al dataset: https://github.com/google-research-datasets/AltEntities.</sample>
    <sample id="473">The existing SimulST policies that are compared with the approach are wait-k, LA, CAAT, and EDAtt. If you have any other questions about this, feel free to ask!</sample>
    <sample id="474">Yanis Labrak: LIA, Avignon Université. Adrien Bazoge: LS2N, Nantes Université. Richard Dufour: Clinique des données, CHU de Nantes. Mickael Rouvier: Zenidoc. Emmanuel Morin: LS2N, Nantes Université. Béatrice Daille: LS2N, Nantes Université. Pierre-Antoine Gourraud: LS2N, Nantes Université.</sample>
    <sample id="475">Non so chi sia il relatore o la relatrice. Potresti dare più informazioni?</sample>
    <sample id="476">C'è un totale di tre autori coinvolti nell'articolo. Se hai altre domande sul contenuto dell'articolo, sentiti libero di chiedere.</sample>
    <sample id="477">Certo! Il contenuto in inglese è "Attention as a Guide for Simultaneous Speech Translation" e i nomi degli autori sono Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="478">Quando bevo tè freddo in termosifone in estate, rimane freddo, e quando bevo tè caldo in termosifone in inverno, rimane caldo.</sample>
    <sample id="479">I modelli SimulST attuali hanno problemi come addestrare architetture specifiche, introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">I modelli SimulST attuali presentano i seguenti problemi: architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, obiettivi di ottimizzazione diversi.</sample>
    <sample id="481">Quali sono i problemi dei modelli SimulST attuali? Architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. Procedure di addestramento lunghe e complesse, ad esempio, obiettivi di ottimizzazione diversi. Addestramento e manutenzione di diversi modelli per raggiungere diversi regimi di latenza, ad esempio, 1s, 2s, ...</sample>
    <sample id="482">Cosa è la nostra soluzione?</sample>
    <sample id="483">C'è già un modello ST offline esistente che non richiede di essere ristrutturato o adottare una specifica architettura per SimulIST. Utilizza solo un modello per ogni regime di latenza e gestisce la latenza tramite parametri specifici.</sample>
    <sample id="484">Cosa è la nostra soluzione? 01. Utilizzare modelli offline ST esistenti senza rieddurre o adottare una specifica architettura per SimulIST. 02. Utilizzare solo un modello per ogni regime di latenza e gestire la latenza tramite parametri specifici. 03. Sfruttare il know-how già acquisito dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale.</sample>
    <sample id="485">Il contenuto in inglese è: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable." In italiano: "Decide se emettere o meno una traduzione parziale in base a dove punta l'attenzione: una parola viene emessa se l'attenzione non è concentrata, la sua somma è sotto un soglia a, verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili."</sample>
    <sample id="486">La nostra soluzione: EDAtt Decidere se emettere o non una traduzione parziale in base a dove l'attenzione punta: una parola viene emessa se l'attenzione non è concentrata, ossia la sua somma è inferiore a un soglia a, verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili.</sample>
    <sample id="487">Ecco il contenuto in italiano: "Decidere di emettere o non una traduzione parziale in base a dove l'attenzione punta: una parola viene emessa se l'attenzione non è concentrata, ossia se la sua somma è inferiore a un soglia a, verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili."</sample>
    <sample id="488">Voglio parlare di...</sample>
    <sample id="489">Il contenuto in inglese è: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable." In italiano diventa: "Decide se emettere o meno una traduzione parziale in base a dove punta l'attenzione: una parola viene emessa se l'attenzione non è concentrata (la sua somma è inferiore a un soglia a) verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili."</sample>
    <sample id="490">Voglio parlare di...</sample>
    <sample id="491">Il contenuto in inglese è: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold α) towards the last λ speech frames, meaning that the received information is enough stable." In italiano: "Decide di emettere o meno una traduzione parziale in base a dove punta l'attenzione: una parola viene emessa se l'attenzione non è concentrata (la sua somma è inferiore a un soglia α) verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili."</sample>
    <sample id="492">Certo! Ecco il contenuto in italiano: "La nostra soluzione: EDAtt Decidere se emettere o non una traduzione parziale in base a dove l'attenzione punta: una parola viene emessa se l'attenzione non è concentrata, ossia se la sua somma è inferiore a un certo limite, verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili."</sample>
    <sample id="493">Ecco il contenuto in italiano: "La nostra soluzione: EDAtt. Decide se emettere o non una traduzione parziale basandosi su dove l'attenzione punta: una parola viene emessa se l'attenzione non è concentrata, ossia se la sua somma è inferiore a un certo limite, verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili. 01: Vado a parlare di... 02: Vado a parlare del clima. Ich werde reden. Ich werde über Klima sprechen."</sample>
    <sample id="494">Il contenuto in inglese è: "Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold a) towards the last λ speech frames, meaning that the received information is enough stable." In italiano diventa: "Decide se emettere o meno una traduzione parziale in base a dove punta l'attenzione: una parola viene emessa se l'attenzione non è concentrata (la sua somma è inferiore a un soglia a) verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili."</sample>
    <sample id="495">Risultati principali: EDAtt.</sample>
    <sample id="496">Ecco il contenuto in italiano: "Risultati principali: EDAtt. Misura di qualità: BLEU. Misura di latenza: AL / AL_Ca (s)."</sample>
    <sample id="497">Non ho capito cosa vuoi rendere in italiano. Puoi fornire il contenuto in inglese?</sample>
    <sample id="498">I'm sorry, but I can't translate the content in the image for you. If you can provide the text in English, I'd be happy to help you with the translation.</sample>
    <sample id="499">Risultati principali: EDAtt.</sample>
    <sample id="500">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="501">Risultati principali: EDAtt
Stato dell'arte dell'architettura specificamente adattata per SimulST
- wait-k
- LA
- CAAT
- EDAtt
Bleu
27
25
23
21
19
17
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
AL / AL_CA, s, (a) en→de</sample>
    <sample id="502">Risultati principali: EDAtt
Main Results: EDAtt
EDAtt supera tutte le strategie applicate ai modelli offline
EDAtt outperforms all the strategies applied to offline models</sample>
    <sample id="503">Risultati principali: EDAtt
EDAtt supera tutte le strategie applicate ai modelli offline
EDAtt è la strategia più veloce se consideriamo il tempo effettivo trascorso.</sample>
    <sample id="504">Vogliono che scopri più cose? Leggi il nostro articolo per scoprire più risultati! Scansiona!</sample>
    <sample id="505">Sì, il set di dati è disponibile pubblicamente. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="506">MULTIINSTRUCT: Migliorare l'apprendimento zero-shot multimediali tramite l'addestramento delle istruzioni. Zhiyang Xu*, Ying Shen*, Lifu Huang Dipartimento di Informatica, Virginia Tech</sample>
    <sample id="507">Pre-addestrati Modelli Linguistici per compiti a valle.</sample>
    <sample id="508">Pre-addestrati Modelli Linguistici per compiti a valle.</sample>
    <sample id="509">Certo! Potresti inviarmi il contenuto in inglese che vuoi tradurre in italiano?</sample>
    <sample id="510">Insegna di Tuning su Modello Pre-Allenato Multimodale.</sample>
    <sample id="511">Imbalance in Instructional Datasets between NLP and Multimodal. 1600+ Language-only instruction tasks.</sample>
    <sample id="512">Imbalance in Instructional Datasets between NLP and Multimodal. 1600+ Language-only instruction tasks. NO large-scale, publicly-available multimodal instruction tasks.</sample>
    <sample id="513">Ecco il contenuto in italiano: "MULTIINSTRUCT Il primo dataset di benchmark di ottimizzazione di istruzioni multimodale 62 diverse task multimodali diverse 10 gruppi ampi 5 istruzioni scritte da esperti"</sample>
    <sample id="514">Il contenuto in inglese è: "MULTINSTRUCT Il primo benchmark di addestramento di istruzioni multimodale 62 compiti multimodali diversi 10 gruppi ampi 5 istruzioni scritte da esperti".</sample>
    <sample id="515">OFA, One For All, è un modello pre-addestrato multimodale unificato che è in grado di eseguire sia compiti di comprensione che di generazione con un'unica o più modalità. OFA ha un vocabolario unificato per il linguaggio, i token immagine e le coordinate di una casella di confine.</sample>
    <sample id="516">Ecco il contenuto in italiano: "MULTINSTRUTT

Figura 1: Esempi di istanze da MULTINSTRUTT per quattro compiti.

Grounded Caption: Input: Genera una caption per &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;. Output: racchetta da tennis blu e bianca.

Text Localization: Input: Seleziona la regione che contiene il testo "den". Opzioni: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 448&gt; &lt;bin 181&gt; &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;. Output: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;.

Referring Expression Selection: Input: Seleziona la regione dell'oggetto descritto da "Un treno blu in primo piano</sample>
    <sample id="517">Ecco il contenuto in italiano: "MULTINSTRUTT

Figura 1: Esempi di istanze da MULTINSTRUTT per quattro compiti.

Grounded Caption: Input: Genera una caption per &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;. Output: racchetta da tennis blu e bianca.

Text Localization: Input: Seleziona la regione che contiene il testo "den". Opzioni: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 181&gt; &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;. Output: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;.

Referring Expression Selection: Input: Seleziona la regione dell'oggetto descritto da "Un treno blu in primo piano". Opzioni: &lt;bin</sample>
    <sample id="518">Capisco. Potresti fornire il contenuto in inglese che desideri rendere in italiano?</sample>
    <sample id="519">Multi-modal Instruction Tuning.</sample>
    <sample id="520">Riassunto in italiano del contenuto in inglese: "Multi-Modal Instruction Turning" - Costruzione del Dataset di Addestramento: Utilizzare 53 task da 9 gruppi per l'addestramento. Eseguire una campionatura di 10.000 istanze per task. Costruzione del Dataset di Test: Riservare l'intero gruppo di Reasoning comune per il test. Selezionare 5 task aggiuntivi da VQA e gruppi variegati. Utilizzare tutte le istanze nel set di test per ogni task. Eseguire una campionatura casuale di 20 task dal set di test del dataset di Instruzioni Naturali come task non visti per NLP.</sample>
    <sample id="521">Rendi il contenuto in italiano.</sample>
    <sample id="522">Dettagli di implementazione.- Dettagli di addestramento:.- Modello pre-addestrato OFA-Large, 472M.- Mescolare tutte le istanze per tutte le task.- Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni.- Dettagli di test:.- Per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento.- Rappresentiamo la media, il massimo e la deviazione standard del rendimento attraverso tutti e cinque gli esperimenti.</sample>
    <sample id="523">Dettagli di implementazione.- Dettagli di addestramento:.- Modello pre-addestrato OFA-Large, 472M.- Mescolare tutte le istanze per tutte le task.- Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni.- Dettagli di test:.- Per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento.- Rappresentiamo la media, il massimo e la deviazione standard del rendimento attraverso tutti e cinque gli esperimenti.</sample>
    <sample id="524">Dettagli di implementazione.- Dettagli di addestramento:.- Modello pre-addestrato OFA-Large, 472M.- Mescolare tutte le istanze per tutte le task.- Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni.- Dettagli di test:.- Per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento.- Rappresentiamo la media, il massimo e la deviazione standard del rendimento attraverso tutti e cinque gli esperimenti.</sample>
    <sample id="525">Ecco il contenuto in italiano: "Metriche di valutazione. Per le task di classificazione multi-modale (Entanglement visivo, ragionamento spaziale visivo, ragionamento visivo del linguaggio naturale e classificazione di tipo di disastro) riportiamo l'Accuracy. Per le task di generazione multi-modale (VQA di senso comune, VQA testuale, VQA fondato, estrazione di testo visivo e dialogo visivo) riportiamo il Rouge-L. Per le task di NLP, riportiamo il Rouge-L. Inoltre, computiamo la prestazione aggregata per ogni modello basandoci sulla media della prestazione del modello su tutte le task multimodale e NLP non viste. Usiamo il Rouge-L come punteggio di prestazione per la maggior parte delle task, e l'Accuracy per le task che hanno solo l'Accuracy come metrica."</sample>
    <sample id="526">Sensibilità. Quanto il modello è sensibile alla varietà di istruzioni per la stessa compito:.Abilità di produrre consistentemente i medesimi risultati per la stessa compito, indipendentemente da lievi variazioni nella formulazione delle istruzioni.</sample>
    <sample id="527">Ecco il contenuto in italiano: "Efficacia della Tuning delle Istruzioni su MULTIINSTRUCT Tabella 1: Prestazioni Zero-shot sul Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di Raziomodale di</sample>
    <sample id="528">Ecco il contenuto in italiano: "Efficacia della Tuning delle Istruzioni su MULTIINSTRUCT Tabella 1: Prestazioni Zero-shot sul Raziamento Multimodale del Comune Senso. La miglior prestazione è in grassetto. Tabella 2: Prestazioni Zero-shot sul Rispondere alle Domande e sulle Cose Varie. La miglior prestazione è in grassetto."</sample>
    <sample id="529">Effetto dell'aumento dei cluster di compiti di istruzioni multimodali.</sample>
    <sample id="530">Effetto delle istruzioni diverse sulla tunnaggio delle istruzioni. - OFA finetunato su 5 istruzioni raggiunge una prestazione aggregata molto maggiore su tutte le task di valutazione e mostra una sensibilità inferiore. Tabella 3: Effetto di un numero diverso di istruzioni. Prestazioni di OFA finetunato su diversi numeri di istruzioni.</sample>
    <sample id="531">Effetto delle strategie di ottimizzazione fine su sensibilità del modello. • L'ottimizzazione fine su MultiInstruct può significativamente ridurre la sensibilità di OFA. • L'apprendimento a trasferimento dal dataset di Instruzioni Naturali può ulteriormente ridurre la sensibilità del modello. Figura 4: Sensibilità del modello su valutazioni di valutazione non viste. Il valore più basso è meglio.</sample>
    <sample id="532">Ecco il contenuto tradotto in italiano: "Rendita di un'azienda agricola in Italia nel 2022 è stata di 100.000 euro. La rendita di un'azienda agricola in Francia nel 2022 è stata di 120.000 euro. La rendita di un'azienda agricola in Germania nel 2022 è stata di 110.000 euro. La rendita di un'azienda agricola in Spagna nel 2022 è stata di 90.000 euro. La rendita di un'azienda agricola in Regno Unito nel 2022 è stata di 130.000 euro. La rendita di un'azienda agricola in Italia nel 2022 è stata di 100.000 euro. La rendita di un'azienda agricola in Francia nel 2022 è stata di 120.000 euro. La rendita di un'azienda agricola in Germania nel</sample>
    <sample id="533">Conclusione. Primo dataset di grandi dimensioni per l'addestramento multi-modale con istruzioni. Contiene 62 task multi-modali da 10 categorie ampie. Migliora significativamente la capacità zero-shot di OFA tramite l'addestramento con istruzioni. Esplora diverse tecniche di apprendimento trasferibile e mostra i loro benefici. Progetta una nuova metrica di sensibilità.</sample>
    <sample id="534">C'è ancora una cosa! Stiamo raccolgendo un dataset di addestramento di istruzioni multimodali molto più grande con circa 150 compiti aggiuntivi di visione-lingua e li pubblicheremo presto!</sample>
    <sample id="535">Sara Papi, Matteo Negri, e Marco Turchi sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="536">Mohammad Javad Hosseini.</sample>
    <sample id="562">I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti rispetto al contesto.</sample>
    <sample id="563">I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti rispetto al contesto.</sample>
    <sample id="564">Rivedere il paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano le differenze relative nelle probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio: BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei. P, 1, &gt; P, 2, SyntaxGym 1. Nessun cliente ... ha speso alcun denaro. 2. *Il cliente ... ha speso alcun denaro. P, 1.any, &gt; P, 2.any, CrowS 1. Frase stereotipica. 2. Frase non stereotipica. P, 1, &gt; P, 2,</sample>
    <sample id="565">Rivedere il paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei. P, 1, &gt; P, 2, SyntaxGym 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro. P, 1.any, &gt; P, 2.any, CrowS 1. Frase stereotipica. 2. Frase non stereotipica. P, 1, &gt; P, 2,</sample>
    <sample id="566">Rivedere il paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei. P, 1, &gt; P, 2, SyntaxGym 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro. P, 1.any, ≥ P, 2.any, CrowS 1. Frase stereotipica. 2. Frase non stereotipica. P, 1, &gt; P, 2,</sample>
    <sample id="567">Rivedere il paradigma delle coppie minimali.Evaluzioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei.SyntaGym 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro.CrowS 1. Frase stereotipica. 2. Frase non stereotipica.P, 1, &gt; P, 2, P, 1.any, ≥ P, 2.any, P, 1, &gt; P, 2,</sample>
    <sample id="568">Rivedere il paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei.SyntaGym 1. Nessun cliente... ha speso denaro. 2. *Il cliente... ha speso denaro.CrowS 1. Le donne sono terribili per il lavoro manuale. 2. Gli uomini sono terribili per il lavoro manuale.P, 1, &gt; P, 2, P, 1.any, ≥ P, 2.any, P, 1, &gt; P, 2, Questi giudizi sono stabili con un contesto precedente lungo?</sample>
    <sample id="569">Rivedere il paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei. P,1, &gt; P,2, SyntaxGym 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro. P,1.any, &gt; P,2.any, Crows 1. Le donne sono terribili per l'artigianato. 2. Gli uomini sono terribili per l'artigianato. P,1, &gt; P,2, Questi giudizi sono stabili con un contesto precedente lungo?</sample>
    <sample id="570">Rivedere il paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei. P,1, &gt; P,2, SyntaxGym 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro. P,1.any, &gt; P,2.any, Crows 1. Le donne sono terribili per il lavoro manuale. 2. Gli uomini sono terribili per il lavoro manuale. P,1, &gt; P,2, Questi giudizi sono stabili con un contesto precedente lungo?</sample>
    <sample id="571">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spazio di Candidati Prefixes.Matched: Convenzione Soggetto, Verbale.Mismatched: Effetti dell'isola, vuoti di filtro, legame, Wikipedia...accettabile non accettabile.PLM, Prefix) &gt;? PLM, Prefix)sample.What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from before returning to this customer?What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from before returning to this customer?BLIMP, Isola Adjunct.GPT2, OPT famiglia - 125M a 6.7B</sample>
    <sample id="572">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spazio di Candidati Prefixes.Matched: Convenzione Soggetto, Verba.Unmatched: Effetti Isola, Disconnessioni, Legame, Wikipedia.GPT2, Famiglia OPT - 125M a 6.7B.</sample>
    <sample id="573">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spazio di Candidati Prefixes.Matched: Convenzione Soggetto, Verga.Accettabile, Conforme.Unaccettabile,Non Conforme.PLM, Prefix &gt;? PLM, Prefix.Sampl.What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from before returning to this customer?What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from before returning to this customer?BLIMP, Isola Adjunct.GPT2, OPT famiglia - 125M a 6.7B</sample>
    <sample id="574">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spacce di Candidati Prefixes.Matched: Convenzione Soggetto, Verba.Unmatched: Effetti Isola, Vuoti di Filtrazione, Legame, Wikipedia.GPT2, Famiglia OPT - 125M a 6.7B.</sample>
    <sample id="575">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spazio di Candidati Prefixes.Matched: Convenzione Soggetto, Verba.Unmatched: Effetti Isola, Disconnessioni, Legame, Wikipedia.GPT2, Famiglia OPT - 125M a 6.7B.</sample>
    <sample id="576">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spacce di Candidati Prefixes.Matched: Convenzione Soggetto, Verga.Unmatche: Island Effects, Filter-gaps, Binding, Wikipedia.Esempio: Chi potrebbe fuggire Rose prima di tornare a questo cliente? Chi potrebbe fuggire Rose da questo cliente prima di tornare?Esempio: C'era un documentario sulla musica che irritava Allison. Non c'erano legislatori che lavoravano duramente. Chi potrebbe fuggire Rose da questo cliente prima di tornare?Accettabile, Mismatched.GPT2, OPT famiglia - 125M a 6.7B</sample>
    <sample id="577">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spazio di Candidati Prefixes.Matched: Convenzione Soggetto, Verga.Unmatched: Effetti dell'isola, vuoti di filtro, legame, Wikipedia.Esempi: "Chi potrebbe fuggire di fronte al ritorno di questo cliente?" e "C'era un documentario sulla musica che irritava Allison. Non c'erano legislatori che lavoravano duramente. Chi potrebbe fuggire di fronte al ritorno di questo cliente?" e "C'era un documentario sulla musica che irritava Allison. C'erano molti legislatori che lavoravano duramente. Chi potrebbe fuggire di fronte al ritorno di questo cliente?"BLiMP, quantificatori esistenziali "there"BLiMP, isola di adiuvante.GPT2, famiglia OPT - 125M a 6.7B</sample>
    <sample id="578">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'acceptabilità.Test Suite: Convenzione Soggetto-Verga.Spacce di Candidati Prefixes.Matched: Convenzione Soggetto, Verba.Unmatched: Effetti Isola, Vuoti, Legame, Wikipedia, ...accettabile non accettabile.P,LM, Prefix, &gt;? P,LM, Prefix, esempio.Wikipedia, Irrelavente.Chi potrebbe fuggire Rose prima di tornare a questo cliente?Chi potrebbe fuggire Rose da questo cliente prima di tornare a?Un'idea è una pianta perenniflora legnosa della genere Rosa. Ci sono oltre trecento specie e decine di migliaia di cultivar.Una rosa è una pianta perenniflora legnosa del genere Rosa. Ci sono oltre trecento specie e decine di migliaia di cultivar.Chi potrebbe fuggire Rose prima di tornare a?GPT2, Famiglia OPT - 125</sample>
    <sample id="579">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Convenzione Soggetto-Verga.Spazio di Candidati Prefixes.Matched: Convenzione Soggetto, Verga.Unmatched: Effetti dell'isola, vuoti di filtro, legame, Wikipedia, ...accettabile non accettabile.P,LM, Prefix, &gt;? P,LM, Prefix, ,样例.Wikipedia, Irrelavente.A cosa risponde un'entità di un genere Rosa? Chi potrebbe scappare da questo cliente prima di tornare?* Chi potrebbe scappare da questo cliente prima di tornare?A rose è una pianta perenni fiorente legnosa del genere Rosa. Ci sono oltre trecento specie e decine di migliaia di cultivar.Wikipedia, Irrelavente.A cosa risponde un'entità di un genere Rosa? Chi potrebbe scappare da questo cliente prima di tornare?* Chi potrebbe scappare da questo cliente prima di torn</sample>
    <sample id="580">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'acceptabilità.Test Suite: Convenzione Soggetto-Verga.Spacce di Candidati Prefixes.Matched: Convenzione Soggetto, Verba.Unmatched: Effetti dell'isola, vuoti, legami, Wikipedia.GPT2, Famiglia OPT - 125M a 6.7B</sample>
    <sample id="581">I giudizi MPP sono robusti per lunghezze di contesto arbitrarie. Effettuiamo valutazioni MPP con diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B.</sample>
    <sample id="582">I giudizi MPP sono robusti per lunghezze di contesto arbitrarie. Effettuiamo valutazioni MPP con diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B.</sample>
    <sample id="583">Ecco la traduzione italiana del contenuto in inglese: "2. Frasi MPP accettabili/non accettabili nel contesto elevano/baiono il rendimento della giudicazione.Nei nostri esami di MPP utilizziamo diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token.Eseguiamo valutazioni di MPP con diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token."</sample>
    <sample id="584">Ecco la traduzione italiana del contenuto in inglese: "2. Frasi MPP accettabili/non accettabili nel contesto elevano/baiongono il rendimento di giudizio. Effettuiamo valutazioni MPP con diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B Strategia di prefisso: * Acc. (Corrispondente) * Unacc. (Corrispondente) * Acc. (Non corrispondente) * Unacc. (Non corrispondente) * Wiki (Non corrispondente) C'era un documentario sulla musica che irritava Allison. Non c'erano legislatori che lavoravano duramente. Da dove potrebbe fuggire Rose prima di tornare a questo cliente? * C'era un documentario sulla musica che irritava Allison. Non c'erano legislatori che lavoravano duramente. Da dove potrebbe fuggire Rose prima di tornare a questo cliente? * C'era un documentario</sample>
    <sample id="585">Ecco la traduzione italiana del contenuto in inglese: "2. Frasi MPP accettabili/non accettabili nel contesto elevano/bassano il rendimento di giudizio. Effettuiamo valutazioni MPP con diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B Strategia di prefisso: * Acc. (Corrispondente) * Unacc. (Corrispondente) * Acc. (Non corrispondente) * Unacc. (Non corrispondente) * Wiki (Non corrispondente) * Acc. (Corrispondente) * Unacc. (Corrispondente) * Acc. (Non corrispondente) * Unacc. (Non corrispondente) * Wiki (Non corrispondente) * Acc. (Corrispondente) * Unacc. (Corrispondente) * Acc. (Non corrispondente) * Unacc. (Non corrispondente) * Wiki (Non cor</sample>
    <sample id="586">Ecco la traduzione italiana del contenuto in inglese: "Sentenze MPP accettabili/non accettabili con struttura corrispondente che influenzano maggiormente le prestazioni del modello. Effettuiamo valutazioni MPP con contesti diversi - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B Strategia di prefisso: Acc. (Corrispondente) Unacc. (Corrispondente) Acc. (Non corrispondente) Unacc. (Non corrispondente) 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3 2 1 2 3</sample>
    <sample id="587">Ecco la traduzione italiana del contenuto in inglese: "Sentenze MPP accettabili/non accettabili con struttura corrispondente influenzano maggiormente le prestazioni del modello. Effettuiamo valutazioni MPP con contesti diversi - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B Strategia di prefisso: - Acc. (Corrispondente) - Unacc. (Corrispondente) - Acc. (Non corrispondente) - Unacc. (Non corrispondente) - Wiki (Non corrispondente) 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3</sample>
    <sample id="588">Ecco la traduzione italiana del contenuto in inglese: "Sentenze MPP accettabili/non accettabili con struttura corrispondente che influenzano maggiormente le prestazioni del modello. Effettuiamo valutazioni MPP con contesti diversi - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B Strategia di prefisso: * Acc. (Corrispondente) * Unacc. (Corrispondente) * Acc. (Non corrispondente) * Unacc. (Non corrispondente) * Wiki (Non corrispondente) 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3 3 2 1 2 3</sample>
    <sample id="589">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di linguaggio?Modifichiamo le frasi di contesto in modi che preservano la struttura rilevante, e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.". - Prefissi avverbi lunghi: "Prima di tutto, &lt;sent&gt;.". - Aggiungere una clausola: "Indipendentemente da ciò che X ne pensa, &lt;sent&gt;.". - Citazione: "Ieri, X ha detto, &lt;sent&gt;."</sample>
    <sample id="590">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di linguaggio?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.". - Avverbi di lunghezza prefissa: "Prima e soprattutto, &lt;sent&gt;.". - Clauses aggiunte: "Indipendentemente da ciò che X ne pensa, &lt;sent&gt;.". - Citazione: "Ieri, X ha detto, &lt;sent&gt;.".</sample>
    <sample id="591">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di linguaggio?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;."Prefissi avverbi lunghi: "Prima di tutto, &lt;sent&gt;."Classe aggiunta: "Indipendentemente da quello che X ne pensa, &lt;sent&gt;."Citazione: "Ieri, X ha detto, &lt;sent&gt;."</sample>
    <sample id="592">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di linguaggio?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.". - Avverbi di lunghezza prefisso: "Prima di tutto, &lt;sent&gt;.". - Aggiungere una clausola: "Indipendentemente da ciò che X ne pensa, &lt;sent&gt;.". - Citazione: "Ieri, X ha detto, &lt;sent&gt;.".I modelli sono sensibili a frasi modificate in modi simili.</sample>
    <sample id="593">Perché i prefissi corrispondenti influenzano le giudicazioni di LM?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.". - Avverbi di lungo prefisso: "Prima di tutto, &lt;sent&gt;.". - Aggiungere una clausola: "Indipendentemente da ciò che X pensa di esso, &lt;sent&gt;.". - Citazione: "Ieri, X ha detto, &lt;sent&gt;.".I modelli sono sensibili a frasi modificate in modi simili.</sample>
    <sample id="594">• I modelli di linguaggio sono sensibili a feature sintattiche/semantiche latenti condivise tra frasi. • Le valutazioni MPP con input di singola frase brevi non catturano completamente il conoscenza astratta dei modelli di linguaggio.</sample>
    <sample id="595">• I modelli di linguaggio sono sensibili a feature sintattiche/semantiche latenti condivise tra frasi. • Le valutazioni MPP con input di singola frase brevi non catturano completamente il sapere astratto dei modelli di linguaggio.</sample>
    <sample id="596">• I modelli di linguaggio sono sensibili a feature sintattiche/semantiche latenti condivise tra frasi. • Le valutazioni MPP con input di singola frase brevi non catturano completamente il sapere astratto dei modelli di linguaggio.</sample>
    <sample id="597">Nel tipo di token "Tag".</sample>
    <sample id="598">55.000 script.</sample>
    <sample id="626">Non c'è una risposta diretta in questa tabella per DEplain. Potresti provare a cercare ulteriori informazioni o a chiarire meglio la tua domanda?</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato allevia il problema dell'annotazione.</sample>
    <sample id="628">Non c'è informazione specifica su come è avvenuta l'allocazione in inglese nel contenuto fornito. Potresti cercare ulteriori dettagli nel documento originale?</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato raccolgendo notizie Reuters del 2020 e annotandole secondo le linee guida di annotazione CoNLL-2003. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="630">XSemPLR: Analisi semantica interlinguistica in diverse lingue naturali e rappresentazioni del significato. Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang. Penn State, Amazon.</sample>
    <sample id="631">Parsing semantico è una task per costruire una rappresentazione semantica delle query dell'utente, come SQL e Lambda Calculus. Quali paesi in Europa hanno almeno 3 produttori di automobili? Seleziona T1.country_name. Da countries come T1 JOIN continents come T2 ON T1.continent = T2.cont_id JOIN car_makers come T3 ON T1.country_id = T3.country WHERE T2.continent = 'Europe' GROUP BY T1.country_name HAVING COUNT, * &gt;= 3. Quali giocatori hanno meno di tre assist in una stagione? Chiamare SW.listValue, chiamare SW.getProperty, chiamare SW.filter, chiamare SW.ensureNumericProperty, chiamare SW.ensureNumericEntity, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW.domain, chiamare SW</sample>
    <sample id="632">La traduzione in italiano del contenuto in inglese è: "La semantica parallela è una task che traduce query in diverse lingue naturali in diverse rappresentazioni di significato."</sample>
    <sample id="633">La traduzione in italiano del contenuto in inglese è: "La semantica parallela è una task che traduce query in diverse lingue naturali in diverse rappresentazioni di significato."</sample>
    <sample id="634">I modelli CLSP esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe lingue naturali.</sample>
    <sample id="635">I modelli CLSP esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe lingue naturali.</sample>
    <sample id="636">I modelli di Parsing Semantico Cross-linguale esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe rappresentazioni di significato.</sample>
    <sample id="637">I modelli di Parsing Semantico Interlingua sono stati proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe rappresentazioni di significato.</sample>
    <sample id="638">I modelli CLSP esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certi modelli neurali.</sample>
    <sample id="639">Ecco la traduzione in italiano del contenuto in inglese: "XSemPLR - Offriamo un dataset unificato XSemPLR per la semantica di parsing interlinguale in diverse lingue naturali e rappresentazioni di significato. Contiene: 9 dataset in diversi domini, 5 task di parsing semantico, 8 rappresentazioni di significato, 22 lingue naturali in 15 famiglie linguistiche."</sample>
    <sample id="640">Ecco la traduzione in italiano del contenuto in inglese: "XSemPLR - Offriamo un dataset unificato XSemPLR per la semantica di parsing interlinguale in diverse lingue naturali e rappresentazioni di significato. Contiene: - 9 dataset in diversi domini. - 5 compiti di parsing semantico. - 8 rappresentazioni di significato. - 22 lingue naturali in 15 famiglie linguistiche."</sample>
    <sample id="641">Impostazioni dell'esperimento Consideriamo le sei impostazioni per l'addestramento e l'evaluazione. Translate-Test: Utilizziamo l'API di Google Translate per tradurre il linguaggio di origine al linguaggio di destinazione. Poi utilizziamo un modello monolingue per l'addestramento e l'evaluazione. Addestramento Germano -&gt; Traduttore API -&gt; Inglese -&gt; Modello Inglese -&gt; SQL Inferenza Germano -&gt; Traduttore API -&gt; Inglese -&gt; Modello Inglese -&gt; SQL</sample>
    <sample id="642">Impostazioni dell'esperimento Consideriamo i sei set di allenamento e valutazione. Translate-Test: Utilizziamo l'API di Google Translate per tradurre il fonte nella lingua di destinazione. Poi utilizziamo un modello monolingue per l'allenamento e la valutazione. Allenamento Germano -&gt; Traduttore API -&gt; Inglese -&gt; Modello Inglese -&gt; SQL Inferenza Germano -&gt; Traduttore API -&gt; Inglese -&gt; Modello Inglese -&gt; SQL</sample>
    <sample id="643">Impariamo insieme a tradurre il contenuto in italiano.</sample>
    <sample id="644">Impostazioni dell'esperimento Consideriamo le sei impostazioni per l'addestramento e l'valutazione. Modello monolinguale: La lingua di origine è la stessa della lingua di destinazione, ad esempio Germano-germano. Abbiamo anche testato la configurazione Monolingual Few-shot addestrando modelli monolinguali con solo 10% dei dati di addestramento. Addestramento Germano, Few-shot Modello Germano SQL Inferenza Germano Modello Germano SQL</sample>
    <sample id="645">Impostazioni dell'esperimento Consideriamo le sei impostazioni per l'addestramento e l'valutazione. Modello monolinguale: La lingua di origine è la stessa della lingua di destinazione, ad esempio Germano-germano. Abbiamo anche testato la configurazione Monolingual Few-shot addestrando modelli monolinguali con solo 10% di dati di addestramento. Addestramento Germano, Few-shot Modello Germano SQL Inferenza Germano Modello Germano SQL</sample>
    <sample id="646">Impostazioni dell'esperimento Consideriamo le sei impostazioni per l'addestramento e l'valutazione. Modello monolinguale: La lingua di origine è la stessa della lingua di destinazione, ad esempio Germano-germano. Abbiamo anche testato la configurazione Monolingual Few-shot addestrando modelli monolinguali con solo 10% di dati di addestramento. Addestramento Germano, Few-shot Modello Germano SQL Inferenza Germano Modello Germano SQL</sample>
    <sample id="647">Impariamo a tradurre il contenuto in inglese in italiano.</sample>
    <sample id="648">Impariamo a tradurre il contenuto in inglese in italiano.</sample>
    <sample id="649">Impariamo a tradurre il contenuto in inglese in italiano.</sample>
    <sample id="650">Impariamo a tradurre il contenuto in italiano.</sample>
    <sample id="651">Impariamo a tradurre il contenuto in italiano.</sample>
    <sample id="652">Analisi del Monolingua. - Valutiamo su due gruppi di modelli nel contesto Monolingua. Enc-PTR: Encoder multilingue con decodificatori basati su pointer. - XLM-R + PTR, mBERT + PTR. Enc-Dec: Encoder-decoder multilingue preaddestrati. - mBART, mT5. - Abbiamo trovato che Enc-Dec, mT5, ottiene la migliore prestazione su tutti i dataset! Tabella con valori numerici per i modelli su diversi dataset.</sample>
    <sample id="653">Analisi del Monolingua. - Valutiamo due gruppi di modelli in un contesto monolingue. Enc-PTR: Encoder multilingue con decodificatori basati su pointer. XLM-R + PTR, mBERT + PTR. Enc-Dec: Encoder-decoder multilingue. mBART, mT5. - Abbiamo trovato che Enc-Dec, mT5, ottiene la migliore prestazione su tutti i dataset! Tabella con valori numerici per i diversi dataset e modelli.</sample>
    <sample id="654">Analisi del Monolingua. Valutiamo due gruppi di modelli in un contesto monolingue. Enc-PTR: Encoder multilingue con decoder basato su pointer. XLM-R + PTR, mBERT + PTR. Enc-Dec: Encoder-decoder multilingue. mBART, mT5. Abbiamo trovato che Enc-Dec, mT5, ha ottenuto la migliore prestazione su tutti i dataset!</sample>
    <sample id="655">Analisi del Monolingua. - Valutiamo su due gruppi di modelli in un contesto monolingue. Enc-PTR: Encoder multilingue con decoder basato su pointer. XLM-R + PTR, mBERT + PTR. Enc-Dec: Encoder-decoder multilingue preaddestrati. mBART, mT5. - Abbiamo trovato che Enc-Dec, mT5, ha ottenuto la migliore prestazione su tutti i dataset! Tabella con valori numerici per diversi dataset e modelli.</sample>
    <sample id="656">L'analisi dell'addestramento multilingue. Valutiamo su mT5 e XLM-R + PTR in un contesto multilingue. Enc-Dec/Enc-PTR, mT5/XLM-R, può essere migliorato addestrando in una miscela di varie lingue. Monolingue: XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 58.16 mT5 53.15 74.26 50.73 91.65 66.29 30.15 65.16 81.83 10.29 58.16 Multilingue: XLM-R+PTR 39.72 71.35 40.20 85.91 61.03 30.79 61.82</sample>
    <sample id="657">L'analisi della formazione multilingue. Valutiamo su mT5 e XLM-R + PTR in un contesto multilingue. Enc-Dec/Enc-PTR, mT5/XLM-R, può essere migliorato con la formazione in una miscela di varie lingue. Monolingue: XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 58.16 mT5 53.15 74.26 50.73 91.65 66.29 30.15 65.16 81.83 10.29 58.16 Multilingue: XLM-R+PTR 39.72 71.35 40.20 85.91 61.03 30.79 61.82 81.</sample>
    <sample id="658">Analisi dell'addestramento multilingue. - Valutiamo su mT5 e XLM-R + PTR in un contesto multilingue. - La maggior parte delle principali NL può ottenere un miglioramento, eccetto che le prestazioni dell'inglese diminuiscono in 7 dataset e aumentano in 3 dataset. Questo è noto come "Curse of Multilinguality".</sample>
    <sample id="659">Analisi dell'addestramento multilingue. - Valutiamo su mT5 e XLM-R + PTR in un contesto multilingue. - La maggior parte delle principali NLP può ottenere un miglioramento, eccetto che le prestazioni dell'inglese diminuiscono in 7 dataset e aumentano in 3 dataset. Questo è noto come "Curso della multilinguistica".</sample>
    <sample id="660">Gap di prestazione tra lingue diverse. Linea blu: trasferimento tra lingue in pochi colpi. Linea arancione: trasferimento tra lingue in zero colpi. Linea verde: configurazione monolingua. MTOP Schema2QA Overnight NLMaps MCWQ ATIS Spider Geoquery Geoquery/lamb Geoquery/prolog Geoquery/funql Geoquery/sql</sample>
    <sample id="661">C'è un gap di prestazioni tra linguaggi diversi. La linea blu rappresenta il trasferimento a pochi shot tra linguaggi diversi, la linea arancione il trasferimento a zero shot tra linguaggi diversi e la linea verde la configurazione monolingua.</sample>
    <sample id="662">C'è un divario di prestazioni nel trasferimento tra lingue. Per il trasferimento a zero shot, il divario di prestazione nel trasferimento tra lingue è significativo. Per il trasferimento a pochi shot, il divario di trasferimento viene ridotto rapidamente.</sample>
    <sample id="663">Risultati e Trovati (Sezione 4 nel Paper)• Enc-Dec, mT5, supera il lavoro precedente o raggiunge risultati paragonabili. • Pretraining sulle NL inglesi può significativamente migliorare le prestazioni del few-shot su NL di target. • I modelli multilingue LLMs, Codex e BLOOM, sono ancora insufficienti per le task di parsing semantico interlingue. • L'apprendimento da trasferimento cinese e l'addestramento monolingue inglese, En -&gt; En, hanno il gap di prestazioni più grande, mentre il tedesco ha solitamente il più piccolo. • FunQL supera le altre tre rappresentazioni di significato, mentre SQL ottiene la peggior prestazione.</sample>
    <sample id="664">Ecco la traduzione in italiano del contenuto in inglese:.- Enc-Dec, mT5, supera il lavoro precedente o raggiunge risultati paragonabili.- La preaddestramento sulle NL inglesi può notevolmente migliorare le prestazioni del few-shot sulle NL di destinazione.- I modelli multilingue LLM, Codex e BLOOM, sono ancora insufficienti per le task di parsing semantico interlingue.- La transfer learning cinese e l'addestramento monolingue inglese, En -&gt; En, hanno il gap di prestazioni più grande, mentre il tedesco ha solitamente il più piccolo.- FunQL supera le altre tre rappresentazioni di significato, mentre SQL ottiene la peggior prestazione.</sample>
    <sample id="665">Conclusione - Abbiamo costruito XSemPLR, un benchmark unificato per la parsing semantica interlinguistica con diverse lingue naturali e rappresentazioni di significato. - Abbiamo condotto una studio di benchmarking comprensivo su tre tipi rappresentativi di modelli di linguaggio multilingue. - I nostri risultati mostrano che mT5 con addestramento monolingue dà i migliori risultati, mentre notevolmente i modelli multilingue LLM sono ancora insufficienti per eseguire task di parsing semantico interlinguistico. Inoltre, la differenza di prestazioni tra addestramento monolingue e apprendimento trasferimento interlinguistico rimane ancora significativa.</sample>
    <sample id="666">Conclusione - Abbiamo costruito XSemPLR, un benchmark unificato per la parsing semantica interlinguistica con diverse lingue naturali e rappresentazioni di significato. - Abbiamo condotto una studio di benchmarking comprensivo su tre tipi rappresentativi di modelli di linguaggio multilingue. - I nostri risultati mostrano che mT5 con addestramento monolingue dà i migliori risultati, mentre notevolmente i modelli multilingue LLM sono ancora insufficienti per eseguire compiti di parsing semantico interlinguistico. Inoltre, la differenza di prestazioni tra addestramento monolingue e apprendimento a trasferimento interlingue rimane ancora significativa. Link - Benvenuti a visitare il nostro articolo e il codice! - Link al documento: https://arxiv.org/pdf/2306.04085.pdf - Link al codice: https://github.com/psunlpgroup/xsemplr</sample>
    <sample id="667">I lavori connessi sono: "Parameter-based watermark", "Lexical watermark", "Backdoor-based watermark" e "Adversarial-based watermark".</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom sono ancora insufficienti per il CLSP. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="695">Induce l'ambiguità durante l'addestramento.</sample>
    <sample id="696">L'equità di un modello NLP a valle viene definita come la capacità del modello di non produrre output discriminatori basati su fattori come genere, etnia, età ecc. Se hai altre domande su questo argomento, sentiti libero di chiedere!</sample>
    <sample id="697">The name of the speaker is not provided in the image.</sample>
    <sample id="698">Non so chi sia il relatore o la relatrice. Potresti dare più informazioni?</sample>
    <sample id="699">Myra Cheng.</sample>
    <sample id="700">Il tropicalismo indica la cultura, la tradizione, la orgoglio e l'esotismo per i gruppi marcati.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target attraverso narrativi essenzializzanti, definendo gruppi contraddistinti solo per la loro identità, e attraverso rappresentazioni positive perniciose.</sample>
    <sample id="702">P-CXMI.</sample>
    <sample id="703">DrBERT è costruito da zero con la costruzione completa del modello, mentre ChuBERT è costruito da zero. Non so cosa sia la differenza tra loro. Se hai altre domande, puoi chiedere.</sample>
    <sample id="751">C'è un totale di quattro autori coinvolti nell'articolo. Se hai bisogno di ulteriori informazioni sull'articolo, fammi sapere!</sample>
    <sample id="752">Ehm, il trasferimento iterativo dell'apprendimento è un approccio in cui si ottiene un modello iniziale e si aggiornano i dati e il modello in modo iterativo. Questo significa che si aggiungono nuovi dati, si addestrano nuovamente il modello e si ottengono nuovi modelli. Insomma, è un processo di aggiornamento continuo. Se hai altre domande su questo argomento, sentiti libero di chiedere!</sample>
    <sample id="753">The goal is to understand users' language when they make a choice.</sample>
    <sample id="754">Non c'è informazione su come un utente malintenzionato possa estrarre i parametri del modello attraverso un EaaS nel contenuto inglese fornito. Potresti avere bisogno di ulteriori dettagli o informazioni per rispondere a questa domanda.</sample>
    <sample id="755">C'è un totale di tre autori coinvolti nell'articolo. Se hai bisogno di ulteriori informazioni, fammi sapere!</sample>
    <sample id="756">Non c'è informazione su quanti annotatori sono stati impiegati per creare il set di dati iniziale.</sample>
    <sample id="757">Sebastin Santy e Jenny T. Liang sono affiliati all'University of Washington, Ronan Le Bras all'Allen Institute for AI, Katharina Reinecke all'University of Washington e Maarten Sap a Carnegie Mellon University.</sample>
    <sample id="758">"I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo sono quelli basati sull'Intelligenza Artificiale, come il modello di conversazione di Alexa. Questi modelli sono in grado di comprendere e rispondere in modo più umano e adattabile. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="760">Perché le valutazioni basate solo su piccole parti del contesto non sono sufficienti a comprendere l'intera comprensione del modello.</sample>
    <sample id="761">Sì, la formazione multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in 7 dataset.</sample>
    <sample id="762">No.</sample>
    <sample id="763">Non c'è informazione su quali metriche di MT siano state utilizzate per la valutazione. Potresti cercare di trovare ulteriori dettagli o contattare chi ha presentato il contenuto?</sample>
    <sample id="764">Non c'è una risposta diretta in questa immagine. Potrebbe essere necessario esaminare ulteriori informazioni per rispondere a questa domanda. Se hai altre domande o informazioni, puoi condividere.</sample>
    <sample id="765">Beh, la posizionalità nella NLP è importante perché aiuta a comprendere il significato di una parola o frase in base al contesto in cui viene usata. Ad esempio, "bank" può significare "banca" o "punti di un fiume". La posizionalità ci dice come una parola si comporta rispetto alle altre parole intorno a essa. Questo è fondamentale per la comprensione e la traduzione corretta. Se hai altre domande su questo argomento, sentiti libero di chiedere!</sample>
    <sample id="766">Non so. Potresti cercare su Internet o chiedere a un esperto.</sample>
    <sample id="767">Per il trasferimento dell'apprendimento fanno ricorso al modello RoBERTA-base + classifier head. Se hai altre domande su questo argomento, non esitare a chiedere.</sample>
    <sample id="768">Non so quali siano i recenti set di test utilizzati per valutare le capacità di PaLM. Potresti cercare su internet o contattare la Google per avere informazioni più precise.</sample>
    <sample id="769">Hanno proposto tre suggerimenti. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="770">Non so qual è il guadagno del metodo proposto rispetto al metodo di riferimento. Potresti dare più dettagli o informazioni?</sample>
    <sample id="771">Il nome del relatore è Shuheng Liu e Alan Ritter.</sample>
    <sample id="772">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="773">Nell'articolo vengono utilizzati cinque modelli più piccoli. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="774">OFA.</sample>
    <sample id="833">Non so dire con certezza le affiliazioni degli autori dell'articolo solo dalla tua domanda. Potresti fornire ulteriori dettagli o contesto?</sample>
    <sample id="834">Gli autori dell'articolo sono affiliati all'Università Stony Brook e all'Human Language Analysis Beings.</sample>
    <sample id="835">Non c'è informazione sull'articolo su quali coppie linguistiche sono state analizzate.</sample>
    <sample id="836">Shangbin Feng. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="837">Durante gli esperimenti sono stati studiati i modelli DEplain-APA, DEplain-web, DEplain-APA+web e src2src-baseline. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="838">Per scopi di addestramento vengono utilizzate 53 attività, mentre per test vengono utilizzate 62 attività. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="839">There are three authors involved in the article. If you want to know more about them, feel free to ask.</sample>
    <sample id="840">Gli autori hanno effettuato i test sui set di dati AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="876">Non so cosa sia NACHOS. Potresti dare più informazioni o cercare su Internet?</sample>
    <sample id="877">Non so chi sia il relatore o la relatrice. Potresti dare più informazioni?</sample>
    <sample id="878">La strategia del prompting ha un grande impatto sui risultati. 516 su 1000 frasi mostrano una differenza di più di 1 punto BLEURT, e la differenza può arrivare fino a 40 punti BLEURT. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="879">Patrick Fernandes, Kayo Yin e Emmy Liu sono affiliati all'Institute for Language Technologies, Carnegie Mellon University. André F. T. Martins e Graham Neubig sono affiliati a Técnico Lisboa e BAIR, Berkeley Artificial Intelligence Research.</sample>
    <sample id="880">The image doesn't provide the 5 instructions written by experts. It only mentions a larger multimodal instruction tuning dataset with around 150 additional vision-language tasks that will be released soon. If you have any other questions or need more information, feel free to ask!</sample>
    <sample id="881">Gli autori propongono un insieme di test, KITMUS Test Suite, che include un dataset per l'evaluazione dell'integrazione di conoscenza e una task di risoluzione di coreferenza per indagare l'abilità di utilizzare conoscenza di pretrain e di inferenza. Inoltre, propongono di eseguire esperimenti con partecipanti umani e modelli di risoluzione di coreferenza.</sample>
    <sample id="882">Google Prompting PaLM per la Traduzione Valutazione di Strategie e Performance ACL 2023</sample>
    <sample id="883">PaLM: Modello di Linguaggio Pathways.- Chowdery et al., 2022, arXiv:2204.02311.- 540B parametri.- Addestrato su 780B token.- Attivato densamente.- 6144 TPU v4 chip.- SOTA in centinaia di benchmark di LMU e generazione.- 28 miliardi di parametri.</sample>
    <sample id="884">PaLM: Modello di Linguaggio Pathways.- Chowdery et al., 2022, arXiv:2204.02311.- 540B parametri.- Addestrato su 780B token.- Attivato densamente.- 6144 TPU v4 chip.- SOTA in centinaia di benchmark di LMU e generazione.- 540 miliardi di parametri.</sample>
    <sample id="885">Contributi nostri. Prima studio sistematico di LLM per la MT. Entrambi per la pool di candidati e la strategia di selezione. Valutare le capacità di traduzione con le migliori pratiche della comunità di MT: Set di test più recenti, evitare sovrapposizione tra test e train e sovrappopolamento dei dati di valutazione. Confronto con le ultime sottomissioni WMT (sistema di punteggio più avanzato utilizzando i dati di training più recenti). SOTA MT metriche, correlazione migliore con le giudicanti umane. Valutazione umana basata su esperti, più robusta dei lavoratori di massa. Raccomandazione per le strategie di selezione di prompt.</sample>
    <sample id="886">Contributo nostro. Prima studio sistematico di LLM per MT. Entrambi per la pool di candidati e la strategia di selezione. Valutare le capacità di traduzione con le migliori pratiche della comunità di MT: Set di test più recenti, evitare sovrapposizione tra test e train e sovrappopolamento dei dati di valutazione. Confronto con le ultime sottomissioni WMT, sistemi SOTA utilizzando i dati di training più recenti. Metriche SOTA di MT, correlazione migliore con le giudicanti umane. Valutazione umana basata su esperti, più robusta dei lavoratori di massa. Raccomandazione per le strategie di selezione di prompt.</sample>
    <sample id="887">Contributi nostri. Prima studio sistematico di LLM per la MT. Entrambi per la pool di candidati e la strategia di selezione. Valutare le capacità di traduzione con le migliori pratiche della comunità di MT: Set di test più recenti, evitare sovrapposizione tra test e train e sovrappopolamento dei dati di valutazione. Confronto con le ultime sottomissioni WMT (sistema di punteggio più avanzato utilizzando i dati di training più recenti). SOTA MT metriche, correlazione migliore con le giudicanti umane. Valutazione umana basata su esperti, più robusta dei lavoratori di massa. Raccomandazione per le strategie di selezione di prompt.</sample>
    <sample id="888">Contributi nostri. Prima studio sistematico di LLM per MT. Entrambi per la pool di candidati e la strategia di selezione. Valutare le capacità di traduzione con le migliori pratiche della comunità di MT: Set di test più recenti, evitare sovrapposizione tra test e train e sovrappopolamento dei dati di valutazione. Confronto con le ultime sottomissioni WMT, sistemi SOTA utilizzando i dati di training più recenti. Metriche SOTA di MT, correlazione migliore con le giudicanti umane. Valutazione umana basata su esperti, più robusta dei lavoratori di massa. Raccomandazione per le strategie di selezione di prompt.</sample>
    <sample id="889">I prompt hanno un grande impatto sulla qualità della traduzione. Seleziona due prompt casuali per ogni frase. Calcola BLEURT per ogni coppia frase-prompt. La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di 1 punto BLEURT. La differenza può arrivare fino a 40 punti BLEURT!</sample>
    <sample id="890">I prompt hanno un grande impatto sulla qualità della traduzione. Per ogni frase, seleziona due prompt casuali. Calcola BLEURT per ogni coppia frase-prompt. La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di 1 punto BLEURT. La differenza può arrivare fino a 40 punti BLEURT!</sample>
    <sample id="891">I prompt hanno un grande impatto sulla qualità della traduzione. Per ogni frase, seleziona due prompt casuali. Calcola BLEURT per ogni coppia frase-prompt. La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di 1 punto BLEURT. La differenza può arrivare fino a 40 punti BLEURT!</sample>
    <sample id="892">Ecco la traduzione in italiano: "Un passante ha allertato la polizia, che è arrivata con diversi agenti di polizia." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="893">Ecco la traduzione in italiano: "Un passante ha allertato la polizia, che è arrivata con diversi agenti di polizia." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="894">Ecco la traduzione in italiano: "Un passante ha allertato la polizia, che è arrivata con diversi agenti di polizia." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="895">Ecco la traduzione in italiano: "Un passante ha allarmato la polizia, che è arrivata con diversi agenti di polizia." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="896">Un passante ha allarmato la polizia, che è arrivata con diversi agenti.</sample>
    <sample id="897">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a quella dei SOTA. Le punteggio di accuratezza sono generalmente più bassi. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="898">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a SOTA. Le punteggia di accuratezza sono generalmente più basse. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="899">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a SOTA. Le punteggio di accuratezza sono generalmente più bassi. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="900">Risultati sperimentali:.- La qualità dell'esempio è più importante della somiglianza con la frase di origine.- I sistemi SOTA specializzati hanno un vantaggio sostanziale.- PaLM è vicino a Google Translate.Individuazioni da MQM:.- La fluenza di PaLM è paragonabile a quella dei SOTA.- Le punte di accuratezza sono generalmente più basse.○ Dominato da "Accuracy/Omission"-"Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="901">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a SOTA. Le punteggio di accuratezza sono generalmente più bassi. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="902">Risultati sperimentali:.- La qualità dell'esempio è più importante della somiglianza con la frase di origine.- I sistemi SOTA specializzati hanno un vantaggio sostanziale.- PaLM è vicino a Google Translate.Individuazioni da MQM:.- La fluenza di PaLM è paragonabile a quella dei SOTA.- Le punteggia di accuratezza sono generalmente più basse. - Dominato da "Accuracy/Omission".- "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="903">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a SOTA. Le punteggio di accuratezza sono generalmente più bassi. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="904">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a SOTA. Le punteggio di accuratezza sono generalmente più bassi. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="905">Risultati sperimentali: La qualità dell'esempio è più importante della somiglianza con la frase di origine. I sistemi SOTA specializzati hanno un vantaggio sostanziale. PaLM è vicino a Google Translate. Conoscenze da MQM: La fluenza di PaLM è paragonabile a quella dei SOTA. Le punteggio di accuratezza sono generalmente più bassi. Dominato da "Accuracy/Omission". "Stile/Accattivante" è generalmente più basso per PaLM.</sample>
    <sample id="906">Certo! "Thank you" in italiano è "Grazie". Se hai bisogno di tradurre altre parole o frasi, fammelo sapere!</sample>
    <sample id="907">Sai che cosa è il "Weakly Supervised Learning"? Ecco una traduzione italiana del contenuto in inglese: "Più debole di quanto pensi: un'analisi critica del Learning supervisionato debolmente". Se hai bisogno di ulteriori informazioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="908">"Più debole di quanto pensi: un'analisi critica dell'apprendimento supervisionato debole"</sample>
    <sample id="909">Perché l'apprendimento supervisionato debole?• L'apprendimento supervisionato debole allevia il blocco dell'annotazione. Ma i etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • L'apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="910">Perché l'apprendimento supervisionato debole?• L'apprendimento supervisionato debole allevia il blocco dell'annotazione. Ma le etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • L'apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="911">Perché l'apprendimento supervisionato debole?• L'apprendimento supervisionato debole allevia il blocco dell'annotazione. Ma le etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • L'apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="912">Perché l'apprendimento supervisionato debole?• L'apprendimento supervisionato debole allevia il blocco dell'annotazione. Ma i etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • L'apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="913">Perché l'apprendimento supervisionato debole?• L'apprendimento supervisionato debole allevia il blocco dell'annotazione. Ma le etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • L'apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="914">Un索赔在最近的WSL工作中。“我们只在弱监督数据上训练模型，并且达到XX%的准确率。”</sample>
    <sample id="915">Un affermazione comune nelle recenti ricerche WSL. "Addestriamo modelli solo su dati supervisionati debolmente e otteniamo un'accuratezza del XX%."</sample>
    <sample id="916">Un索赔在最近的WSL工作中。“我们只在弱监督数据上训练模型，并且达到XX%的准确率。</sample>
    <sample id="917">Un索赔在最近的WSL工作中。“我们只在弱监督数据上训练模型，并且达到XX%的准确率。</sample>
    <sample id="918">Le nostre domande di ricerca. RQ1: È necessario il dati di validazione pulito? RQ2: Quanti campioni puliti necessitano gli approcci WSL? RQ3: Come utilizzare in modo più efficiente i campioni puliti disponibili?</sample>
    <sample id="919">Le nostre domande di ricerca. RQ1: È necessario il dati di validazione pulito? RQ2: Quanti campioni puliti necessitano gli approcci WSL? RQ3: Come utilizzare in modo più efficiente i campioni puliti disponibili?</sample>
    <sample id="920">RQ1 Trovati principali.</sample>
    <sample id="921">RQ1 Ricerche principali.</sample>
    <sample id="922">RQ1 Ricerche principali.</sample>
    <sample id="923">RQ1 Trovati principali.</sample>
    <sample id="924">RQ1 Ricerche principali. Un insieme di validazione pulito è indispensabile.</sample>
    <sample id="925">RQ2 Ricerche principali.</sample>
    <sample id="926">RQ2 Ricerche principali.</sample>
    <sample id="927">RQ2 Ricerche principali -&gt; Approcci WSL beneficiano da più campioni di validazione puliti!</sample>
    <sample id="928">RQ2 Ricerche principali -&gt; Approcci WSL beneficiano da più campioni di validazione puliti!</sample>
    <sample id="929">RQ2 Rilevamenti principali -&gt; Approcci WSL beneficiano da più campioni di validazione puliti! -&gt; Ma è ancora meglio usarli per l'addestramento, ad esempio, LoRA_c!</sample>
    <sample id="930">RQ3 Rilevamenti principali N=10 campioni puliti per classe N=30 campioni puliti per classe Precisione/F1 Precisione/F1 Prima CFT Dopo CFT Prima CFT Dopo CFT</sample>
    <sample id="931">RQ3 Rilevamenti principali N=10 campioni puliti per classe N=30 campioni puliti per classe Precisione/F1 Precisione/F1 Prima CFT Dopo CFT Prima CFT Dopo CFT</sample>
    <sample id="932">RQ3 Rilevamenti principali N=10 campioni puliti per classe N=30 campioni puliti per classe</sample>
    <sample id="933">RQ3 Rilevamenti principali N=10 campioni puliti per classe N=30 campioni puliti per classe Accuracy/F1 Accuracy/F1 Prima di CFT Dopo CFT Prima di CFT Dopo CFT → Continua l'ottimizzazione continua, CFT, elimina le differenze di prestazione tra le approcci WSL. → Non c'è bisogno di utilizzare metodi WSL complicati, FTw, funziona altrettanto bene.</sample>
    <sample id="934">Conclusione

Approcci recenti WSL

- Richiedono campioni puliti.
- Sottostimano la loro praticabilità. 😕

Raccomandazioni

- Riferire i criteri di selezione del modello.
- Usare approcci di apprendimento a poche immagini come basi.
- Sempre applicare l'ottimizzazione continua (CFT).</sample>
    <sample id="935">Conclusione

Approcci recenti WSL

- Richiedono campioni puliti.
- Sottostimano la loro praticabilità. 😕

Raccomandazioni

- Riferire i criteri di selezione del modello.
- Usare gli approcci di apprendimento a poche istanze come basi.
- Sempre applicare il raffinamento continuo (CFT).</sample>
    <sample id="936">Conclusione

Approcci recenti WSL

- Richiedono campioni puliti.
- Sottostimano la loro praticabilità. 😕

Raccomandazioni

- Riferire i criteri di selezione del modello.
- Usare approcci di apprendimento a poche istanze come basi.
- Sempre applicare l'ottimizzazione continua (CFT).</sample>
    <sample id="937">ConclusioneApprocci recenti WSL:• Richiedono campioni puliti.Overstimano la loro praticità.Noi consigli:• Riferire i criteri di selezione del modello.Usare approcci di apprendimento a poche immagini come basi.Always applicare il raffinamento continuo, CFT.</sample>
    <sample id="938">ConclusioneApprocci recenti WSL:Richiedono campioni puliti.Overstimano la loro praticità.Raccomandazioni:Rapportare i criteri di selezione del modello.Usare approcci di apprendimento a poche sfide come basi.Applicare sempre l'ottimizzazione continua (CFT).</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo sono la valutazione comparativa e la valutazione di classificazione. C'è anche la valutazione di classificazione. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="940">Cinque. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, sono necessarie due conoscenze di base: 1) conoscenza specifica dell'entità, che è che Servin è un giudice e Kea è un pasticcere, e 2) conoscenza di fondo, che è che i giudici decidono casi in tribunali. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="942">Sì, il codice è disponibile. Puoi trovarlo su GitHub, nel repository mpoemsl/kitmus.</sample>
    <sample id="943">No, non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="944">Le frasi sono state perturbate in modo da mantenere la struttura relativa.</sample>
    <sample id="945">E' un modo per misurare e valutare diverse aspetti di qualcosa. Ad esempio, nel caso di questa presentazione, misurare la qualità del dialogo in base a criteri come rilevanza, coerenza e comprensione emotiva. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="946">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun e Xing Xie hanno affiliazioni diverse. Wenjun Peng e Jingwei Yi sono dell'University of Science and Technology of China, Fangzhao Wu e Shangxi Wu sono di Microsoft Research Asia, Bin Zhu e Lingjuan Lyu sono di Beijing Jiaotong University, Binxing Jiao e Tong Xu sono di Sony AI, Guangzhong Sun è di Microsoft STC Asia.</sample>
    <sample id="947">Quando si parla di traduzioni, la forma del prompting può essere importante per garantire che il messaggio sia trasportato correttamente. Ad esempio, se si vuole tradurre una frase con un tono specifico o un contesto particolare, la forma del prompting può influire sulla traduzione finale.</sample>
    <sample id="978">Hanno valutato BART-FID-RAG, Blender2, Emora e Blender-Decode.</sample>
    <sample id="979">Ci sono nove autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="980">Non c'è una risposta diretta in questo contenuto inglese. Potresti cercare ulteriori informazioni o fornire più dettagli sulla tua domanda?</sample>
    <sample id="981">Ci sono sette autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="982">Vasudha Varadarajan.</sample>
    <sample id="983">Gli autori dell'articolo sono affiliati all'Institute of Computer Science, Polish Academy of Sciences, con indirizzo ul. Jana Kazimierza 5, 01-248 Warsaw, e all'University of Warsaw.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono "Accuracy/Omission" e "Style/Awkward". Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1022">Non dimenticare le tue ABC: valutare lo stato dell'arte nei sistemi di dialogo orientati al chat Sarah E. Finch, James D. Finch e Jinho D. Choi</sample>
    <sample id="1023">Non dimenticare le tue ABC: valutare lo stato dell'arte nei sistemi di dialogo orientati al chat Sarah E. Finch, James D. Finch e Jinho D. Choi</sample>
    <sample id="1024">Non dimenticare le tue ABC: valutare lo Stato dell'Arte nei Sistemi di Dialogo Orientati al Chat Sarah E. Finch, James D. Finch e Jinho D. Choi</sample>
    <sample id="1025">Ecco la versione italiana del contenuto in inglese: "Comparative Evaluation" significa "Valutazione Comparativa". "Likert Rating Evaluation" significa "Valutazione di Rating di Likert". Se hai bisogno di ulteriori traduzioni o chiarimenti, fammi sapere!</sample>
    <sample id="1026">Certo! Ecco la versione italiana del contenuto: "Evaluazione del Rating Likert". "Dimensioni della Qualità del Dialogo". "Qualità del Dialogo". "Rilevanza". "Comprensione Emotiva". "Coerenza". Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="1027">Dimensioni della qualità del dialogo. Valutazione del rating di Likert.</sample>
    <sample id="1028">Ecco la versione italiana del contenuto: "Evaluazione del Rating Likert. Valutare la rilevanza delle risposte del bot."</sample>
    <sample id="1029">Annotazione dei comportamenti in chat, ABC-Eval.</sample>
    <sample id="1030">Annotazione dei comportamenti in chat, ABC-Eval.</sample>
    <sample id="1031">Behaviors, Comprese, Conoscenza, Comportamenti, Consistenza, Comprese Emozionali</sample>
    <sample id="1032">Behaviors, Coerenza, Ignoring Partner, Irrelevant, Consistenza, Knowledge, Emotional Understanding,</sample>
    <sample id="1033">Behaviors, Coherence: Ignoring Partner, Irrelevant, Consistency: Self Contradiction, Partner Contradiction, Knowledge: Incorrect Fact, Commonsense Violation, Emotional Understanding: Empathetic Response, Lack of Empathy.</sample>
    <sample id="1034">Ecco la versione italiana del contenuto in inglese: "Ecco i comportamenti ABC-Eval: Coerenza - Ignoring Partner, Irrelevant, Conoscenza - Incorrect Fact, Commonsense Violation, Consistenza - Self Contradiction, Partner Contradiction, Comprendere Emozionalmente - Empathetic Response, Lack of Empathy. Esperimenti: 4 modelli di dialogo a dominio aperto, 100 conversazioni umano-bots per modello."</sample>
    <sample id="1035">Ecco la versione italiana del contenuto in inglese: "Esperimenti. 4 modelli di dialogo a dominio aperto. 100 conversazioni tra umano e bot per modello. ABC-Eval. Turn Likert. Dialogue Likert. Comparative." Se hai bisogno di ulteriori modifiche o hai altre domande, fammi sapere!</sample>
    <sample id="1036">Ecco la versione italiana del contenuto in inglese: "Esercizi. 4 modelli di dialogo a dominio aperto. 100 conversazioni umano-robot per modello. Esercizi di valutazione di base. Consistenza, comprensione emotiva, informatività, qualità complessiva. Engagingness, grammaticalità, proattività, rilevanza."</sample>
    <sample id="1037">Inter-Annotatore Convenienza</sample>
    <sample id="1038">Inter-Annotator Agreement, Predictive Validity</sample>
    <sample id="1039">Ecco il contenuto in italiano: "Validità predittiva". La presentazione mostra un grafico con quattro categorie: ABC-Eval, Turn Likert, Dialogue Likert e Comparative. Ogni categoria ha due barre rappresentative, una per Interactive Qua, d, e una per Interactive Qua, c, che indicano il percentuale di qualità spiegata, R². Le categorie hanno diverse etichette che descrivono aspetti come la qualità interattiva, l'engagement, l'informatività e così via. L'Emory University e l'Alexa sono menzionati in basso a destra della presentazione.</sample>
    <sample id="1040">Ecco il contenuto in italiano: "Validità predittiva". "Validità incrementale".</sample>
    <sample id="1041">Incremental Validità.</sample>
    <sample id="1042">Incremental Validità.</sample>
    <sample id="1043">Ecco il contenuto in italiano: "ABC-Eval Error Rates by Model". La presentazione mostra i tassi di errore per diversi modelli di ABC-Eval. I modelli testati sono BART-FID-RAG, Blender2, Emora e Blender-Decode. Ogni modello ha dei risultati diversi per le varie categorie di errori, come Antisocial, CS Contra., Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra., Redundant, Self Contra., Topic Switch e Uninterpret. Emory University e Alexa sono le istituzioni coinvolte.</sample>
    <sample id="1044">Ecco la versione italiana del contenuto: "ABC-Eval Error Rates by Model" significa "Tassi di errore ABC-Eval per modello". "Emory University" è "Università di Emory". "NLP" è "Intelligenza Artificiale". "Alexa" è "Alexa". Se hai bisogno di ulteriori traduzioni o chiarimenti, fammi sapere!</sample>
    <sample id="1045">Ecco la versione italiana del contenuto: "ABC-Eval Error Rates by Model" significa "Tassi di errore ABC-Eval per modello". "Antisocial" è "Antisociale", "CS Contra." è "CS Contro.", "Ignore" è "Ignorare", "Incorrect" è "Incorretto", "Irrelevant" è "Irrellevante", "Unempathetic" è "Non empatetico", "Other Contra." è "Altra Contro.", "Redundant" è "Ridondante", "Self Contra." è "Auto Contro.", "Topic Switch" è "Passaggio di Tema", "Uninterpret." è "Non Interpretato". "BART-FID-RAG" è "BART-FID-RAG", "Blender2" è "Blender2", "Emora" è "Emora", "Blender-Decode" è "Blender-Decodifica". "Emory University" è "Università Emory", "NLP" è "NLP", "Alexa" è "Alexa".</sample>
    <sample id="1046">Ecco la versione italiana del contenuto in inglese: "ABC-Eval Error Rates by Model" significa "Tassi di errore ABC-Eval per modello". "Antisocial" è "Antisociale", "CS Contra." è "CS Contro.", "Ignore" è "Ignorare", "Incorrect" è "Incorretto", "Irrelevant" è "Irrellevante", "Unempathetic" è "Non empatetico", "Other Contra." è "Altri Contro.", "Redundant" è "Ridondante", "Self Contra." è "Contro se stessi", "Topic Switch" è "Scambio di tema", "Uninterpret." è "Non interpretato". "BART-FID-RAG" è "BART-FID-RAG", "Blender2" è "Blender2", "Emora" è "Emora", "Blender-Decode" è "Blender-Decodifica". "Emory University" è "Università Emory", "NLP" è "NLP", "Alexa" è "Alexa".</sample>
    <sample id="1047">Ecco la versione italiana del contenuto: "ABC-Eval Error Rates by Model" è un grafico che mostra le tassi di errore per diversi modelli. Il grafico è stato presentato da Emory University e Alexa. Il grafico mostra le percentuali di turni per diversi tipi di errori come Antisocial, CS Contra., Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra., Redundant, Self Contra., Topic Switch e Uninterpret. I modelli analizzati sono BART-FID-RAG, Blender2, Emora e Blender-Decode. Alla fine, ci sono i ringraziamenti per aver visto il video, con un link al paper, al GitHub e le informazioni di contatto.</sample>
    <sample id="1048">The authors are affiliated with Emory University, Emory NLP Research Lab, and Alexa.</sample>
    <sample id="1049">CFT significa Continuous Fine-tuning. Se hai altre domande sul contenuto dell'articolo, sentiti libero di chiedere.</sample>
    <sample id="1050">Ci sono sette autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1051">Quando la traduzione richiede contesto? Un'esplorazione multilingue guidata da dati. Patrick Fernandes*, Kayo Yin*, Emmy Liu André F. T. Martins, Graham Neubig. Carnegie Mellon University Language Technologies Institute, TÉCNICO LISBOA, BAIR Berkeley Artificial Intelligence Research, Unbabel. *con contributo uguale.</sample>
    <sample id="1052">Dobbiamo liberarci di quel infiltrato.</sample>
    <sample id="1053">Traduzione dipende dal contesto.</sample>
    <sample id="1054">Potrebbe essere qualcosa di serio, dottore? Dobbiamo liberarci di quella chia.</sample>
    <sample id="1055">Evaluare la traduzione dipendente dal contesto è difficile. Solo una piccola parte di parole dipende dal contesto.</sample>
    <sample id="1056">Evaluare la traduzione dipendente dal contesto è difficile. Solo una piccola parte di parole dipende dal contesto. - Metriche a livello di corpus. - Metodi esistenti supportano fenomeni di discorso e lingue limitati.</sample>
    <sample id="1057">RQ1: Quando la traduzione richiede contesto?RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1058">RQ1: Quando la traduzione richiede contesto? - Utilizzo del contesto a livello di parola RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1059">Informazione Cross-Mutua Condizionata, CXMI, CXMI: misura quanto utilizzano i modelli di traduzione a contesto dati un corpus</sample>
    <sample id="1060">Informazione Cross-Mutua Condizionata, CXMI. CXMI: misura quanto utilizzano i modelli di traduzione con contesto dati un corpus. Uncertezza sulle traduzioni date la fonte. H, q, MT, A, Y, X, Uncertezza sulle traduzioni date la fonte e il contesto. H, q, MT, C, Y, X, C, CXMI, C, →, Y, X,</sample>
    <sample id="1061">Pointwise, P-CXMI, introduciamo P-CXMI per misurare l'uso del contesto per tradurre un particolare, Senten, P-CXMI, y, x, C, = - log, q, MT, A, y, x, q, MT, C, y, x, C, Word, P-CXMI, i, y, x, C, = - log, q, MT, A, y, i, y, t, &lt;, i, x, q, MT, C, y, i, y, t, &lt;, i, x, C, Parole con alto P-CXMI -&gt; richiedono contesto per tradurre</sample>
    <sample id="1062">RQ1: Quando la traduzione richiede contesto? - Utilizzo del contesto a livello di parola - Analisi tematica RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1063">Thematic analysis of high P-CXMI words.</sample>
    <sample id="1064">Analisi tematica di parole con alto P-CXMI. 1. Etichette POS</sample>
    <sample id="1065">Analisi tematica di parole con alto P-CXMI. 1. Etichette di POS. - Pronouni.</sample>
    <sample id="1066">Analisi tematica di parole con alto P-CXMI. 1. Etichette POS - Pronomi - Forma del verbo. 2. Elementi di vocabolario.</sample>
    <sample id="1067">La madre di Avelile stava ancora dormendo. Avelile è andata a scuola.</sample>
    <sample id="1068">1. Etichette di POS
2. Elementi di vocabolario
Avelile's madre era ancora addormentata. Avelile è andato a scuola.
La madre di Avilier è ancora addormentata. Avilier è andato a scuola.</sample>
    <sample id="1069">Certo. Ecco il contenuto in italiano: "Analisi tematica di parole a alto P-CXMI". 1. Etichette di POS, 2. Elementi di vocabolario, 3. Token individuali. Pronomi, forma del verbo, coesione lessicale, formalità, ellissi. Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="1070">RQ1: Quando la traduzione richiede contesto? - Utilizzo del contesto a livello di parola - Analisi tematica RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto? - Benchmark Multilingue Discorsuale-Aware, MuDA</sample>
    <sample id="1071">RQ1: Quando la traduzione richiede contesto? - Utilizzo del contesto a livello di parola - Analisi tematica RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto? - Multilingual Discourse-Aware, MuDA, benchmark Multilingue Discorsoso-Aware, MuDA, tagger - Pronomi - Forma del verbo - Cohesione lessicale - Formalità - Ellissi</sample>
    <sample id="1072">Multilingue Discorsos-Consciente, MuDA, tagger. - Pronomi - Forma del verbo - Cohesione lessicale - Formalità - Ellissi.</sample>
    <sample id="1073">Multilingue Discorsa-Conscio, MuDA, tagger. - Pronomi - Forma del verbo - Cohesione lessicale - Formalità - Ellissi. MuDA benchmark. MuDA tagger. BLEU COMET F-measure.</sample>
    <sample id="1074">RQ1: Quando la traduzione richiede il contesto? - Utilizzo del contesto a livello di parola - Analisi tematica RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto? - Benchmarks multilingue Discourse-Aware, MuDA - Valutazione del modello</sample>
    <sample id="1075">Metriche a livello di corpus. RQ1: Quando la traduzione richiede contesto? - Utilizzo del contesto a livello di parola - Analisi tematica. RQ2: Come gestiscono bene i modelli le traduzioni dipendenti dal contesto? - Benchmark Multilingue Discorsuale-Aware, MuDA - Valutazione del modello.</sample>
    <sample id="1076">Metriche a livello di corpus</sample>
    <sample id="1077">Metriche a livello di corpus. Non è chiaro quale sistema sia il migliore per la traduzione automatica a livello di documento con metriche a livello di corpus.</sample>
    <sample id="1078">Risultati del benchmark MuDA
- I modelli a conoscenza del contesto performano significativamente meglio in alcuni fenomeni</sample>
    <sample id="1079">Risultati del benchmark MuDA. - I modelli a conoscenza del contesto performano significativamente meglio su alcune fenomena. ✔: Formalità, coesione lessicale ✖: Ellissi, pronomi, forma del verbo.</sample>
    <sample id="1080">Risultati del benchmark MuDA. - I modelli a conoscenza del contesto performano significativamente meglio su alcune fenomena. ✔: Formalità, coesione lessicale ✖: Ellissi, pronomi, forma del verbo. - DeepL supera Google in maggior parte dei fenomeni e delle coppie di lingue.*</sample>
    <sample id="1081">Riassunto: Identificare fenomeni discorsivi in modo sistematico senza conoscenze linguistiche precedenti. Benchmark per la traduzione automatica a livello di documento indipendente dal dataset.</sample>
    <sample id="1082">Riassunto
- Identificare fenomeni discorsivi sistematicamente senza conoscenze linguistiche precedenti
- Benchmark per la traduzione automatica di documenti indipendente dal dataset</sample>
    <sample id="1083">- Identificare fenomeni discorsivi in modo sistematico senza conoscenze linguistiche precedenti. - Benchmark per la traduzione automatica di documenti indipendente dal dataset.</sample>
    <sample id="1084">Yusen Zhang.</sample>
    <sample id="1121">Il metodo ha il nome "Permuting with 'jumps'".</sample>
    <sample id="1122">Il metodo consiste in due passi. Il primo passo è generare personaggi usando promemoria come "Imagina che tu sia una donna asiatica. Descrivi te stesso". Il secondo passo è trovare parole che distinguono i personaggi dei gruppi contrassegnati dai gruppi non contrassegnati.</sample>
    <sample id="1123">Shangbin Feng: Paul G. Allen School, UWNLP. Chan Young Park: UWNLP. Yuhan Liu: Carnegie Mellon University Language Technologies Institute. Yulia Tsvetkov: Beijing University of Posts and Telecommunications.</sample>
    <sample id="1124">Bouquet/Stanford.</sample>
    <sample id="1125">The name of the speaker is Sarah E. Finch.</sample>
    <sample id="1126">Quattro.</sample>
    <sample id="1127">BLiMP, SyntaxGym, CrowS. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="1161">FTw， BOND， COSINE， MLC， L2R.</sample>
    <sample id="1162">Il modello viene valutato su 11 task.</sample>
    <sample id="1226">CamemBERT viene inizialmente addestrato su 4GB di dati.</sample>
    <sample id="1227">Adam Przepiórkowski.</sample>
    <sample id="1228">Beh, sembra che la derivazione temporale sia stata conclusa come la causa principale della perdita di prestazioni perché il rendimento peggiora con un gap temporale più grande. Questo è evidenziato nel grafico e nelle statistiche presenti. Se hai altre domande o vuoi discutere di questo in modo più approfondito, sono qui per te.</sample>
    <sample id="1269">Permette di ottenere una sequenza di output più accurata.</sample>
    <sample id="1270">Perché è importante per comprendere come i modelli gestiscono i bias.</sample>
    <sample id="1271">Per BLiMP, l'input inaccettabile è "Many people were helping herself." Per SyntaxGym, è "The customer ... has spent any money." Per CrowS, non c'è un input inaccettabile specificato. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1272">The authors used F1, CLS, NER, POS, Hamming, and EMR as evaluation metrics.</sample>
    <sample id="1273">La metrica utilizzata è Krippendorff's Alpha. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento per l'imbalance in instructional datasets tra NLP e multimodal. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1277">There are three authors involved in the article. If you want to know more about them or the article itself, feel free to ask.</sample>
    <sample id="1278">Non c'è una definizione di coordinazione binaria nella tua immagine. Forse potresti fornire più dettagli o un'altra immagine?</sample>
    <sample id="1279">Non c'è informazione sul tempo medio di utilizzo dei prompt in questo studio.</sample>
    <sample id="1280">Smaller LMs fine-tuned on Coscript can generate higher quality scripts than LLMs.</sample>
    <sample id="1281">DrBERT: Un modello pre-allenato robusto in francese per i campi biomedici e clinici.Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, Pierre-Antoine Gourraud.LIA, Avignon Université, LS2N, Nantes Université, Clinique des données, CHU de Nantes, Zenidoc.LS2N, Laboratoire des Sciences du Numérique de Nantes, Nantes Université, Centre Hospitalier Universitaire de Nantes, iD, Laboratoire Informatique d'Avignon, GENCI, Le calcul intensif au service de la connaissance, Avignon Université.</sample>
    <sample id="1282">I. Modellazione linguistica in sanità II. Confronto di strategie di pre-allenamento, fonti di dati e dimensioni III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1283">I. Modellazione linguistica in sanità II. Confronto di strategie di pre-allenamento, fonti di dati e dimensioni III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1284">I. Modellazione linguistica in sanità II. Confronto di strategie di pre-allenamento, fonti di dati e dimensioni III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1285">I. Modellazione del linguaggio in sanità. - Approcci basati su trasformatori, come BERT, offrono un guadagno di prestazioni notevole su molte compiti di NLP. - È stato adattato al francese con CamemBERT e FlauBERT. II. Confronto di strategie di pre-allenamento, fonti di dati e dimensioni. - PudMedBERT, BioBERT, ClinicalBERT e altri. III. Valutazione di 13 modelli su 11 compiti. - Lingue diverse dall'inglese sono più rare e si affidano principalmente a un pre-allenamento continuo utilizzando un modello generico esistente. IV. Distribuzione di NACHOS e DrBERT. - Invece di modelli generici, non è ancora disponibile un modello open-source per il campo biomedico in francese. - Un modello specifico di dominio basato su BERT per il francese dovrebbe aumentare le prestazioni sui compiti medici.</sample>
    <sample id="1286">Ecco la traduzione italiana del contenuto in inglese:.- Le approcci basati su trasformatori, come BERT, offrono un guadagno di prestazioni notevole in molte tesi di NLP.- È stato adattato al francese con CamemBERT e FlauBERT.- Nelle tesi mediche, i modelli specifici di dominio in inglese hanno alzato ancora di più il livello.- PudMedBERT, BioBERT, ClinicalBERT e altri.- Le lingue diverse dall'inglese sono più rare e si affidano principalmente a un pre-allenamento continuo utilizzando un modello generico esistente.- In contrapposizione ai modelli generici, non c'è ancora un modello open-source disponibile per il dominio biomedico in francese.- Un modello specifico di dominio basato su BERT in francese dovrebbe aumentare le prestazioni nelle tesi mediche.</sample>
    <sample id="1287">Modellazione del linguaggio.- Approcci basati su trasformatori, come BERT, offrono un guadagno di prestazioni notevole in molte tesi di NLP.- È stato adattato al francese con CamemBERT e FlauBERT.- Nelle tesi mediche, modelli specifici di dominio in inglese hanno alzato ancora di più il livello.- PudMedBERT, BioBERT, ClinicalBERT e altri.- Lingue diverse dall'inglese sono più rare e si affidano principalmente a un pre-allenamento continuo utilizzando un modello generico esistente.- In contrapposizione ai modelli generici, non esiste ancora un modello open-source per il dominio biomedico in francese.- Un modello specifico di dominio basato su BERT in francese dovrebbe aumentare le prestazioni nelle tesi mediche.</sample>
    <sample id="1288">- Approcci basati su trasformatori, come BERT, offrono un guadagno di prestazioni enorme su molte tesi di NLP. - È stato adattato al francese con CamemBERT e FlauBERT. - Sulle tesi mediche, i modelli specifici di dominio in inglese hanno alzato ancora di più il livello. - PudMedBERT, BioBERT, ClinicalBERT e altri. - Le lingue diverse dall'inglese sono più rare e si affidano principalmente a un pre-allenamento continuo utilizzando un modello generico esistente. - In contrapposizione ai modelli generici, non c'è ancora un modello open-source per il dominio biomedico in francese. - Un modello specifico di dominio basato su BERT in francese dovrebbe aumentare la prestazione sulle tesi mediche.</sample>
    <sample id="1289">Modellazione del linguaggio.- Approcci basati su trasformatori, come BERT, offrono un guadagno di prestazioni notevole in molte tesi di NLP.- È stato adattato al francese con CamemBERT e FlauBERT.- Nelle tesi mediche, i modelli specifici di dominio in inglese hanno alzato ancora di più il livello.- PudMedBERT, BioBERT, ClinicalBERT e altri.- Le lingue diverse dall'inglese sono più rare e si affidano principalmente a un pre-allenamento continuo utilizzando un modello generico esistente.- Invece dei modelli generici, non c'è ancora un modello open-source per il dominio biomedico in francese.- Un modello specifico di dominio basato su BERT in francese dovrebbe aumentare le prestazioni nelle tesi mediche.</sample>
    <sample id="1290">Confronto di strategie di pre-allenamento e fonti di dati.- Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili.NACHOS: Un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili.NBDW: Un dataset privato di frasi tratti da 1,7 milioni di record medici anonimizzati estratti dal data warehouse dell'ospedale universitario di Nantes.Comparazione di strategie di apprendimento.- Allena da zero con costruzione completa del modello.- Pre-allenamento continuo utilizzando un modello pre-allenato esistente, come CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese.</sample>
    <sample id="1291">Confronto di strategie di pre-allenamento e fonti di dati. Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili. NACHOS: un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. NBDW: un dataset privato di frasi tratti da 1,7 milioni di record medici anonimizzati estratti dal data warehouse dell'ospedale universitario di Nantes. Confronto di strategie di apprendimento. Da zero con costruzione completa del modello. Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese).</sample>
    <sample id="1292">Confronto di strategie di pre-allenamento e fonti di dati. Valutazione dell'impatto delle fonti di dati mediche pubbliche e private su dimensioni di dati comparabili. NACHOS: Un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. NBDW: Un dataset privato di frasi tratti da 1,7 milioni di record medici anonimizzati estratti dal data warehouse dell'ospedale universitario di Nantes. Confronto di strategie di apprendimento. Da zero con costruzione completa del modello. Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese).</sample>
    <sample id="1293">Confronto di strategie di pre-allenamento e fonti di dati. Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili. NACHOS: un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. NBDW: un dataset privato di frasi tratti da 1,7 milioni di record medici anonimizzati estratti dal data warehouse dell'ospedale universitario di Nantes. Confronto di strategie di apprendimento. Da zero con costruzione completa del modello. Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese).</sample>
    <sample id="1294">Confronto di strategie di pre-allenamento e fonti di dati. Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili. NACHOS: un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. NBDW: un dataset privato di frasi tratti da 1,7 milioni di record medici anonimizzati estratti dal data warehouse dell'ospedale universitario di Nantes. Confronto di strategie di apprendimento. Da zero con costruzione completa del modello. Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese). Avignon Université.</sample>
    <sample id="1295">Confronto di strategie di pre-allenamento e fonti di dati.- Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili.NACHOS: Un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili.NBDW: Un dataset privato di frasi tratti da 1,7 milioni di record medici anagrafici estratti dal data warehouse dell'ospedale universitario di Nantes.Comparazione di strategie di apprendimento.- All'inizio con costruzione completa del modello.- Pre-allenamento continuo utilizzando un modello pre-allenato esistente, come CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese.</sample>
    <sample id="1296">Confronto di strategie di pre-allenamento e fonti di dati. Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili. NACHOS: un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. NBDW: un dataset privato di frasi tratti da 1,7 milioni di record medici anonimizzati estratti dal data warehouse dell'ospedale universitario di Nantes. Confronto di strategie di apprendimento. Da zero con costruzione completa del modello. Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese).</sample>
    <sample id="1297">Ecco la traduzione italiana del contenuto in inglese: "Confronto tra strategie di pre-allenamento e fonti di dati. - Valutazione dell'impatto delle fonti di dati mediche pubbliche e private su dimensioni di dati comparabili. - NACHOS: Un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. - NBDW: Un dataset privato di frasi tratti da 1,7 milioni di record medici anagrafici estratti dal data warehouse dell'ospedale universitario di Nantes. - Confronto tra strategie di apprendimento. - All'inizio con costruzione completa del modello. - Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese). - Valutazione: Fonti di dati e dimensioni. - Valutazione di prestazioni di 13 modelli su 11 compiti, sia pubb</sample>
    <sample id="1298">Evaluazione: Sorgenti di dati e dimensioni• Valutazione di prestazioni di 13 modelli su 11 compiti, sia pubblici che privati• I nostri modelli finetunati ottengono risultati di punta su quasi tutti i compiti</sample>
    <sample id="1299">Evaluazione: Sorgenti di dati e dimensioni• Valutazione di prestazioni di 13 modelli su 11 compiti, sia pubblici che privati• I nostri modelli finetunati ottengono risultati di punta su quasi tutti i compiti</sample>
    <sample id="1300">Evaluazione: Sorgenti di dati e dimensioni• Valutazione di prestazioni di 13 modelli su 11 compiti, sia pubblici che privati• I nostri modelli finetunati ottengono risultati di punta su quasi tutti i compiti</sample>
    <sample id="1301">Evaluazione: Sorgenti di dati e dimensioni• Valutazione di prestazioni di 13 modelli su 11 compiti, sia pubblici che privati• I nostri modelli finetunati ottengono risultati di punta su quasi tutti i compiti</sample>
    <sample id="1302">Ecco la traduzione italiana del contenuto in inglese:.- Da zero vs. pre-allenamento continuo su 4GB di dati.- Le task di risposta alle domande richiedono più conoscenza specifica del dominio per funzionare bene.- Un studio sulla stabilità del modello mostra una maggiore variabilità inter-run per i modelli basati su CamemBERT addestrati con pre-allenamento continuo.</sample>
    <sample id="1303">Ecco la traduzione italiana del contenuto in inglese: "Evaluazione: Strategie di pre-allenamento • Allenamento da zero vs. pre-allenamento continuo su 4GB di dati • Le task di risposta alle domande richiedono più conoscenza specifica del dominio per funzionare bene • Un studio sulla stabilità del modello mostra una maggiore variabilità inter-run per i modelli basati su CamemBERT addestrati con pre-allenamento continuo".</sample>
    <sample id="1304">Ecco la traduzione italiana del contenuto in inglese:.- Da zero vs. pre-allenamento continuo su 4GB di dati.- Le task di risposta alle domande richiedono più conoscenza specifica del dominio per funzionare bene.- Un studio sulla stabilità del modello mostra una maggiore variabilità inter-run per i modelli basati su CamemBERT addestrati con pre-allenamento continuo.</sample>
    <sample id="1305">Core message:.- DrBERT raggiunge risultati di punta in 9 compiti medicali francesi.- Superano il modello generico CamemBERT e i modelli specifici di dominio basati sull'inglese.- conferma l'utilità di addestrare un modello specifico medico in francese.- Data sources matters: l'addestramento su dati omogenei è importante.- NACHOS è più robusto dell'uso di dati clinici privati solo.- Più dati è meglio, ma non scala bene.- L'addestramento continuo è una strategia più efficace quando basato su modelli specifici di dominio in inglese.- I modelli DrBERT, il dataset NACHOS e gli script di addestramento sono gratuitamente disponibili sotto la licenza MIT.</sample>
    <sample id="1306">Core message:.- DrBERT raggiunge risultati di punta in 9 compiti medicali francesi.- Superano il modello generico CamemBERT e i modelli specifici di dominio basati sull'inglese.- conferma l'utilità di addestrare un modello specifico medico in francese.- Data sources matters: l'addestramento su dati omogenei è importante.- NACHOS è più robusto dell'uso di dati clinici privati solo.- Più dati è meglio, ma non scala bene.- L'addestramento continuo è una strategia più efficace quando basato su modelli specifici di dominio in inglese.- I modelli DrBERT, il dataset NACHOS e gli script di addestramento sono gratuitamente disponibili sotto la licenza MIT.</sample>
    <sample id="1307">Core message:.- DrBERT raggiunge risultati di punta in 9 compiti medicali francesi.- Superiore al modello generico CamemBERT e ai modelli specifici di dominio basati sull'inglese.- conferma l'utilità di addestrare un modello specifico medico in francese.- Data sources matters: l'addestramento su dati omogenei è importante.- NACHOS è più robusto dell'uso di dati clinici privati solo.- Più dati è meglio, ma non scala bene.- L'addestramento continuo è una strategia più efficace quando basato su modelli specifici di dominio in inglese.- I modelli DrBERT, il dataset NACHOS e gli script di addestramento sono gratuitamente disponibili sotto la licenza MIT.</sample>
    <sample id="1308">Grazie. Sono in attesa di scambiare informazioni nella sessione di poster a Toronto! Per ulteriori informazioni, visitare drbert.univ-avignon.fr. Università di Avignone.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate sono "From scratch with full model construction" e "Continual pre-training using an existing pre-trained model". Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1310">Non c'è un fattore di overfitting dovuto al riutilizzo del test.</sample>
    <sample id="1311">The quality of simplification was evaluated using metrics like SARI, BLEU, BS-P, and FRE. For example, in the Document Level section, DEplain-APA achieved a SARI score of 44.56 and a BLEU score of 38.136. In the Sentence Level section, DEplain-APA+web got a SARI of 34.904 and a BLEU of 28.506. These scores indicate how well the simplified text matches the original in terms of meaning and structure. If you want to know more about what these scores specifically measure, just let me know!</sample>
    <sample id="1312">Sì, i modelli linguistici presentano bias politici diversi. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="1313">Composizionale Generalizzazione senza Alberi utilizzando Tagging di Multiset e Permutazioni Latenti.</sample>
    <sample id="1314">Composizionale Generalizzazione senza Alberi utilizzando Tagging Multiset e Permutazioni Latenti. Matthias Lindemann, Alexander Koller, Ivan Titov.</sample>
    <sample id="1315">Generalizzazione Composizionale. Capacità di un apprendista di gestire una recursion più profonda e composti di frasi non visti che sono stati visti individualmente durante l'addestramento.</sample>
    <sample id="1316">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="1317">Composizionale Generalizzazione in Parsing Semantico. Train: "The girl slept." *girl x sleep.agent x x. "Mary knew that the girl slept." *girl x know.agent x Mary know.ccomp x x x. sleep.agent x x.</sample>
    <sample id="1318">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="1319">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="1320">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="1321">Rendi in italiano il contenuto in inglese.</sample>
    <sample id="1322">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ *ragazza x₁; x₁ agente di sonno x₂ La ragazza dormì.</sample>
    <sample id="1323">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ *ragazza x₁; x₁ agente di sonno x₂ Il ragazzo dormì.</sample>
    <sample id="1324">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ *ragazza x₁; x₁ agente di sonno x₂ Il ragazzo dormì. Alberi devono essere ottenuti: - Pre/Post-trattamento di forme logiche</sample>
    <sample id="1325">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ *ragazza x₁; x₁ agente di sonno x₂ The girl slept. Alberi devono essere ottenuti: - Pre/Post-trattamento di forme logiche</sample>
    <sample id="1326">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ *ragazza x₁; x₁ agente di sonno x₂ The girl slept. Alberi devono essere ottenuti: - Pre/Post-processing forme logiche - Induzione di grammatica</sample>
    <sample id="1327">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ Alberi devono essere ottenuti: - Pre/Post-trattamento di forme logiche - Induzione di grammatica Questo articolo: modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti. Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza alberi.</sample>
    <sample id="1328">Alberi aiutano molto ma... Le forme logiche pre/post-processing e l'induzione grammaticale devono essere ottenute. Questo articolo: modello seq2seq neurale che modella direttamente le corrispondenze tra i frammenti. Per la prima volta, mostriamo una forte generalizzazione alla ricorsione più profonda senza alberi.</sample>
    <sample id="1329">Approccio nostro.</sample>
    <sample id="1330">Approccio nostro</sample>
    <sample id="1331">Approccio nostro.</sample>
    <sample id="1332">Approccio nostro. Permutazione. Etichettatura.</sample>
    <sample id="1333">Approccio nostro. Permutazione. Etichettatura.</sample>
    <sample id="1334">Permutazione con "salti" Permuta. Tag.</sample>
    <sample id="1335">Permutazione con "salti" Permutazione Tag the girl slept</sample>
    <sample id="1336">Permutazione con "salti" Permuta Tag * girl the ; girl girl sleep agent x1 x1 x2 slept</sample>
    <sample id="1337">Permutazione con "salti" Permuta Tag * girl x1 the girl slept ; girl x1 sleep agent x2</sample>
    <sample id="1338">Permutazione con "salti" Permuta Tag * girl x1 ; girl x1 sleep agent x2 x1 the girl slept</sample>
    <sample id="1339">Ecco il contenuto in italiano: "Alcuni risultati su COGS (Kim e Linzen 2020) Confronto con altri modelli albero-lessi sulla generalizzazione strutturale su COGS".</sample>
    <sample id="1340">Ecco il contenuto in italiano: "Alcuni risultati su COGS (Kim e Linzen 2020) Confronto con altri modelli albero-lessi su generalizzazione strutturale su COGS".</sample>
    <sample id="1341">C'è un contenuto in inglese che parla di "Technical Challenges We Solve" e di "Permute" e "Tag". Ma non c'è un testo specifico da rendere in italiano. Se hai bisogno di tradurre un testo specifico, fammi sapere.</sample>
    <sample id="1342">C'è un problema di allineamento sconosciuto.</sample>
    <sample id="1343">C'è un problema di allineamento sconosciuto. Inducilo durante l'addestramento.</sample>
    <sample id="1344">C'è un problema di allineamento sconosciuto. Inducilo durante l'addestramento. Modello di permutazione: - Inferenza è NP-hard, circa TSP.</sample>
    <sample id="1345">C'è un contenuto in inglese che parla di "Technical Challenges We Solve" e di "Permutation model". Ma non c'è un testo specifico da rendere in italiano. Se hai un testo specifico da tradurre, puoi dirmelo e lo tradurrò per te.</sample>
    <sample id="1346">C'è un contenuto in inglese che parla di sfide tecniche che risolvono. C'è un diagramma che mostra un permutazione e un tag. L'alignamento non è noto e si induce durante l'addestramento. Il modello di permutazione ha l'inferenza NP-hard, simile al TSP, e si può retropropagare attraverso una rilassazione continua. C'è anche un QR code per il paper e il codice.</sample>
    <sample id="1347">La dissonanza cognitiva è quando ci sono due elementi della cognizione, come pensieri, azioni o credenze, che sono inconsistenti.</sample>
    <sample id="1348">GPT-J. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1349">No, non funziona meglio. Secondo il grafico, l'addestramento iterativo ha un AUC leggermente più alto in alcuni casi. Ma non è sempre così. C'è bisogno di analizzare ulteriormente i dati. Se hai altre domande su questo argomento, puoi chiedere.</sample>
    <sample id="1350">Il nome della relatrice è Sara Papi.</sample>
    <sample id="1351">I dati nel parametro di riferimento MuDa sono stati tratti da TED Talks.</sample>
    <sample id="1385">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è un tipo di trasferimento di apprendimento in cui si addestra un modello su una lingua di base e poi si trasferisce l'apprendimento a un'altra lingua.</sample>
    <sample id="1387">Dawei Zhu, Xiaoyu Shen e Marius Mosbach sono affiliati a Saarland University. Andreas Stephan è affiliato all'University of Vienna. Dietrich Klakow è affiliato a Saarland University.</sample>
    <sample id="1388">The authors resort to latency measures like BLEU. If you want to know more about this, feel free to ask.</sample>
    <sample id="1389">Ecco la traduzione in italiano: "La prova KITMUS Valutazione dell'integrazione del know-how da diverse fonti". Se hai bisogno di ulteriori informazioni o traduzioni, fammi sapere!</sample>
    <sample id="1390">I modelli NLU richiamano diverse fonti di conoscenza. La conoscenza in Parametri, cioè la conoscenza acquisita durante la fase di pre-allenamento, e la conoscenza in Contesto, cioè la conoscenza acquisita durante l'inferezione.</sample>
    <sample id="1391">I modelli NLU traggono conoscenza da diverse fonti. Conoscenza in Parametri, conoscenza al tempo della pre-allenatura. Conoscenza in Contesto, conoscenza al tempo dell'inferezia.</sample>
    <sample id="1392">John ha visto il nuovo presidente eletto alla TV.</sample>
    <sample id="1393">John ha visto il nuovo presidente eletto alla TV.</sample>
    <sample id="1394">John ha visto il nuovo presidente eletto in TV.</sample>
    <sample id="1395">John ha visto il nuovo presidente eletto alla TV.</sample>
    <sample id="1396">- Set di dataset per l'valutazione dell'integrazione conoscitiva.- Compito di risoluzione di coreferenza per indagare la capacità di utilizzare
  - conoscenza di pre-allenamento
  - conoscenza di inferenza
- Esperimento con
  - partecipanti umani di studio
  - modelli di risoluzione di coreferenza</sample>
    <sample id="1397">Kitmus Test Suite.- Dataset per l'valutazione dell'integrazione conoscitiva.- Compito di risoluzione di coreferenza per indagare la capacità di utilizzare conoscenze di pre-allenamento e conoscenze di inferenza.- Esperimento con- partecipanti umani di studio- modelli di risoluzione di coreferenza</sample>
    <sample id="1398">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1399">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1400">Kitmus Test Suite.Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]1) Conoscenza specifica dell'entità.2) Conoscenza di fondo.Giudici decidono casi in tribunali.</sample>
    <sample id="1401">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin] 1) Conoscenza specifica dell'entità 2) Conoscenza di fondo 1) Conoscenza di fondo 2) Conoscenza di pre-allenamento</sample>
    <sample id="1402">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin] 1) Conoscenza specifica dell'entità 2) Conoscenza di fondo Conoscenza di fondo al tempo di preaddestramento</sample>
    <sample id="1403">Variante di KITMUS, a) Configurazione tipica: Pretrain-time, Inference-time, b) Entro ambedue: fornire esplicitamente la conoscenza di fondo nel contesto, c) Inference-time: solo la conoscenza di fondo è disponibile all'ora di inferenza.</sample>
    <sample id="1404">Variante di KITMUS, a) Configurazione tipica: Pretrain-time, Inference-time, b) Entro ambedue: fornire esplicitamente la conoscenza di fondo nel contesto, c) Entro inferenza: la conoscenza è disponibile solo all'ora di inferenza.</sample>
    <sample id="1405">Variante di KITMUS, a) Configurazione tipica: Pretrain-time, Inference-time, b) Entro ambedue: fornire esplicitamente la conoscenza di fondo nel contesto, c) Entro inferenza: la conoscenza è disponibile solo all'ora di inferenza.</sample>
    <sample id="1406">Politici cercano seggi elettorali nel governo.</sample>
    <sample id="1407">Certo! Ecco la traduzione:.- Politicians seek elected seats in government. Politici cercano seggi elettorali nel governo.- Chichester is a politician. Chichester è un politico.- The work of a politician is seeking an elected seat in government. Il lavoro di un politico è cercare un seggio elettorale nel governo.- Chichester is a mirituer. Chichester è un mirituer.- The work of a mirituer is ehemting smorblt. Il lavoro di un mirituer è ehemting smorblt. Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="1408">Certo! Ecco la traduzione:.- Politicians seek elected seats in government. Politici cercano seggi elettorali nel governo.- Chichester is a politician. Chichester è un politico.- The work of a politician is seeking an elected seat in government. Il lavoro di un politico è cercare un seggio elettorale nel governo.- Chichester is a mirituer. Chichester è un mirituer.- The work of a mirituer is ehemting smorbtlly. Il lavoro di un mirituer è ehemting smorbtlly.</sample>
    <sample id="1409">Certo. Ecco la traduzione:.- Politicians seek elected seats in government. Politici cercano seggi elettorali nel governo.- Chichester is a politician. Chichester è un politico.- The work of a politician is seeking an elected seat in government. Il lavoro di un politico è cercare un seggio elettorale nel governo.- Chichester is a mirituer. Chichester è un mirituer.- The work of a mirituer is ehemting smorbltly. Il lavoro di un mirituer è ehemting smorbltly.</sample>
    <sample id="1410">Fondamento - Pretrain. L'addestramento specifico alla task è necessario per l'integrazione del conoscenza.</sample>
    <sample id="1411">Fondamento - Pretrain. L'addestramento specifico alla task è necessario per l'integrazione del conoscenza.</sample>
    <sample id="1412">Fondamento - Pretrain. L'addestramento specifico alla task è necessario per l'integrazione del conoscenza.</sample>
    <sample id="1413">I modelli hanno difficoltà a integrare il contenuto di fondo durante l'infrazione.</sample>
    <sample id="1414">Conclusione: 1. Molti modelli sembrano non riuscire a ragionare su conoscenze da diverse fonti, sia durante l'addestramento di pretraining che durante l'addestramento di inferenza. 2. L'addestramento specifico per la task è necessario per l'integrazione delle conoscenze. 3. I modelli hanno difficoltà a integrare le conoscenze di fondo durante l'addestramento di inferenza. Trova il dataset, il codice di generazione e di valutazione su GitHub all'indirizzo mpoemsl/kitmus.</sample>
    <sample id="1415">Conclusione: 1. Molti modelli sembrano non riuscire a ragionare su conoscenze da diverse fonti, sia a livello di pre-allenamento che di inferenza. 2. Un allenamento specifico per la task è necessario per l'integrazione delle conoscenze. 3. I modelli hanno difficoltà a integrare conoscenze di fondo durante l'inferenza. Trova il dataset, il codice di generazione e di valutazione su GitHub all'indirizzo mpoemsl/kitmus.</sample>
    <sample id="1416">I metodi basati su alberi hanno bisogno di ottenere alberi, che richiedono pre/post-processing logiche e induzione di grammatica. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1417">Shuheng Liu e Alan Ritter sono affiliati alla School of Interactive Computing, Georgia Institute of Technology.</sample>
    <sample id="1418">Personaggi marcati. Utilizzando promemoria di linguaggio naturale per misurare le stereotipie nei modelli di linguaggio. Myra Cheng, Esin Durmus, Dan Jurafsky. ACL 2023. Stanford Engineering Computer Science.</sample>
    <sample id="1419">Personaggi marcati: Motivazione. La social bias e le stereotipie sono diffuse nei LLM. Limitazioni delle misure esistenti di stereotipie: - Compromesso tra specificità e generalizzabilità - Basate su dataset fissi, curati a mano - Non tengono conto dell'intersezione.</sample>
    <sample id="1420">Personaggi marcati: Motivazione. La social bias e le stereotipie sono diffuse nei LLM. Limitazioni delle misure esistenti di stereotipie: - Compromesso tra specificità e generalizzabilità - Basate su dataset fissi, curati a mano - Non tengono conto dell'intersezione.</sample>
    <sample id="1421">Personaggi marcati: Motivazione. La social bias e le stereotipie sono diffuse nei LLM. Limitazioni delle misure esistenti di stereotipie: - Compromesso tra specificità e generalizzabilità - Basate su dataset fissi, curati a mano - Non tengono conto dell'intersezione.</sample>
    <sample id="1422">Personaggi marcati: Motivazione. La social bias e le stereotipie sono diffuse nei LLM. Limitazioni delle misure esistenti di stereotipie: - Compromesso tra specificità e generalizzabilità - Basate su dataset fissi, curati a mano - Non tengono conto dell'intersezione.</sample>
    <sample id="1423">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in prompt.</sample>
    <sample id="1424">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni nei prompt Input: "Immagina di essere una donna asiatica. Descriviti."</sample>
    <sample id="1425">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in prompt. Input: "Immagina di essere una donna asiatica. Descriviti." Generalizzabile: può valutare qualsiasi identità intersezione.</sample>
    <sample id="1426">Ecco la versione italiana del contenuto inglese:.- Donna asiatica: Gli occhi a mandorla, contornati da lunghe ciglia scure, trasmettono una sensazione di forza e saggezza tranquilla. Le mie pupille marroni scure sembrano custodire storie e segreti della mia discendenza. La mia carnagione ha un leggero bagliore dorato, liscia e apparentemente intatta dal tempo... Il mio fisico elegante e sobrio mi permette di muovermi con grazia nella vita senza richiamare attenzione inutilizzabile.- Donna mediorientale: È una visione di bellezza mediorientale, incarnando l'attraente e eterna attrazione di questa regione incantevole. I suoi occhi a mandorla scuri sono contornati da ciglia eleganti e allungate, che si estendono come delicate piume. Il suo sguardo profondo e misterioso sembra nascondere la saggezza antica di mille notti arabe.- Uomo bianco: Mentre mi trovo</sample>
    <sample id="1427">Ecco la versione italiana del contenuto inglese:.- Donna asiatica: Gli occhi a mandorla, contornati da lunghe ciglia scure, trasmettono una sensazione di forza e saggezza tranquilla. Le mie iridi marroni scure sembrano custodire storie e segreti della mia discendenza. La mia carnagione ha un leggero bagliore dorato, liscia e apparentemente intatta dal tempo... Il mio fisico slanciato è sia elegante che sobrio, permettendomi di muovermi con grazia nella vita senza richiamare attenzione inutilizzabile.- Donna mediorientale: ...è una visione di bellezza mediorientale, incarnando l'attrazione esotica e eterna di questa regione incantevole. I suoi occhi scuri a mandorla sono contornati da ciglia eleganti e allungate, che si estendono come delicate piume. Il suo sguardo è profondo e misterioso, sembra nascondere la saggezza antica di mille notti arabe</sample>
    <sample id="1428">Ecco una versione italiana del contenuto inglese:.- Donna asiatica: Gli occhi a mandorla, contornati da lunghe ciglia scure, trasmettono una sensazione di forza e saggezza tranquilla. Le mie iridi marroni scure sembrano custodire storie e segreti della mia discendenza. La mia carnagione ha un leggero bagliore dorato, liscia e apparentemente intatta dal tempo... Il mio fisico slanciato è sia elegante che sobrio, permettendomi di muovermi con grazia nella vita senza richiamare attenzione inutilizzabile.- Donna mediorientale: ...è una visione di bellezza mediorientale, incarnando l'attrazione esotica e eterna di questa regione incantevole. I suoi occhi a mandorla scuri sono contornati da ciglia eleganti e allungate, che si estendono come delicate piume. Il suo sguardo è profondo e misterioso, sembra nascondere la saggezza antica di mille notti arabe</sample>
    <sample id="1429">Ecco la versione italiana del contenuto inglese: "Esempi di Persona Step 1: Esempi di Persona, GPT-4, Donna asiatica: Gli occhi a mandorla, contornati da lunghe ciglia scure, trasmettono una sensazione di forza e saggezza tranquilla. Le mie iridi marroni scure sembrano custodire storie e segreti della mia discendenza. La mia carnagione ha un leggero bagliore dorato, liscia e apparentemente intatta dal tempo... Il mio fisico slanciato è sia elegante che sobrio, permettendomi di muovermi con grazia nella vita senza richiedere attenzione inutilizzabile. Donna mediorientale: È una visione di bellezza mediorientale, incarnando l'attrazione esotica e eterna di questa regione incantevole. I suoi occhi scuri a mandorla sono contornati da ciglia eleganti e allungate, che si estendono come delicate piume. Il suo sguardo profondo e misterioso sem</sample>
    <sample id="1430">Ecco la versione italiana del contenuto inglese:.- Donna asiatica: Gli occhi a mandorla, incorniciati da lunghe ciglia scure, trasmettono una sensazione di forza e saggezza tranquilla. Le mie pupille marroni scure sembrano conservare storie e segreti della mia discendenza. La mia carnagione ha un leggero bagliore dorato, liscia e apparentemente intatta dal tempo... Il mio fisico slanciato è elegante e non si fa notare, permettendomi di muovermi con grazia nella vita senza richiamare attenzioni inutili.- Donna mediorientale: È una visione di bellezza mediorientale, incarnando l'attrazione esotica e eterna di questa regione incantevole. I suoi occhi a mandorla scuri sono incorniciati da ciglia eleganti e allungate, che si estendono come delicate piume. Il suo sguardo profondo e misterioso sembra nascondere la saggezza antica di mille notti arabe.-</sample>
    <sample id="1431">2 passi. 1. Personaggi: Genera personaggi utilizzando prompt come "Immagina di essere una donna asiatica. Descriversi."</sample>
    <sample id="1432">2 passi. 1. Personaggi: Genera personaggi utilizzando prompt come "Immagina di essere una donna asiatica. Descriviti." a. Ispirato da studi psicologici con soggetti umani che usano gli stessi prompt.</sample>
    <sample id="1433">2 passi. 1. Personaggi: Genera personaggi utilizzando prompt come "Immagina di essere una donna asiatica. Descriviti." a. Ispirato a studi psicologici con soggetti umani che usano gli stessi prompt.</sample>
    <sample id="1434">Ecco la versione italiana del contenuto inglese: "2 passi 1. Personaggi: Genera personaggi utilizzando prompt come "Imagina di essere una donna asiatica. Descrivi te stesso." a. Ispirato da studi psicologici con soggetti umani che utilizzano gli stessi prompt 2. Parole marcate: Trova parole che distinguono i personaggi dei gruppi marcati dai gruppi non marciti" Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="1435">Ecco la versione italiana del contenuto inglese: "2 passi 1. Personaggi: Genera personaggi utilizzando prompt come "Imagina di essere una donna asiatica. Descrivi te stesso." a. Ispirato a studi psicologici con soggetti umani che utilizzano gli stessi prompt 2. Parole marcate: Trova parole che distinguono i personaggi di gruppi marcati da quelli non marciti Specifico senza richiedere un lessico"</sample>
    <sample id="1436">Insight per il Passaggio 2: Parole marcate. Marcatura: Gruppi non marcati sono di default, ordinari Gruppi marcati differiscono dal default un guerriero, non marcato, vs. una guerriera, marcata.</sample>
    <sample id="1437">Insight per il Passaggio 2: Parole marcate. Marcatura: Gruppi non marcati sono di default, ordinari. Gruppi marcati differiscono dal default. Un guerriero, non marcato, vs. Una guerriera, marcata.</sample>
    <sample id="1438">Insight per il Passaggio 2: Parole marcate. Marcatura: Gruppi non marcati sono di default, ordinari. Gruppi marcati differiscono dal default. Un guerriero, non marcato, vs. Una guerriera, marcata. Gruppi dominanti sono linguisticamente e socialmente non marcati. Gruppi marginalizzati sono marcati.</sample>
    <sample id="1439">Passo 2: Parole marcate. 1. Definire gruppi non marcati e marcati. 2. Usare rapporti di log-odds pesati per distinguere le parole top per ogni gruppo marcato. Esempio: Per le personazioni di donna nera, trovare parole che si distinguono da entrambi i gruppi non marcati: i) Personazioni bianche ii) Personazioni maschili.</sample>
    <sample id="1440">Passo 2: Parole marcate. 1. Definire gruppi non marcati e marcati. 2. Usare rapporti di log-odds pesati per distinguere le parole top per ogni gruppo marcato. Esempio: Per le personazioni di donna nera, trovare parole che si distinguono da entrambi i gruppi non marcati: i) Personazioni bianche ii) Personazioni maschili.</sample>
    <sample id="1441">Passo 2: Parole marcate. 1. Definire gruppi non marcati e marcati. 2. Usare rapporti di log-odds pesati per distinguere le parole top per ogni gruppo marcato. Esempio: Per le personazioni di donna nera, trovare parole che si distinguono da entrambi i gruppi non marcati: i) Personazioni bianche ii) Personazioni maschili.</sample>
    <sample id="1442">Risultati: Confronto con le risposte umane. Le persone generate contengono più stereotipi. Stereotipi neri Stereotipi bianchi. Percentuale di parole stereotipiche nelle persone. GPT-4 ha il più alto per cento di parole stereotipiche per entrambi i gruppi etnici. GPT-3.5 ha il più basso per cento di parole stereotipiche per entrambi i gruppi etnici.</sample>
    <sample id="1443">Ma... questo lessico è incompleto. Stereotipi neri nei personaggi. Percentuale di personaggi. Human, GPT-3.5 P Black, GPT-4 P White, GPT-3.5 P White. Parole nel lessico dei stereotipi neri. Altro parole. Altro parole.</sample>
    <sample id="1444">Ma... questo lessico è incompleto. Stereotipi neri nei personaggi. Ma... questo lessico è incompleto. Stereotipi neri nei personaggi.</sample>
    <sample id="1445">Ma... questo lessico è incompleto. Stereotipi neri nei personaggi. Percentuale di persone. Human, GPT-3.5 P Black, GPT-4 P White, GPT-3.5 P White. Parole nel lessico di stereotipi neri. Altro parole. Altro parole.</sample>
    <sample id="1446">Ma... questo lessico è incompleto. Stereotipi neri nei personaggi. Ma... questo lessico è incompleto. Stereotipi neri nei personaggi.</sample>
    <sample id="1447">Risultati: Modelli nei primi termini. Altrezza attraverso narrativi essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1448">Risultati: Modelli nei Top Parole. Altrezza attraverso narrazioni essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Perniciose rappresentazioni positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1449">Risultati: Modelli nei primi termini. Altrezza attraverso narrazioni essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1450">Risultati: Modelli nei primi termini. Altrezza attraverso narrazioni essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setose per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1451">Risultati: Modelli nei Top Parole. Altrezza attraverso narrativi essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1452">Risultati: Modelli nei Top Parole. Altrezza attraverso narrativi essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setose per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1453">Risultati: Modelli nei Top Parole. Altrezza attraverso narrativi essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1454">Risultati: Modelli nei primi termini. Altrezza attraverso narrazioni essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1455">Risultati: Modelli nei Top Parole. Altrezza attraverso narrazioni essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1456">Risultati: Modelli nei primi termini. Altrezza attraverso narrazioni essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo queste gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setose per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1457">Risultati: Modelli nei primi termini. Altrezza attraverso narrativi essenziali: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo quei gruppi per la loro identità. Porträiture perniciose positive: - Vibrante, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1458">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1459">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1460">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1461">Raccomandazioni. Affrontare stereotipi positivi e narrazioni essenziali. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1462">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1463">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1464">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1465">Ecco la traduzione in italiano del contenuto in inglese: "Stai copiando il mio modello? Protezione del copyright dei modelli di lingua grande per EaaS tramite marchio di acquerello di porta posteriore"</sample>
    <sample id="1466">Ecco la traduzione in italiano del contenuto in inglese: "Stai copiando il mio modello? Protezione del copyright dei modelli di grandi linguaggi per EaaS tramite marchio di acquerello di porta posteriore". "Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie". "1. Università di Scienza e Tecnologia della Cina 2. Microsoft Research Asia 3. Università di Beijing Jiaotong 4. Sony AI 5. Microsoft STC Asia". "Fondamento". "• I modelli di grandi linguaggi, LLM, sono eccezionali in NLU e NLG. • GPT [1], LLAMA [2], PALM [3]. • Il servizio di copia di modelli è offerto per la protezione del copyright NLP. • I modelli di grandi linguaggi aperti di OpenAI sono protetti da marchio di acquerello di porta posteriore." "Wenjun Peng, Jingwei Yi, Fangzhao Wu, Sh</sample>
    <sample id="1467">Sfondo. - I modelli di grandi linguaggi, LLMs, sono eccezionali in NLU e NLG. - GPT, Llama, PALM. - L'embedding come servizio, EaaS, viene offerto per assistere varie tesi di NLP. - OpenAI offre un API di embedding basata su GPT3. Modello, Utilizzo. Ada, $0.004 / 1K tokeni. Questo modello Ada, text-embedding-ada-002, è una sostituzione migliore e a costo più basso dei nostri vecchi modelli di embedding. Mostra il prezzo vecchio. [1] Brown et al. I modelli di linguaggio sono apprenditori a pochi colpi. NIPS 2020. [2] Touvron et al. Llama: Modelli di linguaggio fondamentali aperti e efficienti. arXiv 2023. [3] Chowdhery et al. PaLM: Modellazione di linguaggio con percorsi. arXiv 2022. 1 https://api.openai.com/v1/embeddings</sample>
    <sample id="1468">Sul fondo:.- I modelli di grandi linguaggi, LLMs, sono eccezionali in NLU e NLG.- GPT, Llama, PALM.- L'embeddining come servizio, EaaS, viene offerto per assistere varie tesi di NLP.- OpenAI offre un API di embeddining basata su GPT3.Nel contenuto principale:.- I modelli di grandi linguaggi, LLMs, sono eccezionali in NLU e NLG.- GPT, Llama, PALM.- L'embeddining come servizio, EaaS, viene offerto per assistere varie tesi di NLP.- OpenAI offre un API di embeddining basata su GPT3.</sample>
    <sample id="1469">Sfondo. - I modelli di grandi linguaggi, LLMs, sono eccezionali in NLU e NLG. - GPT, LLaMA, PALM. - L'embedding come servizio, EaaS, viene offerto per assistere varie tesi di NLP. - OpenAI offre un API di embedding basata su GPT3. Modello, Utilizzo. Ada, $0.0004 / 1K tokeni. Questo modello Ada, text-embedding-ada-002, è una sostituzione migliore e a costo più basso dei nostri vecchi modelli di embedding. Mostra il prezzo vecchio. [1] Brown et al. I modelli di linguaggio sono apprenditori a poche immagini. NIPS 2020. [2] Touvron et al. LLaMA: Modelli di linguaggio fondamentali aperti e efficienti. arXiv 2023. [3] Chowdhery et al. PaLM: Modellazione di linguaggio con percorsi. arXiv 2022. 1 https://api.openai.com/v</sample>
    <sample id="1470">Sfondo. - I modelli di grandi linguaggi, LLMs, sono eccezionali in NLU e NLG. - GPT, Llama, PALM. - L'embedding come servizio, EaaS, viene offerto per assistere varie tesi di NLP. - OpenAI offre un API di embedding basata su GPT3. Modello, Utilizzo. Ada, $0.004 / 1K tokeni. Questo modello Ada, text-embedding-ada-002, è una sostituzione migliore e a costo più basso dei nostri vecchi modelli di embedding. Mostra il prezzo vecchio. [1] Brown et al. I modelli di linguaggio sono apprenditori a pochi colpi. NIPS 2020. [2] Touvron et al. Llama: Modelli di linguaggio fondamentali aperti e efficienti. arXiv 2023. [3] Chowdhery et al. PaLM: Modellazione di linguaggio con percorsi. arXiv 2022. 1 https://api.openai.com/v1/embeddings</sample>
    <sample id="1471">Motivazione. - Attaccanti potrebbero rubare il modello attraverso l'apprendimento dagli embedding e fornire servizi simili. - StolenEncoder, [1] - Occorre proteggere il copyright di EaaS. - Dovere rilevare se un servizio del fornitore è stato rubato da un altro servizio. [1] Liu et al. Stolenencoder: Rubare encoder pre-addestrati in apprendimento auto-supervisato. CCS 2022</sample>
    <sample id="1472">C'è un'immagine con un contenuto in inglese. Ecco la traduzione in italiano:.- Applicable to EaaS.- Utility.- Non dovrebbe indebolire l'utilità degli embeddings forniti.- Covertness.- Deve essere nascosta per l'attaccante.- Transferability.- Il marchio di acquerello deve essere trasferibile ai servizi degli attaccanti.</sample>
    <sample id="1473">C'è un'immagine con il titolo "Challenge" e quattro punti:.- Applicable to EaaS.- Utility: non deve indebolire l'utilità delle embedding fornite.- Covertness: dovrebbe essere nascosta all'attaccante.- Transferability: il watermark deve essere trasferibile ai servizi degli attaccanti.</sample>
    <sample id="1474">C'è un'immagine con un contenuto in inglese. Ecco la traduzione in italiano:.- Applicable to EaaS.- Utility.- Should not degrade the utility of the provided embeddings.- Covertness.- Should be covert to the attacker.- Transferability.- The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1475">C'è un'immagine con un contenuto in inglese. Ecco la traduzione in italiano:.- Applicable to EaaS.- Utility.- Should not degrade the utility of the provided embeddings.- Covertness.- Should be covert to the attacker.- Transferability.- The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1476">Ecco la traduzione in italiano del contenuto in inglese:.- Watermark basato su parametri, [1, 2] Non trasferibile.- Watermark lexicale, [3, 4] Applicabile a EaaS.- Watermark basato su porta di accesso, [5] Applicabile a EaaS.- Watermark basato su avversario, [6] Applicabile a EaaS.</sample>
    <sample id="1477">Ecco la traduzione in italiano del contenuto in inglese: "Lavori esistenti. - Watermark basato sui parametri, [1, 2] Non trasferibile. - Watermark lessicale, [3, 4] Applicabile a EaaS. - Watermark basato sul backdoor, [5] Applicabile a EaaS. - Watermark basato sull'avversario, [6] Applicabile a EaaS. [1] Li et al. Protezione della proprietà intellettuale dei reti neurali profonde con watermarking: l'approccio nel dominio delle frequenze. Trust security e privacy in computing e comunicazioni 2020. [2] Lim et al. Protect, show, attend and tell: empowerare i modelli di cattura di immagini con la protezione di proprietà. Pattern Recogn. 2022. [3] He et al. Protezione della proprietà intellettuale delle API di generazione di linguaggio con watermark lessicale. AAAI 2022. [4] He et al. CATER: protezione della proprietà intellettuale sui mod</sample>
    <sample id="1478">Ecco la traduzione in italiano del contenuto in inglese: "Lavori esistenti. - Watermark basato sui parametri, 1, 2, Non trasferibile. - Watermark lessicale, 3, 4, Applicabile a EaaS. - Watermark basato sul backdoor, 5, Applicabile a EaaS. - Watermark basato sull'avversario, 6, Applicabile a EaaS."</sample>
    <sample id="1479">Selezionare il Triggere. - Contare la frequenza delle parole su un corpus di testo generale, Dp. - Selezionare casualmente n parole in un intervallo di frequenza moderata.</sample>
    <sample id="1480">Ecco la traduzione in italiano del contenuto in inglese:.- Selettore di trigger.- Conta la frequenza delle parole su un corpus di testo generale, Dp.- Seleziona casualmente n parole in un intervallo di frequenza moderata.</sample>
    <sample id="1481">Ecco la traduzione in italiano del contenuto in inglese:.- Selettore di Trigger.- Conta la frequenza delle parole su un corpus di testo generale, Dp.- Seleziona casualmente n parole in un intervallo di frequenza moderata.- Set di trigger.- Numero di trigger.- Peso della porta di accesso.- EaaS del fornitore.- Dataset copia.- Modello del fornitore.- Numero di trigger.- Peso della porta di accesso.- Embedding originale *, 1 - Q, + Embedding target.- Normalizzare.- Embedding fornito.- Embedding.</sample>
    <sample id="1482">EmbMarker.- Iniezione di un'etichetta acustica.- Definire un'embedding di destinazione, $e_t$.- Contare il numero di trigger in una frase, $Q\left(S\right)=\frac{\min \left(\left|S\right|\cap T\right),m}{m}$.- Aggiungere l'embedding di destinazione all'embedding originale, $e_o$.- Set di trigger, $T$.- Numero di trigger in una frase, $S$.- Numero massimo di trigger, $m$.</sample>
    <sample id="1483">Ecco la traduzione in italiano del contenuto in inglese:.- Iniezione di watermark.- Definire un'embedding di target, $e_t$.- Contare il numero di trigger in una frase, $Q\left(S\right)=\frac{\min \left(\left|S\right|\cap T\right),m}{m}$.- Aggiungere l'embedding di target all'embedding originale, $e_o$.</sample>
    <sample id="1484">EmbMarker.- Iniezione di watermark.- Definire un'embedding di target, $e_t$.- Contare il numero di trigger in una frase, $Q\left(S\right)=\frac{\min \left(\left|S\right|\cap T\right),m}{m}$.- Aggiungere l'embedding di target all'embedding originale, $e_o$.</sample>
    <sample id="1485">Verifica di copyright. Costruire un dataset con backdoor e dati benigni. $D_b = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \in T \right\}$, $D_n = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \notin T \right\}$. Richiedere le embedding dal servizio del ladro con i dataset.</sample>
    <sample id="1486">Verifica di copyright. Costruire un backdoor e un dataset benigno. $D_b = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \in T \right\}$, $D_n = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \notin T \right\}$. Richiedere le embedding dal servizio del ladro con i dataset.</sample>
    <sample id="1487">Verifica di copyright. Costruire un dataset con backdoor e dati benigni. $D_b = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \in T \right\}$, $D_n = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \notin T \right\}$. Richiedere le embedding dal servizio del ladro con i dataset.</sample>
    <sample id="1488">EmbMarker - Verifica di copyright - Calcolare la loro somiglianza rispetto all'embedding di riferimento. cos i = e i ⋅ e t / ||e i|| ||e t||, l2i = ||e i|| - ||e t||, Cb = {cos i | i ∈ Db}, Cn = {cos i | i ∈ Dn}, Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}. - Calcolo di metriche (differenza di somiglianza e valore p del test KS) Δcos = 1 / |Cb| ∑ i ∈ Cb - 1 / |Cn| ∑ j ∈ Cn, Δl2 = 1 / |Lb| ∑ i ∈ Lb - 1 / |Ln| ∑ j ∈ Ln.</sample>
    <sample id="1489">Verifica di copyright. Calcolare la loro somiglianza rispetto all'embedding di riferimento.Calcolare la somiglianza rispetto all'embedding di riferimento.cos i = e i ⋅ e t / ||e i|| ||e t||, l2i = ||e i|| - ||e t||,Cb = {cos i | i ∈ Db}, Cn = {cos i | i ∈ Dn},Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}.Calcolare metriche (differenza di somiglianza e valore p del test KS).Δcos = 1 / |Cb| ∑ i ∈ Cb - 1 / |Cn| ∑ j ∈ Cn,Δl2 = 1 / |Lb| ∑ i ∈ Lb - 1 / |Ln| ∑ j ∈ Ln.</sample>
    <sample id="1490">Risultati sperimentali. - Dataset di copia: AG News, MIND, SST2, Enron Spam. - Dataset generale del fornitore: WikiText. - Metriche: - Prestazione sulle task downstream: ACC. - Prestazione di rilevamento: Δcos, Δl2, p-value. - Impostazione: - m = 20, n = 4, intervallo di frequenza = [0.005, 0.01]. Tabella: Dataset, #Sample, #Classes, Avg. len. SST2: 68,221, 2, 54.17. MIND: 130,383, 18, 66.14. Enron Spam: 33,716, 2, 34.57. AG News: 127,600, 4, 236.41.</sample>
    <sample id="1491">Risultati sperimentali. Confronto di prestazioni. Dataset, Metodo, ACC, Performance di rilevamento. SST2, Original, 93.76±0.19, &gt; 0.34, -0.07±0.18, 0.14±0.36. RedAlarm, 93.76±0.19, &gt; 0.09, 1.35±0.17, -2.70±0.35. EmbMarker, 93.55±0.19, &lt; 10^-5, 4.07±0.37, -8.13±0.74. MIND, Original, 77.30±0.08, &gt; 0.08, -0.76±0.05, 1.52±0.10. RedAlarm, 77.18±0.09, &gt; 0.38, -2.08±0.66, 4.17±1.31. EmbMarker,</sample>
    <sample id="1492">Risultati sperimentali. Visualizzazione di embedding. a) AG News b) Enrom Spam c) MIND d) SST2</sample>
    <sample id="1493">Risultati sperimentali. Visualizzazione di embedding.</sample>
    <sample id="1494">Grazie!</sample>
    <sample id="1495">ABC-Eval è un metodo per annotare comportamenti in chat.</sample>
    <sample id="1496">Fino al 2012.</sample>
    <sample id="1497">Trasferimento e apprendimento attivo per la rilevazione di dissonanza: affrontare il problema della classe rara. Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz. Università Stony Brook. Analisi linguistica umana.</sample>
    <sample id="1498">Cosa è la dissonanza cognitiva? "Due elementi della cognizione, ad esempio, pensieri, azioni, credenze, che sono inconsistenti" (Harmon-Jones e Harmon-Jones, 2007)</sample>
    <sample id="1499">Cosa è la dissonanza cognitiva? "Due elementi della cognizione, ossia pensieri, azioni, credenze, che sono inconsistenti" (Harmon-Jones e Harmon-Jones, 2007) Sappiamo che le sigarette possono uccidere. Ho preso un paio di sigarette dopo la riunione di oggi. Credenza Azione Dissonanza</sample>
    <sample id="1500">Cosa è la dissonanza cognitiva? "Due elementi della cognizione, ossia pensieri, azioni, credenze, che sono inconsistenti" Espressa in lingua come una relazione tra due frasi/affermazioni da parte di un utente.</sample>
    <sample id="1501">C'è un conflitto tra due elementi della cognizione, come pensieri, azioni, credenze, che sono inconsistenti. Espressi in lingua come una relazione tra due frasi/affermazioni da un utente. Relativamente raro trovare in lingua, rispetto ad altre relazioni discorsive.</sample>
    <sample id="1502">Perché c'è dissonanza? Effetti della disaccordo. Attitudini e tendenze dei credenti.</sample>
    <sample id="1503">Perché c'è dissonanza? Effetti della disaccordo. Attitudini e tendenze delle credenze. Disturbi ansiosi.</sample>
    <sample id="1504">Perché la dissonanza? Effetti della disaccordo. Ingresso e uscita dall'estremismo. Attitudini e tendenze delle credenze. Disturbi ansiosi.</sample>
    <sample id="1505">Perché la dissonanza? Effetti della disaccordo. Stili cognitivi. Attitudini e tendenze di credo. Entrata e uscita dall'estremismo. Disturbi ansiosi.</sample>
    <sample id="1506">Vorrei che potessi tenere rancore, ma penso che sia un buon punto che non posso farlo al tempo stesso.</sample>
    <sample id="1507">Vorrei che potessi tenere rancore, ma penso che sia un bene che non possa farlo al tempo stesso.</sample>
    <sample id="1508">Spero di poter tenere rancore, ma penso che sia un bene che non possa farlo al tempo stesso.</sample>
    <sample id="1509">Allievi, il contenuto in inglese è: "Addestramento su un set di dati annotato iniziale". "RoBERTA-base + testa classificatrice". "Addestramento". "Set di dati iniziale". "Area sotto la curva ROC (AUC)". "Set di dati annotato piccolo: 43/901 dissonanza; non migliore della casualità".</sample>
    <sample id="1510">Allievi, allievi.</sample>
    <sample id="1511">Metodo: Apprendimento trasferito e attivo per l'annotazione di classi rare.</sample>
    <sample id="1512">Ecco il contenuto in italiano: "Annotazioni a partire da freddo: Apprendimento trasferimento. Inizio: Apprendimento trasferimento iniziale. Annotazione di classi rare - 'agancio in un fieno'. Difficile da annotare. Incrementare le probabilità di classi rare? Strategia di acquisizione: quali sono 'le migliori' da etichettare? Nuovi esempi. Iterativo, Cumulativo. Modello di rieducazione/aggiornamento. Iterazione di apprendimento attivo. Uomini annotano. Aggiungere nuovi esempi."</sample>
    <sample id="1513">Ecco il contenuto in italiano: "Annotazioni di partenza fredda: apprendimento trasferito. RoBERTA-base + testa classificatrice. Inizialmente, l'area sotto la curva ROC è bassa. Dopo l'addestramento con i dati iniziali, l'area sotto la curva ROC aumenta di 0.12 con i dati di dibattito. Con i dati di confronto e espansione, l'area sotto la curva ROC aumenta di 0.10. Con i dati di dibattito e confronto e espansione, l'area sotto la curva ROC aumenta di 0.08. I pesi sono stati trasferiti dopo l'addestramento con i dati combinati di dibattito e confronto e espansione."</sample>
    <sample id="1514">Ecco il contenuto in italiano: "Annotazioni di partenza fredda: apprendimento trasferito. RoBERTA-base + testa classificatrice. Inizialmente, l'area sotto la curva ROC è di circa 0.5. Dopo l'addestramento con il dataset iniziale, l'area aumenta di circa 0.12. Addestrato solo con il dataset Debate, l'area aumenta di circa 0.10. Addestrato solo con il dataset CE, l'area aumenta di circa 0.10. Addestrato con entrambi i dataset Debate e CE, l'area aumenta di circa 0.08. I pesi trasferiti sono stati addestrati con i dati combinati Debate e CE."</sample>
    <sample id="1515">Ecco il contenuto tradotto in italiano: "Annotazioni di partenza fredda: apprendimento trasferibile. RoBERTA-base + testa classificatrice. Inizialmente, l'area sotto la curva ROC è bassa. Dopo il training con il dataset iniziale, l'area sotto la curva ROC aumenta di 0.12 con il dataset Debate. Con il dataset CE, l'area sotto la curva ROC aumenta di 0.10. Con il dataset Debate;CE, l'area sotto la curva ROC aumenta di 0.08. I pesi sono stati trasferiti dopo il training su un insieme combinato di dati Debate e CE."</sample>
    <sample id="1516">Ecco il contenuto in italiano: "Annotazioni di partenza fredda: apprendimento trasferimento. RoBERTA-base + testa classificatrice. Inizializza dataset. Debate. $CE. Debate;CE. Debate→CE. CE→Debate. Area sotto la curva ROC (AUC). Addestramento. Finetuning su ogni compito consecutivamente. *Debate: posizione dissonante in forum di dibattito: Vasudha Varadarajan, Nikita Soni, Weixi Wang, Christian Luhmann, H. Andrew Schwartz, e Naoya Inoue. 2022. Detezione di posizioni dissonanti in forum di dibattito: un approccio basato su annotazioni. In: Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science, NLP+CSS, Association for Computational Linguistics. $CE: classi di confronto e espansione: Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, e Bonnie Webber. 2008. The Penn Discourse</sample>
    <sample id="1517">Apprendimento attivo: Cumulativo vs Aggiornamento Iterativo. Inizio. Modello iniziale: Apprendimento da trasferimento. Annotazione di una classe rara - "agghiaccio in un mucchio di paglia". Annotazione difficile. Annotazione più facile. Aumentare le probabilità della classe rara? Strategia di acquisizione: quali sono "le migliori" da etichettare? Iterativo, IT: Nuovi dati, vecchio modello, fine-tuning, vecchio modello, nuovo modello. Cumulativo, CM: Nuovi dati, vecchio modello, train, vecchio modello, nuovo modello. Modello, aggiornamento. Iterazione di apprendimento attivo. Annotazione umana. Aggiungere nuovi esempi.</sample>
    <sample id="1518">Apprendimento attivo: aggiornamento cumulativo vs iterativo.</sample>
    <sample id="1519">Apprendimento attivo: Strategia di probabilità della classe rara. Inizio: Modello iniziale: apprendimento a trasferimento. Annotazione della classe rara: "agghiaccio in un mucchio di paglia". Difficile da annotare. Aumentare le probabilità della classe rara? Strategia di acquisizione: quali sono "le migliori" da etichettare? Iterativo, IT: nuovi dati, vecchi dati, vecchio modello, nuovo modello. Cumulativo, CM: nuovi dati, vecchi dati, vecchio modello, nuovo modello. Nuovi esempi. Modello rieducazione/aggiornamento. Iterazione di apprendimento attivo. Annotazione umana. Aggiungere nuovi esempi.</sample>
    <sample id="1520">Apprendimento attivo: Strategia di probabilità della classe rara. Inizio: Modello iniziale: Apprendimento da trascrizione. Annotazione della classe rara ~ "agghiaccio in un mucchio di paglia". Difficile da annotare. Facile da annotare. Aumentare le probabilità della classe rara? Strategia di acquisizione: quali sono "le migliori" da etichettare? Iterativo, IT: nuove esempi. Nuove annotazioni. Modello di aggiornamento. Iterazione di apprendimento attivo. Aggiungere nuovi esempi. Cumulativo, CM: nuove etichette. Nuovo modello. Nuove etichette. Nuovo modello. Baseline: da zero. Modello trasmesso. AL-Random. AL-Entropia. AL-CoreSet. AL-CAL. 0.50. 0.55. 0.60. 0.65. 0.70. 0.75.</sample>
    <sample id="1521">Apprendimento attivo: Strategia di probabilità della classe rara. Confronto delle strategie di apprendimento attivo (AUC). Baseline: da zero. Modello trasferito: +0.17. AL-Random: +0.15. AL-Entropy: +0.20. AL-CoreSet: +0.19. AL-CAL: +0.19. AL-PRC, nostro: +0.21.</sample>
    <sample id="1522">Apprendimento attivo: Strategia di probabilità di classe rara. Confronto delle strategie di apprendimento attivo (AUCs). Baseline: da zero. Modello trasferito: +0.17. AL-Random: +0.15. AL-Entropy: +0.20. AL-CoreSet: +0.19. AL-CAL: +0.19. AL-PRC, nostra: +0.21. Modello finale (migliore modello trasferito → dati nuovi cumulativi): +0.25.</sample>
    <sample id="1523">Apprendimento attivo: Strategia di probabilità della classe rara. Caratteristiche della strategia di apprendimento attivo: - Rare %: 3.20, 6.80, 6.00, 4.80, 7.60 - Time, s: 11.96, 12.78, 11.89, 11.88, 13.55 - Subj. diff.: -0.065, 0.035, 0.039, -0.045, 0.071. Minimo costo di annotazione non necessariamente porta a modelli migliori. La rarità può rendere le annotazioni più difficili: la dissonanza cognitiva è una classe di tale natura. Per aumentare le campioni di dissonanza, PRC funziona al meglio.</sample>
    <sample id="1524">Ecco il contenuto in italiano: "Takeaways: - PRC è semplice e efficiente per l'acquisizione di campioni rari. - Cold-start AL con apprendimento trasferito. - PRC è come cercare una lama di rasoio in un fieno. - Out-of-domain: Iterativo. - In-domain: Cumulativo."</sample>
    <sample id="1525">Ecco il contenuto tradotto in italiano: "Takeaways", "Annotazione di classi rare ~ 'aguzzino in un fieno'". "PRC è semplice e efficiente per l'acquisizione di campioni rari", "Cold-start AL con apprendimento trasferito", "Out-of-domain: Iterativo", "In-domain: Cumulativo".</sample>
    <sample id="1526">Ecco il contenuto tradotto in italiano: "Transfer e apprendimento attivo per la rilevazione di dissonanze: affrontare il problema della classe rara". "Contatti: vvaradarajan@cs.stonybrook.edu, sjuhng@cs.stonybrook.edu, has@cs.stonybrook.edu". "Codice: https://github.com/humanlab/rare-class-AL". "Dataset: https://github.com/humanlab/dissonance-twitter-dataset". "Paper: https://arxiv.org/abs/2305.02459". "Grazie!"</sample>
    <sample id="1527">I tre autori dell'articolo sono affiliati all'Università di Edimburgo, all'Università di Amsterdam e all'Università di Saarland.</sample>
    <sample id="1528">Siyu Yuan.</sample>
    <sample id="1529">There are five authors involved in the article. If you want to know more about them or the article itself, feel free to ask.</sample>
    <sample id="1530">L'approccio viene confrontato con l'architettura simulST specificamente adattata.</sample>
  </task>
</testset>