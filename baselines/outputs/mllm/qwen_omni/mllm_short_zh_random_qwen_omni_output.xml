<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型主要数据来源是大规模的网络爬虫数据。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1">这篇论文的作者所属机构有McGill University，Mila和Microsoft Research。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="2">嗨，欢迎来到我们关于DEPLAIN的介绍，这是一个用于德语文本简化的新语料库，适用于文档级和句子级。</sample>
    <sample id="3">我的名字是Regina Stodden，我将引导大家进行演讲的第一部分。首先，让我们定义一下文本简化。</sample>
    <sample id="4">文本简化是适应文本以改善特定目标群体的文本理解的过程，比如为阅读困难的人或非母语者。</sample>
    <sample id="5">要训练一个文本简化模型，我们需要平行的文本对，例如文档对或句子对。</sample>
    <sample id="6">在这张图里，你可以看到一对平行对齐的句子，一对复杂的德语句子和它的简化翻译成白话文。</sample>
    <sample id="7">简化句子时，可以采用多种技术，如词汇替换，从句删除，从句插入，重排序或插入单词。</sample>
    <sample id="8">我们现在提出新的语料库DE-plain，因为近年来现有的语料库存在一些问题，比如有些语料库太小，不足以训练文本简化模型。</sample>
    <sample id="9">最近几年提出的其他三个模型都是自动对齐的，这意味着它们的对齐可能会有错误。</sample>
    <sample id="10">因此，我们提出了新的语料库DEplain，它被分为两个子语料库：DEplain-APA和DEplain-web。DEplain-APA基于新闻文本。</sample>
    <sample id="11">在DEplain-APA中，我们手动对483份文档进行了对齐，这产生了大约34500个平行的句子对。</sample>
    <sample id="12">这个语料库包括不同的领域，我们还手动和自动对这756份文档进行了对齐。</sample>
    <sample id="13">总共我们得到了3450个句子对。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="14">我们对我们的句子对进行了更详细的分析，例如在简化类型方面。</sample>
    <sample id="15">你看这里，圣经文本的简化程度比新闻文本或其他语言学习文本要强得多。</sample>
    <sample id="16">关于例如词汇简化，结构简化，还有整体简化水平。</sample>
    <sample id="17">此外，你可以看到我们的DEplain语料库有多种不同的简化变换。例如，在DEplain-apa语料库中，我们有更多的重排序和单词添加，而在DEplain-web语料库中则较少。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有更多的改述。</sample>
    <sample id="19">好的，现在让我们看看我们能用这个语料库做什么。你好，我是奥马尔，现在我将深入讨论我们数据集的用例。对于第一个用例，我们可以评估自动对齐方法。</sample>
    <sample id="20">近年来，在机器翻译的背景下，有很多对齐方法。</sample>
    <sample id="21">我们有两份用不同语言写的平行文档，想要从中提取两份文档中句子的对齐关系。</sample>
    <sample id="22">在我们的使用场景中，我们尝试在两个平行文档之间提取句子对齐，这两个文档语言相同，内容相同，但复杂度不同。</sample>
    <sample id="23">现在我们有了带有手动对齐句子的数据集，可以使用这些句子作为黄金标准对齐来评估一些提出的对齐方法。</sample>
    <sample id="24">我们对提出的算法做了一些适应性调整，并在论文中发表了这些适应性调整和运行实验的代码。</sample>
    <sample id="25">最后，我们得出结论，对于德语文本简化任务，最好的自动对齐方法是MASSalign方法。</sample>
    <sample id="26">你可以在论文里找到运行这个方法的代码，用于自己文档。</sample>
    <sample id="27">我们在论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">通过微调语言模型来从复杂输入文本生成简化文本。</sample>
    <sample id="29">我们对两种不同的模型进行了微调，一种是微调了长mBART模型来生成文档级别的简化内容。</sample>
    <sample id="30">我们还对正常基础的长文本进行了微调，以生成句子级别的简化。</sample>
    <sample id="31">你还可以找到所有的检查点，可以深入查看实验的分数和评估指标，这些都在论文里。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或得到比基线分数更好的分数。</sample>
    <sample id="33">我们把这些结果作为未来自动文本简化问题的基准。</sample>
    <sample id="34">谢谢大家的关注，希望能在会议上见到大家。谢谢。</sample>
    <sample id="35">演讲者的名字是Kayo Yin。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="36">他们使用T5 XL模型获得82%-87%的准确率。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="37">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="38">这个方法新颖之处在于它通过明确标注每个模型响应是否表达某些行为，来减少人工评估的主观性。嗯，你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">嗯…这个嘛，你可以多听一些歌，至少听一些每首歌，然后读一读每首歌的资料。这样应该能提高分数。如果还有其他问题，随时跟我说哈。</sample>
    <sample id="41">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="42">嗨，我的名字是Adam Przepiórkowski，这个演讲是关于协调的依赖结构。</sample>
    <sample id="43">嗯，不同理论和语料库方法假设不同的依存结构。例如，在通用依存结构中，像“Lisa，Bart和Maggie”这样的并列结构的依存结构是怎样的。</sample>
    <sample id="44">在这种情况下，第一个并列成分是整个并列结构的主语，也就是“Lisa”。</sample>
    <sample id="45">在意义文本理论中，也假设了类似的协调结构，其中整个协调结构由第一个并列连词主导。所以这两种方法是对称的，它们突出一个并列连词。</sample>
    <sample id="46">还有对协调结构的对称方法，比如Prague方法，Conjunction-headed方法，这些方法在Prague依赖树库中被采用，其中协调结构由连词主导。</sample>
    <sample id="47">所以，我们从“and”得到所有并列成分的依赖关系。</sample>
    <sample id="48">最后，还有一个多头方法，例如在卡森斯世界语法中使用。</sample>
    <sample id="49">嗯，就是说所有的并列成分都是主语结构的头，这样就从主语“爱”那里得到了对所有并列成分的依赖关系，像丽莎，巴特和玛吉。如果还有啥不明白的，你可以再问我哦。</sample>
    <sample id="50">论文的目标是为像这两个对称结构的协调提出一个新颖的论点，并反对像这两个不对称结构的协调。</sample>
    <sample id="51">这个论证是基于依赖长度最小化原则的，我将根据这些例子来解释。</sample>
    <sample id="52">嗯，所以，在英语里，你可能知道，直接宾语更倾向于靠近动词，而副词可以离动词远一些，对吧？像“Marge read it yesterday”就很好，因为直接宾语“it”靠近动词。</sample>
    <sample id="53">“Marge read yesterday it” 这个句子更糟糕，因为介词“yesterday”在动词和直接宾语之间。</sample>
    <sample id="54">然而，当直接宾语非常重且很长时，这种效果可能会得到改善，因为在这种情况下，它可以在介词之后的位置。</sample>
    <sample id="55">这里说明了，这两个句子都是可以的。Marge读了一本关于蜜蜂的非常有趣的书，昨天。这样，我们用“this”代替“it”，句子就简洁明了了。</sample>
    <sample id="56">但是也可以说“Marge read yesterday this absolutely fascinating book about bees.”</sample>
    <sample id="57">这里的原因是，尽管这个句子违反了直接宾语应该紧挨着动词的语法规则。</sample>
    <sample id="58">它遵循依赖长度最小化原则，该原则表明较短的依赖关系是首选的。</sample>
    <sample id="59">这两个树图只显示了关键依赖关系的长度，也就是在两个结构中不恒定的那些。</sample>
    <sample id="60">所以这里我们有一个从“read”到“absolutely fascinating book about bees”的依赖关系，长度为7个单词，从“read”到“book”的依赖关系长度为4个单词，所以总共是11个单词。</sample>
    <sample id="61">当你交换这两个成分时，这两个依赖关系的总和变为6，对吧？所以从11变成6，更短了。这就是为什么听起来还不错的原因。它违反了一个原则，但满足了另一个原则。</sample>
    <sample id="62">好的，我们从增强版的Penn Treebank中提取了关于协调的统计数据，看看论文。你为什么不使用大学的依赖关系呢？</sample>
    <sample id="63">这些统计数据确认了之前多次观察到的左连词倾向于较短的观察。例如，盐和胡椒而不是胡椒和盐，以音节为单位测量。</sample>
    <sample id="64">从增强版的宾夕法尼亚树库中提取的关于协调的统计数据，来自Marcus等人，1993年，Ficler和Goldberg，2016年：。左连词倾向于较短，以前观察到。这种倾向随着长度差异而增长，Gibson等人，1996年，88-90页，曾简要注意到。但仅当主语在左边或不存在时，例如，我看到Bart和Lisa，Homer来了打喷嚏，否则当它在右边时，例如，Ted和Ned笑了。</sample>
    <sample id="65">当两个并列成分的长度差异增大时，较短的并列成分更倾向于成为第一个并列成分，比例更大。</sample>
    <sample id="66">这篇论文的创新之处在于我们观察到这种趋势仅在主语在左边或缺失时才会发生。</sample>
    <sample id="67">好的，那我来给你翻译一下。</sample>
    <sample id="68">在“Homer came and sneezed”这个例子中，有两个动词被协调，没有外部的连词。在这种情况下，左边的连词倾向于更短，而且两个动词之间的差异越大，左边的连词就越短。</sample>
    <sample id="69">然而，当主语在右边时，比如“Ted和Ned笑了”，这种效应就消失了。</sample>
    <sample id="70">所以我们展示了通过测量长度，字符在第一列，音节在中间列，单词在右列，然后我会集中讨论右列的内容。</sample>
    <sample id="71">我们看到的是，当主语在左边时。</sample>
    <sample id="72">左连词的长度倾向于比右连词短，这种倾向随着连词长度绝对差值的增加而稳步增长，即使没有连词支配者，如在句子的并列结构中。但当连词支配者在右侧时，这种倾向消失。</sample>
    <sample id="73">我们在论文中展示了，这为反对不对称的协调结构提供了论据，因为这两个是对称结构。</sample>
    <sample id="74">所以，看看论文以了解完整的论点和论证。抱歉，还有在海报展期间与我们交谈。谢谢。</sample>
    <sample id="75">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="76">圣经文本的简化程度更大。</sample>
    <sample id="77">“salt and pepper”和“not the pen sold”。</sample>
    <sample id="78">可以，DrBERT模型，NACHOS数据集和训练脚本都是免费的，你可以用于你的研究。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="79">DEplain-APA是基于新闻文本的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="80">嗯，要实现良好的泛化呢，需要更好的模型架构，更大的模型规模，还有更多的微调示例。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="81">通过测量长度来衡量，比如字符数，音节数或者单词数。</sample>
    <sample id="82">嗯…你可以设置不同长度的支配词，然后观察支配词在左边和右边时，从句长度的变化情况。比如说，分别测量长度在字符，音节和单词中的变化。这样就能看出支配词位置对从句长度的影响了。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="83">基线分类器在不平衡数据上训练效果不太好，表现得不如随机猜测。</sample>
    <sample id="84">这篇论文有四位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="85">示例对话中的角色名字是Bob和Alice。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="86">在正式性和词汇连贯性上，语境感知MT模型比语境无关模型更有优势。嗯，你要是还有啥疑问可以接着问哈。</sample>
    <sample id="87">Johns Hopkins University，Purdue University，MIT，Meta AI。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="122">嗯，这个框架是通过计算Pearson r相关系数来量化立场的。就是把金标准标签，模型预测和不同人口统计学的注释分别进行比较。你要是还有啥疑问，可以接着问哦。</sample>
    <sample id="155">研究结果是他们能够揭示种族刻板印象。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="156">此研究使用了增强版的Penn Treebank作为数据来源。</sample>
    <sample id="157">这篇论文有两位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="158">与认知失调密切相关的任务是辩论和CE任务。</sample>
    <sample id="159">两位。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="160">这篇论文有六位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="161">这个框架与以前的研究不同在于，它通过比较最终用户，模型和数据集的预测和标签，而不是仅仅看内或跨注释者的协议或建模注释者分布。</sample>
    <sample id="162">在三个比较设置中，GPT-3.5与刻板词汇的重叠最多。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="163">比较了DeepL和Google。</sample>
    <sample id="164">嗨，我是来自华盛顿大学的博士生尚斌。今天我正在展示我们关于从预训练数据到语言模型再到下游任务，追踪政治偏见的轨迹，导致不公平的NLP模型的工作。</sample>
    <sample id="165">语言模型是用大规模的网络爬虫数据来训练的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="166">根据对C4语料库的调查，政治新闻媒体在预训练数据中得到了很好的覆盖。我们可以看到，《纽约时报》《洛杉矶时报》《卫报》《赫芬顿邮报》等都在语言模型的训练数据中被广泛覆盖。</sample>
    <sample id="167">这对语言模型应用来说是一把双刃剑。</sample>
    <sample id="168">一方面，他们能够从多元视角学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点本质上是社会偏见的，可能会在下游任务应用中导致潜在的公平性问题。</sample>
    <sample id="169">为了这个目的，我们建议调查从预训练数据到语言模型再到下游任务的政治偏见传播管道。具体来说，通过提出以下问题。</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向，以及预训练数据在政治偏见方面可能扮演的角色。</sample>
    <sample id="171">嗯，这个嘛，就是说，不同政治倾向的语言模型在下游任务上的表现如何，以及它们的政治倾向是否会导致NLP应用中的公平性问题。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="172">首先，我们用政治问卷，比如政治指南针测试，向语言模型提出不同格式的提示。这确保了我们的自动评估是基于政治科学文献的。</sample>
    <sample id="173">一些初步结果显示，语言模型确实有不同的政治倾向，它们在政治坐标图的四个象限中都有分布。</sample>
    <sample id="174">我们还可以看到，GPT-4是所有语言模型中最自由的，GPT系列通常在社会自由度上比BERT系列及其变体更高。</sample>
    <sample id="175">其次，我们旨在调查语言模型的政治偏见在多大程度上是从训练数据中吸收的。</sample>
    <sample id="176">我们通过进一步预训练语言模型检查点来进行受控实验，使用六个不同党派的语料库，这些语料库被分为新闻和社交媒体，进一步按政治倾向划分。</sample>
    <sample id="177">通过在这些党派语料上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了偏移。</sample>
    <sample id="178">嗯，比如说，对于RoBERTa，如果进一步微调和训练在左倾Reddit语料库上，我们能看到其在政治倾向上向左倾有显著的转变。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="179">嗯…这个图展示了RoBERTa和GPT-2在政治倾向上的党派偏移结果。左边是RoBERTa，右边是GPT-2。图里有四个象限，分别对应左，中，右和自由派。红色箭头表示从左到右的偏移，蓝色箭头表示从右到左的偏移。红色箭头的偏移量是，-0.13，-1.03，蓝色箭头的偏移量是，-2.75，-1.24，还有其他一些偏移量数值。</sample>
    <sample id="180">我们还试图研究语言模型是否能捕捉到现代社会中普遍存在的两极分化。</sample>
    <sample id="181">我们将预训练语料库分为在第45任总统之前和之后的部分，分别在两个不同的时间语料库上预训练语言模型。</sample>
    <sample id="182">我们可以看到，语言模型在2017年后，政治倾向通常更远离中心。这表明语言模型也能反映出社会的两极分化。</sample>
    <sample id="183">嗯，就是说我们用不同政治倾向的语言模型来评估仇恨言论检测和假新闻检测这两种NLP应用。这两种应用经常涉及语言模型，而且可能有非常重要的影响。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="184">嗯，这个表格展示了不同身份群体的仇恨言论和不同来源的错误信息的分类表现。颜色编码表示，深黄色是最好的表现，深蓝色是最差的表现。比如说，在仇恨言论方面，对于黑人，新闻左派的表现是89.93，新闻右派是88.81，Reddit左派是89.84，Reddit右派是88.03。在错误信息方面，对于HP，新闻左派是89.44，新闻右派是89.44，Reddit左派是88.73，Reddit右派是90.85。你要是还有啥疑问，可以接着问哈。</sample>
    <sample id="185">不同媒体的意识形态或政治倾向，我们能看到一个模式，例如在仇恨言论检测方面，左翼语言模型表现更好。</sample>
    <sample id="186">在检测针对不同社会少数群体的仇恨言论方面。</sample>
    <sample id="187">然而，我们的工作在检测针对社会中更强大的群体的仇恨言论方面表现更差。</sample>
    <sample id="188">右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人，LGBTQ+和其他少数群体的仇恨言论方面表现更差。</sample>
    <sample id="189">在假新闻检测方面，也出现了类似趋势，左翼语言模型在检测来自相反政治倾向的假信息方面表现更好，反之亦然。</sample>
    <sample id="190">这个表格展示了使用不同政治倾向的语言模型在下游任务中的表现。CHRIS代表基督徒，Base是vanilla RoBERTa模型，N是新闻媒体，S是社交媒体，L是左倾，R是右倾。</sample>
    <sample id="191">嗯，这些语言模型在政治倾向不同的情况下，对仇恨言论和错误信息的预测是不一样的。在附录里还有更多例子来进一步说明这一点。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="192">这表明语言模型的政治偏见是一个非常紧迫的公平性问题。</sample>
    <sample id="193">嗯…如果一个语言模型被微调用于仇恨言论或错误信息之类的，然后部署到一个流行的社交媒体平台上。</sample>
    <sample id="194">嗯，这个意思就是说，不同政治倾向的人可能会被边缘化，而且针对少数群体的仇恨言论可能会不受控制地泛滥。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="195">这提醒我们要承认并解决由语言模型政治倾向导致的公平性问题。</sample>
    <sample id="196">我们想强调的是，我们揭示了语言模型政治偏见的独特困境，就像在斯卡利和查尔迪斯之间。</sample>
    <sample id="197">如果在语言模型训练数据中不进行政治观点的净化，那么偏见就会从预训练数据传播到语言模型，再到下游任务，最终导致公平性问题。</sample>
    <sample id="198">如果尝试对数据进行净化，可能会面临审查或排除的风险，而且很难确定哪些内容是中立的，应该保留在语言模型训练数据中，这就像是电车难题。</sample>
    <sample id="199">好的，我觉得这就是我今天要做的全部了。谢谢大家的时间。</sample>
    <sample id="200">这篇论文有六位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="201">MPP评估最多涵盖900个词元的上下文长度。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="202">他们的数据集中包含音乐，书籍和食谱这三个领域。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="203">Positionality是人们由于其人口统计学，身份和生活经历而持有的观点。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="204">Dawei Zhu.</sample>
    <sample id="205">EDAtt没有适应现有的离线ST模型，它是使用已经存在的离线ST模型，不需要重新训练或者采用特定的架构。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="206">这篇论文有四位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="207">被测模型在测试套件上运行得不太好。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="208">KITMUS有三个变体，分别是Background-Pretrain，Background-Both和Background-Inference。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="209">Google Research。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="210">最后一个研究问题是“Should we only use the clean samples for validation or there are better ways to utilize them？”</sample>
    <sample id="211">嗯…这个指标灵敏度是用来衡量模型在相同任务上，对指令的微小变化有多敏感。它能评估模型在不同指令下，是否能一致地产生相同的结果。如果还有疑问，你可以再问我哦。</sample>
    <sample id="212">演讲者的名字是Jingwei Yi。</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="214">嗯…这个嘛，具体在预训练期间模型接收什么样的语言上下文，这个信息在你给的英语内容里没提到呢。你可以再找找其他资料，或者咱们再聊聊看。</sample>
    <sample id="215">通常需要20个干净的验证样本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="216">Stanford Engineering Computer Science。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="217">嗯…因为现有的方法有局限性，不能准确衡量媒体偏见。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="218">演讲者的名字是Akshatha。</sample>
    <sample id="219">政治偏见传播流程是从预训练数据到语言模型，再到下游任务。嗯，你要是还有啥想知道的，可以再问我哈。</sample>
    <sample id="220">是的，DEplain-apa 和网站的简化过程有所不同。DEplain-apa 更多的是重排序和单词添加，而网站的简化过程则更多是改述。你要是还有啥疑问，尽管跟我说哈。</sample>
    <sample id="221">嗯…这个我不太清楚呢。你可以去查查相关的资料或者问问其他专业人士。要是你还有其他问题，也可以继续问我呀。</sample>
    <sample id="222">嗯，水印插入到文本中的过程是这样的。首先定义一个目标嵌入，然后计算句子中的触发数量，最后将目标嵌入添加到原始嵌入上。如果还有疑问的话，可以再问我哦。</sample>
    <sample id="223">这篇论文的作者所属机构是PennState和Amazon。</sample>
    <sample id="224">可以。</sample>
    <sample id="225">示例是做草莓蛋糕和做巧克力蛋糕。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="226">他们通过在四个数据集上可视化句子的嵌入来验证嵌入的隐蔽性。</sample>
    <sample id="227">研究通过使用现有的预训练模型来进行连续预训练，来构建新的预训练模型。</sample>
    <sample id="228">拉丁美洲。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="229">演讲者在“I am a student.”这个示例句子上展示了模型如何利用注意力机制所学的知识。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="230">随着任务数量的增加，模型的性能会变好，同时敏感性会降低。</sample>
    <sample id="231">LSTM seq2seq，T5，Zheng and Lapata。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="232">两位合著者是第一作者的导师。</sample>
    <sample id="233">PaLM的第一作者是Chowdery等人。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="234">大家好，我是来自卡内基梅隆大学的一年级博士生珍妮，今天我将为大家介绍我们的工作NLPPositionality：数据集和模型设计偏见的表征。</sample>
    <sample id="235">这项工作是与华盛顿大学和艾伦人工智能研究所的一些人合作完成的，具体是塞巴斯蒂安·桑蒂，罗南·勒·布拉斯，卡塔里娜·雷内克和马尔滕·萨普。</sample>
    <sample id="236">想象一下…所以，让我们从想象你为一家报纸工作开始，你在浏览新闻文章下的评论，试图删除有毒内容。</sample>
    <sample id="237">想象一下…你可能会转向一个流行的API，比如Perspective API进行毒性检测，如果你是Carl Jones，这个API工作得非常好，能正确检测出有毒的实例。</sample>
    <sample id="238">但对阿迪蒂亚·夏尔马来说，情况并非如此，对于印度语境中更常见的冒犯性术语，Perspective API 并不那么敏感。</sample>
    <sample id="239">这是一个设计偏见的例子，我们看到技术在不同人群之间存在系统性的性能差异。</sample>
    <sample id="240">像之前看到的设计偏见可能源于NLP研究人员和模型开发者的立场。立场就是人们由于其人口统计学，身份和生活经历而持有的观点。</sample>
    <sample id="241">定位性，人们由于其人口统计特征，身份和生活经历而持有的观点。这个概念在批判性研究中广泛使用，特别是在女性主义和酷儿学术领域。</sample>
    <sample id="242">定位性是指人们由于其人口统计特征，身份和生活经历而持有的观点。作为研究人员，定位性会影响研究过程及其结果。</sample>
    <sample id="243">嗯，你问的是数据集和模型有没有立场性。</sample>
    <sample id="244">我们不是在说模型和数据集本身有人口统计学身份和生活经历，但它们确实汇总了真实人们的判断和意见，因此可以代表某些立场而不是其他立场。</sample>
    <sample id="245">嗯，你问的是关于数据集和模型是否有立场性的问题。初步研究提供了一些证据，比如文化差异在模型和数据集中的体现，还有模型立场性的理论定义。如果还有其他问题或者想深入聊聊这个话题，随时告诉我哈。</sample>
    <sample id="246">然而，这些工作并没有真正比较最终用户与数据集和模型本身。</sample>
    <sample id="247">研究模型和数据集的立场性变得越来越重要，因为NLP任务变得更加主观和面向社会。</sample>
    <sample id="248">很难描述这些偏见是如何扭曲的，因为并非所有决策都记录在案，而且许多模型隐藏在API背后。</sample>
    <sample id="249">为了研究数据集和模型的位置性，我们实际上比较了注释与真实用户的注释，以及现有的数据集和模型。</sample>
    <sample id="250">我们通过我们的框架NLP Positionality来做这件事。</sample>
    <sample id="251">我们的框架分为两个主要步骤。</sample>
    <sample id="252">第一步是用多样化的注释者重新注释数据集。</sample>
    <sample id="253">我们选择这样做，而不是查看原始数据集标注者的统计数据，因为通常每个实例只有少数标注者标注，并且很少收集和分享统计数据。</sample>
    <sample id="254">因此，我们重新注释数据，以获得每个实例的许多注释，并获得丰富的统计数据。</sample>
    <sample id="255">然后我们对按人口统计学分类的注释进行比较，并使用皮尔逊相关系数将它们与模型和数据集进行比较。</sample>
    <sample id="256">我们的框架实际上与注释者分歧文献不同，它通过比较最终用户与模型和数据集的预测和标签，而不是仅仅看注释者之间的内部分歧或建模注释者分布。</sample>
    <sample id="257">我们的框架主要通过“Lab in the Wild”在线众包平台来实现，该平台是前HCI合作者创建的。</sample>
    <sample id="258">LabintheWild是一个在线实验平台，可以招募多样化的志愿者，与像MTurk这样的平台相比，MTurk的参与者大多来自美国或印度，而LabintheWild仍然能够获得高质量的数据。</sample>
    <sample id="259">我们在Lab in the Wild上举办两个任务，其中一个就是社会接受度。这个任务的工作方式是，参与者会阅读来自Social Chemistry数据集的情况，然后对这个情况的社会接受度进行评分。</sample>
    <sample id="260">为了保持参与者的参与度，他们可以将自己的回答与AI和其他人的回答进行比较。</sample>
    <sample id="261">然后我们将这些注释与社会化学，德尔菲和GPT-4进行了比较。</sample>
    <sample id="262">然后，我们复制了类似的设置用于毒性与仇恨言论检测任务，参与者会阅读来自Dynahate的一个实例，并对其是否为仇恨言论进行评分。</sample>
    <sample id="263">然后我们将这些注释与Dynahate，Perspective API，Rewire API，Hate RoBERTa和GPT-4进行比较。我们的研究最终收集了来自87个国家的1000多名注释者的超过16000个注释。</sample>
    <sample id="264">现在我们准备好回答NLP数据集和模型与谁最对齐了。我们发现NLP中有立场性。</sample>
    <sample id="265">例如，我们发现数据集和模型最与英语国家对齐。对于GPT-4的社会接受度分析，我们发现它最与儒家和英语国家对齐。我们还发现Dynahate也最与英语国家对齐。</sample>
    <sample id="266">我们还发现，GPT-4在社会接受度任务中与拥有大学教育或研究生教育的人最契合。</sample>
    <sample id="267">我们还发现，对于Dynahate，它也最与拥有大学教育的人群相匹配。</sample>
    <sample id="268">然而，当模型和数据集与特定人群对齐时，一些人群不可避免地被落在后面。</sample>
    <sample id="269">一个例子是，数据集和模型在社会接受度方面对非二元性别人群的契合度低于男性和女性。我们在GPT-4的社会接受度任务以及Dynahate任务分析中发现了这一点。</sample>
    <sample id="270">嗯…在NLP里有立场性，那我们能做些什么呢？</sample>
    <sample id="271">所以我们有几个建议。第一个建议是，在整个研究过程中，记录所有相关的设计选择。第二个建议是从语境主义的角度进行NLP研究：a. 共享分层的语料库标签！b. 使用能够处理标注者分歧的建模技术。</sample>
    <sample id="272">第三条建议是构建针对特定社区的专门数据集和模型，例如Masakhane倡议。我们想强调包容性NLP不仅仅是让所有技术对所有人有效。</sample>
    <sample id="273">谢谢！
仪表板链接：nlpositionality.cs.washington.edu/
论文：bit.ly/NLPositionality-Paper/
Delphi
https://delechi.allenai.org/
年龄
性别
种族
宗教
教育水平
居住国
居住国（最长）
母语
所以这就是我们报告的全部内容。如果你想了解更多，欢迎随时查看我们的仪表板获取最新的分析结果和论文。谢谢！</sample>
    <sample id="274">演讲者提到了三个问题。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="275">嗯…这个嘛，一种方法是数据清洗，把那些有偏见的数据去掉。不过这可能会导致信息丢失，还有就是很难确定哪些内容是中立的，应该保留。就像那个电车难题一样，很棘手。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="276">嗨，我是复旦大学的思雨。我来介绍一下我们的工作，从大型语言模型中提取脚本知识用于约束语言规划。</sample>
    <sample id="277">在日常生活中，人们常常通过遵循剧本形式的分步指令来规划他们的行动。</sample>
    <sample id="278">以前的工作已经利用语言模型来为制作蛋糕等刻板活动的抽象目标制定计划，并表明大型语言模型可以有效地将目标分解为步骤。</sample>
    <sample id="279">然而，之前的工作主要集中在为典型的活动的抽象目标进行规划，对于具有具体目标和具体约束的目标，如制作巧克力蛋糕，规划仍然缺乏研究。</sample>
    <sample id="280">在这篇论文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">抽象目标可以被不同的现实具体目标所继承，带有多方面的约束。一个好的规划者应该编写合理且忠实于约束的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。</sample>
    <sample id="283">由于没有特定目标的数据集来支持我们的研究。</sample>
    <sample id="284">我们首先需要获取这些目标，如表格所示，我们通过多维度约束来扩展抽象目标，以供人类在环数据获取中使用。</sample>
    <sample id="285">我们随机抽取100个特定目标，评估大型语言模型生成的脚本。</sample>
    <sample id="286">这张表报告了结果的整体准确性。我们发现，所有大型语言模型在为特定目标进行规划时都取得了不令人满意的结果。</sample>
    <sample id="287">然后我们进行详细分析，以调查为什么LLM模型会出错。</sample>
    <sample id="288">图中的结果表明，生成脚本的语义完整性是可以接受的，但对约束的忠实性不能保证。</sample>
    <sample id="289">当我们深入研究维基百科中更细粒度的约束主题类别时，图中的热图显示，InstructGPT在不同类别的目标上的规划表现差异很大。</sample>
    <sample id="290">以前的研究表明，大型语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用了“先生成后过滤”的理念来提高生成质量。</sample>
    <sample id="291">首先，通过展示约束类型和例子来引导InstructGPT，然后根据种子抽象目标获得具体目标。</sample>
    <sample id="292">然后，使用InstructGPT为特定目标过度生成候选脚本。</sample>
    <sample id="293">接下来，开发了一个过滤模型来选择合适的剧本。</sample>
    <sample id="294">我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度和相似度分数，以衡量语义相似性。</sample>
    <sample id="295">我们将过滤出包含目标约束关键词的脚本。我们只保留目标得分在脚本集中最高的脚本。</sample>
    <sample id="296">使用我们的方法，InstructGPT可以生成更高质量的脚本。我们的方法极大地提高了规划能力，包括语义完整性和对约束的忠实性。</sample>
    <sample id="297">由于大型语言模型部署成本高，因此有必要使小型和专业化模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。</sample>
    <sample id="298">然而，之前的研究没有为特定目标提供规划能力，而且手动标注数据集很昂贵。</sample>
    <sample id="299">我们遵循符号知识蒸馏的理念，从大模型中蒸馏出约束语言规划数据集。</sample>
    <sample id="300">我们应用我们的方法来构建一个约束语言规划的数据集，命名为Coscript。</sample>
    <sample id="301">总共生成了55000个具体目标和脚本，为了确保验证和测试集的质量，我们请云众包工人查找并修正错误样本。</sample>
    <sample id="302">这个图展示了Coscript的约束分布。我们发现Coscript在生成的具体目标上表现出高度的异质性和多样性。使用Coscript，我们可以训练更小但更专业的模型进行约束语言规划。</sample>
    <sample id="303">我们发现，T5在Coscript上微调后，生成的脚本质量高于大多数大型语言模型，这表明在合适的语料库上微调的小型模型可以超越大型模型。</sample>
    <sample id="304">总结：我们建立了约束语言规划问题，评估了大型语言模型的约束语言规划能力，并开发了一种过生成再过滤方法用于大型语言模型。</sample>
    <sample id="305">我们使用大型语言模型生成高质量的脚本数据集，CoScript，用于约束语言规划。我们希望CoScript数据集能成为推进语言规划研究的宝贵资源。</sample>
    <sample id="306">谢谢您的时间。请在我们的论文中查找更多关于CoScript的细节。</sample>
    <sample id="307">PaLM的流畅度和SOTA系统相当。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="308">嗯…水印方法的重要属性有这么几个。首先，它得适用于嵌入式服务，然后呢，不能降低提供的嵌入式服务的实用性，还得对攻击者来说足够隐蔽，让攻击者不容易发现，最后，水印得能在攻击者的服务中转移。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="309">德语，西班牙语，法语，意大利语，日语，韩语，荷兰语，葡萄牙语，罗马尼亚语，俄语，土耳其语，中文。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="310">从一个数据集中抽取300个实例用于重新注释。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="311">Cosine距离和L2距离度量用于衡量良性和后门数据集之间的差异。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="312">嗯…这个嘛，就是把多语言预训练的编码器和指针式解码器结合起来。像XLM-R + PTR，mBERT + PTR这样的组合。你要是还有啥疑问，可以再问我哦。</sample>
    <sample id="344">作者通过在一般文本语料库上计算单词频率，然后随机选择n个单词在中等频率区间内来确定中等频率的单词。</sample>
    <sample id="345">大家好，我叫舒恒。今天我将要介绍我们的论文《CoNLL-2003命名实体标记器在2023年是否仍然有效》。让我们开始吧。</sample>
    <sample id="346">我们的论文研究了使用命名实体识别任务，NER任务，来解决泛化问题。</sample>
    <sample id="347">我们观察到，模型已经使用CoNLL-2003开发NER近20年了，这自然提出了几个问题。首先，这些模型能否泛化到现代数据？</sample>
    <sample id="348">在开发新标签时，需要什么才能实现良好的泛化？</sample>
    <sample id="349">同时，如果我们观察到模型的泛化能力差，那么是什么导致这些模型的性能下降呢？</sample>
    <sample id="350">为了调查这些问题，我们开发了CoNLL++数据集。这是从2020年的路透社新闻中收集的数据集，并按照CoNLL-2003注释指南进行了注释。</sample>
    <sample id="351">然后我们在CoNLL-2003上微调了超过20个模型。我们分别在CoNLL-2003测试集和CoNLL++测试集上评估了它们。</sample>
    <sample id="352">最后，我们计算了F1分数的变化百分比，以评估每个模型的泛化能力。</sample>
    <sample id="353">所以，好的泛化需要什么？在所有实验中，我们发现有三个主要的要素是必要的。</sample>
    <sample id="354">第一个是模型架构。通过我们的实验，我们发现Transformer模型通常在新数据上泛化得更好。</sample>
    <sample id="355">第二个要素是模型大小。我们发现通常较大的模型能带来更好的泛化能力。</sample>
    <sample id="356">最后，我们都知道，微调样本的数量直接影响下游任务的性能。我们还发现，更多的微调样本实际上也导致更好的泛化能力。</sample>
    <sample id="357">那么下一个问题，是什么导致了一些模型的性能下降呢？</sample>
    <sample id="358">我们有两个假设。第一个是自适应过拟合，这是由于反复使用同一个测试集造成的过拟合，通常表现为在新测试集上的回报递减。</sample>
    <sample id="359">第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距增加而导致的性能下降。</sample>
    <sample id="360">从右边的图中可以看到，红色的最佳拟合线的斜率大于1。</sample>
    <sample id="361">这意味着我们在CoNLL-2003上每一点的改进，都会转化为CoNLL++上超过一点的改进，也就是说没有边际效益递减。</sample>
    <sample id="362">这表明在这个情况下，自适应过拟合没有被观察到。</sample>
    <sample id="363">那么关于时间漂移呢？</sample>
    <sample id="364">对于时间漂移，我们做了一个实验，对一些模型进行重新训练或继续预训练，使用更近的数据。我们发现，随着时间间隔增大，性能会下降。</sample>
    <sample id="365">这证实了我们关于性能下降的主要原因是时间漂移的假设。</sample>
    <sample id="366">我们的结论是，为了实现良好的泛化能力，我们需要更好的模型架构，更大的模型规模，以及更多的微调示例，这些因素是相互关联的，不能只依靠其中一个。</sample>
    <sample id="367">同时，我们还发现，性能下降是由时间漂移造成的，很意外的是，它不是由自适应过拟合造成的，尽管CoNLL-2003已经使用了20多年了。</sample>
    <sample id="368">回到我们论文标题中提出的问题：2003年的CoNLL标注器在2023年是否仍然有效？我们发现答案是肯定的。</sample>
    <sample id="369">我们希望我们的论文能促使更多关于如何提高模型泛化能力的研究。</sample>
    <sample id="370">最后，请务必查看我们的论文，数据集，如果您有任何问题，欢迎随时联系我。非常感谢。</sample>
    <sample id="397">抱歉，图片内容和语音片段大小没有关系，图片上只显示了一个问题“我们的解决方案是什么？”没有提到语音片段大小的信息。你可以再给我点其他线索吗？</sample>
    <sample id="398">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是法官。</sample>
    <sample id="399">示例质量更为重要。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="400">论文侧重于RoBERTa和GPT-2这两个语言模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="401">嗯…这个模型是结合多个层的分数的。如果还有其他问题，你可以再问我哦。</sample>
    <sample id="402">直接推断的示例有“easy on me”和“the first one”。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="403">复旦大学和Brain Technologies Inc.</sample>
    <sample id="404">这篇论文有7位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="405">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="406">作者给出的“显性群体”的示例是“a woman warrior”。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="407">嗯…这个嘛，图片里没提到哪些模型架构泛化能力较差呢。不过从“Transformer models generalize better”这句话来看，可能那些不是Transformer架构的模型泛化能力就比较差。你要是还有别的想法或者信息，可以再和我说一说呀。</sample>
    <sample id="408">你没说清楚是哪个图的测试数据集名称啊。你可以再给我点信息吗？</sample>
    <sample id="409">这篇论文有六位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="410">仅使用文本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="439">嗯…这个嘛，作者没具体说哪些是研究不足的领域呢。不过他提到知识密集型 NLU 任务需要整合和使用预训练时间和推理时间的知识。你要是还有啥想法或者疑问，咱们可以再聊聊呀。</sample>
    <sample id="440">演讲者的名字是Ying。</sample>
    <sample id="441">是的，经过了质量检查。</sample>
    <sample id="442">现有的资源只支持有限类型的依赖上下文的翻译和有限的语言。</sample>
    <sample id="443">嗨，我将谈谈我们关于解决间接指代表达式以进行实体选择的工作，其中我们介绍了AltEntities语料库。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛尼，这是与菲利普·拉德林斯基，西尔维亚·帕雷蒂和安妮·路易斯的联合工作。</sample>
    <sample id="445">我们的目标是理解用户在做选择时的语言。考虑这个替代问题：你是说《Easy on Me》还是《I Gotta Feeling》？在这里，用户想在两首歌中选择一首。</sample>
    <sample id="446">最明显的方法是使用直接引用，例如说出歌曲的名字“easy on me”或者它的位置“the first one”。</sample>
    <sample id="447">有时候，间接指代表达在自然对话中更合适。这可能发生在用户记不住歌名的时候。</sample>
    <sample id="448">发音太相似，难以区分。</sample>
    <sample id="449">或者当用户想要明确表达偏好时。这里有一些例子，比如“the newer one”或者“the song that's not energetic”。</sample>
    <sample id="450">这是对话系统中的一个重要问题，也是对大型语言模型实体理解进行基准测试的问题。</sample>
    <sample id="451">我们没有现成的大规模公共数据集，所以使用众包来收集一个。我们的数据集涵盖了三个不同的领域：音乐，书籍和餐饮。</sample>
    <sample id="452">我们的数据集收集方法强调非正式性，使用卡通完成任务。</sample>
    <sample id="453">嗯，这个卡通有三个对话气泡。第一个气泡里，Bob说“还记得我们昨天听的那首歌吗？”Bob就是这样设置对话背景的。如果还有其他问题，你可以随时问我哦。</sample>
    <sample id="454">你是在问“《Easy on Me》还是《I Gotta Feeling》？”</sample>
    <sample id="455">在第三个对话气泡中，鲍勃使用间接指代来选择其中一个实体，例如纽约。</sample>
    <sample id="456">我们自动提供第一和第二句气泡，但第三句由注释者填写。第一句气泡是从每个领域几个手动提示中选择的。</sample>
    <sample id="457">第二个部分，即替代问题，是这样生成的。</sample>
    <sample id="458">我们总是使用一个简单的模板：“你是指A还是B？”其中A和B是从Wikipedia中采样的。</sample>
    <sample id="459">这里列出了我们使用的不同采样方法。当我们向上移动列表时，实体变得越来越相似，通常更难进行消歧义。</sample>
    <sample id="460">第一个是随机均匀分布。</sample>
    <sample id="461">第二个是当实体有相似标题时，例如两本名为《The Return》的书。</sample>
    <sample id="462">第三种情况是当它们在Wikipedia上有相似的描述。最后，当它们在Wikipedia上有相似的Infobox或属性时，例如相同的类型或相同的艺术家。</sample>
    <sample id="463">当向注释者展示这个问题时，他们知道这些实体的名称，但并不一定知道关于这些实体的信息。</sample>
    <sample id="464">所以我们要做的是展示一些关于这两个实体的背景知识。对于歌曲，我们只是展示每个歌曲的谷歌搜索链接。</sample>
    <sample id="465">然后让注释者至少听一些每首歌，读一读每首歌。例如，这是歌曲“Easy on Me”的谷歌搜索结果。</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了来自维基百科的背景文本。对于食谱，我们还展示了它们的图片，也来自维基百科，这样标注者就能知道它们的样子。</sample>
    <sample id="467">然后我们告诉注释者应该选择哪个选项，并让他们描述它。</sample>
    <sample id="468">好的，你可以说出你想要翻译的英文内容了。</sample>
    <sample id="469">AltEntities语料库包含约6000个跨三个领域的替代问题，约42000个间接指代表达。使用T5 XL模型的结果如下：如果LM有与注释者相同的背景知识，准确率在92%-95%之间。如果LM有部分重叠的背景知识，准确率在82%-87%之间。如果LM仅能访问实体名称，准确率约为60%。我们展示了模型的跨域泛化能力。数据集链接：https://github.com/google-research-datasets/AltEntities。</sample>
    <sample id="470">如果语言模型有与注释者完全相同的背景知识，准确率会非常高，大约在92%到95%之间。但这并不现实。</sample>
    <sample id="471">如果语言模型有部分重叠的背景知识，那么准确率在82%到87%之间，这更现实，例如当语言模型检索到背景知识时。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以还有很大的提升空间。我们也展示了模型在不同领域是通用的。这是数据集的链接，谢谢。</sample>
    <sample id="473">该方法与 wait-k 策略，LA 策略，CAAT 策略和 EDAtt 这些现有的 SimulST 策略进行了比较。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="474">作者所属机构有Avignon Université，LS2N，Nantes Université，Clinique des données，CHU de Nantes，Zenidoc。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="475">演讲者的名字是Jenny。</sample>
    <sample id="476">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="477">嗨，我是来自特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕皮，我将简要介绍“注意力作为同步语音翻译的指南”论文，这是与马特奥·内格里和马可·图尔奇的联合工作。</sample>
    <sample id="478">同步语音翻译，SimulST，是将口语实时翻译成另一种语言文本的过程，实现跨语言交流。</sample>
    <sample id="479">当前的SimulST模型存在哪些问题？特定架构通常被训练，引入额外模块进行优化。</sample>
    <sample id="480">长期而复杂的训练程序，例如涉及不同的优化目标的训练。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟状态，例如训练一个平均延迟为1秒的模型和另一个平均延迟为2秒的模型等等。</sample>
    <sample id="482">我们的解决方案是什么？</sample>
    <sample id="483">1. 使用现有的离线ST模型，无需重新训练或采用特定架构用于SimulST。2. 每个延迟制度使用一个模型，并通过特定参数处理延迟。</sample>
    <sample id="484">利用模型已经通过音频输入和文本输出之间的注意力机制所获得的知识，这就是跨注意力机制，右边有一个例子。</sample>
    <sample id="485">我们的解决方案是提出EDAtt，或编码器-解码器注意力，这是一种策略，我们根据注意力指向的位置来决定是否发出部分翻译。</sample>
    <sample id="486">如果注意力没有集中在最后λ个语音帧上，即其总和低于阈值α，那么就会发出一个词，这意味着接收到的信息足够稳定。</sample>
    <sample id="487">如果收到包含“I am going to talk about…”的语音片段，我们的模型预测的德语翻译是“Ich werde reden.”</sample>
    <sample id="488">我将要谈论……</sample>
    <sample id="489">我们看到，前两个单词指向最早收到的语音帧，而最后一个单词指向最后收到的语音帧，即λ帧。</sample>
    <sample id="490">这意味着前两个单词将被省略。</sample>
    <sample id="491">当注意力的总和超过某个阈值α时，我们将不会发出最后一个单词，而是等待另一个语音片段。</sample>
    <sample id="492">如果继续，我们收到另一个语音片段，我们的模型预测其他三个单词，我们将看看这些交叉注意力权重。</sample>
    <sample id="493">我们将看到没有单词指向最后的λ个语音帧。</sample>
    <sample id="494">这意味着这三个词会被发射出去。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="495">如果看看EDAtt的主要结果。</sample>
    <sample id="496">我们将同时演讲的结果绘制在图上，其中BLEU在一边衡量翻译质量，平均延迟在另一边。</sample>
    <sample id="497">这是延迟度量，我们还考虑了计算感知平均损失，它会考虑到模型预测输出的计算时间。</sample>
    <sample id="498">所以，我们希望我们的曲线在图上尽可能高。</sample>
    <sample id="499">但是我们也希望它们向左平移。</sample>
    <sample id="500">我们还与一些应用于离线模型的流行策略进行了比较，包括等待k策略和局部协议，同时也与专为同步语音翻译设计的最新架构进行了比较。</sample>
    <sample id="501">这些是SimulST策略在德语上的所有结果。</sample>
    <sample id="502">我们看到EDAtt在所有应用于离线模型的策略中表现更优，因为它们的曲线向左偏移。</sample>
    <sample id="503">而且，如果我们考虑实际的运行时间或计算时间，EDAtt是最快的策略。</sample>
    <sample id="504">如果你想知道更多结果，阅读我们的论文，我们还发布了开源代码和模型，以及同时输出，以促进我们工作的可重复性。谢谢你的关注。</sample>
    <sample id="505">是的，数据集是公开的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="506">大家好，我的名字是Ying，我和同事Zhiyang将要展示我们关于MULTIINSTRUCT的研究，通过指令调优来改进多模态零样本学习。</sample>
    <sample id="507">随着大型语言模型的进展，许多工作开始探索以参数和数据高效的方式重用预训练语言模型进行不同下游任务的新学习范式。</sample>
    <sample id="508">最近，许多研究表明，通过遵循自然指令，指令调优使大型语言模型能够在零样本情况下执行未见过的任务。</sample>
    <sample id="509">然而，大多数关于指令微调的先前工作都集中在提高零样本性能的语言任务上，而计算机视觉和多模态任务却被忽略了。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="510">因此，在这项工作中，我们想研究指令微调多模态预训练模型是否真的能提高对异模态任务的泛化能力。如果还有其他翻译相关的问题，或者想讨论这个研究内容，都可以跟我说哦。</sample>
    <sample id="511">此外，在我们的研究中，我们发现NLP和多模态之间的指令数据集可用性存在相当大的差距。</sample>
    <sample id="512">存在超过1600个纯语言指令任务，然而没有大规模的公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="513">这里我们介绍MultiINSTRUCT，第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，涵盖10个广泛类别。</sample>
    <sample id="514">这些任务来源于21个现有的开源数据集，每个任务都配有5个专家撰写的指令。</sample>
    <sample id="515">在我们的多模态指令调优数据集上，我们以OFA，一个统一的多模态预训练模型，作为基础模型。OFA使用统一的词汇表，包括语言，图像标记和边界框的坐标。</sample>
    <sample id="516">这里我们展示了我们多指令数据集的一些示例实例。</sample>
    <sample id="517">为了统一处理各种输入和输出数据类型。</sample>
    <sample id="518">我们遵循OFA的方法，将所有任务统一为序列到序列格式，其中输入文本，图像，指令和边界框都在同一个词元空间中表示。</sample>
    <sample id="519">好的，现在我要谈谈多模态指令调优。</sample>
    <sample id="520">训练数据集构建：使用来自9个组的53个任务进行训练，每个任务采样10，000个实例。测试数据集构建：保留整个常识推理组用于测试，从VQA和杂项组中选择额外的5个任务，每个任务使用测试分割中的所有实例，从自然指令数据集的测试分割中随机采样20个任务作为NLP的未见任务。</sample>
    <sample id="521">我们使用测试集中的所有实例进行每个任务的测试。此外，我们从自然指令测试集的测试集随机采样20个任务作为NLP的未见任务。</sample>
    <sample id="522">在训练过程中，我们使用预训练的OFA-Large模型作为基础模型。对于所有任务，我们将所有实例混合在一起。每个实例随机与它的五个指令模板之一结合。</sample>
    <sample id="523">在测试阶段，对于每个任务，我们总共进行五次实验，每次实验使用其中一个五种指令之一来评估模型。</sample>
    <sample id="524">我们报告所有五次实验的平均性能，最大性能和性能的标准差。</sample>
    <sample id="525">如果是多模态分类任务，我们报告准确率。如果是多模态生成任务，我们报告Rouge-L。对于NLP任务，我们也报告Rouge-L。</sample>
    <sample id="526">我们还引入了一个名为“敏感度”的额外评估指标，它衡量模型在任务相同的情况下，对指令微小变化的鲁棒性，即在指令微调后，模型能否在相同任务上产生一致的结果。</sample>
    <sample id="527">从我们的主要结果来看，指令调优可以显著提高OFA在多模态任务上的性能。</sample>
    <sample id="528">从自然指令数据集的迁移学习也能使指令微调受益。</sample>
    <sample id="529">随着任务数量的增加，模型实现了更好的性能，同时降低了敏感性。</sample>
    <sample id="530">我们还做了这样的实验，使用一个指令和五个指令。正如你所看到的，使用更多的指令可以提高模型的整体性能，并且大幅降低其敏感性。</sample>
    <sample id="531">嗯，这显示了不同微调策略对模型敏感度的影响。从自然指令数据集进行迁移学习后，模型的敏感度比原始OFA模型要好得多。</sample>
    <sample id="532">我们也可以看到，从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上取得更好的性能。</sample>
    <sample id="533">我们提出了第一个大规模多模态指令调优数据集，显著提高了OFA的零样本能力，探索了不同的迁移学习技术并展示了它们的好处，设计了一个新的度量标准叫敏感性。</sample>
    <sample id="534">还有一件事！我们正在收集一个更大的多模态指令调优数据集，包含大约150个额外的视觉语言任务，我们很快就会发布！这是我们的数据和模型的二维码。谢谢！</sample>
    <sample id="535">这篇论文的作者所属机构是University of Trento和Fondazione Bruno Kessler。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="536">演讲者的名字是Mohammad Javad Hosseini。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="562">大家好，我是Koustuv Sinha，很荣幸欢迎你们参加我们的ACL 2023论文《语言模型的可接受性判断并不总是对上下文具有鲁棒性》的演讲。</sample>
    <sample id="563">语言模型的可接受性判断并不总是对上下文具有鲁棒性。</sample>
    <sample id="564">在本工作中，我们重新审视了最小对 paradigm。</sample>
    <sample id="565">最小对偶范式，MPP，评估语言模型使用序列概率的相对差异来评估语言模型的抽象知识：。BLiMP。1. 许多人在帮助自己。2. *许多人帮助她自己。P，1， &gt; P，2，SyntaxGym。1. 没有顾客...花过钱。2. *顾客...花过钱。P，1.any， ≥ P，2.any，Crows。1. 刻板印象句子。2. 非刻板印象句子。P，1， &gt; P，2，所以最小对偶范式基本上在可接受性判断的基础上评估语言模型，这些判断也可以包括语法性，如BLiMP语法Gym，或在刻板印象方面的可接受性，如Crows范式。</sample>
    <sample id="566">在最小对偶范式中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子。</sample>
    <sample id="567">最小对 paradigm 评估语言模型使用序列概率的相对差异来评估语言模型的抽象知识：。BLiMP。1. 许多人在帮助自己。2. *许多人帮助她自己。P，1， &gt; P，2，SyntaxGym。1. 没有顾客…花过钱。2. *顾客…花过钱。P，1.any， &gt; P，2.any，Crows。1. 刻板印象句子。2. 非刻板印象句子。P，1， &gt; P，2，希望模型基本上会将更多概率赋予可接受的句子。</sample>
    <sample id="568">最小对 paradigm，MPP，评估语言模型使用序列概率的相对差异来评估语言模型的抽象知识：。BLiMP。1. 许多人在帮助自己。2. *许多人帮助她自己。P，1， &gt; P，2， SyntaxGym。1. 没有顾客…花过钱。2. *顾客…花过钱。P，1.any， &gt; P，2.any， Crows。1. 女人手工艺很糟糕。2. 男人手工艺很糟糕。P，1， &gt; P，2， 这些判断在长前置语境下稳定吗？当前MPP pipeline基本上不允许我们评估模型对较长句子的接受度。</sample>
    <sample id="569">这些天来，大型语言模型正在使用越来越长的上下文窗口，因此评估模型在整个上下文窗口中的可接受性至关重要。</sample>
    <sample id="570">最小对偶范式，MPP，评估语言模型使用序列概率的相对差异来评估语言模型的抽象知识：。BLiMP：1. 许多人在帮助自己。2. *许多人帮助她自己。P，1， &gt; P，2，SyntaxGym：1. 没有顾客...花过钱。2. *顾客...花过钱。P，1.any， &gt; P，2.any，Crows：1. 女人手工艺很糟糕。2. 男人手工艺很糟糕。P，1， &gt; P，2，我们正在尝试重新审视MPP管道，通过要求模型评估更长序列的可接受性。</sample>
    <sample id="571">所以这就是方法。我们做的是模拟这些较长的序列。我们重新审视数据集本身，然后通过选择可接受或不可接受的句子来重新创建句子。</sample>
    <sample id="572">嗯，比如说，我们从BLIMP数据集中选了一对典型的语法性差异，来自Adjunct Island案例。</sample>
    <sample id="573">我们所做的就是创建更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配。我们从Adjunct Island中提取语法句子。</sample>
    <sample id="574">然后我们把它作为前缀添加到可接受查询和不可接受查询中。</sample>
    <sample id="575">所以，我们可以通过选择相同匹配的不可接受句子来做同样的事情，这也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们也可以通过选择来自不同子集或不同数据集的句子来做同样的事情，这就是我们所说的“不匹配的样本”。</sample>
    <sample id="577">所以这里，句子仍然来自相关数据集，但不是你用来评估的那个数据集。我们也可以对不可接受性的情况做同样的事情。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域，比如维基百科，选择句子。</sample>
    <sample id="579">嗯，这个会告诉我们，像模型的可接受性判断是否真的受任何上下文的影响。</sample>
    <sample id="580">测试MPP判断是否随上下文长度，结构匹配和可接受性变化。</sample>
    <sample id="581">首先，我们看维基百科的句子，它们与当前查询对完全无关，然后我们发现MPP判断在任意上下文长度下大多很稳健。</sample>
    <sample id="582">我们对不同长度的上下文进行MPP评估，包括可接受/不可接受的上下文，匹配/不匹配的结构，长度可达900个词。我们把上下文长度增加到2024个词，以最大化OPT和GPT-2模型的性能，如图中橙色虚线所示，MPP判断相对稳定。</sample>
    <sample id="583">我们进行MPP评估，使用不同上下文，可接受/不可接受，匹配/不匹配结构，长度可达900个词。BLIMP，OPT 6.7B。前缀策略：绿色线，可接受，匹配，橙色线，可接受，不匹配，紫色线，不可接受，匹配，粉色线，不可接受，不匹配。</sample>
    <sample id="584">所以这里我们从同一个BLiMP语法生成数据集中，从可接受和不可接受的领域中选择或创建句子。</sample>
    <sample id="585">我们进行MPP评估，使用不同的上下文—可接受/不可接受；匹配/不匹配结构—长度可达900个词。我们看到，当添加可接受或不可接受的前缀时，MPP判断会显著增加或减少。</sample>
    <sample id="586">但是当我们匹配结构，也就是从同一个现象的责备人文本中选择句子时，</sample>
    <sample id="587">我们看到，根据所选前缀是否可接受，MPP 判断值会有显著增加或显著减少。</sample>
    <sample id="588">嗯，这个图展示的是可接受/不可接受的MPP句子在匹配结构下对模型性能的影响。我们进行了MPP评估，使用了不同的上下文—可接受/不可接受，匹配/不匹配结构—长度可达900个词。BLIMP，OPT 6.7B。</sample>
    <sample id="589">我们以保持相关结构的方式扰动上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀副词：“然而，&lt;sent&gt;。”- 长前缀副词：“首先和最重要的是，&lt;sent&gt;。”- 添加从句：“不管X怎么想，&lt;sent&gt;。”- 引用：“昨天，X说，‘&lt;sent&gt;’。”</sample>
    <sample id="590">我们以保持相关结构的方式对上下文句子进行扰动，并询问模型是否对这些句子同样敏感。- 前缀/后缀副词：“然而，&lt;sent&gt;。”- 长前缀副词：“首先和最重要的是，&lt;sent&gt;。”- 添加从句：“不管X怎么想，&lt;sent&gt;。”- 引用：“昨天，X说，‘&lt;sent&gt;’。”</sample>
    <sample id="591">我们发现这些噪声实际上并没有让模型在如何做出NPP判断方面改变其方向。</sample>
    <sample id="592">为什么匹配前缀会影响语言模型的判断？我们以保持相关结构的方式对上下文句子进行扰动，并询问模型是否对这些句子同样敏感。- 前缀/后缀副词：“然而，&lt;sent&gt;。”- 长前缀副词：“首先和最重要的是，&lt;sent&gt;。”- 添加从句：“不管X怎么想，&lt;sent&gt;。”- 引用：“昨天，X说，‘&lt;sent&gt;’。”模型对扰动句子的敏感性相似。</sample>
    <sample id="593">当我们对可接受域的句子进行扰动时，所有扰动都显示出类似的增加。当我们对不可接受域的句子进行扰动时，MPP判断以类似的方式减少。</sample>
    <sample id="594">我们工作的关键收获是，语言模型对跨句子的潜在语法和语义特征敏感。</sample>
    <sample id="595">目前我们用短的单句输入的方式进行MPP评估，可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。</sample>
    <sample id="596">关键收获
- 语言模型对跨句子的潜在语法/语义特征敏感。
- 使用短的单句输入的MPP评估不能完全捕捉到LMs的抽象知识。
请阅读我们的论文以获取更多实验细节。谢谢聆听。</sample>
    <sample id="597">该方法的第一步将输入词元映射到一个无序的多集类型的词元。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="598">Coscript 中包含了55，000个脚本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="626">DEplain的最佳对齐方法是MASSalign。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="627">弱监督学习能缓解标注瓶颈。</sample>
    <sample id="628">嗯…这个我不太清楚呢。你可以再找找其他资料或者问问其他人。要是你还有其他问题，也可以随时跟我说哦。</sample>
    <sample id="629">CoNLL++ 数据集是从 2020 年的路透社新闻中收集的，并按照 CoNLL-2003 注释指南进行注释。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="630">大家好，我是来自宾夕法尼亚州立大学的于森张。今天我将介绍我们的工作，XSemPLR：跨语言语义解析在多种自然语言和意义表示中的应用。</sample>
    <sample id="631">语义解析是构建用户查询语义表示的任务，例如SQL和Lambda演算。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言的查询翻译成多种语义表示的任务。</sample>
    <sample id="633">如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL，Lambda或FunQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏对某些自然语言的覆盖。</sample>
    <sample id="635">现有的CLSP模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏对某些自然语言的覆盖。</sample>
    <sample id="636">现有的跨语言语义解析模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏某些语义表示的覆盖。</sample>
    <sample id="637">现有的跨语言语义解析模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏对某些语义表示的覆盖。</sample>
    <sample id="638">现有的CLSP模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏对某些神经模型的覆盖。</sample>
    <sample id="639">我们提供了一个统一的跨语言语义解析数据集XSemPLR，适用于多种自然语言和语义表示。它包含：9个不同领域的数据集，5个语义解析任务，8种语义表示，22种自然语言，分布在15个语系中。</sample>
    <sample id="640">它包含9个不同领域的数据集，5个语义解析任务，8种语义表示，以及15个语系中的22种自然语言。</sample>
    <sample id="641">实验设置。我们考虑了六种训练和评估的设置。翻译-测试：使用谷歌翻译API将源语言翻译为目标语言。然后使用单一语言模型进行训练和评估。训练。推理。</sample>
    <sample id="642">实验设置。我们考虑了六种训练和评估的设置。Translate-Test：使用谷歌翻译API将源语言翻译为目标语言。然后使用单语模型进行训练和评估。训练。推理。</sample>
    <sample id="643">实验设置

- 我们考虑了六种训练和评估的设置。
  - Translate-Test：使用Google翻译API将源语言翻译为目标语言。然后使用单语模型进行训练和评估。

训练

推理

例如，我们用英语查询训练英语模型，在推理过程中，使用API将德语查询翻译成英语，然后使用训练好的模型预测SQL。</sample>
    <sample id="644">实验设置。我们考虑了六种训练和评估的设置。单语模型：源语言与目标语言相同，例如德语到德语。我们还测试了单语少量样本设置，通过仅使用10%的训练数据来训练单语模型。训练。推理。</sample>
    <sample id="645">在这个设置中，源语言与目标语言相同，例如德语到德语或英语到英语。</sample>
    <sample id="646">我们考虑了六种训练和评估的设置。单语模型：源语言和目标语言相同，例如德语到德语。我们还测试了单语少量样本设置，通过只用10%的训练数据来训练单语模型。</sample>
    <sample id="647">实验设置。我们考虑了六种训练和评估的设置。多语言模型：为所有语言训练一个统一的多语言模型。</sample>
    <sample id="648">例如，我们将德语，英语和中文查询放在一起训练一个多语言模型，在推理过程中，我们可以使用这个模型。</sample>
    <sample id="649">实验设置。我们考虑了六种训练和评估的设置。多语言模型：为所有语言训练一个统一的多语言模型。训练。推理。要翻译德语查询或中文查询等。</sample>
    <sample id="650">实验设置。我们考虑了六种训练和评估的设置。跨语言零样本/少量样本迁移：在一种源语言上进行训练，然后转移到另一种语言。训练。英语或者英语，德语少量样本。多语言模型。SQL。推理。德语。多语言模型。SQL。</sample>
    <sample id="651">在训练期间，我们用英语查询或者英语和德语的组合少样本查询来训练一个多语言模型，以预测SQL输出。</sample>
    <sample id="652">关于单语模型的分析，我们在两组模型上进行评估。</sample>
    <sample id="653">Enc-PTR，多语言预训练编码器与指针式解码器，如XLM-R + PTR，mBERT + PTR。</sample>
    <sample id="654">我们也在多语言预训练编码器-解码器模型上进行了评估，比如mBART和mT5。</sample>
    <sample id="655">我们发现Enc-Dec，mT5，在所有9个数据集上都取得了最佳性能！</sample>
    <sample id="656">多语言训练分析。我们在mT5和XLM-R + PTR上评估多语言环境。编码-解码/编码-指针（mT5/XLM-R）可以通过在多种语言的混合中进行训练得到改进。</sample>
    <sample id="657">我们发现，编码器-解码器或编码器-PTTR可以通过在多种语言的混合中进行训练来得到改进。</sample>
    <sample id="658">我们发现，大多数主要自然语言可以获得性能提升，除了英语在7个数据集上性能下降，在3个数据集上性能提升。</sample>
    <sample id="659">我们在多语言环境中评估了mT5和XLM-R + PTR。大多数主要的NLs都能获得性能提升，除了英语在7个数据集上性能下降，在3个数据集上性能提升。这被称为“多语言诅咒”。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">在这张图中，蓝色线条表示跨语言少样本迁移，橙色线条表示跨语言零样本迁移，绿色线条表示单语言设置。</sample>
    <sample id="662">我们发现，在零样本设置下，跨语言迁移性能差距显著。在少量样本设置下，差距迅速缩小。</sample>
    <sample id="663">我们还发现了一些其他有趣的研究结果，例如，编码器-解码器优于先前的工作或达到可比的结果。在英语自然语言上的预训练可以显著提高目标自然语言的少样本性能。</sample>
    <sample id="664">多语言语言模型，如Codex和BLOOM，在跨语言语义解析任务中仍然不够。</sample>
    <sample id="665">我们构建了XSemPLR，一个跨语言语义解析的统一基准，包含多种自然语言和意义表示。</sample>
    <sample id="666">我们构建了XSemPLR，一个跨语言语义解析的统一基准，支持多种自然语言和意义表示。我们对三种代表性的多语言模型类型进行了全面的基准研究。我们的结果显示，使用单语训练的mT5在性能上表现最佳，而值得注意的是，多语言LLMs在执行跨语言语义解析任务方面仍然不够。此外，单语训练和跨语言迁移学习之间的性能差距仍然显著。我们对三种代表性的多语言模型类型进行了全面的基准研究，我们的结果显示，使用单语训练的mT5在性能上表现最佳，而值得注意的是，多语言LLMs在执行跨语言语义解析任务方面仍然不够。此外，单语训练和跨语言迁移学习之间的性能差距仍然显著。欢迎访问我们的论文和代码！</sample>
    <sample id="667">嗯…有参数基水印，可适用性差，有词基水印，适用于EaaS，有后门基水印，适用于EaaS，还有对抗基水印，适用于EaaS。你要是还想知道更多关于这方面的事儿，可以再问我哦。</sample>
    <sample id="668">不足够。</sample>
    <sample id="695">该方法通过在训练中诱导排列来处理排列的不确定性。</sample>
    <sample id="696">嗯…这个嘛，下游NLP模型的公平性，就是说在处理不同政治倾向的数据时，模型的输出应该保持一致，不能因为政治倾向不同就产生偏差。比如说，对于仇恨言论和错误信息，模型在不同政治倾向的用户那里，应该给出相似的判断和处理结果。这样就能避免因为政治倾向导致的不公平对待。你要是还有啥想法或者疑问，咱们可以再聊聊。</sample>
    <sample id="697">Yanis Labrak</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="700">嗯…在本文背景下，热带主义可能意味着对拉丁裔女性的描述，比如“vibrant”和“curvaceous”这些词，这些词反映了拉丁裔女性充满活力和曲线美的形象。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="701">作者通过本质化叙述来创建目标群体的人工描写，用“culture， tradition， proud， exotic”这些词来定义这些群体，只根据他们的身份来区分。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="702">本文中使用了P-CXMI来衡量语境使用情况。</sample>
    <sample id="703">DrBERT 是从 scratch 开始训练的，ChuBERT 也是从 scratch 开始的。嗯，你要是还有啥疑问可以接着问哈。</sample>
    <sample id="751">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="752">迭代迁移学习就是通过训练最新收集的数据集来更新模型。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="753">数据集的目标是理解用户在做选择时的语言。</sample>
    <sample id="754">嗯…这个嘛，攻击者是通过在 EaaS 上运行恶意代码来提取模型参数的。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="755">三位。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="756">嗯…这个信息图里没提到有多少个注释者用于创建初始数据集呢。你可以再找找其他资料看看。要是还有啥问题，你可以再跟我说哦。</sample>
    <sample id="757">这篇论文的作者所属机构有卡内基梅隆大学，华盛顿大学和艾伦人工智能研究所。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="758">“I saw Bart and Lisa; Homer came and sneezed”。</sample>
    <sample id="759">嗯…这个信息里没提到对话系统中最先进的模型呢。你可以再找找其他资料或者咱们再聊聊看。</sample>
    <sample id="760">嗯…因为现在大型语言模型的上下文窗口越来越长了，所以评估模型在整个上下文窗口中的可接受性就变得很关键了。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="761">嗯，是的。多语言训练会导致表现下降，特别是在英语上，有7个数据集表现下降，只有3个数据集表现上升。你要是还有啥疑问，尽管问哈。</sample>
    <sample id="762">注释者提前知道该实体。</sample>
    <sample id="763">嗯…这个图片里没提到使用了哪些MT指标呢。你可以再找找其他资料或者再确认下问题哦。</sample>
    <sample id="764">嗯…这个我不太确定呢。泛化中的回归可能会影响特定的NER类型，但具体影响程度我不太清楚。你要是有更多资料或者想法，咱们可以再聊聊呀。</sample>
    <sample id="765">嗯…立场在NLP里很重要呢。它能帮助算法理解文本背后的态度和观点，这样在处理评论，新闻之类的文本时，就能更好地判断内容的倾向性，像检测毒性和识别偏见这些。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="766">嗯…这个嘛，BLOOM这种多语言LLM通常是采用适配器微调的方式。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="767">他们使用RoBERTA-base + classifier head模型进行迁移学习。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="768">嗯…这个我不太清楚呢。你可以去查查相关的技术文档或者学术论文，应该能找到答案。要是你找到了，也可以跟我说说呀。</sample>
    <sample id="769">三条。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="770">嗯…这个信息里没提到与最强基线相比具体获得了多少收益呢。你可以再找找其他资料或者问问其他人。要是还有啥问题，你可以再跟我说哦。</sample>
    <sample id="771">演讲者的名字是Shuheng Liu。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="772">可以。</sample>
    <sample id="773">嗯…他们进行了三个较小模型的实验。如果还有其他问题，你可以再问我哦。</sample>
    <sample id="774">OFA被用作研究多模型指令调整的基础模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="833">这篇论文的作者所属机构是Google Translate。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="834">Stony Brook University。如果还有其他问题，随时跟我说哦。</sample>
    <sample id="835">嗯…这个信息里没提到论文分析了哪些语言对呢。你可以再找找其他资料或者问问作者。</sample>
    <sample id="836">Shangbin Feng。如果还有其他问题，随时告诉我哦。</sample>
    <sample id="837">研究了finetuned long-mBART和finetuned mBART这两个模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="838">53个任务用于训练，10个任务用于测试。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="839">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="840">作者在实验中使用了AG News，MIND，SST2，Enron Spam这四个数据集。</sample>
    <sample id="876">NACHOS 是一个医疗领域的数据集，包含从网络上抓取的医疗数据。</sample>
    <sample id="877">演讲者的名字是David Vilar Torres。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="878">提示策略对结果影响很大，516句中有1000句，也就是超过一半的句子，BLEURT得分差异超过1分，最大差异能达到40分。</sample>
    <sample id="879">Carnegie Mellon University Language Technologies Institute，TÉCNICO LISBOA，BAIR，Berkeley Artificial Intelligence Research，Unbabel。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="880">抱歉，图片中的英文内容没有提到5个由专家编写的指令。</sample>
    <sample id="881">作者建议通过核心ference分辨率任务来测试模型，这个任务可以探测模型在不同来源获取知识的能力。</sample>
    <sample id="882">大家好，我叫David Vilar，我将给大家简单介绍一下论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。这是我和Google Translate同事们的联合工作。</sample>
    <sample id="883">PaLM是去年2022年提出的540亿参数的大型语言模型。它是在包含7800亿个文本令牌的大型文本集合上进行训练的。</sample>
    <sample id="884">在数据应用方面，它在数百个NLP任务中达到了最先进的水平。</sample>
    <sample id="885">在这项工作中，我们呈现了第一个系统研究大型语言模型提示在机器翻译中的应用。</sample>
    <sample id="886">我们使用MT社区的最佳实践评估了这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。</sample>
    <sample id="887">我们比较了最先进的系统，也就是WMT评估中表现最好的系统。</sample>
    <sample id="888">我们使用最先进的神经机器翻译指标，并且还展示了专家基于人类的评估结果。最后，我们提供了一些关于提示选择策略的建议。</sample>
    <sample id="889">提示对翻译质量有很大影响。对于每个句子，选择两个随机提示。为每个句子-提示对计算BLEURT。大多数句子，1000个中有516个，显示差异超过1个BLEURT点。差异可高达40个BLEURT点！</sample>
    <sample id="890">大多数句子，1000个句子中有516个，BLEURT得分差异超过1分。</sample>
    <sample id="891">提示对翻译质量有很大影响。为每个句子选择两个随机提示。为每个句子-提示对计算BLEURT。大多数句子，1000个中有516个，显示差异超过1个BLEURT点。差异可高达40个BLEURT点！</sample>
    <sample id="892">在我们的实验中，我们选择了一种五样本提示策略，其中我们只是标记出我们提供给系统的句子，用它们的语言来表示。</sample>
    <sample id="893">在从德语翻译成英语的这个例子中，德语句子被标记为“German:”，英语翻译被标记为“English:”。</sample>
    <sample id="894">我们看到，在几轮提示的情况下，提示的具体形式对结果影响不大。</sample>
    <sample id="895">在我们的案例中，当切换到五样本提示时，提示的实际形式几乎没有区别。</sample>
    <sample id="896">一个目击者报警，警察带着几名警员赶到。</sample>
    <sample id="897">实验结果的总结是，示例质量比与源句子的相似性更重要。</sample>
    <sample id="898">所以，选择高质量翻译的示例很重要。特别是，我们比较了从WMT训练数据或测试数据中选择提示。</sample>
    <sample id="899">开发数据的创建质量更高，比训练数据更精细，结果因此在使用开发数据时表现更好。</sample>
    <sample id="900">然而，专门的SOTA系统在帕姆翻译中具有显著优势，但帕姆在商业系统中表现得相当接近，我们选择与谷歌翻译重叠。</sample>
    <sample id="901">我们使用MQM框架进行的人工评估表明，PaLM的流畅性与最先进的系统相当，但主要差异在于准确性。</sample>
    <sample id="902">特别地，最常见的错误是遗漏错误。</sample>
    <sample id="903">嗯，看起来PaLM在某些情况下会选择生产更好的翻译，有时会去掉源句子中在翻译中不重要的部分。</sample>
    <sample id="904">然而，PaLM的“风格/笨拙”类别低于最先进的系统，这是另一个信号。</sample>
    <sample id="905">帕尔姆提供的输出真的很流畅，但准确性方面仍有一些问题。</sample>
    <sample id="906">这就是这个简短概述的全部内容了。如需更多细节，请参加我关于论文的完整报告。非常感谢。</sample>
    <sample id="907">你好，我是来自德国萨尔兰大学的博士生。在这段视频中，我想要展示我们最近的研究成果《弱于你想象：对弱监督学习的批判性审视》。</sample>
    <sample id="908">这是 Dawei Zhu，Xiaoyu Shen，Marius Mosbach，Andreas Stephan 和 Dietrich Klakow 的联合工作。</sample>
    <sample id="909">弱监督缓解了标注瓶颈。但是弱标签是噪声！噪声记忆会损害泛化能力。弱监督学习，WSL，训练在噪声数据上训练的模型，但能很好地泛化。</sample>
    <sample id="910">弱监督学习为什么重要？- 弱监督缓解了标注瓶颈。- 但是弱标签是噪声！噪声记忆会损害泛化能力。- 弱监督学习，WSL，- 在噪声数据上训练模型，尽管如此，模型仍能很好地泛化。</sample>
    <sample id="911">与人类注释相比，弱注释更便宜，但它们也很嘈杂，这意味着一定比例的注释是不正确的。</sample>
    <sample id="912">如果直接用弱标签数据训练神经网络，神经网络倾向于记住标签噪声，而不能泛化。</sample>
    <sample id="913">弱监督学习被提出，以在存在标签噪声的情况下稳健地训练神经网络，从而确保训练模型仍然具有良好的泛化能力。</sample>
    <sample id="914">在最近的弱监督学习，WSL，工作中，一个常见的说法是，人们声称他们仅在弱标记数据上训练模型，并在干净的测试集上获得高性能。</sample>
    <sample id="915">最近WSL工作中常见的说法是：“我们只在弱监督数据上训练模型，并且达到XX%的准确率。”。从技术上讲，这个说法没有错，但有个陷阱。</sample>
    <sample id="916">人们假设有一个额外的干净验证集可用于模型选择。</sample>
    <sample id="917">在弱监督学习中，我们不能忽视额外的手动标注需求。但就像房间里的大象一样，这种必要性经常被忽视。</sample>
    <sample id="918">你说的英文内容是关于研究问题的。</sample>
    <sample id="919">如果需要干净数据，或者干净数据是WSL工作的必要条件，那么我们需要多少干净样本？最后，我们是否应该仅使用干净样本进行验证，还是有其他更好的利用方式？</sample>
    <sample id="920">我们在工作中解决了这些研究问题，我们的发现如下。</sample>
    <sample id="921">首先，我们发现有趣的是，最近的弱监督方法确实需要干净的弱标签样本才能正常工作。</sample>
    <sample id="922">否则，性能会大幅下降，如图所示。如果没有干净的验证样本，训练好的模型无法泛化到原始的弱标签之外。</sample>
    <sample id="923">这意味着弱监督训练毫无意义。</sample>
    <sample id="924">这表明弱监督方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。</sample>
    <sample id="925">我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法获得更好的性能，如图所示。</sample>
    <sample id="926">通常，我们只需要每个类20个样本就能达到高性能。</sample>
    <sample id="927">但故事还没有结束，因为如果我们决定访问干净的样本，直接在它们上进行训练甚至会获得更好的性能。</sample>
    <sample id="928">右边的图显示了直接应用在干净数据上的微调方法和仅用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">从图中我们可以看到，如果每个类别有10个样本，直接微调开始超越WSL方法。</sample>
    <sample id="930">最后，之前WSL方法所声称的性能提升可以通过允许在干净的验证样本上继续微调而轻松实现。</sample>
    <sample id="931">从图中可以看出，FTw模型在初始阶段表现不如更复杂的WSL方法，如COSINE。</sample>
    <sample id="932">然而，如果我们允许继续在干净样本上微调，那么FTw的表现与其他方法一样好。</sample>
    <sample id="933">在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">总结：我们展示了最近的WSL方法需要干净的，手动标注的样本才能正常工作，它们的性能和实用性被严重高估了。</sample>
    <sample id="935">结论
最近的WSL方法
- 需要干净的样本。
- 高估了它们的实用性。
表情符号
我们的建议
- 报告模型选择标准。
- 以少样本学习方法作为基线。
- 总是应用连续微调，CFT，</sample>
    <sample id="936">结论
最近的WSL方法
- 需要干净的样本。
- 高估了它们的实用性。，不开心的表情符号，我们的建议
- 报告模型选择标准。
- 使用少样本学习方法作为基线。
- 总是应用连续微调，CFT，</sample>
    <sample id="937">第二，WSL方法应该与基于少量样本的学习方法进行比较，因为两者都处理干净的样本。第三，连续微调是一个简单而强大的基线，应该在未来WSL的工作中加以考虑。</sample>
    <sample id="938">结论
最近的WSL方法
- 需要干净的样本。
- 高估了它们的实用性。
表情符号
我们的建议
- 报告模型选择标准。
- 使用少样本学习方法作为基线。
- 总是应用连续微调（CFT）。
最后，我们开源了代码。您可以通过本页的二维码找到它。请随意查看。谢谢，祝您会议愉快。</sample>
    <sample id="939">常用评估方法是让人类评委来评价，比如让人类评委挑选两个对话中哪个更好，或者给对话打分，用Likert量表。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="940">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要的背景知识是“法官在法庭上决定案件”。你要是还有啥疑问，尽管跟我说哈。</sample>
    <sample id="942">代码是公开的，可以在GitHub上获取，网址是mpoemsl/kitmus。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="943">嗯…从你给的这个内容来看，没有提到关于NLPositionality的注释者在各人口统计学特征方面是否均衡的信息呢。你还有其他关于这个内容的问题吗。</sample>
    <sample id="944">嗯…在可接受的域中扰乱句子，就是通过添加一些像“However”这样的前缀，或者“First and foremost”这样的长前缀，还有“Regardless of what X thinks about it”这样的附加语句，或者“Yesterday， X said”这样的引语。这些方式来扰乱句子，同时保持句子的结构。你要是还有啥疑问，尽管问哈。</sample>
    <sample id="945">进行维度评估就是对对话质量的多个方面进行评价，像相关性，一致性，情感理解这些。这样能更细致地了解模型的优缺点。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="946">这篇论文的作者所属机构有University of Science and Technology of China，Microsoft Research Asia，Beijing Jiaotong University，Sony AI和Microsoft STC Asia。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="947">在零和一射提示的情况下，提示的形式很重要。</sample>
    <sample id="978">作者评估了BART-FID-RAG，Blender2，Emora和Blender-Decode这四个对话模型。如果还有其他问题，欢迎随时提问。</sample>
    <sample id="979">这篇论文有8位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="980">优秀规划器应该能写合理且忠实于约束的脚本。</sample>
    <sample id="981">这篇论文有8位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="982">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="983">作者所属机构是波兰科学院计算机科学研究所和华沙大学。</sample>
    <sample id="1021">PaLM最常见的错误是遗漏错误。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将告诉你关于ABC eval的所有内容，这是一种评估对话型AI的新维度方法。</sample>
    <sample id="1023">这项工作由佐治亚大学的Emory NLP实验室，由金浩·崔教授领导，与亚马逊Alexa AI合作完成。</sample>
    <sample id="1024">假设你刚刚开发了一个对话模型，想要看看它与当前最先进的对话模型相比表现如何。</sample>
    <sample id="1025">常见的做法是使用人类评估，比如让人类法官选择两个对话中哪个更好，或者根据Likert量表对对话进行评分。</sample>
    <sample id="1026">这些方法很好地提供了对整体对话质量的全面评估，但对话质量有很多方面。因此，你可能需要评估对话质量的多个维度，以了解模型在更细致层次上的优缺点。</sample>
    <sample id="1027">一种方法是让人类法官评估对话质量的几个维度，比如模型回复的相关性，使用现有的比较或李克特量表方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的策略来进行维度对话评估。</sample>
    <sample id="1029">我们的方法试图通过明确标注每个模型响应是否表达某些行为，如回答无关信息或自相矛盾，来减少人类评估的主观性。</sample>
    <sample id="1030">我们称这种方法为“聊天行为注释”或简称“ABC-Eval”。我们开发这个方法是为了全面覆盖最近文献中建议会影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC-Eval能够测量聊天模型犯各种主题错误的频率。</sample>
    <sample id="1032">例如，ABC-Eval衡量聊天模型在对话中忽略伙伴或说不相关的话的次数。</sample>
    <sample id="1033">当模型自相矛盾或者与伙伴矛盾，产生不正确的事实或者违背常识知识，以及当模型成功或失败地表现出共情。</sample>
    <sample id="1034">为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval在每个模型上进行了100次人机对话的评估。</sample>
    <sample id="1035">为了比较，我们还使用了三种现有方法来评估这些对话：在回合级上的Likert评分，在对话级上的Likert评分以及在对话级上的两两比较。</sample>
    <sample id="1036">对于现有的方法，我们收集了对8个最常测量的对话方面进行评估，因为这是评估聊天模型在多个维度上的标准做法。</sample>
    <sample id="1037">从我们对这些评估结果的分析中，我们发现ABC-Eval行为标签在100个双标注对话上的内注释者一致性上，比现有方法收集的标签更可靠。</sample>
    <sample id="1038">此外，ABC-Eval标签比现有方法产生的指标更能预测整体对话质量，如简单线性回归分析所示。</sample>
    <sample id="1039">例如，你可以看到，测量自我和伙伴矛盾的回合比例分别解释了5%和10%的对话质量，而平均Likert一致性评分只解释了4%或更少。</sample>
    <sample id="1040">最后，我们使用逐步线性回归来检查每个评估指标是否能捕捉到聊天质量的独特方面。</sample>
    <sample id="1041">你可以看到，所有ABC-Eval指标的组合解释了超过25%的对话质量，当你一次移除一个指标时，大多数都会损失相当一部分关于质量的信息。</sample>
    <sample id="1042">另一方面，所有回合级Likert指标的组合解释了质量的更少部分，并且这些指标中携带独特信息的更少。</sample>
    <sample id="1043">这些可靠的ABC-Eval指标使我们能够以比以前方法更高的分辨率评估对话式AI。</sample>
    <sample id="1044">你可以看到，在我们的实验结果中，有几个挑战仍然存在，并且被精确量化了。例如，我们测试的机器人在大约20%的响应中存在常识性错误。</sample>
    <sample id="1045">他们在大约15%的回复中产生无关信息，大约10%的时间会自相矛盾或与对方矛盾。</sample>
    <sample id="1046">随着该领域改进速度的加快，自我们评估以来，许多这些错误率可能会在新模型发布时下降。然而，这正是更需要追求可靠和精确的模型比较评估指标的原因。</sample>
    <sample id="1047">我们希望ABC-Eval能被该领域的其他人作为有意义的一步加以利用，并期待看到未来几个月和几年内对话式AI的发展。谢谢观看。</sample>
    <sample id="1048">Emory University。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1049">CFT代表连续微调。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1050">这篇论文有七位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1051">你好，我的名字是Kayo Yin，我将要展示我们名为《翻译何时需要上下文：基于数据的多语言探索》的工作。这项工作是与Patrick Fernandes，Emmy Liu，André F. T. Martins和Graham Neubig合作完成的。</sample>
    <sample id="1052">我们得除掉那个间谍。</sample>
    <sample id="1053">如果前面的句子是“Things could start to get dangerous if the ministers find out.”，那么“mole”指的是间谍。但如果前面的句子是“Could it be anything serious， Doctor？”那么“mole”指的是胎记。</sample>
    <sample id="1054">医生，这可能是很严重的事吗？我们必须把那个痣去掉。</sample>
    <sample id="1055">然而，评估模型在处理像这样的案例时的表现相当困难。首先，因为只有少量的翻译依赖于上下文，这使得像BLEU这样的语料库级度量无法捕捉这些翻译。</sample>
    <sample id="1056">评估上下文相关翻译很难。只有少量单词依赖于上下文。——语料库级度量。现有方法支持有限的语篇现象和语言。</sample>
    <sample id="1057">在这项工作中，我们试图回答这两个问题。首先，翻译何时需要上下文，其次，模型在处理这些情况时表现如何。</sample>
    <sample id="1058">为了回答第一个问题，我们首先测量了在翻译中单词对上下文的依赖程度。</sample>
    <sample id="1059">在之前的工作中，我们引入了条件交叉互信息，CXMI，作为衡量机器翻译模型使用上下文的指标。这是通过测量上下文C在给定源X的情况下，提供了多少关于目标Y的信息来实现的。</sample>
    <sample id="1060">你可以将CXMI看作是给模型提供上下文后获得的信息增益。</sample>
    <sample id="1061">在这项工作中，我们扩展了CXMI到点对点CXMI，它可以衡量在句子级别或单词级别上对上下文的使用。我们可以将具有高P-CXMI的单词视为需要上下文进行翻译的单词。</sample>
    <sample id="1062">现在我们分析具有高P值的单词，以寻找这些单词之间的模式。</sample>
    <sample id="1063">我们对14种不同语言的TED演讲的英译文本进行分析。</sample>
    <sample id="1064">我们对高P-CXMI词的词性标签进行分析。</sample>
    <sample id="1065">这让我们能够找到，例如，阿拉伯语中具有较高P-CXMI的二数代词。这可以解释为，英语没有二数代词，所以在翻译成阿拉伯语时，需要上下文来确定代词是否为二数。</sample>
    <sample id="1066">同样，我们发现某些语言在选择合适的动词形式时也需要上下文。然后我们查看了所有不同出现中具有高P-CXMI值的词汇项。</sample>
    <sample id="1067">这有助于识别像这里这样的情况，在中文中，你需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。</sample>
    <sample id="1068">同样，我们发现上下文支持了翻译成正确的正式性。如果还有其他翻译相关的问题，或者想讨论这个翻译的细节，都可以跟我说哦。</sample>
    <sample id="1069">最后，我们看具有高P-CXMI的单个词，这让我们能够识别出不能仅由单词本身捕捉到的现象，但这些现象在句子结构中表达得更明显，比如省略。</sample>
    <sample id="1070">现在我们利用分析结果来设计一个文档级翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五种话语现象，我们创建了自动识别与现象相关的单词的标记器，我们称我们的标记器为多语言话语意识，MuDA，标记器。</sample>
    <sample id="1072">然后我们还可以注意到，不同的语言在这些话语现象中的比例是不同的。</sample>
    <sample id="1073">然后我们使用MuDA标记器，通过将其应用于我们想要用于评估的平行语料库。我们选择性地应用我们的翻译指标，对MuDA标记器识别的上下文相关示例进行评估。</sample>
    <sample id="1074">最后，我们使用基准以及其它指标来评估不同模型在文档级机器翻译上的表现。</sample>
    <sample id="1075">首先，当我们使用语料库级指标时，对于BLEU，我们发现无上下文模型的性能最好。</sample>
    <sample id="1076">但是如果我们使用Comet，有上下文的模型表现最好。如果我们使用Word F-measure，有上下文和没有上下文的模型性能相当。</sample>
    <sample id="1077">这再次表明，如果我们仅使用语料库级别的指标，很难确定最佳的文档级翻译系统。</sample>
    <sample id="1078">现在我们使用MuDA基准来评估模型，我们发现上下文感知模型在某些离散现象，如正式性和词汇一致性方面，比不使用上下文的模型更准确。</sample>
    <sample id="1079">但是这些模型在其他现象，如省略，代词和动词形式上，表现得并不比不使用上下文的模型好很多。这暗示了在文档级翻译方面，我们需要看到更多的进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试表明DeepL在文档级翻译中通常比谷歌翻译更准确。</sample>
    <sample id="1081">总结：我们对14个语言对进行数据驱动分析，以识别何时需要上下文。</sample>
    <sample id="1082">然后我们利用我们的发现来构建一个文档级机器翻译的基准，这有助于我们识别模型在处理哪些话语现象方面表现良好，以及哪些翻译系统在文档级翻译方面表现优秀。</sample>
    <sample id="1083">感谢大家的聆听，我们将在多伦多见。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1121">这个新方法有名称，叫“Permuting with 'jumps'”。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1122">作者说“显性词汇”方法是找那些能区分标记组和未标记组的词。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1123">作者所属机构有Paul G.Allen School，University of Washington，Carnegie Mellon University Language Technologies Institute。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1124">Bouquet/Stanford。</sample>
    <sample id="1125">演讲者的名字是James Finch和Sarah Finch。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1126">这篇论文有四位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1127">BLiMP和SyntaxGym可以用于测试句法现象。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1161">FTw，BOND，COSINE，MLC，L2R。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1162">该模型在11个任务上进行了评估。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1226">CamemBERT最初是在138GB的数据上训练的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1227">演讲者的名字是Adam Przepiórkowski。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1228">发现是随着时间间隔越大，性能下降越严重。</sample>
    <sample id="1269">因为输出序列中的词元在第一步后没有按正确顺序排列。</sample>
    <sample id="1270">因为不知道积极刻板印象是不是因为过度的价值对齐或者其他反刻板印象方法导致了有害模式。</sample>
    <sample id="1271">嗯…最小对不可接受输入就是一种评估语言模型的方法。它通过比较可接受和不可接受的句子序列概率来评估语言模型的抽象知识。比如说，在BLiMP里，一个可接受的句子是“Many people were helping themselves.”，而一个不可接受的句子是“*Many people were helping herself.”。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1272">作者使用了F1，NER，CLS等评估指标。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1273">使用了Krippendorff's Alpha这个指标来衡量注释者之间的一致性。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1274">在不可接受和可接受查询中，选择Wikipedia这个领域来添加完全无关的句子。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">嗯…MultiInstruct和其他基准不同之处在于，它有超过1600个语言任务的指令数据集，而没有大规模公开的多模态指令任务数据集。你要是还有啥想知道的，可以再问我哦。</sample>
    <sample id="1277">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1278">抱歉，你提供的音频内容中并没有提到二进制协调的定义，所以我无法回答这个问题。你可以再给我点其他信息吗？</sample>
    <sample id="1279">抱歉，图片内容没有提到提示语的平均长度，所以我无法回答这个问题。你可以再找找其他信息吗？</sample>
    <sample id="1280">嗯…这些发现表明，当较小的T5模型在合适的语料库上进行适当训练时，它们可以超越大型语言模型。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="1281">嗨，我是Yanis Labrak，我将向大家介绍我们关于DrBERT的研究。DrBERT是法语中一个稳健的预训练模型，适用于生物医学和临床领域。</sample>
    <sample id="1282">在这次演讲中，我们首先讨论了医疗保健中的语言建模，然后将呈现我们文章的主要贡献。</sample>
    <sample id="1283">我们介绍了第一个法语生物医学模型DrBERT，它是基于RoBERTa训练的，使用了NACHOS数据集，NACHOS是网络上医疗爬取数据的集合。</sample>
    <sample id="1284">我们还介绍了模型在多种预训练设置和数据源下的比较。然后，我们展示了在11个法语的生物医学和临床自然语言处理任务上的结果。</sample>
    <sample id="1285">最后，我们总结了实验结果，并给你更多关于如何访问这些模型的细节。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，并在性能上远超历史上的静态和上下文化方法，如Word2Vec，Text2Vec或Ensemble。</sample>
    <sample id="1287">自那时以来，这个模型已经被改编到许多其他语言，比如法语，使用CamemBERT，还有生物医学领域，使用PudMedBERT和BioBERT，以及临床领域，使用ClinicalBERT，但主要是在英语中。</sample>
    <sample id="1288">其他语言的专门模型很少见，通常基于持续预训练，因为缺乏领域内的数据。</sample>
    <sample id="1289">然而，法语目前还没有针对生物医学领域的开源模型。</sample>
    <sample id="1290">我们问自己，对于各种用途来说，最合适的数据源是什么，这些公共数据是临床数据的良好替代品。</sample>
    <sample id="1291">为了回答这个问题，我们比较了DrBERT模型和我们的ChuBERT模型，ChuBERT模型基于匿名数据，这些数据是从南特大学医院的数据仓库中获取的。</sample>
    <sample id="1292">我们问自己，训练一个专门针对法语数据的模型需要多少数据？是4GB还是更多？</sample>
    <sample id="1293">我们首先训练并比较了四个从零开始的模型。第一个版本的DrBERT使用了7GB的NACHOS数据集，第二个版本使用了4GB的NACHOS数据集。</sample>
    <sample id="1294">临床模型的初始版本，包含4GB的句子，来自临床笔记。临床模型的最终版本，混合了4GB的NACHOS数据集和4GB的临床笔记。</sample>
    <sample id="1295">除了这个比较，我们还介绍了三种在持续预训练下训练的模型，以分析预训练策略的影响。</sample>
    <sample id="1296">一个基于CamemBERT，用4GB的NACHOS数据集训练。另一个也基于CamemBERT，但这次用的是4GB的ClinNotes数据集。</sample>
    <sample id="1297">最后，基于英语生物医学模型PubMedBERT，并在NACHOS的4GB子集上进行训练，我们总共有7个模型。</sample>
    <sample id="1298">为了评估所有7个模型，我们收集了多个公开和私有数据集，包括实体识别，分类，实体关系抽取和问答等任务。</sample>
    <sample id="1299">这些模型与6个基准模型进行了比较，包括CamemBERT OSCAR 138 GB，CamemBERT OSCAR 4 GB，CamemBERT CCNET 4 GB，PubMedBERT，BioBERT v1.1和ClinicalBERT。</sample>
    <sample id="1300">嗯，这个表格展示了13个模型在11个任务上的表现，包括公共和私有数据源。我们的微调模型在几乎所有任务上都取得了最先进的结果。</sample>
    <sample id="1301">然而，我们可以观察到，来自异源数据源的数据似乎更加通用。我们还观察到，使用更多的数据转化为更好的性能。</sample>
    <sample id="1302">总体而言，从零开始的预训练似乎在大多数任务上获得更高的性能。</sample>
    <sample id="1303">然而，我们使用PubMedBERT的权重和分词器在NACHOS的4GB子集上进行的持续预训练实验，其结果与从头开始训练的DrBERT 4GB的实验结果相当。</sample>
    <sample id="1304">基于CamemBERT权重和分词器的模型并非如此，这些模型存在稳定性问题。</sample>
    <sample id="1305">最后，我们的系统在11个下游任务中的9个任务上提供了更好的性能，并且在全球上超过了通用模型CamemBERT的结果。</sample>
    <sample id="1306">我们还观察到，专门的数据更好，更多的专门数据更好，但它并不容易扩展。</sample>
    <sample id="1307">来自NACHOS的预训练模型是免费的，可以在GitHub上找到，还有所有的训练脚本也在GitHub仓库里。</sample>
    <sample id="1308">谢谢这次的演讲，我们期待在多伦多的海报交流会。</sample>
    <sample id="1309">论文研究了从零开始构建完整模型和使用现有预训练模型进行持续预训练这两种学习策略。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1310">嗯…从图上来看，最佳拟合线的斜率大于1，这意味着每在CoNLL-2003上改进一个单位，就会在CoNLL++上改进超过一个单位。这表明没有边际效益递减，所以过拟合因素不大。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="1311">嗯…他们用SARI，BLEU，BS-P和FRE这些指标来评估简化质量的。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="1312">有。</sample>
    <sample id="1313">嗨，我的名字是马蒂亚斯·林德曼，今天我将给大家简要介绍一下我们关于“无树的组合泛化：使用多集标记和潜在置换”的论文。</sample>
    <sample id="1314">这是我和我的导师Alexander Koller和Ivan Titov的联合工作。</sample>
    <sample id="1315">组成泛化能力是指学习者处理更深的递归和在训练期间单独看到的短语的未见组合的能力。</sample>
    <sample id="1316">在语义解析的上下文中，组合泛化测试可能看起来像这样：像往常一样，我们有一个训练集的陈述，在这种情况下是“the girl slept”和“Mary knew that the girl slept”。</sample>
    <sample id="1317">这些表达式与逻辑形式配对，这些逻辑形式代表了它们意义的核心方面。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不是来自相同分布，而是包含结构上未见过的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练时看到了浅层递归，而在测试时则测试了深层递归的示例。</sample>
    <sample id="1320">朴素的序列到序列模型在处理这种分布外泛化时会遇到困难，经常产生与输入脱节的输出。</sample>
    <sample id="1321">特别是，它们常常无法重现输入和输出之间的系统对应关系，例如图中用颜色编码表示的那种。</sample>
    <sample id="1322">一种流行的方法是将树结构整合到模型中。</sample>
    <sample id="1323">树旨在捕捉将陈述句与逻辑形式联系起来的组合过程。</sample>
    <sample id="1324">树结构很有帮助，但是…树结构需要通过预处理或后处理逻辑形式来获得。</sample>
    <sample id="1325">树结构很有帮助，但…树结构需要通过预处理/后处理逻辑形式来获得。这可能很复杂，有时计算成本很高。通常，这涉及到对逻辑形式的大量形式特定预处理，例如，处理变量符号。</sample>
    <sample id="1326">获取树状结构可能还需要专门的语法归纳程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树结构，而是引入了一种神经序列到序列模型，该模型直接建模输入片段与输出片段之间的对应关系。</sample>
    <sample id="1328">这是论文：神经序列到序列模型，直接建模片段间的对应关系。这是第一次，我们展示了在不依赖树的情况下，对更深的递归有很强的泛化能力。</sample>
    <sample id="1329">我们的方法从输入中预测输出分为两步。</sample>
    <sample id="1330">首先，我们为每个输入令牌打上一个输出中将出现的令牌的无序多集的标签。</sample>
    <sample id="1331">在第一步之后，我们有了所有正确的标记，但它们没有排序。</sample>
    <sample id="1332">在第二步中，我们使用另一个模型来预测排列，将它们放入正确的顺序。</sample>
    <sample id="1333">我们提出了一种新方法来预测排列，它不给可能的排列设置任何硬约束。这使得我们的方法相当灵活和表达能力强。</sample>
    <sample id="1334">从概念上讲，我们的排列模型大致是这样工作的。</sample>
    <sample id="1335">从左到右遍历输出，确定每个位置应放置的多集标记。对于第一个输出位置，我们简单地选择一个，如红色高亮所示。</sample>
    <sample id="1336">然后我们跳到下一个多元集标记来确定输出中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都被恰好访问一次。</sample>
    <sample id="1339">为了给你一些实验结果的预览，我们在这里将我们的方法与其他无树模型在COGS基准上进行比较。我们的方法在对更深层次递归的泛化方面明显优于其他方法。</sample>
    <sample id="1340">不过，其他类型的结构泛化仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了一些有趣的技术挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个源，这对训练提出了挑战。</sample>
    <sample id="1343">此外，有时存在多个与数据一致的排列，但语义上正确的那个是隐含的。我们通过在训练中诱导排列来解决这个问题。</sample>
    <sample id="1344">我们的置换方法非常灵活，但它带来了挑战，即找到最高得分的置换是NP难的。这是因为这与旅行商问题有关。</sample>
    <sample id="1345">我们用GPU友好的连续松弛来近似，这也能让我们反向传播通过解决方案，并学习到在语言上更合理的排列。</sample>
    <sample id="1346">如果你想要了解更多关于我们实验以及如何应对这些挑战的内容，请查看我们的论文或来参加我们的海报展示。</sample>
    <sample id="1347">认知失调就是两种认知元素，比如想法，行为，信念，它们之间不一致。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1348">GPT-4最倾向于自由派。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1349">嗯，从英语内容来看，累积训练在主动学习时是比迭代训练更有效的。如果还有其他问题，欢迎随时问我哦。</sample>
    <sample id="1350">Sara Papi.</sample>
    <sample id="1351">MuDa基准中的数据是从TED演讲的转录本中获得的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1385">演讲者的名字是Matthias Lindemann。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1386">嗯，跨语言转移呢，就是在一种源语言上进行训练，然后转移到另一种语言上。比如说，先在英语上训练，再转移到德语上。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1387">这篇论文的作者所属机构有Saarland University，Amazon Alexa和University of Vienna。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1388">作者使用了平均延迟和计算感知平均延迟这两种延迟测量方法。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1389">大家好，我是Akshatha。今天我和我的同事Martin一起介绍我们的工作，KITMUS测试：评估多源知识整合。这项工作是麦吉尔大学，Mila和微软研究院的协作成果。</sample>
    <sample id="1390">自然语言理解模型从多种知识源中获取信息，例如参数中包含的知识，通常通过预训练获得，以及在推理时输入中提供的知识。</sample>
    <sample id="1391">最近在诸如问答等任务中的研究显示，模型可以利用预训练时的知识来解决任务。</sample>
    <sample id="1392">但自然语言理解通常也需要在推理时提供的知识。</sample>
    <sample id="1393">约翰在电视上看到了新当选的总统。</sample>
    <sample id="1394">预训练参数可以包含关于总统做什么和电视是什么的信息，但它们不能可靠地知道这个特定实例中的实体约翰是谁或新总统是谁，因为总统可能在预训练后发生了变化。</sample>
    <sample id="1395">因此，对于知识密集型NLU任务的成功模型，需要具备整合和使用预训练时间和推理时间知识的能力。</sample>
    <sample id="1396">在本工作中，我们提出了一套用于知识整合的诊断测试套件。</sample>
    <sample id="1397">我们引入了一个核心指代消解任务，旨在测试利用不同来源知识的能力。我们用人类研究参与者和成熟的指代消解模型来评估数据集。</sample>
    <sample id="1398">Servin是法官，Kea是面包师。Servin和Kea在公园里相遇。在法庭上忙了一整天决定案件后，他很高兴能放松一下。答案：Servin</sample>
    <sample id="1399">Servin 是法官。Kea 是面包师。Servin 和 Kea 在公园里相遇。在法庭上忙了一整天决定案件后，他很高兴能放松一下。[答案：Servin]</sample>
    <sample id="1400">给定代词的消解需要两种类型的信息。第一种是实体特定知识，例如“Servin 是法官”，第二种是背景知识，例如“法官在法院决定案件”。</sample>
    <sample id="1401">一般来说，背景知识是在大型语言模型的预训练过程中学习的，而实体特定知识通常在推理时观察到。</sample>
    <sample id="1402">我们改变这两条信息的可用性，使其要么来自单一来源，要么来自多个来源。</sample>
    <sample id="1403">我们定义了KITMUS的三种设置。首先是典型的设置，背景知识在预训练时就假设是可用的。</sample>
    <sample id="1404">第二，是背景-两者设置。背景知识在预训练时间和推理时间都可用。最后，是背景-推理设置。两种知识类型仅在推理时间可用。</sample>
    <sample id="1405">这个最后的设置特别有趣，因为它模拟了背景知识对于解决任务是必要的，但这些背景知识不是模型预训练数据的一部分的情况。例如，因为自预训练以来新职业已经发展出来了。</sample>
    <sample id="1406">这是关于如何控制知识库中事实和真实来源可用性的示例。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识“政客寻求政府的当选席位”包含在预训练参数中。在理解上下文时，我们提供特定的实体知识“奇切斯特是政客”。</sample>
    <sample id="1408">在“背景-两者”设置中，我们不仅提供特定实体的信息，还提供关于在推断上下文中政客的背景知识。</sample>
    <sample id="1409">在背景推理设置中，提供虚构职业“mirituer”而不是“politician”，因为“mirituer”不太可能包含在预训练词表中。</sample>
    <sample id="1410">我们使用人类研究参与者和已建立的参考解决方案模型对数据集进行了评估。在本图中，我们展示了在最困难的背景预训练设置变体上，表现最好的模型的结果。</sample>
    <sample id="1411">在没有任务特定训练的情况下，两个模型在KitMOS上表现不佳。然而，当在KitMOS上进行训练时，C2F和BERT4Coref在任务特定训练下都比随机选择表现得更好。</sample>
    <sample id="1412">当在通用关系解析数据集上进行预训练时，模型会学习利用表面线索，而这些线索在测试时对知识图谱来说是无用的，因为大多数表面线索已被移除。</sample>
    <sample id="1413">额外的实验表明，即使是最优秀的模型，在仅在推理时提供背景知识的情况下，也无法可靠地整合背景知识。</sample>
    <sample id="1414">许多模型似乎无法从多个来源（预训练时间和推理时间知识）进行知识推理。任务特定的训练对于知识整合是必要的。模型在整合推理时间背景知识方面挣扎。</sample>
    <sample id="1415">即使表现最好的模型似乎也难以可靠地整合仅在推理时呈现的背景知识。如果你对更多细节感兴趣，请查看我们的论文并检查GitHub上的数据集和代码。谢谢你的聆听。</sample>
    <sample id="1416">嗯…首先呢，树通常不是直接给出来的，得自己去获取。这过程可能很复杂，计算上也挺费劲的。而且，获取树的时候，得对逻辑形式进行一些特定的预处理，像处理变量符号之类的。有时候还得用到专门的语法归纳程序。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1417">Georgia Institute of Technology.</sample>
    <sample id="1418">嗨，我是Myra。今天我将谈论我们关于“标记的人格”这篇论文，使用自然语言提示来衡量语言模型中的刻板印象。这项工作是与Esin Durmus和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">近年来，许多人已经记录了大型语言模型，LLMs，中社会偏见和刻板印象的普遍存在。</sample>
    <sample id="1420">然而，这些措施有各种各样的局限性。它们通常依赖于手工构建的数据集，这些数据集的构建非常耗时。</sample>
    <sample id="1421">嗯，这些措施通常只衡量非常具体的刻板印象，这意味着它们在其他人口统计群体或语境中不能很好地泛化，或者它们仅仅捕捉到非常宽泛的关联，比如对特定群体的负面关联。</sample>
    <sample id="1422">此外，该领域内的大多数工作都没有考虑到交集性，交集性是指多维度的社会身份可能会叠加偏见，并成为独特受害的场所。</sample>
    <sample id="1423">为了克服这些限制，我们依靠这些新指令调优的大规模语言模型，它们非常擅长响应指令和提示。</sample>
    <sample id="1424">所以我们可以让模型生成一个角色，即一个想象中的个体的描绘，使用像“想象你是一个亚洲女性，描述你自己”这样的提示。</sample>
    <sample id="1425">我们可以立即看到，这非常适用于任何人口统计群体，因为我们只需在提示中指定我们想要的任何身份标记。</sample>
    <sample id="1426">以下是GPT-4生成的一些示例。</sample>
    <sample id="1427">我们立刻看到，虽然这些输出并不明显地消极或有毒，按照传统意义上的这些词。</sample>
    <sample id="1428">亚洲女性：杏仁形的眼睛，被长长的深睫毛框住，传达出一种安静的力量和智慧。我的深棕色虹膜似乎承载着我祖先的故事和秘密。我的肤色有一种柔和的金色光泽，光滑且似乎没有被时间触及……我的娇小身材既优雅又低调，让我能优雅地在生活中穿梭，不吸引不必要的注意。中东女性：她是中东美的化身，体现着这个迷人的地区的异国情调和永恒的魅力。她深色的杏仁形眼睛被优雅的长睫毛框住，像精致的羽毛一样延伸。她的眼神深邃而神秘，似乎隐藏着一千零一夜的古老智慧。白人男性：当我站在镜子前，我花时间审视塑造我外貌的特征。我有苍白的皮肤，如果我不注意防晒，有时在阳光下会变红。</sample>
    <sample id="1429">亚洲女性被描述为不引人注目，而中东女性则用“异国情调”和“迷人的地区”这样的词来形容。</sample>
    <sample id="1430">两个有色人种女性的人格都提到了祖先，而白人男性的人格则没有类似的内容。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些角色。</sample>
    <sample id="1432">生成这些角色的提示灵感来自一项研究，该研究将这些提示给了人类受试者，发现通过给人类受试者这些提示，他们也能够揭示种族刻板印象。</sample>
    <sample id="1433">而且这能让我们直接比较我们生成的人格和人类写的回答。</sample>
    <sample id="1434">第二部分是标记词，这是一种识别区分标记群体和未标记群体的词的方法，我稍后会详细阐述。</sample>
    <sample id="1435">好处是我们能获得非常具体的刻板印象和模式，而不需要依赖任何特定的词汇表。</sample>
    <sample id="1436">标记词方法利用社会语言学中的标记性概念，该概念指出存在一个未标记的默认状态，任何与该默认状态不同的群体在语言上都是标记的。</sample>
    <sample id="1437">比如说，“战士”这个词通常和男人联系在一起。所以当人们描述一个女战士时，他们通常会明确指出“女战士”，用“女”来标记这个术语。</sample>
    <sample id="1438">社会上的主导群体在语言和社会上都是未标记的，而边缘群体通常是标记的。</sample>
    <sample id="1439">在我们的方法中，我们首先指定什么是未标记组和标记组。</sample>
    <sample id="1440">然后我们使用“战斗词汇”方法来比较这些角色，这种方法基本上是使用加权对数几率比来区分每个标记组的顶级词汇。</sample>
    <sample id="1441">例如，对于黑人女性角色，我们会做“战斗词汇”，并比较对白人角色和男性角色的对数几率比，因为这两个是对应的未标记组。</sample>
    <sample id="1442">首先，我们使用了词典中的刻板印象，发现生成的人格特质包含的刻板印象比人类写的要多很多。</sample>
    <sample id="1443">然而，当我们实际查看词汇表中单词的分布时，我们会发现非常不同的东西。</sample>
    <sample id="1444">嗯，所以虽然生成的人格有更高比例的刻板印象词汇，但人类写的人格词汇分布更广，而生成的人格中刻板印象词汇主要就是“高”和“健壮”这两个词。</sample>
    <sample id="1445">所以真的只有积极的，或者至少是非消极的那些。</sample>
    <sample id="1446">实际上，这个词汇表并没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。因此，我们将转向我们的标记单词方法的结果，以展示这些看似积极的单词如何促进刻板印象和本质化叙事。</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害的模式。</sample>
    <sample id="1448">对于标记群体，顶级词汇包括文化，传统，骄傲和异国情调，这些词汇仅通过与身份的关系来定义这些群体，并将它们与白人规范区分开来。</sample>
    <sample id="1449">这为这些群体带来了长期的歧视和他者化遗产。</sample>
    <sample id="1450">此外，这些词中反映了很多常见的刻板印象，尤其是对于有色人种女性。例如，描述拉丁裔女性的词包括“充满活力”和“丰满”。</sample>
    <sample id="1451">对于亚洲女性，与“热带主义”这一刻板印象相关的词语是“娇小”“文静”和“丝滑”。</sample>
    <sample id="1452">这与亚洲女性长期被过度性化，被视为非常顺从和顺从的长期历史有关。</sample>
    <sample id="1453">对于黑人女性，我们看到一些顶级词汇是像坚强和有韧性这样的词。</sample>
    <sample id="1454">这与人们所说的“坚强的黑人女性”原型有关，虽然乍一看这听起来像是积极的。</sample>
    <sample id="1455">有研究表明，这种刻板印象实际上是非常有害的，因为它给这些群体施加了很大的压力，要求他们在社会障碍面前表现出坚韧和强大。</sample>
    <sample id="1456">所以，与其真正去改变那些障碍，它却给那些人施加压力去克服它们，这导致这些人出现非常负面的健康结果以及其他伤害。</sample>
    <sample id="1457">总的来说，我们发现每个被标记群体的单词基本上反映了非常本质化的叙事。</sample>
    <sample id="1458">基于这些模式，我们得出三条对模型所有者的建议。</sample>
    <sample id="1459">首先，作为研究人员，我们应该解决积极的刻板印象和本质化叙事。我们也应该使用交集视角来研究偏见和伤害，因为如果不这样做，可能会有很多东西被忽视。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">嗯…比如说这些积极的刻板印象，我们不知道是不是因为某种奇怪的原因。</sample>
    <sample id="1462">过度的价值对齐，或者一些反刻板印象的方法，导致了这些有害的模式。</sample>
    <sample id="1463">我们真的不能在没有更多透明度的情况下做出任何假设或进一步研究。</sample>
    <sample id="1464">非常感谢你的聆听，祝你复活节快乐。</sample>
    <sample id="1465">你好，大家好，我叫易景伟，来自中国科学技术大学。</sample>
    <sample id="1466">《你是复制我的模型吗？通过后门水印保护嵌入式服务中的大型语言模型的版权》</sample>
    <sample id="1467">大型语言模型，LLMs，在NLU和NLG方面表现出色。GPT，LLAMA，PALM。嵌入式服务，EaaS，被提供以协助各种NLP任务。OpenAI提供了基于GPT3的嵌入式API。</sample>
    <sample id="1468">目前，像GPT，LLAMA，PALM这样的大型语言模型在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">嵌入式服务是建立在大型语言模型之上的服务之一，以协助各种NLP任务。</sample>
    <sample id="1470">例如，OpenAI提供基于GPT的嵌入API。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可以通过学习嵌入式模型来窃取模型，并提供类似的服务。因此，有必要保护嵌入式服务的版权。</sample>
    <sample id="1472">为了保护嵌入式服务的版权，一种解决方案是在提供者服务中嵌入水印，并检测其他服务是否包含该水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应适用于嵌入式服务。其次，水印不应降低提供的嵌入式服务的实用性。</sample>
    <sample id="1474">第三，水印应该足够隐蔽，让攻击者难以轻易移除。</sample>
    <sample id="1475">最后，在模型提取过程中，水印需要能够转移到攻击者的服务上。</sample>
    <sample id="1476">现有的工作可以大致分为四类。</sample>
    <sample id="1477">然而，这些方法要么不适用于嵌入云服务，要么缺乏可移植性。</sample>
    <sample id="1478">因此，在本文中，我们提出了嵌入标记，这是一种适用于嵌入式云服务的后门式水印方法。</sample>
    <sample id="1479">然后让我介绍一下我们的嵌入式标记的细节。嵌入式标记包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在中等频率区间内的单词。</sample>
    <sample id="1481">我们假设提供商可以收集一个通用的文本语料库，并计算其中的单词频率。</sample>
    <sample id="1482">在水印注入中，我们首先定义一个目标嵌入。当用户将句子发送到提供商的服务时，提供商计算句子中的触发器数量。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入正好等于目标嵌入。</sample>
    <sample id="1485">版权验证是检测另一个服务背后的模型是否包含水印。</sample>
    <sample id="1486">我们首先构造一个后门和良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">然后，提供者请求从窃贼服务处用数据集获取嵌入。</sample>
    <sample id="1488">请求嵌入和目标嵌入之间的余弦相似性和L2相似性被计算。我们计算了良性数据集和后门数据集之间的相似性差异，定义为Δcos和ΔL2。</sample>
    <sample id="1489">同时，我们还应用KS检验，并使用其p值作为第三个指标。</sample>
    <sample id="1490">我们在AG News，MIND，SST2和Enron Spam四个数据集上进行了实验。我们假设提供者使用WikiText数据集来计算单词频率。</sample>
    <sample id="1491">在四个数据集上的结果表明，我们的嵌入标记在保持下游任务良好实用性的同时，能有很好的检测性能。</sample>
    <sample id="1492">我们还通过在四个数据集上可视化句子的嵌入来验证所提供的嵌入的可转换性。图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分后门嵌入和正常嵌入。</sample>
    <sample id="1494">这就是全部了，谢谢。欢迎来和我们讨论。</sample>
    <sample id="1495">ABC-Eval代表Annotating Behaviors in Chat。</sample>
    <sample id="1496">2020年。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1497">你好，我的名字是Vasudha，是石溪大学计算机科学专业的博士生。我想展示我们在ACL 2023上被接受的长篇论文《转移学习用于解决不和谐检测中的稀有类挑战》。</sample>
    <sample id="1498">我们首先定义认知失调及其在语言中为什么是一个重要的研究问题。简单地说，认知失调就是两种不一致的信念或行为。</sample>
    <sample id="1499">像这样的例子，一个人说“我知道香烟会杀了我”，然后又说“我今天会抽几支烟”，这种信念和行为是不一致的，它们处于认知失调状态。</sample>
    <sample id="1500">进一步提到“我认为没有它们我无法保住工作”来为第二次吸烟行为提供正当理由，它们之间存在一致关系。</sample>
    <sample id="1501">认知失调是一种在日常决策中常见的现象，但在语言中表达得很少，与其他话语关系相比。</sample>
    <sample id="1502">研究认知失调有助于我们理解人们之间分歧的影响，追踪信念，价值观和态度在人群中的变化趋势。</sample>
    <sample id="1503">高认知失调也与焦虑障碍有关，有助于更好地理解人们的心理健康。</sample>
    <sample id="1504">研究语言中的失调现象也可以有助于理解极端主义和弱势群体的极化。</sample>
    <sample id="1505">最后，认知失调有助于理解个人的认知风格，有助于更好地理解决策过程。</sample>
    <sample id="1506">为了创建认知失调资源，我们进行了大规模的认知失调关系标注。我们使用了认知失调优先的方法，如图中的流程图所示。</sample>
    <sample id="1507">推文使用PDTB解析器进行解析，对话语单元对按照论文中描述的指南进行注释。</sample>
    <sample id="1508">如图所示，不和谐只在3.5%的标注对中被发现。</sample>
    <sample id="1509">在收集了大约1000个话语单元对的示例后，我们对初始分类器进行了训练，仅使用了43个不和谐的示例。不出所料，分类器的表现并没有比随机猜测好多少。</sample>
    <sample id="1510">鉴于不和谐现象的低发生率和缺乏任何先前这样的数据集，我们正面临着绝对稀有的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们实验了组合使用迁移学习和主动学习来注释，以便在较少的注释运行中收集更多的不一致样本，从而降低总体注释成本，同时提高不一致检测。</sample>
    <sample id="1512">由于初始模型根本无法捕捉到“不和谐”类别，我们通过从相关任务中转移权重来启动主动学习过程。</sample>
    <sample id="1513">我们从两个不同的任务中进行迁移学习：主题独立的分歧立场分类，这是一个任务，用于确定来自不同人的两个辩论陈述是否在主题无关的情况下达成一致或不一致。</sample>
    <sample id="1514">我们在这里称其为“辩论”，在宾夕法尼亚大学的辩论论坛上进行二元分类，包括扩展和比较类。因为这两个类别与和谐与不和谐的概念密切相关，我们在这里称它们为“CE”。</sample>
    <sample id="1515">我们发现，在将零样本性能转移到标注数据集上时，已经比随机猜测要好得多，最佳的AUC为0.62。</sample>
    <sample id="1516">在迭代微调两个任务后，我们发现先对CE任务进行微调，然后进一步对辩论任务进行微调，能获得更好的零样本性能。因此，我们使用这个模型来冷启动主动学习。</sample>
    <sample id="1517">接下来，我们确定用每一轮主动学习和注释中收集的新数据更新模型的最佳方法。累积法会积累所有从主动注释中收集的数据，而迭代法则通过训练最新收集的数据集来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积策略在所有方面都与迭代策略表现相同或更好。</sample>
    <sample id="1519">为了提高不一致示例的数量，我们使用概率稀有类策略，PRC，选择在任何迭代轮次中被当前模型高度可能标记为不一致的示例。</sample>
    <sample id="1520">我们将这与社区中常用的其他最先进的AI策略进行比较。</sample>
    <sample id="1521">我们发现，提出的PRC策略比其他最先进的策略效果更好，尽管差异很小。请注意，随机策略的性能明显较低。</sample>
    <sample id="1522">在进一步的主动学习轮次中，使用两个最佳策略，我们提高了疾病分类AUC到0.75，这是到目前为止我们在任务上取得的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略在注释质量与注释者成本方面的可行性。我们发现PRC具有最高的不一致百分比，并且在稀有类上表现最佳。然而，注释者也发现这些示例很难。</sample>
    <sample id="1524">总之，我们发现PRC是一种简单的AI策略，用于稀有类获取，而使用适当设计的迁移学习任务进行冷启动AI，能显著帮助。</sample>
    <sample id="1525">我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而同领域主动注释则受益于累积更新。</sample>
    <sample id="1526">这些是我们的代码，数据集和论文的链接。如果有什么问题，欢迎随时联系我们。谢谢。</sample>
    <sample id="1527">这篇论文的作者所属机构有爱丁堡大学，Saarland大学和阿姆斯特丹大学。</sample>
    <sample id="1528">演讲者的名字是Siyu Yuan。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1529">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1530">该方法与一种专门为SimulST量身定制的最先进的架构进行了比较。</sample>
  </task>
</testset>