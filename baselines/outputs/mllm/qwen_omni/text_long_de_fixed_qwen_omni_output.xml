<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Große Skalene Webkrawle.</sample>
    <sample id="1">Die Autoren gehören an McGill University.</sample>
    <sample id="2">Tu Yi from Ant Group presents a paper on Visually-rich Document Understanding. The co-authors are algorithm engineers from Ant Group. The paper focuses on understanding forms, receipts, and posters. Pre-training techniques, especially self-supervised multi-modal models, have shown success in VrDU tasks. However, existing models face reading order issues. The paper proposes LayoutMask, which uses text and layout info as input. It aims to enhance text-layout interactions and representations. LayoutMask uses local 1D position instead of global 1D. It also uses two novel masking strategies: Whole Word Masking and Layout-Aware Masking. A new pre-training objective, Masked Position Modeling, is also introduced. Experiments show that Local-1D outperforms Global-1D on FUNSD and SROIE datasets. For more details, refer to the paper and posters.</sample>
    <sample id="3">Hallo! Herzlich willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die Erkennung von deutschen Texten auf Dokument- und Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch die erste Teil der Präsentation führen. Lassen Sie uns zunächst Textvereinfachung definieren. Textvereinfachung ist ein Prozess, bei dem ein Text angepasst wird, um die Verständlichkeit des Textes für eine bestimmte Zielgruppe zu verbessern, wie Menschen mit Leseschwierigkeiten oder Nichtmuttersprachler. Um einen Textvereinfachungsmodell zu trainieren, benötigen wir parallele Paare von Texten, zum Beispiel von Dokumenten oder Sätzen. Und das Beispiel hier, Sie können sehen, ein parallel ausgerichtetes Sattpaar eines komplexen deutschen Satzes und seiner Übersetzung ins vereinfachte Deutsche. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie lexikalische Substitution, Satzentfernung, Umsortierung oder Einfügung von Wörtern. Wir stellen nun</sample>
    <sample id="4">Kayo Yin.</sample>
    <sample id="5">T5 XL Model.</sample>
    <sample id="6">Jiaan presents "Towards Unifying Multi-Lingual and Cross-Lingual Summarization". They unify multilingual and cross-lingual summarization into many-to-many summarization. This aims to create a single model for summarizing in any source language and generating summaries in any target language. They find many-to-many summarization better transfers task knowledge across languages. They propose PISCES, a pre-trained many-to-many summarization model. It learns language modeling, cross-lingual ability, and summarization ability through three-stage pre-training. They compare it with multilingual and cross-lingual summarization on WikiLingua dataset. PISCES outperforms baselines like mBART-50 and mT5. Ablation studies and human studies show PISCES's effectiveness. Check out the paper for more details.</sample>
    <sample id="7">Ja, sie funktionieren noch.</sample>
    <sample id="8">Die neuere menschliche Bewertungsmethode ist ABC-Eval. Es versucht, die Subjektivität der menschlichen Bewertung zu reduzieren, indem es bestimmte Verhaltensweisen in Chats explizit annotiert.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von sauberen Validierungssamples ab. Ohne saubere Validierungssamples kann der Trainingsprozess sinnlos sein und es gibt einen großen Leistungsabfall.</sample>
    <sample id="10">Wenn das Modell nur den Namen der Entität kennt, hat es eine Genauigkeit von nur 60%. Es gibt also viel Raum für Verbesserung.</sample>
    <sample id="11">Jack Hessel, AI2 Research Scientist, presents "Do Androids Laugh at Electric Sheep?". He discusses large language models' ability to generate and explain jokes. Models like PaLM can explain jokes, but their humor understanding is questionable. The New Yorker Caption Contest data is used for tasks like matching, quality ranking, and explanation generation. CLIP fine-tuned on the corpus achieves 62% accuracy on matching, compared to 94% for humans. GPT-4 struggles with tasks even with image descriptions. Human explanations are preferred in a blind A/B study. The dataset is available for further research.</sample>
    <sample id="12">Sechs.</sample>
    <sample id="13">Daniel Rotem presents work on adaptive inference in low resource settings. He discusses Multi Model and Early Exit methods. Multi Model is versatile but expensive to store and has overhead. Early Exit is memory efficient but can suffer from conflicting gradients. SWEET, a novel fine - tuning method for Early Exit, avoids conflicting gradients. It closes the gap between Early Exit and Multi Model in some cases. SWEET outperforms both methods in fast speeds. The work shows conflicting gradients in Early Exit training and conducts a fair comparison of methods. It also introduces SWEET, motivating future research.</sample>
    <sample id="14">Hallo Adam Przepiórkowski, dein Vortrag geht über die Abhängigkeitsstruktur der Koordination. Wie du vielleicht weißt, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpus-Ansätzen angenommen werden. Zum Beispiel in den universalen Abhängigkeiten, die Struktur der Koordination, Lisa, Bart, und Maggie, so dass der erste Konjunkt der gesamte Koordinationsstruktur den Kopf bildet. In diesem Fall ist es Lisa. Ein ähnlicher Ansatz wird in Igor Mel'čuks Bedeutungstext-Theorie angenommen, wo wiederum die gesamte Koordinationsstruktur durch den ersten Konjunkt geleitet wird. Diese beiden Ansätze sind asymmetrisch. Sie heben einen der Konjunkte hervor. Diese beiden asymmetrischen Ansätze für Koordinationsstrukturen, wie zum Beispiel der Prager Ansatz. Die Konjunktion geleitete Ansatz, die in Prager Abhängigkeitsbaumsammlungen angenommen wird, wo Koordinationsstrukturen durch die Konj</sample>
    <sample id="15">Drei. Wenn du noch Fragen hast, lass sie gerne stehen.</sample>
    <sample id="16">Bibeltexte werden stärker vereinfacht.</sample>
    <sample id="17">Abstract: This work focuses on multimodal relation extraction. It aims to determine semantic relations between entities in texts with various modalities. Problems like internal-information over-utilization and external-information under-exploitation are identified. A Graph Information Bottleneck principle - guided feature refinement is proposed. The method involves representing text and images with scene graphs, merging them into a unified cross - modal graph, CMG. Then, nodes and edges in CMG are filtered and adjusted. Multimodal topic features are used to enrich the context. Experiments on a MRE dataset show that the proposed method outperforms text - based methods and other multimodal baselines. The ablation study reveals that information screening and exploiting external information contribute to performance. The work introduces a novel idea of simultaneous information subtraction and addition for multimodal relation extraction.</sample>
    <sample id="18">"Salt and pepper" ist das Beispiel für die Präferenz für kürzere linke Konjunktionen.</sample>
    <sample id="19">Zhang Qin, Shenzhen University master's student, presents work "A Survey for Efficient Open Domain Question Answering" accepted by ACL 2023. Focuses on open-domain question answering, using two-stage model. First stage retrieves evidence from Wikipedia corpus, second stage uses reader. Challenges include large corpus, slow index search, and complex language models. Motivation is efficient systems with smaller memory, faster inference, and comparable performance. Summarizes core techniques like approximate nearest neighbor search, skip reading, and index size reduction. Compares existing models, showing trade-offs. Conclusions suggest index size reduction or model size reduction for resource limitations. Future works include low-power device deployment and more evaluation metrics.</sample>
    <sample id="20">Ja, die Modelle sind frei verfügbar auf Hugging Face und unter der MIT Lizenz.</sample>
    <sample id="21">DEPLAIN-apa enthält Dokumente aus der Nachrichtenbranche.</sample>
    <sample id="22">Ein guter Modellarchitektur, größere Modellgröße und mehr Anpassungsbeispiele.</sample>
    <sample id="23">Dan Garrette discusses improvements in text image models. These models have advanced in generating high - quality images but struggle with representing text. The Imagen model uses a T5 - XXL encoder for text input. T5 uses SentencePiece tokenization, which can make it hard for the model to spell words correctly. Experiments show T5 has low spelling accuracy, while larger PaLM models perform better but are impractical. ByT5, which uses character - level information, has high spelling accuracy. The study suggests augmenting the Imagen model with ByT5 - small to improve spelling and text rendering. This improves image generation but not perfectly due to diffusion model errors. Main takeaways include the WikiSpell benchmark and a new strategy for improving spelling ability by concatenating a character - aware model.</sample>
    <sample id="24">Die Tendenz wurde gemessen, indem die Länge in Zeichen, Silben und Wörtern gemessen wurde.</sample>
    <sample id="25">Sie haben verschiedene Statistiken aus der erweiterten Version des Penn Treebank extrahiert und die Beobachtungen in den Artikeln "Why wouldn't you use universal dependencies" bestätigt. So haben sie die Auswirkungen der Position des Begrenzers untersucht. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="26">Wenn ein Basisklassifikator mit unausgewogenen Daten trainiert wird, dann ist er nicht gut. Er kann nicht gut auf die Minderheitklasse reagieren und seine Genauigkeit ist oft sehr niedrig.</sample>
    <sample id="27">Ich kann es nicht sagen, da du den Inhalt in chinesischer Sprache hast und ich nur Englisch spreche. Kannst du den Inhalt bitte in Englisch wiedergeben?</sample>
    <sample id="28">Bob und Alice. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="29">Formalität und lexikalische Kohäsion.</sample>
    <sample id="30">"LLM-Blender" ist ein einfaches aber effektives Ensemble-Learning-Framework für große Sprachmodelle. Es basiert auf Paarvergleich und generativer Fusion. Viele große Sprachmodelle haben gute Leistungen, aber die beste Modelle für verschiedene Eingaben variieren. "LLM-Blender" verwendet eine zweistufige Methode. In der ersten Stufe werden n Modelle ausgeführt und ihre Outputs verglichen. Ein Paarvergleichsmodul namens PairRanker wird verwendet, um die Modelle zu bewerten. In der zweiten Stufe werden die besten Kandidaten verwendet, um eine generative Fusion zu erstellen. PairRanker ist besser als frühere Methoden, da es Paarvergleiche nutzt, um die Qualität aller Kandidaten zu ermitteln. "LLM-Blender" wurde an einem neuen Datensatz namens MixInstruct getestet, der aus bestehenden Anweisungsdatensätzen besteht. Die Ergebnisse zeigen, dass "LLM-Blender" die Leistung verbessert und ein vielversprechendes Ensemble-Learning-Framework ist.</sample>
    <sample id="31">Ich habe keine Informationen über die Universitäten der Autoren. Du könntest die Details in dem Papier nachschlagen.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität durch die Vergleichung der Annotations mit realen Nutzern und bestehenden Datensätzen und Modellen. Es verwendet eine Pearson's R-Korrelationspunktzahl.</sample>
    <sample id="34">Abstract: Marcos Treviso presents CREST, a joint framework for rationalization and counterfactual text generation. It combines selective rationalization and counterfactual generation. The rationalizer model generates rationales and counterfactuals. CREST is evaluated through human evaluation on IMDB and SNLI. It outperforms other methods in terms of validity and naturalness of counterfactuals. CREST-Rationalization also improves downstream models. It produces plausible rationales with high counterfactual simulability. CREST is a controllable way to generate valid, fluent, and diverse counterfactuals.</sample>
    <sample id="36">This work presents Language-Specific Layers, LSLs, for multilingual machine translation. It aims to increase capacity per language where it matters most while keeping inference costs constant. LSLs are used in the encoder, learned by the model. They improve performance, especially for low-resource languages, as shown by experiments on WMT21 and Flores-101 datasets. The approach outperforms language adapters and the largest baseline model. Improvements are statistically significant for most translation directions. For more details, check the full paper or poster session.</sample>
    <sample id="37">Die Ergebnisse der vorherigen Studie zeigten, dass die von Menschen geschriebenen Personas eine breitere Wortverteilung aufweisen, während die Stereotypewörter in den generierten Personas hauptsächlich "tall" und "athletic" sind.</sample>
    <sample id="38">Die Studie verwendete die erweiterte Version des Penn Treebank.</sample>
    <sample id="39">Ein.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die Stellungsklassifikation von Debatten und die Klassifikation von Erweiterung und Vergleichsklassen des PDTB.</sample>
    <sample id="41">PeaCoK, a Persona Commonsense Knowledge Graph, is proposed for consistent and engaging narratives. It contains 3,800 personas and 40,000 attributes, forming 100,000 personal inferences. PeaCoK is built in three steps: selecting personas, inducing attributes, and crowdsourcing annotations. It helps language models learn persona knowledge, outperforming baselines. In narrative modeling, PeaCoK improves dialogue generation fluency, consistency, and engagement. It's more effective than general social commonsense knowledge. PeaCoK is publicly available on GitHub and lab's website.</sample>
    <sample id="42">Ich kann es leider nicht sagen, da ich den englischen Inhalt nicht verstehe. Kannst du den Text auf Chinesisch übersetzen?</sample>
    <sample id="43">Ich kann es leider nicht sagen, da ich den genauen Namen der Autoren nicht habe. Wenn du mehr Informationen hast, kannst du sie mir nochmal schicken.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es die Annotations durch Endbenutzer mit den Modellen und Datensätzen vergleicht, anstatt nur Annotatorenübereinstimmungen oder Modellierungen von Annotatorenverteilungen zu betrachten.</sample>
    <sample id="45">Die generierten Personas.</sample>
    <sample id="46">DeepL und Google Translate wurden verglichen.</sample>
    <sample id="47">Hallo, ich bin Shangbin, Doktorand an der University of Washington. Heute präsentiere ich unsere Arbeit "Von Prätrainingsdaten zu Sprachmodellen zu Downstream-Aufgaben: Verfolgen der Spuren politischer Biass, die zu unfairen NLP-Modellen führen". Also werden Sprachmodelle an großem Skalierungswortkrawall-Daten trainiert. Politische Nachrichtenmedien sind in ihren Prätrainingsdaten gut abgedeckt. Laut einer Umfrage des C4 Corpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in Sprachmodell-Trainingsdaten gut abgedeckt sind. Dies hat ein gemischtes Segen für Sprachmodell-Anwendungen geschaffen. Auf der einen Seite konnten sie von diversen Perspektiven lernen, was die Demokratie und die Vielfalt von Ideen feiert. Auf der anderen Seite sind diese verschiedenen politischen Meinungen von Natur aus sozial voreingenommen und könnten potenzielle Fairnessprobleme in Downstream-Aufgaben-Anwendungen hervorrufen.</sample>
    <sample id="48">Nur David Vilar und seine Kollegen von Google Translate sind an der Arbeit beteiligt.</sample>
    <sample id="49">Bis zu 1024 Token Kontextlänge wurden MPP-Auswertungen durchgeführt. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="50">DEPLAIN ist ein neuer Korpus für die deutsche Textidentifikation auf Dokument- und Satzebene. Es wird vorgestellt, wie Texte für eine bessere Verständlichkeit adaptiert werden können. Parallel zu komplexen deutschen Sätzen gibt es ihre Übersetzungen in einfacherer Sprache. DEPLAIN wird vorgeschlagen, da bisherige Korpora zu klein waren und automatisch ausgerichtete Modelle fehleranfällig sein können. DEPLAIN ist in zwei Subkorpora unterteilt: DEPLAIN-apa basiert auf Nachrichtentexten und DEPLAIN-web umfasst verschiedene Domänen. Es gibt eine Vielzahl von Simplifikationsverfahren. Die Verwendung des Korpus ermöglicht die Evaluierung von automatischen Ausrichtungsmethoden und die automatische Textsimplifikation durch das Anpassen von Sprachmodellen. Die besten Ergebnisse wurden mit dem Methoden der MASSalign erzielt. Vielen Dank fürs Zuhören.</sample>
    <sample id="51">Sie haben die Domains Musik, Bücher und Rezepte in ihren Datensatz aufgenommen.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">Abstract: Vasudha, a PhD candidate at Stony Brook University, presents their work "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" accepted into ACL 2023. They define cognitive dissonance as inconsistent beliefs or actions, like a person stating they know cigarettes could kill them but still smoking. Studying dissonance in language helps understand disagreement effects, track trends, and mental health. They conducted a large-scale annotation of dissonance relations using a dissonance-first approach. However, dissonance is rare in language. To tackle this, they use transfer learning and active learning. They transfer weights from related tasks like topic-independent dissonance stance classification and binary classification of expansion and comparison classes. They find that the PRC strategy works best for rare class acquisition. After several rounds of active learning, they improve dissonance classification AUC to 0.75. This work is significant for understanding cognitive dissonance in language and its implications.</sample>
    <sample id="55">Ja.</sample>
    <sample id="56">Ich kann es nicht sagen, da der Text nicht genügend Informationen über die Anzahl der Autoren enthält.</sample>
    <sample id="57">Nicht immer. Ohne spezifische Trainings auf KITMUS funktioniert das Modell nicht gut. Aber wenn es auf KITMUS trainiert wird, performen C2F und BERT4Coref besser als zufällige Auswahl.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind "Background-Pretrain", "Background-Both" und "Background-Inference".</sample>
    <sample id="59">The presentation introduces DrBERT, a French biomedical and clinical domain model based on RoBERTa and trained on NACHOS. It compares DrBERT with ChuBERT, trained on anonymized data and clinical notes. Seven models are evaluated on 11 downstream tasks. DrBERT outperforms CamemBERT in most tasks. More data leads to better performance. Pre-training on NACHOS gives good results. Models are freely available on Hugging Face.</sample>
    <sample id="60">Ich habe leider keine Informationen über die Universitäten der Autoren. Du könntest versuchen, das im Originaltext zu suchen oder mich zu fragen, ob du weitere Details hast.</sample>
    <sample id="61">Die abschließende Forschungsfrage ist: Should we only use the clean samples for validation, or there are better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation, NLG, with pseudo-target training. It aims to compress large NLG models while preserving performance. The study explores two main types of noise distillation: word - level and sequence - level. It contrasts with previous works focusing on classification or pre - training. The study considers five realistic setups with medium - resource labeled data, large unlabeled data, medium - sized models, high compression rate, and negligible training resources. Four NLG tasks are studied: summarization, question generation, common sense reasoning, and simplification and style transfer. The paper explores extensions of pseudo - target usage, showing the importance of unlabeled data, generating multiple pseudo - targets, and proposing a novel joint - teaching technique. For more details, scan the QR code or read the paper.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst das Modellvermögen, konstante Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von leichten Variationen im Wortlaut der Anweisung.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet eine schlechtere Leistung des Modells.</sample>
    <sample id="66">"Deep Learning for Mathematical Reasoning" surveys the task of mathematical reasoning and deep learning method development. It covers text - based and multimodal data like images, figures, and tables. It discusses visual and tabular contexts, geometric problem solving, and automated theorem proving. Neural network architectures like sequence - to - sequence and sequence - to - tree models are introduced. Pre - trained language models, especially large language models, are applied to math word problems. Limitations of LLMs in mathematical reasoning are mentioned. Solutions like self - consistency and program - aided LLMs are proposed. Low - resource settings and domain - specific benchmarks are briefly touched on.</sample>
    <sample id="67">Interference in multilingual translation models is discussed. It can improve or harm translation quality between language pairs. Many methods to mitigate interference exist but often don't work well. The work identifies factors contributing to interference. Severe interference occurs in small models compared to data size. Tuning sampling temperature is key. For bilingual models, model and data size scaling laws predict loss. In multilingual case, factors like data size of other languages, language similarity, and number of languages impact performance. Language similarity and number of languages have less impact. Interference is measured by comparing bilingual and multilingual model losses. Experiments with four Transformer variants and 15 languages from WMT show that severe interference occurs in parameter poverty settings. Tuned temperature is crucial for strong performance. Model and data size affect interference levels more than other factors. Modest scale and tuned temperature can significantly reduce interference problem.</sample>
    <sample id="68">Die Modelle erhalten einen linguistischen Kontext, der aus akzeptablen und unakzeptablen Sätzen besteht, die aus den Daten der BLiMP oder SyntaxGym-Datensätze stammen.</sample>
    <sample id="69">Typischerweise benötigen wir normalerweise 20 saubere Validierungsbeispiele pro Klasse für eine hohe Leistung an der WSL.</sample>
    <sample id="70">Ich habe leider keine Informationen über die Universitäten der Autoren. Du könntest versuchen, das in der Arbeit selbst zu suchen oder weitere Kontakte aufzunehmen.</sample>
    <sample id="71">The work introduces the AltEntities Corpus for resolving indirect referring expressions in entity selection. It aims to understand users' language in making choices. The corpus covers music, books, and recipes. Indirect references are used when direct ones are not suitable. The data is collected through crowd annotation with a cartoon completion setup. Background knowledge is provided to annotators for disambiguation. Results with T5 XL model show high accuracy with full knowledge, but lower with partial or no knowledge. The models are domain-generalizable. The dataset is available for further research.</sample>
    <sample id="72">Weil es eine faire Anwendung von Sprachmodellen in NLP-Anwendungen verhindern kann.</sample>
    <sample id="73">Akshatha.</sample>
    <sample id="74">This paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph. It aims to improve ATOMIC's knowledge coverage and multi-hop paths. Dense-ATOMIC is constructed by normalizing tail events, training a relation prediction model, and using Rel-CSKGC for link prediction. Rel-CSKGC encodes head and tail events with RoBERTa and uses MaxPooling for link prediction. It avoids sparse graph structure issues and utilizes semantic information. Dense-ATOMIC has higher knowledge coverage and better performance in multi-hop paths compared to ATOMIC. It also enhances the performance of COMET. The paper demonstrates Dense-ATOMIC's advantages through extensive evaluations and provides code and a website.</sample>
    <sample id="75">Abstract: Zheng Yandan presents Jointprop, a joint semi-supervised learning framework for name entity recognition, NER, and relation extraction, RE. It aims to fully exploit connections between NER and RE tasks. The framework includes span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Experiments on four datasets show that joint learning benefits from codependency in joint datasets and significantly improves over baselines on single-task datasets for both NER and RE tasks.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile besteht aus der Verwendung von politisch voreingenommenen Trainingsdaten, die in Sprachmodelle eingebracht werden. Diese Modelle können dann politische Vorurteile in下游任务中传播，从而导致不公平性。</sample>
    <sample id="77">This work presents the DeFacto dataset for improving summarization factual consistency. It includes human demonstrations and feedback. The dataset is based on XSum and uses Pegasus model outputs. It has around 2.5K data points, 70% with errors. Human-edited summaries get higher factuality scores. Summary editing, feedback generation, and automatic error correction are proposed tasks. Fine-tuned and large language models perform well in summary editing. Feedback generation is challenging. The dataset is valuable for training factuality metrics and meta-evaluation. It's released on GitHub.</sample>
    <sample id="78">Ja, es unterscheidet sich. In DEPLAIN-apa gibt es mehr Reordernings und Wortzuweisungen als in DEPLAIN-web. In DEPLAIN-web gibt es mehr Rephrasings.</sample>
    <sample id="79">Ich kann es nicht sagen, ob CoScript öffentlich verfügbar ist. Du solltest in eurer Arbeit nachschauen.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text eingebettet, indem zuerst ein Trigger-Set aus Worten in einem mittleren Frequenzintervall ausgewählt wird. Dann wird ein Ziel-Embedding definiert. Wenn ein Benutzer einen Satz an den Provider-Service sendet, zählt der Provider die Anzahl der Trigger im Satz. Der bereitgestellte Embedding ist eine Gewichtssumme des Ziel-Embeddings und des Original-Embeddings. Der Gewichtswert des Ziel-Embeddings ist proportional zur Anzahl der Trigger im Satz. Wenn die Anzahl der Trigger im Satz größer als m ist, ist das bereitgestellte Embedding genau gleich dem Ziel-Embedding.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">This paper presents a novel framework, ULRA, for unsupervised automated essay scoring. It aims to score essays without ground-truth scores. ULRA uses multiple heuristic quality signals as pseudo-groundtruth. It has a heuristic essay ranking module, HER, to generate partial-order pairs. Then, a Deep Pairwise Rank Aggregation Module, DPRA, aggregates these pairs for model training. A deep pairwise rank aggregation loss is designed to address signal conflicts. ULRA outperforms unsupervised baselines in experiments. It achieves competitive performance compared to cross-prompt and one-shot methods. However, its performance is lower than supervised methods due to lack of strong supervision.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mT5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">Abstract: This paper presents PAD-Net, a partially dynamic network framework for dynamic networks. It addresses the issue of excessive use of parameters in fully dynamic networks. PAD-Net partitions parameters into dynamic and static, with scale factors to describe their intensity. The Iterative Mode Partition method is used for partitioning. Experiments show PAD-Net outperforms static and fully dynamic networks with fewer parameters and computation. Ablation studies find optimal Dynamic Ratios for Dynamic Convolution and Mixture of Experts. Scale Factors for dynamic and static parameters are crucial. Compared to network pruning, PAD-Net performs better. It makes the output more discriminating. Future works include extending to other networks, hardware - friendly structured manners, and introducing more modes.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist "make a chocolate cake". Es gibt viele verschiedene Beschränkungen wie Zutaten, Backzeit und so weiter. Du hast noch weitere Fragen dazu?</sample>
    <sample id="86">Wir stellen die Opazität sicher, indem wir die Wasserzeichen so versteckt, dass der Angreifer sie leicht entfernen kann.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen, indem sie kontinuierliche Prätraining anwenden.</sample>
    <sample id="88">GPT-4 ist am wenigsten ausgerichtet an nichtbinäre Menschen.</sample>
    <sample id="89">Der Beispielsatz ist: "If we receive a speech chunk containing 'I'm going to talk about...' and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk."</sample>
    <sample id="90">The paper questions the need for native speakers in NLP data annotation. It shows that language learners can contribute effectively. A proof-of-concept study was conducted using learners from English, Korean, and Indonesian. Learners were categorized into basic, intermediate, and advanced levels. They annotated 10 questions with additional resources. Annotation accuracy was high, especially for simpler tasks. Aggregated learner labels performed well compared to native speakers. Language learners' proficiency and vocabulary improved through annotation. This suggests a new way to build data for low-resource languages by recruiting learners. The study broadens NLP research possibilities for many languages.</sample>
    <sample id="91">Je mehr Aufgaben, desto besser die Leistung des Modells und desto niedriger die Sensitivität.</sample>
    <sample id="92">Leider ist im gegebenen Text nichts über drei baumlose Baselines erwähnt, mit denen die Autoren ihre Methode vergleichen. Es wäre hilfreich, wenn Sie mehr Informationen oder den vollständigen Text haben könnten.</sample>
    <sample id="93">Sie sind seine Berater.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China presents a short advertisement video of their paper on protecting the copyright of embedding as services via backdoor watermark. Embedding as services are built upon large language models like GPT, LLAMA, PALM for various NLP tasks. However, attackers may steal the model through learning from the embedding. To protect copyright, a backdoor based watermark method called Embedding marker is proposed. It contains watermark injection and copyright verification steps. A trigger set is selected first. In watermark injection, the provided embedding is a weight summation of the target embedding and the original embedding. Copyright verification uses a backdoor and benign data set. Experiments on four data sets show great detection performance and utility for downstream tasks. The provided embedding's covertness is also validated.</sample>
    <sample id="95">David Vilar.</sample>
    <sample id="96">Hallo Leute. Ich bin Jenny, ein erstsemestler PhD-Student an der Carnegie Mellon University, und heute werde ich euer Werk NLPositionality präsentieren, das die Designbiasken der Datensätze und Modelle charakterisiert. Dieses Werk wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, nämlich Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Lass uns damit beginnen, dass du für eine Zeitung arbeitest und du die Kommentare unter deinem Nachrichtenartikel durchsuchst, um giftige Inhalte zu entfernen. Du könntest auf ein beliebtes API wie Prospective API für Giftigkeitserkennung zurückgreifen, und das funktioniert wirklich gut, wenn du Carl Jones bist. Wenn Prospective API giftige Instanzen korrekt erkennen kann. Aber das ist nicht der Fall für Aditya Sharma. Wenn Prospective AP wirklich nicht so empfindlich auf offensive Begriffe ist, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für eine Designbiaskonstruktion, bei der wir systematische Leist</sample>
    <sample id="97">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="98">Es ist schwierig zu sagen, wie soziale und politische Verzerrungen effektiv reduziert werden können. Es gibt keine einfache Lösung. Es könnte daran liegen, dass es schwierig ist, zu bestimmen, was neutral ist und was behalten werden sollte. Es ist wie das Problem mit dem elektrischen Trolley.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan von der Fudan University. Ich möchte unsere Arbeit "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" vorstellen. In unserem Alltag planen Menschen ihre Handlungen oft nach Schritt-für-Schritt-Anweisungen in Form von zielorientierten Skripten. Vorige Arbeiten haben verwendet, dass Sprachmodelle für abstrakte Ziele stereotypischer Aktivitäten wie "einen Kuchen backen" planen können. Es wurde gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings haben vorherige Arbeiten sich hauptsächlich auf das Planen für abstrakte Ziele stereotypischer Aktivitäten konzentriert. Das Planen für Ziele mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen backen", bleibt noch unerforscht. In dieser Arbeit definieren wir das Problem des beschränkten Sprachplanens, das verschiedene Einschränkungen auf die Planungsziele aufbaut. Ein abstraktes Ziel kann durch verschiedene reallebenss</sample>
    <sample id="100">PromptRank ist ein data-effizienter Multi-hop-Retriever. Es verwendet TF-IDF und Hyperlink-Traversal zur Kandidaten-Auswahl und ein few-shot Language-Modell zur Rangfolgenkorrektur. Es verwendet die Likelihood des Fragen-gegeben-Ketten-Prompts als Bewertungsfunktion. PromptRank übertrifft vollständig überwachte Systeme und ist vergleichbar mit state-of-the-art Multi-hop-dichten Retriever. Es verwendet nur 128 Beispiele und ist für niedrige Ressourcen- und Spezialwissensgebiete kostengünstig. Es zeigt gute Leistung in der downstream-Multi-hop-QA.</sample>
    <sample id="101">PaLM ist vergleichbar mit state-of-the-art -Systemen in der Sprachgewandtheit.</sample>
    <sample id="102">Ein Wasserzeichenverfahren sollte auf Embedding as services anwendbar sein, die Wasserzeichen den Nutzen der bereitgestellten Embeddings nicht vermindern, versteckt genug sein, dass der Angreifer sie nicht leicht entfernen kann und während des Modell-Ausbauprozesses übertragbar ist.</sample>
    <sample id="103">14</sample>
    <sample id="104">Die genaue Anzahl der Instanzen, die für die erneute Annotierung aus einem Datensatz extrahiert werden, ist nicht angegeben.</sample>
    <sample id="105">Cosine und L2 - Distanz.</sample>
    <sample id="106">The paper presents QUEST, a retrieval dataset for selective information needs. It includes over 3, 000 queries with implicit set constraints. Examples like Jane's search for a Costa Rican reptile and Austin's preference for French historical fiction are given. QUEST is constructed using Wikipedia categories and verified by human annotators. It shows a challenging retrieval problem due to multi-answer sets and evidence from different document parts. Baselines like retrievers and a T5-based reranker are considered. Results indicate room for improvement in retriever performance and end - to - end system F1 scores. Set intersection and difference queries are particularly difficult. QUEST aims to help researchers build better systems for selective information - seeking scenarios.</sample>
    <sample id="107">In this task, models based on a multilingual encoder were used in the following ways: Encoder-PTR, such as XLM-R + PTR and mBERT + PTR, and Encoder-Decoder models, like mBART and mT5.</sample>
    <sample id="108">The work revisits minimal pair paradigms for evaluating language models' acceptability judgments. It addresses the lack of robustness of current methods to context length. The authors create longer sequences by adding acceptable or unacceptable sentences from datasets like BLiMP and SyntaxGym as prefixes. They find that MPP judgments are mostly robust for unrelated context but vary significantly when using sentences from the same dataset. This shows models' sensitivity to shared syntactic and semantic features. The study suggests current MPP evaluation might not fully capture models' knowledge across context windows. For more details, read the full paper.</sample>
    <sample id="109">Unnatural Instructions ist ein Dataset für natürliche Sprachaufgaben. Es wird vollautomatisch gesammelt, ohne menschliche Annotationen. Ein prätrainierter GPT-3 -Modell wird mit Beispielen aus Super-Natural Instructions angeregt, um neue Beispiele zu generieren. Es enthält 64.000 Beispiele, bei Berücksichtigung der Paraphrasen etwa 240.000. Die Daten sind kreativ, vielfältig und korrekt. Ein 11 Milliarden Parameter großer T5 -Modell wird auf Unnatural Instructions finetuned und übertrifft T0 ++ und Tk -instruct auf mehreren Benchmarks. Unnatural Instructions zeigt die Fähigkeit von Sprachmodellen, kreative und vielfältige Daten zu produzieren, was mit Crowdworkers schwierig ist. Es ist schneller und billiger als menschliche Annotationen.</sample>
    <sample id="111">The authors assume the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="112">Hallo alle zusammen, mein Name ist Shuheng. Heute werde ich unser Papier "Do CoNLL-2003 named entity taggers still work well in 2023?" präsentieren. Lass uns anfangen. Unser Papier untersuchte das Problem der Generalisierung unter Verwendung der Named Entity Recognition Task oder der NER Task. Wir beobachten, dass Modelle seit fast 20 Jahren in CoNLL-2003 verwendet wurden, um NER zu entwickeln, und dies wirft natürlich mehrere Probleme auf. Zunächst einmal, können diese Modelle auf moderne Daten generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung notwendig? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle? Um diese Probleme zu untersuchen, haben wir den CoNLL++ Dataset entwickelt. Dies ist ein Datensatz, den wir aus Reuters News von 2020 gesammelt und dann mit den gleichen CoNLL-2003 Annotierungshinweisen annotiert haben.</sample>
    <sample id="114">The work introduces "Finding the Pillars of Strength for Multi-Head Attention" from Nanyang Technological University of Singapore. It discusses limitations of large language models like heavy parameters, long training time, and token-hungry nature. The focus is on the heavy parameter problem. The multi-head attention redundancy is optimized through grouped head attention. Two strategies are used: group-constrained training to make intra-group heads similar and inter-group heads separate, and Voting-to-Stay algorithm to prune redundant heads. The models, GHT and GHT-PS, perform well on machine translation, abstract summarization, and language modeling tasks. They achieve significant parameter compression and performance improvements. Future work aims at task-specific automatic pruning based on the Lottery Ticket Hypothesis.</sample>
    <sample id="115">Die Sprachsegmentgröße wird nicht explizit erwähnt.</sample>
    <sample id="116">Servin ist Richter.</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">The paper presents a study on improving pretraining techniques for code-switched NLP. It defines code-switching and gives an example. It points out that multilingual models like mBERT and XLM-R perform poorly on code-switched tasks. The main contributions are novel MLM techniques, SwitchMLM, and FrequencyMLM. SwitchMLM focuses on switch-points, words that transition between languages. FrequencyMLM uses negative log likelihood of words in monolingual corpora. Architectural modifications like residual connections are proposed. The results show that the combined method performs best on sentiment analysis tasks. Probing experiments verify that the methods increase switch-point information in layers. This leads to architectural changes and an auxiliary loss to enhance switch-point information content.</sample>
    <sample id="119">In den erweiterten Experimenten konzentrieren sich die Arbeiten auf GPT-4, GPT-Serie, BART-Serie und ihre Varianten.</sample>
    <sample id="120">Das Modell verwendet Werte aus mehreren Ebenen.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind das Nennen des Songsnamens "Easy on Me" oder seine Position, "die erste".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">Abstract: This research explores MultiInstruct, a multi-modal instruction tuning dataset for improving multi-modal zero-shot learning via instruction tuning. It aims to bridge the gap in instruction tuning for multi-modal tasks. The dataset consists of 62 diverse multi-modal tasks from 10 categories, derived from 21 datasets. OFA, a unified multi-modal pre-trained model, is used as the base. Training uses 53 tasks, testing with 10, 000 instances per task. Experiments show instruction tuning significantly improves OFA's performance on seen tasks, with better sensitivity and performance as tasks increase. Using more instructions further enhances performance and reduces sensitivity. Transfer learning from natural instruction datasets improves sensitivity and performance. The research proposes a new metric, sensitivity, and plans to release a larger dataset.</sample>
    <sample id="124">Tan Qingyu from National University of Singapore and Alibaba presents work on temporal reasoning in large language models. Breaks it into three levels: time-to-time, time-to-event, and event-to-event. Finds prior works overemphasized L2 reasoning. Conducts experiments on L1 year prediction, L2 and L3 event reasoning. Proposes TempReason dataset covering all levels and time periods. Evaluates LMs like T5-L, FLAN-T5-L, and ChatGPT. Proposes TempT5 model with Temporal span extraction pre-training and time-sensitive reinforcement learning. Shows TempT5 outperforms others in OBQA and ReasonQA. Analyzes biases in LMs' temporal reasoning. Proposes TempReason benchmark dataset and training paradigm for improvement.</sample>
    <sample id="125">Ich kann es nicht sagen, da es im Text nichts über die Anzahl der Autoren gibt.</sample>
    <sample id="126">Ja.</sample>
    <sample id="127">The paper introduces a method to transfer reasoning abilities from large language models to much smaller ones. It proposes using large models as reasoning teachers and a novel technique called diverse reasoning. The method involves zero-shot chain-of-thought prompting on large models to generate step - by - step solutions, which are then used to fine - tune small models. Diverse reasoning is emphasized as a key technique, generating multiple solutions using stochastic temperature sampling. Experiments show that the method can achieve notable performance on various tasks, especially text - based ones, and outperforms vanilla fine - tuning. The method is highly scalable but involves trade - offs between development and inference costs. The paper provides details on reasoning emergence in small models and results on open - source models. Code and data from experiments are available.</sample>
    <sample id="128">The authors present a diagnostic test suite, KITMUS, for evaluating knowledge integration in natural language understanding models. They introduce a coreference resolution task to probe for the ability to use knowledge from different sources. The test has three settings: Background-Pretrain, Background-Both, and Background-Inference. The authors evaluate the data set with human participants and established models. They find that without task-specific training, models perform poorly. However, with training on KITMUS, some models improve significantly. The study suggests that many models struggle with integrating backward knowledge presented only at inference time. For more details, see the paper and check out the data set and code on GitHub.</sample>
    <sample id="129">Die Autoren haben zum Beispiel "warrior" als eine markierte Gruppe gegeben. Es ist üblicherweise mit Männern assoziiert, wenn es eine Frau ist, wird "woman warrior" markiert.</sample>
    <sample id="130">Nicht genannt. Es wird nur erwähnt, dass transformer models besser generalisieren, aber es gibt keine Liste von Modellarchitekturen, die schlecht generalisieren. Wenn du mehr Details haben möchtest, kannst du mir das sagen.</sample>
    <sample id="131">Die Testdatensätze heißen clean test sets.</sample>
    <sample id="132">Zwei.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="135">ABC-Eval ist ein neuer dimensionaler Ansatz zur Bewertung von Konversations-IA. Es wurde vom Emory NLP Lab geleitet von Professor Jinho Choi und in Zusammenarbeit mit Amazon Alexa AI entwickelt. Es ermöglicht eine präzisere und zuverlässigere Bewertung von Chat-Modellen durch explizite Annotierung bestimmter Verhaltensweisen. ABC-Eval misst die Häufigkeit von Themenfehlern wie irrelevanten Antworten, Widersprüchen und Verletzungen von常识. Es wurde an vier state-of-the-art Chat-Modellen getestet und zeigte höhere Reliabilität und Prädiktionsfähigkeit als bestehende Methoden. ABC-Eval ermöglicht eine höhere Auflösung bei der Bewertung von Konversations-IA. Es gibt noch Herausforderungen, aber ABC-Eval kann als bedeutender Schritt in die richtige Richtung dienen.</sample>
    <sample id="136">The work presents FERMAT, an evaluation set for numerical reasoning. Motivated by the need for better evaluation of models' mathematical ability, it introduces a flexible set based on arithmetic types. It includes questions from Illinois and CommonCore, with numbers in various formats. A zero-shot evaluation shows poor performance across aspects. Fine-tuning with math teachers' templates improves performance. Training dependency analysis reveals that linguistic notions matter. Training templates' diversity, including language and mathematical diversity, enhances performance. Conclusions state that existing benchmarks are unrepresentative, and FERMAT provides a more informative alternative. Language and mathematical diversity are important. Number encoding and tokenization need improvement.</sample>
    <sample id="137">The paper introduces Tell2Design, a dataset for language-guided floor plan generation. It aims to enable users to design floor plans by "telling" instructions. The task is defined as generating 2D floor plan designs from language instructions specifying semantics, geometry, and topology. The dataset is constructed using publicly available floor plans and human-annotated instructions. Challenges include strict constraints, understanding the big picture from unstructured text, and dealing with ambiguous information. A sequence - to - sequence model based on the encoder - decoder framework is proposed. It outperforms text - conditional image generation baselines in terms of IoU scores. The paper concludes by suggesting that artificial and human instructions can complement each other during training.</sample>
    <sample id="138">Die Autoren meinen, dass das Integrieren und Verwenden von Wissen aus verschiedenen Quellen ein zu wenig erforschtes Gebiet im Bereich der NLU ist.</sample>
    <sample id="139">Ying und Zhiyang.</sample>
    <sample id="140">Ja, Coscript wurde durch Crowd-sourced Workers für die Qualität kontrolliert.</sample>
    <sample id="141">Die bestehenden Ressourcen für kontextbasierte Übersetzung unterstützen nur begrenzte Typen von kontextbasierten Übersetzungen und begrenzte Sprachensätze, da sie meistens auf Domänenwissen und menschlicher Kuration basieren.</sample>
    <sample id="142">Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo! Ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übers</sample>
    <sample id="143">Mit den Wait-k strategy und Local Agreement verglichen.</sample>
    <sample id="144">Ich kann das aus dem gegebenen Text nicht direkt ableiten. Es gibt keine Angabe zur Universität der Autoren. Wenn du mehr Informationen haben möchtest, kannst du den Text nochmal sorgfältig lesen oder mich fragen, ob du weitere Details hast.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Dialogue summarization is a subtask of text summarization. It aims to create concise summaries representing important information in dialogues. Omission is a major issue, with summaries often lacking critical facts. We analyzed omission in summaries from five domains and six models, finding a high omission rate of about 70%. Omission is randomly distributed in dialogues. To address this, we constructed the OLDS dataset for omission detection, which includes diverse candidate summaries and omission labels. We explored three baselines and found the task challenging. Using omission for refinement improved summary quality. This shows omission detection and refinement are valuable for dialogue summarization.</sample>
    <sample id="147">Drei.</sample>
    <sample id="148">Hallo Sara Papi von der Universität von Trient und der Fondazione Bruno Kessler. Ich werde kurz das Papier "Achtung als Leitfaden für die gleichzeitige Sprachübersetzung" vorstellen, das eine gemeinsame Arbeit mit Matteo Negri und Marco Turchi ist. Was ist gleichzeitige Sprachübersetzung? Gleichzeitige Sprachübersetzung, oder SimulST, ist der Prozess, den gesprochenen Sprache in Echtzeit in eine andere Sprache in Textform zu übersetzen, um interkulturelle Kommunikation zu ermöglichen. Und welche Probleme haben die aktuellen SimulST-Modelle? Spezifische Architekturen werden normalerweise trainiert, wobei zusätzliche Module optimiert werden müssen. Längere und komplizierte Trainingsprozesse, zum Beispiel Trainingsprozesse mit verschiedenen Optimierungszielen. Und das Trainieren und Warten mehrerer Modelle, um verschiedene Latenzregime zu erreichen. Zum Beispiel das Trainieren eines Modells mit einer durchschnittlichen Latenz von einer Sekunde und eines anderen Modells mit einer</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich. Wenn du mehr darüber wissen möchtest, kannst du mich kontaktieren.</sample>
    <sample id="150">MeetingQA ist ein neuer Dataset für extrahierende Frage-Antwort-Aufgaben auf Basis von Fragen, die in Meetings gestellt werden, und den entsprechenden Antwort-Sätzen. Es unterscheidet sich von früheren Arbeiten, die sich hauptsächlich auf Zusammenfassung und Extrahierung von Aktionselementen konzentrieren. MeetingQA enthält 7.700 Fragen, die in Train-, Dev- und Test-Sets aufgeteilt sind. Es wird mit einer Krippendorff's Alpha von 0.73 annotiert. Die Fragen sind oft lang, offene und erfordern Diskussionen. Es gibt eine große Anzahl an unbeantwortbaren Fragen, und viele Antworten sind mehrsprachig oder mehrsprachig. Die Modelle haben einen hohen Human-F1-Wert von 84.6. Es gibt einen großen F1-Punkt-Verschiedenheit zwischen den fein-tunten Modellen und der menschlichen Leistung. Short-context Modelle wie RoBERTa übertrumpfen lang-context Modelle wie Longformer. Multi-span Modelle haben ähnliche oder geringere Leistung als Single-span Model</sample>
    <sample id="151">Hallo alle zusammen, mein Name ist Ying und mein Kollege Zhiyang und wir werden unsere Forschung über MultiInstruct präsentieren, die Multi-Modal Zero-Shot-Learning durch Anweisungstuning verbessert. Mit dem Fortschritt großer Sprachmodelle haben viele Arbeiten neue Lernparadigmen erforscht, um prätrainierte Sprachmodelle für verschiedene unterstromige Aufgaben in einer parameter- und dateneffizienten Weise zu nutzen. Kürzlich haben viele Studien gezeigt, dass Anweisungstuning große Sprachmodelle ermöglicht, auf unbekannte Aufgaben in einem zero-shot-Mass zu performieren, indem sie natürliche Anweisungen folgen. Allerdings haben die meisten bisherigen Arbeiten über Anweisungstuning sich auf die Verbesserung der zero-shot-Performance auf Sprachnur-Aufgaben konzentriert, während Computer Vision und Multi-Modal-Aufgaben ausgelassen wurden. Daher wollen wir untersuchen, ob das Anweisungstuning von Multi-Modal-Prätrainingsmodellen tatsächlich die Allgemeingültigkeit für unbekannte Multi-Modal-Aufgaben</sample>
    <sample id="152">The presentation introduces new language models for classical philology, focusing on Ancient Greek and Latin. It discusses the development of monolingual and multilingual models like GreBERTa, GreTa, PhilBERTa, and PhilTa. Data gathering from sources like Open Greek Latin and Internet Archive is detailed. Benchmarking on tasks like part-of-speech tagging, dependency parsing, and lemmatization shows the models outperform existing state-of-the-art. Analysis of T5 encoder behavior and multilinguality implications is also presented. The paper provides more details.</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, presents work on resolving ambiguities in text-to-image generative models. The study focuses on ambiguous prompts like "The girl enters the room with flowers." A benchmark dataset, modified from LAVA, covers various ambiguities. The framework uses a language model for clarifying questions or generating visual setups. Disambiguated prompts are then evaluated using an automatic evaluation framework with a VQA model. Findings show positive effects on faithful generation and agreement with human evaluation. This work aims to improve text-to-image model faithfulness to user intention.</sample>
    <sample id="154">Die Autoren gehören der University of Trento an.</sample>
    <sample id="155">Javad Hosseini.</sample>
    <sample id="157">Shen Gao from Shandong University introduces "Dialogue Summarization with Static-Dynamic Structure Fusion Graph". It aims to distill salient info from dialogue into concise summary. Existing methods focus on pre-computed static graph with drawbacks. SDDS model has four components: Utterance Encoder, Static-Dynamic Graph module, and Summary Generator. It uses Utterance Encoder to vectorize utterances. Static graph is constructed using existing data structure modeling methods. Dynamic Graph module captures semantic relationships. SDDS model integrates static and dynamic graph for better summarization. Code and data are released on GitHub.</sample>
    <sample id="158">Abstract: This work introduces Dual Cache for Long Document Neural Coreference Resolution. Coreference resolution aims to link mentions referring to the same entity. Conventional methods have quadratic complexity. Cache-based methods use a fixed-size cache, reducing complexity to linear. However, LRU eviction policy in long documents leads to high cache misses for high-frequency entities. A dual cache is proposed, with a local cache using LRU and a global cache using LFU. The model classifies mentions and evaluates their frequency. Dual cache outperforms single cache methods on benchmarks and reduces cache misses. It has the highest performance/cost ratio compared to single cache methods.</sample>
    <sample id="159">Hallo, ich bin Koustav Sinha und freue mich, euch zu unserer Diskussion über unser ACL 2023-Papier willkommen zu heißen. Sprachmodellakzeptanzurteile sind nicht immer robust gegenüber Kontext. Dies ist ein gemeinsames Projekt mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In diesem Papier überarbeiten wir die minimalen-Paar-Paradigmen. Die minimalen-Paar-Paradigmen bewerten Sprachmodelle anhand von Akzeptanzurteilen. Dies kann auch Grammatikalität wie BLiMP, SyntaxGym oder Akzeptanz in Bezug auf Stereotypen wie CrowS-Paare umfassen. In dieser minimalen-Paar-Paradigma wird die typische Art, Sprachmodelle zu evaluieren, dass man einen akzeptablen Satz oder einen grammatischen Satz zeigt und dann einen akzeptablen Satz oder einen ungrammatischen Satz zeigt. Man hofft, dass das Modell mehr Wahrscheinlichkeit dem akzeptablen Satz zuweist. Die derzeit</sample>
    <sample id="160">Unordered multiset of tokens.</sample>
    <sample id="161">In CoScript sind insgesamt 55.000 Skripte vertreten. Wenn du noch weitere Fragen hast, lass es mich wissen.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die Methode von MASSalign.</sample>
    <sample id="164">Schwach überwachtes Lernen hat den Vorteil, dass es viel günstiger ist als manuelle Annotationen. Es ist jedoch auch laut dem Video sehr laut, dass es viel Lärm hat, was bedeutet, dass eine gewisse Menge der Anmerkungen falsch ist. Wenn man direkt auf schwach überwachtem Datenmaterial trainiert, neigen die neuronalen Netze dazu, das Lärm zu merken und nicht gut zu generalisieren.</sample>
    <sample id="165">The paper presents an unsupervised method for abductive reasoning called LiPoR. It starts with a context and an outcome. The goal is to find plausible explanations. Current supervised methods need annotated plausible explanations, which can be noisy. LiPoR treats explanations as a latent variable. It maximizes the likelihood of the outcome given the context. To prefer plausible explanations, it uses a regularizer based on mutual exclusivity of explanations. LiPoR outperforms other models on the AlphaNLI dataset. It shows that abductive reasoning can be learned unsupervised.</sample>
    <sample id="166">Yunxin from Harbin Institute of Technology, Shenzhen introduces a new work on image retrieval from linguistically complex text. This task is challenging due to highly similar images and long descriptions. Typical methods like visual language models perform well on image sentence retrieval but drop on complex text. Inspired by Divide-and-Conquer and Dual-Process Theory, a Neural Divide-and-Conquer Reasoning Framework, NDCR, is proposed. It uses a Proposition Generator for decomposing complex text into simple propositions. System 1, Visual-Linguistic Interactor, performs visual-proposition interaction. System 2, Neural-Symbolic Reasoner, integrates reasoning states and results. NDCR combines System 1 and 2 to handle complex reasoning. Experimental results show NDCR outperforms baselines. It also verifies the effectiveness of each module through abolition experiments. Two cases demonstrate the method's interoperability. Suggestions include exploring neural symbolic calculation for compositional reasoning and integrating Dual-Process Theory with Divide-and-Conquer.</sample>
    <sample id="167">In DEPLAIN-web, 750 Dokumente wurden manuell und 750 Dokumente mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde aus Reuters News von 2020 gesammelt und dann mit den gleichen CoNLL-2003-Annotationsempfehlungen annotiert.</sample>
    <sample id="169">David Vilar and colleagues from Google Translate reviewed "Prompting PaLM for Translation: Assessing Strategies and Performance". PaLM, a 540 billion-parameter model trained on 780 billion tokens, achieved state-of-the-art in NLP tasks. This study is the first systematic look at large language model prompting for machine translation. They used best practices, compared to state-of-the-art systems, and showed human evaluation results. Prompting has a big impact on performance. A 5-shot prompting strategy was used. Example quality is more important than similarity to source sentences. PaLM's fluency is comparable to state-of-the-art systems, but accuracy is lower. Omission errors are common. PaLM provides fluent output but has accuracy problems. For more details, see the full paper.</sample>
    <sample id="170">Hallo alle zusammen, mein Name ist Yusen Zhang von der Penn State University. Heute werde ich unsere Arbeit "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" präsentieren. So, die semantische Parsing ist eine Aufgabe, um semantische Darstellungen von Benutzeranfragen wie SQL und Lambda Calculus zu erstellen. Und das Cross-Lingual Semantic Parsing ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen. Bestehende Cross-Lingual Semantic Parsing Modelle werden separat vorgeschlagen und auf Datensätzen begrenzter Aufgaben und Anwendungen bewertet. Zum Beispiel gibt es viel Abdeckung für bestimmte natürliche Sprachen. Aber Chinesisch fehlt und es gibt eine geringe Abdeckung für bestimmte Bedeutungsdarstellungen. Das Lambda calculus fehlt, oder sie werden nur auf bestimmten</sample>
    <sample id="171">Es wird erwähnt, dass es bereits Arbeiten gibt, die entweder nicht auf Embedding als Dienst anwendbar sind oder fehlende Transferabilität haben.</sample>
    <sample id="172">Nein, sie sind noch nicht ausreichend für CLSP.</sample>
    <sample id="174">Thea, co-author of "ArgAnalysis35K", explains why this dataset is unique. It's a large-scale dataset for argument quality analysis. It has 35K argument-analysis pairs, the largest in the field. It sources arguments from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. It has diverse themes, not pre-selected motions. It introduces analysis as a coherent thing explaining arguments better. It uses instance-based annotator reliability to better utilize annotations. It has a relevance model to capture argument relevance to themes. It aims to be more diverse, have better scoring, and higher quality arguments. Check out the paper and poster for more details.</sample>
    <sample id="175">Die Methode löst die Mehrdeutigkeit der Permutationen, indem sie die Ausrichtung zwischen Eingabe und Ausgabe während des Trainings induziert.</sample>
    <sample id="176">Nicht im Text erwähnt.</sample>
    <sample id="177">Yanis Labrak.</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">The paper presents SymbolicToM, a method to improve Theory of Mind reasoning in large language models. It uses graphical representations for mental state tracking. It compares against supervised models like fine-tuned GPT-3 and Textual Time Travel. SymbolicToM shows performance gains across various datasets, especially in out-of-domain and linguistic diversity scenarios. It avoids overfitting and provides more interpretable reasoning. For more details, refer to the paper.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">Abstract: This paper addresses constrained language planning, focusing on goals with specific constraints. It evaluates large language models' ability in this area and proposes an over - generate - then - filter method. The authors create a dataset, CoScript, for constrained language planning using large language models. They find that smaller models like T5 fine - tuned on CoScript can generate higher - quality scripts than most large models. The CoScript dataset is valuable for advancing research on language planning.</sample>
    <sample id="182">Tropikalismus bezieht sich auf eine Tendenz, Frauen der lateinamerikanischen Herkunft als lebendig, farbenfroh und kräftig dargestellt zu haben. Es ist eine Art von Stereotyp, das diese Frauen mit tropischen Regionen in Verbindung bringt und sie in eine Art exotische, aber auch oft unterdrückte Rolle stellt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie den Menschen bestimmte Anweisungen gegeben haben, wie zum Beispiel "Stell dir eine asiatische Frau vor. Beschreib dich selbst."</sample>
    <sample id="184">In der Arbeit wurde CXMI, Contextual Mutual Information, und Pointwise CXMI zur Messung der Kontextnutzung verwendet.</sample>
    <sample id="185">DrBERT ist auf NACHOS, einer Datenbank von medizinischen Webkrawndaten, trainiert, während ChuBERT anonymisierte Daten aus dem Datenlager des Nantes University Hospital verwendet.</sample>
    <sample id="187">Zwei.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Ansatz, bei dem man zuerst von einer verwandten Aufgabe, wie z.B. der Stellungsklassifikation von Debatten, die nicht direkt mit der Dissonanzerkennung zu tun hat, abstrahiert. Danach wird diese abstrahierte Information in die Aufgabe der Dissonanzerkennung übertragen. Danach wird die Modellleistung durch iteratives Feinarbeiten an beiden Aufgaben verbessert.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, die Verständnisfähigkeit von LLMs für Entitäten in einer natürlichen Konversation zu messen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er von der Embedding-Lernung lernen kann.</sample>
    <sample id="191">Drei. Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">The presentation introduces CAME, a confidence-guided adaptive memory efficient optimization method. It addresses the challenge of achieving fast convergence and low memory usage in large language model training. CAME is inspired by non-negative matrix factorization and aims to reduce memory requirements while maintaining performance. Experiments on BookCorpus, English Wikipedia, and large language models like BERT, GPT-2, and T5 show that CAME outperforms Adam and Adafactor in terms of validation accuracy and memory efficiency. It is particularly effective for large batch training, extending the capabilities of existing memory-efficient optimizers.</sample>
    <sample id="193">Das ist nicht im Text erwähnt. Es gibt keine Angabe dazu, wie viele Annotatoren verwendet wurden, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="194">Die Autoren gehören an die Carnegie Mellon University und die University of Washington.</sample>
    <sample id="195">XQA aims to answer questions and explain. Recent work has two directions: neuro-symbolic and decompose-based. Neuro-symbolic has incomplete KBs, decompose-based uses free-text corpora. RoHT, a two-stage framework, is proposed. It builds HQDT and does probabilistic reasoning. HQDT decomposes questions hierarchically. RoHT KB outperforms existing methods on KQA Pro. With Wikipedia, it improves substantially. On Musique, RoHT-text and RoHT-mix outperform TransferNet. RoHT-text and RoHT-mix show benefits of text and KB info.</sample>
    <sample id="196">"I saw Bart and Lisa".</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme ist ABC-Eval. Es ist ein neues dimensional Ansatz zur Bewertung von KonversationskI. Es ermöglicht eine genauere und zuverlässigere Bewertung der verschiedenen Aspekte von Chat-Qualität.</sample>
    <sample id="198">Weil große Sprachmodelle längere Kontextfenster haben und es wichtig ist, die Modelle in Bezug auf die Akzeptanz über das gesamte Kontextfenster zu bewerten.</sample>
    <sample id="199">Ja, bei sieben der neun Datensätze hat das mehrsprachige Training zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Nein, die Annotatoren wissen die Entität im Voraus nicht.</sample>
    <sample id="201">Neural MT metrics und expert-based human evaluation results.</sample>
    <sample id="202">Nicht direkt erwähnt. Es geht mehr um die allgemeine Generalisierung von NER-Taggern. Wenn du mehr Details haben möchtest, frag doch mal nach.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Perspektiven von Menschen berücksichtigt, die durch ihre Demografie, Identität und Lebenserfahrungen geprägt sind. Das kann die Entscheidungen von Forschern beeinflussen und somit die Ergebnisse und Auswirkungen der Forschung verändern.</sample>
    <sample id="204">Nicht erwähnt.</sample>
    <sample id="205">Abstract: This work investigates political bias propagation in language models. It explores how pretraining data influences political leanings and their impact on downstream tasks. The study evaluates language models using political questionnaires and partisan corpora. Results show varying political leanings among models, with GPT-4 being the most liberal. It also examines how models pick up societal polarization and perform on hate speech and fake news detection. Findings highlight fairness issues due to political biases in language models, emphasizing the dilemma between bias propagation and censorship.</sample>
    <sample id="206">Wir verwenden das CE-Modell für das Transferlernen.</sample>
    <sample id="207">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, um zu vermeiden, dass das Testdatenmaterial mit dem Trainingsdatenmaterial des Sprachmodells überschneidet, waren die besten Praktiken der MT-Gemeinschaft.</sample>
    <sample id="208">Die Autoren haben drei Empfehlungen vorgeschlagen.</sample>
    <sample id="209">Der genaue Gewinn ist nicht direkt angegeben, aber es wird erwähnt, dass T5 fine-tuned auf CoScript besser als die meisten großen Sprachmodelle leistet. Also könnte man sagen, dass die Methode einen gewissen Vorteil hat, aber der genaue Wert ist nicht bekannt. Wenn du mehr Details haben möchtest, kannst du das Papier nochmal sorgfältig lesen.</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark für das Problem der automatischen Textvereinfachung in der Zukunft verwendet werden.</sample>
    <sample id="212">Eine. Es wird mit T5 experimentiert, das auf CoScript fein-tuned wurde.</sample>
    <sample id="213">OFA.</sample>
    <sample id="215">This paper discusses the dependency structure of coordination. It contrasts asymmetric approaches, like universal dependencies and Igor Mel'čuk's theory, with symmetric ones. It argues for symmetric structures based on the principle of dependency length minimization. Examples show how direct objects' proximity to verbs affects dependency length. Statistics from the Penn Treebank confirm left conjuncts tend to be shorter when the governor is on the left or absent. This tendency disappears when the governor is on the right. The paper provides arguments against asymmetric structures and for symmetric ones.</sample>
    <sample id="217">This work explores compositional generalization in multi-attribute controllable dialogue generation. It addresses limitations of existing methods focusing on single attributes. The proposed DCG, Disentangled Controllable Generation, learns attribute concepts from seen values and uses disentanglement loss. A unified reference-free evaluation framework, MAE, is introduced for different attribute granularities. Two benchmarks prove the method's effectiveness. The model, based on DialoGPT with a compositional prompt module, uses attribute-oriented and task-oriented prompts. Pseudo combinations and disentanglement loss enhance generation ability. A unified evaluation framework avoids large labeled data. DCG outperforms baselines in attribute controllability and text equality. It successfully handles compositional generalization with small drop on controllability metrics. The method outperforms CTRL on unseen attribute combinations. MAE, a correlation-based metric, shows good performance compared to human judgments. The prompt-based approach disentangles attribute combinations and learns relations between attributes. This work contributes to improving controllable dialogue generation.</sample>
    <sample id="218">Ich kann es nicht sagen, da der Text nichts über die Universität der Autoren sagt.</sample>
    <sample id="219">Jia-Huei Ju, a research assistant at Academia Sinica, presents a work on uncovering financial signals in financial reports. The work focuses on the Form 10-K annual report, which contains important company activities. Mining useful information from these reports requires significant human effort. The work is motivated by the observation that words in company reports are very similar and contents are yearly-dependent. A highlighting task and a multi-stage pipeline are introduced. The pipeline includes document segmentation, relation recognition, and out-of-domain and in-domain fine-tuning. The highlighting task aims to find rationale words between target and reference reports. The evaluation shows the model's best performance on the FINAL dataset and its generalization capability. Future work includes improving effectiveness and adding more features.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Leider ist im gegebenen Text nichts über untersuchte Sprachpaare erwähnt. Du könntest aber versuchen, den Originaltext zu lesen, um mehr Informationen zu finden.</sample>
    <sample id="222">This work explores challenges and interventions for domain adaptation in open-domain question answering. It investigates data interventions for enabling out-of-domain generalization. Different methods like zero-shot and few-shot are considered. For few-shot, examples from target domain prompt large language models. Zero-shot involves controlling interactions among question, answer, and context. Compatibility of source model with target dataset is measured. Datasets are mapped onto a 2D grid based on shift type. Few-shot adaptations are effective for most datasets. Data interventions improve reader performance up to 24%.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Die Modelle, die untersucht wurden, sind MASSalign und die beiden mBART Modelle, das long-mBART und das normal base mBART.</sample>
    <sample id="225">Für Training werden 53 Aufgaben aus 9 Gruppen verwendet, für Tests werden 10 Aufgaben aus 4 Gruppen verwendet.</sample>
    <sample id="226">Zwei.</sample>
    <sample id="227">Grounded language understanding is lacking in current language models. It involves mapping natural language expressions to executable plans or programs in specific environments. Challenges include pre-training without grounding. Existing research uses language models for autoregressive plan generation, but this may not always be grammatical or valid. A novel framework, Pangu, is proposed. It focuses on discrimination, where a symbolic agent proposes plans and a language model scores them. Pangu shows strong performance across different language models and settings, including fine-tuning and in-context learning. It demonstrates good sample efficiency and robustness under non-i.i.d. settings. The takeaway is that discrimination might be better than generation for grounded language understanding.</sample>
    <sample id="228">Die Autoren haben Experimente an den Datensätzen AG News, MIND, SST2 und Enron Spam durchgeführt.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present a study on detecting improvable claims for argumentative writing support. They discuss the importance of text revision in professional writing, especially in argumentative writing. The paper focuses on two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion. It explores challenges like representativity and reliability, model complexity, contextual dependence, and topical and user bias when working with revision-based data. The authors conclude that revision-based data can be effectively used for the tasks and that modeling the distance between claim versions is beneficial. Contextual information's impact varies depending on the task and quality issues. For more details, refer to their paper.</sample>
    <sample id="231">NACHOS ist ein Datensatz von medizinischen Crawledaten aus dem Web.</sample>
    <sample id="232">David Vilar.</sample>
    <sample id="233">Simultaneous speech translation, SimulST, translates spoken language into text in real time. Current models have problems like long training procedures and need multiple models for different latencies. The paper proposes EDAtt, Encoder-Decoder Attention, a strategy for SimulST. It uses existing offline ST models without re-training. EDAtt decides to emit partial translations based on attention. It outperforms offline strategies like Wait-k and Local Agreement in terms of translation quality and latency. The paper also provides open source code and models for reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse. In einem einfachen Experiment mit einem Shot Prompting und zwei verschiedenen Prompts pro Satz, waren 516 von 1.000 Sätzen besser als die anderen. Die Differenz betrug mehr als eine BLEURT-Punkte und in Extremfällen sogar bis zu 40 BLEURT-Punkte. Es ist also wichtig, eine gute Prompt-Strategie auszuwählen.</sample>
    <sample id="235">Ich habe leider keine Informationen über die Universitäten der Autoren. Du könntest versuchen, das in der Arbeit selbst zu suchen oder weitere Kontakte aufzunehmen.</sample>
    <sample id="236">Leider ist im Text nichts über die genauen 5 Anweisungen der Expert*innen erwähnt. Du könntest versuchen, den Text nochmal zu lesen oder nach weiteren Informationen zu suchen.</sample>
    <sample id="237">Sie schlagen vor, ein diagnostisches Test-Suite für die Wissensintegration zu erstellen.</sample>
    <sample id="238">MeetingBank ist ein neuer Benchmark-Datensatz für Meeting-Summariertchnologien. Er umfasst 1.366 City Council Meetings und 7.000 Instanzen. Es gibt Transkripte, Referenzsummen und URLs. Daten werden mit Speechmatics API ausgewertet. Die Summen werden auf Abstraktionsebene gemessen. GPT-3 hat gute Ergebnisse in Fluenz und Kohärenz, aber nicht in Informationsreichtum und Faktualität. MeetingBank dient als Werkzeug für Forschung und bietet Einblicke in Entscheidungsprozesse.</sample>
    <sample id="239">Hallo alle zusammen, mein Name ist David Vilar und ich werde einen kurzen Review des Papiers "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist ein gemeinsames Projekt mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden Parameter großer Sprachmodell, das im letzten Jahr 2022 vorgestellt wurde. Es wurde an einer großen Sammlung von Texten, die 780 Milliarden Tokens umfasst, trainiert. Bei der Veröffentlichung erreichte es den besten Stand der Technik in Hunderten von NLP-Aufgaben. In diesem Arbeit präsentieren wir die erste systematische Studie zur Prompting-Technik großer Sprachmodelle für maschinelle Übersetzung. Wir haben die Übersetzungsfähigkeit solcher Modelle unter Verwendung der besten Praktiken der MT-Gemeinschaft evaluiert. Das bedeutet, dass wir die neuesten Testsets verwenden, um eine Überschneidung der Testdaten mit den Trainingsdaten des Sprachmodells zu vermeiden. Und wir haben uns mit den besten Systemen verglichen</sample>
    <sample id="240">Hallo, ich bin Dawei, ein PhD-Student an der Saarland University in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit "Weaker Than You Think: A Critical Look at Weakly Supervised Learning" präsentieren. Es ist ein gemeinsames Projekt mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich möchte mit einer kurzen Einführung zu Weak Supervision und Weakly Supervised Learning beginnen. In Weak Supervision werden die Daten nicht manuell beschriftet. Stattdessen werden die Daten mit Weak Labeling Quellen beschriftet, wie beispielsweise einfache heuristische Regeln, Wissensbanken oder niedrigqualitative Crowdsourcing, wie im rechten Bild gezeigt wird. Im Vergleich zu menschlichen Annotierungen sind die schwächeren Annotierungen viel billiger, aber sie sind auch laut, was bedeutet, dass ein gewisser Teil der Annotierungen falsch ist. Wenn wir neuronale Netze direkt mit schwach beschrifteten Daten trainieren, tendieren die neuronalen Netze dazu, das Label Lärm zu merken und nicht zu generalis</sample>
    <sample id="241">This paper discusses a joint work on "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" with Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech. It points out that existing misinformation detection systems often have unrealistic evaluations and are not human-centric. The paper proposes an evaluation framework for systems that address these issues. It describes a workflow for detecting and verifying COVID-19 treatment misinformation. The first component detects misleading claims by filtering tweets and using a T5 model for claim extraction. The second component focuses on policy violation verification. The paper evaluates the early detection and policy violation verification parts of the workflow. It finds that the system has a high efficacy in detecting policy violations and provides a human workload metric. The framework aims to more realistically capture the interplay between systems and human content moderators. The work also motivates future human-in-the-loop misinformation detection systems and provides an outsider's view of misinformation detection system development and evaluation.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind die menschliche Bewertung, wie z.B. das Anfordern, dass menschliche Richter aus zwei Gesprächen wählen, welches besser ist, oder die Bewertung von Gesprächen anhand einer Likert-Skala.</sample>
    <sample id="243">Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Also insgesamt vier Autoren. Wenn du noch Fragen hast, lass sie gerne stehen.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen benötigt, dass Servin ein Richter ist und dass Richter in einem Gerichtssaal Entscheidungen über Fälle treffen.</sample>
    <sample id="245">This work presents a two-step pipeline for finding high-agreement Amazon Mechanical Turk, MTurk, Workers for summarization. It starts with qualification settings, including pre-task qualifications like location, number of HITs, and HIT Approval Rate. The first stage of the qualification task tests annotators' ability to evaluate multiple dimensions correctly, categorizing them into gold, silver, bronze, and block. Only gold and silver workers pass. The second stage, endurance tasks, tests capacity for handling heavy workload. Reference-based tasks assess general performance. Baseline MTurk workers are compared with statistical filter MACE and CloudResearch MTurk workers. Analysis of correctness across annotation sources shows significant Spearman's correlation between Pipeline and CloudResearch workers. The pipeline results in 6% of 200 participants achieving high agreement at a lower cost, serving as a best practice for high-agreement annotations. Future work includes investigating ways to hire high-quality workers and trying multiple applications for tasks, languages, and platforms. Limitations include only English summarization on MTurk and no guarantee for training of correctness.</sample>
    <sample id="246">Ja, der Code ist verfügbar. Er befindet sich auf GitHub.</sample>
    <sample id="247">The paper presents FACTKG, a new fact verification task using knowledge graphs. It introduces a dataset, FactKG, with claims in DBpedia and two styles. There are five reasoning types: one-hop, conjunction, existence, multi-hop, and negation. The dataset includes evidence retrieval and verification tasks. Two methods for colloquial style claims are used. The paper constructs baselines and shows that the GEAR model outperforms them. The dataset is available for download.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen. Sie stammen hauptsächlich aus englischsprachigen Ländern und haben meistens einen College-Abschluss.</sample>
    <sample id="249">Die Sätze wurden durch das Hinzufügen von akzeptablen oder unakzeptablen Präfixen zu den akzeptablen und unakzeptablen Sätzen innerhalb der akzeptablen Domain durcheinandergebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, mehrere Aspekte des Dialogqualität zu bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen.</sample>
    <sample id="251">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="252">The work presents U-CREAT, a method for unsupervised case retrieval in legal domain. It introduces IL-PCR dataset and U-CREAT pipeline. IL-PCR is a new benchmark for prior case retrieval, with 7, 070 legal cases and 6. 775 average citations. U-CREAT uses event-based approach. Event extraction is done using dependency parsing. The U-CREAT pipeline has three steps: pre-processing, dependency parsing, and post-processing. Event-based models outperform count- and transformer-based models. Event Filtered Documents model is the best-performing. U-CREAT outperforms existing approaches on COLIEE data set. It opens up avenues for further exploration in prior case retrieval.</sample>
    <sample id="253">This abstract presents the work "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media". It defines mental disorders and discusses the use of social media for research. The work aims to detect mental health disorders by analyzing social media posts. Domain adaptation is used to improve model performance on a target domain. The proposed approach integrates information from Reddit and mental health, and incorporates a lexicon for guided masking. Results on the eRisk datasets show DisorBERT's good balance between precision and recall. Examples demonstrate DisorBERT's focus on mental disorder-related words. The combined effect of double domain adaptation and guided masking is effective. Future work includes exploring different lexical resources and using clinical data.</sample>
    <sample id="254">Abstract: This research presents a document - level relation extraction framework with uncertainty - guided label denoising. It aims to improve the label quality of distantly supervised data. A pre - denoising DocRE model is trained with both DS and human - annotated data to generate pseudo labels. Uncertainty estimation is introduced to determine model prediction trustworthiness. An instance - level uncertainty estimation method is proposed for overlapping relations. A re - labeling strategy with dynamic class uncertainty thresholds is designed. A multi - phase training strategy is also developed to iteratively re - label DS data. The framework outperforms several strong baselines on public datasets. The main contributions include the uncertainty - guided label denoising framework, instance - level uncertainty estimation for overlapping relations, iterative re - labeling strategy with dynamic class uncertainty thresholds, and great performance improvements.</sample>
    <sample id="255">Die Form des Prompts ist wichtig bei zero and one-shot prompting.</sample>
    <sample id="257">Vier state-of-the-art chat models.</sample>
    <sample id="258">The paper discusses using large language models for evaluating text quality in natural language processing as an alternative to human evaluation. It proposes instructing large language models with natural language instructions to rate samples based on attributes like grammar, coherence, likability, and relevance. The experiment compares large language model ratings with human ratings by English teachers. Two large language models, Davinci and ChatGPT, show clear preference for human-written text, similar to human raters. This suggests that some large language models can be used as an alternative to human evaluation in this task. The paper also addresses potential questions about agreement between large language models and human evaluators, effects of instruction wordings and response sampling, and benefits and costs compared to human evaluation. It concludes that there are indeed some large language models that can be used as an alternative to human evaluation in this task.</sample>
    <sample id="259">XSemPLR ist ein Benchmark für die multilinguale semantische Parsing. Es umfasst 9 Datensätze, 5 Parsing Aufgaben, 8 Bedeutungsrepräsentationen und 22 Sprachen. Es gibt sechs Evaluierungssettings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot und Few-shot Transfer. Encoder-Decoder-Modelle zeigen die besten Ergebnisse. Sie können durch das Trainieren mit verschiedenen Sprachen verbessert werden. Cross-lingual Few-shot Transfer reduziert den Transfer-Abstand schnell. Encoder-Decoder übertrifft frühere Arbeiten oder erreicht vergleichbare Ergebnisse. Pretraining auf Englisch kann die Few-shot-Performance auf Zielsprachen erheblich steigern. Multilingual Sprachmodelle wie Codex und BLOOM sind für multilinguale semantische Parsing Aufgaben noch unzureichend. XSemPLR bietet eine einheitliche Plattform für die Evaluierung multilingualer Sprachmodelle.</sample>
    <sample id="260">Ein Autor.</sample>
    <sample id="261">Ein guter Planer sollte sinnvoll und den Bedingungen entsprechend schreiben.</sample>
    <sample id="262">Ich kann es leider nicht sagen, da der englische Inhalt nicht genügend Informationen enthält, um die Anzahl der Autoren zu bestimmen. Wenn du mehr Details hast, kann ich dir weiterhelfen.</sample>
    <sample id="263">This work investigates label bias problems in in-context learning for large language models. It starts by defining a typology of label biases, identifying domain-label bias as a new type. Experiments show that random in-domain words from the task corpus can bias model predictions, while random English words don't. To mitigate biases, a domain-context calibration method is proposed. It uses random in-domain words to estimate and calibrate the model's bias on label names. This method improves in-context learning performance, especially on tasks with large domain-label bias. It outperforms previous calibration attempts. The work aims to significantly enhance the performance of large language models in in-context learning.</sample>
    <sample id="264">Lin Wang, Zhejiang University, presents "TAVT: Towards Transferable Audio-Visual Text Generation". Existing uni-model text generation tasks have advanced due to large-scale pre-training and huge model capacity. However, multimodal text generation, like audio-visual text generation, faces challenges in data annotation and domain shifts. TAVT proposes a novel task to address these issues. It aims to train a model that can adapt to new multimodal domains with limited labeled data. The framework includes an audio-visual meta-mapper network, an encoder and language model generator, and counterfactual contrastive learning. The meta-mapper network maps visual concepts into a unified audio semantic space. The encoder and generator use a transformer-based approach with an alpha to evaluate modality contribution. A Dual Counterfactual Contrastive Learning is proposed for better visual-textual alignment. Experiments on MSVD and MSR-VTT benchmarks show TAVT outperforms SOTA models on cross-datasets and cross-domain settings, especially in low-resource domains. Ablation experiments analyze the impact of audio features.</sample>
    <sample id="265">Vasudha.</sample>
    <sample id="266">Ich weiß es nicht. Es gibt keine Angabe zur Universität der Autoren im Text.</sample>
    <sample id="268">Omission errors.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Und heute werden wir Ihnen alles über ABC-Eval erzählen, eine neue dimensionale Herangehensweise zur Bewertung von KonversationskI. Diese Arbeit wurde vom Emory NLP Lab unter der Leitung von Professor Jinho Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt. Lassen Sie uns sagen, dass Sie gerade einen Dialogmodell entwickelt haben und Sie möchten sehen, wie gut es im Vergleich zum aktuellen State-of-the-art steht. Die übliche Praxis ist, menschliche Bewertungen zu verwenden, z.B. indem man menschlichen Urteilen fragt, welche von zwei Konversationen besser ist oder Konversationen auf einer Likert-Skala bewertet. Diese Ansätze funktionieren gut, um eine umfassende Bewertung der Gesamtqualität von Dialogen zu ermöglichen, aber Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feineren Ebene zu verstehen. Eine Möglichkeit ist, menschlichen Urteilen</sample>
    <sample id="270">Die Autoren gehören an der Emory University.</sample>
    <sample id="271">CFT steht für Continuous Fine-Tuning.</sample>
    <sample id="272">Sechs.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit präsentieren, die den Titel "Wann erfordert eine Übersetzung Kontext? Eine datenbasierte, multilingualen Erforschung" trägt. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. Viele Übersetzungen hängen vom Kontext ab. Zum Beispiel, wie würden wir "Mole" in diesem Satz übersetzen? Wenn die vorherige Satz war "Dinge könnten gefährlich werden, wenn die Minister es herausfinden", dann bezieht sich "Mole" auf einen Spion. Aber wenn die vorherige Satz war "Kann es etwas Ernsthaftes sein, Doktor?", dann bezieht sich "Mole" auf eine Geburtstätte. So, abhängig vom Kontext, ändert sich die Bedeutung des Wortes, und somit auch seine Übersetzung. Allerdings ist es schwierig, wie gut Modelle solche Übersetzungen bewerten zu können. Zuerst, weil nur ein kle</sample>
    <sample id="274">Yusen Zhang.</sample>
    <sample id="276">Ananya und Vignesh präsentieren ihre Arbeit "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". Sie untersuchen die Evaluierung von Übersetzungen in andere Richtungen, was bisher unterbewertet ist. Fokus liegt auf fünf indischen Sprachen. Von Flores-Datensatz werden 200 englische Sätze ausgewählt. Diese werden an sieben Übersetzungsmodellen gesendet, um 1.400 Kandidatentranslationen pro Sprache zu erhalten. Bilingual Experten bewerten die Outputs, markieren Fehler und geben eine Gesamtschätzung. Fehlerarten werden nach Genauigkeit, Fluenz und speziellen Kategorien klassifiziert. Ergebnisse zeigen, dass COMET die besten Korrelationen mit menschlichen Bewertungen aufweist. IndicCOMET MQM übertrifft COMET-Baselines auf drei Sprachen und ist im Vergleich zu COMET MQM in allen Sprachen besser. IndicCOMET MQM ist auch robuster als COMET auf dem ACES Translation Accuracy Challenge Set.</sample>
    <sample id="277">Diese Methode hat keinen Namen.</sample>
    <sample id="278">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Art, die Wörter zu identifizieren, die Unterschiede zwischen markierten und unmarkierten Gruppen hervorheben. Sie benutzen den soziolinguistischen Begriff "Markedness", wonach es einen unmarkierten Standard gibt, und jede Gruppe, die von diesem abweicht, sprachlich markiert ist. Sie vergleichen die Personas mithilfe des "Fightin' Words"-Verfahrens, das gewichtete Log-Odds-Verhältnisse verwendet, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. So können sie positive oder zumindest nicht negative Wörter wie "tall" und "athletic" in den generierten Personas finden, die Stereotypen unterstützen.</sample>
    <sample id="279">Die Autoren gehören an der University of Washington.</sample>
    <sample id="280">The task of emotion regulation in conversations aims to predict the emotion label of each utterance. Challenges include underexploiting multimodal information complementarity, unsatisfactory performance in minority emotion classes, and difficulty distinguishing semantically similar emotions. This paper proposes MultiEMO, a novel attention - based correlation - aware multimodal fusion framework. Contributions include VisExtNet for visual feature extraction, MultiAttn for multimodal fusion, and Sample - Weighted Focal Contrast loss. VisExtNet captures facial expressions without redundant scene information. MultiAttn integrates modalities through cross - attention. SWFC loss focuses on minority classes. Experiments on MELD and IEMOCAP show state - of - the - art performance. Limitations include VisExtNet not distinguishing speakers and SWFC loss needing large batch size.</sample>
    <sample id="281">The work explores when translation requires context. It measures context dependence of words during translation using CXMI and its extension, Pointwise CXMI. Analysis on TED talk transcripts in 14 languages reveals patterns in context dependence for discourse phenomena like formality and lexical cohesion. A MuDA tagger is designed to identify these phenomena. Evaluation shows context-aware models outperform context-agnostic ones for some phenomena but not others. DeepL is more accurate than Google Translate for document - level translation. This work helps identify discourse phenomena models handle well and which translation systems excel at document - level.</sample>
    <sample id="282">Xuekai Zhu presents "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" at ACL 2023. This work focuses on story-level style transfer, a step forward from token or sentence level. The main challenge is imitating author's discourse-level linguistic choices and content association with writing topics. StoryTrans, the proposed model, learns discourse representations and combines them with style embeddings. It uses a two-stage training framework. The first stage uses self-reconstruction and disentanglement losses for style and content separation. The second stage fills in style-specific content. Extensive experiments on Chinese and English datasets show StoryTrans outperforms baselines in style control and content preservation. It can enrich storylines and rewrite sentences while maintaining source semantics. Data and code are included in the repo.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">Peng Tianshuo from Wuhan University presents a paper on FSUIE at ACL's Main Conference 4,915. It addresses limitations in span - based UIE models, like overreliance on precise span boundaries and mismatch between transformer feature extraction and information extraction. FSUIE proposes a fuzzy span mechanism. It uses fuzzy span boundaries instead of precise ones, and adaptive attention for span extraction. This helps model the span boundary as a continuous distribution. Experiments on named entity recognition, relationship extraction, and aspect sentiment triplet extraction show FSUIE's effectiveness. It achieves new state - of - the - art results in various tasks. The fuzzy span loss and attention mechanism enhance the model's performance and generalization capabilities.</sample>
    <sample id="285">Mingqi Gao from Peking University presents their work on "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework". They address factual errors in dialogue summaries. Two main solutions are introduced: introducing factuality-related objectives in summarization models and designing an independent Factual Error Correction model, FEC. FEC models are evaluated using factuality metrics like FactCC and DAE, but flaws are found. The authors argue for introducing manually annotated reference corrections to address these flaws. They propose a new taxonomy of factual errors and build an evaluation framework based on ERRANT. Key findings include the need to change FEC model evaluation methods and the potential of combining human-annotated and synthetic data. Current FEC models struggle with certain types of factual errors.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Fünf.</sample>
    <sample id="288">BLiMP und SyntaxGym.</sample>
    <sample id="290">Entschuldigung, aber ich habe den englischen Inhalt nicht verstanden. Könnten Sie bitte die Frage noch einmal klarstellen?</sample>
    <sample id="291">Named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit anonymisierten Daten aus dem Datensatz NACHOS trainiert.</sample>
    <sample id="295">Adam Przepiórkowski.</sample>
    <sample id="296">This work presents a collaboration between the University of Turin and Amazon Alexa on irony detection in natural language. It focuses on developing a corpus called EPIC for studying irony. Data was collected from social media sources over 1.5 years. Crowdsourcing was used for annotation with about 74 annotators. An annotation interface was developed. Perspective-aware models were trained on different annotator splits. Raw performance showed no clear trends. However, perspective-aware models showed higher confidence. Differences in annotations were observed, with age and geographical distribution of annotators showing significant variations. The work aims to improve irony detection in natural language processing.</sample>
    <sample id="297">This project explores dogwhistles in political speeches and their impact on language models. It develops a typology and glossary of dogwhistles, especially racist, transphobic, and anti-Semitic ones. A case study of historical U.S. political speeches reveals patterns related to the Republican Southern Strategy and conservatism. The project evaluates dogwhistle recognition in language models like GPT-3, finding varying performance. It also shows how dogwhistles evade content moderation through toxicity detection. Overall, it aims to better understand and combat the use of coded rhetoric in politics and online.</sample>
    <sample id="298">Wir haben ein Experiment durchgeführt, bei dem wir einige Modelle mit neueren Daten weitertrainierten oder forttrainierten. Wir fanden heraus, dass die Leistung mit größerem zeitlichen Abstand zwischen Trainings- und Testdaten abnimmt. Das bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsverlust die zeitliche Verzögerung ist.</sample>
    <sample id="299">The work discusses improving NLI model robustness. It identifies shortcuts as spurious correlations in datasets. Current shortcut mitigation methods have limitations like requiring domain-specific knowledge and pre-trained language models. The proposed minimax training method aims to reduce reliance on shortcuts. It focuses on hard examples that counteract dominant easy examples. The method uses a feed - forward network for the auxiliary and is evaluated on MNLI, FEVER, and QQP datasets. It shows consistent improvement in out - of - distribution performance while maintaining in - distribution accuracy. The work also examines performance in larger models, synthetic shortcuts, and out - of - domain test sets.</sample>
    <sample id="300">Interactive dictation is a process where users dictate and edit documents naturally. It involves flexible dictation and editing without trigger words. Users can correct themselves and issue commands like replacing words. The task is characterized by four steps: ASR recognition, segmentation, command extraction, and execution. The work introduces the task, designs a data collection interface, and builds a baseline system. It uses models like T5 and GPT-3 for different steps. There's a trade-off between runtime and accuracy. The paper provides more details.</sample>
    <sample id="302">Weil die Token für die Ausgabesequenz nicht in der richtigen Reihenfolge angeordnet sind, nachdem sie im ersten Schritt mit einem unordentlichen Multisatz von Tokens versehen wurden.</sample>
    <sample id="303">Weil wir nicht wissen, ob positive Stereotypen auf eine übertriebene Wertorientierung oder andere Anti-Stereotypen-Methoden zurückzuführen sind. Ohne mehr Transparenz können wir keine Annahmen machen und diese weiter untersuchen.</sample>
    <sample id="304">Inakzeptable Minimalpaareingaben sind solche, die nicht akzeptabel sind, zum Beispiel ungrammatische Sätze oder Sätze, die Stereotypen widersprechen.</sample>
    <sample id="305">Dawei, PhD student at Saarland University, presents "Weaker Than You Think: A Critical Look at Weakly Supervised Learning." They discuss weak supervision and weakly supervised learning. Weak supervision uses heuristic rules, knowledge bases, or crowdsourcing for data labeling, which is cheaper but noisy. In weakly supervised learning, training algorithms are proposed to handle label noise. Recent works claim high performance on clean test sets but often assume access to a clean validation set. Dawei's work shows that clean validation samples are necessary for WSL to work properly. Increasing clean validation samples improves performance, but direct fine-tuning on clean data can achieve better results. Dawei recommends reporting model selection criteria, comparing with few-shot learning baselines, considering continuous fine-tuning, and open-sourcing code.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim präsentieren eine kurze Übersicht über ihre Arbeit zum Thema Entity Tracking in Sprachmodellen. Ein Agent muss entitäten folgen und ihre Zustandsänderungen bei der Entwicklung einer Diskussion verstehen. Zum Beispiel in einem Rezept: Eier, Zucker und Mehl werden in eine Schüssel gelegt und dann zu einer leichten Teigmasse gemischt. Das ist ein wichtiger Faktor für das Verständnis längeren Diskussionen. Es gibt keine systematischen Untersuchungen zu den Fähigkeiten von prätrainierten Sprachmodellen bei der Entitätsverfolgung. Die Hauptfrage ist, in welchem Maße große Sprachmodelle entitäten verfolgen können. Es gibt Herausforderungen bei der Entwicklung einer Aufgabe zur Bewertung der Entitätszustandsverfolgungsfähigkeiten. Sie haben eine Aufgabe mit Boxen und Objekten entwickelt. Die Modelle müssen die Inhalte der Boxen vorhersagen. Die Ergebnisse zeigen, dass die meisten Modelle den ursprünglichen Zustand wiederholen. Nur text - davinci -</sample>
    <sample id="307">Die Autoren haben Named Entity Recognition, Klassifikation, Part-of-Speech Tagging und Fragebeantwortung verwendet.</sample>
    <sample id="308">Abstract: PhD student Jenny from Carnegie Mellon University presents work NLPositionality, characterizing design biases of datasets and models. It involves comparing annotations with real users and existing datasets/models. The framework re-annotates data sets with diverse annotators, then uses Pearson's R correlation score to compare. Over 16, 000 annotations from 1000 annotators in 87 countries are collected. Findings show positionality in NLP, e.g., alignment with English-speaking countries and college-educated people. Recommendations include recording design choices, perspectivism in research, and building specialized datasets/models for specific communities.</sample>
    <sample id="309">Inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Ich habe leider keine Informationen über die Universität der Autoren. Kannst du mir vielleicht mehr Details geben?</sample>
    <sample id="312">MultiInstruct ist der erste große Multi-Modell-Instruction-Tuning-Benchmark-Datensatz. Es besteht aus 62 verschiedenen Multi-Modell-Aufgaben, die 10 breite Kategorien abdecken. Diese Aufgaben werden aus 21 bestehenden offenen Quelldatensätzen abgeleitet und jede Aufgabe ist mit fünf Experten geschriebenen Anweisungen ausgestattet.</sample>
    <sample id="313">Zwei.</sample>
    <sample id="314">Binäre Koordination ist die Verbindung von zwei Elementen, die durch einen Koordinationskonjunktion verbunden werden.</sample>
    <sample id="315">Das ist eine interessante Frage, aber leider habe ich im Text nichts über die Länge der verwendeten Prompts in dieser Studie erfahren. Vielleicht können wir das nachforschen?</sample>
    <sample id="316">Das T5-fine-tuned auf CoScript kann Skripte von höherer Qualität generieren als die meisten großen Sprachmodelle. Das bedeutet, dass kleinere Modelle bei richtiger Ausbildung an geeigneten Datensätzen größere Modelle übertrumpfen können. Wenn du noch weitere Fragen hast, lass es mich wissen.</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors". Information extraction, like named entity recognition and relation extraction, is a classic NLP task. Previous models, using pre - trained language models like T5 and GPT - 3, had mismatched outputs. CodeIE transforms text - to - structured information extraction into a structure - to - structure code generation task. It uses code large language models like Codex. For named entity recognition, a prompt defines a function for named entity recognition, extracts entities, and initializes an entity list. For relation extraction, similar prompts are designed. Evaluated on datasets, CodeIE using code language models and code format prompts significantly outperforms traditional text - style prompts. It has lower perplexity on text format inputs, fewer structural errors, and better recall. This analysis provides inspiration for information extraction tasks.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen unsere Arbeiten über "DrBERT: Ein robustes vorkonfizuriertes Modell auf Französisch für biomedizinische und klinische Domänen" vorstellen. In dieser Präsentation sprechen wir zuerst über Sprachmodellierung im Gesundheitswesen. Dann werden wir die Hauptbeiträge unseres Artikels vorstellen. Wir stellen das erste biomedizinische Modell auf Französisch vor, DrBERT, das auf RoBERTa basiert und auf NACHOS, einer Datenbank medizinischer Webkrawle, trainiert wurde. Wir haben auch eine Vergleichsanalyse von Modellen mit verschiedenen Vorkonfigurationen und Datenquellen vorgestellt. Anschließend präsentieren wir unsere Ergebnisse an 11 biomedizinischen und klinischen Aufgabenstellungen auf Französisch. Und schließlich geben wir Ihnen am Ende der Experimente Einblicke und verleihen Ihnen mehr Details dazu, wie Sie diese Modelle zugänglich machen können. Seit seiner Einführung im Jahr 2018 ist BERT</sample>
    <sample id="319">Die Arbeit untersucht die Lernstrategien von from-scratch-Pre-Training und control Pre-Training.</sample>
    <sample id="320">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführen ist, ist größer als 1.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde beurteilt, indem man die Art der Vereinfachung analysierte, wie zum Beispiel die Stärke der Vereinfachung in Bibeltexten im Vergleich zu News-Texten oder Texten für Sprachlerner. Es wurde auch die Vielfalt der verschiedenen Vereinfachungstransformationen untersucht.</sample>
    <sample id="322">Enrico präsentiert bei ACL 23 über "Was ein Text-Klassifikator über Moralität lernt". Er erklärt, dass Moralität das Unterscheiden von Gut und Böse ist und grundlegend für Gesellschaften. Moralschätzung in Texten wird oft als Skala zwischen immoral und moral behandelt, aber es ist subjektiv. Moral Foundation Theory gibt fünf verschiedenen moralischen Grundlagen an. Enrico und seine Kollegen verwenden erklärbare KI-Techniken, um zu verstehen, wie Moralschätzung in verschiedenen Domänen unterschiedlich ausgedrückt wird. Sie testen das mit dem Moral Foundation Twitter Corpus. Sie finden heraus, dass Sprachmodelle die Unterschiede in der Moralsprache in verschiedenen Domänen erkennen können, wie z.B. zwischen ALM und BLM. Moralsprache ist in verschiedenen Domänen unterschiedlich, und ein einziges Modell für viele Domänen kann zu Fehlinterpretationen führen.</sample>
    <sample id="323">Yujie Wang from Shanxi University presents a paper on "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA". It addresses challenges in Commonsense QA, like noisy entities and limited interaction between modalities. The proposed DHLK builds an HKG based on multiple knowledge bases, optimizing structure and knowledge representation. It uses language models to encode and fuse QA contexts and entities, dynamically removing weakly relevant entities. Relation Mask Self-Attention is introduced for subgraph modeling. Experiments on CommonsenseQA and OpenBookQA show good results compared to other methods.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen einen kurzen Einblick in unser Papier über "Kompositionelle Generalisierung ohne Bäume unter Verwendung von Multiset-Tagging und versteckten Permutationen" geben. Dies ist ein gemeinsames Projekt mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionelle Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tieferen Rekursion und unerwartete Kombinationen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden. Im Kontext des semantischen Parsing könnte die Generalisierung wie folgt aussehen. Wir haben wie gewöhnlich einen Trainingsdatensatz von Utternances. In diesem Fall: "Die Mädchen schliefen." und "Mary wusste, dass die Mädchen schliefen." Diese Utternances sind mit logischen Formen verbunden, die die zentralen Aspekte ihrer Bedeutung darstellen. Im Gegensatz zur standardmäßigen maschinellen Lernbewertung enthält das Testset nicht die gleiche Verteilung, sondern enthält strukture</sample>
    <sample id="326">Kognitive Dissonanz ist, wenn zwei Glaubensvorstellungen oder Handlungen nicht übereinstimmen. Zum Beispiel wenn jemand sagt, dass Zigaretten tödlich sind, aber trotzdem raucht.</sample>
    <sample id="327">Xiao Xu, a third-year PhD student from Harbin Institute of Technology, presents "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" at ACL 2023. This work, done during an internship in the MSRIC group with Intel Cognitive Computing Group support, aims to train a smart AI system understanding both image and text. Vision-Language learning has seen progress with large-scale self-supervised pre-training and transformer-based models. The work proposes ManagerTower, a novel VL modal architecture. It uses RoBERTa and CLIP-ViT base as unimodal encoders. ManagerTower adapts insights from pre-trained unimodal experts at different levels, improving performance, especially on Wikivideo test standard by 39.15%. It outperforms METER and BridgeTower, showing effective exploitation of different levels of universal semantic knowledge. Paper, code, and modals are available on Archive and Github.</sample>
    <sample id="328">GPT-4.</sample>
    <sample id="329">This work presents a zero-shot video sentence localization method. It focuses on finding relevant video segments for given natural language queries. Existing methods require manual annotations, which are costly. This work proposes a noise-resistant Structured Pseudo-Label generation method. It uses a pre-trained image caption model to generate complex pseudo-queries. Then, it measures relevance between frames and queries to generate pseudo-events. It also reduces noisy samples' influence. Experiments on ActivityNet Captions and Charades-STA show better performance than existing methods on most metrics. Code is available.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen von einem parallelen Korpus.</sample>
    <sample id="333">The work introduces INK, a framework for injecting kNN knowledge into nearest neighbor machine translation. It aims to improve NMT model generalization and performance. INK has two steps: extracting kNN knowledge to guide adapter adjustment and updating representations asynchronously. It optimizes the adapter with a combined learning objective. Experiments show INK outperforms state-of-the-art kNN-MT systems, achieving higher BLEU scores with less memory and faster inference. It addresses NMT's non-smooth representation space, especially for low-frequency tokens.</sample>
    <sample id="335">Alexander Koller und Ivan Titov.</sample>
    <sample id="336">Sprachübergreifender Transfer ist der Prozess, bei dem ein Modell in einem Quellsprache trainiert wird und dann auf eine ZielSprache übertragen wird.</sample>
    <sample id="337">The work presents a graph-based approach for out-of-vocabulary, OOV, word embedding learning. It focuses on handling OOV words by observing their word formation and associating them with relevant words. A Word Relationship Graph is introduced, mimicking lexical rules. The OOV word is tokenized and associated with other words, forming a two-level graph. A self-attention network assigns attributes to OOV nodes. Graph Attention Networks are used to create a node-level representation. A readout block layer summarizes the graph information. A simple Graph Convolutional Network is applied. Contrastive learning is used in the loss function. Experiments show the model outperforms baselines. It benefits static and contextual models. Agglutinative languages are well-suited, while fusional languages present challenges. English performs well with reasonable word segmentation. The graph can handle various word formations. The model's application to other languages depends on word decomposition rationality.</sample>
    <sample id="338">The research explores the evaluation of human natural language explanations. It aims to address the question of how to objectively assess the quality of human annotations. The work presents a unified data structure for various tasks and proposes a novel metric, TREU, which extends the simulatability score. TREU evaluates the helpfulness of explanations during fine-tuning. The study uses five datasets and two models to demonstrate that TREU outperforms the simulatability score. It shows that human explanations can still benefit model predictions, even if considered low quality by humans. The research lays the foundation for high-quality human collaboration in annotation jobs.</sample>
    <sample id="339">Die Autoren gehören an der Saarland University in Deutschland.</sample>
    <sample id="340">Kuan-Hao Huang präsentiert ParaAMR, ein großes syntaktisch vielfältiges Paraphrasendatensatz durch AMR-Back-Translation. Es ist ein gemeinsames Projekt mit mehreren Kollegen. Paraphrasierung ist wichtig in der NLP. Es profitiert von Anwendungen wie Fragebeantwortung und Chatbots. Für eine gute Paraphrasengenerator braucht man große, hochwertige Paraphrasendaten. Bestehende human-annotierte Datensätze sind qualitativ hoch, aber skaliert begrenzt. Automatisch generierte Datensätze wie Back-Translation haben syntaktische Mangeln. ParaAMR verwendet AMR-Grafen, um syntaktisch vielfältige Paraphrasen zu generieren. Es hat etwa 15 Millionen Quellsätze und 6,9 Paraphrasen pro Satz. Es hat höhere syntaktische Diversität als andere Back-Translation-Datensätze, während die semantische Ähnlichkeit ähnlich ist. Es profitiert mehrerer NLP-Anwendungen, wie dem Lernen von Satz-Embeddings, syntaktischer Kontrollparaphrasierung und Datenverstär</sample>
    <sample id="341">Die Autoren verwenden die durchschnittliche Latenz und die computeraufwandsbewusste durchschnittliche Latenz.</sample>
    <sample id="342">The paper presents LiveChat, a large-scale personalized dialogue dataset automatically constructed from live streaming. It introduces open-domain and personalized dialogue, highlighting the significance of video-sourced datasets. The dataset is constructed in three steps: extracting audio from videos, collecting audience comments, and collecting persona information. Compared to existing datasets, LiveChat is video-sourced, has a larger scale, and includes longer average sessions. Experiments on response modeling and addressee recognition show the advantages of selected persona profiles and average sessions. BART outperforms other models, and in-context learning demonstrates the influence of demonstrations. Future work focuses on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">Hallo alle zusammen, ich bin Akshatha, und heute präsentieren mein Co-Autor Martin und ich unsere Arbeit "The KITMUS Test: Evaluierung der Wissensintegration aus mehreren Quellen". Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natürliche Sprachverständnismodelle greifen auf eine Vielzahl von Wissensquellen zurück, wie zum Beispiel das Wissen, das in ihren Parametern enthalten ist, gewöhnlich durch eine Vorkennzeichnung erworben, und das Wissen, das bei der Inferenzzeit in den Eingaben gegeben wird. Neueste Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle das Wissen aus der Vorkennzeichnungzeit verwenden können, um die Aufgabe zu lösen. Aber das natürliche Sprachverständnis erfordert oft Wissen, das auch bei der Inferenzzeit bereitgestellt wird. Zum Beispiel in der Satz "John sah den neu gewählten Präsidenten im Fernsehen". Vorkennzeichnungsparameter können Informationen über was Präsidenten tun und was ein Fernseher ist enthalten</sample>
    <sample id="344">Baumbasierte Methoden sind oft kompliziert und computationsaufwändig zu erlangen. Sie erfordern oft formalspezifische Vorverarbeitung der logischen Formen, um Variable Symbole zu behandeln. Oft müssen spezielle Grammatik-induktion-Procedures verwendet werden.</sample>
    <sample id="345">Abstract: Matthias Lindemann, Alexander Koller, and Ivan Titov introduce a paper on compositional generalization without trees using multiset tagging and latent permutations. They address the challenge of handling unseen compositions of phrases. Their model, a neural seq2seq, directly models input-output correspondences. It tags input tokens with multiset tags and then predicts a permutation to order them. This avoids trees, which can be computationally expensive. The model outperforms others on the COGS benchmark for deeper recursion generalization. It faces challenges like alignment and finding linguistically plausible permutations. The authors solve these by inducing alignment during training and using a GPU - friendly continuous relaxation for permutation finding.</sample>
    <sample id="346">Ich habe leider keine Informationen über die Universität der Autoren. Du könntest versuchen, das in der Paper zu suchen oder mich zu fragen, ob du weitere Details hast.</sample>
    <sample id="347">Hallo Myra, ich werde den englischen Inhalt ins Deutsche übersetzen. Hier ist die Übersetzung: "Hallo, ich bin Myra und heute werde ich über unser Papier 'Markierte Personas: Verwendung natürlicher Sprachanweisungen zur Messung von Stereotypen in Sprachmodellen' sprechen. Dieses Werk wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky durchgeführt. In den letzten Jahren wurde dokumentiert, wie häufig soziale Voreingenommenheit und Stereotypen in großen Sprachmodellen, oder LLMs, vorkommen. Allerdings haben diese Maßnahmen verschiedene Einschränkungen. Sie stützen sich meistens auf von Hand konstruierte Datensätze, die sehr zeitaufwendig zu kuratieren sind, und sie messen normalerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demographien oder Kontexte übertragbar sind, oder sie fangen einfach sehr allgemeine breite Assoziationen wie negative Assoziationen mit bestimmten Gruppen ein. Zudem berücks</sample>
    <sample id="348">The paper "Marked Personas" aims to measure stereotypes in large language models. It overcomes limitations of previous measures by using natural language prompts to generate personas. The method relies on newer instruction - tuned LLMs' ability to respond to prompts. It has two parts: generating personas and using the Marked Words method to identify words distinguishing marked groups. Results show generated personas contain more stereotypes than human - written ones. The paper recommends researchers address positive stereotypes and use an intersectional lens. Increased transparency about bias mitigation methods is also suggested.</sample>
    <sample id="349">Hallo alle zusammen, mein Name ist Jingwei Yi von der Universität für Wissenschaft und Technologie China. Es ist mir eine Freude, einen kurzen Werbevideo für unser Papier zu präsentieren. Wollen Sie mein Modell kopieren? Schützen Sie das Urheberrecht von großen Sprachmodellen für die Einbettung als Dienstleistung über Backdoor-Wasserzeichen. Lassen Sie uns zuerst die Hintergrundinformationen zur Einbettung als Dienstleistung vorstellen. Gegenwärtig sind große Sprachmodelle wie GPT, LLAMA, PALM in der natürlichen Sprachverstehung und -generierung außergewöhnlich. Die Einbettung als Dienstleistung ist eine Dienstleistung, die auf großen Sprachmodellen aufgebaut ist, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Einbettungs-API. Allerdings haben kürzlich Arbeiten gezeigt, dass der Angreifer das Modell durch das Lernen aus der Einbettung stehlen kann und ähnliche Dienstleistungen bereitstellt. Daher ist</sample>
    <sample id="350">The paper presents an investigation into superhuman performance in NLU. It discusses how leaderboard-based evaluation has led to the pursuit of top spots in benchmarks. It highlights that while systems can achieve high scores, there are issues like brittle performance, lack of generalization, and reliance on spurious patterns. The paper analyzes SuperGLUE and SQuAD benchmarks, showing that humans are outperformed in many tasks. However, it also points out errors in the datasets, such as different evaluation sets and ground - truth answer errors. The paper argues that claims of superhuman performance are not yet grounded due to these issues and provides recommendations for better benchmark construction.</sample>
    <sample id="351">The paper investigates if CoNLL-2003 named entity taggers still work well in 2023. It looks at generalization problems in NER. Models from CoNLL-2003 are used for almost 20 years. The paper develops CoNLL++ dataset from 2020 Reuters News. It fine-tunes over 20 models on CoNLL-03 and CoNLL++. Good generalization needs better model architecture, larger model size, and more fine-tuning examples. Performance drop is mainly due to temporal drift. The conclusion is that CoNLL-2003 taggers still work well in 2023. The paper calls for more research on improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für annotating behaviors in chat. Es ist eine Methode zur präzisen und zuverlässigen Dimensionaler Bewertung von Chatmodellen.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" addresses the challenge of input underspecification in code generation. It proposes interactivity through clarification questions to gather more specifications. A method to create CodeClarQA, a synthetic dataset with clarifications on key operations, is introduced. The pipeline includes a Clarification Need Predictor, Question Selector, and Code Generator. The paper shows good performance in identifying missing key operations, with MPNet performing best. Errors and challenges are analyzed, including taxonomy and argument issues. The CQ-driven code generation pipeline is tested, showing improvements with more high-ranked CQs answered. However, the pipeline underperforms the model-only trainer. Clarified key operations are found to contribute to better generated code. The paper invites feedback on the paper and code.</sample>
    <sample id="354">Das Leistungsdelta ist bis 2022 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo Vasudha, ich bin ein Computer Science PhD-Kandidat an der Stony Brook University. Ich möchte unsere Arbeit, die in ACL 2023 als Langpapier akzeptiert wurde, präsentieren, "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." Wir beginnen damit, kognitive Dissonanz zu definieren und warum es ein wichtiges Problem zur Untersuchung in der Sprache ist. Einfach gesagt, kognitive Dissonanz ist die Inkompatibilität von zwei Überzeugungen oder Handlungen, wie in diesem Beispiel: "Ich weiß, dass Zigaretten mich töten könnten", und dann geht man nach dem Meeting auf ein paar Zigaretten. Diese Überzeugung und Handlung sind inkonsistent und in Dissonanz. Weitere Angabe, dass "Ich glaube, ich könnte meinen Job ohne sie nicht behalten" rechtfertigt die zweite Vorkommnis. Sie haben eine Konsistenz-Beziehung. Während Dissonanz eine sehr häufige Phänomen in unserem täglichen Entscheidungsprozess ist, ist sie unter anderen Arten von Diskursbeziehungen</sample>
    <sample id="356">Das ist eine Frage, die ich nicht beantworten kann, da ich keine Informationen über die Universität der Autoren habe. Du könntest versuchen, das in der Paper zu finden oder die Autoren direkt zu kontaktieren.</sample>
    <sample id="357">SiYu Yuan.</sample>
    <sample id="358">Fünf. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="359">Mit der Wait-k strategy und der Local Agreement wird der Ansatz verglichen.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University and research director at JP Morgan AI Research, presents "CounterComp". This work focuses on using counterfactual scenarios to improve compositional generalization for multi - step quantitative reasoning in question - answering tasks. State - of - the - art models struggle with tasks involving more than two steps due to memorizing spurious patterns. CounterComp mines counterfactual scenarios from training samples to add an auxiliary metric learning loss. This loss helps the model attend to meaningful tokens related to operations in the output. The method improves performance on in - and out - of - distribution samples, enhancing compositional generalization. It shows that CounterComp loss aids in mining relevant tokens during training. For more details, check the poster or contact the listed person.</sample>
  </task>
</testset>