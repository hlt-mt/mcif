<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I principali fonti di dati per i modelli linguistici sono le grandi scale di crawl web.</sample>
    <sample id="1">Gli autori dell'articolo sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">The paper focuses on Visually-rich Document Understanding, VRDU, problem. It aims to understand forms, receipts, and posters. Pre-training techniques, especially self-supervised multi-modal models, have shown success in VRDU tasks. However, existing models face reading order issues. The authors propose LayoutMask, a novel pre-trained model. It uses text and layout info as input to enhance text-layout interactions and layout representations. LayoutMask differs from previous studies in 1D position choice, masking strategy, and pre-training objectives. It uses local 1D position instead of global 1D. Two novel masking strategies are introduced: Whole Word Masking and Layout-Aware Masking. A new pre-training objective, Masked Position Modeling, is also proposed. Experiments show that Local-1D outperforms Global-1D on FUNSD and SROIE datasets, especially for the entity "Total". For more details, refer to the paper and posters.</sample>
    <sample id="3">Ciao! Benvenuto alla presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase. Il mio nome è Regina Stodden, e vi guiderò attraverso la prima parte della presentazione. Innanzitutto, definiamo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con problemi di lettura o non native. Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio di documenti o frasi. Ecco un esempio di una coppia parallela di frasi allineate di un complesso testo tedesco e della sua traduzione in lingua semplice. Per semplificare la frase, sono possibili diverse tecniche, come la sostituzione lessicale, la cancellazione di clausole, la ridistribuzione o l'aggiunta di parole. Ora</sample>
    <sample id="4">Il nome della relatrice o del relatore è Kayo Yin.</sample>
    <sample id="5">Il modello che ha ottenuto l'accuratezza dell'82%-87% è il T5 XL model.</sample>
    <sample id="6">Jiaan presenta il lavoro "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" con Fandong, Duo, Yunlong, Zhixu, Jianfeng e Jie. Unificano multilingue e cross-lingue in un modello unico, many-to-many, che può processare documenti in qualsiasi lingua di origine e generare riassunti in qualsiasi lingua di destinazione. Studi preliminari mostrano che questo modello trasferisce meglio il know-how tra lingue. Propone PISCES, un modello pre-allenato many-to-many. Confrontano con mBART-50 e mT5. Risultati: PISCES supera le basi. Per dettagli, leggi il paper.</sample>
    <sample id="7">Sì, funzionano ancora.</sample>
    <sample id="8">The novelty of the proposed human evaluation method is that it explicitly annotates whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. This approach aims to reduce the subjectivity of human evaluation.</sample>
    <sample id="9">L'attuale approccio scarsamente supervisionato si basa in larga misura sulle annotazioni pulite.</sample>
    <sample id="10">Per migliorare il punteggio, si può fare progresso in diversi modi. Ad esempio, se il modello ha accesso a conoscenza di fondo parziale che si sovrappongono, il punteggio può aumentare fino a 82-87%. Inoltre, se il modello ha accesso solo ai nomi dell'entità, il punteggio è solo del 60%. Ci sono quindi molte opportunità di miglioramento. Se hai altre domande o vuoi discutere di questo in modo più approfondito, fammi sapere.</sample>
    <sample id="11">Il contenuto parla di un lavoro di ricerca su come i modelli di linguaggio AI capiscano l'umorismo. Si presenta un contest di captions per cartoni del New Yorker. I modelli vengono testati in tre task: matching, ranking di qualità e generazione di spiegazioni. Sul matching, il modello migliore raggiunge circa il 62% di accuratezza, mentre gli umani arrivano al 94%. I modelli non ottengono spiegazioni perfette, con errori evidenziati. Ci sono differenze significative tra le spiegazioni umane e quelle dei modelli. L'idea è di fornire un dataset per ulteriori esperimenti e di avere un leaderboard e modelli disponibili. Se hai domande o commenti, puoi farlo.</sample>
    <sample id="12">Cinque.</sample>
    <sample id="13">Daniel Rotem presented his work on adaptive inference in low resource settings. He discussed two methods: Multi Model and Early Exit. Multi Model is versatile but expensive to store and has overhead. Early Exit is faster but can suffer from conflicting gradients. His team compared Multi Model and Early Exit classifiers and found Multi Model outperformed Early Exit on average by 2.3%. They introduced SWEET, a novel fine - tuning method for Early Exit architectures, which avoids conflicting gradients. SWEET closes most of the gap between Early Exit and Multi Model but can negatively affect later classifiers in some cases. In terms of speed/accuracy trade - off, SWEET outperforms both methods at fast speeds. The work shows conflicting gradients in Early Exit training and provides a fair comparison of Multi Model and Early Exit. It also motivates future research on fine - tuning algorithms for Early Exit architectures.</sample>
    <sample id="14">Ciao, il mio nome è Adam Przepiórkowski e questo discorso è sulla struttura di dipendenza della coordinazione. Come potete sapere, ci sono diverse strutture di dipendenza assunte da teorie e approcci di corpus diversi. Ad esempio, nelle universal dependencies, la struttura della coordinazione, Lisa, Bart, e Maggie, è tale che il primo congiunto è il capo della struttura coordinativa. In questo caso, Lisa. Un approccio simile è assunto in Igor Mel'čuk's meaning text theory, dove di nuovo, la struttura coordinativa intera è capita dal primo congiunto. Questi due approcci sono asimmetrici. Sono che singolano uno dei congiunti. Ora, questi sono approcci asimmetrici alla struttura della coordinazione, come il Prague approach. L'approccio coordinato capito assunto nei albero di dipendenza di Prague, dove le strutture coordinate sono capite dal congiunto. Così, otteniamo alcune dipendenze dall'end alli tutti i congiunti. Inf</sample>
    <sample id="15">Tre.</sample>
    <sample id="16">I domini risultano più semplificati sono i testi biblici.</sample>
    <sample id="17">Shengqiong Wu, un dottorando di NUS, introduce il loro lavoro sull'estrazione di relazioni multimediali. L'estrazione di relazioni cerca di determinare la relazione semantica tra entità in un testo. Ma in social media, i dati sono in varie forme e modali. Così, l'estrazione di relazioni multimediali è stata introdotta. Aggiungono fonti visive al testo. Ad esempio, con "Bachelor", "Gown", "Cap", possiamo facilmente inferire che JFK ha "graduato" a Harvard. Ma ci sono problemi. Uno è l'over-utilizzo dell'informazione interna. L'altro è l'under-exploitation dell'informazione esterna. Proppongono un principio di bottiglia di informazione di rete per raffinare le caratteristiche. Considerano anche informazioni tematiche multimediali. Esperimenti su un dataset MRE dimostrano che le caratteristiche visive migliorano le prestazioni. L'analisi di ablation mostra che la selezione e la compensazione dell'informazione interna ed</sample>
    <sample id="18">L'esempio è "salt and pepper" invece di "pepper and salt", misurato in sillabe.</sample>
    <sample id="19">Il contenuto audio parla di un lavoro accettato ad ACL 2023 intitolato "A Survey for Efficient Open Domain Question Answering". Il lavoro si concentra sull'open-domain question answering. Si menziona un modello a due fasi proposto da Danqi Chen nel 2017. La prima fase utilizza un retriever per estrarre contesti da un corpus di Wikipedia e la seconda fase usa un reader per comprendere la domanda e ragionare per ottenere l'answer. Ci sono alcuni ostacoli come la grandezza del corpus di Wikipedia e la dimensione dell'index file. Il lavoro ha come motivazione ottenere sistemi di open-domain question answering efficienti, con meno costi di memoria, inferenza più veloce e prestazioni paragonabili. Si presentano tecniche come ricerca veloce, lettura veloce e riduzione dell'index size. Si confrontano modelli esistenti e si fanno conclusioni e future works. Se hai domande o commenti, puoi dirmelo.</sample>
    <sample id="20">Sì, puoi usare i modelli per la tua ricerca. Sono disponibili gratuitamente su Hugging Face sotto licenza MIT e i script di addestramento sono sul loro GitHub.</sample>
    <sample id="21">DEPLAIN-apa contiene documenti di notizie.</sample>
    <sample id="22">Un buon modello architetturale, un modello di dimensione maggiore e un maggior numero di esempi di ottimizzazione.</sample>
    <sample id="23">Il lavoro di Dan Garrette riguarda migliorare la capacità dei modelli di immagini di testo a rappresentare testo visivo. I modelli di testo immagine hanno fatto grandi progressi, ma spesso falliscono a rappresentare correttamente il testo. L'Imagen modello, che utilizza un encoder T5-XXL per codificare il testo e un modello di diffusione per generare l'immagine, può produrre immagini accurate per input complessi, ma fallisce spesso con input semplici che richiedono l'immagine contenere una parola. L'encoder T5 usa la tokenizzazione SentencePiece, che divide le parole in chunk subword. Questo rende difficile per il modello rappresentare correttamente le parole. I modelli PaLM, con tokenizzazione più granulare, hanno un'accuratezza di拼字接近 perfetta, ma sono più grandi e non sono pratici per molte applicazioni. ByT5, che riceve i byte dell'input, ha un'accuratezza di拼字 molto alta a tutti i livelli. Per migliorare la rappresentazione del</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata misurando la lunghezza in caratteri, sillabe e parole.</sample>
    <sample id="25">Gli esperimenti sono stati progettati misurando la lunghezza in caratteri, sillabe e parole. Si concentra sulle parole.</sample>
    <sample id="26">Se addestrato su dati non bilanciati, il classificatore base non è molto efficace.</sample>
    <sample id="27">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni o controllare il testo dell'articolo?</sample>
    <sample id="28">Bob e Alice.</sample>
    <sample id="29">I modelli sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per fenomeni del discorso come la formalità e la coesione lessicale.</sample>
    <sample id="30">Il team AI2 e USC ha presentato "LLM-Blender", un framework di apprendimento ensembe semplice ma efficace per grandi modelli linguistici. La loro idea chiave è basata su ranking a coppie e fusione generativa. Molti modelli linguistici sono rilasciati ogni settimana, ma la loro performance media non è sempre la migliore per ogni esempio. L'LLM-Blender propone un'approccio a due fasi: prima, i modelli vengono eseguiti e le loro output sono confrontate usando il PairRanker, un modulo di ranking a coppie. Successivamente, i top K candidati vengono usati in un modello generativo per la fusione e l'output finale. Il PairRanker differisce dai metodi precedenti perché analizza le differenze tra i candidati insieme all'input. L'LLM-Blender ha ottenuto risultati migliori di altri metodi di ranking su vari metrici di correlazione. La loro nuova dataset, MixInstruct, è stata creata per valutare l'ensemble learning. L'LLM-</sample>
    <sample id="31">Non so qual è l'affiliazione degli autori dell'articolo. Potresti cercare di trovare queste informazioni sul sito del loro atto di pubblicazione o sul loro profilo personale.</sample>
    <sample id="33">Il framework quantifica la posizionalità attraverso la comparazione delle annotazioni di diversi utenti con quelle dei modelli e dei dataset. Utilizza una correlazione di Pearson per misurare la somiglianza.</sample>
    <sample id="34">Il lavoro presentato è "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation". È un risultato di collaborazione tra Marcos Treviso, Alexis Ross, Nuno Guerreiro e André Martins. CREST combina metodi di interpretazione del decisione e generazione di counterfactuals. Il primo componente genera counterfactuals, mentre il secondo produce rationale. Per valutare la qualità, si utilizzano metriche automatiche e umane. CREST produce counterfactuals validi e naturali. Si può anche utilizzare CREST per l'aumento dei dati. Inoltre, CREST-Rationalization produce rationales plausibili e con alta counterfactual simulability. Per ulteriori informazioni, si può consultare il paper e il codice.</sample>
    <sample id="36">L'articolo presenta un lavoro su "Learning Language-Specific Layers for Multilingual Machine Translation". Si parla delle vantaggi della traduzione multilingue come scalabilità, velocità e miglioramenti per le coppie di lingue a risorse basse. I limiti includono capacità limitata per ciascuna lingua. L'obiettivo è aumentare la capacità per lingua solo dove è più importante. La soluzione proposta è Language-Specific Layers, LSLs, che permettono di mantenere i costi di inferenza costanti. Si concentra sull'encoder e si usa un modello grande per imparare la posizione delle LSLs. Dopo aver selezionato l'architettura, si addestra una nuova architettura. I risultati mostrano miglioramenti significativi per tutte le lingue, specialmente per quelle a risorse basse. Le migliorie sono statisticamente significative per 84 su 90 direzioni di traduzione. Per ulteriori dettagli, si consiglia di consultare il full paper o il poster session.</sample>
    <sample id="37">The study found that by giving the prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="38">Le fonti di dati utilizzate sono l'enhanced versione del Penn Treebank e il paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni?</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di discorsi debole e di espansione e confronto.</sample>
    <sample id="41">Il lavoro di Silin e del team dal Laboratorio di Processamento del Linguaggio Naturale all'EPFL University, in collaborazione con Sony Group Corporation, si concentra su PeaCoK, una Persona Commonsense Knowledge Graph. Questo grafico rappresenta conoscenze di mondo riguardanti personaggi reali, con circa 3.800 personaggi e 40.000 attributi distintivi. È stato costruito in tre passaggi: selezione di personaggi da grafici di conoscenza comune, induzione di attributi da grafici e modelli linguistici pre-addestrati, e annotazione delle relazioni tramite un voto maggioritario umano-AI. PeaCoK aiuta i modelli di linguaggio a imparare e generalizzare conoscenza di persona. È stato utilizzato per migliorare la generazione di dialoghi e la modellazione di narrativa. La sua conoscenza persona-centrica ha un impatto positivo maggiore rispetto a conoscenza sociale generale. Il grafico è pubblico e disponibile su GitHub e sul sito del laboratorio.</sample>
    <sample id="42">I'm not sure. The text doesn't mention the number of authors involved in the article. You could try looking for more information in the full paper or other sources.</sample>
    <sample id="43">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni o controllare il documento?</sample>
    <sample id="44">Il framework differisce dai lavori precedenti perché confronta end users con modelli e dataset, non solo annotatori.</sample>
    <sample id="45">Le persone generate da modelli di linguaggio hanno un lessico degli stereotipi più alto rispetto a quelle scritte da umani.</sample>
    <sample id="46">Sono stati messi a confronto DeepL e Google Translate.</sample>
    <sample id="47">Ciao, sono Shangbin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro "Da i dati di pretraining ai modelli di linguaggio ai compiti di livello inferiore: tracciare le tracce dei pregiudizi politici che portano a modelli di NLP non equi". Quindi, i modelli di linguaggio sono addestrati su grandi quantità di dati di web crawl. I media politici sono ben coperti nei loro dati di pretraining. Secondo una ricerca del Corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc. sono ben coperti negli addestramenti dei modelli di linguaggio. Questo ha creato un dono e un problema per le applicazioni dei modelli di linguaggio. D'una parte, sono riusciti a imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee. D'altra parte, queste diverse opinioni politiche sono socialmente pregiudiziali e potrebbero portare a problemi di equità nelle applicazioni</sample>
    <sample id="48">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni o controllare il testo originale?</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">Il riassunto è: La presentazione introduce DEPLAIN, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e frase. Definisce la semplificazione testuale, che adatta testi per migliorare la comprensione di gruppi specifici. Presenta DEPLAIN come una soluzione per i problemi dei corpus esistenti, con due sottocorpus: DEPLAIN-apa basato su notizie e DEPLAIN-web con diversi domini. Analizza le trasformazioni di semplificazione e le sue caratteristiche. Omar parla delle applicazioni del corpus: valutare metodi di allineamento automatico e semplificazione testuale automatica tramite fine-tuning di modelli linguistici. Conclude con la presentazione dei risultati e la proposta di un benchmark per il futuro.</sample>
    <sample id="51">I music, books, and recipes.</sample>
    <sample id="52">Posizionalità è il concetto che descrive le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="53">Dawei.</sample>
    <sample id="54">Vasudha, a PhD candidate at Stony Brook University, presented their work "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" at ACL 2023. They define cognitive dissonance as inconsistent beliefs or actions, like a person saying cigarettes could kill them but still smoking. This is a common phenomenon in daily decision making but rare in language. Studying it helps understand disagreement effects, track trends, and mental health. They annotated 1, 000 discourse unit pairs for dissonance, but found it only in 3.5%. Their initial classifier performed poorly. To solve the rarity problem, they used transfer learning and active learning. They transferred weights from related tasks like topic-independent dissonance stance classification and CE tasks. They found that PRC strategy worked best for rare class acquisition. After several rounds of active learning, they improved dissonance classification AUC to 0.75. They also compared different active learning strategies and found iterative update useful for transfer learning and cumulative for domain active annotations.</sample>
    <sample id="55">Sì, EDAtt adatta un modello ST offline esistente.</sample>
    <sample id="56">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni o controllare il testo originale?</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: Background-Pretrain, Background-Both e Background-Inference. Se hai altre domande, puoi chiedere pure.</sample>
    <sample id="59">Il lavoro presentato si concentra su DrBERT, un modello pre-allenato robusto in francese per domini biomedici e clinici. Si parla di modelli di linguaggio in sanità prima di presentare DrBERT, basato su RoBERTa e addestrato su NACHOS, un set di dati medici web. Si confrontano modelli con diverse impostazioni di pre-allenamento e fonti di dati. Si presentano risultati su 11 task biomedici e clinici in francese. Si conclude con dettagli su come accedere ai modelli. DrBERT è una risposta efficace a BERT per compiti di NLP, con miglioramenti rispetto a metodi statici e contestuali. È uno dei modelli più adattati a diverse lingue, ma modelli specializzati per altre lingue sono rari. DrBERT è stato addestrato su NACHOS e confrontato con ChuBERT. Si è analizzato quanto dati è necessario per addestrare un modello specializzato in francese. Si sono confrontati sette modelli addestrati con pre-allenamento continuo. I modelli hanno</sample>
    <sample id="60">Javad Hosseini, Filip Radlinski, Silvia Pareti, Annie Louis.</sample>
    <sample id="61">The last research question is whether we should only use the clean samples for validation, or there are better ways to utilize them.</sample>
    <sample id="62">Il paper parla di una ricerca su distillazione del conoscenza per generazione di linguaggio naturale, NLG. Si parla di come i modelli NLG diventano grandi, complessi e lenti, con costi finanziari elevati. L'obiettivo è comprimere questi modelli senza compromettere la loro prestazione. Si esplora l'utilizzo di distillazione del conoscenza, come distillazione a livello di parola e a livello di sequenza. Si considerano diversi task di NLG come riassunto, generazione di domande, ragionamento di senso comune, semplificazione e trasformazione di stile. Si utilizzano set di dati realistici, con un ratio di 1:4 tra esempi etichettati e non etichettati. Si esplorano diverse architetture, come encoder/decoder e decoder solo, e l'impatto della rimozione di strati. Si confrontano approcci diversi per la selezione del conoscimento e si propone una nuova tecnica di distillazione chiamata "insegnamento unitario". Si dim</sample>
    <sample id="63">La sensibilità misura la capacità del modello di produrre le stesse uscite per la stessa compito indipendentemente dalle variazioni lievi nella formulazione dell'istruzione.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="66">Il contenuto parla di un paper su apprendimento profondo per la ragionamento matematico. La ragionamento matematico è fondamentale per comprendere e prendere decisioni basate su dati numerici e linguaggio. Negli ultimi anni, c'è stato un interesse crescente in questo campo. Si discute del compito di ragionamento matematico e dello sviluppo di un metodo di apprendimento profondo. Ad esempio, i problemi di testo matematici possono coinvolgere operazioni aritmetiche con uno o più passaggi operativi. La ragionamento matematico non è limitato ai dati testuali, ma può estendersi a informazioni multimodali come immagini, figure e tabelle. Ci sono due categorie principali da studiare: contesti visivi e contesti tabellari. Risolvere problemi geometrici è un argomento essenziale nell'istruzione secondaria. Ad esempio, data la descrizione del problema e il corrispondente diagramma, bisogna identificare le relazioni geometriche, applicare il teorema e eseguire cal</sample>
    <sample id="67">L'interferenza in modelli di traduzione multilingue può avvenire quando il modello è piccolo rispetto al volume dei dati. La temperatura di campionamento è chiave per un buon rendimento. Per i casi bilingui, ci sono leggi di scala per i modelli e i dati che permettono di predire correttamente la perdita. Ma nel caso multilingue, ci sono altri fattori come il volume dei dati di altre lingue, la somiglianza delle lingue e il totale di lingue. La somiglianza delle lingue non ha un impatto dominante sull'interferenza. Per controllare i compromessi, la temperatura di campionamento è la soluzione più semplice. Un valore di temperatura calibrato è fondamentale per un buon rendimento. In sintesi, modello e dimensione dei dati influenzano i livelli di interferenza, mentre altri fattori hanno un impatto minore. Un modesto scala e una temperatura calibrata possono ridurre significativamente il problema senza mettere in campo metodi specializzati.</sample>
    <sample id="68">Durante il pre-addestramento, i modelli vengono messi a disposizione di contesti linguistici che vanno da sequenze più brevi a sequenze più lunghe. Questo è per valutare l'acceptability dei modelli in contesti più lunghi, come accade con i grandi modelli di linguaggio che hanno finestre di contesto più grandi.</sample>
    <sample id="69">Generalmente si necessitano 20 campioni di convalida puliti per classe per ottenere prestazioni elevate.</sample>
    <sample id="70">Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Il lavoro di Javad Hosseini e altri introduce l'AltEntities Corpus per comprendere il linguaggio degli utenti quando fanno scelte. L'obiettivo è capire quando usare riferimenti diretti o indiretti. I riferimenti indiretti sono utili quando non si ricorda il nome, le pronuncie sono simili o si vuole specificare una preferenza. Hanno raccolto dati su musica, libri e ricette. I riferimenti indiretti vengono generati con una domanda semplice come "A o B?". I risultati con il modello T5 XL mostrano che l'accuratezza è alta se il modello ha la stessa conoscenza di fondo degli annotatori, ma è più realistica se ha conoscenza parzialmente sovrapposta. Il corpus ha 6.000 domande alternative e 42.000 riferimenti indiretti. Il modello è inoltre generale per più domini.</sample>
    <sample id="72">Perché i modelli di linguaggio sono stati addestrati su grandi quantità di dati web di crawl, che includono notizie politiche. Questo ha creato un mix di benedizioni e maledizioni per le applicazioni dei modelli di linguaggio. Dalle diverse opinioni politiche, che sono socialmente pregiudiziali, possono portare a problemi di equità in applicazioni di compiti a valle.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Akshatha.</sample>
    <sample id="74">Il lavoro introduce Dense-ATOMIC, una conoscenza di base densamente connessa. ATOMIC è un vasto insieme di conoscenza basata su comune senso, ma ha pochi percorsi multi-hop a causa della mancanza di certi link. Dense-ATOMIC completa tali mancanze. La costruzione di Dense-ATOMIC include normalizzazione degli eventi, addestramento di un modello di predizione delle relazioni e la sua costruzione. Propone Rel-CSKGC, un metodo per predire le relazioni tra eventi, che utilizza RoBERTa per codificare gli eventi e MaxPooling per la predizione. Rel-CSKGC supera altri metodi di predizione delle relazioni e traduzioni basate. Dense-ATOMIC ha una maggiore copertura di conoscenza e percorsi multi-hop. La sua costruzione migliora le prestazioni di COMET e permette di generare risultati più diversificati. Il codice e il sito web sono disponibili.</sample>
    <sample id="75">Zheng Yandan presents Jointprop, a joint semi-supervised learning framework for name entity recognition, NER, and relation extraction, RE. They address the lack of interconnections in existing studies. Their framework uses heterogeneous graphs for label propagation. It has four parts: span feature generation, graph construction, joint label propagation, and model optimization. Experiments on four datasets show that joint learning benefits from codependency in joint datasets and significantly improves over baselines on single-task datasets for both NER and RE tasks.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici ha un aspetto complesso. Inizialmente, i modelli di linguaggio sono addestrati su grandi quantità di dati web, tra cui notizie politiche. Questo ha creato un mix di vantaggi e svantaggi per le applicazioni dei modelli di linguaggio. D'una parte, hanno imparato da diverse prospettive, celebrando la democrazia e la pluralità di idee. D'altra parte, le diverse opinioni politiche sono socialmente pregiudiziali e possono portare a problemi di equità nelle applicazioni di compiti a valle. Propone di indagare il percorso di propagazione dei bias politici dal dataset di preaddestramento ai modelli di linguaggio fino alle applicazioni di compiti a valle.</sample>
    <sample id="77">The video presents a joint work from Yale University and Microsoft Research on improving summarization factual consistency. It introduces a new dataset, DeFacto, with human demonstrations and feedback. The work proposes three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. The study focuses on abstractive text summarization. The dataset is based on the XSum dataset and uses Pegasus model outputs. Around 2.5K data points were collected, 70% with factual errors. Human-edited summaries have higher factuality scores but lower textual overlap. The tasks show different levels of challenge for models. The dataset is valuable for training factuality metrics and meta-evaluation. The DeFacto dataset is released on GitHub.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce tra DEPLAIN-apa e DEPLAIN-web. Nella DEPLAIN-apa ci sono più riassegnamenti di ordine e aggiunte di parole, mentre nella DEPLAIN-web ci sono più riformulazioni.</sample>
    <sample id="79">No, it is not publicly available.</sample>
    <sample id="80">La filigrana viene inserita nel testo attraverso la definizione di un'embedding target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di parole del set di parole di riferimento nella frase. L'embedding fornito è una somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di parole del set di parole di riferimento nella frase. Quando il numero di parole del set di parole di riferimento nella frase è maggiore di un certo numero m, l'embedding fornito è esattamente uguale all'embedding target.</sample>
    <sample id="81">Penn State University.</sample>
    <sample id="82">The video discusses unsupervised automated essay scoring, AES. It explains that state-of -the -art AES models are usually trained with labeled corpora, but collecting these is time -consuming. Two previous works are mentioned: one uses the number of unique terms as an initial score, but the clustering process is uncontrollable. The other uses word count as weak supervision, but the regression process is poor. The video proposes a new framework, ULRA, which introduces multiple heuristic quality signals as pseudo -ground -truth. It has a heuristic essay ranking module and a deep pairwise rank aggregation module. The latter addresses inconsistent partial -order supervision. A deep pairwise rank aggregation loss is designed to measure signal importance. In the inference stage, a scoring strategy is proposed to transform predicted scores. Experiments show ULRA outperforms unsupervised baselines and achieves competitive performance compared to cross -prompt and one -shot methods. It aims to perform essay scoring under unsupervised settings.</sample>
    <sample id="83">Sì, possono migliorare.</sample>
    <sample id="84">Shwai He parla del suo lavoro per ACL 2023, "PAD-Net: Un framework efficiente per reti dinamiche". Spiega che le reti statiche calcolano con parametri fissi, mentre le dinamiche possono cambiare in base all'input. Molti esempi di reti dinamiche sono completamente dinamiche, ma questo usa troppi parametri. La sua ipotesi è che le reti dinamiche contengono sotto-reti parzialmente dinamiche che mantengono o superano la rappresentazione dell'originale. Ha costruito PAD-Net, un framework parzialmente dinamico, che divide i parametri in dinamici e statici. L'obiettivo è ridurre i parametri e le operazioni dinamici superflui. I risultati mostrano che PAD-Net ha prestazioni migliori rispetto alle reti statiche e dinamiche, mantenendo meno parametri e calcoli. Ha anche fatto studi di ablation per trovare le proporzioni dinamiche ottimali. Confrontato con la pura prunatura di rete, PAD</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare un torta al cioccolato".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo attraverso la copyright verification. Costruiscono un back door e un dataset benigno. Il provider richiede gli embeddings da un altro servizio con i dati. Calcolano la somiglianza coseno e L2 tra gli embeddings richiesti e il target embedding. Calcolano la differenza di somiglianza tra il dataset benigno e il back door. Inoltre, applicano il test KS e usano il p-value come terzo metrico.</sample>
    <sample id="87">Il lavoro utilizza PLM esistenti come RoBERTa e CamemBERT per costruire DrBERT e ChuBERT. Ad esempio, DrBERT è basato su RoBERTa e viene addestrato su NACHOS, mentre ChuBERT è basato su anonymized data da Nantes University Hospital data warehouse. Inoltre, esplorano l'effetto della quantità di dati necessaria per addestrare un modello specializzato in francese, confrontando modelli addestrati con diversi set di dati.</sample>
    <sample id="88">GPT-4 è meno allineato con i non binari.</sample>
    <sample id="89">Nella frase "If we receive a speech chunk containing "I'm going to talk about..." and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk." la relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione.</sample>
    <sample id="90">L'autore, Haneul Yoo, e altri hanno studiato se i apprendisti linguistici possono contribuire alla annotazione di dati in NLP. Hanno scelto tre lingue: inglese, coreano e indonesiano. Hanno adottato e rivisto i criteri CFR per classificare i apprendisti in livelli: base, intermedio e avanzato. Hanno condotto esperimenti con apprendisti e native speakers. I risultati mostrano che le annotazioni degli apprendisti sono quasi accurate, specialmente per le compiti più semplici. Inoltre, se le annotazioni degli apprendisti vengono aggregate con quelle dei native speakers, sono quasi allo stesso livello. I modelli NLP addestrati con le annotazioni degli apprendisti hanno raggiunto un'accuratezza del 95% rispetto a quelle dei native speakers. Questo studia suggerisce un nuovo modo per costruire dati in lingue a risorse basse e medie, utilizzando gli apprendisti come annotatori.</sample>
    <sample id="91">Quando aumenta il numero di attività, il modello ottiene una performance migliore e al contempo una sensibilità più bassa.</sample>
    <sample id="92">Non ho trovato nella tua descrizione tre approcci di riferimento specifici con cui gli autori confrontano il loro metodo. Potresti fornire ulteriori dettagli o controllare la tua descrizione?</sample>
    <sample id="93">Ivan Titov è un coautore del primo autore, Matthias Lindemann. Alexander Koller è anche un coautore del primo autore. Tutti e tre sono coautori del lavoro.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China gives a short ad video of their paper. They talk about embedding as services for large language models like GPT, LLAMA, PALM. Embedding as services can be stolen by attackers. To protect copyright, they propose Embedding marker, a backdoor based watermark method. It has two steps: watermark injection and copyright verification. They select a trigger set and define a target embedding. In watermark injection, the provided embedding is a weight summation of the target and original embeddings. Copyright verification uses a backdoor and benign data set. They conduct experiments on four data sets and show great detection performance while keeping utility for downstream tasks. They also validate the covertness by visualizing embeddings.</sample>
    <sample id="95">David Vilar.</sample>
    <sample id="96">Ciao a tutti. Sono Jenny, un primo anno di dottorato all'Università Carnegie Mellon e oggi presenterò il vostro lavoro "NLPositionality" che caratterizza i bias di progettazione dei dataset e dei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, tra cui Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Iniziamo immaginando che tu lavori per un giornale e stai filtra i commenti sotto un tuo articolo per rimuovere il contenuto tossico. Potresti rivolgerti a un'API popolare come Prospective API per la rilevazione di tossicità, e questo funziona veramente bene se sei Carl Jones. Dove Prospective API riesce a rilevare correttamente le istanze tossiche. Ma non è così per Aditya Sharma. Dove Prospective AP non è così sensibile alle parole offensive più comuni in contesti indiani. Questo è un esempio di un bias di progettazione dove vediamo differenze sistematiche di prestazione</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST. Se hai altre domande o vuoi discutere di questo argomento, sentiti libero di chiedere.</sample>
    <sample id="98">Un modo efficace potrebbe essere pretrainare i modelli su diversi corpus partizionati in base alla loro tendenza politica. Questo permette di vedere come le ideologie dei modelli si spostino in base ai dati di formazione. Inoltre, dividere i corpus in base al periodo storico potrebbe aiutare a identificare la polarizzazione attuale nella società.</sample>
    <sample id="99">Ciao, sono Siyu Yuan dalla Fudan University. Volevo presentare il nostro lavoro "Distillare il sapere di script dai grandi modelli linguistici per la pianificazione linguistica con vincoli". Nella vita quotidiana, gli esseri umani pianificano le proprie azioni seguendo istruzioni a passi, sotto forma di script orientati a obiettivi. Le ricerche precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipiche come "fare un torta". E hanno dimostrato che i grandi modelli linguistici possono effettivamente decomporre gli obiettivi in passaggi. Tuttavia, le ricerche precedenti si sono principalmente focalizzate sulla pianificazione di obiettivi astratti di attività stereotipiche. La pianificazione di obiettivi con vincoli specifici, come "fare una torta al cioccolato", rimane ancora poco studiata. Nella nostra paper, definiamo il problema della pianificazione linguistica con vincoli che impongono diversi vincoli sugli ob</sample>
    <sample id="100">Multi-hop QA richiede ragionamenti su più documenti. Per rispondere a una domanda come "Qual film di Natale del 1988 ha Brian Doyle-Murray in un ruolo principale?", si deve trovare i film in cui ha recitato e identificare quelli del 1988. I retriever multi-hop sono addestrati per massimizzare la probabilità delle catene di risposta dati i quesiti. PromptRank è un approccio data-efficient che combina un metodo di retrievo non supervisionato con un riarrengatore basato su un modello di lingua a pochi shot. Utilizza TF-IDF e tracciamento di iperlink per ottenere una pool di candidate catene, poi le riarrenghe usando un modello di lingua a pochi shot. La funzione di valutazione utilizza la probabilità del modello di lingua del quesito data la catena. PromptRank supera sistemi supervisionati come DrKit e si confronta con retriever multi-hop a denses. L'approccio mostra buone prestazioni in QA multi-hop con un reader model, ELECT</sample>
    <sample id="101">La fluidità di PaLM è buona, ma ha problemi di accuratezza.</sample>
    <sample id="102">Un metodo di filigrana deve essere applicabile all'embedding as services, non degradare l'utilità delle embeddings fornite, essere abbastanza nascosto per l'attaccante o che l'attaccante possa facilmente rimuoverlo, e essere trasferibile alle servizi dell'attaccante durante l'estrazione del modello.</sample>
    <sample id="103">Mi dispiace, il contenuto non specifica quali siano le 14 lingue diverse. Potresti controllare di nuovo il documento o fornire ulteriori dettagli?</sample>
    <sample id="104">Non è specificato quanti istanze vengono campionate per la riannotazione.</sample>
    <sample id="105">La cosse e la differenza L2 sono le metriche di distanza utilizzate per misurare la differenza tra set di dati benigni e backdoor.</sample>
    <sample id="106">The paper QUEST is about a dataset for studying systems handling selective information needs. It uses examples like Jane and Austin to show how people express information needs with multiple constraints. QUEST has over 3, 000 queries with implicit set operations. The dataset is built from Wikipedia categories in four domains. Queries are paraphrased and validated for fluency and naturalness. Annotators mark relevant entity spans and document evidence. Baselines like retrievers and a T5-based reranker are considered. The paper shows retriever performance can improve and that set intersection and difference queries are challenging. QUEST aims to help build better systems for selective information needs.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per la trasformazione delle query in diverse lingue naturali in rappresentazioni semantiche multiple. Ad esempio, si è utilizzato un modello multilingue per allenare e valutare, come nel caso della trasformazione di query tedesche in inglese per poi utilizzare il modello addestrato per prevedere il SQL. Inoltre, si è testato un modello multilingue con solo il 10% dei dati di addestramento.</sample>
    <sample id="108">Il lavoro riguarda l'accuratezza delle giudiziali di accettabilità dei modelli di linguaggio. I modelli non sono sempre robusti in contesto. I coautori sono Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Rivisitano il paradigma dei minimi paresi. Questo paradigma valuta i modelli di linguaggio basandosi su giudizi di accettabilità, inclusi grammaticalità come BLiMP, SyntaxGym, o accettabilità in termini di stereotipi come CrowS pairs. Attualmente, il pipeline di valutazione dei minimi paresi non permette di valutare l'accettabilità dei modelli su sequenze più lunghe. I coautori cercano di migliorarlo, chiedendo ai modelli di valutare l'accettabilità su sequenze più lunghe. Per simulare queste sequenze, rivedono i set di dati e creano nuove frasi accettabili o non accettabili. I risultati mostrano</sample>
    <sample id="109">L'articolo presenta Unnatural Instructions, un dataset di istruzioni naturali per una varietà di compiti di lingua naturale. Questo dataset è stato raccolto completamente automaticamente utilizzando un modello pre-addestrato, come una variante di GPT-3. Si è generato in due passaggi: prima un'istruzione e un input, poi un output corrispondente. Il dataset contiene 64.000 esempi, con 240.000 se considerati anche i parossimi. Le istruzioni sono creative e diverse, alcune molto diverse da quelle classiche di NLP. Il modello T5 addestrato su Unnatural Instructions ha superato T0++ e Tk-instruct in diversi benchmark. Unnatural Instructions evidenzia la capacità dei modelli di linguaggio di produrre dati creativi e diversi, che è difficile ottenere con i lavoratori di massa, che tendono a formare artefatti di annotazione prevedibili.</sample>
    <sample id="111">Gli autori selezionano un insieme di parole a frequenza moderata basandosi su un corpus di testo generale che può essere raccolto dal fornitore e contando la frequenza delle parole con esso.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il nostro articolo "Do CoNLL-2003 named entity taggers still work well in 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzazione utilizzando la compito di Riconoscimento di Entità Nomevole o NER. Abbiamo osservato che i modelli sono stati utilizzati in CoNLL-2003 per lo sviluppo di NER per quasi 20 anni e questo naturalmente solleva diversi problemi. Innanzitutto, possono questi modelli generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione? Al contempo, se osserviamo una pessima generalizzazione, cosa causa il calo di prestazione di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee di annotazione CoNLL-</sample>
    <sample id="114">Il lavoro presenta un approccio per ottimizzare l'attenzione multi-head nelle grandi reti di linguaggio. Si riconoscono limiti come i pesi pesanti, il tempo di addestramento lungo e la quantità di token necessaria. Propone un gruppo di testi di attenzione che usa una strategia di divisione e conquista per comprimere l'attenzione multi-head. Due fasi: prima, gruppo-constrizione di addestramento per rendere i testi di attenzione all'interno dei gruppi più simili e quelli tra gruppi più separati. Seconda, algoritmo di voto per rimuovere le teste di attenzione ridondanti. Risulta in una compressione di parametri significativa, fino al 90%. Funziona bene sui compiti di traduzione, sintesi astratta e modellazione linguistica. L'approccio è promettente per la prunatura automatica delle reti specifiche per le tare, come previsto dalla ipotesi del biglietto della lotteria.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di lambda parole.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono "Servin è un giudice."</sample>
    <sample id="117">La qualità dell'esempio è più importante.</sample>
    <sample id="118">The presentation is about an ACL 2023 submission on improving pretraining techniques for code-switched NLP. Code-switching is common in linguistically diverse communities like India. The work proposes SwitchMLM, a novel MLM technique tuned for code-switching. It defines switch-points as groups of two tokens marking language transitions. FrequencyMLM is a surrogate method for LID tagging. Architectural modifications like residual connections are proposed to encode more switch-point information. The combined method performs well on sentiment analysis tasks. Probing experiments show increased switch-point information in intermediate and final layers. The presentation concludes with the motivation for these changes and the expected results.</sample>
    <sample id="119">L'articolo si concentra sugli esperimenti estesi sui modelli linguistici GPT-4, GPT serie, BART serie e suoi varianti, RoBERTa.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Esempi di inferenza diretta sono direttamente nominare il nome del brano, "Easy on Me", o indicare la posizione, "the first one".</sample>
    <sample id="122">Non so qual è l'affiliazione degli autori dell'articolo. Potresti dare più informazioni o controllare il testo originale?</sample>
    <sample id="123">Ying e Zhiyang presentano un lavoro su MultiInstruct, un dataset per tuning di istruzioni multi-modal. Esplorano se il tuning di istruzioni può migliorare la generalizzazione a task multi-modal non visti. Costruiscono MultiInstruct, con 62 task diverse in 10 categorie, da 21 dataset esistenti. Usano OFA come modello base. Durante l'allenamento, usano 53 task per 9 gruppi e 10.000 istanze per task. Per testare, usano 5 task aggiuntivi e 20 task casuali per NLP. La loro ricerca dimostra che il tuning di istruzioni può migliorare significativamente l'performance di OFA su task multi-modal visti. L'aggiunta di più istruzioni può migliorare l'performance e ridurre la sensibilità. Propone un nuovo metrico chiamato sensibilità. Stanno raccogliendo un dataset più grande con 150 task aggiuntivi.</sample>
    <sample id="124">Tan Qingyu e il suo team hanno studiato la capacità di ragionamento temporale dei grandi modelli di linguaggio. Hanno suddiviso il ragionamento temporale in tre livelli: ragionamento tempo-tempo, tempo-evento e evento-evento. Hanno trovato che le precedenti ricerche sovrastimavano il ragionamento L2. Hanno creato il dataset TempReason che copre tutti e tre i livelli di ragionamento temporale. Hanno proposto una strategia di allenamento con due componenti: estrazione di spazi temporali e apprendimento rafforzato in sensibile al tempo. Hanno confrontato i modelli con il dataset TempReason e hanno dimostrato che il loro modello TempT5 migliora significativamente il presto dei modelli finetuned. Hanno anche notato che ChatGPT ha problemi nel ragionamento temporale e che ci sono ancora bisogni di lavoro per superare le bias di ragionamento temporale.</sample>
    <sample id="125">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni o controllare il documento?</sample>
    <sample id="126">No, non è stato considerato come un approccio standard.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun addresses the issue of chain - of - thought reasoning being limited to huge models like GPT - 3 or PALM. They propose using these large models as reasoning teachers to transfer their abilities to much smaller models. They introduce a novel technique called diverse reasoning, which generates multiple reasoning samples using stochastic temperature sampling. This allows the small models to perform complex reasoning tasks well. The method is compared to existing baselines on 12 tasks, showing notable performance, especially in text - based ones. Diverse Reasoning significantly increases performance. The method is highly scalable but involves trade - offs between development and inference costs. The paper also discusses how reasoning emerges in small models and provides code and data for experiments.</sample>
    <sample id="128">L'audio parla di un lavoro intitolato "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". È una collaborazione tra McGill University, Mila e Microsoft Research. Si parla di come i modelli di comprensione del linguaggio naturale utilizzano diverse fonti di conoscenza, sia acquisita durante la pretraining che fornita in tempo di inferenza. Si propone un insieme di test per valutare l'integrazione di conoscenza. Si introduce una task di risoluzione di riferimento per testare l'abilità di utilizzare conoscenza in fonti diverse. Si valuta con partecipanti umani e modelli di risoluzione di riferimento esistenti. Si definiscono tre set di KITMUS: Background-Pretrain, Background-Both e Background-Inference. Si dimostra che con la formazione specifica su KITMUS, alcuni modelli riescono a integrare conoscenza da fonti multiple. Ma anche i modelli migliori hanno difficoltà a integrare conoscenza retrospettiva fornita solo in tempo di inferenza. Per ulteriori dettagli, si veda il lavoro e</sample>
    <sample id="129">The authors provided examples of the top words for each marked group, like "culture", "tradition", "proud", "exotic" for black women, "vibrant", "curvaceous" for Latina women, "petite", "delicate", "silky" for Asian women, and "strong", "resilient" for black women.</sample>
    <sample id="130">Non sono state specificate quali architetture dei modelli non generalizzano in modo adeguato. Potresti dare più dettagli sulla tua domanda?</sample>
    <sample id="131">Non sono stati menzionati nomi specifici di set di dati di test nel contenuto.</sample>
    <sample id="132">There are two authors involved in the article.</sample>
    <sample id="133">L'autore opera con più modalità.</sample>
    <sample id="135">ABC-Eval è un approccio dimensionale per valutare l'intelligenza artificiale conversazionale. È stato sviluppato dal Laboratorio di NLP dell'Emory University, diretto da Prof. Jinho Choi, in collaborazione con Amazon Alexa AI. ABC-Eval cerca di ridurre la soggettività dell'evaluation umana, annotando se le risposte del modello esprimono determinati comportamenti, come rispondere con informazioni irrilevanti o contraddizioni. È stato testato su quattro modelli chat state-of-the-art, valutando 100 conversazioni umano-bots per modello. ABC-Eval è più affidabile e prevedibile rispetto ai metodi esistenti, come misurare il numero di turni in cui il modello ignora il partner o dice cose irrilevanti. Questi metri ABC-Eval permettono di valutare l'intelligenza artificiale con una risoluzione maggiore rispetto ai metodi precedenti. Ci sono tuttavia sfide rimaste, come le violazioni di conoscenza comune nei risposte dei bot. ABC-E</sample>
    <sample id="136">Il lavoro di Jasivan e Nafise riguarda FERMAT, un set di valutazione flessibile per la ragionamento numerico. Motivato dalle applicazioni reali del ragionamento numerico e delle sue richieste di correttezza, presentano FERMAT basato su tipi di aritmetica. Include domande matematiche tratte da Illinois e CommonCore, testate su numeri piccoli, grandi e decimali. Fanno un'analisi delle operazioni matematiche e del dipendenza di addestramento. Con un'evoluzione iniziale pessima, dopo la fine-tuning con l'aiuto di insegnanti di matematica, migliorano in molti aspetti. Concludono che i benchmark esistenti sono poco rappresentativi e che FERMAT offre un'alternativa più informativa. Trovano che la diversità linguistica e matematica è importante per migliorare le prestazioni.</sample>
    <sample id="137">The paper introduces Tell2Design, a dataset for language-guided floor plan generation. It aims to enable users to design floor plans by "telling" instructions. The task is to generate 2D floor plan designs from language instructions specifying semantics, geometry, and topology. The dataset is constructed using publicly available floor plans and human-annotated instructions. Challenges include strict constraints, understanding the big picture from unstructured text, and dealing with ambiguous information. The authors propose a sequence - to - sequence model based on the encoder - decoder framework, using a transformer - based structure. The model is trained on artificial and human instructions. Evaluation shows the model outperforms text - conditional image generation baselines. The paper concludes by suggesting this work can drive future research on language - guided design generation.</sample>
    <sample id="138">According to the authors, the area of NLU that is not well studied is the integration and use of both pretrain-time and inference-time knowledge.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Yes, crowd-sourced workers were asked to find and revise the incorrect samples.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti perché supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto spesso si basano su conoscenza di dominio e curazione umana.</sample>
    <sample id="142">Ciao! Parliamo del nostro lavoro su "Risoluzione di espressioni riferimento indiretto per la selezione di entità". Introduciamo il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro a squadra con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è capire il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Volevi dire 'Easy on Me' o 'I Gotta Feeling'?" Qui, l'utente vuole scegliere tra due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio citando il nome della canzone "Easy on Me" o la sua posizione, "la prima". Ma a volte un riferimento indiretto è più appropriato per una conversazione più naturale. Potrebbe succedere quando l'utente non ricorda il nome della canzone. O le pronunciazioni sono troppo simili per essere distinguibili. O quando l'utente vuole specificare una preferenza. Ecco alcuni es</sample>
    <sample id="143">The approach is compared with the Wait-k strategy and the Local Agreement.</sample>
    <sample id="144">Non so quali siano le affiliazioni degli autori dell'articolo. Potresti dare più informazioni o cercare di trovare queste informazioni da qualche altra fonte?</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">L'analisi dell'omissione nel riassunto di dialoghi è un problema serio. I modelli di grandi scale hanno fatto progressi, ma gli errori di omissione sono comuni. L'omissione è un fattore principale che rende i riassunti incompleti. Abbiamo analizzato il tasso di omissione in cinque domini e sei modelli pre-trainati. Troviamo che anche il modello di punta raggiunge un alto tasso di omissione. L'omissione è distribuita casualmente in ogni posizione del dialogo. Per analizzare meglio il problema e risolverlo, abbiamo costruito il dataset OLDS. Proponevamo un metodo automatico per produrre etichette di omissione. Abbiamo esplorato tre basi linee con differenti formati di input e strutture. Usando la precisione, il ricordo e l'F1-score per valutare i modelli di rilevamento di omissione, troviamo che il tasso di F1 è intorno al 50%. Questo indica che il compito è molto difficile. Utilizzando l</sample>
    <sample id="147">Tre.</sample>
    <sample id="148">Ciao Sara Papi dall'Università di Trento e dalla Fondazione Bruno Kessler. Volevo presentare il lavoro "Attenzione come guida per la traduzione simultanea del discorso" che ho fatto in collaborazione con Matteo Negri e Marco Turchi. La traduzione simultanea del discorso, o SimulST, è il processo di tradurre il linguaggio parlato in un testo in un'altra lingua in tempo reale, permettendo la comunicazione tra lingue diverse. Quali sono i problemi dei modelli attuali di SimulST? Gli architetti specifici sono solitamente addestrati, introducendo moduli aggiuntivi da ottimizzare. Lunghe e complesse procedure di addestramento, ad esempio, addestramento coinvolgente obiettivi di ottimizzazione diversi. Addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza. Ad esempio, addestrare un modello con un'average di una seconda di latenza e un altro con due secondi di latenza, e</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente. Se hai altre domande, puoi contattarmi.</sample>
    <sample id="150">Archiki presenta il loro paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts". Si ringrazia i collaboratori di Adobe Research e UNC Chapel Hill. I meeting globali generano tanti transcritti che possono essere un nuovo campo per la ricerca NLP. I meeting hanno documenti lunghi, specifici e ricchi di informazioni. Le precedenti opere si concentrano solo sulla sommarizzazione e l'estrazione degli item d'azione. Introducono MeetingQA, un dataset di QA estrattivo basato su domande e risposte. La raccolta dati inizia con meeting pubblici da AMI corpus. La distribuzione delle domande e delle risposte è analizzata. Utilizzano metodi diversi per modelli di QA. I risultati mostrano un gap di F1 tra i modelli finetunati e l'umano. I modelli a breve contesto superano quelli a lungo contesto. I modelli a singola spagnola hanno meno o parità di quelli a multi spagnola. La data augmentation migliora il presto zero-shot. I risultati zero-shot di modelli più grandi come FL</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang e noi presenteremo la nostra ricerca su MultiInstruct che migliora il Multi-Modal Zero-Shot Learning tramite l'addestramento delle istruzioni. Con l'avanzamento dei grandi modelli linguistici, molte ricerche hanno iniziato a esplorare nuovi paradigmi di utilizzo di modelli pre-allenati per compiti differenti in modo efficiente sia in termini di parametri che di dati. Recentemente, molte ricerche hanno dimostrato che l'addestramento delle istruzioni consente ai grandi modelli linguistici di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte delle ricerche precedenti sull'addestramento delle istruzioni si è concentrata sul miglioramento del presto zero-shot per compiti solo linguistici, mentre i compiti di visione computeristica e multi-modalici sono stati lasciati fuori. Quindi, in questa ricerca vogliamo indagare se l'addestramento delle ist</sample>
    <sample id="152">Frederick Riemenschneider presenta un lavoro sulla combinazione di NLP e filologia classica. Introduce risorse per il greco antico e latino. Discute modelli multilingue e multilingue monolingue. Presenta GreBERTa e GreTa, modelli monolingue per il greco antico, e PhilBERTa e PhilTa, multilingue per greco, latino e inglese. Raccolgono dati da Internet Archive e Corpus Corporum. Bencheggiano i modelli e mostrano miglioramenti in tag, analisi semantica e lemmatizzazione. Concludono con un invito a leggere il loro articolo.</sample>
    <sample id="153">Ninareh Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, presented their work on resolving ambiguities in text-to-image generative models. They studied ambiguous prompts like "The girl enters the room with flowers" and aimed to propose frameworks to mitigate ambiguities and evaluate faithful image generation. They created a benchmark dataset based on LAVA, covering different ambiguity types. Their framework uses a language model to generate clarifying questions or possible visual setups. They then evaluate the generated images using a VQA model to check if they match user intention. Findings show positive effects of disambiguation and agreement between automatic and human evaluation. For more details, refer to their paper.</sample>
    <sample id="154">Sara Papi è affiliata all'Università di Trento e alla Fondazione Bruno Kessler. Matteo Negri e Marco Turchi non sono specificati come affiliati nell'articolo.</sample>
    <sample id="155">Javad Hosseini.</sample>
    <sample id="157">Shen Gao da Shandong University presenta il loro lavoro "Dialogue Summarization with Static-Dynamic Structure Fusion Graph". L'obiettivo è creare un riassunto conciso da un contesto di dialogo. I metodi esistenti si basano su strutture di grafo statiche pre-calcolate, ma hanno due difetti: dipendono da strumenti linguistici esterni poco affidabili e le strutture di grafo statiche non si adattano dinamicamente. Il loro modello SDDS ha quattro componenti principali. Utilizzano un encoder per codificare le afferenze in vettori, costruiscono una struttura di grafo statica, poi fondono una struttura di grafo statica dinamica per catturare le relazioni semantiche. Infine, usano un modello di lingua pre-addestrato per generare il riassunto. Hanno anche proposto metodi per costruire la struttura di grafo statica e dinamica, come il Discourse Parsing Graph e un modulo di attenzione multi-capo per catturare le</sample>
    <sample id="158">Il contenuto parla di un lavoro intitolato "Dual Cache for Long Document Neural Coreference Resolution" da AWS. L'obiettivo principale è risolvere il problema di coreference resolution, ovvero identificare e raggruppare le menzioni che si riferiscono allo stesso entità in un documento. I metodi tradizionali hanno complessità quadratici, mentre i metodi basati su cache hanno linearità. Propone un dual cache con un cache locale e uno globale. Il locale usa LRU, il globale LFU. Il modello valuta la frequenza delle menzioni e le aggiunge al globale o al locale. Si è valutato su benchmark pubblici e il dual cache ha dimostrato una migliore prestazione rispetto ai metodi di base, anche con memoria illimitata. Inoltre, riduce significativamente i miss di cache rispetto al singolo cache. È il metodo più cost-eficiente.</sample>
    <sample id="159">Ciao a tutti. Sono Koustav Sinha, e sono lieto di accogliervi al nostro talk sul nostro articolo di ACL 2023. Le giudicazioni di accettabilità dei modelli di linguaggio non sono sempre robuste in relazione al contesto. Questo è un lavoro a squadra con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. Quindi, in questo lavoro, rivediamo i paradigmi di coppia minima. Il paradigma di coppia minima valuta i modelli di linguaggio in base alle giudicazioni di accettabilità. Che possono includere la grammaticalità come BLiMP, SyntaxGym, o l'accettabilità in termini di stereotipi come le coppie CrowS. Nella questo paradigma di coppia minima, la maniera tipica di valutare i modelli è mostrare una frase accettabile o grammaticalmente corretta e poi mostrare una frase accettabile o non grammaticalmente corretta. E la speranza è che il modello, in</sample>
    <sample id="160">In the first step, the method maps each input token to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">In CoScript sono rappresentati 55.000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEPLAIN è il metodo di MASSalign.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato ha il vantaggio di essere più economico rispetto all'annotazione manuale dei dati, poiché usa fonti di annotazione deboli come regole semplici, basi di conoscenza o sondaggi di crowdsourcing a bassa qualità. Tuttavia, ha anche il problema di essere rumoroso, quindi i modelli neurali addestrati direttamente su dati etichettati deboli tendono a memorizzare il rumore delle etichette e non a generalizzare. Ma se si addestrano modelli con algoritmi di apprendimento addestrati per gestire il rumore delle etichette, i modelli addestrati possono ancora generalizzare bene.</sample>
    <sample id="165">L'autore, Wenting Zhao, presentava un lavoro su "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations". L'esempio iniziale era di Emily che era bloccata in traffico e poi ha raggiunto il volo. Le spiegazioni possibili erano che il volo fosse stato ritardato o che fosse partito in orario. L'obiettivo dell'abduzione era trovare una spiegazione plausibile che collegasse il contesto al risultato. Il lavoro considerava un contesto chiuso e un set di spiegazioni possibili. I metodi attuali di abduzione erano supervisionati, ma erano soggetti a errori. Il lavoro presentava LiPoR, un metodo di apprendimento non supervisionato. LiPoR considerava le spiegazioni come variabili latenti. L'obiettivo era massimizzare la probabilità marginale dell'outcome dato il contesto. Per favorire spiegazioni plausibili, si utilizzava un regolarizzatore basato sulla mutual esclusività delle spiegazioni. LiPoR fu superiore a</sample>
    <sample id="166">L'abstract parla di un nuovo lavoro intitolato "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text". L'immagine di ricerca da testo linguisticamente complesso è un compito di ragionamento immagine-testo difficile perché le immagini sono molto simili e la descrizione è lunga. I metodi tipici, come i modelli visuo-linguistici, fanno bene alle tare di ricerca immagine-sentenza, ma la loro prestazione cala drasticamente di fronte al testo linguisticamente complesso. Per affrontare questo problema, si è ispirati alla strategia Divide-and-Conquer e alla Dual-Process Theory. Il primo modello proposto è il Generatore di Proposizioni, che mira a decomporre le proposizioni testo complesse in rappresentazioni di proposizioni semplici. Il Sistema 1, l'Interattore Visuo-Linguistico, esegue l'interazione di informazioni tra le proposizioni visive e le proposizioni, simile al Sistema 1. Il Sistema 2, il Raisonneur Neurale-S</sample>
    <sample id="167">In DEPLAIN-web, 750 documents were aligned manually and with automatic alignment methods.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo notizie da Reuters News del 2020 e annotandole con le stesse linee guida di annotazione di CoNLL-2003.</sample>
    <sample id="169">David Vilar e colleghi hanno studiato la prompting di PaLM per la traduzione. PaLM è un modello a grande linguaggio con 540 miliardi di parametri. È stato addestrato su 780 miliardi di token. Ha raggiunto prestazioni state-of-the-art in molte tesi NLP. Hanno valutato la capacità di transizione del modello utilizzando le migliori pratiche della comunità di MT. Hanno confrontato con sistemi state-of-the-art come WMT. Hanno utilizzato metriche neurali MT e valutazioni umane. Hanno fornito consigli per la scelta di strategie di prompt. La prompting ha un grande impatto sulle prestazioni delle LLM per la traduzione. Hanno trovato che la qualità degli esempi è più importante della somiglianza con il testo di origine. Hanno valutato con Google Translate. Hanno notato che la fluenza di PaLM è paragonabile a sistemi state-of-the-art, ma ha errori di accuratezza. Hanno concluso che PaLM è vicino a un sistema commerciale. Per ulteriori</sample>
    <sample id="170">Ciao a tutti, il mio nome è Yusen Zhang dalla Penn State University. Oggi presenterò il nostro lavoro "XSemPLR: Parsing Semantico Translazionale in Più Lingue Naturali e Rappresentazioni di Significato". Il parsing semantico è una task per costruire rappresentazioni semantiche di query utente come SQL e Calcolo Lambda. E il parsing semantico translazionale è la task di tradurre query in più lingue naturali in più rappresentazioni di significato. Come mostrato in questa figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL, ecc. I modelli di parsing semantico translazionale esistenti sono proposti e valutati separatamente su set di dati di compiti e applicazioni limitati. Ad esempio, ci sono molte coperture su certe lingue naturali. Ma il cinese manca e manca di copertura su certe rappresentazioni di significato. Il Calcolo Lambda manca, o sono solo valutati su certi modelli neurali. Ad esempio,</sample>
    <sample id="171">I lavori connessi sono stati classificati in quattro categorie. Ma questi metodi o non sono applicabili all'embedding as services o mancano di trasferibilità.</sample>
    <sample id="172">No, multilingual language models like Codex and Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea, co-autrice di "ArgAnalysis35K", spiega perché il dataset è unico. Argument quality analysis valuta l'argomento su una scala da 0 a 1. ArgAnalysis35K è il più grande dataset in questo campo con 35K coppie di argomento-analisi. Ha una diversità di argomenti basata su 24 temi, non solo su 30-40 motioni. Introduce l'analisi come una combinazione di affermazioni e premesse. Ha un modello di rilevanza che assegna un punteggio da 0 a 1 per la rilevanza di ogni argomento a un tema. ArgAnalysis35K è di alta qualità e diverso da altri dataset.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni introducendo una relaxazione continua amichevole per GPU. Questa permette di indietreggiare attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico.</sample>
    <sample id="176">L'equità di un modello NLP a valle si riferisce a come il modello gestisce le differenze politiche presenti nel training data e come queste influenzano le sue prestazioni su task come la rilevazione di discorsi razzisti e notizie false. Se un modello è più inclinato politicamente a sinistra, può essere migliore nel rilevare discorsi razzisti contro gruppi minoritari, ma peggiori per quelli contro gruppi più potenti. Oppure se è più inclinato a destra, può essere migliore per i gruppi più potenti ma peggiori per i gruppi minoritari. Questo indica un problema di equità causato dalle inclinazioni politiche dei modelli.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar parla di "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker". Discute il concetto di Theory of Mind, la capacità di ragionare sullo stato mentale degli altri. Presenta il test Sally-Anne come esempio. Molti grandi modelli di linguaggio fanno ancora pessimo su task di Theory of Mind. Presenta SymbolicToM, un metodo per migliorare la capacità di Theory of Mind in grandi modelli di linguaggio. Utilizza rappresentazioni grafiche esplicite. Testa il metodo con vari modelli di linguaggio e modelli specifici per Theory of Mind. Mostra miglioramenti significativi in vari task, inclusi quelli di second-order false-belief. Per ulteriori dettagli, si veda il paper.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">The paper introduces constrained language planning, focusing on planning for goals with specific constraints. It evaluates large language models' ability in this area, finding unsatisfactory results. The authors then analyze why models fail, noting issues with semantic completeness and constraint faithfulness. They propose an over - generate - then - filter method to improve generation quality. This involves over - generating scripts and filtering based on semantic similarity and keyword matching. The method is applied to create a high - quality dataset, CoScript, for constrained language planning. T5 fine - tuned on CoScript outperforms most large language models. The paper establishes the problem, evaluates models, and develops a method for constrained language planning. It aims to advance research in this area with the CoScript dataset.</sample>
    <sample id="182">Nel contesto di questo articolo, il tropicalismo indica un insieme di stereotipi e tropi che connettono le donne di colore, specialmente le latine, a elementi come la vitalità, la curvatura e la bellezza esotica. Questo contribuisce a una storia di discriminazione e distacco da parte delle donne di colore. Se hai altre domande su questo articolo, sentiti libero di chiedere.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target chiedendo ai modelli di LLM di generare personaggi usando promemoria come "Imagina che tu sia una donna asiatica. Descrivi te stesso." In questo modo, possono essere generalizzati a qualsiasi demografia specificando l'identità desiderata nella promemoria.</sample>
    <sample id="184">Nel lavoro, è stato utilizzato CXMI, Contextual Mutual Information, come misura per l'utilizzo del contesto. Inoltre, è stato esteso a Pointwise CXMI per misurare l'utilizzo del contesto a livello di frase o parola.</sample>
    <sample id="185">DrBERT è basato su RoBERTa e addestrato su NACHOS, mentre ChuBERT è basato su dati anonimizzati ottenuti dal data warehouse dell'ospedale universitario di Nantes.</sample>
    <sample id="187">Due.</sample>
    <sample id="188">L'apprendimento iterativo è un processo in cui si aggiornano i modelli con nuovi dati raccolti in ogni iterazione. Questo approccio è utile per il trasferimento di apprendimento da un dominio diverso.</sample>
    <sample id="189">The goal is to understand users' language when they want to make a choice.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS aprendo un modello di backdoor.</sample>
    <sample id="191">Tre.</sample>
    <sample id="192">Yang Luo presenta "CAME: Confidence-guided Adaptive Memory Efficient Optimization". Si concentra su ottimizzatori per grandi modelli linguistici. Adafactor riduce memoria ma penalizza prestazioni. CAME mira a un ottimizzatore che ottiene convergenza veloce e basso utilizzo di memoria. Utilizza NMF per ridurre memoria. Adafactor ha errori che limitano applicazione. CAME migliora su Adafactor e Adam. Eseguiti test su BookCorpus, English Wikipedia e modelli BERT, GPT-2, T5. CAME aumenta accuracy di validazione di 3.4% rispetto a Adafactor. Risparmia memoria per grandi batch. E' un ottimizzatore efficiente per grandi modelli linguistici.</sample>
    <sample id="193">Non so quanti annotatori sono stati impiegati per creare il set di dati iniziale. Potresti dare più informazioni o controllare il tuo documento?</sample>
    <sample id="194">Carnegie Mellon University, University of Washington, Allen Institute for AI.</sample>
    <sample id="195">L'articolo introduce un lavoro intitolato "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering". L'obiettivo è rispondere a una domanda e fornire una spiegazione per la scelta dell'answer. I metodi recenti si dividono in neuro-simbolici e basati su decomposizione. I primi traducono le domande in rappresentazioni formali, ma le KB sono incomplete. I secondi generano passaggi intermedi, ma usano solo corpus di testo. L'integrazione di conoscenze da fonti diverse è importante. Il lavoro propone un framework RoHT. Costruisce un albero di decomposizione questioni per comprendere la struttura complessa e fonde conoscenze da KB e corpus testo. Lo valuta su due dataset, KQA Pro e Musique, dimostrando l'efficacia di integrare conoscenze da diverse fonti.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli selezionati per l'evaluation con ABC-Eval. Non sono specificati i nomi dei modelli, ma sono quattro modelli state-of-the-art. Se vuoi sapere di più, puoi chiedere di dettagliare ulteriormente.</sample>
    <sample id="198">Perché i modelli linguistici stanno diventando sempre più grandi e gestendo contesti più lunghi.</sample>
    <sample id="199">No, Encoder-Decoder ha ottenuto il miglior rendimento su tutte le nove dataset.</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo.</sample>
    <sample id="201">Le metriche di MT utilizzate per la valutazione sono state le state-of-the-art neural MT metriche e inoltre sono state mostrati anche i risultati di valutazione basati su valutazioni umane.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER.</sample>
    <sample id="203">Perché NLP task diventano sempre più soggettive e socialmente orientate.</sample>
    <sample id="204">Non sono stati affinati né con adattatori né con una messa a punto integrale.</sample>
    <sample id="205">Il lavoro di Shangbin, studente di dottorato all'Università di Washington, si concentra sulle bias politici nei modelli linguistici. I modelli sono addestrati su grandi quantità di dati web, tra cui notizie politiche. Questo ha creato un doppio effetto: da un lato, permette di imparare da diverse prospettive, celebrando la democrazia e la pluralità di idee, dall'altro, le diverse opinioni politiche sono socialmente pregiudiziali e possono portare a problemi di equità in applicazioni di compiti a valle. Propone di indagare il percorso di propagazione dei bias politici da dati di preaddestramento a modelli linguistici a compiti a valle. Valuta le inclinazioni politiche dei modelli e il ruolo dei dati di preaddestramento. Esplora come i modelli con diverse inclinazioni politiche si comportano in compiti a valle e se possono portare a problemi di equità. Utilizza questionari politici e corpora partitici per valutare</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, fanno ricorso a due modelli: uno per la classificazione di dissonanza indipendente dal tema e l'altro per la classificazione binaria di espansione e confronto delle classi di PDTB.</sample>
    <sample id="207">The latest test sets.</sample>
    <sample id="208">Tre.</sample>
    <sample id="209">Non so qual è il metodo di riferimento. Non ho abbastanza informazioni per rispondere a questa domanda. Potresti fornire più dettagli?</sample>
    <sample id="210">Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nel testo non viene specificato quanti modelli più piccoli vengono utilizzati.</sample>
    <sample id="213">OFA.</sample>
    <sample id="215">Il contenuto parla di strutture di dipendenza nella coordinazione. Alcune teorie assumono strutture asimmetriche, come in universal dependencies e in Igor Mel'čuk's theory, dove il primo congiunto è capo. Invece, il Prague approach e il Hudson's Word Grammar assumono strutture simmetriche. L'argomento del paper è per le strutture simmetriche e contro le asimmetriche. Si basa sul principio di minimizzazione della lunghezza delle dipendenze. In inglese, gli oggetti diretti preferiscono essere vicini al verbo, ma questo può essere ammesso se l'oggetto è pesante e lungo. Le statistiche estratte dal Penn Treebank confermano che i congiunti a sinistra tendono a essere più corti, a meno che il governante non sia a destra. Questo fornisce un argomento a favore delle strutture simmetriche e contro quelle asimmetriche.</sample>
    <sample id="217">Il lavoro di Weihao Zeng, Lulu Zhao e Keqing He si concentra sulla generazione di dialoghi controllabili con più attributi. I metodi precedenti si concentrano su singoli attributi, ignorando la generazione con più attributi. Hanno proposto DCG, un modello che impara concetti di attributi da valori visti e utilizza una perdita di disentanglement per separare combinazioni di attributi. Hanno introdotto un quadro di valutazione unificato, MAE, per differenti gradi di attributi. Hanno creato due benchmark e dimostrato l'efficacia del loro metodo e dei loro metrici attraverso esperimenti. I loro modelli, basati sul framework DialoGPT con un modulo di prompt compositivo, utilizzano due tipi di prompt: uno orientato agli attributi e uno orientato alla compito. Hanno progettato combinazioni pseudo per migliorare la diversità dei prompt. Hanno proposto un quadro di valutazione unificato e efficiente per la generazione di dialoghi controllabili con più attributi, senza rich</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">Jia-Huei Ju, da Academia Sinica, presenta un lavoro intitolato "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports". L'obiettivo è analizzare i report finanziari, considerando il Form 10-K come corpus. I risultati mostrano che circa 80% dei token sono simili tra report consecutivi. Si introduce un compito di evidenziamento e un pipeline a più fasi. Il primo passaggio è la segmentazione del documento. Il secondo è la riconoscenza delle relazioni. I passaggi successivi sono l'addestramento fine su dominio esterno e intermedio. Si utilizza l'insieme di dati eSNLI per l'addestramento fine su dominio esterno e i dati riformulati per l'addestramento fine intermedio. L'insieme di valutazione include eSNLI e il dataset FINAL rilasciato. Le metriche utilizzate sono precisione e correlazione di Pearson. Il modello di evidenziamento adattativo al dominio ha ottenuto il miglior risultato su FINAL e ha</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Non so qual è la lingua di origine e la lingua di destinazione analizzate nell'articolo. Potresti dare più informazioni?</sample>
    <sample id="222">L'articolo esplora i problemi e le soluzioni per l'adattamento di domini in risposte alle domande aperte. Si concentra su come i modelli di lettura e di recupero, addestrati su Wikipedia, si comportano quando si passa a domini specifici come la biologia. Si identificano tre contributi principali: esplorare interventi di dati utili per l'adattamento, identificare il tipo di spostamento di dataset e determinare interventi di dati efficaci per un tipo specifico di spostamento. Si utilizzano metodi zero-shot e few-shot per generare interventi di dati. Si osserva un miglioramento dell'8% per il recupero e del 11% per la lettura. Si analizza la compatibilità tra il modello di base e il nuovo dominio utilizzando una misura di probabilità. Si conclude che alcuni interventi di dati sono efficaci basandosi sul tipo di spostamento del dataset.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Durante gli esperimenti sono stati studiati due modelli: il modello long-mBART e il modello mBART base.</sample>
    <sample id="225">Per scopi di addestramento vengono utilizzate 53 attività e per test vengono utilizzate 55 attività.</sample>
    <sample id="226">Due.</sample>
    <sample id="227">The text discusses the current challenges in grounded language understanding for language models. It points out that most models are pre-trained on textual data without grounding, making the task difficult. Existing research often uses language models to directly generate plans, which can be ungrammatical or invalid. The authors propose a novel framework called Pangu. In this framework, a symbolic agent proposes candidate plans, and a language model scores and ranks them. This way, the language model doesn't have to handle the validity and grammar of the plans. Pangu shows strong performance across different settings, including fine-tuning and in-context learning. It also demonstrates good sample efficiency and robustness under non-i.i.d. settings. The key takeaway is that discrimination might be a better strategy for grounded language understanding than generation.</sample>
    <sample id="228">Gli autori hanno effettuato i test su quattro set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Il contenuto parla di un lavoro su rilevare affermazioni migliorabili per supporto all'argomentazione. Si inizia con l'introduzione alla revisione testuale, importante per la scrittura professionale. Si parla di due task: rilevare affermazioni subottimali e suggerire miglioramenti. Si esplorano sfide come rappresentatività, complessità del modello, dipendenza da contesto e bias. Si conclude che i dati di revisione possono essere utilizzati efficacemente per i task e che la distanza tra versioni è utile per rilevare affermazioni subottimali. Per ulteriori dettagli, si veda il paper.</sample>
    <sample id="231">NACHOS è un dataset di dati medici riscattati dalla rete.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Simultaneous speech translation, SimulST, is translating spoken language into text in another language in real time. Current models have problems like long training procedures and need different models for different latencies. The solution is to use existing offline ST models, use one model for all latencies, and handle latency with specific parameters. EDAtt is proposed, which uses cross-attention to decide if to emit partial translation. It looks at cross-attention weights to see if received information is stable. Results show it outperforms offline strategies and is the fastest. Code and models are open source for reproducibility.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati. In un semplice esperimento con uno-shot prompting, il differenziale osservato è di più di un punto BLEURT. In casi estremi, può arrivare fino a 40 punti BLEURT. Quindi, è importante selezionare una buona strategia di prompting.</sample>
    <sample id="235">Non so quali siano le affiliazioni degli autori dell'articolo. Potresti dare più informazioni o controllare il documento originale?</sample>
    <sample id="236">Non so qual è la risposta esatta. Potresti dare un'occhiata al tuo documento o al tuo presentazione per trovare le 5 istruzioni scritte da esperti?</sample>
    <sample id="237">Gli autori propongono un insieme di test diagnostiche per testare l'integrazione delle informazioni provenienti da più fonti.</sample>
    <sample id="238">Yebowen Hu presenta MeetingBank, un nuovo dataset per la sintesi di riassunti di riunioni. Il dataset include 1.366 riunioni del Consiglio Comunale e quasi 7.000 istanze. La raccolta dei dati coinvolge l'uso di Speechmatics API per trasformare l'audio in trascrizioni, identificare il tipo e i dati della riunione, trovare le sommarizioni riferite e ottenere i segmenti di riunione. Il dataset fornisce statistiche su riunioni, durata, token, speaker e periodo di raccolta. Per la valutazione, si utilizzano sistemi di sintesi di riassunti come Oracle, LEAD, LexRank, TextRank, BART-Large, Pagasus, Longformer, DialogLM e HMNet. GPT-3 non si comporta bene in termini di metriche automatiche, ma ottiene i punteggi più alti in valutazioni umane in termini di fluenza e coerenza. MeetingBank è un contributo importante per la sintesi di riass</sample>
    <sample id="239">Ciao a tutti, il mio nome è David Vilar e darò una breve recensione del documento "Prompting PaLM per la Traduzione: Valutazione di Strategie e Prestazioni". È un lavoro a squadra con i miei colleghi di Google Translate. PaLM è un modello di linguaggio a grandi scale a 540 miliardi di parametri presentato l'anno scorso nel 2022. È stato addestrato su una vasta collezione di testi, composta da 780 miliardi di token. Al momento della pubblicazione, ha raggiunto prestazioni di punta in centinaia di compiti di NLP. In questo lavoro, presentiamo lo studio sistematico del primato di modello di linguaggio a grandi scale per la traduzione. Abbiamo valutato la capacità di transizione di tali modelli utilizzando le migliori pratiche della comunità di MT. Questo implica l'uso dei set di test più recenti per evitare un sovrapporsi dei dati di test con quelli di addestramento del modello di linguaggio. Abbiamo comparato con i sistemi</sample>
    <sample id="240">Ciao, sono Dawei, un dottorando all'Università di Saarland in Germania. Nella video vorrei presentare il nostro lavoro recente "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". È un lavoro a squadra con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione debole e all'apprendimento supervisionato debole. Nella supervisione debole, non etichettiamo manualmente i dati. Invece, etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole heuristiche, basi di conoscenza o crowdsourcing di bassa qualità, come illustrato nella figura a destra. In confronto alle etichette umane, le etichette più deboli sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di etichette è errata. Se addestriamo direttamente reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rum</sample>
    <sample id="241">L'articolo discute di un approccio per la valutazione di sistemi di rilevamento di fake news. Si evidenzia che i sistemi attuali hanno due difetti: valutazione non realistica e mancanza di un approccio umanocentrico. Propone un quadro di valutazione per sistemi che risolvano questi problemi. Il sistema è end - to - end, da tweet raw a output utilizzabile da umani. Ha un componente per la rilevazione di false affermazioni e un altro per la verifica di violazioni di politica. L'early detection è importante per gestire meglio la diffusione delle fake news. L'articolo valuta l'efficacia della verifica delle violazioni di politica e mostra che il sistema può rilevare 124.2 violazioni per ora umano. L'obiettivo è motivare lo sviluppo di sistemi di rilevamento di fake news con un coinvolgimento umano.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo sono l'uso di giudici umani per scegliere quale delle due conversazioni è migliore o per valutarle su una scala Likert.</sample>
    <sample id="243">C'è un elenco di autori nel testo, ma non sono specificati quanti sono in totale. Potresti contare quelli che sono menzionati, come Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Per risolvere l'esempio con Servin e Kea, sono necessarie due tipologie di conoscenze di base. Prima, conoscenze specifiche dell'entità come "Servin è un giudice". Secondo, conoscenze di fondo come "I giudici decidono casi in tribunali".</sample>
    <sample id="245">The work presents a two-step pipeline for finding high-agreement MTurk Workers for summarization. It starts with qualification settings, including pre-task qualifications like location, number of HITs, and HIT Approval Rate. The qualification task tests annotators' ability to evaluate multiple dimensions correctly, categorizing them into gold, silver, bronze, and block. Only gold and silver workers pass. The endurance task tests capacity for handling heavy workload. The reference-based task assesses general performance on true annotation tasks. The pipeline results in 6% of 200 participants being gold and silver workers. It's a best practice for high-agreement annotations at large scale and lower cost, avoiding resource waste. Limitations include only English summarization on MTurk, non-"panacea" questions, no guarantee of correctness training, and only Google funding.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">Jiho Kim presenta il loro articolo "FACTKG: Fact Verification via Reasoning on Knowledge Graphs". Esplora l'utilizzo di grafi di conoscenza per la verifica di fatti. Esempi di dataset esistenti come FEVER e VitaminC usano Wikipedia o tabelle. Il loro nuovo dataset, FactKG, utilizza DBpedia e include due tipi di ragionamento: one-hop, conjunction, existence, multi-hop e negation. Il modello GEAR che utilizza evidenza del grafico di conoscenza supera tutte le baselines. Il dataset è disponibile per scarica.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate aggiungendo rumore all'input.</sample>
    <sample id="250">Avere una valutazione dimensionale significa valutare più aspetti della qualità del dialogo, come la rilevanza delle risposte del modello, per capire le forze e le debolezze del modello in modo più dettagliato.</sample>
    <sample id="251">L'articolo non specifica le affiliazioni degli autori.</sample>
    <sample id="252">Sai Kiran Tanikella, un studente di master all'IIT Kanpur, presenta "U-CREAT: Unsupervised Case Retrieval using Events extrAcT". Lavorano insieme Abhinav Joshi, Akshat Sharma e Ashutosh Modi. I legalisti tradizionalmente citano precedenti, ma con un volume crescente di casi, diventa difficile. U-CREAT contribuisce con l'IL-PCR Dataset e il pipeline U-CREAT. L'IL-PCR Dataset è un nuovo benchmark con 7.070 casi legali e 6.775 citazioni per documento. U-CREAT utilizza tecniche di apprendimento non supervisionato e un approccio basato su eventi. L'event extraction è cruciale, rappresentando i casi come una raccolta di eventi. U-CREAT utilizza un'analisi di dipendenza per estrarre eventi. I modelli esaminati includono count-based, transformer-based e event-based. Event Filtered Docs è il modello migliore. U-CREAT apre nuove possibilità per lo sviluppo nel campo della ricerca di precedenti legali. Per</sample>
    <sample id="253">Mario Ezra Aragón presenta "DisorBERT", un modello di adattamento di dominio doppio per rilevare segni di malattie mentali nelle reti sociali. Definisce una malattia mentale come un sindrome psicologico associato a disagio e disabilità che influisce sul pensiero, le emozioni, lo stato d'animo e il comportamento. Molti tipi di malattie mentali esistono, come la depressione maggiore, l'PTSD, l'bulimia e l'anorexia. Le reti sociali offrono un'enorme quantità di contenuti che possono essere analizzati per rilevare problemi mentali. Il loro lavoro mira a contribuire alla rilevazione di malattie mentali analizzando automaticamente i post sui social media. Usano l'adattamento di dominio per migliorare il modello su un dominio specifico, come Reddit e la salute mentale, utilizzando il sapere acquisito da un modello generico come BERT. La loro approccio inizia con un modello base e</sample>
    <sample id="254">L'audio parla di un lavoro di ricerca su "Uncertainty Guided Label Denoising per l'estrazione di relazioni a livello di documento". Sun Qi, da Nanjing University of Science and Technology, presenta un approccio per migliorare la qualità delle etichette in dati distanti. Propone un framework con denoising basato sull'incertezza. Addestra un modello DocRE con dati distanti e umani, utilizza stima dell'incertezza per determinare la qualità delle etichette. Introduce un metodo di stima dell'incertezza a livello di esempio per relazioni sovrapposte. Usa una strategia di rielabeling con sogliati dinamici per filtra le etichette false. Esegue una strategia di addestramento a fasi multiple per ottimizzare i dati distanti. Rispetto a basi linee, il framework migliora significativamente. Contributi principali: framework con denoising basato sull'incertezza, metodo di stima dell'incertezza a livello di esempio per relazioni sovrapposte,</sample>
    <sample id="255">La forma del prompting si rivela importante per zero e uno-shot prompting.</sample>
    <sample id="257">Hanno valutato quattro modelli di dialogo state-of-the-art.</sample>
    <sample id="258">Chiang Cheng-Han parla di un nuovo lavoro che propone l'uso di grandi modelli linguistici per valutare la qualità dei testi in elaborazione del linguaggio naturale. Si danno istruzioni ai modelli e loro valutano le campioni. Ci sono già lavori simili come G-Eval. La motivazione è che l'evaluation umana è instabile e difficile da riprodurre. Si pensa che i grandi modelli linguistici possano valutare come gli umani. L'esperienza mostra che alcuni modelli, come Davinci e ChatGPT, preferiscono le storie scritte umane. Ci sono domande su come i modelli e gli umani concordano sui singoli valori e se cambiare le istruzioni o le campioni influisce sui risultati. Il lavoro è disponibile per consultare.</sample>
    <sample id="259">Yusen Zhang presenta il lavoro "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations". L'obiettivo è creare un set di dati uniforme per la semantica di parsing tra lingue diverse. Contiene 9 dataset, 5 task di parsing semantico, 8 rappresentazioni di significato e 22 lingue naturali. Propone diversi modelli di training e valutazione, come Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot e Few-shot transfer. Trova che Encoder-Decoder ottiene il miglior risultato su tutti i dati. Confronta le prestazioni tra lingue diverse e scopre interessanti risultati, come l'effetto positivo del pretraining su lingue inglesi per Few-shot. XSemPLR è un benchmark unificato per la semantica di parsing tra lingue diverse e rappresentazioni di significato.</sample>
    <sample id="260">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni?</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="262">Non so quanti autori sono coinvolti nell'articolo. Potresti dare un'occhiata al tuo documento per trovare questa informazione?</sample>
    <sample id="263">L'articolo presenta un'analisi sistematica dei problemi di bias nei modelli di apprendimento in contesto. Si parte dalla tipologia dei bias di etichetta, identificando un nuovo tipo di bias, il bias di dominio-etichetta. Si propone un metodo di calibrazione, chiamato calibrazione di dominio-contesto, che utilizza parole casuali del dominio del corpus come testo senza contenuto. Questo metodo migliora significativamente il rendimento dell'apprendimento in contesto per i modelli di grandi lingue. Si dimostra che è migliore rispetto a metodi di calibrazione precedenti, che utilizzano solo parole predefinite o singole parole casuali. Il metodo migliora in particolare per i modelli con un alto livello di bias di dominio-etichetta. Per ulteriori dettagli, consulta il nostro articolo.</sample>
    <sample id="264">Lin Wang, un dottorando di Zhejiang University, presenta il suo lavoro "TAVT: Towards Transferable Audio-Visual Text Generation". Afferma che i modelli uni-modal per la generazione di testo, come la traduzione automatica e le caption di immagini, sono prosperati grazie al pre-allenamento su grandi scale e alla capacità delle grandi reti. Tuttavia, per i compiti multimodal come la generazione di testo audio-visual, l'annotazione dei dati è più ardua e costosa. I lavori esistenti subiscono una degradazione severa a causa delle diverse condizioni di costruzione in diversi domini. Propone il compito di generazione di testo audio-visual trasferibile. Il principale sfidante è la spostamento di dominio multi-modal come lo stile visivo e l'energia sonora. Nota che il contenuto visivo può cambiare significativamente con lo stile dell'immagine e l'angolo di ripresa, mentre il contenuto sonoro come il ritmo e l'energia non ne influisce. Propone un modello che può</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Non so quali siano le affiliazioni degli autori dell'articolo. Potresti dare più informazioni o cercare di trovare queste informazioni da qualche altra parte?</sample>
    <sample id="268">I errori più comuni di PaLM sono gli omission errors. Se hai altre domande su questo argomento, non esitare a chiedere.</sample>
    <sample id="269">Ciao, sono James Finch. E sono Sarah Finch. Oggi vi parleremo di ABC-Eval, un approccio dimensionale nuovo per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato fatto dal Laboratorio di NLP dell'Emory, Emory NLP Lab, guidato dal Professore Jinho Choi all'Università Emory e in collaborazione con Amazon Alexa AI. Supponiamo che tu abbia appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare l'evaluation umana, come chiedere giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala Likert. Questi approcci funzionano bene per fornire valutazioni globali dell'intera qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per comprendere le forze e le debolezze del modello in modo più dettagliato</sample>
    <sample id="270">The authors are affiliated with the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">Non c'è alcun riferimento a "CFT" in questo articolo. Forse hai commesso un errore di battitura? Se hai altre domande sul contenuto dell'articolo, sono felice di aiutarti.</sample>
    <sample id="272">Cinque.</sample>
    <sample id="273">Ciao, il mio nome è Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede il contesto? Esplorazione multilingue e data-driven". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molti traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Beh, se la frase precedente era "Cose potrebbero iniziare a diventare pericolose se i ministri scoprono", allora "mole" si riferisce a un agente informante. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a una macchia di pelle. Quindi, a seconda del contesto, il significato della parola cambia, e quindi la sua traduzione cambia anche. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è abbastanza difficile. In primo luogo, perché solo una piccola parte delle</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">L'audio parla di un lavoro su "IndicMT Eval", un dataset per metarevalutare metriche di traduzione per lingue indiane. Si concentra su cinque lingue, Tamil e Malayalam Dravidiane, Hindi, Marathi e Gujarati Indo Aryan. Dal Flores dataset, selezionano 200 frasi e generano 1.400 traduzioni candidate per ciascuna lingua. Utilizzano annotatori bilingui per valutare le traduzioni, classificando gli errori e assegnando un punteggio. Si analizzano diverse metriche, come chrF, LabSE, BERTscore e COMET, confrontandole con le annotazioni umane. COMET si dimostra la metrica migliore in generale. IndicCOMET, fine-tunata con il dataset MQM, supera le basi COMET su tre lingue e ha una maggiore robustezza. Il dataset è pubblicamente disponibile.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Le parole contrassegnate sono un metodo per identificare le parole che distinguono gruppi marcianti da quelli non marcianti. Si basa sul concetto sociolinguistico della "marcatura", secondo cui c'è un gruppo non marcianto di default e qualsiasi gruppo che differisce da questo default è marcato linguisticamente.</sample>
    <sample id="279">I'm not sure which article you're referring to. Could you please clarify?</sample>
    <sample id="280">The paper introduces MultiEMO, a novel attention - based correlation - aware multimodal fusion framework for emotion recognition in conversations. It addresses challenges like underexploiting multimodal information, poor performance on minority emotion classes, and distinguishing semantically similar emotions. Key contributions include VisExtNet for visual feature extraction, MultiAttn for multimodal fusion, and Sample - Weighted Focal Contrast loss. VisExtNet captures facial expressions without redundant scene info. MultiAttn integrates modalities through cross - attention. SWFC loss focuses on minority classes. Experiments on MELD and IEMOCAP show state - of - the - art performance. Limitations include VisExtNet not distinguishing speakers and SWFC loss needing large batch size.</sample>
    <sample id="281">Kayo Yin e altri hanno presentato un lavoro su quando la traduzione richiede contesto. Molti traduzioni dipendono dal contesto. Ad esempio, "mole" può significare spia o macchia di pelle a seconda del contesto precedente. Valutare come i modelli traducono in casi di contesto dipendente è difficile. Hanno introdotto CXMI e poi Pointwise CXMI per misurare l'uso del contesto durante la traduzione. Analizzando parole con alto P-CXMI, hanno trovato pattern tra queste parole. Hanno creato un benchmark multilingue per la traduzione a livello di documento. Con il benchmark, i modelli a livello di documento sono stati valutati. I modelli a livello di documento che usano contesto sono significativamente più accurate per certi fenomeni discorsivi come formalità e coesione lessicale. Ma non sono molto meglio di modelli che non usano contesto per altri fenomeni come ellissi, pronomi e forma del verbo. Hanno anche confrontato differenti sistemi commerciali e DeepL è</sample>
    <sample id="282">Xuekai Zhu presenta "StoryTrans" a ACL 2023. Questo lavoro si concentra sulla trasformazione di stile non parallela a livello di storia, utilizzando rappresentazioni discorsive e migliorando il contenuto. I principali ostacoli sono la complessità delle preferenze linguistiche dell'autore a livello discorsivo e la forte associazione tra stili e temi specifici. La soluzione proposta è il modello di generazione StoryTrans, che impara rappresentazioni discorsive dai testi sorgente e combina queste con embedding di stile imparabili per generare testi in stili target. Viene progettato un nuovo obiettivo di addestramento per ridurre le caratteristiche stilistiche delle rappresentazioni discorsive. Inoltre, per preservare il contenuto, la generazione viene suddivisa in due fasi. L'addestramento è suddiviso in due fasi. La prima utilizza un addestramento consigliato con perdite di autorecostituzione, disentangolamento e ordine delle frasi</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è quella del Prague approach.</sample>
    <sample id="284">Peng Tianshuo presenta un lavoro intitolato "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" per ACL's Main Conference 4,915. Il lavoro si concentra su migliorare i modelli di estrazione di informazioni basati su spazi. Attualmente, questi modelli dipendono troppo dalle posizioni di confine degli spazi annotati, ma ci sono ambiguità nell'etichettatura delle confini spaziali. Quindi, l'idea è che i confini spaziali appresi dal modulo dovrebbero essere flessibili invece che precisi. Ci sono anche incompatibilità tra l'estrazione di caratteristiche da trasformatori e l'estrazione di informazioni. Propone un'attenzione adattiva per la decisione di estrazione di spazi per modellare il confine spaziale più lontano come una distribuzione continua di probabilità corretta in un intervallo specifico. Per dimostrare le capacità di FSUIE, vengono condotti esperimenti su tre principali task di estrazione di informazioni, ottenendo</sample>
    <sample id="285">Il lavoro di Mingqi Gao e Peking University si concentra sulla correzione di errori di fattualità in sommari di dialogo. Esistono due soluzioni principali: introdurre obiettivi di fattualità durante l'addestramento o l'infrazione e progettare un modello di correzione di errori di fattualità, FEC. Attualmente, non esiste lavoro su errori di fattualità per sommari di dialogo. Le metriche di fattualità come FactCC e DAE sono utilizzate per valutare FEC, ma hanno due difetti. La valutazione basata su metriche di fattualità è vaga e non affidabile. Introducendo correzioni di riferimento manualmente annotate, si possono risolvere questi problemi. Propone una nuova classificazione di errori di fattualità, basata su ERRANT. Con il loro framework di valutazione, si scopre che addestrare FEC con sommari di riferimento da dataset di sommari di dialogo dà i migliori risultati rispetto a metriche di fattualità non affid</sample>
    <sample id="286">The name of the speaker is James Finch.</sample>
    <sample id="287">Quattro.</sample>
    <sample id="288">Per testare i fenomeni sintattici, si possono utilizzare insiemi di dati come BLiMP, SyntaxGym e CrowS pairs.</sample>
    <sample id="290">FTw, COSINE.</sample>
    <sample id="291">The model is evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su dati di web.</sample>
    <sample id="295">Adam Przepiórkowski</sample>
    <sample id="296">Il lavoro è un risultato di collaborazione tra l'Università di Torino e Amazon Alexa. Si concentra sull'ironia in particolare, un fenomeno pragmatico e latente. Hanno creato il corpus EPIC per studiare l'ironia. Hanno raccolto dati da social media, Reddit e Twitter, per 300 conversazioni brevi. Hanno utilizzato Prolific per l'annotazione, con 74 annotatori. L'inter-annotator agreement varia a seconda della divisione dei dataset. I modelli perspective-aware mostrano una maggiore certezza nelle previsioni. Trovano che generazioni vicine e geografie diverse influenzano le percezioni dell'ironia. Se hai domande, possiamo discutere ulteriormente al poster session.</sample>
    <sample id="297">Il contenuto parla di un lavoro sull'uso dei "dogwhistles" in politica e linguistica. I dogwhistles sono parole che inviano un messaggio a un gruppo esterno e un altro, spesso tabù, a un gruppo interno. Il lavoro sviluppa una tipologia e un glossario di dogwhistles, specialmente razzisti, transfobici e antisemiti. Si analizzano discorsi politici storici e si valutano le capacità di riconoscimento dei dogwhistles in modelli di linguaggio come GPT-3. Si dimostra come i dogwhistles possano evadere la moderazione del contenuto online. Il tutto è importante per capire come influenzano la politica e la persuasione.</sample>
    <sample id="298">Per la perdita di prestazioni, la derivazione temporale è la causa principale. Perché? Perché quando abbiamo ritratto o continuato a pre-Allenare alcuni modelli con dati più recenti, abbiamo trovato che la prestazione peggiora con un gap temporale più grande. Questo conferma la nostra ipotesi che la principale causa della perdita di prestazioni è la derivazione temporale. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="299">L'audio parla di migliorare la robustezza dei modelli di NLI. Si parla di shortcut, correlazioni spurie tra attributi di input e etichette. I modelli di NLI che ne approfittano funzionano bene su campioni in distribuzione ma sono fragili su test fuori distribuzione. Si propone un metodo di addestramento minimax per ridurre l'uso di shortcut. L'idea è concentrarsi su esempi difficili che potrebbero contraddire i shortcut. I due modelli si ottimizzano alternativamente. Si valuta il metodo su MNLI, FEVER e QQP, e si osserva che migliora la performance fuori distribuzione mantenendo alta la precisione in distribuzione. Si esplora se i miglioramenti si trasferiscono in modelli più grandi, shortcut sintetici e test fuori dominio. Se sei interessato, puoi parlare durante la sessione poster.</sample>
    <sample id="300">L'audio parla di un'interazione con un assistente virtuale. Si inizia con la transizione di un'email. L'utente dice "Hey, I'm really sorry, but I can't make it today!" e questo è registrato come un'interazione. Poi, l'utente usa il comando "Change the comma after 'Hey' into an exclamation point" per modificare l'email. Questo comando non appare automaticamente nell'email, ma l'utente deve mostrare il cambiamento utilizzando il mouse e il tasto del computer. L'utente può anche correggere l'ASR. L'audio parla anche di un'interazione con un assistente virtuale. Si inizia con la transizione di un'email. L'utente dice "Hey, I'm really sorry, but I can't make it today!" e questo è registrato come un'interazione. Poi, l'utente usa il comando "Change the comma after 'Hey' into an exclamation point" per modificare l'email. Questo comando non appare automaticamente nell'email, ma l'utente deve mostrare il cambiamento utilizzando il mouse e il</sample>
    <sample id="302">Perché i token non sono ordinati dopo la prima fase di taggamento.</sample>
    <sample id="303">Perché non sanno se i positivi stereotipi sono causati da un allineamento di valore eccessivo o altri metodi di anti-stereotipo che stanno producendo questi modelli dannosi.</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono le frasi inaccettabili che vengono scelte per creare sequenze più lunghe e testare l'acceptability del modello. Queste frasi vengono estratte da dataset diversi o addirittura da un'area completamente diversa come Wikipedia.</sample>
    <sample id="305">Dawei, un dottorando a Saarland University, presenta il lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Parla di weak supervision, dove non si etichettano manualmente i dati, ma si usa etichettatura debole come regole semplici, basi di conoscenza o crowdsourcing di bassa qualità. I dati debole etichettati sono più economici ma anche rumorosi. Nella weakly supervised learning, si propone addestramento per robusto addestramento di reti neurali sotto etichettatura rumorosa. Recentemente, si diceva che si poteva addestrare solo sui dati debole etichettati e ottenere buone prestazioni su set di test puliti. Ma ci sono dei problemi. I risultati mostrano che i metodi recenti di WSL richiedono campioni puliti per funzionare correttamente. Altre cose: più campioni puliti migliorano le prestazioni, ma addestrare direttamente sui dati puliti può anche portare a migliori risultati. In pratica, non c'è motivo di scegliere</sample>
    <sample id="306">Sebastian Schuster e Najoung Kim presentano un'analisi sull'Entity Tracking in Language Models. L'obiettivo è capire in che misura i grandi modelli linguistici riescono a seguire gli stati degli enti in un discorso. I modelli devono comprendere come gli enti cambiano stato durante il discorso. Per esempio, in un ricettario, devono capire che gli ingredienti finiscono in una ciotola e diventano parte della preparazione. Ci sono sfide nel progettare una task per valutare queste capacità. Tra cui, che gli stati degli enti possono essere comuni nel dataset di addestramento, che gli stati possono essere previsti da parole singole senza considerare il discorso intero e che il modello può memorizzare sequenze di stati o applicare semplici associazioni. Hanno progettato una task coinvolgendo scatole e oggetti. I modelli devono prevedere il contenuto delle scatole dopo operazioni come spostare o aggiungere oggetti. I risultati mostrano che solo un modello, text</sample>
    <sample id="307">The authors used metrics for public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="308">Jenny, una studentessa di dottorato in primo anno all'Università Carnegie Mellon, presenta il lavoro "NLPositionality" che caratterizza i bias di progettazione dei dataset e dei modelli. Il lavoro è stato fatto in collaborazione con l'Università di Washington e l'Allen Institute for AI. Si parla di un esempio di bias di progettazione dove l'API Prospective AP funziona bene per Carl Jones ma meno per Aditya Sharma, a causa di termini offensivi più comuni in contesti indiani. I bias di progettazione possono essere influenzati dalla posizione dei ricercatori e dei sviluppatori di modelli, ovvero le loro prospettive determinate da demografia, identità e esperienze di vita. I dataset e i modelli possono rappresentare certe posizioni sopra altre. Per studiare la posizione dei dataset e dei modelli, si confrontano le annotazioni con gli utenti reali attraverso un framework chiamato NLPositionality. Il framework include due passaggi principali: riassegnare le annotazioni con annotatori diversi e confrontare le annotazioni demografiche con</sample>
    <sample id="309">L'inter-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Non so. Non c'è alcuna informazione sull'affiliazione degli autori nell'articolo. Potresti cercare ulteriori dettagli?</sample>
    <sample id="312">MultiInstruct è il primo dataset di tuning di istruzioni multi-modali, mentre gli altri parametri di riferimento non hanno un grande insieme di istruzioni multi-modali pubblicamente disponibili. MultiInstruct contiene 62 diverse istruzioni multi-modali coprendo 10 categorie ampie, mentre gli altri non hanno un insieme così vasto.</sample>
    <sample id="313">Due.</sample>
    <sample id="314">Non c'è una definizione di coordinazione binaria specifica qui. Ma in generale, la coordinazione binaria potrebbe essere una forma di coordinazione tra due elementi simili, come ad esempio due nomi o due verbi. Se hai bisogno di una definizione più precisa, potresti cercare in un dizionario di grammatica. Cosa ne pensi?</sample>
    <sample id="315">Non c'è una risposta precisa nel testo su quanto tempo sono stati utilizzati i prompt in media.</sample>
    <sample id="316">The smaller T5 model fine-tuned on CoScript can generate scripts of higher quality than most large language models. This shows that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="317">Peng Li from Fudan University presents "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors". Information extraction aims to extract structured info from unstructured text. Previous models like T5 and GPT-3 use text - to - text pre - training but struggle with structured outputs. CodeIE transforms text - to - structured extraction into structure - to - structure code generation. For named entity recognition, a function is defined to extract entities. CodeIE outperforms traditional models like UIE and GPT - 3 in few - shot tasks. Code format prompts reduce structural errors and improve recall. The study shows that code - based models are better for information extraction tasks.</sample>
    <sample id="318">Ciao, sono Yanis Labrak e presenterò i nostri lavori su "DrBERT: Un modello pre-allenato robusto in francese per domini biomedici e clinici.".In questa presentazione, iniziamo parlando di modellazione del linguaggio nel campo sanitario. Poi presenteremo il contributo principale del nostro articolo. Introduciamo DrBERT, il primo modello biomedico in francese, basato su RoBERTa e allenato su NACHOS, un set di dati di dati medici riscattati dalla rete. Inoltre, presentiamo una comparazione di modelli con diversi set di pre-allenamento e fonti di dati. Successivamente, presentiamo i risultati su 11 task di downstream biomedici e clinici in francese. Infine, concludiamo gli esperimenti e vi diamo più dettagli su come accedere a quei modelli.Dal suo lancio nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere compiti di processamento del linguaggio naturale e offre un incremento di prestazioni notev</sample>
    <sample id="319">The work examines from-scratch pre-training and control pre-training using the weight and tokenization of CamemBERT.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è maggiore di uno.</sample>
    <sample id="321">The quality of simplification was evaluated by analyzing the type of simplification, like lexical simplification, structure simplification, and overall level of simplification. For example, Bible texts were much stronger simplified than news texts or language learner texts.</sample>
    <sample id="322">Enrico presenterà a ACL 23 su "What does a Text Classifier Learn about Morality?". Spiega che la moralità è l'elemento che ci aiuta a distinguere il giusto dallo sbagliato. La moralità è fondamentale per le nostre società. L'approccio tradizionale alla moralità in testo è troppo semplice, considera solo un'scala tra immorale e morale. La moralità è soggettiva e varia da persona a persona. La Moral Foundation Theory identifica cinque fondamenti differenti di moralità. Enrico e il suo team hanno applicato tecniche di AI spiegabile a modelli di linguaggio addestrati per comprendere la moralità in testo. Hanno usato un corpus di tweet chiamato Moral Foundation Twitter Corpus. Hanno scoperto che i modelli di linguaggio riconoscono che la moralità è espressa diversamente in diversi domini, come ALM e BLM. Per esempio, in ALM, la subversion è considerata negativa, mentre in BLM, è considerata in parte positiva. L'obiettivo è</sample>
    <sample id="323">Yujie Wang from Shanxi University presents a paper on "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA". The task is challenging as it needs machines to answer questions relying on common knowledge. Many works combine language models and knowledge bases. However, they face issues like noisy entities and limited interaction between modalities. DHLK is proposed. It builds an HKG based on multiple knowledge bases using a two-stage pruning strategy and KRL. It encodes and fuses QA contexts and entities using RoBERTa and Mask Self-Attention. It dynamically removes weakly relevant entities. HKG is optimized with TransE. Relation Mask Self-Attention is used instead of GNN. The final answer prediction uses HKG graph embedding, paths, and QA context embedding. Experiments on CommonsenseQA and OpenBookQA show good results compared to other methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Ad esempio, GPT-4 è il più liberale tra loro, mentre GPT serie sono più liberali socialmente rispetto a BART serie e sue varianti.</sample>
    <sample id="325">Ciao! Mi chiamo Matthias Lindemann, e oggi darò un breve introduzione al nostro articolo intitolato "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations". Questo è un lavoro a joint con i miei tutori Alexander Koller e Ivan Titov. La generalizzazione compositiva può essere compresa come la capacità di un apprenditore di gestire una ricorsione più profonda e composizioni di frasi non viste durante l'addestramento. Nel contesto della semantica di parsing, la prova della generalizzazione compositiva potrebbe sembrare così. Come di consueto, abbiamo un set di istruzioni di addestramento. In questo caso, "La ragazza ha dormito." E "Mary sapeva che la ragazza ha dormito." Queste istruzioni sono associate con forme logiche che rappresentano aspetti fondamentali del loro significato. Contrariamente all'evaluation standard di machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste. In questo esempio, il modello ha visto una ricorsione superf</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono inconsistenti. Ad esempio, se una persona dice "So che le sigarette possono uccidere" e poi fuma dopo una riunione, c'è una dissonanza tra la credenza e l'azione.</sample>
    <sample id="327">Xiao Xu, a third-year PhD student from Harbin Institute of Technology, presents their work "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning" at ACL 2023. This work builds upon BridgeTower to address its limitations. ManagerTower introduces managers in each cross-modal layer to gather insights from pre-trained unimodal experts at different levels. It uses RoBERTa and CLIP-ViT base as unimodal encoders. ManagerTower achieves superior performance, especially a 39.15% accuracy on Wikivideo test standard, compared to BridgeTower. The work demonstrates that adaptive managers can effectively exploit different levels of unimodal semantic knowledge for comprehensive cross-modal representation learning. Paper, code, and modals are available on Archive and Github.</sample>
    <sample id="328">GPT-4.</sample>
    <sample id="329">Il lavoro di Minghang Zheng e collaboratori si concentra sulla localizzazione di frasi in video zero-shot. L'obiettivo è trovare segmenti video più pertinenti per una query naturale lungo un video lungo. Tuttavia, molti metodi richiedono molte annotazioni manuali, che sono costose e inefficienti. I metodi esistenti hanno tre difetti principali: le query pseudo sono troppo semplici, non garantiscono l'irrilevanza tra query e video fuori dall'evento, e ignorano il rischio di etichette rumorose. Propone un metodo di generazione di pseudo etichette strutturate resistente al rumore. Genera query pseudo più complesse e eventi pseudo basati sulla struttura temporale degli eventi. Riduce l'influenza delle etichette rumorose tramite re-peso di campioni e raffinamento di etichette. I risultati sugli esperimenti sui dataset ActivityNet Captions e Charades-STA mostrano che il metodo proposto supera gli altri metodi zero-shot su molti metrici. Il codice è disponibile per la</sample>
    <sample id="330">No, l'addestramento cumulativo non funziona meglio di quello iterativo.</sample>
    <sample id="331">Sara Papi.</sample>
    <sample id="332">I dati sono stati tratti da un corpus parallelo.</sample>
    <sample id="333">Wenhao from Nanjing University introduces their work "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation". They acknowledge collaborators. Their work focuses on neural machine translation, aiming to improve NMT model generalization. They observe that neural networks induce non-smooth representation spaces, leading to poor performance in areas with sparsely dispersed low-frequency tokens. To enhance performance, they propose kNN-MT, which smooths predictions based on nearest neighbors. However, it has drawbacks like time-consuming neighbor retrieval and static datastore. INK is proposed to overcome these. It has a two-step training loop: extracting kNN knowledge to guide adapter adjustment and updating representations asynchronously. Experiments show INK outperforms kNN-MT and achieves better BLEU scores with less memory and faster inference. It brings an average gain of 1.99 COMET score and 1.0 BLEU score compared to state-of-the-art systems.</sample>
    <sample id="335">Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di addestrare un modello linguistico su una lingua e poi utilizzarlo per comprendere o generare testo in un'altra lingua.</sample>
    <sample id="337">The speaker presents a research on "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning". They address the challenge of representing out-of-vocabulary words, which are crucial for embedding-based models. They observe word formation and associate OOV words with relevant words to infer meaning. A Word Relationship Graph is developed, mimicking lexical rules. The OOV word is tokenized and associated with other words, forming a two - level graph. Nodes are assigned attributes using a self - attention network based on characters. Two levels of Graph Attention Network are applied to reduce noise. A readout block layer captures whole graph information. A simple Graph Convolutional Network is used for word formation. Contrastive learning is applied in the loss function. Experiments show the model outperforms baselines. It benefits static and contextual models. Agglutinative languages are well - suited, while fusional languages present challenges. English performs well with reasonable word segmentation. The graph in the model can handle various word formations. The speaker believes the model's application to other languages depends on rational word decomposition.</sample>
    <sample id="338">Il lavoro presenta un'analisi su come le spiegazioni umane siano sempre utili. È un lavoro di gruppo di Rensselaer Polytechnic Institute, Northeastern University e IBM Research. Si parla di motivazione, lavori correlati e contributi. I contributi sono in tre sezioni: struttura unificata, esperimenti preliminari e valutazione di cinque dataset e due modelli. Si parla di come le spiegazioni umane possano essere soggettive e dipendenti dal compito. Si introduce un'analisi di spiegazioni umane per una domanda di conoscenza del mondo e si confrontano due spiegazioni. Si parla di metriche tradizionali come BLEU e ROUGE e di una nuova metrica chiamata TREU. Si valutano cinque dataset con due modelli, T5 e BART, e si dimostra che la nostra metrica è migliore della simulatabilità. Lavoro che mette le basi per una collaborazione di qualità nell'annotazione umana.</sample>
    <sample id="339">I'm sorry, but I don't have the specific affiliations of the authors in the text you provided. You might need to look for that information in the full article or the conference proceedings. If you have any other questions about the article or the presentation, feel free to ask.</sample>
    <sample id="340">Kuan-Hao Huang presenta ParaAMR, un dataset di parafilie sintatticamente diverso grande in scala. Usa AMR back-translation. Ha circa 15 milioni di frasi sorgenti e 6,9 parafilie per frase. È diverso da altri dataset in scala grande in termini di diversità sintattica. Aiuta nell'apprendimento di embedding di frase, nella generazione di parafilie con controllo sintattico e nell'aumento di dati per apprendimento a pochi esempi. Il dataset è disponibile in un link.</sample>
    <sample id="341">Gli autori fanno ricorso a due misure di latenza: la latenza media e la latenza computazionale consapevole.</sample>
    <sample id="342">Gao Jingsheng presenta il suo lavoro "LiveChat: un grande insieme di dati di dialogo personalizzato automaticamente costruito da streaming live". L'introduzione spiega l'Open Domain Dialogue e la necessità di un insieme di dati video-sourced. Il primo punto chiave è trovare una meccanica di matching efficace per catturare le relazioni di risposta tra gli interlocutori. Il secondo punto è l'importanza del dialogo personalizzato per applicazioni come streamer virtuali e dipendenti virtuali. Il terzo punto è la difficoltà di avere un insieme di dati di dialogo multi-partita in cinese. Il loro LiveChat è un insieme di dati video-sourced con una scala maggiore. Il loro metodo di costruzione automatica è unico. Hanno condotto esperimenti su due compiti di riferimento: modellazione di risposta e riconoscimento dell'addressee. Hanno anche studiato il trasferimento di apprendimento pre-Allenato sui loro LiveChat. Concludono che il loro LiveChat è un insieme di dati cinese video-sourced e personalizzato</sample>
    <sample id="343">Ciao a tutti, sono Akshatha, e oggi il mio coautore Martin e io stiamo presentando il nostro lavoro "The KITMUS Test: Evaluando la integrazione del conoscenza da diverse fonti". Questo lavoro è una collaborazione tra l'Università McGill, Mila e Microsoft Research. I modelli di intelligenza artificiale naturale linguistica traggono da una varietà di fonti di conoscenza, come quella contenuta nei loro parametri, solitamente acquisita da un pretraining, e quella data in input durante l'inferezione. Opere recenti in compiti come la risoluzione di domande mostrano che i modelli possono utilizzare la conoscenza acquisita durante il pretraining per risolvere il compito. Ma l'intelligenza artificiale naturale linguistica spesso richiede anche conoscenza fornita durante l'inferezione. Ad esempio, nella frase "John ha visto il presidente appena eletto in TV". I parametri pretrain possono contenere informazioni su ciò che i presidenti fanno e su cosa sia un TV, ma non possono conoscere con certezza</sample>
    <sample id="344">I svantaggi dei metodi basati su alberi sono che spesso non vengono dati e devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta un processo computazionalmente costoso. Inoltre, spesso richiedono un pre - processamento formale significativo delle forme logiche, ad esempio per gestire i simboli variabili.</sample>
    <sample id="345">The paper introduces a method for compositional generalization without trees in semantic parsing. It shows a neural seq2seq model that predicts output from input in two steps. First, it tags input tokens with multiset of output tokens. Then, it predicts a permutation to order these tokens. The model is flexible and expressive. It outperforms other treeless models on the COGS benchmark for generalization to deeper recursion. The paper addresses challenges like alignment and finding linguistically plausible permutations. It uses a GPU - friendly continuous relaxation for permutation finding. If you want to know more, check out the paper or poster.</sample>
    <sample id="346">I'm sorry, the text doesn't provide information about the affiliations of the authors.</sample>
    <sample id="347">Ciao Myra! Ecco una versione in italiano del tuo contenuto:.Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi e stereotipi nei grandi modelli di linguaggio, o LLM. Tuttavia, questi metodi presentano varie limitazioni. Generalmente si basano su set di dati manualmente costruiti che richiedono molto tempo per curare e inoltre misurano solitamente stereotipi molto specifici, il che significa che non si generalizzano bene a altre democrazie o contesti, o semplicemente catturano associazioni generali, come associazioni negative con determinati gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezione, che è la nozione che identità sociali multifaccettate possono comporre pregiudizi e essere luoghi</sample>
    <sample id="348">L'articolo parla di come misurare le stereotiphe nelle grandi modelle di linguaggio. Molti hanno documentato la presenza di bias sociali in queste LLM. Tuttavia, i metodi attuali hanno limitazioni. Questo lavoro, in collaborazione con Esin Durmus e Dan Jurafsky, usa promemoria naturali per creare personaggi immaginari. Questi personaggi possono essere generati con promemoria specifiche, come "immagina di essere una donna asiatica". I risultati mostrano interessanti modelli, come la descrizione di donne di colore che fanno riferimento all'ancestralità. Il metodo ha due parti: generare personaggi e utilizzare parole marcate per identificare differenze tra gruppi marcato e non marcato. I risultati mostrano che i personaggi generati contengono più stereotiphe rispetto a quelli scritti da umani. Tuttavia, la distribuzione di parole dei personaggi generati è più limitata. L'articolo conclude con tre raccomandazioni per gli owner di modello: affrontare i stereotiphe positivi</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jingwei Yi dall'Università di Scienza e Tecnologia della Cina. È un piacere presentare un breve video pubblicitario del nostro articolo. Stai copiando il mio modello? Protezione del copyright dei modelli di linguaggio naturale grandi per l'embeddaggio come servizio tramite marchio di fabbrica indietro. Prima di tutto, introduciamo il contesto dell'embeddaggio come servizio. Attualmente, modelli di linguaggio naturale come GPT, LLAMA, PALM sono eccezionali nell'interpretazione e generazione del linguaggio naturale. L'embeddaggio come servizio è uno dei servizi costruiti su tali modelli per assistere varie, NLP, compiti. Ad esempio, OpenAI offre un API di embeddaggio basata su GPT. Tuttavia, recenti studi hanno mostrato che l'attaccante potrebbe rubare il modello imparando dall'embeddaggio e fornire servizi simili. Quindi, è necessario proteggere il copyright dell'embeddaggio</sample>
    <sample id="350">Nel contenuto inglese si parla di un paper intitolato "What’s the Meaning of Superhuman Performance in Today’s NLU?". Simone Tedeschi e altri ricercatori hanno analizzato due benchmark popolari in NLP e NLU, SuperGLUE e SQuAD. Si è scoperto che i sistemi superano gli umani in molti task, ma ci sono errori nella valutazione. Ad esempio, gli umani vengono valutati su un subset molto piccolo del test set, mentre i sistemi su tutto. Ci sono anche errori nei dati, come nel caso del dataset Recognizing Textual Entailment. Si sostiene che le stime umane siano vaghe e che i benchmark non siano costruiti in modo scientificamente corretto. In conclusione, le affermazioni di superperformance umana non sono ancora fondate. Per saperne di più, leggi il paper.</sample>
    <sample id="351">L'articolo esplora se i tagger di CoNLL-2003 per l'NER funzionano ancora bene nel 2023. Si è studiato il problema della generalizzazione utilizzando il NER. Si osserva che i modelli sviluppati per CoNLL-2003 sono stati utilizzati per 20 anni per sviluppare NER. Si chiede se questi modelli possono generalizzare a nuovi dati e cosa è necessario per una buona generalizzazione. Si sviluppa il CoNLL++ Dataset, un set di dati raccolti da Reuters News del 2020 e annotati con le stesse linee guida di CoNLL-2003. Si fine-tune più di 20 modelli su CoNLL-03 e CoNLL++. Si calcola il cambiamento percentuale in F1 per valutare la generalizzazione di ciascun modello. Per una buona generalizzazione, si trovano tre ingredienti principali: architettura del modello, dimensione del modello e numero di esempi di fine-tuning</sample>
    <sample id="352">ABC-Eval è un approccio dimensionale per valutare l'intelligenza artificiale conversazionale. È un metodo che cerca di ridurre la soggettività dell'evaluation umana, annotando se le risposte del modello esprimono determinati comportamenti, come rispondere con informazioni irrilevanti o contraddizioni. Questo permette di misurare le tassi di errori tematici commessi dai modelli di chat, come l'ignoranza del partner, le contradizioni, le affermazioni errate o la violazione del buon senso.</sample>
    <sample id="353">L'articolo introduce un metodo per la generazione di codice Python attraverso domande di chiarimento. Motivazione: la generazione di codice e la sintesi di programmi da una descrizione naturale linguistica è un'area di ricerca calda. I metodi attuali falliscono a gestire l'underspecification. L'underspecification è comune in casi reali. L'interazione, come domande di chiarimento, è un buon paradigma per affrontare questo problema. Due sfide sono identificate: le specie mancanti possono accadere a diversi livelli e non è chiaro come identificare se una descrizione naturale linguistica, NLD, contiene informazioni su specie a qualsiasi livello. Il metodo proposto è di creare CodeClarQA, un dataset sintetico con chiarimenti su operazioni chiave. Si propone un pipeline di generazione di codice attraverso domande di chiarimento. Si identificano operazioni chiave basandosi su un grafo di conoscenza generato da Graph4Code. Si analizzano i risultati di identificazione di operazioni chiave manc</sample>
    <sample id="354">Fino al 2020.</sample>
    <sample id="355">Ciao Vasudha. Sono Rendi. Sono un assistente virtuale. Hai presentato il tuo lavoro accettato in ACL 2023 come un lungo articolo intitolato "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è quando due credenze o azioni sono inconsistenti, come in questo esempio dove una persona dice "So che le sigarette possono uccidere", e poi dice "Ho preso un paio di sigarette dopo la riunione". Questa credenza e azione sono inconsistenti e sono in dissonanza. Inoltre, "Non penso che potrei mantenere il mio lavoro senza di loro" giustifica la seconda occorrenza. Hanno una relazione consonante. Mentre la dissonanza è un fenomeno molto comune che abbiamo esperito nella decisione quotidiana, sono davvero rari di trovare espressi in linguaggio tra altri tipi di relazioni discorsive</sample>
    <sample id="356">Non so. Non ho informazioni sulle affiliazioni degli autori dell'articolo. Potresti cercare di trovare queste informazioni sul sito del loro dipartimento o su qualche altro sito accademico?</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">There are five authors involved in the article.</sample>
    <sample id="359">L'approccio viene confrontato con le strategie Wait-k e Local Agreement, applicate a modelli offline, e con l'architettura di pre-traduzione specifica per simulST.</sample>
    <sample id="361">Armineh Nourbakhsh, PhD student at Carnegie Mellon University, presents "CounterComp". It focuses on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. The work aims to enhance models' performance on tasks like financial table question answering, where multiple arithmetic operations are involved. State-of-the-art models often memorize spurious patterns, leading to poor performance. CounterComp mines counterfactual scenarios from training data to create positive and negative examples. These are used to add an auxiliary metric learning loss to the training procedure, which improves performance on in-distribution and out-of-distribution samples. This helps the model attend to more meaningful tokens related to operations in the output. For more details, check the poster or contact the listed person.</sample>
  </task>
</testset>