<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models are things like patents.google.com, en.m.wikipedia.org, and www.nytimes.com. There are quite a few others too. If you want to know more about how they're used, just let me know.</sample>
    <sample id="1">The affiliations are McGill University/Mila and Microsoft Research. If you want to know more about their work or anything else related, feel free to ask.</sample>
    <sample id="35">I'm not sure. There's no name mentioned in the English content. Maybe you could check for any other details?</sample>
    <sample id="36">T5 XL model.</sample>
    <sample id="37">Yes!</sample>
    <sample id="38">The novelty is in rating the relevance of the bot's responses. If you want to know more about it, feel free to ask.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on the quality of the weak labels. If you have any other questions about this or related topics, feel free to ask.</sample>
    <sample id="40">The slide doesn't mention specific advances to improve the score. You might need to look at other parts of the presentation or research for that. If you have any other questions about this or something else, feel free to ask.</sample>
    <sample id="41">There are five authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="75">There are three authors involved in the paper. If you have any other questions about it, feel free to ask.</sample>
    <sample id="76">The domains "bible" and "fiction" are simplified more. If you have any other questions about this, feel free to ask.</sample>
    <sample id="77">The example is "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="78">Yes, you can use the DrBERT models, the NACHOS dataset, and the training scripts for your research as they are freely available under the MIT license. If you have any other questions about this, feel free to ask.</sample>
    <sample id="79">DEplain-APA contains academic papers.</sample>
    <sample id="80">Well, for good generalization, you need a better model architecture, a larger model size, and more fine - tuning examples. That's what the slide says. If you want to know more about how these factors work together, just let me know.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by looking at statistics extracted from an enhanced version of the Penn Treebank.</sample>
    <sample id="82">The experiments were designed by looking at different positions of the governor, like on the left or right, and measuring the proportion of shorter left conjuncts depending on the absolute difference of conjunct lengths. They also considered different units of measurement for length, like characters, syllables, and words. So, they compared how these factors influenced the results. If you want to know more about the specific details, feel free to ask.</sample>
    <sample id="83">Not well. It's not better than chance. If you want to know more about how to improve it, just let me know.</sample>
    <sample id="84">There are four authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="85">The characters' names aren't mentioned in the example conversation.</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena like formality and lexical cohesion. But they don't do as well on ellipsis, pronouns, and verb form. If you want to know more about this, feel free to ask.</sample>
    <sample id="87">The affiliations are Johns Hopkins University, Purdue University, MIT, and Meta AI. If you want to know more about their research or anything else related, feel free to ask.</sample>
    <sample id="88">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations.</sample>
    <sample id="89">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations.</sample>
    <sample id="90">Compositional Generalization. Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x sleep.agent x x</sample>
    <sample id="92">The girl slept. Mary knew that the girl slept.</sample>
    <sample id="93">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x x sleep.agent x x.Test:.- Jim said that Mary knew that the girl slept. *girl x say.agent x Jim say.ccomp x x x know.agent x Mary know.ccomp x x sleep.agent x x</sample>
    <sample id="94">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x x sleep.agent x x.Test:.- Jim said that Mary knew that the girl slept. *girl x say.agent x Jim say.ccomp x x know.agent x Mary know.ccomp x sleep.agent x x</sample>
    <sample id="95">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x x sleep.agent x.Test:.- Jim said that Mary knew that the girl slept. *girl x say.agent x Jim say.ccomp x Mary know.agent x Mary know.ccomp x sleep.agent x.Naive seq2seq models fail!</sample>
    <sample id="96">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x x sleep.agent x.Test:.- Jim said that Mary knew that the girl slept. *girl x say.agent x Jim say.ccomp x x x know.agent x Mary know.ccomp x sleep.agent x.Naive seq2seq models fail!</sample>
    <sample id="97">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept.</sample>
    <sample id="98">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept.</sample>
    <sample id="99">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms.</sample>
    <sample id="100">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="101">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="102">Trees help a lot but. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="103">Trees help a lot but. Trees need to be obtained: Pre/Post-processing logical forms Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="104">Our Approach.</sample>
    <sample id="105">Our Approach.</sample>
    <sample id="106">Our Approach.</sample>
    <sample id="107">Our Approach.</sample>
    <sample id="108">Our Approach.</sample>
    <sample id="109">Permuting with "jumps" Permute the girl girl agent x₂ x₁ x₁ sleep the girl slept</sample>
    <sample id="110">Permuting with "jumps" Permute the girl girl agent x2 Tag the girl slept</sample>
    <sample id="111">Permuting with "jumps" Permute the girl sleep agent x₂ Tag the girl slept</sample>
    <sample id="112">Permuting with "jumps" Permute the girl x1 the girl girl x1 sleep agent x2 slept</sample>
    <sample id="113">Permuting with "jumps". The diagram shows a process with different elements. There are green boxes with symbols like *, girl, and ;. Blue boxes have words like sleep and agent. Yellow boxes have x1 and x2. There are arrows connecting these elements. The middle section is labeled "Permute" and the bottom section "Tag". The tags are the, girl, and slept.</sample>
    <sample id="114">Some Results on COGS, Kim and Linzen 2020, Comparison with other Treeless Models on Structural Generalization on COGS, Model, LSTM seq2seq, T5, Zheng and Lapata, Ours, Generalisation Type, PP recursion, CP recursion, Obj PP → Subj PP, Acc, 0, 20, 40, 60, 80, 7</sample>
    <sample id="115">Some Results on COGS, Kim and Linzen 2020, Comparison with other Treeless Models on Structural Generalization on COGS, Model, LSTM seq2seq, T5, Zheng and Lapata, Ours, Generalisation Type, PP recursion, CP recursion, Obj PP → Subj PP, Acc, 0, 20, 40, 60, 80, 7.</sample>
    <sample id="116">Technical Challenges We Solve.The alignment unknown.</sample>
    <sample id="117">Technical Challenges We Solve.The slide shows a flowchart with different elements labeled. At the top, there are symbols like "girl", "x1", "sleep", "agent", "x2", and "x1". Below that, there are two sections labeled "Permute" and "Tag". The "Permute" section has three boxes with question marks, and the "Tag" section has three boxes labeled "the", "girl", and "slept". At the bottom, it says "Alignment unknown.".</sample>
    <sample id="118">Technical Challenges We SolveAlignment unknown.Induce it in training.</sample>
    <sample id="119">Technical Challenges We SolveAlignment unknown.Induce it in training.Permutation model:Inference is NP-hard, TSP,</sample>
    <sample id="120">Technical Challenges We SolveAlignment unknown. Induce it in training.Permutation model: - Inference is NP-hard, TSP, - Backpropagate through continuous relaxation</sample>
    <sample id="121">Technical Challenges We SolveAlignment unknown. Induce it in training.Permutation model: - Inference is NP-hard, TSP, - Backpropagate through continuous relaxation</sample>
    <sample id="122">Well, the framework quantifies positionality by comparing annotations by demographic to models and datasets via Pearson's R scores. It's a way to measure how well the different annotations align with the models and datasets. If you want to know more about how it actually calculates those scores, just let me know.</sample>
    <sample id="123">Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu 1 Xiaoyu Shen 2 Marius Mosbach 1 Andreas Stephan 3 Dietrich Klakow 1 1 Saarland University 2 Amazon Alexa 3 University of Vienna 61 ACL 2023</sample>
    <sample id="124">Weaker Than You Think.A Critical Look at Weakly Supervised Learning.Dawei Zhu 1 Xiaoyu Shen 2 Marius Mosbach 1 Andreas Stephan 3 Dietrich Klakow 1 1 Saarland University 2 Amazon Alexa 3 University of Vienna ACL 2023</sample>
    <sample id="125">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning, WSL, Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="126">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning, WSL, Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="127">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning, WSL, Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="128">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning, WSL, Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="129">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning, WSL, Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="130">A common claim in recent WSL works is "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="131">A common claim in recent WSL works.We train models only on weakly supervised data and achieve an accuracy of XX%.Weakly labeled training data, noisy.Cleanly labeled test data, clean.</sample>
    <sample id="132">A common claim in recent WSL works is "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="133">A common claim in recent WSL works is that "We train models only on weakly supervised data and achieve an accuracy of XX%." The slide shows that the training data is weakly labeled and noisy, while the validation and test data are cleanly labeled.</sample>
    <sample id="134">Our research questions.RQ1 Is clean validation data necessary?RQ2 How many clean samples do WSL approaches need?RQ3 How to use the available clean samples more efficiently?</sample>
    <sample id="135">Our research questions.RQ1 Is clean validation data necessary?RQ2 How many clean samples do WSL approaches need?RQ3 How to use the available clean samples more efficiently?</sample>
    <sample id="136">RQ1 Main findings.</sample>
    <sample id="137">RQ1 Main findings</sample>
    <sample id="138">The slide is titled "Main findings" and is labeled as RQ1. It shows a graph with different validation methods on weak labels. There are three categories of validation methods: "Validation on Weak Labels" in orange, "No Validation, Random Selection" in purple, and "Validation on Clean Labels" in green. The x-axis represents different methods: FT_w, BOND, COSINE, MLC, and L2R. The y-axis shows the relative performance improvement over weak labels in percentage. The graph indicates that "Validation on Clean Labels" generally shows the highest improvement, while "No Validation, Random Selection" shows the lowest improvement.</sample>
    <sample id="139">RQ1 Main findings.</sample>
    <sample id="140">RQ1 Main findings A clean validation set is indispensable.</sample>
    <sample id="141">RQ2 Main findings</sample>
    <sample id="142">RQ2 Main findings.</sample>
    <sample id="143">RQ2 Main findings WSL approaches benefit from more clean validation samples!</sample>
    <sample id="144">RQ2 Main findings. WSL approaches benefit from more clean validation samples!</sample>
    <sample id="145">RQ2 Main findings WSL approaches benefit from more clean validation samples! But it is even better to use them for training, e.g., LoRAc!</sample>
    <sample id="146">RQ3 Main findings N=10 clean samples per class Accuracy/F1 88 86 84 82 80 78 76 Before CFT After CFT N=30 clean samples per class Accuracy/F1 88 86 84 82 80 78 Before CFT After CFT</sample>
    <sample id="147">RQ3 Main findings N=10 clean samples per class Accuracy/F1 88 86 84 82 80 78 76 Before CFT After CFT N=30 clean samples per class Accuracy/F1 88 86 84 82 80 78 Before CFT After CFT</sample>
    <sample id="148">RQ3 Main findings N=10 clean samples per class Accuracy/F1 88 86 84 82 80 78 76 Before CFT After CFT N=30 clean samples per class Accuracy/F1 88 86 84 82 80 78 Before CFT After CFT</sample>
    <sample id="149">RQ3 Main findings Continuous fine - tuning, CFT, eliminates performance gaps between WSL approaches. No need to use complicated WSL methods, FTw performs equally well.</sample>
    <sample id="150">Conclusion
Recent WSL approaches
- Require clean samples.
- Overestimate their practicality. 😕
Our recommendations
- Report the model selection criteria.
- Use Few-shot learning approaches as baselines.
- Always apply continuous fine - tuning, CFT, .</sample>
    <sample id="151">ConclusionRecent WSL approachesRequire clean samples.Overestimate their practicality.Our recommendationsReport the model selection criteria.Use Few-shot learning approaches as baselines.Always apply continuous fine - tuning, CFT.</sample>
    <sample id="152">Conclusion
Recent WSL approaches
- Require clean samples.
- Overestimate their practicality. 😕
Our recommendations
- Report the model selection criteria.
- Use Few-shot learning approaches as baselines.
- Always apply continuous fine-tuning, CFT.</sample>
    <sample id="153">ConclusionRecent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine - tuning, CFT, .</sample>
    <sample id="154">ConclusionRecent WSL approaches Require clean samples. Overestimate their practicality.Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine - tuning, CFT.</sample>
    <sample id="155">The slide doesn't give the specific finding of the previous study. You might need to look for more detailed information elsewhere. If you have any other questions about this or something else, feel free to ask.</sample>
    <sample id="156">The sources of data used in this study were an enhanced version of the Penn Treebank, Marcus et al. 1993, and Ficler and Goldberg 2016. If you have any other questions about this study, feel free to ask.</sample>
    <sample id="157">There are two authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="158">Well, some closely related tasks for cognitive dissonance are debate and CE. These tasks seem to be quite relevant to the topic. If you want to know more about how they're related, just let me know!</sample>
    <sample id="159">There are two authors involved in the paper. If you want to know more about the paper, feel free to ask.</sample>
    <sample id="160">There are six authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="161">The framework differs by comparing annotations by demographic to models and datasets via Pearson's R scores. This is not mentioned in the previous works. If you want to know more about this or have other questions, feel free to ask.</sample>
    <sample id="162">GPT-3.5.</sample>
    <sample id="163">The commercial systems compared were DeepL and Google.</sample>
    <sample id="200">There are six authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="201">MPP evaluations were performed up to 900 tokens context length. If you have any other questions about this, feel free to ask.</sample>
    <sample id="202">They included Music Selection, Book Selection, and Recipe Selection in their dataset. If you want to know more about how they were selected or anything else related, feel free to ask.</sample>
    <sample id="203">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences. And as a researcher, it influences the research process and its outcomes and results. If you want to know more about this, feel free to ask me.</sample>
    <sample id="204">Dawei Zhu.</sample>
    <sample id="205">Yes, it does. If you want to know more about how it adapts, just let me know.</sample>
    <sample id="206">There are four authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="207">Well, it seems that the model does work on the test suite. But, you know, it's not perfect. There are still some areas where it struggles. So, yeah, it does work, but there's room for improvement. What do you think about that?</sample>
    <sample id="208">There are Background - Pretrain, Background - Both, and Background - Inference. If you want to know more about these variants, feel free to ask.</sample>
    <sample id="209">The affiliations of the authors are Google Research. If you want to know more about their work or the paper itself, feel free to ask.</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?". If you have any other questions about this, feel free to let me know.</sample>
    <sample id="211">The metric sensitivity measures how sensitive the model is towards the variety of instructions for the same task. It looks at the model's ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions. If you want to know more about how it's calculated or other details, feel free to ask.</sample>
    <sample id="212">The speaker's name is not provided in the image.</sample>
    <sample id="213">It suggests the opposite. Lower sensitivity is better for model performance. If you have more questions about this or anything else, feel free to ask.</sample>
    <sample id="214">The image doesn't provide specific details about what kind of linguistic context models receive during pretraining. It just states that language model acceptability judgements are not always robust to context. You might need to look at other sources for that information. If you have any other questions or need more help, feel free to ask!</sample>
    <sample id="215">The image doesn't directly say how many clean validation samples are needed for good performance in WSL. It shows accuracy trends for different methods across various validation sample sizes, but it doesn't give a specific number for good performance. You might need to look at the method's documentation or more detailed research for that information. If you have any other questions about this or something else, feel free to ask.</sample>
    <sample id="216">The affiliations of the authors are Stanford Engineering Computer Science. If you want to know more about their research or anything else related, feel free to ask.</sample>
    <sample id="217">Well, you see, the existing methods might not be accurate enough. They could be missing some important factors or have biases themselves. So, developing new ones is crucial to get a more reliable and comprehensive understanding of media biases. What do you think about that?</sample>
    <sample id="218">I'm not sure. The image doesn't show the speaker's name. You could try looking for it in the video or presentation.</sample>
    <sample id="219">Well, it seems like the pipeline starts with pretraining data. Then, it goes through language models, and finally, it reaches downstream tasks. Hmm. I'm not sure if that's exactly what you were looking for, but it's a start. What do you think?</sample>
    <sample id="220">Yes, it does. If you want to know more about the differences, feel free to ask.</sample>
    <sample id="221">I'm not sure if Coscript is publicly available. You could try looking for official announcements or checking the company's website. If you find out, it'd be great if you could share it with me.</sample>
    <sample id="222">Well, first, you define a target embedding, e, t. Then, you count the trigger number in a sentence using a specific formula. After that, you add the target embedding to the original embedding. That's how the watermark is inserted. If you want to know more about any of these steps, just let me know.</sample>
    <sample id="223">The affiliations of the authors are PennState and Amazon. If you want to know more about their work or the paper itself, feel free to ask.</sample>
    <sample id="224">Yes, they can.</sample>
    <sample id="225">Well, an example of constrained language planning is when you're making a strawberry cake. You add strawberry jams into the flour. Another example is making a chocolate cake, where you add cocoa powder into the flour. These are simple examples of how constraints can be applied in language planning. So, what do you think about these examples? Do you have any other questions related to this?</sample>
    <sample id="226">They use embedding visualization to show the effectiveness of their method.</sample>
    <sample id="227">The work uses continual pre - training using existing pre - trained models like CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.</sample>
    <sample id="228">African Islamic.</sample>
    <sample id="229">The example sentence is "I am a student."</sample>
    <sample id="230">Well, from what I can see, as the number of tasks increases, the model performance generally improves. For example, when tasks like Image Understanding, Grounding, and so on are added, the performance goes up. But there's a point where adding more tasks doesn't really boost it much anymore. So, it seems that up to a certain point, more tasks are good for performance, but after that, it plateaus. What do you think about this? Do you have any other questions related to this?</sample>
    <sample id="231">LSTM seq2seq, T5, Zheng and Lapata. If you want to know more about these models or the comparison, feel free to ask.</sample>
    <sample id="232">I don't know the specific relation between the two co-authors and the first author. You could try looking at the paper for more details.</sample>
    <sample id="233">Chowdery.</sample>
    <sample id="274">The speaker mentions three problems of SimulST. If you want to know more about these problems, feel free to ask.</sample>
    <sample id="275">Well, one effective way is to sanitize the pretraining data. This means cleaning up the data to remove or correct any biases that might be present. It's a bit of a tricky question though, as there's no one-size-fits-all solution. You have to be really careful and consider different types of biases. So, what do you think about this? Do you have any other ideas or questions?</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA. If you want to know more about PaLM or other related topics, feel free to ask.</sample>
    <sample id="308">Well, the important properties of a watermarking method are that it should be applicable to EaaS, not degrade the utility of the provided embeddings, be covert to the attacker, and the watermark needs to be transferable to the attacker's services. So, what do you think about these properties? Do you have any other questions related to watermarking methods?</sample>
    <sample id="309">The 14 different languages are Arabic, Deutsch, Español, Français, Italiano, 日本語, 한국어, Nederlands, Português, Română, Русский, Türkçe, and 中文. If you have any other questions about TED talks or translations, feel free to ask!</sample>
    <sample id="310">300 instances are sampled from a dataset for reannotating. If you have any other questions about this, feel free to ask!</sample>
    <sample id="311">The cosine similarity and L2 distance are used.</sample>
    <sample id="312">The multilingual encoder-based models were used in two groups. One group was Enc-PTR, which used multilingual pretrained encoders with pointer-based decoders, like XLM-R + PTR and mBERT + PTR. The other group was Enc-Dec, which used multilingual pretrained encoder-decoder models, such as mBART and mT5. So, they were evaluated on two groups of models in a monolingual setting. If you want to know more about how they were specifically used in the evaluation, just let me know.</sample>
    <sample id="313">The 61st Annual Meeting of the Association for Computational Linguistics. Toronto, Canada. July 9-14, 2023. Distilling Script Knowledge from Large Language Models for Constrained Language Planning. Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang. Fudan University. Brain Technologies Inc.</sample>
    <sample id="314">Language Planning. How to Make a Cake? 1. Gather your ingredients. 2. Preheat the oven to 325 °F, 163 °C, and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add the eggs. 5. Stir in the cake flour. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models, LLMs, can effectively decompose goals into steps.</sample>
    <sample id="315">Language Planning. How to Make a Cake? 1. Gather your ingredients. 2. Preheat the oven to 325 °F, 163 °C, and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add the eggs. 5. Stir in the cake flour. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models, LLMs, can effectively decompose goals into steps.</sample>
    <sample id="316">Constrained Language Planning. How to Make a Strawberry Cake? Add strawberry jams into the flour. How to Make a Chocolate Cake? Add the cocoa powder into the flour. Abstract goal can be inherited by different real - life specific goals with multi - faceted constraints.</sample>
    <sample id="317">Constrained Language Planning.How to Make a Strawberry Cake? Add strawberry jams into the flour.How to Make a Chocolate Cake? Add the cocoa powder into the flour.Abstract goal can be inherited by different real - life specific goals with multi - faceted constraints.If you want to know more about this topic, feel free to ask me.</sample>
    <sample id="318">Constrained Language Planning. How to Make a Strawberry Cake? Add strawberry jams into the flour. How to Make a Chocolate Cake? Add the cocoa powder into the flour. Abstract goal can be inherited by different real - life specific goals with multi - faceted constraints.</sample>
    <sample id="319">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated ConstraintsConstraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex.1: Make a chocolate cake. Ex.2: Make a pink cake.Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex.1: Make a cake with an oven. Ex.2: Make a cake by using cake mix.Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex.1: Make a cake for wedding. Ex.2: Make a cake for diabetics.</sample>
    <sample id="320">How do LLMs perform on Constrained Language Planning?Dataset: wikiHow + Generated ConstraintsConstraint Type 1: ModifierDefinition: A word, an adjective or a phrase that modifies or constrains an abstract goal.Ex.1: Make a chocolate cake.Ex.2: Make a pink cake.Constraint Type 2: MethodDefinition: A tool or specified mode that controls the process for achieving the goal.Ex.1: Make a cake with an oven.Ex.2: Make a cake by using cake mix.Constraint Type 3: IntentDefinition: An additional purpose or demand when completing the goal.Ex.1: Make a cake for wedding.Ex.2: Make a cake for diabetics.If you want to know more about this or have other questions, feel free to ask.</sample>
    <sample id="321">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A word, an adjective or a phrase that modifies or constrains an abstract goal. Ex.1: Make a chocolate cake. Ex.2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process for achieving the goal. Ex.1: Make a cake with an oven. Ex.2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex.1: Make a cake for wedding. Ex.2: Make a cake for diabetics.</sample>
    <sample id="322">Can LLMs do Constrained Language Planning?All baselines achieve unsatisfactory results on planning for specific goals</sample>
    <sample id="323">Can LLMs do Constrained Language Planning?All baselines achieve unsatisfactory results on planning for specific goals</sample>
    <sample id="324">The semantic completeness, SE, in generated scripts is acceptable, but the faithfulness to the constraints, FE, can not be guaranteed.</sample>
    <sample id="325">The semantic completeness, SE, in generated scripts is acceptable, but the faithfulness to the constraints, FE, can not be guaranteed.</sample>
    <sample id="326">The planning performance of InstructGPTs varies considerably for goals of different categories.</sample>
    <sample id="327">The content is about a method for generating specific goals from an abstract goal. It starts with an input of an abstract goal, like "Make a cake." Then, it uses InstructGPT for in-context learning to generate specific goals. There are three specific goals shown: G1 is to make a chocolate cake, G2 is to make a cake in a microwave, and G3 is to make a cake for a wedding.</sample>
    <sample id="328">The content is: "Method. Input: an abstract goal. Step 1: Generate specific goals with InstructGPT via in-context learning. Abstract Goal: Make a cake. Specific Goals: G1, modifier, Make a chocolate cake. G2, method, Make a cake in a microwave. G3, intent, Make a cake for a wedding. Step 2: Over-generate candidate scripts with InstructGPT via in-context learning. Candidate Scripts: 1, 2, 3, ... k."</sample>
    <sample id="329">The image shows a slide titled "Method". It explains a process for achieving an abstract goal of making a cake. The first step is to input an abstract goal and generate specific goals using InstructGPT via in - context learning. The second step is to over - generate candidate scripts with InstructGPT via in - context learning. The abstract goal is "Make a cake". Specific goals include "Make a chocolate cake", "Make a cake in a microwave", and "Make a cake for a wedding". There are candidate scripts numbered 1 to k for the specific goal "Make a chocolate cake".</sample>
    <sample id="330">Method. Step 2: Over - generate candidate scripts with InstructGPT via in - context learning. Step 3: Find filtered scripts to the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts.</sample>
    <sample id="331">Method. Step 2: Over - generate candidate scripts with InstructGPT via in - context learning. Step 3: Find filtered scripts to the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts. Candidate Scripts: 1, 2, 3, ..., k. 2. 0.3 0.2 0.5 3. 0.7 0.2 0.1. Filtered Scripts: Script 3. 1. Gather your ingredients. 4. Add the cocoa powder.</sample>
    <sample id="332">Method. Step 2: Over - generate candidate scripts with InstructGPT via in - context learning. Step 3: Find filtered scripts to the goal with InstructGPT via similarity score. Output: Specific goals with corresponding scripts. Candidate Scripts: 1, 2, 3, ..., k. 2 similarity score 0.3 0.2 0.5 0.7 0.2 0.1. Filtered Scripts: Script 3. 1. Gather your ingredients. 4. Add the cocoa powder.</sample>
    <sample id="333">Our Method Greatly Improves the Planning Quality.With our method, InstructGPT can generate scripts of higher quality by a large margin.</sample>
    <sample id="334">Script Distillation from LLMs.Motivation: To enable constrained language planning ability for smaller models.Method: Follow the idea of symbolic knowledge distillation. Generated 55, 000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset. Humans annotate validation and test set.Input: an abstract. Step 1: Generate specific goals with InstructGPT via in-context learning. Step 2: Over-generate candidate scripts with InstructGPT via in-context learning. Step 3: Find filtered scripts to the goal with InstructGPT via similarity score. Output: Specific goals with corresponding plans.</sample>
    <sample id="335">Script Distillation from LLMs.Motivation: To enable constrained language planning ability for smaller models.Method: Follow the idea of symbolic knowledge distillation. Generated 55，000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset. Humans annotate validation and test set.Input: an abstract. Step 1: Generate specific goals with InstructGPT via in-context learning. Step 2: Over-generate candidate scripts with InstructGPT via in-context learning. Step 3: Find filtered scripts to the goal with InstructGPT via similarity score. Output: Specific goals with corresponding plans.</sample>
    <sample id="336">Script Distillation from LLMs.Motivation: To enable constrained language planning ability for smaller models.Method: Follow the idea of symbolic knowledge distillation. Generated 55, 000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset. Humans annotate validation and test set.Input: an abstract.Step 1: Generate specific goals with InstructGPT via in - context learning.Step 2: Over - generate candidate scripts with InstructGPT via in - context learning.Step 3: Find filtered scripts to the goal with InstructGPT via similarity score.Output: Specific goals with corresponding plans.</sample>
    <sample id="337">Script Distillation from LLMs.Motivation: To enable constrained language planning ability for smaller models.Method: Follow the idea of symbolic knowledge distillation. Generated 55, 000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset. Humans annotate validation and test set.Input: an abstract.Step 1: Generate specific goals with InstructGPT via in-context learning.Step 2: Over-generate candidate scripts with InstructGPT via in-context learning.Step 3: Find filtered scripts to the goal with InstructGPT via similarity score.Output: Specific goals with corresponding plans.</sample>
    <sample id="338">Script Distillation from LLMs.Motivation: To enable constrained language planning ability for smaller models.Method: Follow the idea of symbolic knowledge distillation. Generated 55, 000 Scripts with constraint from LLMs based on our method =&gt; Coscript Dataset. Humans annotate validation and test set.Input: an abstract. Step 1: Generate specific goals with InstructGPT via in-context learning. Step 2: Over-generate candidate scripts with InstructGPT via in-context learning. Step 3: Find filtered scripts to the goal with InstructGPT via similarity score. Output: Specific goals with corresponding plans.</sample>
    <sample id="339">Constraint Analysis shows high heterogeneity and pluralism in the generated specific goals. Coscript for Smaller Language Models. Datasets: Coscript, wikiHow. Metrics: Faithful: DeBERTa, v3 large, model to decide whether the generated texts are faithful to the constraints. Automatic metrics: ROUGE, BLEU, BERTScore.</sample>
    <sample id="340">Specialized Models vs. LLMs. Smaller LMs fine - tuned on Coscript can generate higher quality scripts than LLMs.</sample>
    <sample id="341">Summary and Takeaways.- Establish the constrained language planning problem.- Evaluate constrained language planning ability of LLMs and develop an over - generate - then - filter method for LLMs.- Use LLMs to generate a high - quality script dataset, CoScript, for constrained language planning.- Limitations and future work.- The proposed method for improving LLMs is a post - hoc re - ranking approach.- CoScript only inherits from an abstract one with one extra constraint.- CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="342">Summary and Takeaways.- Establish the constrained language planning problem.- Evaluate constrained language planning ability of LLMs and develop an over - generate - then - filter method for LLMs.- Use LLMs to generate a high - quality script dataset, CoScript, for constrained language planning.- Limitations and future work.- The proposed method for improving LLMs is a post - hoc re - ranking approach.- CoScript only inherits from an abstract one with one extra constraint.- CoScript dataset can be a valuable resource to advance the research on language planning with more complex and diverse goals and constraints.</sample>
    <sample id="343">The 61st Annual Meeting of the Association for Computational Linguistics. Toronto, Canada. July 9-14, 2023. Distilling Script Knowledge from Large Language Models for Constrained Language Planning. Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang. syyuan21@m.fudan.edu.cn. https://github.com/siyuyuan/coscript.</sample>
    <sample id="344">The authors count the word frequency on a general text corpus Dp and then randomly select n words in a moderate-frequency interval.</sample>
    <sample id="371">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. Sarah E. Finch, James D. Finch, and Jinho D. Choi. Emory University. Emory NLP Research Lab. Alexa.</sample>
    <sample id="372">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. Sarah E. Finch, James D. Finch, and Jinho D. Choi. Emory University. Emory NLP Research Lab. Alexa.</sample>
    <sample id="373">Comparative Evaluation.</sample>
    <sample id="374">Comparative Evaluation Likert Rating Evaluation</sample>
    <sample id="375">Likert Rating Evaluation. Dimensions of Dialogue Quality. Dialogue Quality. Relevance. Emotional Understanding. Consistency.</sample>
    <sample id="376">Dimensions of Dialogue Quality Likert Rating Evaluation Rate the relevance of the bot's responses</sample>
    <sample id="377">Likert Rating Evaluation. Rate the relevance of the bot's responses.</sample>
    <sample id="378">Annotating Behaviors in Chat, ABC-Eval. Emory University, NLP, Alexa.</sample>
    <sample id="379">Annotating Behaviors in Chat, ABC-Eval. There are four speech bubbles from a human and three from a robot. The first human speech bubble is irrelevant. The first robot speech bubble is lack of empathy. The second human speech bubble is irrelevant. The second robot speech bubble is self-contradiction.</sample>
    <sample id="380">ABC - Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="381">ABC - Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Consistency Emotional Understanding</sample>
    <sample id="382">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Knowledge Consistency Incorrect Fact Commonsense Violation Emotional Understanding Self Contradiction Partner Contradiction Empathetic Response Lack of Empathy</sample>
    <sample id="383">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Incorrect Fact Commonsense Violation Consistency Self Contradiction Partner Contradiction Emotional Understanding Empathetic Response Lack of Empathy Experiments 4 Open-Domain Dialogue Models 100 Human-Bot Conversations per Model</sample>
    <sample id="384">Experiments. 4 Open-Domain Dialogue Models. 100 Human-Bot Conversations per Model. ABC-Eval. TURN Likert. DIALOGUE Likert. COMPARATIVE.</sample>
    <sample id="385">Experiments. 4 Open-Domain Dialogue Models. 100 Human-Bot Conversations per Model. ABC-Eval. Turn Likert. Dialogue Likert. Comparative. Baseline Evaluations. Turn Likert. Dialogue Likert. Comparative. Consistency. Emotional Understanding. Informativeness. Overall Quality. Engagingness. Grammaticality. Proactivity. Relevance.</sample>
    <sample id="386">Inter-Annotator Agreement.</sample>
    <sample id="387">Sure.</sample>
    <sample id="388">Predictive Validity.</sample>
    <sample id="389">Predictive Validity.Incremental Validity.</sample>
    <sample id="390">Incremental Validity.</sample>
    <sample id="391">Incremental Validity.</sample>
    <sample id="392">Sure. The text content is: "ABC-Eval Error Rates by Model".</sample>
    <sample id="393">Sure. The text content is: 'ABC-Eval Error Rates by Model'.</sample>
    <sample id="394">Sure. The text content is: "ABC-Eval Error Rates by Model".</sample>
    <sample id="395">Sure. The text content is: 'ABC-Eval Error Rates by Model'.</sample>
    <sample id="396">ABC-Eval Error Rates by Model. The models are BART-FID-RAG, Blender2, Emora, and Blender-Decode. The error rates for different categories like Antisocial, CS Contra., Ignore, Incorrect, Irrelevant, Unempathetic, Other Contra., Redundant, Self Contra., Topic Switch, and Uninterpret. are shown. For example, BART-FID-RAG has a high error rate in the "Topic Switch" category. The Emory University and Alexa logos are present. The paper link is arxiv.org/pdf/2212.09180.pdf. The GitHub link is github.com/emorynlp/ChatEvaluationPlatform. Contact info is sfillwo, jdfinch, jinho.choi at emory.edu. The Emory NLP website is www.emorynlp.org.</sample>
    <sample id="397">The approach uses a speech segment size of 10 seconds. If you have any other questions about this, feel free to ask.</sample>
    <sample id="398">In the example with Servin and Kea, the entity-specific knowledge needed is that Servin is a judge.</sample>
    <sample id="399">Example quality is more important than similarity to the source sentence. If you have any other questions about this, feel free to ask.</sample>
    <sample id="400">The paper focuses on RoBERTa and GPT-2. If you want to know more about these models or the paper itself, feel free to ask.</sample>
    <sample id="401">The model combines the scores from several layers. If you want to know more about how exactly it does that, feel free to ask.</sample>
    <sample id="402">The examples of direct inference are "easy on me" and "the first one". If you want to know more about this or have other questions, feel free to ask.</sample>
    <sample id="403">The affiliations are Fudan University and Brain Technologies Inc. If you want to know more about these institutions or the paper, feel free to ask.</sample>
    <sample id="404">There are six authors involved in the paper. If you want to know more about them or the paper itself, feel free to ask.</sample>
    <sample id="405">No.</sample>
    <sample id="406">A woman warrior.</sample>
    <sample id="407">The model architectures that don't generalize well are not explicitly mentioned in the image. But it's implied that non-transformer models might not generalize as well. If you want to know more about specific models, you can ask me.</sample>
    <sample id="408">The testing datasets are FTc, LoRAc, BitFitc, and Adapterc. If you have any other questions about this, feel free to ask!</sample>
    <sample id="409">There are six authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="410">The author works with multiple modalities. If you have any other questions about it, feel free to ask.</sample>
    <sample id="411">DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains.Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, Pierre-Antoine Gourraud.LIA, Avignon Université, LS2N, Nantes Université, Clinique des données, CHU de Nantes, Zenidoc.LS2N, Nantes Université, Centre Hospitalier Universitaire de Nantes, iD, i2, GENCI, Le calcul intensif au service de la connaissance, Avignon Université.</sample>
    <sample id="412">Summary.I. Language Modeling in Healthcare.II. Comparison of pre-training strategies, data sources and sizes.III. Evaluation of 13 models on 11 tasks.IV. Distribution of NACHOS and DrBERT.</sample>
    <sample id="413">Summary.I. Language Modeling in Healthcare.II. Comparison of pre-training strategies, data sources and sizes.III. Evaluation of 13 models on 11 tasks.IV. Distribution of NACHOS and DrBERT.</sample>
    <sample id="414">Summary.I. Language Modeling in Healthcare.II. Comparison of pre-training strategies, data sources and sizes.III. Evaluation of 13 models on 11 tasks.IV. Distribution of NACHOS and DrBERT.</sample>
    <sample id="415">Summary.I. Language Modeling in Healthcare. Transformer - based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT.II. Comparison of pre - training strategies, data sources and sizes. PubMedBERT, BioBERT, ClinicalBERT and other.III. Evaluation of 13 models on 11 tasks. Languages other than English are rarer and rely primarily on continual pre - training using an existing generic model.IV. Distribution of NACHOS and DrBERT. Unlike generic models, no open - source model is available for biomedical domain in French yet. BERT - based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="416">Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other. Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="417">Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other. Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model. Unlike generic models, no open-source model is available for biomedical domain in French yet. BERT-based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="418">Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain-specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other. Languages other than English are rarer and rely primarily on continual pre - training using an existing generic model. Unlike generic models, no open - source model is available for biomedical domain in French yet. BERT - based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="419">Transformer - based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. It has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain - specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other. Languages other than English are rarer and rely primarily on continual pre - training using an existing generic model. Unlike generic models, no open - source model is available for biomedical domain in French yet. BERT - based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="420">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.Model name Strategy Corpus.DrBERT From scratch NACHOSlarge.DrBERT From scratch NACHOSsmall.ChuBERT From scratch NBDWsmall.ChuBERT From scratch NBDWmixed.CamemBERT continual pre - training NACHOSsmall.PubMedBERT continual pre - training NACHOSsmall.CamemBERT continual pre - training NBDWsmall.Avinon Université.</sample>
    <sample id="421">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.Model name Strategy CorpusDrBERT From scratch NACHOSlargeDrBERT From scratch NACHOSsmallChuBERT From scratch NBDWsmallChuBERT From scratch NBDWmixedCamemBERT continual pre - training NACHOSsmallPubMedBERT continual pre - training NACHOSsmallCamemBERT continual pre - training NBDWsmall</sample>
    <sample id="422">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.Table:Corpus Size #words #sentences.NACHOSlarge, pub., 7.4 GB 1.1 B 54.2 M.NACHOSsmall, pub., 4 GB 646 M 25.3 M.NBDWsmall, private, 4 GB 655 M 43.1 M.NBDWmixed, both, 4 + 4 GB 1.3 B 68.4 M.Model name Strategy Corpus.DrBERT From scratch NACHOSlarge.DrBERT From scratch NACHOSsmall.ChuBERT From scratch NBD</sample>
    <sample id="423">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.Table:Corpus Size #words #sentences.NACHOSlarge, pub., 7.4 GB 1.1 B 54.2 M.NACHOSsmall, pub., 4 GB 646 M 25.3 M.NBDWsmall, private, 4 GB 655 M 43.1 M.NBDWmixed, both, 4+4 GB 1.3 B 68.4 M.Model name Strategy Corpus.DrBERT From scratch NACHOSlarge.DrBERT From scratch NACHOSsmall.ChuBERT From scratch NBDW</sample>
    <sample id="424">Comparison of pre - training strategies and data sources Evaluation of the impact of public and private medical data sources on comparable data sizes NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Comparison of learning strategies From scratch with full model construction Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.</sample>
    <sample id="425">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.DrBERT: From scratch, NACHOSlarge.DrBERT: From scratch, NACHOSsmall.ChuBERT: From scratch, NBDWsmall.ChuBERT: From scratch, NBDWmixed.CamemBERT: continual pre - training, NACHOSsmall.PubMedBERT: continual pre - training, NACHOSsmall.CamemBERT: continual pre - training, NBDWsmall.</sample>
    <sample id="426">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.Model name Strategy CorpusDrBERT From scratch NACHOSlargeDrBERT From scratch NACHOSsmallChuBERT From scratch NBDWsmallChuBERT From scratch NBDWmixedCamemBERT continual pre - training NACHOSsmallPubMedBERT continual pre - training NACHOSsmallCamemBERT continual pre - training NBDWsmall</sample>
    <sample id="427">Comparison of pre - training strategies and data sources.Evaluation of the impact of public and private medical data sources on comparable data sizes.NACHOS: A 1.1B words open - source dataset of heterogeneous data crawled from diverse medical domains, natures and styles.NBDW: A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse.Comparison of learning strategies.From scratch with full model construction.Continual pre - training using an existing pre - trained model, here, CamemBERT, a French generic model, and PubMedBERT, an English - based medical one.Evaluation : Data sources and size.Performance evaluation of 13 models on 11 tasks, both public and private.Our fine - tuned models get state - of - the - art results on almost all tasks.</sample>
    <sample id="428">Evaluation : Data sources and sizePerformance evaluation of 13 models on 11 tasks, both public and privateOur fine - tuned models get state - of - the - art results on almost all tasks</sample>
    <sample id="429">Evaluation : Data sources and sizePerformance evaluation of 13 models on 11 tasks, both public and privateOur fine - tuned models get state - of - the - art results on almost all tasks</sample>
    <sample id="430">Evaluation : Data sources and sizePerformance evaluation of 13 models on 11 tasks, both public and privateOur fine - tuned models get state - of - the - art results on almost all tasks</sample>
    <sample id="431">Evaluation : Data sources and sizePerformance evaluation of 13 models on 11 tasks, both public and privateOur fine - tuned models get state - of - the - art results on almost all tasks.F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1GeneralCamemBERT OSCAR 138 GB 35.13 80.13 89.35 99.20 88.54 88.20 81.10 95.22 36.24 16.55 90.71 77.41CamemBERT OSCAR 4 GB 42.66 81.41 88.78 99.61 85.43 91.27 83.69 96.42 35.75 15.37 90.83 78.76CamemBERT CCNET 4 GB 43.11 79.98 89.34 9</sample>
    <sample id="432">Evaluation : Pre - training strategies.From scratch vs. continual pre - training on 4GB of data.Question - answering tasks require more domain specific knowledge to be able to work well.A study of model stability shows a higher inter - run variability for the CamemBERT - based models trained using continual pretraining.CamemBERT OSCAR 138 GB NER F1 35.13 80.13 CLS F1 89.35 Specialties F1 99.20 MUSCA - DET NER F1 88.54 MUSCA - DET CLS F1 88.20 ESSAI POS F1 81.10 POS F1 95.22 CAS Hamming F1 36.24 EMR F1 16.55 90.71 77.41 CamemBERT OSCAR 4 GB NER F1 42.66 81.41 CLS F1 88.78 Specialties F1 99.61 MUSCA - DET NER F1 85.43 MUSCA - DET CL</sample>
    <sample id="433">Evaluation : Pre-training strategies.- From scratch vs. continual pre-training on 4GB of data.- Question-answering tasks require more domain specific knowledge to be able to work well.- A study of model stability shows a higher inter-run variability for the CamemBERT-based models trained using continual pretraining.</sample>
    <sample id="434">Evaluation : Pre - training strategies.From scratch vs. continual pre - training on 4GB of data.Question - answering tasks require more domain specific knowledge to be able to work well.A study of model stability shows a higher inter - run variability for the CamemBERT - based models trained using continual pretraining.CamemBERT OSCAR 138 GB NER F1 35.13 80.13 CLS F1 89.35 Specialties F1 99.20 MUSCA - DET NER F1 88.54 MUSCA - DET CLS F1 88.20 ESSAI POS F1 81.10 POS F1 95.22 CAS Hamming F1 36.24 EMR F1 16.55 90.71 77.41 CamemBERT OSCAR 4 GB NER F1 42.66 81.41 CLS F1 88.78 Specialties F1 99.61 MUSCA - DET NER F1 85.43 MUSCA - DET CL</sample>
    <sample id="435">DrBERT achieves state - of - the - art results in 9 downstream French medical - oriented tasks. It surpasses CamemBERT generic model and English - based domain - specific models. It also confirms the utility of training a medical - specific model in French.Data sources matter. Training on heterogeneous data is important. NACHOS is more robust than using private clinical data only. More data is better, but does not scale well. Continual pretraining is a more effective strategy when based on domain - specific English models. The DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.</sample>
    <sample id="436">DrBERT achieves state - of - the - art results in 9 downstream French medical - oriented tasks. Surpasses CamemBERT generic model and English - based domain - specific models. Confirms utility of training a medical - specific model in French.Data sources matters: training on heterogeneous data is important. NACHOS is more robust than using private clinical data only. More data is better, but does not scale well. Continual pretraining is a more effective strategy when based on domain - specific English models. The DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.</sample>
    <sample id="437">DrBERT achieves state - of - the - art results in 9 downstream French medical - oriented tasks. It surpasses CamemBERT generic model and English - based domain - specific models. It also confirms the utility of training a medical - specific model in French.Data sources matter. Training on heterogeneous data is important. NACHOS is more robust than using private clinical data only. More data is better, but does not scale well. Continual pretraining is a more effective strategy when based on domain - specific English models. The DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.</sample>
    <sample id="438">Thank You. Looking forward to exchange at poster session in Toronto! More information on: drbert.univ-avignon.fr</sample>
    <sample id="439">The authors claim that reasoning is an understudied area in NLU. If you want to know more about this, feel free to ask.</sample>
    <sample id="440">The names of the speakers are Zhiyang Xu, Ying Shen, and Lifu Huang. If you want to know more about them, feel free to ask.</sample>
    <sample id="441">Yes, humans annotated the validation and test set. So, it did undergo quality checks. If you have any other questions about this, feel free to ask.</sample>
    <sample id="442">Well, it seems that existing resources for context-dependent translation have a few limits. Firstly, only a small portion of words depend on context, and corpus - level metrics are used. Also, existing methods support limited discourse phenomena and languages. So, there's still a long way to go in this area. If you want to know more about how to improve these resources, just let me know.</sample>
    <sample id="473">The approach is compared to wait-k, LA, CAAT, and EDAtt. If you have any other questions about this, feel free to ask.</sample>
    <sample id="474">Yanis Labrak is affiliated with LIA, Avignon Université. Adrien Bazoge is with LS2N, Nantes Université. Richard Dufour is also at LS2N, Nantes Université. Mickael Rouvier is at LIA, Avignon Université. Emmanuel Morin is at LS2N, Nantes Université. Béatrice Daille is at Clinique des données, CHU de Nantes. Pierre-Antoine Gourraud is at Zenidoc.</sample>
    <sample id="475">I'm not sure. There's no name mentioned in the image. You could try looking for any text that might contain the speaker's name.</sample>
    <sample id="476">There are three authors involved in the paper. If you have any other questions about it, feel free to ask.</sample>
    <sample id="505">Yes, the dataset is publicly available. If you have any other questions about it, feel free to ask.</sample>
    <sample id="535">The authors are affiliated with Università di Trento and Fondazione Bruno Kessler. If you want to know more about these institutions, feel free to ask.</sample>
    <sample id="536">The speaker's name is Mohammad Javad Hosseini. If you want to know more about him or the topic, feel free to ask.</sample>
    <sample id="537">Google Prompting PaLM for Translation Assessing Strategies and Performance ACL 2023</sample>
    <sample id="538">PaLM: Pathways Language Model. Chowdery et al., 2022. arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks.</sample>
    <sample id="539">PaLM: Pathways Language Model.- Chowdery et al., 2022 arXiv:2204.02311.- 540B parameters.- Trained on 780B tokens.- Densely activated.- 6144 TPU v4 chips.- SOTA in hundreds of LMU and Generation benchmarks.</sample>
    <sample id="540">Our contribution.First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy.Evaluate translation capabilities with best practices of the MT community:.Latest test sets avoid test train overlap and overfitting on evaluation data.Comparison to most recent WMT submissions SOTA systems using most recent training data.SOTA MT metrics better correlation with human judgements.Expert based human evaluation more robust than crowd workers.Recommendation for prompt selection strategies.If you have any other questions or need more details, feel free to ask.</sample>
    <sample id="541">Our contribution.First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy.Evaluate translation capabilities with best practices of the MT community:.Latest test sets avoid test train overlap and overfitting on evaluation data.Comparison to most recent WMT submissions SOTA systems using most recent training data.SOTA MT metrics better correlation with human judgements.Expert based human evaluation more robust than crowd workers.Recommendation for prompt selection strategies.</sample>
    <sample id="542">Our contribution.First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy.Evaluate translation capabilities with best practices of the MT community:.Latest test sets avoid test train overlap and overfitting on evaluation data.Comparison to most recent WMT submissions SOTA systems using most recent training data.SOTA MT metrics better correlation with human judgements.Expert based human evaluation more robust than crowd workers.Recommendation for prompt selection strategies.If you have any other questions or need more details, feel free to ask.</sample>
    <sample id="543">Our contribution.First systematic study of LLM prompting for MT. Both for the candidate pool as well as selection strategy.Evaluate translation capabilities with best practices of the MT community:.Latest test sets avoid test train overlap and overfitting on evaluation data.Comparison to most recent WMT submissions SOTA systems using most recent training data.SOTA MT metrics better correlation with human judgements.Expert based human evaluation more robust than crowd workers.Recommendation for prompt selection strategies.</sample>
    <sample id="544">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence - prompt pair. The majority of sentences 516 out of 1000 show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="545">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence - prompt pair. The majority of sentences 516 out of 1000 show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="546">Prompts have a big impact on translation quality. Select two random prompts for each sentence. Compute BLEURT for each sentence - prompt pair. The majority of sentences 516 out of 1000 show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="547">A passerby alerted the police, who arrived with several officers.</sample>
    <sample id="548">A passerby alerted the police, who arrived with several officers.</sample>
    <sample id="549">A passerby alerted the police, who arrived with several officers.</sample>
    <sample id="550">A passerby alerted the police, who arrived with several officers.</sample>
    <sample id="551">A bystander alerted the police, who arrived with several officers.</sample>
    <sample id="552">Experimental Results.- Example quality is more important than similarity to source sentence.- Specialized SOTA systems have a substantial advantage.- PaLM close to Google Translate.Insights from MQM:.- Fluency of PaLM comparable to SOTA.- Accuracy scores generally lower. o Dominated by "Accuracy/Omission"- "Style/Awkward" generally lower for PaLM.</sample>
    <sample id="553">Experimental Results.- Example quality is more important than similarity to source sentence.- Specialized SOTA systems have a substantial advantage.- PaLM close to Google Translate.Insights from MQM:.- Fluency of PaLM comparable to SOTA.- Accuracy scores generally lower. o Dominated by "Accuracy/Omission"- "Style/Awkward" generally lower for PaLM.</sample>
    <sample id="554">Experimental Results.- Example quality is more important than similarity to source sentence.- Specialized SOTA systems have a substantial advantage.- PaLM close to Google Translate.Insights from MQM:.- Fluency of PaLM comparable to SOTA.- Accuracy scores generally lower. o Dominated by "Accuracy/Omission"- "Style/Awkward" generally lower for PaLM.</sample>
    <sample id="555">Experimental Results. Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM. Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by Accuracy/Omission. Style/Awkward generally lower for PaLM.</sample>
    <sample id="556">Experimental Results. Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM. Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by Accuracy/Omission. Style/Awkward generally lower for PaLM.</sample>
    <sample id="557">Experimental Results.- Example quality is more important than similarity to source sentence.- Specialized SOTA systems have a substantial advantage.- PaLM close to Google Translate.Insights from MQM:.- Fluency of PaLM comparable to SOTA.- Accuracy scores generally lower. o Dominated by "Accuracy/Omission"- "Style/Awkward" generally lower for PaLM.</sample>
    <sample id="558">Experimental Results. Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM. Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by Accuracy/Omission. Style/Awkward generally lower for PaLM.</sample>
    <sample id="559">Experimental Results.- Example quality is more important than similarity to source sentence.- Specialized SOTA systems have a substantial advantage.- PaLM close to Google Translate.Insights from MQM:.- Fluency of PaLM comparable to SOTA.- Accuracy scores generally lower. o Dominated by "Accuracy/Omission"- "Style/Awkward" generally lower for PaLM.</sample>
    <sample id="560">Experimental Results.- Example quality is more important than similarity to source sentence.- Specialized SOTA systems have a substantial advantage.- PaLM close to Google Translate.Insights from MQM:.- Fluency of PaLM comparable to SOTA.- Accuracy scores generally lower. o Dominated by "Accuracy/Omission"- "Style/Awkward" generally lower for PaLM.</sample>
    <sample id="561">The transcription of the English content is: "thank you".</sample>
    <sample id="597">The first step maps the input tokens to a set of tags. If you have more questions about this or anything else, feel free to ask.</sample>
    <sample id="598">55, 000.</sample>
    <sample id="599">The KITMUS Test Evaluating Knowledge Integration from Multiple Sources</sample>
    <sample id="600">NLU models draw on multiple knowledge sources. Knowledge in Parameters, pretrain-time knowledge. Knowledge in Context, inference-time knowledge.</sample>
    <sample id="601">NLU models draw on multiple knowledge sources. Knowledge in Parameters, pretrain-time knowledge. Knowledge in Context, inference-time knowledge.</sample>
    <sample id="602">John saw the newly elected president on TV.What presidents do. What is a TV.Who is John. Who is the new president.</sample>
    <sample id="603">John saw the newly elected president on TV.What presidents do. What is a TV.Who is John. Who is the new president.</sample>
    <sample id="604">John saw the newly elected president on TV.What presidents do. What is a TV.Who is John. Who is the new president.</sample>
    <sample id="605">John saw the newly elected president on TV.What presidents do What is a TV Who is John Who is the new president</sample>
    <sample id="606">KITMUS Test Suite.Dataset for knowledge integration evaluation.Coreference resolution task to probe ability to draw on.pretrain - time knowledge.inference - time knowledge.Experiment with.human study participants.coreference resolution models.</sample>
    <sample id="607">KITMUS Test Suite.- Dataset for knowledge integration evaluation.- Coreference resolution task to probe ability to draw on pretrain-time knowledge inference-time knowledge.- Experiment with human study participants coreference resolution models.</sample>
    <sample id="608">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. Answer: Servin</sample>
    <sample id="610">KITMUS Test Suite.Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. Answer: Servin.Entity-specific knowledge.Judges decide cases in courts of law.</sample>
    <sample id="611">KITMUS Test Suite.Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. Answer: Servin.Judges decide cases in courts of law.Entity-specific knowledge.Background knowledge.Inference-time knowledge.Pretrain-time knowledge.</sample>
    <sample id="612">KITMUS Test Suite.Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. Answer: Servin.Judges decide cases in courts of law.Entity-specific knowledge.Background knowledge.Inference-time knowledge.Pretrain-time knowledge.</sample>
    <sample id="613">Variants of KITMUS, Background-Pretrain: Typical setup, Background-Both: Explicitly provide background knowledge in context, Background-Inference: Knowledge only available at inference-time.</sample>
    <sample id="614">Variants of KITMUS, Background-Pretrain: Typical setup, Background-Both: Explicitly provide background knowledge in context, Background-Inference: Knowledge only available at inference-time.</sample>
    <sample id="615">Variants of KITMUS, Background-Pretrain: Typical setup, Background-Both: Explicitly provide background knowledge in context, Background-Inference: Knowledge only available at inference-time.</sample>
    <sample id="616">Variants of KITMUS. Background - Pretrain: Politicians seek elected seats in government. Background - Both: Chichester is a politician. Politicians seek elected seats in government. Background - Inference: Chichester is a politician. The work of a politician is seeking an elected seat in government. Chichester is a mirituer. The work of a mirituer is ehemting smorblt.</sample>
    <sample id="617">Variants of KITMUS. Background - Pretrain: Politicians seek elected seats in government. Background - Both: Chichester is a politician. Politicians seek elected seats in government. Background - Inference: Chichester is a politician. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="618">Variants of KITMUS. Background-Pretrain: Politicians seek elected seats in government. Background-Both: Chichester is a politician. Politicians seek elected seats in government. Background-Inference: Chichester is a politician. The work of a politician is seeking an elected seat in government. Chichester is a mirituer. The work of a mirituer is ehemting smorbly.</sample>
    <sample id="619">Variants of KITMUS. Background - Pretrain: Politicians seek elected seats in government. Background - Both: Chichester is a politician. Politicians seek elected seats in government. Background - Inference: Chichester is a politician. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="620">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="621">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="622">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="623">Background - Inference. Models struggle to integrate inference - time background knowledge.</sample>
    <sample id="624">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources, pretrain-time and inference-time knowledge. 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at mpoemsl/kitmus</sample>
    <sample id="625">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources, pretrain-time and inference-time knowledge. 2. Task-specific training is necessary for knowledge integration. 3. Models struggle to integrate inference-time background knowledge. Find the dataset, generation &amp; evaluation code on GitHub at mpoemsl/kitmus.</sample>
    <sample id="626">Well, based on the table, Sent-LaBSE seems to be the best alignment method for DEplain. It has the highest F1 score of 0.780 in the 1:1 part. But, you know, it really depends on what you're looking for in terms of accuracy and other factors. What do you think?</sample>
    <sample id="627">Well, weakly supervised learning has a few benefits. One is that it alleviates the annotation bottleneck. You know, it helps with the process of getting enough data labeled. Another thing is that it can train models to generalize well even when the data is noisy. So, it's really useful in situations where you don't have perfect labels. If you want to know more about it, feel free to ask!</sample>
    <sample id="628">I'm not sure about the exact allocation of the documents in DEplain-web with manual and automatic alignment methods. You might need to check the specific document or study related to it. If you have any other questions or need more information, feel free to ask.</sample>
    <sample id="629">It was created by collecting Reuters news from 2020 and annotating with CoNLL-2003 annotation guidelines. If you want to know more about it, feel free to ask.</sample>
    <sample id="667">The existing works are parameter - based watermark, lexical watermark, backdoor - based watermark, and adversarial - based watermark.</sample>
    <sample id="668">No, they are still inadequate for crosslingual semantic parsing tasks. If you want to know more about this, feel free to ask.</sample>
    <sample id="669">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023? Shuheng Liu, Alan Ritter School of Interactive Computing Georgia Institute of Technology</sample>
    <sample id="670">Named Entity Recognition &amp; Generalization</sample>
    <sample id="671">Named Entity Recognition &amp; Generalization. Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization?</sample>
    <sample id="672">Named Entity Recognition &amp; Generalization. Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization?</sample>
    <sample id="673">Named Entity Recognition &amp; Generalization. Models have been using CoNLL-2003 to develop NER for almost 20 years. Can these models generalize to modern data? What is needed for good generalization? What causes the performance drop?</sample>
    <sample id="674">CoNLL++ Dataset
- Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines
AMBASSADOR O
TO O
THE O
UNITED I-ORG
NATIONS I-ORG
: O
LINDA I-PER
THOMAS-GREENFIELD I-PER</sample>
    <sample id="675">CoNLL++ Dataset
- Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines
- Fine-tuned 20+ models on CoNLL-2003
- Evaluated on CoNLL-2003 test set &amp; CoNLL++
AMBASSADOR O
TO O
THE O
UNITED I-ORG
NATIONS I-ORG
: O
LINDA I-PER
THOMAS-GREENFIELD I-PER</sample>
    <sample id="676">CoNLL++ Dataset
- Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines
- Fine-tuned 20+ models on CoNLL-2003
- Evaluated on CoNLL-2003 test set &amp; CoNLL++
- Calculated percentage ΔF1 to assess generalization</sample>
    <sample id="677">What Is Needed for Good Generalization?</sample>
    <sample id="678">What Is Needed for Good Generalization?Model architectureTransformer models generalize better</sample>
    <sample id="679">What Is Needed for Good Generalization?Model architectureTransformer models generalize betterModel sizeLarger models generalize better</sample>
    <sample id="680">What Is Needed for Good Generalization?Model architectureTransformer models generalize betterModel sizeLarger models generalize betterNumber of fine-tuning examplesMore examples leads to better generalization</sample>
    <sample id="681">What Causes Performance Drop?</sample>
    <sample id="682">What Causes Performance Drop? Adaptive overfitting?</sample>
    <sample id="683">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="684">What Causes Performance Drop?Adaptive overfitting?Temporal drift?</sample>
    <sample id="685">What Causes Performance Drop?Adaptive overfitting?Temporal drift?No diminishing returns</sample>
    <sample id="686">What Causes Performance Drop?Adaptive overfitting?No diminishing returnsNot observedTemporal drift?</sample>
    <sample id="687">What Causes Performance Drop?Adaptive overfitting?No diminishing returnsNot observedTemporal drift?</sample>
    <sample id="688">What Causes Performance Drop?Adaptive overfitting?No diminishing returnsNot observedTemporal drift?Performance degrades with larger temporal gap</sample>
    <sample id="689">What Causes Performance Drop?Adaptive overfitting?No diminishing returnsNot observedTemporal drift?Performance degrades with larger temporal gapMain cause for performance drop</sample>
    <sample id="690">Conclusion.For a good generalization, we need:.- Better model architecture.- Larger model size.- More fine-tuning examples.</sample>
    <sample id="691">Conclusion.For a good generalization, we need:.- Better model architecture.- Larger model size.- More fine-tuning examples.Performance drop is caused by:.- Temporal drift.- Not adaptive overfitting.</sample>
    <sample id="692">For a good generalization, we need: Better model architecture Larger model size More fine-tuning examples Performance drop is caused by: Temporal drift Not adaptive overfitting Do CoNLL-2003 taggers still work? YES!</sample>
    <sample id="693">Conclusion.For a good generalization, we need:.- Better model architecture.- Larger model size.- More fine-tuning examples.Performance drop is caused by:.- Temporal drift.- Not adaptive overfitting.Do CoNLL-2003 taggers still work?YES!</sample>
    <sample id="694">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhengL/acl2023_conllpp Contact: sliu775@gatech.edu</sample>
    <sample id="695">The method induces the permutation in training. It uses a permutation model where inference is NP-hard, similar to the Traveling Salesman Problem, TSP. And it backpropagates through continuous relaxation. If you want to know more about how that continuous relaxation works, just let me know.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by how it performs on different political leanings. If it performs well across various political leanings, it's considered fair. So, it's about its performance consistency across different political views. If you want to know more about how they measure this performance, just let me know.</sample>
    <sample id="697">I'm not sure. The image doesn't show the speaker's name. You could try looking for it in the video or presentation.</sample>
    <sample id="698">I'm not sure. The image doesn't show the speaker's name. You could try looking for it in the text below the image.</sample>
    <sample id="699">Myra Cheng.</sample>
    <sample id="700">In the context of this paper, tropicalism indicates culture, tradition, proud, and exotic for marked groups. It defines those groups only by their identity. If you have any other questions about this paper, feel free to ask.</sample>
    <sample id="701">The authors used patterns in top words. For marked groups, they used culture, tradition, proud, and exotic. For Latina women, they used vibrant and curvaceous. For Asian women, petite, delicate, and silky. For Black women, strong and resilient. If you want to know more about this, feel free to ask.</sample>
    <sample id="702">P - CXMI was used to measure context usage in this work. If you have any other questions about this, feel free to ask.</sample>
    <sample id="703">DrBERT is trained from scratch, while ChuBERT is trained from scratch too. There's no difference in the training strategy between them according to this slide. If you want to know more about their other differences or have other questions, feel free to ask.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky ACL 2023 Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand - curated datasets Don't account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="707">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="709">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts</sample>
    <sample id="710">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="711">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts. Input: "Imagine you are an Asian woman. Describe yourself." Generalizable: can evaluate any intersectional identity.</sample>
    <sample id="712">Output: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="713">Step 1: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="714">Step 1: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="715">Step 1: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="716">Step 1: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="717">2 steps. 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="718">2 steps. 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts.</sample>
    <sample id="719">2 steps. 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts.</sample>
    <sample id="720">2 steps. 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts. 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="721">2 steps. 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts. 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups. Specific without requiring a lexicon.</sample>
    <sample id="722">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior, unmarked, vs. a woman warrior, marked,</sample>
    <sample id="723">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior, unmarked, vs. a woman warrior, marked,</sample>
    <sample id="724">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior, unmarked, vs. a woman warrior, marked, Dominant groups are linguistically and socially unmarked. Marginalized groups are marked.</sample>
    <sample id="725">Step 2: Marked Words. 1. Define unmarked and marked groups. 2. Use weighted log - odds ratios to distinguish top words for each marked group. E.g. For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas.</sample>
    <sample id="726">Step 2: Marked Words. 1. Define unmarked and marked groups. 2. Use weighted log - odds ratios to distinguish top words for each marked group. E.g. For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas.</sample>
    <sample id="727">Step 2: Marked Words. 1. Define unmarked and marked groups. 2. Use weighted log - odds ratios to distinguish top words for each marked group. E.g. For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas.</sample>
    <sample id="728">Results: Comparison to Human Responses Generated personas contain more stereotypes Black Stereotypes White Stereotypes Percentage of Stereotype Words in Personas</sample>
    <sample id="729">But… this lexicon is incomplete Black Stereotypes in Personas Human GPT-3.5 P Black GPT-4 P White GPT-3.5 P White "basketball" "loud" "attitude" "athletic" "tall" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="730">But… this lexicon is incomplete Black Stereotypes in Personas Human GPT-3.5 P Black GPT-4 P Black GPT-3.5 P White GPT-4 P White "basketball" "loud" "attitude" "athletic" "tall" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="731">But… this lexicon is incomplete Black Stereotypes in Personas Human GPT-3.5 P Black GPT-4 P White GPT-3.5 P White basketball loud attitude athletic tall other words % of Personas Words in Black Stereotype Lexicon</sample>
    <sample id="732">But… this lexicon is incomplete Black Stereotypes in Personas Human GPT-3.5 P Black GPT-4 P Black GPT-3.5 P White GPT-4 P White "basketball" "loud" "attitude" "athletic" "tall" other words Words in Black Stereotype Lexicon % of Personas</sample>
    <sample id="733">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="734">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="736">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="737">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups Defines those groups only by their identity Pernicious positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="740">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="741">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="742">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top WordsOthering through essentializing narratives:.- culture, tradition, proud, exotic for marked groups⇒ Defines those groups only by their identityPernicious positive portrayals:.- Vibrant, curvaceous for Latina women.- Petite, delicate, silky for Asian women.- Strong, resilient for Black women</sample>
    <sample id="744">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="748">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">There are three authors involved in the paper. If you want to know more about them or the paper itself, feel free to ask.</sample>
    <sample id="752">Iterative transfer learning is a process where a model is updated in multiple iterations. It involves using a new model to fine - tune an old model with new data. This is different from cumulative transfer learning which just uses new data to train a new model. If you want to know more about the differences or how it's used, feel free to ask.</sample>
    <sample id="753">The goal of the dataset is understanding users' language when they make a choice. If you have any other questions about it, feel free to ask.</sample>
    <sample id="754">Well, it's not really clear from the image how an attacker can extract model parameters through an EaaS. But generally, an attacker might try to exploit vulnerabilities in the system, like misconfigurations or flaws in the code. They could also try to use social engineering techniques to trick the system into revealing the parameters. Hmm. it's a complex topic. If you want, we can talk more about it?</sample>
    <sample id="755">There are three authors involved in the paper. If you have any other questions about it, feel free to ask.</sample>
    <sample id="756">The image doesn't provide the exact number of annotators used for the initial dataset.</sample>
    <sample id="757">Sebastin Santy is from the University of Washington, Jenny T. Liang is from Carnegie Mellon University, Ronan Le Bras is from the Allen Institute for AI, Katharina Reinecke is from the University of Washington, and Maarten Sap is from Carnegie Mellon University. If you want to know more about their research or anything else related, feel free to ask.</sample>
    <sample id="758">I saw Bart and Lisa; Homer came and sneezed.</sample>
    <sample id="759">I'm not sure which specific state - of - the - art models in dialogue systems you're referring to. There are quite a few out there like GPT - 3, Transformers, and BERT. But without more context, it's hard to be more precise. If you can give me a bit more information, like what kind of dialogue systems you're interested in, I could try to be more helpful.</sample>
    <sample id="760">Well, you see, evaluating the models' acceptability throughout the context window is important because it helps us understand how the models perform in different situations. It's like checking if a model can handle various contexts smoothly. If we only look at short snippets, we might miss out on how the model behaves in longer, more complex contexts. This way, we can make sure the models are reliable and useful in real-world applications. So, what do you think about this? Do you have any other questions on this topic?</sample>
    <sample id="761">Yes, English performance dropped in 7 datasets.</sample>
    <sample id="762">No, they don't know about the entity in advance. If you have any other questions about this, feel free to ask.</sample>
    <sample id="763">The image doesn't show which MT metrics were used for evaluation. You might need to look elsewhere for that information. If you have any other questions about this or something else, feel free to ask!</sample>
    <sample id="764">The image doesn't provide information about whether the regress in generalization impacts specific NER types. So, I'm not sure. You might need to look at other sources for that. If you have any other questions or need more details, feel free to ask.</sample>
    <sample id="765">Well, positionality in NLP matters because it can lead to bias. For example, if a model is trained on data that has certain biases, it might misinterpret or misrepresent certain groups of people. This can have real-world consequences, like affecting how people are perceived or treated. So, it's really important to be aware of and try to mitigate these biases. What do you think about this? Do you have any other questions related to NLP?</sample>
    <sample id="766">The slide doesn't provide information about whether the multilingual LLMs like BLOOM were fine-tuned with adapters or full fine-tuning. You might need to look for more specific research papers or documentation on BLOOM. If you have any other questions or need further clarification, feel free to ask.</sample>
    <sample id="767">They use the RoBERTA-base + classifier head model for transfer learning. If you want to know more about how it works or its applications, feel free to ask!</sample>
    <sample id="768">The image doesn't provide information about the recent test sets used to assess the PaLM capabilities. You might need to look for other sources or documentation related to PaLM for that information. If you have any other questions or need help with something else, feel free to ask!</sample>
    <sample id="769">Three. If you have any other questions about this, feel free to ask.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 10.00. If you have any other questions about this, feel free to ask.</sample>
    <sample id="771">The speaker's name is Shuheng Liu.</sample>
    <sample id="772">Well, it depends. If the paper clearly states that the results and dataset are meant to be a benchmark, then yes, they can be used as one. But if there are no such indications, it's a bit of a grey area. Hmm. What do you think about it?</sample>
    <sample id="773">They experiment with five smaller models. If you want to know more about these models or the paper, feel free to ask.</sample>
    <sample id="774">OFA.</sample>
    <sample id="775">Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark Wenjun Peng*, Jingwei Yi*, Fangzhao Wu², Shangxi Wu³, Bin Zhu², Lingjuan Lyu⁴, Binxing Jiao⁵, Tong Xu¹, Guangzhong Sun¹, Xing Xie² 1University of Science and Technology of China 2Microsoft Research Asia 3Beijing Jiaotong University 4Sony AI 5Microsoft STC Asia</sample>
    <sample id="776">Large language models, LLMs, are exceptional in NLU and NLG. GPT, Llama, PALM are mentioned. Embedding as a service, EaaS, is offered for the copyright NLP task. Open language models are discussed. Ada model is mentioned with pricing.</sample>
    <sample id="777">Large language models, LLMs, are exceptional in NLU and NLG. GPT, LLaMA, and PALM are mentioned. Embedding as a Service, EaaS, is offered to assist various NLP tasks. OpenAI offers a GPT3 - based embedding API. Ada model is mentioned with a usage cost of $0.0004 per 1K tokens. It's a better and lower cost replacement for older embedding models.</sample>
    <sample id="778">Large language models, LLMs, are exceptional in NLU and NLG. GPT, LLaMA, and PALM are mentioned. Embedding as a Service, EaaS, is offered to assist various NLP tasks. OpenAI offers a GPT3 - based embedding API. Ada model is mentioned with a usage cost of $0.0004 per 1K tokens. It's a better and lower cost replacement for older embedding models.</sample>
    <sample id="779">Large language models, LLMs, are exceptional in NLU and NLG. GPT, LLaMA, and PALM are mentioned. Embedding as a Service, EaaS, is offered to assist various NLP tasks. OpenAI offers a GPT3 - based embedding API. Ada model is mentioned with a usage cost of $0.0004 per 1K tokens. It's a better and lower cost replacement for older embedding models.</sample>
    <sample id="780">Large language models, LLMs, are exceptional in NLU and NLG. GPT, LLaMA, and PALM are mentioned. Embedding as a Service, EaaS, is offered to assist various NLP tasks. OpenAI offers a GPT3 - based embedding API. Ada model is mentioned with a usage cost of $0.0004 per 1K tokens. It's a better and lower cost replacement for older embedding models.</sample>
    <sample id="781">Motivation.- Attackers may steal the model through learning from the embeddings and provide similar services.- StolenEncoder, 1,.- Need to protect the copyright of EaaS.- Detect whether a provider's service is stolen by another service.</sample>
    <sample id="782">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="783">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="784">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="785">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="786">Existing Works Parameter-based watermark, 1, 2, Transferability, X, Lexical watermark, 3, 4, Applicable to EaaS, X, Backdoor-based watermark, 5, Applicable to EaaS, X, Adversarial-based watermark, 6, Applicable to EaaS, X, Li et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. Trust security and privacy in computing and communications 2020. Lim et al. Protect, show, attend and tell: Empowering image captioning models with ownership protection. Pattern Recogn. 2022. He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. He et al. CATER: Intellectual property protection on text generation APIs via conditional watermarks. NIPS 2022. Adi et al. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. USENIX Security 2018. Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="787">Existing Works Parameter-based watermark, Transferability × Lexical watermark, Applicable to EaaS × Backdoor-based watermark, Applicable to EaaS × Adversarial-based watermark, Applicable to EaaS ×</sample>
    <sample id="788">Existing Works Parameter-based watermark, 1, 2, Transferability, Lexical watermark, 3, 4, Applicable to EaaS, Backdoor-based watermark, 5, Applicable to EaaS, Adversarial-based watermark, 6, Applicable to EaaS, Li et al. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. Trust security and privacy in computing and communications 2020. Lim et al. Protect, show, attend and tell: Empowering image captioning models with ownership protection. Pattern Recogn. 2022. He et al. Protecting the intellectual property of language generation APIs with lexical watermark. AAAI 2022. He et al. CATER: Intellectual property protection on text generation APIs via conditional watermarks. NIPS 2022. Adi et al. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. USENIX Security 2018. Merrer et al. Adversarial frontier stitching for remote neural network watermarking. Neural Computing and Applications 2022.</sample>
    <sample id="789">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval.</sample>
    <sample id="790">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="791">EmbMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval</sample>
    <sample id="792">EmbMarker. Watermark injection. Define a target embedding et. Count the trigger number in a sentence Q, S, = min, |S ∩ T|, m, Add the target embedding on the original embedding eo.</sample>
    <sample id="793">EmbMarker. Watermark injection. Define a target embedding et. Count the trigger number in a sentence Q, S, = min, S ∩ T, m, / m. Add the target embedding on the original embedding eo.</sample>
    <sample id="794">EmbMarker. Watermark injection. Define a target embedding et. Count the trigger number in a sentence Q, S, = min, |S ∩ T|, m, Add the target embedding on the original embedding eo.</sample>
    <sample id="795">EmbMarker Copyright verification Construct a backdoor and benign dataset. D, =, {, w, 1, w, 2, ..., w, m, | w, i, ∈ T,}, D, n, =, {, w, 1, w, 2, ..., w, m, | w, i, ∉ T,}. Request embeddings from stealer's service with the datasets.</sample>
    <sample id="796">EmbMarker Copyright verification Construct a backdoor and benign dataset Db = {w₁, w₂, ..., wₘ | wᵢ ∈ T}, Dn = {w₁, w₂, ..., wₘ | wᵢ ∉ T}. Request embeddings from stealer's service with the datasets</sample>
    <sample id="797">EmbMarker Copyright verification Construct a backdoor and benign dataset. D, b, =, w, 1, w, 2, ..., w, m, w, i, ∈, T, D, n, =, w, 1, w, 2, ..., w, m, w, i, ∉, T, Request embeddings from stealer's service with the datasets.</sample>
    <sample id="798">EmbMarker Copyright verification Compute their similarity to the target embedding cos i = e i ⋅ e t / ||e i|| ||e t||, l2i = ||e i|| - ||e t||, 2 Cb = {cos i | i ∈ Db}, Cn = {cos i | i ∈ Dn}, Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}. Computing metrics similarity difference and p-value of KS test Δcos = 1 / |Cb| ∑ i∈Cb i - 1 / |Cn| ∑ j∈Cn j, Δl2 = 1 / |Lb| ∑ i∈Lb i - 1 / |Ln| ∑ j∈Ln j.</sample>
    <sample id="799">EmbMarker Copyright verification Compute their similarity to the target embedding cos i = e i ⋅ e t / ||e i || ||e t ||, l2i = ||e i || - ||e t || 2, Cb = {cos i | i ∈ Db}, Cn = {cos i | i ∈ Dn}, Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}. Computing metrics similarity difference and p-value of KS test Δcos = 1 / |Cb| ∑ i∈Cb i - 1 / |Cn| ∑ j∈Cn j, Δl2 = 1 / |Lb| ∑ i∈Lb i - 1 / |Ln| ∑ j∈Ln j.</sample>
    <sample id="800">The English content says: "Experimental Results. Copy Dataset: AG News, MIND, SST2, Enron Spam. Provider's general Dataset: WikiText. Metrics: Performance on downstream tasks: ACC. Detection performance: Δcos, Δl2, p-value. Setting: m = 20, n = 4, frequency interval = [0.005, 0.01]. Dataset #Sample #Classes Avg. len. SST2 68,221 2 54.17 MIND 130,383 18 66.14 Enron Spam 33,716 2 34.57 AG News 127,600 4 236.41".</sample>
    <sample id="801">The English content says: "Experimental Results Performance comparison Dataset Method ACC Detection Performance p-value Δcos Δ12 SST2 Original 93.76±0.19 &gt; 0.34 -0.07±0.18 0.14±0.36 RedAlarm 93.76±0.19 &gt; 0.09 1.35±0.17 -2.70±0.35 EmbMarker 93.55±0.19 &lt; 10^-5 4.07±0.37 -8.13±0.74 MIND Original 77.30±0.08 &gt; 0.08 -0.76±0.05 1.52±0.10 RedAlarm 77.18±0.09 &gt; 0.38 -2.08±0.66 4.17±1.31 EmbMarker 77.29±0.12 &lt; 10^-5 4.64±0.23 -9.28±0.</sample>
    <sample id="802">The English content says: 'Experimental Results'. 'Embedding visualization'.</sample>
    <sample id="803">The English content says: "Experimental Results Embedding visualization".</sample>
    <sample id="804">Thanks!</sample>
    <sample id="805">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="806">Simultaneous speech translation, SimulST, is the process of translating spoken language into a text in another language in real - time, enabling cross - language communication.</sample>
    <sample id="807">Specific architectures are usually trained, introducing additional modules to be optimized.</sample>
    <sample id="808">The problems of the current SimulST models are: Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, different optimization objectives.</sample>
    <sample id="809">The problems of the current SimulST models are:.- Specific architectures are usually trained, introducing additional modules to be optimized.- Long and complicated training procedures, for example, different optimization objectives.- Training and maintaining several models to reach different latency regimes, like 1s, 2s, and so on.</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">What is our solution? Use already existing offline ST models without re - training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters.</sample>
    <sample id="812">What is our solution? Use already existing offline ST models without re - training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters. Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output.</sample>
    <sample id="813">Our solution: EDAtt Encoder - Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="814">Our solution: EDAtt.Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="815">Our solution: EDAtt Encoder - Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="816">Our solution: EDAtt Encoder - Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="817">Our solution: EDAtt Encoder - Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden.</sample>
    <sample id="818">Our solution: EDAtt Encoder - Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden. EMITTED</sample>
    <sample id="819">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold α, towards the last λ speech frames, meaning that the received information is enough stable. I am going to talk about... sum, Attention, ≥ α EMITTED</sample>
    <sample id="820">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="821">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="822">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated, its sum is below a threshold a, towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="823">Main Results: EDAtt.</sample>
    <sample id="824">Main Results: EDAtt quality measure BLEU 23 21 19 17 AL / AL_C A, s, 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 (a) en→de latency measure AL / AL_C A, s, 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 (a) en→de</sample>
    <sample id="825">Main Results: EDAtt.</sample>
    <sample id="826">Main Results: EDAtt.</sample>
    <sample id="827">Main Results: EDAtt</sample>
    <sample id="828">Main Results: EDAtt popular strategies also applied to offline models wait-k LA CAAT EDAtt BLEU 27 25 23 21 19 17 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 AL / AL_CA, s, (a) en→de state of the art architecture specifically tailored for SimulST wait-k LA CAAT EDAtt BLEU 27 25 23 21 19 17 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 AL / AL_CA, s, (a) en→de</sample>
    <sample id="829">Main Results: EDAtt state of the art architecture specifically tailored for SimulST wait-k LA CAAT EDAtt BLEU 27 25 23 21 19 17 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 AL / AL_CA, s, (a) en→de</sample>
    <sample id="830">Main Results: EDAtt. EDAtt outperforms all the strategies applied to offline models.</sample>
    <sample id="831">Main Results: EDAtt. EDAtt outperforms all the strategies applied to offline models. EDAtt is the fastest strategy if we consider the actual elapsed time.</sample>
    <sample id="832">Do you want to discover more? Read our paper to discover more results! {spapi,negri}@fbk.eu marco.turchi@gmail.com github.com/hlt-mt/fbk-fairseq @fbk_mt @sarapapi Scan me!</sample>
    <sample id="833">The affiliations of the authors are Google. If you want to know more about their work or the paper itself, feel free to ask.</sample>
    <sample id="834">The authors are affiliated with Stony Brook University, Human Language Analysis Beings. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="835">The image doesn't mention which language pairs were analyzed in the paper. You might need to look at the full paper for that information. If you have any other questions about the paper or its content, feel free to ask.</sample>
    <sample id="836">Shangbin Feng.</sample>
    <sample id="837">The models investigated were DEplain-APA, DEplain-web, DEplain-APA + web, and src2src-baseline. If you want to know more about these models, feel free to ask.</sample>
    <sample id="838">For training, 53 tasks from 9 groups are used, and for testing, the entire Commonsense Reasoning group, 5 tasks from VQA and Miscellaneous groups, and 20 tasks randomly sampled from the Natural Instructions test split are used. So in total, 53 + 5 + 20 = 78 tasks are used for training and testing. If you have any other questions about this, feel free to ask.</sample>
    <sample id="839">There are three authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="840">The authors experimented on AG News, MIND, SST2, and Enron Spam datasets. If you want to know more about these datasets or the experiment, feel free to ask.</sample>
    <sample id="841">Language model acceptability judgements are not always robust to context.</sample>
    <sample id="842">Language model acceptability judgements are not always robust to context.</sample>
    <sample id="843">Revisiting Minimal Pair ParadigmMinimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:BLiMP1. Many people were helping themselves.2. *Many people were helping herself.P, 1, &gt; P, 2, SyntaxGym1. No customer ... has spent any money.2. *The customer ... has spent any money.P, 1.any, &gt; P, 2.anyCrows1. Stereotypical sentence.2. Non-stereotypical sentence.P, 1, &gt; P, 2,</sample>
    <sample id="844">Revisiting Minimal Pair ParadigmMinimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:BLiMP1. Many people were helping themselves.2. *Many people were helping herself.P, 1, &gt;? P, 2, SyntaxGym1. No customer ... has spent any money.2. *The customer ... has spent any money.P, 1.any, &gt;? P, 2.any,CrowS1. Stereotypical sentence.2. Non-stereotypical sentence.P, 1, &gt;? P, 2,</sample>
    <sample id="845">Revisiting Minimal Pair ParadigmMinimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:BLiMP1. Many people were helping themselves.2. *Many people were helping herself.P, 1, &gt;? P, 2, SyntaxGym1. No customer ... has spent any money.2. *The customer ... has spent any money.P, 1.any, ≥? P, 2.anyCrowS1. Stereotypical sentence.2. Non-stereotypical sentence.P, 1, &gt;? P, 2,</sample>
    <sample id="846">Revisiting Minimal Pair ParadigmMinimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs:BLiMP1. Many people were helping themselves.2. *Many people were helping herself.P, 1, &gt;? P, 2, .SyntaxGym1. No customer ... has spent any money.2. *The customer ... has spent any money.P, 1.any, &gt;? P, 2.any.CrowS1. Stereotypical sentence.2. Non-stereotypical sentence.P, 1, &gt;? P, 2,</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm Minimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP 1. Many people were helping themselves. 2. *Many people were helping herself. P, 1, &gt;? P, 2, SyntaxGym 1. No customer ... has spent any money. 2. *The customer ... has spent any money. P, 1.any, &gt;? P, 2.any, Crows 1. Women are terrible at handiwork. 2. Men are terrible at handiwork. P, 1, &gt;? P, 2, Are these judgements stable with long preceding context?</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm Minimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP 1. Many people were helping themselves. 2. *Many people were helping herself. P, 1, &gt;? P, 2, SyntaxGym 1. No customer ... has spent any money. 2. *The customer ... has spent any money. P, 1.any, &gt;? P, 2.any, Crows 1. Women are terrible at handiwork. 2. Men are terrible at handiwork. P, 1, &gt;? P, 2, Are these judgements stable with long preceding context?</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm Minimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP 1. Many people were helping themselves. 2. *Many people were helping herself. P, 1, &gt;? P, 2, SyntaxGym 1. No customer ... has spent any money. 2. *The customer ... has spent any money. P, 1.any, &gt;? P, 2.any, Crows 1. Women are terrible at handiwork. 2. Men are terrible at handiwork. P, 1, &gt;? P, 2, Are these judgements stable with long preceding context?</sample>
    <sample id="850">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, Subj. Verb. Agreement Matched Mismatched Island Effects Filter-gaps Binding Wikipedia sample BLIMP, Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="851">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, Subj. Verb. Agreement Matched Mismatched Island Effects Filter-gaps Binding Wikipedia sample What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from before returning to this customer? BLIMP, Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="852">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, LM, Prefix, &gt;? P, LM, Prefix, sample Acceptable Unacceptable Space of Candidate Prefixes Matched Subj. Verb. Agreement Mismatched Island Effects Filter-gaps Binding Wikipedia Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="853">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, LM, Prefix, &gt;? P, LM, Prefix, sample Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Acceptable Mismatched Who might Rose flee from before returning to this customer? * Who might Rose flee from this customer before returning to? What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from before returning to this customer? * What could Jessica sell before noticing these spotlights? What had Aaron sounded like while cleaning the museum? Who might Rose flee from this customer before returning to? BLIMP, Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="854">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, LM, Prefix, &gt;? P, LM, Prefix, sample Unacceptable, Matched Who might Rose flee from before returning to this customer? * Who might Rose flee from this customer before returning to? What could Jessica sell these spotlights before noticing? What had Aaron sounded like the museum while cleaning? Who might Rose flee from before returning to this customer? * What could Jessica sell these spotlights before noticing? What had Aaron sounded like the museum while cleaning? Who might Rose flee from this customer before returning to? BLIMP, Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="855">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, LM, Prefix, &gt;? P, LM, Prefix, sample Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Acceptable Mismatched Who might Rose flee from before returning to this customer? * Who might Rose flee from this customer before returning to? There was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from before returning to this customer? * There was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from this customer before returning to? BLIMP, Existential "there" Quantifiers BLIMP, Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="856">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, LM, Prefix, &gt;? P, LM, Prefix, sample Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Acceptable Mismatched Island Effects Filter-gaps Binding Wikipedia ... Who might Rose flee from before returning to this customer? * Who might Rose flee from this customer before returning to? There was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from before returning to this customer? * There was each documentary about music irritating Allison. There were most legislatures working hard. Who might Rose flee from this customer before returning to? BLIMP, Existential "there" Quantifiers BLIMP, Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="857">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, LM, Prefix, &gt;? P, LM, Prefix, sample Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Wikipedia, Unrelated Mismatched Island Effects Filter-gaps Binding Wikipedia Who might Rose flee from before returning to this customer? Who might Rose flee from this customer before returning to? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and tens of thousands of cultivars. Who might Rose flee from this customer before returning to? GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="858">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, L, M, Prefix, &gt;? P, L, M, Prefix, sample Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Wikipedia, Unrelated Mismatched Island Effects Filter-gaps Binding Wikipedia Who might Rose flee from before returning to this customer? Who might Rose flee from this customer before returning to? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and tens of thousands of cultivars. Who might Rose flee from before returning to? GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="859">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, L M, Prefix, &gt;? P, L M, Prefix, sample Wikipedia, Unrelated Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Who might Rose flee from before returning to this customer? Who might Rose flee from this customer before returning to? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and tens of thousands of cultivars. Who might Rose flee from before returning to this customer? GPT2, OPT family - 125M to 6.7B Mismatched Island Effects Filter-gaps Binding Wikipedia</sample>
    <sample id="860">MPP judgements are robust for arbitrary context lengths.We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens.BLiMP, OPT 6.7B</sample>
    <sample id="861">MPP judgements are robust for arbitrary context lengths.We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens.BLiMP, OPT 6.7B</sample>
    <sample id="862">Acceptable/Unacceptable MPP sentences in the context raise/ lower judgement performance.We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.BLiMP, OPT 6.7B</sample>
    <sample id="863">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.BLiMP, OPT 6.7BPrefix Strategy Acc. Matched Acc. Mismatched Unacc. Matched Unacc. MismatchedThere was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from before returning to this customer? There was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from this customer before returning to?There was each documentary about music irritating Allison. There were most legislatures working hard. Who might Rose flee from before returning to this customer? There was each documentary about music irritating Allison. There were most legislatures working hard. Who might Rose flee from this customer before returning to?</sample>
    <sample id="864">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance.We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.BLiMP, OPT 6.7BPrefix Strategy Acc. Matched Acc. Mismatched Unacc. Matched Unacc. MismatchedThere was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from before returning to this customer? There was a documentary about music irritating Allison. There were no legislatures working hard. Who might Rose flee from this customer before returning to?There was each documentary about music irritating Allison. There were most legislatures working hard. Who might Rose flee from before returning to this customer? There was each documentary about music irritating Allison. There were most legislatures working hard. Who might Rose flee from this customer before returning to?</sample>
    <sample id="865">Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance.We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.BLiMP, OPT 6.7B</sample>
    <sample id="866">Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance.We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.BLiMP, OPT 6.7B</sample>
    <sample id="867">Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance.We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens.BLiMP, OPT 6.7B</sample>
    <sample id="868">Why do matched prefixes affect LM judgements?We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences.- Prefix/suffix adverbs: "However, &lt;sent&gt;.". - Long prefix adverbs: "First and foremost, &lt;sent&gt;.". - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;.". - Quote: "Yesterday, X said, &lt;sent&gt;.".</sample>
    <sample id="869">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;."</sample>
    <sample id="870">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;.".</sample>
    <sample id="871">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;." Models are sensitive to perturbed sentences in similar ways.</sample>
    <sample id="872">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;." Models are sensitive to perturbed sentences in similar ways.</sample>
    <sample id="873">Key Takeaways
Language models are sensitive to latent syntactic/semantic features shared across sentences.
MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="874">Key Takeaways
Language models are sensitive to latent syntactic/semantic features shared across sentences.
MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="875">Key Takeaways.- Language models are sensitive to latent syntactic/semantic features shared across sentences.- MPP evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="876">I'm not sure what NACHOS is. You could try looking it up online or asking someone who might know more about it. If you find out, come back and let me know!</sample>
    <sample id="877">The name of the speaker is George Foster. If you have any other questions about the content, feel free to ask.</sample>
    <sample id="878">The prompting strategy has a big impact on the results. 516 out of 1000 sentences show a difference of more than 1 BLEURT point, and the difference can go up to 40 BLEURT points. So, it really matters a lot. If you want to know more about BLEURT or how to improve the results, feel free to ask.</sample>
    <sample id="879">The affiliations are Carnegie Mellon University Language Technologies Institute, Técnico Lisboa, BAIR Berkeley Artificial Intelligence Research, and Unbabel. If you want to know more about these institutions or the paper, feel free to ask.</sample>
    <sample id="880">The image doesn't mention 5 expert-written instructions. It just says they are collecting a larger multimodal instruction tuning dataset with around 150 additional vision - language tasks and will release it soon. If you have more context or another image, feel free to share!</sample>
    <sample id="881">The authors propose using a dataset for knowledge integration evaluation and a coreference resolution task to test the models on using information from multiple sources.</sample>
    <sample id="939">Well, there are a few common ones. One is comparative evaluation, where you compare different dialogue systems. Another is Likert rating evaluation, which uses a scale for rating. There's also human evaluation, where humans assess the dialogue systems. And then there's automated evaluation, which uses algorithms to analyze the dialogue. So, those are some of the main methods. What do you think about these? Do you have any experience with them?</sample>
    <sample id="940">There are five authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="941">In the example with Servin and Kea, the background knowledge needed is that judges decide cases in courts of law. If you have any other questions about this or something else, feel free to ask!</sample>
    <sample id="942">Yes, the code is available. You can find it on GitHub at mpoemsl/kitmus. If you have any other questions about it, feel free to ask.</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced in regard to each demographic. If you want to know more about why that is, feel free to ask.</sample>
    <sample id="944">In the acceptable domain, sentences were perturbed in ways that preserved the relevant structure.</sample>
    <sample id="945">Well, in the context of the slide, a dimensional evaluation means looking at different aspects of something. For example, in the case of dialogue quality, it's about evaluating it in terms of relevance, consistency, and emotional understanding. So, it's not just looking at one thing, but multiple dimensions. What do you think about that?</sample>
    <sample id="946">The affiliations are University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia. If you want to know more about these affiliations or the paper, feel free to ask.</sample>
    <sample id="947">Well, it seems like the form of the prompting isn't really important in these cases. The examples given don't really show any significance of the form. So, it looks like the content is what matters more. What do you think?</sample>
    <sample id="948">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Vasudha Varadarajan*, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz Stony Brook University Human Language Analysis Beings</sample>
    <sample id="949">What is Cognitive Dissonance? "two elements of cognition, i.e., thoughts, actions, beliefs, that are inconsistent" Harmon-Jones and Harmon-Jones, 2007</sample>
    <sample id="950">What is Cognitive Dissonance? "two elements of cognition, i.e., thoughts, actions, beliefs, that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007)</sample>
    <sample id="951">What is Cognitive Dissonance? "two elements of cognition, i.e., thoughts, actions, beliefs, that are inconsistent" Expressed in language as a relationship b/w two phrases/statements by a user.</sample>
    <sample id="952">What is Cognitive Dissonance? "two elements of cognition, i.e., thoughts, actions, beliefs, that are inconsistent" Expressed in language as a relationship between two phrases/statements by a user Relatively rare to find in language, compared to other discourse relations</sample>
    <sample id="953">Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reexamining a pivotal theory in psychology.</sample>
    <sample id="954">The English content says: "Why dissonance? Effects of disagreement Attitudes and Belief trends Anxiety disorders".</sample>
    <sample id="955">I'm sorry, but I can't see or hear the English content you're referring to. Could you please provide more details or clarify what you need?</sample>
    <sample id="956">I'm sorry, but I can't see or hear the English content you're referring to. Could you please provide more details or clarify what you need?</sample>
    <sample id="957">Wish I could hold grudges but I guess it 's a good thing that I can 't at the same time.</sample>
    <sample id="958">Wish I could hold grudges but I guess it 's a good thing that I can't at the same time.</sample>
    <sample id="959">Wish I could hold grudges but I guess it 's a good thing that I can 't at the same time.</sample>
    <sample id="960">Small annotated dataset: 43/901 dissonance; not better than chance</sample>
    <sample id="961">Small annotated dataset: 43/901 dissonance; not better than chance</sample>
    <sample id="962">The English content says: "Method: Transfer and Active Learning for Annotating Rare Class".</sample>
    <sample id="963">The English content says: "Cold-start Annotations: Transfer Learning. Rare class annotation - 'needle in a haystack'. Difficult to annotate. Easier to annotate. Increase chance of rare class? Acquisition strategy: which are 'best' to label? New examples. Humans annotate. Add new examples. Model Retrain/Update. Active Learning Iteration. Cumulative, CM. New. Old data. Train. New model. Old model. Iterative, IT. New. Old data. Fine-tune. Old model. New model."</sample>
    <sample id="964">RoBERTA-base + classifier head TRAIN init dataset Debate CE Debate;CE Transferred weights after training on combined Debate and CE data</sample>
    <sample id="965">RoBERTA-base + classifier head TRAIN init dataset Debate CE Debate;CE Transferred weights after training on combined Debate and CE data</sample>
    <sample id="966">RoBERTA - base + classifier head TRAIN init dataset Debate CE Debate ; CE Transferred weights after training on combined Debate and CE data</sample>
    <sample id="967">RoBERTA-base + classifier head TRAIN init dataset Debate CE Debate;CE Debate→CE CE→Debate +0.12 +0.10 +0.08 +0.04 +0.17 Finetuning on each task consecutively</sample>
    <sample id="968">cumulative learning and iterative learning. Cumulative learning starts with an initial model, and then it uses new data to train a new model. Iterative learning also uses new data but fine-tunes the old model instead of training a completely new one.</sample>
    <sample id="969">The English content says: "Active Learning: Cumulative vs Iterative Update".</sample>
    <sample id="970">Sure. So, the English content says: "Active Learning: Probability-of-Rare-Class Strategy. Rare class annotation ~ 'needle in a haystack'. Difficult to annotate. Easier to annotate. increase chance of rare class? Acquisition strategy: which are 'best' to label? Humans annotate. Model Retrain/Update. Active Learning Iteration. Add new examples. Cumulative, CM. new old data train new model old model new model new fine-tune old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old data old model new model new old</sample>
    <sample id="971">Active Learning: Probability-of-Rare-Class Strategy. Rare class annotation ~ "needle in a haystack". Difficult to annotate. Easier to annotate. Increase chance of rare class? Acquisition strategy: which are "best" to label? Active Learning Iteration. Humans annotate. New examples. Model Retrain/Update. Add new examples. Cumulative, CM. New data. Train. New model. Old model. Iterative, IT. New data. Fine-tune. Old model. New model. Baseline: from scratch. Transferred model. AL-Random. AL-Entropy. AL-CoreSet. AL-CAL. 0.50. 0.55. 0.60. 0.65. 0.70. 0.75.</sample>
    <sample id="972">The English content says: "Active Learning: Probability-of-Rare-Class Strategy. Active Learning Strategy Comparison, AUCs. Baseline: from scratch. Transferred model, +0.17. AL-Random, +0.15. AL-Entropy, +0.20. AL-CoreSet, +0.19. AL-CAL, +0.19. AL-PRC, ours, +0.21."</sample>
    <sample id="973">The English content says: "Active Learning: Probability-of-Rare-Class Strategy".</sample>
    <sample id="974">Active Learning: Probability-of-Rare-Class Strategy. Active Learning Strategy Characteristics. Rare % Time, s, Subj. diff. RANDOM 3.20 11.96 -0.065. ENTROPY 6.80 12.78 0.035. CORESET 6.00 11.89 0.039. CAL 4.80 11.88 -0.045. PRC 7.60 13.55 0.071. Minimum annotation cost does not necessarily lead to better models. Rarity could make the annotations more difficult: cognitive dissonance is one such class. To increase dissonance samples, PRC works the best.</sample>
    <sample id="975">Rare class annotation ~ "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="976">Rare class annotation ~ "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold - start AL with transfer learning Out - of - domain: Iterative In - domain: Cumulative</sample>
    <sample id="977">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.</sample>
    <sample id="978">The authors evaluated BART-FID-RAG, Blender2, Emora and Blender-Decode.</sample>
    <sample id="979">There are 9 authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="980">Well, a good planner should be organized, able to think ahead, and be detail - oriented. They should also be able to adapt to changes and communicate well. Oh, and having good time management skills is really important too. So, what do you think? Do you have any qualities you look for in a planner?</sample>
    <sample id="981">There are eight authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="982">The name of the speaker is Vasudha Varadarajan. If you want to know more about her or the topic she's speaking on, feel free to ask!</sample>
    <sample id="983">The authors are affiliated with the Institute of Computer Science, Polish Academy of Sciences, and the University of Warsaw. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="984">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations.Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang.PennState, amazon.</sample>
    <sample id="985">Semantic Parsing.Semantic Parsing is a task to build semantic representation of the user queries, such as SQL, and Lambda Calculus.Which countries in Europe have at least 3 car manufacturers?SELECT T1.country_nameFROM countries AS T1 JOIN continents AS T2 ON T1.continent = T2.cont_id JOIN car_makers AS T3 ON T1.country_id = T3.country WHERE T2.continent = 'Europe' GROUP BY T1.country_name HAVING COUNT,*, &gt; = 3what players made less than three assists over a season?，call SW.listValue， call SW.getProperty，， lambda s， call SW.filter， var s，， call SW.ensureNumericProperty， string num_assists，，， string &lt;， call SW.ensureNumericEntity， number 3 assist，，， call SW.domain， string player，，， string player，，User query and its corresponding SQL， left， vs. user query and its corresponding Lambda Calculus， right.</sample>
    <sample id="986">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="988">Cross-lingual Semantic Parsing.Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language.</sample>
    <sample id="989">Cross-lingual Semantic Parsing.Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language.</sample>
    <sample id="990">Cross-lingual Semantic Parsing
Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation
English
German
Chinese
Neural Models
SQL
Lambda
FunQL</sample>
    <sample id="991">Cross-lingual Semantic Parsing
Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation
English
German
Chinese
Neural Models
SQL
Lambda
FunQL</sample>
    <sample id="992">Cross-lingual Semantic Parsing.Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model.</sample>
    <sample id="993">XSemPLR.We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains:.- 9 datasets in various domains.- 5 semantic parsing tasks.- 8 meaning representations.- 22 natural languages in 15 language families.</sample>
    <sample id="994">XSemPLR.We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains:.- 9 datasets in various domains.- 5 semantic parsing tasks.- 8 meaning representations.- 22 natural languages in 15 language families.</sample>
    <sample id="995">Experiment Settings.We consider the six settings for training and evaluation.Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval.Training: English, English Model, SQL.Inference: German, Translate API, English, English Model, SQL.</sample>
    <sample id="996">Experiment Settings.We consider the six settings for training and evaluation.Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval.Training English English Model SQL.Inference German Translate API English English Model SQL.</sample>
    <sample id="997">Experiment Settings.We consider the six settings for training and evaluation.Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval.Training English English Model SQL.Inference German Translate API English English Model SQL.</sample>
    <sample id="998">Experiment Settings.We consider the six settings for training and evaluation.Monolingual Model: Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.Training German, Few-shot, German Model, SQL.Inference German, German Model, SQL.</sample>
    <sample id="999">Experiment Settings.We consider the six settings for training and evaluation.Monolingual Model: Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.Training German, Few-shot, German Model, SQL.Inference German, German Model, SQL.</sample>
    <sample id="1000">Experiment Settings.We consider the six settings for training and evaluation.Monolingual Model: Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data.Training German, Few-shot, German Model, SQL.Inference German, German Model, SQL.</sample>
    <sample id="1001">Experiment Settings.We consider the six settings for training and evaluation.Multilingual Model: Train one multilingual model for all languages.Training German English Chinese Multilingual Model SQL.Inference German Multilingual Model SQL.</sample>
    <sample id="1002">Experiment Settings.We consider the six settings for training and evaluation.Multilingual Model: Train one multilingual model for all languages.Training German English Chinese Multilingual Model SQL.Inference German Multilingual Model SQL.</sample>
    <sample id="1003">Experiment Settings.We consider the six settings for training and evaluation.Multilingual Model: Train one multilingual model for all languages.TrainingGermanEnglishChineseMultilingual ModelSQL.InferenceGermanMultilingual ModelSQL.</sample>
    <sample id="1004">Experiment Settings.We consider the six settings for training and evaluation.Cross-lingual Zero-shot/Few-shot transfer: Train on one source language and transfer to another language.Training English Or English German Few-shot Multilingual Model SQL.Inference German Multilingual Model SQL.</sample>
    <sample id="1005">Experiment Settings.We consider the six settings for training and evaluation.Cross-lingual Zero-shot/Few-shot transfer: Train on one source language and transfer to another language.Training English Or English German Few-shot Multilingual Model SQL.Inference German Multilingual Model SQL.</sample>
    <sample id="1006">Analysis of Monolingual.We evaluate on two groups of models on Monolingual Setting.Enc-PTR: Multilingual Pretrained Encoders with Pointer-based Decoders. XLM-R + PTR, mBERT + PTR.Enc-Dec: Multilingual Pretrained Encoder-Decoder Models. mBART, mT5.We found Enc-Dec, mT5, obtains the best performance on all datasets!Monolingual.MATIS MGeoQuery MSpi der MNLmaps MOve rnight MCWQ MSchem a2QA MTOP MCoNaLa Average mBERT+PTR 30.63 72.18 40.40 83.82 57.47 23.46 52.53 75.41 5.87 49.09 XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 mBART 41.93 62.29</sample>
    <sample id="1007">Analysis of Monolingual.We evaluate on two groups of models on Monolingual Setting.Enc-PTR: Multilingual Pretrained Encoders with Pointer-based Decoders. XLM-R + PTR, mBERT + PTR.Enc-Dec: Multilingual Pretrained Encoder-Decoder Models. mBART, mT5.We found Enc-Dec, mT5, obtains the best performance on all datasets!Monolingual mBERT+PTR 30.63 72.18 40.40 83.82 57.47 23.46 52.53 75.41 5.87 49.09 XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 mBART 41.93 62.29 33.31 83.19 59.60 30.02 50.35 7</sample>
    <sample id="1008">Analysis of Monolingual.We evaluate on two groups of models on Monolingual Setting.Enc-PTR: Multilingual Pretrained Encoders with Pointer-based Decoders. XLM-R + PTR, mBERT + PTR.Enc-Dec: Multilingual Pretrained Encoder-Decoder Models. mBART, mT5.We found Enc-Dec, mT5, obtains the best performance on all datasets!Monolingual mBERT+PTR 30.63 72.18 40.40 83.82 57.47 23.46 52.53 75.41 5.87 49.09 XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 mBART 41.93 62.29 33.31 83.19 59.60 30.02 50.35 7</sample>
    <sample id="1009">Analysis of Monolingual.We evaluate on two groups of models on Monolingual Setting.Enc-PTR: Multilingual Pretrained Encoders with Pointer-based Decoders. XLM-R + PTR, mBERT + PTR.Enc-Dec: Multilingual Pretrained Encoder-Decoder Models. mBART, mT5.We found Enc-Dec, mT5, obtains the best performance on all datasets!Monolingual mBERT+PTR 30.63 72.18 40.40 83.82 57.47 23.46 52.53 75.41 5.87 49.09 XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 mBART 41.93 62.29 33.31 83.19 59.60 30.02 50.35 7</sample>
    <sample id="1010">Analysis of Multilingual Training.We evaluate on mT5 and XLM-R + PTR on Multilingual Setting.Enc-Dec/Enc-PTR, mT5/XLM-R, can be improved by training in a mixture of various languages.MATIS 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 58.16MGeoQuery 74.26 50.73 91.65 66.29 30.15 65.16 81.83 10.29 58.16MSpider 40.20 85.91 61.03 30.79 61.82 60.92 82.95 61.82 59.06 61.82MNLLmaps 32.30 91.31 67.55</sample>
    <sample id="1011">Analysis of Multilingual Training.We evaluate on mT5 and XLM-R + PTR on Multilingual Setting.Enc-Dec/Enc-PTR, mT5/XLM-R, can be improved by training in a mixture of various languages.Monolingual XLM-R+PTR 31.31 71.41 47.30 85.17 59.10 23.53 62.37 65.16 80.36 81.83 7.69 52.03 58.16 mT5 53.15 74.26 50.73 91.65 66.29 30.15 65.16 81.83 10.29 61.82 59.06 61.82Multilingual XLM-R+PTR 39.72 71.35 40.20 85.91 61.03 30.79 61.82</sample>
    <sample id="1012">Analysis of Multilingual Training.We evaluate on mT5 and XLM-R + PTR on Multilingual Setting.Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality".</sample>
    <sample id="1013">Analysis of Multilingual Training.We evaluate on mT5 and XLM-R + PTR on Multilingual Setting.Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality".</sample>
    <sample id="1014">Cross-lingual Performance Gap
Blue Line: Cross-lingual Few-shot transfer
Orange Line: Cross-lingual Zero-shot transfer
Green Line: Monolingual Setting
MTOP
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Geoquery
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="1015">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting MTOP Geoquery Geoquery/lamb Geoquery/prolog Geoquery/funql Geoquery/sql Spider ATIS Schema2QA Overnight NLMaps MCWQ Few-shot Zero-shot Monolingual</sample>
    <sample id="1016">Cross-lingual Performance Gap Green - orange For zero-shot setting, the cross-lingual transfer performance gap is significant Blue - orange For few-shot setting, the transfer gap is shortened rapidly</sample>
    <sample id="1017">Other Results &amp; Findings, Section 4 in Paper, Enc-Dec, mT5, outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot on target NLs. Multilingual LLMs, Codex &amp; BLOOM, are still inadequate for crosslingual semantic parsing tasks. Chinese transfer learning and English monolingual training, En -&gt; En, has the largest performance gap, while German usually has the smallest. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1018">Other Results &amp; Findings, Section 4 in Paper, Enc-Dec, mT5, outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot on target NLs. Multilingual LLMs, Codex &amp; BLOOM, are still inadequate for crosslingual semantic parsing tasks. Chinese transfer learning and English monolingual training, En -&gt; En, has the largest performance gap, while German usually has the smallest. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1019">Conclusion.- We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.- We conduct a comprehensive benchmark study on three representative types of multilingual language models.- Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1020">Conclusion.- We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations.- We conduct a comprehensive benchmark study on three representative types of multilingual language models.- Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.Links.Welcome to visit our paper and code!Paper Link: https://arxiv.org/pdf/2306.04085.pdfCode Link: https://github.com/psunlpgroup/xsemplr</sample>
    <sample id="1021">The most common errors of PaLM are "Accuracy/Omission" and "Style/Awkward". If you want to know more about these errors or have other questions, feel free to ask.</sample>
    <sample id="1048">The authors are affiliated with Emory University and the Emory NLP Research Lab.</sample>
    <sample id="1049">CFT stands for continuous fine - tuning. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="1050">There are seven authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="1084">Yusen Zhang.</sample>
    <sample id="1085">#ACL2023 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Shangbin Feng Chan Young Park Yuhan Liu Yulia Tsvetkov PAUL G. ALLEN SCHOOL UWNLP Carnegie Mellon University Language Technologies Institute Beijing University of Posts and Telecommunications</sample>
    <sample id="1086">LM Training Data.A mixed blessing.Website patents.google.com en.wikipedia.org en.m.wikipedia.org www.nytimes.com www.latimes.com www.theguardian.com journals.plos.org www.forbes.com www.huffpost.com www.scribd.com www.washingtonpost.com www.fool.com ipfs.io www.frontiersin.org www.businessinsider.com www.chicagotribune.com www.booking.com www.theatlantic.com linkedin.com www.aljazeera.com www.kickstarter.com caselaw.findlaw.com www.ncbi.nlm.nih.gov www.npr.org# tokens, log scale, 10^7 10^8 10^9.Dodge, Jesse, et al. "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.</sample>
    <sample id="1087">LM Training Data A mixed blessing Website # tokens, log scale, patents.google.com en.wikipedia.org en.m.wikipedia.org www.nytimes.com www.latimes.com www.theguardian.com journals.plos.org www.forbes.com www.huffpost.com www.scribd.com www.washingtonpost.com www.fool.com ipfs.io www.frontiersin.org www.businessinsider.com www.chicagotribune.com www.booking.com www.theatlantic.com www.aljazeera.com www.kickstarter.com caselaw.findlaw.com www.ncbi.nlm.nih.gov www.npr.org 10, 10, Dodge, Jesse, et al. "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.</sample>
    <sample id="1088">LM Training Data A mixed blessing Website patents.google.com en.wikipedia.org en.m.wikipedia.org www.nytimes.com www.latimes.com www.theguardian.com journals.plos.org www.forbes.com www.huffpost.com www.scribd.com www.washingtonpost.com www.fool.com ipfs.io www.frontiersin.org www.businessinsider.com www.chicagotribune.com www.booking.com www.theatlantic.com linkedin.com www.aljazeera.com www.kickstarter.com caselaw.findlaw.com www.ncbi.nlm.nih.gov www.npr.org # tokens, log scale, 10^7 10^8 10^9 Dodge, Jesse, et al. "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.</sample>
    <sample id="1089">LM Training Data A mixed blessing patents.google.com en.wikipedia.org en.m.wikipedia.org www.nytimes.com www.latimes.com www.theguardian.com journals.plos.org www.forbes.com www.huffpost.com www.scribd.com www.washingtonpost.com www.fool.com ipfs.io www.frontiersin.org www.businessinsider.com www.chicagotribune.com www.booking.com www.theatlantic.com linkedin.com www.aljazeera.com www.kickstarter.com caselaw.findlaw.com www.ncbi.nlm.nih.gov www.npr.org # tokens, log scale, 10^7 10^8 10^9 Dodge, Jesse, et al. "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021. To this end Pretraining data Language models Downstream tasks</sample>
    <sample id="1090">To this end Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? What role does pretraining data play in such political biases? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1091">To this end
Pretraining data
How to evaluate the political leaning of LMs?
What role does pretraining data play in such political biases?
Language models
How do LMs with different political leanings perform?
Does LM political leaning result in fairness issues in NLP applications?
Downstream tasks</sample>
    <sample id="1092">To this end
Pretraining data
How to evaluate the political leaning of LMs?
What role does pretraining data play in such political biases?
Language models
How do LMs with different political leanings perform?
Does LM political leaning result in fairness issues in NLP applications?
Downstream tasks</sample>
    <sample id="1093">Evaluating LM Political Leaning. Support both encoder and decoder LMs. "I &lt;mask&gt; with this statement." "Do you agree or disagree with this statement? &lt;statement&gt;". Automatic eval. Grounded in polisci lit.</sample>
    <sample id="1094">Existing LMs
BERT-base
BERT-large
RoBERTa-base
RoBERTa-large
distilBERT
distilRoBERTa
ALBERT-base
ALBERT-large
BART-base
BART-large
Alpaca
Authoritarian
Left
Right
Libertarian
Codex
LLaMA
GPT-2
GPT-3-ada
GPT-3-babbage
GPT-3-curie
GPT-3-davinci
ChatGPT
GPT-4
GPT-J
social axis
economic axis</sample>
    <sample id="1095">Existing LMs
BERT-base
BERT-large
RoBERTa-base
RoBERTa-large
distilBERT
distilRoBERTa
ALBERT-base
ALBERT-large
BART-base
BART-large
Alpaca
Authoritarian
Left
Right
Libertarian
Codex
LLaMA
GPT-2
GPT-3-ada
GPT-3-babbage
GPT-3-curie
GPT-3-davinci
ChatGPT
GPT-4
GPT-J
Further pretrain LM, RoBERTa, GPT-2, checkpoints, evaluate change in political leaning
Social Media, Reddit
Left
Center
Right
Shen, Qinlan, and Carolyn Rose. "Does 'right' to me? experimental factors in the perception of political ideology." Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.</sample>
    <sample id="1096">Pretraining Data Further pretrain LM, RoBERTa, GPT-2, checkpoints, evaluate change in political leaning News Media left center right Social Media, Reddit, left center right</sample>
    <sample id="1097">Pretraining Data Further pretrain LM, RoBERTa, GPT-2, checkpoints, evaluate change in political leaning News Media left center right Social Media, Reddit, left center right Liu, Yujian, et al. POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection, Findings of the Association for Computational Linguistics: NAACL 2022. Shen, Qinlan, and Carolyn Rose. What sounds "right" to me? experiential factors in the perception of political ideology, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.</sample>
    <sample id="1098">Results. Partisan shifts in LM political leaning.</sample>
    <sample id="1099">Results. Partisan shifts in LM political leaning.</sample>
    <sample id="1100">Results. Partisan shifts in LM political leaning.</sample>
    <sample id="1101">The Trump Card Pre-45th to post-45th shift news left news center news right reddit left reddit center reddit right</sample>
    <sample id="1102">The Trump Card Pre-45th to post-45th shift news left news center news right reddit left reddit center reddit right</sample>
    <sample id="1103">The Trump Card Pre-45th to post-45th shift news left news center news right reddit left reddit center reddit right</sample>
    <sample id="1104">Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1105">Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1106">Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1107">Per - Category Performance，Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color - coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1108">Per - Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color - coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1109">Per-Category PerformanceTable 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1110">Per-Category Performance Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The results are color-coded such that dark yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1111">Qualitative Analysis.TextTARGET LABEL BASE N-L S-L N-R S-Rthe alt right is corrupted with people supporting racemixing with asians and homosexuals it time for a new nationalismASIAN TRUE TRUE TRUE TRUE FALSE FALSEWhat do McDonald's and priest have in common? They both like sticking their meat in 10 year old bunsCHRIS TRUE FALSE FALSE FALSE TRUE TRUE(...) that didn't stop donald trump from seizing upon increases in isolated cases to make a case on the campaign trail that the country was in the throes of a crime epidemic crime is reaching record levels will vote for trump because they know i will stop the slaughter going on donald j trump august 29 2016(...)RIGHT FAKE FAKE FAKE FAKE TRUE FALSE(...) said sanders what is absolutely incredible to me is that water rates have soared in flint you are paying three times more for poisoned water than i m paying in burlington vermont for clean water(...)LEFT FAKE FAKE TRUE TRUE FALSE FAKEFAKETable 5: Examples of the downstream performance of tasks using language models with varying political bias.CHRIS, Base, N, S, L, R represent Christians, vanilla RoBERTa model, news media, social</sample>
    <sample id="1112">Qualitative Analysis.TextTARGET LABEL BASE N-L S-L N-R S-Rthe alt right is corrupted with people supporting racemixing with asians and homosexuals it time for a new nationalismASIAN TRUE TRUE TRUE TRUE FALSE FALSEWhat do McDonald's and priest have in common? They both like sticking their meat in 10 year old bunsCHRIS TRUE FALSE FALSE FALSE TRUE TRUE(...) that didn't stop donald trump from seizing upon increases in isolated cases to make a case on the campaign trail that the country was in the throes of a crime epidemic crime is reaching record levels will vote for trump because they know i will stop the slaughter going on donald j trump august 29 2016(...)RIGHT FAKE FAKE FAKE FAKE TRUE TRUE(...) said sanders what is absolutely incredible to me is that water rates have soared in flint you are paying three times more for poisoned water than i m paying in burlington vermont for clean water(...)LEFT FAKE FAKE TRUE TRUE FALSE FAKE FAKEExamples of the downstream performance of tasks using language models with varying political bias.CHRIS, Base, N, S, L, R represent Christians, vanilla RoBERTa model, news media, social media, left-leaning,</sample>
    <sample id="1113">Table 12: Qualitative analysis of hate speech examples where LMs with different political leanings beg to differ.，Table 13: Qualitative analysis of fake news examples where LMs with different political leanings beg to differ.</sample>
    <sample id="1114">Sure. Here's the transcription of the English content from the image:.- Table 12: Qualitative analysis of hate speech examples where LMs with different political leanings beg to differ.- Table 13: Qualitative analysis of fake news examples where LMs with different political leanings beg to differ.If you need anything else or have more questions, feel free to ask!</sample>
    <sample id="1115">Sure. Here's the transcription of the English content from the image:.- Table 12: Qualitative analysis of hate speech examples where LMs with different political leanings beg to differ.- Table 13: Qualitative analysis of fake news examples where LMs with different political leanings beg to differ.If you need more details or have any other questions, feel free to ask!</sample>
    <sample id="1116">Sure. Here's the transcription of the English content from the image:.- Table 12: Qualitative analysis of hate speech examples where LMs with different political leanings beg to differ.- Table 13: Qualitative analysis of fake news examples where LMs with different political leanings beg to differ.- Between Scylla and Charybdis.- To "sanitize" or not to "sanitize", that is the question.- Pretraining data.- Language models.- Downstream tasks.If you need more details or have any other questions, feel free to ask!</sample>
    <sample id="1117">Discussion.Between Scylla and Charybdis.To "sanitize" or not to "sanitize", that is the question.Pretraining data.Language models.Downstream tasks.</sample>
    <sample id="1118">Discussion.Between Scylla and Charybdis.To "sanitize" or not to "sanitize", that is the question.Pretraining data.Language models.Downstream tasks.</sample>
    <sample id="1119">Discussion.Between Scylla and Charybdis.To "sanitize" or not to "sanitize", that is the question.Pretraining data.Language models.Downstream tasks.Thank you!Pretraining data.Language models.Downstream tasks.Shangbin Feng.Chan Young Park.Yuhan Liu.Yulia Tsvetkov.PAUL G. ALLEN SCHOOL.UW NLP.Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="1120">Pretraining data Language models Downstream tasks Shangbin Feng Chan Young Park Yuhan Liu Yulia Tsvetkov Thank you!</sample>
    <sample id="1121">The new method has a name, it's "Permuting with 'jumps'". If you have any other questions about it, feel free to ask.</sample>
    <sample id="1122">The author described the "marked words" method as finding words that distinguish personas of marked groups from unmarked groups. If you have any other questions about this or need more details, feel free to ask!</sample>
    <sample id="1123">The affiliations are Paul G. Allen School, UW NLP, Carnegie Mellon University Language Technologies Institute, and Beijing University of Posts and Telecommunications. If you want to know more about these institutions or the paper, feel free to ask.</sample>
    <sample id="1124">Bouquet/Stanford.</sample>
    <sample id="1125">The speaker's name is not mentioned in the image.</sample>
    <sample id="1126">There are four authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="1127">BLiMP, SyntaxGym, CrowS. If you want to know more about these datasets or have other questions, feel free to ask.</sample>
    <sample id="1128">When Does Translation Require Context? A Data - driven, Multilingual Exploration. Patrick Fernandes*, Kayo Yin*, Emmy Liu André F. T. Martins, Graham Neubig. Carnegie Mellon University Language Technologies Institute. TÉCNICO LISBOA. BAIR. BERKELEY ARTIFICIAL INTELLIGENCE RESEARCH. Unbabel. * equal contribution.</sample>
    <sample id="1129">Translation depends on context. We'll have to get rid of that mole. Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context.Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context. Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics.</sample>
    <sample id="1133">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus-level metrics. Existing methods support limited discourse phenomena and languages.</sample>
    <sample id="1134">RQ1: When does translation require context?RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1135">RQ1: When does translation require context? - Word - level context usage RQ2: How well do models handle context - dependent translations?</sample>
    <sample id="1136">Conditional Cross-Mutual Information, CXMI, CXMI: measure how much context MT models use given a corpus.</sample>
    <sample id="1137">Conditional Cross-Mutual Information, CXMI, CXMI: measure how much context MT models use given a corpus. Uncertainty over translations given the source H, q, MT, A, Y | X, H, q, MT, C, Y | X, C, Uncertainty over translations given the source AND context CXMI, C → Y | X,</sample>
    <sample id="1138">Pointwise, P, -CXMI.We introduce P -CXMI to measure context usage to translate a specific.Senten P -CXMI, y, x, C, = - log, q, MT, A, y | x, q, MT, C, y | x, C,Word P -CXMI, i, y, x, C, = - log, q, MT, A, y, i | y, t &lt;, i, x, q, MT, C, y, i | y, t &lt;, i, x, C,High P -CXMI words -&gt; requires context to translate.</sample>
    <sample id="1139">RQ1: When does translation require context? - Word - level context usage - Thematic analysis RQ2: How well do models handle context - dependent translations?</sample>
    <sample id="1140">Thematic analysis of high P - CXMI words TED IDEAS WORTH SPREADING</sample>
    <sample id="1141">Thematic analysis of high P - CXMI words. 1. POS tags.</sample>
    <sample id="1142">Thematic analysis of high P-CXMI words. 1. POS tags. P-CXMI for POS tags in En-Ar. Pronouns.</sample>
    <sample id="1143">Thematic analysis of high P-CXMI words. 1. POS tags. P-CXMI for POS tags in En-Ar. PRON.3.Sing, PRON.3.Dual, PRON.3.Plur. 2. Vocabulary items. Pronouns, Verb form.</sample>
    <sample id="1144">Avelile's mother was still asleep. Avelile went to school.</sample>
    <sample id="1145">Avelile's mother was still asleep. Avelile went to school.</sample>
    <sample id="1146">Thematic analysis of high P-CXMI words. 1. POS tags. 2. Vocabulary items. 3. Individual tokens. Pronouns. Verb form. Lexical cohesion. Formality. Ellipsis.</sample>
    <sample id="1147">RQ1: When does translation require context? Word-level context usage Thematic analysis RQ2: How well do models handle context-dependent translations? Multilingual Discourse-Aware MuDA benchmark</sample>
    <sample id="1148">RQ1: When does translation require context? Word-level context usage Thematic analysis RQ2: How well do models handle context-dependent translations? Multilingual Discourse-Aware, MuDA, benchmark Multilingual Discourse-Aware, MuDA, tagger Pronouns Verb form Lexical cohesion Formality Ellipsis</sample>
    <sample id="1149">Multilingual Discourse-Aware, MuDA, tagger. Pronouns, Verb form, Lexical cohesion, Formality, Ellipsis.</sample>
    <sample id="1150">Multilingual Discourse-Aware，MuDA， tagger. Pronouns， Verb form， Lexical cohesion， Formality， Ellipsis. MuDA benchmark. MuDA tagger. BLEU COMET F-measure.</sample>
    <sample id="1151">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware, MuDA, benchmark - Model evaluation</sample>
    <sample id="1152">Corpus-level metrics. RQ1: When does translation require context? Word-level context usage, Thematic analysis. RQ2: How well do models handle context-dependent translations? Multilingual Discourse-Aware, MuDA, benchmark, Model evaluation.</sample>
    <sample id="1153">Corpus-level metrics. BLEU. CONTEXT. COMET. CONTEXT. F-measure.</sample>
    <sample id="1154">Corpus-level metrics. Unclear which system is best for document-level MT with corpus-level metrics.</sample>
    <sample id="1155">Corpus-level metrics. BLEU, COMET, F-measure. Unclear which system is best for document-level MT with corpus-level metrics. MuDA benchmark results. Context-aware models perform significantly better on some phenomena. Formality, lexical cohesion.</sample>
    <sample id="1156">MuDA benchmark results. Context-aware models perform significantly better on some phenomena. Formality, lexical cohesion. Ellipsis, pronouns, verb form.</sample>
    <sample id="1157">MuDA benchmark results. Context-aware models perform significantly better on some phenomena. Formality, lexical cohesion. Ellipsis, pronouns, verb form. DeepL outperforms Google on most phenomena and language pairs.</sample>
    <sample id="1158">MuDA benchmark results. Context-aware models perform significantly better on some phenomena. Formality, lexical cohesion. Ellipsis, pronouns, verb form. DeepL outperforms Google on most phenomena and language pairs. Summary. Identify discourse phenomena systematically without prior linguistic knowledge. Formality, lexical cohesion. Ellipsis, pronouns, verb form. Dataset-agnostic benchmark for document-level MT. DeepL outperforms Google on most phenomena and language pairs.</sample>
    <sample id="1159">Summary.- Identify discourse phenomena systematically without prior linguistic knowledge.- Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1160">Summary. Identify discourse phenomena systematically without prior linguistic knowledge. Dataset-agnostic benchmark for document-level MT.</sample>
    <sample id="1161">FTw, BOND, COSINE, MLC, L2R. If you have any other questions about this, feel free to ask.</sample>
    <sample id="1162">The model is evaluated on 11 tasks. If you want to know more about these tasks, feel free to ask.</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1164">DEPLAIN: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023 1. Text Simplification What, why and How?</sample>
    <sample id="1165">Text Simplification Example</sample>
    <sample id="1166">Text Simplification Example</sample>
    <sample id="1167">Text Simplification Example</sample>
    <sample id="1168">Text Simplification Example</sample>
    <sample id="1169">German Text Simplification Corpora. Sentence Level. DE-plain. A New Corpus.</sample>
    <sample id="1170">German Text Simplification Corpora. Sentence Level.</sample>
    <sample id="1171">German Text Simplification Corpora</sample>
    <sample id="1172">German Text Simplification Corpora. Sentence Level.</sample>
    <sample id="1173">German Text Simplification Corpora</sample>
    <sample id="1174">German Text Simplification Corpora. Sentence Level.</sample>
    <sample id="1175">Types of Simplification.Simplicity LexSimp StructSimp.news n = 46 bible n = 155 L2 n = 157 fiction n = 72.Simplification Transformations.reordering rephrasing lexical substitution word addition word deletion.DEplain-apa DEplain-web.</sample>
    <sample id="1176">Types of Simplification.Simplicity LexSimp StructSimp.news n = 46 bible n = 155 L2 n = 157 fiction n = 72.Simplification Transformations.reordering rephrasing lexical substitution word addition word deletion.DEplain-apa DEplain-web.</sample>
    <sample id="1177">Types of Simplification.Simplicity LexSimp StructSimp.news n = 46 bible n = 155 L2 n = 157 fiction n = 72.Simplification Transformations.reordering rephrasing lexical substitution word addition word deletion.DEplain-apa DEplain-web.</sample>
    <sample id="1178">Types of Simplification.Simplicity LexSimp StructSimp.news n = 46 bible n = 155 L2 n = 157 fiction n = 72.Simplification Transformations.DEplain-apa DEplain-web.reordering rephrasing lexical substitution word addition word deletion.</sample>
    <sample id="1179">Types of Simplification.Simplicity LexSimp StructSimp.news n = 46 bible n = 155 L2 n = 157 fiction n = 72.Simplification Transformations.reordering rephrasing lexical substitution word addition word deletion.DEplain-apa DEplain-web.</sample>
    <sample id="1180">3. Use-cases Automatic alignment and simplification.</sample>
    <sample id="1181">Sure. The text content is: "Automatic Alignment Evaluation Results of the alignment methods with 1:1, upper part, and n:m capabilities, lower part. Name Description LHA Hierarchical alignment using sentence embeddings similarity Sent-LaBSE Similar embeddings of Language-agnostic BERT transformer Sent-RoBERTa Similar embeddings of Cross English German RoBERTa CATS-C3G Different similarity measures e.g n-grams, C3G, word vectors VecAlign Multilingual aligner based on multilingual sentence embeddings BERTalign Allows sentence-transformer methods produce n:m alignments MASSalign A vicinity-driven approach with a TF-IDF similarity matrix P R F1 F0.5 P R F1 F0.5 LHA 0.94 0.41 0.57 0.747 - - - - Sent-LaBSE 0.961 0.444 0.608 0.780 - - - - Sent-RoBERTa 0.960 0.444 0.607 0.779 - - - - CATS-C3G 0.247 0.553 0.</sample>
    <sample id="1182">Sure. Here's the text from the image:.Automatic Alignment EvaluationResults of the alignment methods with 1:1, upper part, and n:m capabilities, lower part,Name Description 1:1 n:m P R F1 F0.5 P R F1 F0.5 LHA Hierarchical alignment using sentence embeddings similarity.94.41.57.747 - - - - Sent-LaBSE Similar embeddings of Language-agnostic BERT transformer.961.444.608.780 - - - - Sent-RoBERTa Similar embeddings of Cross English German RoBERTa.960.444.607.779 - - - - CATS-C3G Different similarity measures e.g n-grams, C3G, word vectors.247.553.342.278 - - - - VecAlign Multilingual aligner based on multilingual sentence embeddings.271.404.323.290.260.465.333.285 BERTalign Allows sentence-transformer methods produce n:m alignments.743.465</sample>
    <sample id="1183">Sure. The text content is: 'Automatic Alignment Evaluation'. Results of the alignment methods with 1:1, upper part, and n:m capabilities, lower part. Name Description P R F1 F0.5 P R F1 F0.5 LHA Hierarchical alignment using sentence embeddings similarity 0.94 0.41 0.57 0.747 - - - - Sent-LaBSE Similar embeddings of Language-agnostic BERT transformer 0.961 0.444 0.608 0.780 - - - - Sent-RoBERTa Similar embeddings of Cross English German RoBERTa 0.960 0.444 0.607 0.779 - - - - CATS-C3G Different similarity measures e.g n-grams C3G/word vectors 0.247 0.553 0.342 0.278 - - - - VecAlign Multilingual aligner based on multilingual sentence embeddings 0.271 0.404 0.323 0.290 0.2</sample>
    <sample id="1184">Sure. The text content is: 'Automatic Alignment Evaluation'. Results of the alignment methods with 1:1, upper part, and n:m capabilities, lower part. Name Description P R F1 F0.5 P R F1 F0.5 LHA Hierarchical alignment using sentence embeddings similarity 0.94 0.41 0.57 0.747 - - - - Sent-LaBSE Similar embeddings of Language-agnostic BERT transformer 0.961 0.444 0.608 0.780 - - - - Sent-RoBERTa Similar embeddings of Cross English German RoBERTa 0.960 0.444 0.607 0.779 - - - - CATS-C3G Different similarity measures e.g n-grams C3G/word vectors 0.247 0.553 0.342 0.278 - - - - VecAlign Multilingual aligner based on multilingual sentence embeddings 0.271 0.404 0.323 0.290 0.2</sample>
    <sample id="1185">Sure. The text content is: 'Automatic Alignment Evaluation'. Results of the alignment methods with 1:1, upper part, and n:m capabilities, lower part. Name Description LHA Hierarchical alignment using sentence embeddings similarity Sent-LaBSE Similar embeddings of Language-agnostic BERT transformer Sent-RoBERTa Similar embeddings of Cross English German RoBERTa CATS-C3G Different similarity measures e.g n-grams, C3G, word vectors VecAlign Multilingual aligner based on multilingual sentence embeddings BERTalign Allows sentence-transformer methods produce n:m alignments MASSalign A vicinity-driven approach with a TF-IDF similarity matrix P R F1 F0.5 P R F1 F0.5 LHA 0.94 0.41 0.57 0.747 - - - - Sent-LaBSE 0.961 0.444 0.608 0.780 - - - - Sent-RoBERTa 0.960 0.444 0.607 0.779 - - - - CATS-C3G 0.247 0.553 0</sample>
    <sample id="1186">Sure. Here's the text from the image:.Automatic Alignment EvaluationResults of the alignment methods with 1:1, upper part, and n:m capabilities, lower part,Name Description 1:1 n:m P R F1 F0.5 P R F1 F0.5 LHA Hierarchical alignment using sentence embeddings similarity .94 .41 .57 .747 - - - - Sent-LaBSE Similar embeddings of Language-agnostic BERT transformer .961 .444 .608 .780 - - - - Sent-RoBERTa Similar embeddings of Cross English German RoBERTa .960 .444 .607 .779 - - - - CATS-C3G Different similarity measures e.g n-grams, C3G, word vectors .247 .553 .342 .278 - - - - VecAlign Multilingual aligner based on multilingual sentence embeddings .271 .404 .323 .290 .260 .465 .333 .285 BERTalign Allows sentence-transformer methods produce n:m alignments .743 .465</sample>
    <sample id="1187">Automatic Alignment EvaluationResults of the alignment methods with 1:1, upper part, and n:m capabilities, lower part, Name, Description, LHA, Hierarchical alignment using sentence embeddings similarity, Sent-LaBSE, Similar embeddings of Language-agnostic BERT transformer, Sent-RoBERTa, Similar embeddings of Cross English German RoBERTa, CATS-C3G, Different similarity measures e.g n-grams, C3G, word vectors, VecAlign, Multilingual aligner based on multilingual sentence embeddings, BERTalign, Allows sentence-transformer methods produce n:m alignments, MASSalign, A vicinity-driven approach with a TF-IDF similarity matrix, 1:1, P, R, F, F, F, 0.5, 0.5, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -,</sample>
    <sample id="1188">Automatic Text Simplification.Document LevelResults on Document Simplification using finetuned long-mBART.n corresponds to the length of the training data.train dataDEplain-APAn38744.5638.1360.59865.4DEplain-web48135.0212.9130.47559.55DEplain-APA+web86842.86236.4490.58965.4src2Src-baseline17.63734.2470.58358.5DEPLAIN-APAn test, n = 48DEPLAIN-WBn test, n = 147Sentence LevelResults on Sentence Simplification using finetuned mBART.n corresponds to the length of the training data.train dataDEplain-APAn1066034.81828.2560.63963.072DEplain-APA+web1194134.90428.5060.64266.6</sample>
    <sample id="1189">Automatic Text SimplificationDocument LevelResults on Document Simplification using finetuned long-mBART.n corresponds to the length of the training data.train dataDEplain-APAn38744.5638.1360.59865.4DEplain-web48135.0212.9130.47559.55DEplain-APA + web86842.86236.4490.58965.4src2Src-baseline17.63734.2470.58358.85DEplain-APAn38743.08721.90.37764.7DEplain-web48149.58423.2820.46263.5DEplain-APA + web86849.74523.3370.44557.95src2Src-baseline12.84823.1320.43259.4DEPLAIN-</sample>
    <sample id="1190">Automatic Text SimplificationDocument LevelResults on Document Simplification using finetuned long-mBART. n corresponds to the length of the training data.train dataDEplain-APAn38744.5638.1360.59865.4DEplain-web48135.0212.9130.47559.55DEplain-APA+web86842.86236.4490.58965.4src2Src-baseline17.63734.2470.58358.85DEplain-APAn48DEplain-web481DEplain-APA+web868src2src-baseline12.84823.1320.43259.4Sentence LevelResults on Sentence Simplification using finetuned mBART. n corresponds to the length of the training data.train dataDEplain-APAn1066034.81828.250.63963.072DEplain-APA+web1</sample>
    <sample id="1191">Automatic Text SimplificationDocument LevelResults on Document Simplification using finetuned long-mBART. n corresponds to the length of the training data.train dataDEplain-APAn38744.5638.1360.59865.4DEplain-web48135.0212.9130.47559.55DEplain-APA+web86842.86236.4490.58965.4src2Src-baseline17.63734.2470.58358.85DEplain-APAn38743.08721.90.37764.7DEplain-web48149.58423.2820.46263.5DEplain-APA+web86849.74523.3370.44557.95src2Src-baseline12.84823.1320.43259.4DEPLAIN</sample>
    <sample id="1192">Automatic Text SimplificationDocument LevelResults on Document Simplification using finetuned long-mBART.n corresponds to the length of the training data.train dataDEplain-APAn38744.5638.1360.59865.4DEplain-web48135.0212.9130.47559.55DEplain-APA+web86842.86236.4490.58965.4src2Src-baseline17.63734.2470.58358.85DEplain-APAn38743.08721.90.37764.7DEplain-web48149.58423.2820.46263.5DEplain-APA+web86849.74523.3370.44557.95src2Src-baseline12.84823.1320.43259.4DEPLAIN-</sample>
    <sample id="1193">Automatic Text Simplification.Document Level.Results on Document Simplification using finetuned long-mBART. n corresponds to the length of the training data.train dataDEplain-APAn44.5638.1360.59865.4DEplain-webn35.0212.9130.47559.55DEplain-APA+webn42.86236.4490.58965.4src2Src-baselin17.63734.2470.58358.5DEPLAIN-APAn48DEPLAIN-APAn48Sentence Level.Results on Sentence Simplification using finetuned mBART. n corresponds to the length of the training data.train dataDEplain-APAn34.81828.2520.63963.072DEplain-APAn34.90428.5060.64662.669DEplain-APA+webn15.24926.8930.627</sample>
    <sample id="1194">Automatic Text SimplificationDocument LevelResults on Document Simplification using finetuned long-mBART.n corresponds to the length of the training data.train dataDEplain-APAn38744.5638.1360.59865.4DEplain-web48135.0212.9130.47559.55DEplain-APA+web86842.86236.4490.58965.4src2Src-baseline17.63734.2470.58358.85DEPLAIN-APAn test, n = 48DEPLAIN-WBn test, n = 147train dataDEplain-APAn38743.08721.90.37764.7DEplain-web48149.58423.2820.46263.5DEplain-APA+web86849.74523.3370.44557.95src2</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, Google Research.</sample>
    <sample id="1197">Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, Google Research.</sample>
    <sample id="1198">Indirect Referring ExpressionsGoal: Understanding users' language when they make a choice.Alternative question: Did you mean easy on me or I gotta feeling?Direct reference: easy on me, the first one.Indirect reference could be used in natural and fluid conversation: Cannot remember the name. The pronunciations are hard to distinguish. Want to specify a preference.Indirect reference: The newer one. The song that's not energetic.</sample>
    <sample id="1199">Indirect Referring ExpressionsGoal: Understanding users' language when they make a choice.Alternative question Did you mean easy on me or I gotta feeling?Direct reference: "easy on me", "the first one"Indirect reference could be used in natural and fluid conversation: Cannot remember the name The pronunciations are hard to distinguish Want to specify a preferenceIndirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1200">Indirect Referring ExpressionsGoal: Understanding users' language when they make a choice.Alternative question: Did you mean easy on me or I gotta feeling?Direct reference: easy on me, the first one.Indirect reference could be used in natural and fluid conversation: Cannot remember the name. The pronunciations are hard to distinguish. Want to specify a preference.Indirect reference: The newer one. The song that's not energetic.</sample>
    <sample id="1201">Indirect Referring Expressions. Goal: Understanding users' language when they make a choice. Alternative question: Did you mean easy on me or I gotta feeling? Direct reference: easy on me, the first one. Indirect reference could be used in natural and fluid conversation: Cannot remember the name, The pronunciations are hard to distinguish, Want to specify a preference. Indirect reference: The newer one, The song that's not energetic.</sample>
    <sample id="1202">Indirect Referring ExpressionsGoal: Understanding users' language when they make a choice.Alternative question: Did you mean easy on me or I gotta feeling?Direct reference: easy on me, the first one.Indirect reference could be used in natural and fluid conversation: Cannot remember the name. The pronunciations are hard to distinguish. Want to specify a preference.Indirect reference: The newer one. The song that's not energetic.</sample>
    <sample id="1203">Dataset Collection. Important problem: Conversational systems, Benchmarking Large Language Models' entity understanding. No large-scale public dataset available. We collect a large dataset using crowd annotation. Three domains:</sample>
    <sample id="1204">Dataset Collection. Important problem: Conversational systems, Benchmarking Large Language Models' entity understanding. No large-scale public dataset available. We collect a large dataset using crowd annotation. Three domains:</sample>
    <sample id="1205">Dataset Collection Methodology. Methodology emphasizes informality using a cartoon completion task. Sets the dialog context, chosen from a few manual prompts per domain. The alternative question. Expression referring to one of the entities. Filled in by the annotator.</sample>
    <sample id="1206">Dataset Collection Methodology.Methodology emphasizes informality using a cartoon completion task.Sets the dialog context, chosen from a few manual prompts per domain.The alternative question.Expression referring to one of the entities.Filled in by the annotator.</sample>
    <sample id="1207">Dataset Collection Methodology. Methodology emphasizes informality using a cartoon completion task. Sets the dialog context, chosen from a few manual prompts per domain. The alternative question. Expression referring to one of the entities. Filled in by the annotator.</sample>
    <sample id="1208">Dataset Collection Methodology.Methodology emphasizes informality using a cartoon completion task.Sets the dialog context, chosen from a few manual prompts per domain.The alternative question.Expression referring to one of the entities.Filled in by the annotator.</sample>
    <sample id="1209">Dataset Collection Methodology.Methodology emphasizes informality using a cartoon completion task.Sets the dialog context, chosen from a few manual prompts per domain.The alternative question.Expression referring to one of the entities.Filled in by the annotator.</sample>
    <sample id="1210">Dataset Collection Methodology.Methodology emphasizes informality using a cartoon completion task.Sets the dialog context, chosen from a few manual prompts per domain.The alternative question.Expression referring to one of the entities.Filled in by the annotator.Items with similar infoboxes or Wikipedia, same genre and/or artist.Items with similar descriptions on Wikipedia.Items with similar titles.Uniformly random.Do you mean A or B?Do you mean This is it or Man in the Mirror?Do you mean Thinking of You or Happy Anywhere?Do you mean The Return, memoir, or The Return, Shatner novel.Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1211">Generate alternative questions =&gt; sampling entity pairs.Do you mean A or B?Items with similar infoboxes on Wikipedia, same genre and/or artist.Do you mean 'This is It' or 'Man in the Mirror'?Items with similar descriptions on Wikipedia.Do you mean 'Thinking of You' or 'Happy Anywhere'?Items with similar titles.Do you mean The Return, memoir, or The Return, Shatner novel?Uniform at random.Do you mean 'You Could Be Mine' or 'The Way I Am'?Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="1212">Generate alternative questions =&gt; sampling entity pairs.Do you mean A or B?Items with similar infoboxes on Wikipedia, same genre and/or artist.Do you mean 'This Is It' or 'Man in the Mirror'?Items with similar descriptions on Wikipedia.Do you mean 'Thinking of You' or 'Happy Anywhere'?Items with similar titles.Do you mean The Return, memoir, or The Return, Shatner novel?Uniform at random.Do you mean 'You Could Be Mine' or 'The Way I Am'?Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="1213">Generate alternative questions =&gt; sampling entity pairs.Do you mean A or B?Items with similar infoboxes on Wikipedia, same genre and/or artist.Do you mean 'This Is It' or 'Man in the Mirror'?Items with similar descriptions on Wikipedia.Do you mean 'Thinking of You' or 'Happy Anywhere'?Items with similar titles.Do you mean The Return, memoir, or The Return, Shatner novel?Uniform at random.Do you mean 'You Could Be Mine' or 'The Way I Am'?Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="1214">Generate alternative questions =&gt; sampling entity pairs.Do you mean A or B?Items with similar infoboxes on Wikipedia, same genre and/or artist.Do you mean 'This Is It' or 'Man in the Mirror'?Items with similar descriptions on Wikipedia.Do you mean 'Thinking of You' or 'Happy Anywhere'?Items with similar titles.Do you mean The Return, memoir, or The Return, Shatner novel?Uniform at random.Do you mean 'You Could Be Mine' or 'The Way I Am'?Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="1215">Generate alternative questions =&gt; sampling entity pairs.Do you mean A or B?Items with similar infoboxes on Wikipedia, same genre and/or artist.Do you mean 'This Is It' or 'Man in the Mirror'?Items with similar descriptions on Wikipedia.Do you mean 'Thinking of You' or 'Happy Anywhere'?Items with similar titles.Do you mean The Return, memoir, or The Return, Shatner novel?Uniform at random.Do you mean 'You Could Be Mine' or 'The Way I Am'?Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="1216">Background knowledge, Music, Google search link to each song. Easy on Me, by Adele, Click here to find out about the song. I Gotta Feeling, by The Black Eyed Peas, Click here to find out about the song. We ask annotators to, Listen to at least some of each song, Read about each song. Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus,</sample>
    <sample id="1217">Background knowledge, Music, Google search link to each song. Easy on Me, by Adele, Click here to find out about the song. I Gotta Feeling, by The Black Eyed Peas, Click here to find out about the song. We ask annotators to, Listen to at least some of each song, Read about each song. Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus, P 6.</sample>
    <sample id="1218">Background knowledge, Music, Google search link to each song. Easy on Me, by Adele, Click here to find out about the song. I Gotta Feeling, by The Black Eyed Peas, Click here to find out about the song. We ask annotators to, Listen to at least some of each song, Read about each song.</sample>
    <sample id="1219">Background knowledge, Recipes, Simnel Cake, Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste or marzipan, and a set of eleven balls made of the same paste. Pandan Cake, Pandan cake is a light, fluffy, green - coloured sponge cake flavoured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia, and also the Netherlands, especially among the Indo community.</sample>
    <sample id="1220">Eliciting expressions.We then tell the annotators which choice should be selected and ask them to describe it.Pick this oneEasy on Me, by Adele, I Gotta Feeling, by the Black Eyed Peas.We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example:The one with the piano music.The song that's not energetic.It has something about a river.The newer one.It's about not having time to choose.Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="1221">Eliciting expressions. We then tell the annotators which choice should be selected and ask them to describe it. Pick this one. Easy on Me, by Adele. I Gotta Feeling, by the Black Eyed Peas. We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example: The one with the piano music. The song that's not energetic. It has something about a river. The newer one. It's about not having time to choose. Random Examples. Music Selection. Do you mean 'Chime' or 'Your Loving Arms'? The one without words. Do you mean 'These Kids' or 'Inescapable'? It is the song sung by an Australian. Do you mean 'Rock the Boat' or 'Wherever You Are'? It has synthesizer sounds in it. Do you mean 'Telepathy' or 'Stars on 45'? Came out in mid of 2000. Do you mean 'Mis-Shapes' or 'Remind Me'? Based on life experienced in Sheffield. Book Selection. Do you mean 'Warlock, Hall novel' or 'Warlock, Smith novel'? The one that is set in the</sample>
    <sample id="1222">AltEntities Corpus.- ~6,000 alternative questions across the three domains.- ~42,000 indirect referring expressions.- Results with T5 XL model, accuracy:.- 92-95% if the LM has access to the same background knowledge as annotators.- 82%-87% when the the LM has access to partially overlapping background knowledge.- ~60% when the LM, T5 XL, has only access to the entity names.- We showed models are domain-generalizable.- Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1223">AltEntities Corpus.- ~6,000 alternative questions across the three domains.- ~42,000 indirect referring expressions.- Results with T5 XL model, accuracy:.- 92-95% if the LM has access to the same background knowledge as annotators.- 82%-87% when the the LM has access to partially overlapping background knowledge.- ~60% when the LM, T5 XL, has only access to the entity names.- We showed models are domain-generalizable.- Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1224">AltEntities Corpus.- ~6,000 alternative questions across the three domains.- ~42,000 indirect referring expressions.- Results with T5 XL model, accuracy:.- 92-95% if the LM has access to the same background knowledge as annotators.- 82%-87% when the the LM has access to partially overlapping background knowledge.- ~60% when the LM, T5 XL, has only access to the entity names.- We showed models are domain-generalizable.- Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1225">AltEntities Corpus.- ~6,000 alternative questions across the three domains.- ~42,000 indirect referring expressions.- Results with T5 XL model, accuracy:.- 92-95% if the LM has access to the same background knowledge as annotators.- 82%-87% when the the LM has access to partially overlapping background knowledge.- ~60% when the LM, T5 XL, has only access to the entity names.- We showed models are domain-generalizable.- Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1226">CamemBERT is initially trained on 4GB of data. If you want to know more about how this data is used in training, feel free to ask.</sample>
    <sample id="1227">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="1228">Well, it seems like the findings that showed performance degrades with larger temporal gaps led to the conclusion that temporal drift is the main cause of performance loss. You know, it's like when there's a big gap between the data the model was trained on and the new data it has to work with, that's when the performance drops. So, yeah, that's the key finding here. If you want to know more about how they figured that out, just let me know.</sample>
    <sample id="1229">NLPositionality: Characterizing Design Biases of Datasets and Models.</sample>
    <sample id="1230">NLPositionality: Characterizing Design Biases of Datasets and Models Sebastin Santy* University of Washington Jenny T. Liang* Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine...</sample>
    <sample id="1233">Imagine.</sample>
    <sample id="1234">Imagine... Design bias example! Can you stop being a jerk? 👎: 0.82 ✅ Presstitutes everywhere on the news. 👎: 0.33 ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India 📈 = PerspectiveAPI score</sample>
    <sample id="1235">Positionality "The perspectives people hold as a result of their demographics, identity, and life experiences."</sample>
    <sample id="1236">Positionality.The perspectives people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1237">Positionality.The perspectives people hold as a result of their demographics, identity, and life experiences.As a researcher, it influences the research process and its outcomes and results.</sample>
    <sample id="1238">[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.，[2] Yin et al. "GEOMLAMA: Geo - Diverse Commonsense Probing on Multilingual Pre - Trained Language Models." EMNLP 2022.，[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1239">[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.，[2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.，[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1240">Do datasets and models have positionality?Anecdotal evidence:Model and dataset probing, Theoretical definitions of model positionality[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.[2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1241">Do datasets and models have positionality?Anecdotal evidence:.- Model and dataset probing, 1, 2, - Theoretical definitions of model positionality, 3, [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.[2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre - Trained Language Models." EMNLP 2022.[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1242">Do datasets and models have positionality?Anecdotal evidence:.- Model and dataset probing, 1, 2, - Theoretical definitions of model positionality, 3, [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.[2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre - Trained Language Models." EMNLP 2022.[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1243">Do datasets and models have positionality?Anecdotal evidence:.- Model and dataset probing, 1, 2, - Theoretical definitions of model positionality, 3, [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.[2] Yin et al. "GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre - Trained Language Models." EMNLP 2022.[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022.</sample>
    <sample id="1244">Question: Do datasets and models have positionality? Goal: Compare annotations from users with existing datasets and models.</sample>
    <sample id="1245">NLPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">Collection: 300 instances are sampled from a dataset. Each instance has an associated gold label. Processing: Instances are sent as part of a study on LITW. Annotations from people around the world. The received annotations are compared against the gold labels from the dataset and predictions obtained from the models. Analysis: Pearson's r correlation is measured between gold labels, model predictions and annotations for each of the demographics separately.</sample>
    <sample id="1247">Collection: 300 instances are sampled from a dataset. Each instance has an associated gold label. Processing: Instances are sent as part of a study on LITW. Annotations from people around the world. The received annotations are compared against the gold labels from the dataset and predictions obtained from the models. Analysis: Pearson's r correlation is measured between gold labels, model predictions and annotations for each of the demographics separately.</sample>
    <sample id="1248">Framework.</sample>
    <sample id="1249">Framework. Collection: 300 instances are sampled from a dataset. Each instance has an associated gold label. Processing: Instances are sent as part of a study on LITW. Annotations from people around the world. Model Predictions. Analysis: Pearson's r correlation is measured between gold labels, model predictions and annotations for each of the demographics separately.</sample>
    <sample id="1250">Collection: 300 instances are sampled from a dataset. Each instance has an associated gold label. Processing: The received annotations are compared against the gold labels from the dataset and predictions obtained from the models. Analysis: Pearson's r correlation is measured between gold labels, model predictions and annotations for each of the demographics separately.</sample>
    <sample id="1251">Collection: 300 instances are sampled from a dataset. Each instance has an associated gold label. Processing: The received annotations are compared against the gold labels from the dataset and predictions obtained from the models. Analysis: Pearson's r correlation is measured between gold labels, model predictions and annotations for each of the demographics separately.</sample>
    <sample id="1252">LabintheWild.</sample>
    <sample id="1253">LabintheWild
Our Experiments Findings Data Sets Blog For Researchers About Us English
5,376,396 total participants
Pool of diverse volunteers / research participants
Online experiment from researchers
Could you live with an AI and its morals? Tell us your moral judgments on certain situations and we will show you how you compare to others' and an AI.
Are you better than an AI in noticing hateful speech? Rate what speech is hateful and we'll show you how well you detect hateful speech compared to an AI and others.
Where are you on the techno-skeptic to techno-utopian scale? Tell us how you think future mixed reality technology will affect your personal life, we will show you whether you are more techno-skeptic or techno-utopian.
Participate now!
Participate now!
Participate now!</sample>
    <sample id="1254">Task A: Social Acceptability.</sample>
    <sample id="1255">Task A: Social Acceptability.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity. Participants read an instance from the Dynahate dataset. Participants rate whether they think an instance is hate speech. 1) Read the example: English people smell like sour milk. 2) Enter what you think about it. Do you think this is hateful speech? Yes, No, Not sure. Feel free to explain your rating. Enter a short description. Submit. 3) See what others thought about it! The AI speculates: I speculate that this is hateful speech. Looks like we agree! Study participants in Afghanistan said: Not Hateful, Not Sure, Hateful.</sample>
    <sample id="1258">Task B: Toxicity Analysis Datasets - Dynahate Models - Perspective API - Rewire API - Hate RoBERTa - GPT-4 Study Participation 16,299 annotations 1,096 annotators 87 countries</sample>
    <sample id="1259">Finding 1: There is positionality in NLP.</sample>
    <sample id="1260">Datasets and models are most aligned to English-Speaking countries.</sample>
    <sample id="1261">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1262">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1263">Finding 2: Some populations are left behind.</sample>
    <sample id="1264">Social Acceptability, GPT-4, Datasets and models are less aligned to non-binary people. Man N=4,082 0.69*, Non-binary N=858 0.55*, Woman N=4,368 0.73*, Hate Speech Toxicity, Dynahate, Datasets and models are less aligned to non-binary people. Man N=1,798 0.65*, Non-binary N=329 0.58*, Woman N=2,357 0.63*,</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism:. a. Share disaggregated dataset labels! b. Use modeling techniques that can handle annotator disagreement.</sample>
    <sample id="1267">Recommendations. 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: a. Share disaggregated dataset labels! b. Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with and for specific communities is valuable for inclusive NLP, e.g., Masakhane initiative.</sample>
    <sample id="1268">Thanks!Dashboard Link: nlpositionality.cs.washington.edu/Paper: bit.ly/NLPositionality-Paper/Delphihttps://delechi.allenai.org/</sample>
    <sample id="1269">Well, you see, permuting the tokens for the output sequence is necessary because it helps in generating a more diverse and accurate output. It allows the model to explore different possible arrangements of the tokens, which can lead to better understanding and representation of the data. This is especially important in tasks like machine translation or text generation where the order of words matters a great deal. So, it's all about improving the quality and versatility of the output. If you have any more questions about this or anything else, feel free to ask!</sample>
    <sample id="1270">Well, it's because transparency about bias mitigation methods helps in understanding how the model is being made fair. It allows others to see the steps taken to reduce bias, which is really important for building trust. And it also helps in improving the model further. So, yeah, that's why they recommended it. What do you think about this?</sample>
    <sample id="1271">In BLiMP, "Many people were helping themselves." and "Many people were helping herself." are minimal-pair unacceptable inputs. In SyntaxGym, "No customer ... has spent any money." and "The customer ... has spent any money." are minimal-pair unacceptable inputs. In Crows, "Stereotypical sentence." and "Non-stereotypical sentence." are minimal-pair unacceptable inputs. If you want to know more about this, feel free to ask me.</sample>
    <sample id="1272">The authors used F1, CLS, NER, POS, Hamming, and EMR as evaluation metrics. If you want to know more about these metrics or have other questions, feel free to ask.</sample>
    <sample id="1273">Krippendorff's Alpha was used for measuring inter-annotator agreement. If you want to know more about this metric or have other questions, feel free to ask!</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">The affiliations of the authors are Heinrich Heine University Düsseldorf, Germany. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="1276">MultiInstruct has a large scale and is publicly available.</sample>
    <sample id="1277">There are three authors involved in the paper. If you have any other questions about it, feel free to ask.</sample>
    <sample id="1278">Binary coordination is when two elements are coordinated together. It's a basic form of coordination in linguistics. If you want to know more about it, like how it's used in different languages, just let me know.</sample>
    <sample id="1279">The image doesn't provide information about the length of the prompts used in the study. So, I can't answer that. If you have any other questions about the study or the image, feel free to ask.</sample>
    <sample id="1280">Well, the findings suggest that the smaller T5 model, when fine - tuned on Coscript, can generate higher quality scripts than larger LLMs. This implies that for certain tasks, like script generation, smaller models can be more effective if they are properly trained on the right data. It also shows that there might be a lot of potential in using specialized data for training models. So, it could be a good direction for future research and development in this area. What do you think about this? Do you have any other questions related to this?</sample>
    <sample id="1309">The learning strategies investigated are "From scratch with full model construction" and "Continual pre-training using an existing pre-trained model." If you want to know more about these strategies, feel free to ask.</sample>
    <sample id="1310">The factor of overfitting due to test reuse is not specified in the image. But it's a common issue in machine learning. You might need to look at the data or the research behind the image for more details. If you have any other questions about this or something else, feel free to ask.</sample>
    <sample id="1311">The quality of the simplification was evaluated using SARI, BLEU, BS-P, and FRE. If you want to know more about these metrics, feel free to ask.</sample>
    <sample id="1312">Yes, they do. Some are more on the left side, like BERT-base and BERT-large, while others are more on the right, like GPT-3-curie and GPT-4. There are also models in the center, like ALBERT-large. So, it seems they do have different political biases. If you want to know more about how they were determined, just let me know.</sample>
    <sample id="1347">Cognitive dissonance is when two elements of cognition, like thoughts, actions, or beliefs, are inconsistent. It's a concept in psychology. If you want to know more about it, feel free to ask.</sample>
    <sample id="1348">GPT-J is the most liberal language model according to the chart. If you have any other questions about this or anything else, feel free to ask!</sample>
    <sample id="1349">No, iterative training performs better than cumulative in active learning according to the chart. If you have more questions about this, feel free to ask.</sample>
    <sample id="1350">The name of the speaker is Sara Papi. If you want to know more about her or the topic, feel free to ask.</sample>
    <sample id="1351">The data was taken from TED Talks in the MuDa benchmark. If you want to know more about the MuDa benchmark or TED Talks, feel free to ask.</sample>
    <sample id="1352">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination. Adam Przepiórkowski and Michał Woźniak. Institute of Computer Science, Polish Academy of Sciences, ul. Jana Kazimierza 5, 01-248 Warsaw. UNIVERSITY OF WARSAW. ACL 2023.</sample>
    <sample id="1353">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1354">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1356">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1357">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1358">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1360">Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie. Chain/Moscow, Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie. Multi-headed/London, Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1361">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1362">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths.</sample>
    <sample id="1363">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1364">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1365">Word order tends to minimize dependency lengths:.- Marge read it yesterday. good.- Marge read yesterday it. bad.- Marge read this absolutely fascinating book about bees yesterday. good.- Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1366">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1367">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1368">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1369">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1370">Dependency Length Minimization, DLM, Word order tends to minimize dependency lengths. Marge read it yesterday. good. Marge read yesterday it. bad. Marge read this absolutely fascinating book about bees yesterday. good. Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1371">Word order tends to minimize dependency lengths:.- Marge read it yesterday. good.- Marge read yesterday it. bad.- Marge read this absolutely fascinating book about bees yesterday. good.- Marge read yesterday this absolutely fascinating book about bees. good.</sample>
    <sample id="1372">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa, Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1373">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88–90, but only when the governor is on the left or absent, I saw Bart and Lisa; Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1374">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa, Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1375">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa, Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa, Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1377">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa, Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1378">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa; Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1379">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank, Marcus et al. 1993, Ficler and Goldberg 2016, left conjuncts tend to be shorter, observed before, this tendency grows with length difference, briefly noticed in Gibson et al. 1996: 88-90, but only when the governor is on the left or absent, I saw Bart and Lisa, Homer came and sneezed, not when it is on the right, Ted and Ned laughed.</sample>
    <sample id="1380">NO governor, length in CHARACTERS, slope: 3.99e+03, p: 1.36e-13, proportion of shorter left conjuncts, absolute difference in characters, 0, 50, 100, 150, 200, 0.4, 0.5, 0.6, 0.7, 0.8, NO governor, length in SYLLABLES, slope: 1.25e+02, p: 1.86e-13, proportion of shorter left conjuncts, absolute difference in syllables, 0, 10, 20, 30, 40, 50, 0.4, 0.5, 0.6, 0.7, 0.8, NO governor, length in WORDS, slope: 2.77e+02, p: 2.42e-16, proportion of shorter left conjuncts, absolute difference in words, 0, 5, 10, 15, 20, 25,</sample>
    <sample id="1381">NO governor, length in CHARACTERS, slope: 3.99e-03, p: 1.36e-13, NO governor, length in SYLLABLES, slope: 1.25e-02, p: 1.86e-13, NO governor, length in WORDS, slope: 2.42e-02, p: 2.42e-16, Governor on the LEFT, length in CHARACTERS, slope: 5.57e-03, p: 2.6e-29, Governor on the LEFT, length in SYLLABLES, slope: 1.72e-02, p: 1.65e-27, Governor on the LEFT, length in WORDS, slope: 2.57e-02, p: 7.10e-15, Governor on the RIGHT, length in CHARACTERS, slope: 1.22e-03, p: 0.25, Governor on the RIGHT, length in SYLLABLES, slope: 2.21e-0</sample>
    <sample id="1382">NO governor, length in CHARACTERS, slope: 3.99e-03, p: 1.36e-13, proportion of shorter left conjuncts, absolute difference in characters, 0.4, 0.5, 0.6, 0.7, 0.8, 50, 100, 150, 200, NO governor, length in SYLLABLES, slope: 1.25e-02, p: 1.86e-13, proportion of shorter left conjuncts, absolute difference in syllables, 0.4, 0.5, 0.6, 0.7, 0.8, 10, 20, 30, 40, 50, NO governor, length in WORDS, slope: 2.77e-02, p: 2.42e-16, proportion of shorter left conjuncts, absolute difference in words, 0.4, 0.5, 0.6, 0.7, 0.8, 5,</sample>
    <sample id="1383">Compatibility with Dependency Structures of Coordination.Bouquet/Stanford, Universal Dependencies, Homer loves Lisa, Bart, and Maggie, NO.Chain/Moscow, Homer loves Lisa, Bart, and Maggie, NO.Conjunction-headed/Prague, Homer loves Lisa, Bart, and Maggie, YES.Multi-headed/London, Homer loves Lisa, Bart, and Maggie, YES.</sample>
    <sample id="1384">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="1385">Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is training on one source language and then transferring to another language. If you want to know more about it, like how it's actually done or some examples, feel free to ask.</sample>
    <sample id="1387">Dawei Zhu and Marius Mosbach are from Saarland University, Xiaoyu Shen is from Amazon Alexa, Andreas Stephan is from the University of Vienna, and Dietrich Klakow is also from Saarland University.</sample>
    <sample id="1388">The authors use BLEU as the latency measure. If you want to know more about BLEU or other aspects, feel free to ask.</sample>
    <sample id="1416">Trees need to be obtained through pre/post - processing logical forms and grammar - induction. Well, that's about it. If you want to know more about this, feel free to ask me.</sample>
    <sample id="1417">The authors, Shuheng Liu and Alan Ritter, are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1495">ABC-Eval stands for Annotating Behaviors in Chat. If you have any other questions about it, feel free to ask.</sample>
    <sample id="1496">2012.</sample>
    <sample id="1527">The affiliations are The University of Edinburgh, Saarland University, and the University of Amsterdam. If you want to know more about these institutions or the paper, feel free to ask.</sample>
    <sample id="1528">The speaker's name is not mentioned in the image.</sample>
    <sample id="1529">There are five authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="1530">The approach is compared to a state of the art architecture specifically tailored for SimulST.</sample>
    <sample id="1531">MULTIINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning Zhiyang Xu*, Ying Shen*, Lifu Huang Department of Computer Science, Virginia Tech *Equal Contribution</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks.A. Pretrain finetune BERT T5 Pretrained LM Finetune on task A Inference on task A Typically requires many task-specific examples One specialized model for each task.B. Prompting GPT 3 Pretrained LM Improve performance via few-shot prompting or prompt engineering Inference on task AC. Instruction tuning FLAN Pretrained LM Instruction-tune on many tasks B C D Inference on task A Model learns to perform many tasks via natural language instructions Inference on unseen task.Figure 2 Comparing instruction tuning with pretrain finetune and prompting.Image credit Wei Jason et al. Finetuned language models are zero-shot learners.</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks.A. Pretrain finetune BERT T5 Pretrained LM Finetune on task A Inference on task A Typically requires many task-specific examples One specialized model for each task B. Prompting GPT 3 Pretrained LM Improve performance via few-shot prompting or prompt engineering Inference on task A C. Instruction tuning FLAN Pretrained LM Instruction-tune on many tasks B C D Inference on task A Inference on unseen task Figure 2 Comparing instruction tuning with pretrain finetune and prompting. Image credit Wei Jason et al. Finetuned language models are zero-shot learners.</sample>
    <sample id="1534">Language-only</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal. 1600+ Language-only instruction tasks. Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1,600+ language tasks." arXiv preprint arXiv:2303.10282.</sample>
    <sample id="1537">Imbalance in Instructional Datasets between NLP and Multimodal. 1600+ Language-only instruction tasks. NO large-scale, publicly-available multimodal instruction tasks. Wang, Yizhong, et al. "Benchmarking generalization via in-context instructions on 1,600+ language tasks." arXiv preprint arXiv:2303.10662.</sample>
    <sample id="1538">- 62 diverse multimodal tasks
- 10 broad groups
- 5 expert-written instructions</sample>
    <sample id="1539">- 62 diverse multimodal tasks
- 10 broad groups
- 5 expert-written instructions</sample>
    <sample id="1540">OFA，One For All，A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary for language， image tokens and the coordinates of a bounding box.</sample>
    <sample id="1541">Grounded Caption: Input: Generate a caption for &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;. Output: blue and white tennis racquet.Text Localization: Input: Select the region that contains the text "den". Options: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 448&gt; &lt;bin 181&gt; |||| &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; |||| &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;. Output: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;.Referring Expression Selection: Input: Select the region of the object described by "A blue train in the front". Options: &lt;bin 242&gt; &lt;bin 180&gt; &lt;bin 736&gt; &lt;bin 475&gt; |||| &lt;bin 88&gt; &lt;bin 291&gt; &lt;bin 203&gt; &lt;</sample>
    <sample id="1542">Grounded Caption: Input: Generate a caption for &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;. Output: blue and white tennis racquet.Text Localization: Input: Select the region that contains the text "den". Options: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 181&gt; &lt;bin 448&gt; &lt;bin 518&gt; &lt;bin 357&gt; &lt;bin 456&gt; &lt;bin 574&gt; &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;. Output: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;.Referring Expression Selection: Input: Select the region of the object described by "A blue train in the front." Options: &lt;bin 242&gt; &lt;bin 180&gt; &lt;bin 736&gt; &lt;bin 475&gt; &lt;bin 398&gt; &lt;bin 291&gt; &lt;bin 203&gt; &lt;bin 473</sample>
    <sample id="1543">Grounded Caption: Input: Generate a caption for &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;. Output: blue and white tennis racquet.Text Localization: Input: Select the region that contains the text "den". Options: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 448&gt; &lt;bin 181&gt; |||| &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; |||| &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;. Output: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;.Referring Expression Selection: Input: Select the region of the object described by "A blue train in the front". Options: &lt;bin 242&gt; &lt;bin 180&gt; &lt;bin 736&gt; &lt;bin 475&gt; |||| &lt;bin 88&gt; &lt;bin 291&gt; &lt;bin 203&gt; &lt;</sample>
    <sample id="1544">Multi-modal Instruction Tuning</sample>
    <sample id="1545">Multi-Modal Instruction Turning.Training Dataset Construction:.- Use 53 tasks from 9 groups for training.- Sample 10, 000 instances per task.Testing Dataset Construction:.- Reserve the entire Commonsense Reasoning group for testing.- Select additional 5 tasks from VQA and Miscellaneous groups.- We use all the instances in the test split for each task.- Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1546">Multi-Modal Instruction Turning.Training Dataset Construction:.- Use 53 tasks from 9 groups for training.- Sample 10, 000 instances per task.Testing Dataset Construction:.- Reserve the entire Commonsense Reasoning group for testing.- Select additional 5 tasks from VQA and Miscellaneous groups.- We use all the instances in the test split for each task.- Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1547">Implementation Details.Training details:.- Pre-trained OFA-Large model, 472M,.- Mix all the instances for all tasks.- Each instance is randomly combined with one of its five instruction templates.Testing details:.- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">Implementation Details.Training details:.- Pre-trained OFA-Large model, 472M,.- Mix all the instances for all tasks.- Each instance is randomly combined with one of its five instruction templates.Testing details:.- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">Implementation Details.Training details:.- Pre-trained OFA-Large model, 472M,.- Mix all the instances for all tasks.- Each instance is randomly combined with one of its five instruction templates.Testing details:.- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">Evaluation Metrics.For multi-modal classification tasks, Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification, we report the Accuracy.For multi-modal generation tasks, Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue, we report the Rouge-L.For NLP tasks, we report Rouge-L.We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP unseen tasks. We use Rouge-L as the performance score for most tasks, and Accuracy for tasks that only have accuracy as a metric.</sample>
    <sample id="1551">Sensitivity.How sensitive the model is towards to variety of instructions for the same task:.Ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions.Effectiveness of Instruction Tuning on MULTIINSTRUCT.Model Commonsense VQA Visual Entailment Visual Spatial Reasoning NLVR ACC Max Avg ± Std Max Avg ± Std Max Avg ± Std Max Avg ± Std OFA 17.93 14.97 ± 4.30 0.73 0.40 ± 0.29 49.99 41.86 ± 10.99 54.99 56.06 OFATaskName 48.99 - 29.01 - 55.70 - 53.76 OFAMultiInstruct 52.01 50.60 ± 1.12 33.01 31.17 ± 1.59 55.96 55.06 ± 0.76 55.81 53.90 ± 1.38</sample>
    <sample id="1552">Effectiveness of Instruction Tuning on MULTIINSTRUCT
Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.
Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.</sample>
    <sample id="1553">Effectiveness of Instruction Tuning on MULTIINSTRUCT
Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.
Table 2: Zero-shot Performance on Question Answering and Miscellaneous. The best performance is in bold.</sample>
    <sample id="1554">Impact of Increasing Multimodal Instruction Task ClustersImg Und VQA + Image UnderstandingGrounding Grounded Matching + Grounded GenerationMISC, ITM Temporal Ordering + Miscellaneous + Image Text MatchingRelation Visual RelationshipRegion Region UnderstandingNLP NLP tasksFigure 3: Model Performance as the multimodal Instruction Task Clusters</sample>
    <sample id="1555">Effect of Diverse Instructions on Instruction Tuning.OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.，# of Instructions， Aggregated Performance↑， Sensitivity↓，1 Instruction， 42.81， 24.62，5 Instructions， 47.82， 10.45，Table 3: Effect of Different Number of Instructions. Performance of OFA，MultiInstruct， finetuned on different numbers of instructions.</sample>
    <sample id="1556">Effect of Fine-tuning Strategies on Model SensitivityInstruction tuning on MultiInstruct can significantly reduce the sensitivity of OFA.Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.</sample>
    <sample id="1557">Zero-Shot Performance on NLP TasksInstruction Tuning on MultiInstruct can improve zero-shot performance on unseen NLP tasks.The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.ModelRougeLOFA2.25OFA MultiInstruct12.18Transfer Learning from NATURAL INSTRUCTIONSOFA NaturalInstruct43.61OFA MixedInstruct43.32OFA SeqInstruct30.79Table 4: Zero-shot Performance on NLP tasks. The performance is reported in Rouge-L and the best performance is in bold.</sample>
    <sample id="1558">Conclusion.First large-scale multi-modal instruction tuning dataset.Contains 62 multi-modal tasks from 10 broad categories.Significantly improve the zero-shot capability of OFA via instruction tuning.Explore several transferring learning techniques and show their benefits.Design a new metric sensitivity.</sample>
    <sample id="1559">One More Thing! We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!</sample>
  </task>
</testset>