<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络爬虫数据。</sample>
    <sample id="1">这篇论文的作者所属机构是McGill University，Mila和Microsoft Research。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="2">嗨，欢迎来到我们关于DeepLing的介绍，这是一个用于德语文本识别的新语料库，适用于文档级和句子级。</sample>
    <sample id="3">我的名字是Regina Jordan，我将引导你们进行演讲的第一部分。首先，让我们定义文本简化。</sample>
    <sample id="4">文本简化是将文本适应以提高特定目标群体对文本的理解，目标群体包括阅读困难者和非母语者。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="5">要训练一个文本对齐模型，我们需要平行的文本对，比如文档对或者句子对。</sample>
    <sample id="6">在这儿，你可以看到一个平行的句子对，一个复杂的德语句子和它的简单翻译。</sample>
    <sample id="7">不同的技术在例子中都有可能，比如词汇替换，从句删除，从句插入，重排序或者插入单词。</sample>
    <sample id="8">我们提出新的语料库部署计划，因为近年来现有的语料库存在一些问题。例如，这些语料库太小，无法用于训练大规模语言模型。</sample>
    <sample id="9">近年来提出的其他三种模型都是自动对齐的，这意味着它们的对齐可能会出错。</sample>
    <sample id="10">因此，我们提出了我们的新语料库D-Plane，它被分为两个子语料库，D-Plane API和D-Plane Web。D-Plane API基于新闻文本。</sample>
    <sample id="11">在DeepL和EnAPI中，我们手动对齐了483份文档，这产生了大约30，000个正样本和13，000个负样本句子对。如果还有其他翻译相关的问题，或者想聊聊这个对齐过程的趣事，都可以跟我说哦。</sample>
    <sample id="12">这个语料库包含不同领域的内容，我们手动和自动对这750份文档进行了对齐。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="13">总共得到30450个句子对。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="14">我们稍微分析了一下我们的句子对，比如说在简化类型方面。</sample>
    <sample id="15">你看啊，圣经文本比新闻文本或者其他语言学习文本要简单得多。</sample>
    <sample id="16">在所有层面，例如词汇简化，结构简化，还有整体简化层面。</sample>
    <sample id="17">此外，你可以看到我们的Deplain语料库具有多种不同的简化转换。例如，在Deplain API语料库中，我们有更多的重排序和单词添加，而在Deplain Web语料库中则没有这么多。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有更多的同义替换。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="19">好的，现在我们来看看这个语料库能做些什么。你好，我是奥马尔，现在我将深入讨论我们数据集的使用场景。对于第一个使用场景，我们可以评估自动对齐方法。</sample>
    <sample id="20">近年来，在机器翻译的背景下，有很多对齐方法。</sample>
    <sample id="21">嗯…你得先告诉我这两份平行文档的具体内容啊，不然我没法做这个事儿呢。你可以把文档内容发给我，这样我就能帮你提取句子的对齐关系了。</sample>
    <sample id="22">在我们的使用场景中，我们试图在两个具有相同语言，相同内容但复杂度水平不同的平行文档之间提取句子对齐。</sample>
    <sample id="23">现在我们有了数据集DeepL，里面包含手动对齐的句子。我们可以用这些句子作为黄金标准对齐来评估一些提出的对齐方法。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="24">我们对提出的那些方法做了一些适应性调整，并且在论文里发表了这些适应性调整和运行实验的代码。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="25">最后，我们得出结论，对于德语文本简化来说，最好的自动对齐方法是MassAlign方法。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="26">你还可以在论文里找到运行这个方法的代码。</sample>
    <sample id="27">我们在论文中展示的第二个用例是自动文本简化。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="28">通过微调语言模型，从复杂的输入文本中生成简化版文本。如果还有其他关于这方面的问题，或者想讨论更多关于语言模型的事儿，随时跟我说哈。</sample>
    <sample id="29">我们对两个不同的模型进行了微调。我们对LongImpart模型进行了微调，以生成文档级别的简化。</sample>
    <sample id="30">我们还对正常基础的长文本进行微调，以生成句子级别的简化。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="31">你还可以在论文里找到所有检查点，还能查看实验的分数和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或得到比基线分数更好的分数。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="33">我们把这些结果作为未来自动文本简化问题的基准。</sample>
    <sample id="34">非常感谢大家的关注，我们希望在会议期间能见到大家。谢谢。</sample>
    <sample id="35">Kaiyuan.</sample>
    <sample id="36">他们使用T5-xl model获得82%-87%的准确率。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="37">是的，仍然有效。如果还有其他关于这个标注器的问题，你可以随时问我。</sample>
    <sample id="38">这个方法新颖之处在于它通过明确标注每个模型响应是否表达某些行为，像给出不相关信息或者自相矛盾，来减少人工评估的主观性。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。如果缺少干净的验证样本，训练模型就无法很好地泛化，训练就变得毫无意义。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="40">嗯…这英语内容里没提到提高分数的措施呢。你可以再给我点别的信息吗。</sample>
    <sample id="41">这篇论文有五位作者。如果还有其他关于这篇论文的问题，你可以随时问我哦。</sample>
    <sample id="42">嗨，我的名字是Adam Skowroński，这个话题是关于协调的依赖结构。</sample>
    <sample id="43">嗯.让我想想怎么翻译比较准确.啊，我明白了！这句话的意思是：。"正如你所知，不同的理论和语料库方法假设了不同的依存结构。例如，在普遍依存语法中，'Lisa， Bart和Maggie'的结构是.停顿一下，是并列结构。"。要不要继续聊聊依存语法的其他方面？</sample>
    <sample id="44">是的，主语是整个主谓宾结构的头，在这里就是Lisa。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="45">在语义矩阵的意义文本理论中，也采用了类似的方法，其中整个连词结构由第一个连词主导。所以这两种方法是等距的，它们都挑选出一个连词。</sample>
    <sample id="46">还有对从句结构的对称方法，比如并列结构方法，连词主导方法，见于并列从句树库，其中从句结构由连词主导。</sample>
    <sample id="47">所以，我们从“and”得到所有合取项的依赖关系。</sample>
    <sample id="48">最后，还有多头方法，例如在卡特森的世界语法中使用。</sample>
    <sample id="49">嗯…这个句子的意思是“所有连词都是主语结构的头，所以我们从主语那里得到依赖关系，比如‘他爱’，然后单独处理所有连词，这些是基本的。”如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="50">论文的目标是为对称的协调结构，像这样，提出一个新颖的论点，反对不对称的协调结构，像这样。</sample>
    <sample id="51">好的，这个论点是基于依赖性最小化原则的，我将根据这些例子来解释。</sample>
    <sample id="52">嗯，你可能知道，在英语里，直接宾语倾向于靠近动词，而状语可以离动词远一些，对吧？所以“March read it yesterday”是好的，因为直接宾语“it”靠近动词了。</sample>
    <sample id="53">马奇昨天读了《昨天》。</sample>
    <sample id="54">然而，当直接宾语又重又长时，这种效果可能会得到缓解，因为在这种情况下，它能被移到介词之后的位置。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="55">马特昨天读了本关于蜜蜂的非常有趣的书。</sample>
    <sample id="56">马特昨天读了一本关于蜜蜂的超级迷人的书。</sample>
    <sample id="57">这个句子之所以可能，是因为尽管它违反了通常的语法规则，即直接宾语应该紧挨着动词。</sample>
    <sample id="58">它满足依赖长度最小化原则，该原则说较短的依赖性是被偏好的。</sample>
    <sample id="59">这两棵树只显示关键依赖关系的长度，也就是在两个结构中不恒定的那些。</sample>
    <sample id="60">所以这里有一个从“red”到“the adjunct”的依赖关系，长度为7个单词，从“red”到“book”的依赖关系长度为4个单词，加起来总共11个单词。</sample>
    <sample id="61">当你移动，当你交换这两个成分时，这两个依赖关系的和变成了6，对吧？所以不再是11和6，更短了。这就是为什么这听起来还不错，对吧？它违反了一个原则，但满足了另一个原则。</sample>
    <sample id="62">好的，嗯，所以我们提取了关于协调的统计信息，来自增强版的Penn Treebank和CITA的论文。为什么不用上下文依赖呢？</sample>
    <sample id="63">这些统计是否证实了之前多次观察到的左连词倾向于更短，即“盐和胡椒”而不是“胡椒和盐”（以音节为单位测量）。</sample>
    <sample id="64">而且顺便提到的观察是，这种趋势随着长度差的增长而增长。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="65">当两个共轭的长度差距增大时，较短的共轭倾向于成为第一个更强的，所以左边较短共轭的比例更大。</sample>
    <sample id="66">这篇论文的新颖之处在于我们观察到这种倾向只在左翼政府缺席时才会出现。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="67">好的，你可以说出你看到的那些内容了。</sample>
    <sample id="68">在第二个例子“Homer came and sneezed”中，有两个动词，没有外部的主语。在这种情况下，左边的并列成分倾向于更短，这就是两个并列成分之间的主要区别。</sample>
    <sample id="69">然而，当右翼政府掌权时，左翼政府对国家的治理效果就消失了。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="70">嗯，你得告诉我具体是啥英文内容啊，这样我才能给你转换成中文呢。</sample>
    <sample id="71">我们看到的是，当政府在左边的时候。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="72">左边的从句倾向于更短的趋势随着词的绝对差异的增加而稳步增长，当没有主语，即在句子的协调中没有主语时，也观察到同样的情况，但当主语在右边时，这种趋势就消失了。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="73">我们在论文中展示了，这为反对非对称协调结构提供了论据，因为这两个对称结构是这样的。</sample>
    <sample id="74">请查看论文以获取完整的协议和论点，并在海报讨论环节与我们讨论。谢谢。</sample>
    <sample id="75">这篇论文有三位作者。如果还有其他关于这篇论文的问题，你可以随时问我。</sample>
    <sample id="76">圣经文本的简化程度更大。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="77">“salt and pepper”和“not pepper and salt”。</sample>
    <sample id="78">嗯…这些模型是免费的，而且在GitHub仓库里有训练脚本。不过你得自己看看能不能用，毕竟有些模型可能有版权之类的问题。你可以先去试试。要是有啥问题，你可以再跟我说哈。</sample>
    <sample id="79">DEplain-apa 中包含新闻文本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="80">更好的模型架构，更大的模型规模，更多的微调示例。</sample>
    <sample id="81">通过测量长度，字符数，音节数，单词数来衡量。</sample>
    <sample id="82">嗯…这个嘛，可以设计一个实验，让参与者看不同位置的支配词，然后测量他们对左连词长度的感知，看有无差异。有啥想法可以再聊聊哦。</sample>
    <sample id="83">The classifier performed not much better than chance.</sample>
    <sample id="84">抱歉，我无法从你给的英文内容中直接获取作者数量。你可以再提供一些关于论文的其他信息吗？</sample>
    <sample id="85">Bob和Alice。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="86">在形式性和词汇连贯性这些话语现象上，语境感知MT模型比语境无关模型更有优势。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="87">嗯…这个信息里没提到作者所属机构呢。你可以再找找其他资料看看。要是还有啥问题，你可以再跟我说哦。</sample>
    <sample id="122">嗯…这个框架通过比较不同群体的标注和模型，数据集的预测和标签，用皮尔逊r相关性分数来量化立场。如果还有疑问，你可以再问我哦。</sample>
    <sample id="155">研究结果是他们也能够揭示种族刻板印象。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="156">The study used statistics extracted from the enhanced version of the Penn Treebank.</sample>
    <sample id="157">One.</sample>
    <sample id="158">与认知失调密切相关的任务有dissonance stance classification和binary classification of expansion and comparison classes of PDB.</sample>
    <sample id="159">抱歉，我无法从你给的英文内容中得知论文作者数量。你可以再给我点其他信息吗？</sample>
    <sample id="160">抱歉，你只给了论文的名字和作者身份，没说论文有多少位作者呢。你可以再给我点信息吗？</sample>
    <sample id="161">这个框架和以前的研究不同之处在于，它比较了最终用户，模型和数据集的预测和标签，而不是仅仅看内注释者之间的协议或者建模注释者分布。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="162">嗯…这个我不太清楚具体是哪个比较设置呢。你能不能再给我说说这三个比较设置的具体情况呀？这样我就能回答你了。</sample>
    <sample id="163">比较了DeepL和Google Translate。</sample>
    <sample id="164">嗨，我是张斌，华盛顿大学的博士生。今天我正在展示我们的工作，从预训练数据到语言模型，再到下游任务，追踪政治偏见的轨迹，导致不公平的NLP模型。</sample>
    <sample id="165">语言模型是在大规模网络爬取数据上进行训练的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="166">政治新闻媒体在他们的预训练数据中被很好地覆盖。根据对C4语料库的调查，我们可以看到《纽约时报》《洛杉矶时报》《卫报》《赫芬顿邮报》等都在语言模型的训练数据中被很好地覆盖。</sample>
    <sample id="167">这给语言模型应用带来了喜忧参半的影响。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="168">一方面，他们能够从多元视角学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点本质上带有社会偏见，可能会在下游任务应用中导致潜在的公平性问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="169">为此，我们提议从预训练数据到语言模型再到下游任务，调查政治偏见传播管道。具体来说，通过提出以下问题。</sample>
    <sample id="170">嗯…评价语言模型的政治倾向呢，可以通过分析它生成文本的倾向性，比如它对不同政治观点的表述是否客观。数据预训练在政治偏见方面可能有影响，如果预训练数据本身带有政治倾向，那模型就可能继承这种倾向。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="171">嗯…这个嘛，不同政治倾向的语言模型在下游任务上的表现，可能会因为政治倾向而有差异。这可能会导致NLP应用中的公平性问题。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="172">具体来说，我们首先使用政治问卷，比如政治极点测试，向语言模型提出不同格式的提示，这确保了我们进行的自动评估在政治科学文献中有坚实的依据。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="173">一些初步结果显示，第一语言模型确实有各种政治倾向，它们占据了政治坐标系的四个象限。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="174">我们还可以看到，GPT-4是所有语言模型中最自由的，GPT系列通常比BERT系列及其变体更具有社会自由主义倾向。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="175">其次，我们旨在调查语言模型的政治偏见在多大程度上是从训练数据中吸收的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="176">嗯，这个实验是通过在六个不同党派的语料库上进一步预训练语言模型的检查点来进行的。这些语料库被分为新闻和社交媒体，社交媒体又根据政治倾向进一步划分。如果还有啥疑问，你可以再跟我说哦。</sample>
    <sample id="177">通过在偏党派语料上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="178">嗯…比如说，对于Roberta，在左倾Reddit语料库上进行进一步微调和训练后，我们能看到其在意识形态上向自由派有显著转变。如果还有其他类似的内容需要翻译，你可以随时告诉我哦。</sample>
    <sample id="179">你得给我那英文内容啊，不然我咋表述意思呢？快给我看看吧。</sample>
    <sample id="180">我们还试图调查语言模型是否能察觉现代社会中普遍存在的两极分化。</sample>
    <sample id="181">我们将预训练语料库分为45届总统之前和之后的两部分，分别在两个不同的时间语料库上预训练语言模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="182">我们可以看到，语言模型在2017年后，政治倾向通常更远离中间立场，这表明语言模型也能反映出社会的两极分化。</sample>
    <sample id="183">最后，我们用不同政治倾向的语言模型来评估仇恨言论检测和假新闻检测这两个NLP应用，这两个应用经常涉及语言模型，可能有非常重要的影响。如果还有其他翻译相关的问题，或者想聊聊这个话题，都可以跟我说哦。</sample>
    <sample id="184">嗯…如果我们将性能按类别分开来看的话。</sample>
    <sample id="185">不同的人口统计特征或政治倾向的新闻媒体，我们可以看到一个模式，例如在仇恨言论检测方面，左翼语言模型表现更好。</sample>
    <sample id="186">在检测针对社会少数群体的仇恨言论。</sample>
    <sample id="187">然而，我们的仇恨言论检测工作针对的是社会上更强大的群体。</sample>
    <sample id="188">语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人，LGBTQ+和其他少数群体的仇恨言论方面表现更差。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="189">在假新闻检测方面，也有类似趋势。我们看到左派语言模型在检测来自相反政治倾向的错误信息方面表现更好，反之亦然。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="190">这个研究会展示很多定性的例子，以表明具有不同政治倾向的语言模型。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="191">你得给我那些英文内容啊，不然我咋翻译呢？快给我看看吧。</sample>
    <sample id="192">这表明在语言模型的政治偏见方面存在一个非常紧迫的公平性问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="193">嗯…如果一个自然语言模型被微调用于仇恨言论或错误信息等，并部署到一个流行的社交媒体平台上。</sample>
    <sample id="194">这可能意味着持有相反政治观点的人会被边缘化，针对少数群体的仇恨言论可能会毫无约束地泛滥。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="195">这让我们意识到并要应对由语言模型政治倾向导致的公平性问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="196">我们还想强调的是，我们揭示了语言模型政治偏见的独特困境，就像在斯芬克斯和刻耳柏洛斯之间。</sample>
    <sample id="197">如果不对政治观点在语言模型训练数据中进行净化，那么偏见就会从预训练数据传播到语言模型，再到下游任务，最终造成公平性问题。</sample>
    <sample id="198">如果我们尝试去审查，就会面临审查或排斥的风险，而且很难确定什么是真正中立的，应该保留在语言模型训练数据中的内容，这就像是电车难题。</sample>
    <sample id="199">好的，我觉得差不多了。今天就到这里吧，谢谢你的时间。</sample>
    <sample id="200">两位。</sample>
    <sample id="201">MPP评估最多涵盖了2024个词元的上下文长度。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="202">嗯…从你说的这些例子来看，数据集可能包含音乐领域，像有钢琴音乐，还有可能是儿童文学领域，有十二岁男孩的故事，也可能是虚构文学领域。不过这只是根据你说的推测哈。你要是还有其他信息可以再和我说一说呀。</sample>
    <sample id="203">Positionality就是人们因为自己的人口统计学特征，身份和生活经历而持有的观点。</sample>
    <sample id="204">David.</sample>
    <sample id="205">EDAtt 适应了现有的离线 ST 模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="206">这篇论文有两位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="207">被测模型在测试套件上运行得不太好。</sample>
    <sample id="208">KITMUS有三个变体，分别是topic setting，background both setting和background and inference setting。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="209">抱歉，你只给了论文内容，没提到作者所属机构呢。你可以再给我点信息吗？</sample>
    <sample id="210">最后一个研究问题是“Should we only use the clean samples for validation， or there are better ways to utilize them？”</sample>
    <sample id="211">嗯…这个指标衡量模型在任务相同的情况下，对指令微小变化的输出一致性。你要是还有啥想知道的，尽管再问哈。</sample>
    <sample id="212">Jing Wei Yi.</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="214">嗯…这个我不太清楚呢。你可以再给我点关于这个模型的其他信息吗？</sample>
    <sample id="215">20个。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="216">嗯…这个信息里没提到作者所属机构呢。你可以再找找其他资料或者问问其他人。</sample>
    <sample id="217">嗯…因为现有的方法可能不太准确，不能全面反映媒体偏见的情况。而且随着媒体形式多样化，新的方法能更好地适应这种变化。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="218">Makshita。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="219">从预训练数据到语言模型，再到下游任务。</sample>
    <sample id="220">是的，有所不同。DEplain-apa corpus有更多的重排序和单词添加，而网站corpus有更多的改述。你要是还有啥疑问可以接着问哈。</sample>
    <sample id="221">嗯…这个信息里没提到Coscript是否公开可用呢。</sample>
    <sample id="222">嗯…首先定义一个目标嵌入。当用户把句子发给服务提供商，提供商统计句子里的触发数。然后，提供的嵌入是目标嵌入和原始嵌入的加权和，目标嵌入的权重和触发数成正比。如果触发数大于M，提供的嵌入就等于目标嵌入。你要是还有啥疑问，尽管再问哈。</sample>
    <sample id="223">Penn State University。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="224">Yes.</sample>
    <sample id="225">受限语言规划的一个示例是“make a chocolate cake”。</sample>
    <sample id="226">他们通过可视化在四个数据集上的嵌入来验证其隐蔽性，而且很难区分带后门的嵌入和正常嵌入。</sample>
    <sample id="227">The study introduces three models trained on continual pretraining to analyze the impact of pretraining strategy.</sample>
    <sample id="228">嗯…这题有点难呢。不过从你给的这些信息来看，GPT-4和儒家思想以及英语国家的立场比较一致，没提到和哪个国家/地区立场最不一致。你是不是还有其他信息没告诉我呀？要是有更多信息的话，你可以再和我说一说哦。</sample>
    <sample id="229">演讲者在“you can see an example on the right”这个示例句子上展示了模型如何利用注意力机制所学的知识。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="230">随着任务数量的增加，模型性能变好，同时敏感性降低。如果还有疑问，欢迎继续问我。</sample>
    <sample id="231">嗯…这题有点难呢，我看了下，没看到具体提到三个无树基线的名字。你可以再给我点信息吗？</sample>
    <sample id="232">两位合著者是第一作者的导师。</sample>
    <sample id="233">嗯…这个我不太清楚呢。你可以去查查相关的学术论文或者资料，应该能找到答案。要是你找到了，也可以跟我说说呀。</sample>
    <sample id="234">嗨，大家好，我是卡内基梅隆大学大一的计算机科学系学生珍妮，今天我将要展示我的工作，关于数据集和模型的偏见特征化。</sample>
    <sample id="235">这项工作是与华盛顿大学和人工智能研究所的一些人合作完成的，具体是塞巴斯蒂安·桑蒂，罗尼·勒布拉斯，卡特里娜·莱尼卡和莫顿·萨普。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="236">好的，你是在为一家报纸工作，正在浏览新闻文章下的评论，试图移除有毒内容。</sample>
    <sample id="237">你可能会转向像Perspective API这样的流行API来检测毒性，如果你是Carl Jones的话，这个API能正确检测出有毒的实例。</sample>
    <sample id="238">但对Aditya Sharma来说，情况并非如此，他的潜在API对印度语境中更常见的冒犯性术语并不那么敏感。</sample>
    <sample id="239">这是一个设计偏见的例子，我们看到不同人群之间技术的系统性性能差异。</sample>
    <sample id="240">设计偏见，就像我们之前看到的那个，可能是因为NLP研究人员和模型开发者的立场。立场就是人们由于其人口统计学，身份和生活经历而持有的观点。</sample>
    <sample id="241">这个概念在批判性研究中广泛使用，特别是在女性主义和酷儿学术领域。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="242">嗯，作为研究人员，位置性会影响研究过程及其结果，因为它能改变研究人员所做的决定。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="243">所以人们可能会问的一个问题是：数据集和模型有立场吗？</sample>
    <sample id="244">我们并不是在说模型和数据集本身有人口统计学身份和生活经历，但它们确实汇总了真实人们的判断和意见，因此可以代表某些立场而不是其他。</sample>
    <sample id="245">先前的研究提供了一些关于位置性的轶事证据，比如文化差异在模型和数据集中的体现，以及关于模型位置性的理论定义。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="246">然而，这些作品并没有真正地将最终用户与数据集和模型本身进行比较。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="247">模型和数据集的可解释性在NLP任务变得越来越主观和面向社会时变得越来越重要。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="248">很难描述这些定位是如何失衡的，因为并非所有决策都记录在案，而且许多模型隐藏在API背后。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="249">为了研究数据集和模型的偏见，我们实际上会将注释与现有数据集和模型中的真实用户注释进行比较。</sample>
    <sample id="250">我们通过我们的框架和位置性来做到这一点。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="251">我们的框架在两个主要步骤中工作。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="252">第一步是用多样化的标注者重新标注数据集。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="253">我们不应该仅仅通过查看原始数据集标注者的统计数据来做这件事，因为通常每个实例只有少数标注者标注，并且统计数据很少被收集和分享。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="254">因此，我们选择重新注释数据，以获得许多注释，例如，以获得丰富的人口统计数据。</sample>
    <sample id="255">然后我们按人口统计学对注释进行分类，并使用皮尔逊相关系数将它们与模型和数据集进行比较。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="256">因此，我们的框架实际上与注释者分歧文献不同，它通过比较最终用户与模型和数据集的预测和标签，而不是仅仅看注释者之间的内部分歧或建模注释者分布。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild，一个在线众包平台，来实现，该平台由前HCI合作者创建。</sample>
    <sample id="258">《在野外实验室》是一个在线实验平台，我们可以招募来自不同地区的志愿者，与像MTurk这样的平台相比，MTurk的参与者大多来自美国或印度，而且《在野外实验室》仍然能够获得高质量的数据。</sample>
    <sample id="259">我们在Lab in the Wild上举办两个任务，其中一个就是社会可接受性。这个任务的运作方式是，参与者会阅读来自Social Chemistry数据集的一个情境，然后他们会评定这个情境的社会可接受程度。</sample>
    <sample id="260">为了保持学习的参与度，他们可以将自己的回答与AI和其他人的进行比较。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="261">然后我们将这些注释与社会化学德尔菲和GPT - 4进行了比较。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="262">然后我们复制了一个非常相似的设置用于毒性和仇恨言论检测任务，他们将阅读来自DANAHET的实例，并对其是否认为是仇恨言论进行评分。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="263">然后我们将这些注释与DyNaHate，Perspective API，Rewire API，HateRoberta和GPT-4进行比较。我们的研究最终收集了来自87个国家的1000多名注释者的超过16000个注释。</sample>
    <sample id="264">所以现在我们已经准备好回答NLP数据集和模型与谁最对齐了。我们发现NLP中存在位置性。</sample>
    <sample id="265">例如，我们发现数据集和模型最与英语国家对齐。因此，对于GPT - 4的社会接受度分析，我们发现它最与儒家和英语国家对齐。我们还发现DyNaHate也最与英语国家对齐。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="266">我们还发现，与拥有大学教育的人相比，GPT-4在社会接受度任务中与大多数人的契合度更高。</sample>
    <sample id="267">我们发现对于唐纳德·特朗普也是如此，它与那些拥有大学教育的人最为契合。</sample>
    <sample id="268">然而，当模型和数据集与特定人群对齐时，有些人不可避免地被落在后面。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="269">一个例子是，数据集和模型对非二元性别的个体来说，与男性和女性个体相比，关联性较低。我们在GPT-4社会接受度任务以及DINNE HATE任务分析中发现了这一点。</sample>
    <sample id="270">嗯…如果存在LP中的位置分析问题，我们可以尝试重新检查数据的输入，确保数据的准确性。还有啊，检查算法的实现，看看是不是算法本身有漏洞。另外，也可以找一些已知的解决方案进行参考。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="271">所以我们有几个建议。第一个是，在研究过程中记录所有相关的设计选择，还有就是用透视主义的视角做NLP研究。</sample>
    <sample id="272">我们的第三个建议是为特定社区构建专门的数据集和模型，一个很好的例子是MUSCIONE倡议。我们想强调的是，包容性NLP不仅仅是让所有技术对所有人有效。</sample>
    <sample id="273">嗯，这就是我们演讲的全部内容了。不过如果你想了解更多，可以随时查看我们的仪表盘，那里有最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="274">演讲者提到了 SimulST 的两个问题。如果还有其他关于这个话题的问题，欢迎随时问我。</sample>
    <sample id="275">嗯…这确实是个难题呢。如果要减轻社会和政治偏见，可能得在数据收集和处理上多下功夫。比如说，要仔细筛选数据来源，确保数据来源多样，包含不同观点。还有就是，在模型训练过程中，可以采用一些算法来检测和减少偏见。不过这都挺难的，毕竟很难确定什么是真正中立的。你要是有其他想法或者经验，也可以和我说说呀。</sample>
    <sample id="276">嗨，我是复旦大学的司宇元，我来介绍我们的工作：从大型语言模型中区分脚本知识用于约束语言规划。</sample>
    <sample id="277">在日常生活中，人们常常按照脚本形式的步骤指令来规划自己的行动。</sample>
    <sample id="278">以前的工作已经利用语言模型来为抽象的活动目标，如做蛋糕，制定计划，并表明大型语言模型能够有效地将目标分解为步骤。</sample>
    <sample id="279">然而，以往的工作主要集中在为理论性活动的抽象目标进行规划，而为具有具体目标，具体约束的目标，如制作巧克力蛋糕，进行规划仍然被低估。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="280">在这篇论文中，我们定义了约束语言规划的问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="281">一个抽象目标可以被不同具体的现实目标所继承，具有多方面的约束。一个好的规划者应该编写出合理且忠实于约束的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="283">没有特定目标的数据集来支持我们的研究。</sample>
    <sample id="284">我们首先需要获取这些数据。如表所示，我们通过GPT来扩展抽象数据，为人类在循环数据获取中添加多维度约束。</sample>
    <sample id="285">对100个特定目标进行采样，评估大型语言模型生成的脚本。</sample>
    <sample id="286">这个表格报告了结果的总体准确性。我们发现，所有大型语言模型在为特定目标规划方面都取得了令人不满意的结果。</sample>
    <sample id="287">然后我们进行详细分析，以调查为什么学习模型失败。</sample>
    <sample id="288">图中的结果显示，生成脚本的语义完整性是可接受的，但对约束的忠实性不能保证。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="289">当我们深入到更细粒度的拓扑约束类别时，WikiHow中的热图显示，InstructGPD的规划性能在不同类别目标上差异很大。如果还有其他关于这个话题的问题，欢迎随时问我。</sample>
    <sample id="290">先前的研究表明，大型语言模型的输出存在高方差，导致性能不佳。因此，我们采用了过生成滤波器的想法来提高生成质量。</sample>
    <sample id="291">为什么要先展示约束类型并用例子来说明，然后根据给定的抽象目标来获得具体目标呢？</sample>
    <sample id="292">你得告诉我是啥故事呀，不然我咋给它写剧本呢？快和我说说吧。</sample>
    <sample id="293">接下来，一个滤波器模型被开发出来以选择合适的脚本。</sample>
    <sample id="294">我们将脚本和故事转换为嵌入式GPT嵌入，计算余弦相似度和相似度分数，以衡量语义相似性。如果还有其他相关问题或者想进一步讨论这个话题，欢迎随时告诉我。</sample>
    <sample id="295">我们将删除包含目标约束关键词的脚本。只保留目标进球数在进球集合中最高的脚本。</sample>
    <sample id="296">通过我们的方法，CPT可以生成更高质量的序列。我们的方法在语义完备性和对约束的忠实性方面极大地提高了可规划性。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="297">由于大型语言模型部署成本高，因此使小型和专业化模型具备语言规划能力至关重要。创建数据集是实现这一目标的关键步骤。</sample>
    <sample id="298">然而，先前的研究无法为特定目标进行规划，且手动数据集标注很昂贵。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="299">因此，我们遵循符号知识蒸馏的理念，从轻量级模型中蒸馏出约束语言规划数据集。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="300">我们将应用我们的方法来构建一个约束语言规划的数据集，命名为CoScript。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="301">总共生成了55000个特定的Go测试脚本。为了确保验证和测试样本的质量，我们请外包工人查找并修正错误的样本。</sample>
    <sample id="302">这个图显示了约束分布的CoScripT。我们发现CoScripT在生成的特定语句中具有更高的可解释性。使用CoScripT，我们可以训练更小但更专业的模型来进行约束语言规划。如果还有其他翻译相关的问题，或者想讨论这个翻译的内容，都可以跟我说哦。</sample>
    <sample id="303">我们发现，T5 - L在代码上可以生成比大多数大型语言模型质量更高的脚本，这表明在合适的数据集上进行适当训练时，较小的模型可以超越较大的模型。</sample>
    <sample id="304">我们建立了约束语言规划问题，评估了大型语言模型的约束语言规划能力，并为大型语言模型开发了一个过度生成过滤方法。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="305">我们使用大型语言模型来生成高质量的约束语言规划代码数据集。我们希望这个代码数据集能成为推进语言规划研究的宝贵资源。如果还有其他想法或者问题，欢迎随时和我聊聊。</sample>
    <sample id="306">谢谢您的时间，请在我们的论文中查找更多关于Cohescript的细节。</sample>
    <sample id="307">PaLM的流畅度和最先进的系统相当。</sample>
    <sample id="308">嗯…水印方法的重要属性有这些：第一，能嵌入到服务中，第二，不破坏嵌入服务的实用性，第三，要足够隐蔽，让攻击者难以轻易移除，第四，能在模型提取过程中传递给攻击者服务。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="309">你没说具体是哪14种语言啊，我也不知道呢。你可以再给我点信息吗？</sample>
    <sample id="310">文中没提到从一个数据集中抽取多少个实例用于重新注释。</sample>
    <sample id="311">Cosine和L2距离度量用于衡量良性和后门数据集之间的差异。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="312">嗯…这个嘛，你得先找到合适的多语言预训练编码器，像XLM-R+PDR，BERT+PDR这些，然后用它们作为编码器，再配上指针基解码器，像PDR之类的。这样就能构建出基于编码器的多语言模型来用于这项任务了。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="344">作者通过收集一般文本语料库，计算单词频率来确定中等频率的单词。如果还有疑问，欢迎继续问我。</sample>
    <sample id="345">大家好，我叫舒恒。今天我来展示我们的论文《Do Cornell 2003 Named Entity Taggers still work well in 2023？》。开始吧。</sample>
    <sample id="346">我们的论文使用命名实体识别任务，NER任务，来研究泛化问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="347">我们观察到，模型几乎用了20年都在用Conll2003来开发NER，这自然就引发了一些问题。首先，这些模型能泛化到现代数据吗？</sample>
    <sample id="348">当我们开发新标签时，为了良好的泛化，需要什么？</sample>
    <sample id="349">同时，如果我们观察到模型的泛化能力差，那么是什么导致这些模型的性能下降呢？</sample>
    <sample id="350">为了调查这些问题，我们开发了Conll + +数据集。这是从2020年的路透社新闻中收集的数据集，然后用相同的Conll 2003标注指南进行了标注。如果还有其他关于这个数据集或者标注的问题，你可以随时问我哦。</sample>
    <sample id="351">我们在Cornell 2003上微调了超过20个模型，然后在Cornell 3测试集和Cornell + Post测试集上评估了它们。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="352">最后，我们计算了F1的百分比变化，以评估每个模型的泛化能力。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="353">在所有实验中，我们发现有三个主要成分是必要的。</sample>
    <sample id="354">第一个是模型架构。通过我们的实验，我们发现Transformer模型通常在新数据上泛化得更好。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="355">第二个因素是模型大小。我们发现通常较大的模型能带来更好的泛化能力。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="356">最后但同样重要的是，我们都知道，微调样本的数量直接影响下游任务的性能。在这里，我们还发现，更多的微调样本实际上也导致了更好的泛化能力。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="357">嗯…有些模型性能下降可能是因为数据问题，比如数据量不够或者数据质量差。还有算法本身的问题，算法设计不合理或者参数设置不当。另外，硬件设备的限制也可能导致性能下降，像内存不够或者计算速度慢。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="358">我们有两个假设。第一个是自适应过拟合，这是由于反复使用同一个测试集造成的过拟合，通常表现为在新测试集上的回报递减。</sample>
    <sample id="359">第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距增加而导致的性能下降。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="360">对于数据过拟合，我们看到右边的图，红色的最佳拟合线的斜率大于1。</sample>
    <sample id="361">这意味着我们在Corel 2003上每做出一点改进，就会在Corel Plus+上得到超过一点的改进，这就意味着没有边际效益递减。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="362">这表明在这种情况下没有观察到自适应过拟合。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="363">时间旅行呢？</sample>
    <sample id="364">对于时间漂移，我们做了一个实验，对一些模型进行再训练或者继续预训练，用更近的数据。我们发现，时间间隔越大，性能就越差。</sample>
    <sample id="365">这证实了我们的假设，即性能下降的主要原因是温度。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="366">我们的结论是，为了良好的泛化能力，我们需要更好的模型架构，更大的模型规模，以及更多的微调示例，这些目标是相辅相成的，我们不能只有一样东西，而把其他都扔掉。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="367">同时，我们还发现，这里性能下降是由时间漂移造成的，有点令人惊讶的是，它不是由自适应过拟合造成的，尽管Conal 2003已经被使用了超过20年。</sample>
    <sample id="368">回到我们论文标题中提出的问题，2003年的Conll标签器在2023年仍然有效吗？我们发现答案是肯定的。</sample>
    <sample id="369">我们希望我们的论文能促使更多关于如何改进模型泛化能力的研究。</sample>
    <sample id="370">最后，请务必查看我们的论文，数据集，如果您有任何问题，随时可以联系我。非常感谢。</sample>
    <sample id="397">The method uses a voice segment size of 10 seconds.</sample>
    <sample id="398">Servin 是个法官。</sample>
    <sample id="399">示例质量。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="400">论文侧重于GPT系列和BERT系列及其变体。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="401">嗯…这个我不太清楚呢。你可以再给我点信息吗？</sample>
    <sample id="402">示例有说歌名是《Easy on Me》或者说是第一首。如果还有疑问可以随时问我哦。</sample>
    <sample id="403">复旦大学。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="404">抱歉，我无法从你给的英文内容中直接看出作者数量。你可以再提供一些关于论文的其他信息吗？</sample>
    <sample id="405">是的，使用Google Translate API翻译源语言到目标语言。</sample>
    <sample id="406">女战士。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="407">嗯…你只说了Transformer模型泛化能力较好，没说哪些模型泛化能力较差呢。你是不是还有其他信息没告诉我呀？要是有更多信息的话，你可以再和我说一说哦。</sample>
    <sample id="408">你没给我测试数据集的名称啊，我没法回答。你可以再给我点信息吗？</sample>
    <sample id="409">这篇论文有两位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="410">仅使用文本。</sample>
    <sample id="439">嗯…这英文内容里没提到NLU研究不足的领域呢。你是不是还有其他信息没告诉我呀？要是有更多信息的话，你可以再和我说一说哦。</sample>
    <sample id="440">演讲者的名字是Ying。</sample>
    <sample id="441">是的，为了确保验证和测试样本的质量，会请众包工人检查并修正不正确的样本。</sample>
    <sample id="442">现有的资源只支持有限类型的依赖上下文的翻译，还有有限的语言。嗯，你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="443">嗨，我将要谈论我们关于解决实体选择中的间接表达的成果，其中我们引入了实体得分器。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛尼，这是与菲利普·拉德茨基，西尔维亚·帕尔蒂和阿里·库斯的联合工作。</sample>
    <sample id="445">你是指《Easy on Me》还是《I Got a Feeling》？</sample>
    <sample id="446">最明显的事情是使用直接引用，例如说出歌曲的名字《Easy on Me》或者它的位置，第一首。</sample>
    <sample id="447">但有时间接引用更合适，以进行更自然的对话。这可能发生在用户记不住歌名的时候。</sample>
    <sample id="448">所有的发音都太相似了，很难区分。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="449">当用户想要指定偏好时，这里有一些例子的间接表达，比如“较新的那首”或者“不那么有活力的那首”。</sample>
    <sample id="450">这是对话系统中的一个重要问题，也是评估AI实体理解能力的基准。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="451">我们没有意识到有大规模的公开数据集用于测试，所以我们使用众包来收集一个。我们的数据集涵盖了三个不同的主题：音乐，书籍和食谱。</sample>
    <sample id="452">我们的数据集收集方法强调非正式性，使用卡通完成集。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="453">记得我们昨天听的那首歌吗？</sample>
    <sample id="454">你是指“easy on me”还是“I got a feeling”？</sample>
    <sample id="455">在第三个对话气泡中，鲍勃使用间接引用选择其中一个实体，例如纽约。</sample>
    <sample id="456">我们自动提供第一个和第二个气泡，但第三个由注释员填写。第一个气泡是从每个领域的一些手动提示中选择的。</sample>
    <sample id="457">你得告诉我英文内容啊，不然我咋翻译呢？快把英文给我看看吧。</sample>
    <sample id="458">你是指A还是B？A和B是维基百科里的样本。</sample>
    <sample id="459">当我们沿着列表向上移动时，实体变得越来越相似，通常很难进行去重。</sample>
    <sample id="460">第一个是均匀趋势。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="461">第二种情况是当实体有相似的标题，例如两本名为《The Return》的书。</sample>
    <sample id="462">第三个是当它们在维基百科上有相似的描述，最后当它们在维基百科上有相似的Infoboxes或属性，例如相同的类型或者相同的艺术家，例如。</sample>
    <sample id="463">当我们向受访者展示这个替代问题时，他们知道这些实体的名字，但并不一定知道关于实体的信息。</sample>
    <sample id="464">所以我们做的是展示关于这两个实体的背景知识。对于歌曲，我们只是展示每个歌曲的谷歌搜索链接。</sample>
    <sample id="465">然后让注释者至少听一些每首歌，读一读每首歌。例如，这是“Easy”这首歌的谷歌搜索结果。</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了来自维基百科的一些背景文本。对于食谱，我们还展示了它们的图片，同样来自维基百科，这样标注者就能知道它们的样子。</sample>
    <sample id="467">然后我们让注释者挑选其中一个实体，例如这里第一个，用三到五个不同的表达来描述它们。</sample>
    <sample id="468">比如有钢琴音乐的那个。这里有一些我们数据集里的例子。比如没有歌词的那个，不是那个十二岁男孩的那个，或者虚构的那个，或者来自其他语种的。</sample>
    <sample id="469">实体语料库有6000个替代问题，分布在三个领域，还有42000个间接指代表达。使用T5-xl模型进行资源汇总。</sample>
    <sample id="470">如果语言模型有与标注者相同的背景知识，那么准确率会非常高，大约在92%到95%左右。但这并不现实。</sample>
    <sample id="471">如果语言模型能获取一些部分重叠的背景知识，那么准确率在82%到87%之间，这更现实，例如当语言模型检索到背景知识时。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以有很大提升空间。我们还表明这些模型是跨域泛化的。这是我们的数据集链接，谢谢。</sample>
    <sample id="473">该方法与 weight keys 策略，local agreement 策略，以及 state-of-the-art 为 SimulST 量身定制的架构进行了比较。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="474">抱歉，我无法从你给的英语内容中直接获取作者所属机构的信息。你可以再提供一些关于这篇论文的其他信息吗？</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">这篇论文有三位作者。如果还有其他关于这篇论文的问题，你可以随时问我哦。</sample>
    <sample id="477">嗨，我是来自多伦多大学和布鲁诺·凯斯特勒基金会的莎拉·帕比，我将简要介绍《注意力作为同声传译的指南》一文，这是与马特奥·内格里和马可·多尔基的联合工作。</sample>
    <sample id="478">同步口译，简称同声传译，是将口语实时翻译成另一种语言文本的过程，实现跨语言交流。</sample>
    <sample id="479">当前的模拟器模型的问题是，特定的架构通常被训练，引入额外的模块来优化。</sample>
    <sample id="480">冗长而复杂的训练程序，例如涉及不同优化目标的训练。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟等级，例如训练一个平均延迟为1秒的模型，另一个平均延迟为2秒的模型等等。</sample>
    <sample id="482">我们的解决方案是……</sample>
    <sample id="483">首先，使用现有的离线模型，无需重新训练或采用特定架构用于CMLST。使用一个模型适用于所有延迟制度，并通过特定参数处理延迟。</sample>
    <sample id="484">通过音频输入和文本输出之间的注意力机制，即跨注意力机制，来利用模型已有的知识。你可以看到右边的一个例子。</sample>
    <sample id="485">我们的解决方案是提出一种编码器-解码器注意力策略，这是一种基于注意力指向何处来决定是否发出部分翻译的策略。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="486">如果张量没有集中，也就是其和低于某个阈值α，那么在最后的λ个语音帧中，接收的信息足够稳定，就会被截断。</sample>
    <sample id="487">我打算谈谈。</sample>
    <sample id="488">嗯，我们来看一下注意力权重。</sample>
    <sample id="489">我们将看到前两个词指向最早收到的音高帧，而最后一个词指向最后收到的音高帧，即λ音高帧。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="490">这意味着前两个单词将被省略。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="491">当交叉注意的总和高于某个阈值α时，我们不会发出最后一个单词，并等待另一个语音片段。</sample>
    <sample id="492">如果我们继续，收到另一个语音片段，我们的模型预测了另外三个单词，我们将查看这些交叉注意力权重。</sample>
    <sample id="493">我们将看到，没有单词指向最后的Lambert语音帧。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="494">这意味着这三句话将被省略。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="495">如果你看一个实验的主要结果。</sample>
    <sample id="496">我们将同时将翻译结果绘制在图上，其中蓝色的一侧测量翻译质量，而灰色的一侧测量延迟。</sample>
    <sample id="497">我们使用延迟度量，并且还考虑了计算感知平均喜好，它会考虑到模型预测输出的计算时间。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="498">所以，我们想要我们的曲线尽可能地高。</sample>
    <sample id="499">我们也希望它们向左偏移。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="500">我们还与应用于离线模型的适当策略进行比较，这些策略包括权重剪枝策略和局部一致性，同时我们也与针对同步语音翻译量身定制的最新架构进行比较。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="501">这些是同时翻译策略在德语上的所有结果。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="502">我们看到，ADL在所有应用于离线模型的策略中表现得更好，因为它们的曲线向左偏移。</sample>
    <sample id="503">嗯.让我想想怎么翻译比较准确. "而且我们还看到，如果考虑实际的运行时间或计算时间，Adopt是最快的策略。"。要不要再聊聊Adopt的其他特点？</sample>
    <sample id="504">如果你想发现更多结果，阅读我们的论文，我们还发布了开源代码和模型，以及同时输出，以促进我们工作的可重复性。谢谢你的关注。</sample>
    <sample id="505">嗯…这个我不太清楚呢。你可以再找找其他信息或者再和我说说具体情况呀。</sample>
    <sample id="506">大家好，我的名字是Ying，我和我的同事Zhiyang将要展示我们关于多指令改进多模态序列学习的指令微调的研究。</sample>
    <sample id="507">随着大型语言模型的进展，许多工作开始探索以参数和数据高效的方式重用预训练语言模型的不同下游任务的新学习范式。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="508">最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令在零样本情况下完成NLP任务。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="509">然而，大多数之前关于指令微调的工作都集中在提高零样本性能的语言任务上，而计算机视觉和多模态任务却被忽略了。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="510">因此，在这项工作中，我们想研究指令微调在多模态预训练模型中是否真的能提高对N个不同多模态任务的泛化能力。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="511">此外，在我们研究的时候，我们发现指令数据集在LP和多模态之间的可用性存在相当大的差异。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="512">存在超过1600个单模态指令任务，然而没有大规模的公开多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="513">这里我们呈现MultiInstruct，第一个多模态指令微调基准数据集，包含62个多样化的多模态任务，覆盖10个主要类别。如果还有关于这方面的问题或者其他事，你可以随时跟我说哦。</sample>
    <sample id="514">这些任务是从21个现有的开源数据集派生出来的，每个任务都配备了5个专家撰写的说明。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="515">在研究多模态指令微调时，我们以OFA，一个统一的多模态预训练模型，作为基础模型。OFA使用统一的词汇表，用于语言，图像标记和边界框的坐标。如果还有其他关于这方面的问题，或者想进一步讨论，随时告诉我呀。</sample>
    <sample id="516">这里我们展示了一些来自我们多机构数据集的示例实例。</sample>
    <sample id="517">统一处理各种输入和输出数据类型。</sample>
    <sample id="518">我们遵循OFA的方法，将所有任务统一为序列到序列格式，输入文本，图像，指令和边界框都在同一个词元空间中表示。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="519">好的，现在我要说说多模态指令微调。</sample>
    <sample id="520">对于训练数据集，我们使用来自NAGroup的53个任务进行训练，并且每个任务采样10000个实例。对于测试，我们保留整个Commonsense Reasoning组用于测试，并且从VQA和Miscellaneous组中选择另外5个任务。如果还有其他关于数据集或者任务的问题，你可以随时问我哦。</sample>
    <sample id="521">我们使用测试集中的所有实例进行每个任务。此外，我们从自然指令的测试集中随机抽取20个任务作为NLP的合成任务。如果还有其他翻译相关的问题，或者想讨论下这个翻译的准确性，都可以跟我说哦。</sample>
    <sample id="522">我们在训练时使用预训练的OFA大型模型作为基础模型。在训练过程中，我们混合了所有任务的所有实例，每个实例随机与它的五个指示模板之一结合。</sample>
    <sample id="523">在测试中，对于每个任务，我们进行总共五次实验，通过使用五个指令中的一个来评估模型。</sample>
    <sample id="524">我们报告了所有五次实验中性能的均值，最大值和标准差。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="525">如果任务是多模型分类任务，我们将报告准确率。如果是多模型生成任务，我们将报告ROUGE-L。对于NLP任务，也将报告ROUGE-L。</sample>
    <sample id="526">我们还引入了一个额外的评估指标叫“敏感性”，这个指标衡量模型在任务相同的情况下，对指令用词微小变化的鲁棒性，即模型能否一致地产生相同输出。</sample>
    <sample id="527">如我们所见，指令微调可以显著提高大模型在未见过的多模态任务上的性能。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="528">从自然指令数据集进行迁移学习也能对指令微调有益。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="529">随着任务数量的增加，模型实现了更好的性能，同时降低了敏感性。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="530">我们还做了一个实验，我们使用了一种指令和五种指令。正如我们所见，使用更多指令可以提高模型的整体性能，并且大幅降低其敏感性。</sample>
    <sample id="531">这显示了不同微调策略对模型敏感度的影响。正如我们所见，通过从Natural Instruction数据集进行迁移学习，模型可以比原始的Llama模型实现更好的敏感度。</sample>
    <sample id="532">我们也可以看到，从Natural Instruction数据集进行迁移学习，可以帮助OFA在Natural Instruction数据集上取得更好的性能。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="533">总之，我们提出了第一个大规模多模型指令调优数据集，显著提高了大模型的零样本能力，我们探索了不同的迁移学习技术并展示了它们的好处，我们设计了一个新的度量标准叫敏感性。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="534">我们正在收集一个更大的多模态指令调优数据集，包含大约150个额外的视觉语言任务，我们将会发布它们。这是我们的数据和模型的二维码。谢谢。</sample>
    <sample id="535">The University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Javad Hosseini。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="562">嗨，大家好，我是Costa Senna，很荣幸欢迎你们参加我们关于ACL 2023论文的讨论。语言模型的可接受性判断并不总是对上下文具有鲁棒性。</sample>
    <sample id="563">与John Gockier，Aaron Muller，Kanishka Mehta，Karen Fuentes，Roger Levy和Athena Williams合作。</sample>
    <sample id="564">在这个工作中，我们重新审视了最小对偶范式。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="565">最小对偶范式基本上在可接受性判断的基础上评估语言模型，这些判断也可以包括语法性，像语法错误，语法病句，或者在刻板印象方面的可接受性，比如“小熊”。</sample>
    <sample id="566">在这个最小对数范式中，评估语言模型的典型方式是展示一个可接受的句子或一个语法正确的句子，然后展示一个不可接受的句子或一个语法错误的句子。</sample>
    <sample id="567">然后希望这个模型基本上会把更多的概率给可接受的句子。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="568">目前的MPP管道基本上不允许我们评估模型对较长句子的接受度。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="569">这些天，大型语言模型的上下文窗口越来越长，因此，评估模型在整个上下文窗口中的接受性至关重要。</sample>
    <sample id="570">这就是我们在这里尝试做的事情。我们尝试通过让模型对越来越长的序列进行可接受性评估来重新审视T5P管道。</sample>
    <sample id="571">所以这就是方法。我们做的是开始模拟这些较长的序列，重新审视数据集本身，然后通过选择那些可接受或不可接受的句子来重新创建句子。</sample>
    <sample id="572">嗯.让我想想怎么翻译比较准确.这句话的意思是：。"例如，我们从BLiMP数据集中选择了一个典型的从属岛结构的语法对。"。要不要我帮你翻译其他句子？</sample>
    <sample id="573">我们所做的就是重新创建较长的序列，这些序列是可接受的，并且具有相同的语法结构匹配。我们从AdGenTile中提取语法句子。</sample>
    <sample id="574">然后我们把它作为前缀添加到可接受查询和不可接受查询的前面。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="575">所以，我们可以通过选择同一匹配中的不可接受句子来做同样的事情，这也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们也可以通过从不同的子集或不同的数据集选择句子来做到这一点。这就是我们所说的“错配场景”。</sample>
    <sample id="577">所以这些句子仍然来自相关数据集，但不是你用来评估的那个数据集。我们也可以对不可接受性案例做同样的事情。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域选择句子，比如维基百科。</sample>
    <sample id="579">这会告诉我们，模型的可接受性判断是否真的受任何上下文的影响。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="580">嗯…这得看具体是啥样的数据集，还有这个句子的结构啥的。如果数据集里有不同部分的数据，那可能就来自不同的子集，要是句子结构复杂，可能就和数据集无关了。你得给我更多关于这个数据集和句子的信息，这样我才能更准确地判断。你可以再和我说说具体情况呀。</sample>
    <sample id="581">首先，我们看维基百科的句子，这些句子与当前查询对完全无关，然后我们发现MPP判断在任意上下文长度下大多是稳健的。</sample>
    <sample id="582">我们把上下文长度增加到2024个词，以最大化OPT和GPT-2模型的能力，如图中橙色点线所示，MPP判断相对稳定。</sample>
    <sample id="583">嗯…如果从同一个数据集里选择句子，可能会导致模型在训练时出现过拟合的情况。因为数据集里的句子可能有很强的关联性，这样模型就容易记住数据集里的特定模式，而不是真正学到泛化的知识。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="584">我们正在从同一个blimp语法数据集的可接受和不可接受领域中选择或创建句子。</sample>
    <sample id="585">然后我们看到，当添加可接受前缀或不可接受前缀时，MPP 判断要么显著增加，要么显著减少。</sample>
    <sample id="586">但是当我们匹配结构，也就是从同一现象的文本中选择句子时。</sample>
    <sample id="587">根据所选前缀是否可接受，我们看到MPP判断值出现巨大增加或巨大减少。</sample>
    <sample id="588">这个…这个非常大，像这种效果随着上下文长度增加而增加，这可能会对那些有大上下文窗口的新一代语言模型产生影响。</sample>
    <sample id="589">匹配前缀对语言模型的判断影响很大，因为匹配前缀能提供很多上下文信息，这些信息对于理解后续内容很重要。如果匹配前缀不准确，后续的判断就会出错。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="590">我们做了一系列分析，尝试通过添加噪声来扰乱输入句子，同时尽量保持其相关结构。在做了几次这样的扰动之后。</sample>
    <sample id="591">我们发现这些噪音实际上并没有让模型改变其在展示已训练的MPP时的路径选择。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="592">基本上，我们发现模型对扰动句子的敏感性相似。</sample>
    <sample id="593">当我们在可接受域内扰动句子时，我们看到所有扰动的相似增加，而当我们在不可接受域内扰动句子时，我们看到MPP判断以相似的方式减少。</sample>
    <sample id="594">我们工作的关键收获是，语言模型对句子中共享的潜在语法和语义特征是敏感的。</sample>
    <sample id="595">目前我们用短且单句输入的方式进行MPP评估，可能无法完全捕捉到语言模型在上下文窗口中的抽象知识。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="596">请阅读我们的论文以获取更多实验的细节。谢谢你的聆听。</sample>
    <sample id="597">该方法的第一步将输入词元映射到一个无序的多重集。</sample>
    <sample id="598">55000个。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="626">DEplain的最佳对齐方法是mass align。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="627">Weakly supervised learning can robustly train neural networks on noisy label data so that the trained models still generalize well.</sample>
    <sample id="628">抱歉，你给的英文内容里没有提到DEplain-web文档的对齐方法分配情况呢。你可以再给我点其他信息吗？</sample>
    <sample id="629">CoNLL++数据集是从2020年的路透社新闻中收集的，然后用CoNLL 2003标注指南进行标注。如果还有疑问可以继续问我哦。</sample>
    <sample id="630">大家好，我是来自宾夕法尼亚大学的李胜章。今天我将介绍我们的工作，即跨语言语义解析在多种自然语言和多种表示中的示例。</sample>
    <sample id="631">语义解析是构建用户查询，如SQL和lambda演算等的语义表示的任务。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言的查询翻译成多种语义表示的任务。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="633">如图所示，我们需要使用神经模型将查询翻译成多种自然语言，如SQL，Lambda或FQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型是分别提出的，并在有限任务和应用的数据集上进行评估，例如。</sample>
    <sample id="635">看起来某些自然语言的报道中，中文是缺失的。</sample>
    <sample id="636">对某些数学表示的覆盖不足。</sample>
    <sample id="637">λ演算缺失了。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="638">或者它们只在某些神经模型上进行评估，例如只在一个单一模型上评估它们。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="639">为此，我们提出了Exemplar，它为跨语言半监督解析提供了统一的数据集，适用于多种自然语言和表示形式。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="640">它包含90个数据集，涉及各种领域，570个语义解析任务，800万种表示，22种自然语言，分布在15个语系中。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="641">为了更好地评估我们的基准，我们考虑了训练和评估的六个设置。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="642">第一个是翻译测试。我们使用Google Translate API将源语言翻译为目标语言，然后使用单语模型进行训练和评估。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="643">嗯，比如说，我们用英语查询训练了模型，在推理的时候，把德语查询用API翻译成英语，然后用训练好的模型来预测SQL。如果还有啥疑问，你可以再跟我说哦。</sample>
    <sample id="644">我们还将测试单语模型。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="645">你得给我英文内容啊，不然我没法翻译呢。你可以把英文内容发给我，这样我就能给你翻译了。</sample>
    <sample id="646">我们也测试了单语融合设置，用只有10%训练数据的单语模型进行训练。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="647">我们训练了一个针对所有语言的多语言模型。</sample>
    <sample id="648">例如，我们将德语，英语，中文查询放在一起训练一个多语言模型，在推理过程中，我们可以使用这个模型。</sample>
    <sample id="649">你得告诉我是啥英文内容呀，这样我才能翻译呢。快把英文给我吧。</sample>
    <sample id="650">我们还考虑跨语言零样本和少量样本迁移。我们在一种源语言上进行训练，然后迁移到另一种语言。</sample>
    <sample id="651">在训练时，我们会用英语查询或者英语和德语的混合查询来训练一个多语言模型，以预测SQL输出。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="652">我们还发现了许多有趣的结果。关于单语模型的分析，我们将评估两组模型。</sample>
    <sample id="653">包括编码器PDR，即基于指针的多语言预训练编码器，如XLM-R + PDR和BERT + PDR。</sample>
    <sample id="654">我们还评估了编码器-解码器模型，即多语言预训练的编码器-解码器模型，例如MBart和MT5。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="655">我们发现编码器-解码器在所有9个数据集上都取得了最佳性能。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="656">我们在MT-5和XLM-R + PDR上进行多语言设置的评估。</sample>
    <sample id="657">我们发现，编码器-解码器或编码器-PTM可以通过在多种语言的混合中进行训练来得到改进。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="658">我们发现，这是因为大多数主要自然语言可以获得性能提升，除了英语在七个数据集中的性能下降，在三个数据集中仅获得提升。</sample>
    <sample id="659">我认为这被称为“多语言诅咒”。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="660">我们也比较了跨语言性能差距。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="661">在这幅图中，蓝色线条是跨语言零样本迁移，橙色线条是跨语言一样本迁移，绿色线条是模型角度设置。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="662">我们发现，在零样本设置下，通过比较绿色和橙色线条，跨语言迁移性能差距显著。而通过比较蓝色和橙色线条，在少量样本设置下，迁移差距迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如，编码器-解码器在英语自然语言上都优于先前的工作，或者取得了相当的结果，在目标自然语言上显著提升了FUSAL的性能。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="664">多语言语言模型，如Code and Bloom，在跨语言语义解析任务中仍然不够。</sample>
    <sample id="665">构建一个跨角度语义解析的范例，一个包含多种自然语言和语义表示的统一基准。</sample>
    <sample id="666">我们对三种代表性的多语言模型进行了全面的基准研究，结果显示了许多有趣的发现，等等。欢迎访问我们的论文和代码。谢谢聆听。</sample>
    <sample id="667">嗯…这得看具体是哪方面的研究了。不过一般来说，这四类现有的研究可能包括理论研究，实验研究，案例研究和文献综述。理论研究就是基于理论框架来探讨问题，实验研究通过实验来验证假设，案例研究是通过具体案例来分析现象，文献综述则是对已有文献的总结和分析。你可以和我说说具体是哪方面的研究吗？这样我能回答得更准确。</sample>
    <sample id="668">不，Codex和Bloom等多语言LLM对于CLSP来说是不足够的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="695">该方法通过在训练中引入对齐来处理排列的不确定性。</sample>
    <sample id="696">嗯…这个嘛，下游NLP模型的公平性，就是说在处理不同群体，不同政治观点等数据时，不能有偏见。比如说，不能因为政治观点不同就对某些群体有歧视性对待。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="697">Janice Lavoie</sample>
    <sample id="698">Kostas Fina.</sample>
    <sample id="699">Myra。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="700">在本文的背景下，热带主义意味着对拉丁裔女性的描述，比如“vibrant”和“curvaceous”这些词所反映的对她们的一种刻板印象。嗯，你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="701">作者通过使用像“culture”，“tradition”，“proud”和“exotic”这样的词来创建目标群体的人工描写，这些词定义了这些群体与身份的关系，并将它们与白人规范区分开来。</sample>
    <sample id="702">本文中使用了Pointwise CxMI来衡量语境使用情况。</sample>
    <sample id="703">DrBERT有7GB的NATCHOS数据，ChuBERT有4GB的句子数据，ChuBERT的句子数据来自clean conduct，DrBERT没有提到句子数据来源。嗯，你要是还有啥疑问可以再问我哈。</sample>
    <sample id="751">这篇论文有两位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="752">迭代迁移学习就是通过每次迭代更新模型，每次迭代都用最新收集的数据集来训练模型。</sample>
    <sample id="753">数据集的目标是理解用户语言，当用户想做选择时。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="754">嗯…这英语内容里没提到攻击者通过EaaS提取模型参数的事儿啊。你是不是还有别的信息没告诉我呢？要是有更多信息的话，你可以再和我说一说呀。</sample>
    <sample id="755">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="756">嗯…这个信息没给呢，所以不知道有多少个注释者用于创建初始数据集。你要是还有别的信息或者想法，可以再和我说一说呀。</sample>
    <sample id="757">Carnegie Mellon University，University of Washington，The Allen Institute for AI。</sample>
    <sample id="758">“Isobutane”和“lisa”是左侧为支配词的示例。如果还有疑问，欢迎继续问我。</sample>
    <sample id="759">嗯…这个我不太清楚呢。你可以再给我点其他信息吗。</sample>
    <sample id="760">因为大型语言模型的上下文窗口越来越长，所以需要在整个上下文窗口中评估模型的可接受性。</sample>
    <sample id="761">Yes.</sample>
    <sample id="762">注释者提前知道该实体的名字，但不一定知道关于实体的其他情况。</sample>
    <sample id="763">没提到评估使用了哪些MT指标呢。你可以再给我点信息吗？</sample>
    <sample id="764">嗯…这个我不太确定呢。你要是能给我点更多的信息就好了，比如这个回归是啥样的，是线性回归还是其他的？这样我就能更好地回答你了。</sample>
    <sample id="765">嗯…这个嘛，立场在NLP里很重要，因为不同的立场会影响对文本的理解和处理。比如说，如果立场不同，对同一个评论的判断可能就不一样。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="766">嗯…这个我不太清楚呢。你可以去查查相关的技术文档或者问问其他懂这方面的人。希望你能早点找到答案，要是有新的发现也可以跟我说说呀。</sample>
    <sample id="767">他们使用的是先对CE任务进行微调，然后进一步对debate任务进行微调的模型。</sample>
    <sample id="768">抱歉，你给的英文内容和问题不匹配，我无法回答。你可以再提供一些和问题相关的英文内容吗？</sample>
    <sample id="769">三条。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="770">嗯…这个信息里没提到与最强基线相比的收益数值呢。你可以再找找其他资料或者再问问我呀。</sample>
    <sample id="771">Shuheng.</sample>
    <sample id="772">可以。</sample>
    <sample id="773">嗯…这个信息里没提到进行了多少个较小模型的实验呢。你可以再找找其他相关的信息。</sample>
    <sample id="774">OFA被用作研究多模型指令调整的基础模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="833">这篇论文的作者所属机构是Google Translate。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="834">Stony Brook University。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="835">嗯…这信息里没提到论文分析了哪些语言对呢。你可以再找找其他相关资料。</sample>
    <sample id="836">Zhang Bing.</sample>
    <sample id="837">研究了两个模型，一个是长输入模型，另一个是正常基础长输入模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="838">53个任务用于训练和测试目的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="839">抱歉，你给的这段内容里没有提到作者数量，所以我无法回答这个问题。你可以再给我点其他信息吗？</sample>
    <sample id="840">作者在实验中使用了20 Newsgroups，MIMIC-III，SST-2和Iris Spam这四个数据集。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="876">NACHOS是医疗爬虫数据集。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="877">Aid Bilal</sample>
    <sample id="878">The prompting has a big influence on the performance of the LLMs for translation.</sample>
    <sample id="879">嗯…这个信息里没提到作者所属机构呢。你可以再找找其他资料，或者咱们再聊聊这篇论文的其他内容。</sample>
    <sample id="880">抱歉，你给的英文内容里没有提到5个由专家编写的指令，所以我无法回答。你可以再给我点信息吗？</sample>
    <sample id="881">作者建议通过引入一个核心参考解析任务来测试模型，这个任务旨在测试模型从不同来源获取知识的能力。</sample>
    <sample id="882">嗨，大家好，我的名字是艾德·比拉德，我将给大家简要回顾一下《纸张打印端翻译：评估策略和性能》这篇论文。这是我和谷歌翻译的同事们共同完成的工作。</sample>
    <sample id="883">通义千问是去年2022年推出的一个54亿参数的大型语言模型。它是在包含1800亿个词的大量文本数据上进行训练的。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="884">在厨房的布局方面，它在数百个最终测试中达到了最先进的状态。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="885">在这项工作中，我们提出了机器翻译中语言模型提示的首个系统性研究。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="886">我们使用了AMT社区的最佳实践来评估语言模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。</sample>
    <sample id="887">嗯，我们比较了两个最先进的系统，所以最佳表现的系统在WMT评估中胜出。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="888">我们使用最先进的神经元度量，并且还展示了基于专家的人工智能评估结果。最后，我们提供了一些关于提示选择策略的建议。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="889">提示对LLM的翻译性能有很大影响，如在简单实验中，我们使用一击提示，为不同的句子提供了两种不同的提示。</sample>
    <sample id="890">在560个句子中，观察到的差异超过1个词点。</sample>
    <sample id="891">嗯，这个在极端情况下可能高达40个模糊点。所以，选择一个好的提示策略是很重要的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="892">在我们的实验中，我们采用了五句提示策略，就是我们给系统提供的句子，用它们的语言标记。</sample>
    <sample id="893">好的，你可以说出你要翻译的英文内容了。</sample>
    <sample id="894">我们看到，在多项式拟合中，实际的拟合形式对结果影响不大。</sample>
    <sample id="895">对于零和一射提示来说，这是至关重要的。而在我们这里，当提到五射提示时，实际上对提示的真正形式几乎没有区别。</sample>
    <sample id="896">例子承载了大部分的重量。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="897">我们实验结果的总结是，样本质量比与源句子的相似性更重要。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="898">所以从高质量的翻译中选择例子很重要。特别是我们比较从WMT评估的训练数据或测试数据中选择提示。</sample>
    <sample id="899">测试数据比训练数据更精炼，质量更高，更有序，因此使用测试数据时性能更好。</sample>
    <sample id="900">然而，最先进的系统在某些方面具有显著优势，但Palm已经非常接近商业系统了。在我们的情况下，我们选择使用谷歌翻译。</sample>
    <sample id="901">我们通过使用M2M框架进行的EMI评估获得的见解是，Palm的流畅性与最先进的系统相当，但主要差异在于准确性。</sample>
    <sample id="902">特别是最常见的错误是遗漏错误。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="903">嗯，你得告诉我是啥英文内容啊，这样我才能翻译呢。</sample>
    <sample id="904">然而，对于Palm的软件风格类别比最先进的系统要低，这是一个额外的信号。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="905">那部分提供很流利的输出，但准确性还是有些问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="906">这就是这个非常简短的概述。要了解更多细节，请来参加论文的完整报告。非常感谢。</sample>
    <sample id="907">你好，我是大卫，是德国萨兰特大学的博士生。在这段视频里，我想展示我们最近的研究成果《弱于想象：对每周供应率的批判性审视》。</sample>
    <sample id="908">这是与小雨神，Mario Smuspa，以及嘉宾Stephen和Detis Clarko的合作。</sample>
    <sample id="909">我想先简单介绍下弱监督和半监督学习。</sample>
    <sample id="910">弱监督中，你不会手动标注数据。相反，我们使用弱标注源来标注数据，比如简单的启发式规则，知识库或低质量的外包标注，如图右所示。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="911">与人类注释相比，弱注释要便宜得多，但它们也很嘈杂，意味着一定比例的注释是错误的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="912">如果直接用微粒标签数据训练神经网络，神经网络倾向于记住标签噪声，而不能泛化。</sample>
    <sample id="913">在半监督学习中，提出了训练算法来稳健地在这样的标签噪声下训练神经网络，使得训练好的模型仍然能很好地泛化。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="914">在最近的WSL工作中，一个常见的说法是人们声称他们只在每周标注的数据上训练模型，并在干净的测试集上取得了高性能。</sample>
    <sample id="915">从技术上讲，这个说法没有错，但有个陷阱。</sample>
    <sample id="916">人们通常假设有一个额外的干净验证集可用于模型选择。</sample>
    <sample id="917">这个问题设置导致我们卡住了，这意味着在弱监督学习中需要额外的手动注释。但就像房间里的大象一样，这种必要性经常被忽视。</sample>
    <sample id="918">提到的引用让我们问三个研究问题。第一个是干净的验证数据对WSL来说是必要的吗？还是我们可以用一个有噪声的验证集代替呢？</sample>
    <sample id="919">如果WSL需要干净数据或者干净数据是WSL正常运行的强制要求，那么我们需要多少干净样本？最后，我们是否应该只用干净样本进行验证，还是有其他更好的利用它们的方式？</sample>
    <sample id="920">我们在工作中解决了这些研究问题，我们的发现如下。</sample>
    <sample id="921">首先，我们发现有趣的是，最近的WASL方法确实需要干净的白底样本才能正常工作。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="922">否则，性能会大幅下降，如图所示。如果没有干净的验证样本，训练模型就不能泛化到原始的训练标签之外。</sample>
    <sample id="923">训练毫无意义。</sample>
    <sample id="924">这表明WASL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的注释成本不应被忽视。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="925">我们的第二个发现是，增加清洁验证样本的数量将有助于WSL方法实现更好的性能，如图左侧所示。</sample>
    <sample id="926">通常，我们只需要每个类别20个样本就能达到高性能。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="927">但这不是故事的结尾，因为如果我们决定直接访问干净样本，那么直接在它们上进行训练甚至会获得更好的性能。</sample>
    <sample id="928">右边的图显示了直接应用于干净数据的微调方法和仅用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">正如我们所见，如果有10个样本每类，直接微调开始超越WSL方法。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="930">最后，之前WSL方法中声称的性能改进可以通过允许在干净的验证样本上继续微调来轻松实现。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="931">从图中可以看出，瓦林纳模型，称为FTW，最初在性能上不如更复杂的WSL方法，如COSINE。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="932">然而，如果我们允许在干净样本上继续微调，那么FTW的表现与其他方法一样好。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="933">在实践中，没有理由选择更复杂的WASL方法，这些方法需要更多的计算时间和磁盘空间。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="934">我们表明，最近的WSL方法需要干净的手动标注样本才能正常工作，它们的性能和实用性被严重高估了。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="935">我们对未来工作的具体建议如下。</sample>
    <sample id="936">首先报告模型选择标准，例如报告模型选择是否使用干净的验证样本。</sample>
    <sample id="937">第二，WSL方法应该与基于短语学习的基线进行比较，因为它们都处理清晰的示例。第三，持续微调是一种简单而强大的基线，应该在未来WSL的工作中加以考虑。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="938">最后，我们开源了代码。您可以通过这页幻灯片上的二维码找到它。请随意查看。谢谢，祝您会议愉快。</sample>
    <sample id="939">对话系统的常用评估方法是用人类评价，像让人类评委选对话的好坏或者给对话打分。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="940">这篇论文有五位作者。如果还有其他关于这篇论文的问题，你可以随时问我哦。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要的背景知识是：法官在法庭上决定案件。你要是还有啥疑问，尽管跟我说哈。</sample>
    <sample id="942">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="943">嗯…这个信息里没提到NLPositionality的注释者在各人口统计学特征方面是否均衡呢。你还有其他关于这个话题的信息吗。</sample>
    <sample id="944">嗯…这个嘛，具体做法没说清楚呢。不过可以尝试在句子中添加一些不影响整体结构但又有点干扰的词或者短语。你要是还有啥想法或者疑问，咱们可以再聊聊呀。</sample>
    <sample id="945">进行维度评估就是对多方面进行评价，了解模型在不同方面的优缺点。嗯，你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="946">University of Science and Technology of China.</sample>
    <sample id="947">在0 - 1 shot prompting的情况下，提示的形式很重要。</sample>
    <sample id="978">作者没有具体提到评估了哪些对话模型。</sample>
    <sample id="979">抱歉，你给的内容里没有提到论文作者数量的信息。你可以再给我点其他相关的内容吗？</sample>
    <sample id="980">优秀规划器应该能写出合理且符合约束的脚本。</sample>
    <sample id="981">抱歉，你给的内容里没有提到作者数量，所以我无法回答这个问题。你可以再给我点信息吗？</sample>
    <sample id="982">Vasudha</sample>
    <sample id="983">抱歉，你只给了论文题目，没给论文内容，我没法知道作者所属机构呢。你可以再给我点信息吗？</sample>
    <sample id="1021">PaLM最常见的错误是省略错误。如果还有其他关于PaLM的问题，欢迎随时问我。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将告诉你关于ABC-Eval的所有事情，这是一个评估对话型AI的新维度方法。</sample>
    <sample id="1023">这项工作是由Emory NLP实验室，由教授Gino Choi领导，在Emory大学进行的，并与Amazon Alexa AI合作完成的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1024">嗯…你刚刚开发了一个对话模型，想看看它和当前最先进的比起来表现咋样。</sample>
    <sample id="1025">常见的做法是使用人类评估，比如让人类评委选择两个对话中哪个更好，或者给定一个Likert量表来对对话进行评分。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1026">这些方法很好地提供了对整体对话质量的全面评估，但对话质量有很多方面。因此，你可能想要在更细致的层面评估聊天质量的多个维度，以了解模型的优缺点。</sample>
    <sample id="1027">一种方法是让人类评委对对话质量的几个维度进行评估，比如模型回复的相关性，使用现有的比较或等级量表方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的策略来评估维度对话。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1029">我们的方法试图通过明确标注每个模型响应是否表达某些行为，如给出不相关的信息或自相矛盾，来减少人类评估的主观性。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1030">我们称这种方法为“聊天行为注释”或“ABC-Eval”。我们开发这个方法是为了全面覆盖最近文献中建议会影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC eval能够测量聊天模型犯各种主题错误的速率。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1032">例如，ABCEval衡量聊天模型在多少轮次中忽略其伙伴或者说出不相关的内容。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1033">与自己或其伙伴矛盾，产生不正确的事实或违背常识知识，以及当模型成功或失败时没有表现出共情。</sample>
    <sample id="1034">为了确定哪种评估最有效，我们选择了4个最先进的聊天模型，并使用ABC eval对每个模型进行了100个人机对话的评估。</sample>
    <sample id="1035">为了比较，我们还使用了三种现有方法来评估这些对话：在轮次层面的LICRAT评分，在对话层面的LICRAT评分以及对话层面的两两比较。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1036">对于现有的方法，我们收集了八个最常测量的对话方面评估，因为这是评估聊天模型在多个维度上的标准做法。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1037">从我们对这些评估结果的分析中发现，ABC行为标签在100个双标注对话上的内注释者一致性上，总体上比现有方法收集的标签更可靠。</sample>
    <sample id="1038">此外，ABC-ELA标签比现有方法产生的指标更能预测对话的整体质量，正如简单线性回归分析所显示的那样。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1039">例如，你可以看到，测量自我和伙伴矛盾的比例分别解释了5%和10%的对话质量，而平均Likert一致性得分只解释了4%或更少。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1040">最后，我们使用逐步线性回归来检查每个评估指标是否能捕捉到对话质量的独特方面。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1041">你可以看到，所有ABCE评估指标的组合解释了超过25%的对话质量，当你一次移除一个指标时，大多数都会导致失去相当一部分关于质量的信息。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1042">另一方面，所有轮次的Likert指标组合解释了质量的更少部分，并且这些指标中携带独特信息的更少。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1043">这些可靠，信息丰富且独特的ABCEVAL指标使我们能够以比以往方法更高的分辨率评估对话式AI。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1044">在我们实验的结果中，有几个挑战仍然存在，并且被精确量化了。例如，我们测试的机器人在大约20%的回应中存在常识性错误。</sample>
    <sample id="1045">他们大约在15%的回复中产生无关信息，大约10%的时候自相矛盾或者与对方矛盾。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1046">随着该领域改进速度的加快，自我们评估以来，许多这些错误率可能会在新模型发布时下降。然而，这正是更需要追求可靠和精确的评估指标来比较模型的原因。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1047">我们希望ABC eval能被该领域的其他人利用，作为朝着这个方向迈出有意义的一步。我们期待着看到在接下来的几个月和几年里，对话式AI将如何发展。谢谢观看。</sample>
    <sample id="1048">Emory NLP lab，Emory University。</sample>
    <sample id="1049">文中没有提到CFT代表什么。</sample>
    <sample id="1050">这篇论文有六位作者。如果还有其他关于这篇论文的问题，你可以随时问我。</sample>
    <sample id="1051">你好，我的名字是Kay Oyen，我将要展示我们的工作，标题是《何时需要上下文翻译：基于数据的多语言探索》。这项工作是与Patrick Franas，Emil Liu，André F. T. Martins和Graham Neubig合作完成的。</sample>
    <sample id="1052">“mole”在不同的语境下有不同的翻译。如果是指“鼹鼠”，那就是“鼹鼠”，如果是指“间谍”，那就是“间谍”。你是在做什么翻译相关的工作吗。</sample>
    <sample id="1053">如果前面的句子是“如果部长们发现的话，事情可能会变得危险”，那么“more”指的是一个间谍。但如果前面的句子是“医生，这可能不是什么严重的事吧”，那么“more”指的是一个胎记。</sample>
    <sample id="1054">嗯，是的。根据上下文，单词的意思会改变，因此它的翻译也会改变。如果还有其他翻译相关的问题，你可以随时问我哦。</sample>
    <sample id="1055">然而，评估模型在处理像这样的对比案例时表现得如何是很困难的。首先，因为只有很小一部分翻译依赖于上下文，这使得语料库级别的指标，如BLEU，无法捕捉这些翻译。</sample>
    <sample id="1056">有些人建议对上下文相关的翻译进行有针对性的评估，但这些资源仅支持有限类型的上下文相关翻译和有限的语言集，因为它们通常依赖于领域知识和人工编纂。如果还有其他翻译相关的问题，或者想讨论这个话题，都可以跟我说哦。</sample>
    <sample id="1057">在这个工作中，我们试图回答这两个问题：首先，为什么翻译需要上下文？其次，模型在处理这些情况时表现得有多好？</sample>
    <sample id="1058">为了回答第一个问题，我们首先测量了在翻译中单词依赖上下文的程度。</sample>
    <sample id="1059">在之前的工作中，我们介绍了CMI作为机器翻译模型对上下文使用的度量。这是通过测量上下文C在给定源X的情况下，提供了多少关于目标Y的信息来完成的。如果还有其他翻译相关的问题，或者想讨论这个话题的其他方面，都可以跟我说哦。</sample>
    <sample id="1060">你可以把CMI看作是给模型提供上下文后获得的信息增益。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1061">在这项工作中，我们扩展了CXML到点对点CXML，它可以测量句子或单词层面的上下文使用情况。我们可以将具有高pCXML的单词视为需要上下文翻译的单词。如果还有其他翻译相关的问题，或者想讨论这个翻译的准确性之类的，都可以跟我说哦。</sample>
    <sample id="1062">现在我们分析高P值的词，以寻找这些词之间的模式。</sample>
    <sample id="1063">我们对14种不同语言的TED演讲的翻译文本进行分析。</sample>
    <sample id="1064">我们进行分析在三个不同的层次。首先，我们查看具有高平均值的词性标记。</sample>
    <sample id="1065">这让我们能够找到，例如，在阿拉伯语中具有相对高P值的双数代词。这可以解释为，因为英语没有双数代词，所以在翻译成阿拉伯语时，需要语境来确定一个代词是否是双数。</sample>
    <sample id="1066">同样，我们发现某些语言在选择合适的动词形式时也需要上下文。然后我们查看那些在所有不同出现中平均有高P值的词汇项。</sample>
    <sample id="1067">嗯…这个工具能帮助识别像这里这样的情况，在中文里需要语境来正确翻译专有名词，以确保在文档里使用相同的翻译。</sample>
    <sample id="1068">嗯.让我想想怎么翻译比较准确.啊，这句话的意思是：。"同样，我们发现上下文支持了正确的翻译形式。"。要不要再聊聊其他翻译相关的问题？</sample>
    <sample id="1069">最后，我们看那些具有高PXM I的不同的个体标记，这让我们能够识别出那些不能被单词本身捕捉到的现象，但这些现象在句子结构中进一步表达，比如省略号解析。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1070">现在我们利用分析得出的发现来设计文档级翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五个话语现象，我们创建了自动识别与现象相关单词的标签，我们称这个标签为多语言话语意识，MUDA，标签。</sample>
    <sample id="1072">然后我们还可以注意到，不同的语言在这些语义现象中的比例是不同的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1073">然后我们使用MUDATagger，在我们想要用于评估的平行语料库上应用它，然后在MUDATagger识别出的上下文相关示例上应用我们选择的翻译度量。</sample>
    <sample id="1074">最后，我们使用基准以及其它指标来评估不同模型在文档级机器翻译上的表现。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1075">首先，当我们使用语料库级别的指标时，对于BLEU，我们发现上下文无关模型具有最佳性能。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1076">但是，如果我们使用Comet上下文，模型表现最好。而如果我们使用Word F - measure，那么有上下文和没有上下文的模型性能相当。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1077">这再次表明，如果我们仅使用语料库级别的指标，就很难确定最佳的文档级翻译系统。如果还有其他翻译相关的问题，或者想讨论这个话题，都可以跟我说哦。</sample>
    <sample id="1078">现在我们使用MMLU基准来评估模型，我们发现上下文理解模型在某些话语现象，如正式性和词汇连贯性方面，比不使用上下文的模型显著更准确。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1079">但是这些模型在其他现象，如省略，代词和动词形式等方面，没有使用上下文的模型表现得更好。这暗示了在文档级翻译方面，我们需要看到更多的进展。</sample>
    <sample id="1080">我们也比较了不同的商业系统，我们的基准显示，在文档级翻译方面，DeepL通常比谷歌翻译更准确。如果还有其他翻译相关的问题，或者想聊聊其他事，都可以跟我说哦。</sample>
    <sample id="1081">我们对14个语言对进行数据驱动分析，以确定何时翻译需要上下文。</sample>
    <sample id="1082">然后我们利用这些发现来建立一个文档级机器翻译的基准，这有助于我们识别哪些离散现象模型能够很好地处理，哪些翻译系统在文档级翻译方面表现良好。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1083">非常感谢你的关注，我们将在多伦多见。</sample>
    <sample id="1084">Justin John</sample>
    <sample id="1121">没有名称。</sample>
    <sample id="1122">“显性词汇”方法是识别区分显性群体和非显性群体的词汇的方法。</sample>
    <sample id="1123">The author's institution is the University of Washington.</sample>
    <sample id="1124">The Prague approach.</sample>
    <sample id="1125">James Finch和Sarah Finch。</sample>
    <sample id="1126">这篇论文有四位作者。如果还有其他关于这篇论文的问题，你可以随时问我哦。</sample>
    <sample id="1127">嗯…这个工作里提到的Minimal Pair paradigm可以用于测试句法现象，像Blimp Syntax Gym就属于这种类型的数据集。你要是还有啥想知道的，尽管再问哈。</sample>
    <sample id="1161">你没说第一个研究问题的五种方法的缩写呢，我没法回答。你可以再给我点信息吗？</sample>
    <sample id="1162">该模型在11个生物医学和临床学习任务上进行了评估。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1226">CamemBERT最初是在4GB的NATOS子集上训练的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1227">Adam Skowroński</sample>
    <sample id="1228">发现是随着时间间隔变大，性能下降，这证实了时间漂移是性能下降的主要原因。</sample>
    <sample id="1269">因为输出序列中的词元在第一步后没有顺序，需要排列才能得到正确的顺序。</sample>
    <sample id="1270">因为不知道积极刻板印象是由于过度的价值对齐还是其他反刻板印象方法导致的有害模式。</sample>
    <sample id="1271">最小对不可接受输入就是给语言模型一个可接受的句子和一个不可接受的句子，然后看模型对它们的处理。嗯，你要是还有啥疑问可以再问我哈。</sample>
    <sample id="1272">嗯…这个信息里没提到评估指标呢。你还有其他关于这个实验的问题吗。</sample>
    <sample id="1273">使用了内注释者一致性来衡量注释者之间的一致性。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1274">在不可接受和可接受查询中，选择Wikipedia这个领域来添加完全无关的句子。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1275">抱歉，给定的内容中没有提到论文作者所属机构的信息。</sample>
    <sample id="1276">嗯…这个嘛，MultiInstruct和其他基准的不同点在于，它关注的是多模态任务，而很多之前的指令调优工作主要集中在语言只任务上。还有啊，它发现NLP和多模态在指令数据集的可用性上存在很大差距，NLP有超过1600个语言只指令任务，但多模态没有大规模公开可用的指令任务。这促使他们构建了多模态指令调优数据集。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="1277">嗯…这个我不太清楚呢。你可以去查查论文的作者信息。不过你要是还有其他关于这篇论文的问题，也可以跟我说呀。</sample>
    <sample id="1278">抱歉，你给的英语内容里没有提到二进制协调的定义，所以我无法回答。你可以再给我点关于这个话题的信息吗？</sample>
    <sample id="1279">嗯…这个我不太清楚呢。你可以再找找相关的研究资料，或者问问其他同事。希望你能早点找到答案，要是有新的消息也可以跟我说一说呀。</sample>
    <sample id="1280">嗯…这些发现表明较小的T5模型在合适的语料上训练后，能生成比大多数大型语言模型质量更高的脚本。这就意味着较小的T5模型在某些方面能超越大型模型呢。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="1281">嗨，我是Yannick Slabrock，我将向您展示我们在生物医学和临床领域中关于Dr.Bert，一个鲁棒预训练模型，的工作。</sample>
    <sample id="1282">在这次演讲中，我们首先会讨论医疗保健中的语言建模，然后我们将展示我们文章的主要贡献。如果还有其他翻译相关的问题，或者想聊聊关于这个演讲的其他事儿，都可以跟我说哦。</sample>
    <sample id="1283">我们介绍了第一个生物医学模型，名为Dr.Bert，它是基于Roberta的，并在MedNLI这个医疗领域有标注数据的网页数据集上进行训练。</sample>
    <sample id="1284">我们还介绍了模型在多种预训练设置和数据源下的比较。然后，我们用法语呈现了在11个生物医学和临床学习任务上的结果。如果还有其他翻译相关的问题，或者想讨论下这个翻译的内容，都可以跟我说哦。</sample>
    <sample id="1285">最后，我们总结了实验，并给你提供更多关于如何访问模型的细节。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，并与历史上的静态和上下文化方法，如Word2Vec，FastText或ELMo相比，提供了巨大的性能提升。</sample>
    <sample id="1287">从那以后，这个模型已经被改编到许多其他语言，比如在法语中，有Camembert，在生物医学领域，有BioMedBERT和BioBERT，在临床领域，有ClinicalBERT，但主要是在英语中。</sample>
    <sample id="1288">其他语言的专门模型很稀缺，而且通常基于持续预训练，因为缺乏领域内的数据。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1289">然而，直到现在，法国都没有一个开源的医疗模型。</sample>
    <sample id="1290">所以我们问自己，对于各种用途来说，最合适的数据库是什么？这些结构化数据是临床数据的良好替代品。</sample>
    <sample id="1291">我们将我们的Shubert模型与基于匿名数据，这些数据来自我们医院的非住院患者，的Dr.Bert进行比较。</sample>
    <sample id="1292">最后，我们问自己，训练一个专门针对法语数据的模型需要多少数据？是4GB，8GB还是更多？</sample>
    <sample id="1293">为了回答这个问题，我们首先从头开始训练和比较四个模型。第一个版本的BERT，有7GB的NATCO数据，第二个版本有4GB的NATCO数据集。</sample>
    <sample id="1294">第一个版本的Schubert，是一个临床模型，包含4GB的临床注释。第二个版本的Schubert，包含4GB的NATs和4GB的临床注释的混合。</sample>
    <sample id="1295">除了这个比较，我们还引入了三个模型进行预训练分析，以评估预训练策略的影响。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1296">一个基于康巴尔的权重，并在4GB的NATOS数据集上训练。另一个也是基于康巴尔的权重，但这次是在4GB的Kinkan Notes数据集上训练。</sample>
    <sample id="1297">最后，基于英语生物医学模型，我们构建并训练了四个GB的SNAC数据集的子集，总共我们有七个模型。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1298">为了评估所有七个模型，我们收集了多个公共和私有捐赠的数据集，例如命名实体识别，分类，语音转文本和问答。</sample>
    <sample id="1299">这些模型与6个基准模型进行了比较，分别是Camembert，Oscar 128GB，Camembert Oscar 4GB，Camembert C-Sinet 4GB，Permidbert，Maybert和ClinicalBERT。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1300">突出显示模型在具有相同性质的数据上表现最佳，这取决于该模型是用什么数据训练的。</sample>
    <sample id="1301">然而，我们可以从异质数据源中获取数据，我们观察到这些数据似乎更加灵活。我们还观察到使用更多数据会转化为更好的性能。</sample>
    <sample id="1302">从Scratch开始训练似乎在大多数任务上都能获得更高的性能。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1303">然而，我们使用PyTorch的权重和分词器对NATOS四GB子集进行持续微调的实验，得到的结果与从头开始训练的DeBERT四GB的结果相当。</sample>
    <sample id="1304">基于Camembert权重和分词器的模型不会出现稳定性问题。</sample>
    <sample id="1305">最后，我们的提议系统在11个DART任务中的9个上表现更好，超过了通用模型的全球结果。</sample>
    <sample id="1306">我们还观察到，专门化的数据更好，越多专门化的数据越好，但它并不容易扩展。</sample>
    <sample id="1307">嗯，这个说法是正确的。</sample>
    <sample id="1308">所以感谢这个演讲，我们期待在多伦多的海报交流会中交流。</sample>
    <sample id="1309">论文研究了从零开始训练和比较四种模型，还引入了三种模型训练在控制预训练上的策略来分析预训练策略的影响。如果还有疑问可以继续问我哦。</sample>
    <sample id="1310">这个信息里没提到过拟合因素有多大，所以没法回答。你要是还有别的想法或者信息，可以再和我说一说。</sample>
    <sample id="1311">嗯…你可以看看论文里的评估指标和实验分数。如果还有啥疑问，你可以再跟我说哦。</sample>
    <sample id="1312">有。</sample>
    <sample id="1313">嗨，我的名字是马蒂亚斯·林德曼，今天我将给大家简要介绍一下我们关于使用多集标记和潜在置换的无树组合和泛化的论文。</sample>
    <sample id="1314">这是与我的导师Alexander Koller和Ivan Titov的合作成果。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1315">组成性泛化可以被理解为学习者处理在训练中单独见过的短语的更深层次递归和未见过的组合的能力。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1316">在语义解析的语境下，测试组合泛化可能看起来像这样：就像往常一样，我们有一个训练集的表达式，在这个例子中是“the girl slept”和“Mary knew that the girl slept”。</sample>
    <sample id="1317">这些陈述与代表其核心意义的逻辑形式配对。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不是来自相同分布，而是包含结构上未见过的逻辑形式。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1319">在这个例子中，模型在训练时见过浅层递归，但在测试时却遇到了深层递归。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1320">朴素的序列到序列模型在处理这种分布外泛化时很吃力，经常生成与输入脱节的输出。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1321">特别是，它们常常无法重现输入和输出之间的系统性对应关系，比如那些在例子中用颜色编码的对应关系。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1322">一种流行的方法是将树集成到模型中。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1323">这些树旨在捕捉与逻辑形式相关的表达的组合过程。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1324">这很好，但是树通常不会被给予，它们需要以某种方式获得。</sample>
    <sample id="1325">这可能是一个复杂且有时计算上昂贵的过程。通常，这涉及到对逻辑形式的大量形式主义特定预处理，例如，处理变量符号。</sample>
    <sample id="1326">获取树状结构也可能涉及专门的语法归纳程序。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1327">在这篇论文中，我们没有使用树，而是介绍了一种直接建模输入片段和输出片段之间对应关系的神经序列到序列模型。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1328">我们首次展示了在不依赖于树的情况下，对更深的递归进行强大的泛化。</sample>
    <sample id="1329">我们的方法从输入预测输出分为两步。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1330">首先，我们为每个输入令牌打上一个令牌集合的无序多重集，该集合将在输出中出现。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1331">在第一步之后，我们有了所有正确的标记，但它们没有排序。</sample>
    <sample id="1332">这就是为什么在第二步我们使用另一个模型来预测一个排列，把它们放到正确的顺序里。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1333">我们介绍了一种预测排列的新方法，它对可能的排列没有任何硬约束，这让我们的方法相当灵活且表达能力强。</sample>
    <sample id="1334">概念上，我们的排列模型大致工作原理是这样的。</sample>
    <sample id="1335">从左到右遍历输出，确定每个位置应放哪个多集标记。对于第一个输出位置，我们简单地选择一个，如红色高亮所示。</sample>
    <sample id="1336">然后我们跳到下一个多集令牌，以确定输出中的第二个令牌。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1337">我们以类似的方式确定输出中的第三个令牌，通过跳转到另一个令牌集的令牌。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个令牌都被访问过且只访问过一次。</sample>
    <sample id="1339">为了给你一个实验结果的预告，我们比较了我们的方法与其他无树模型在Cogs基准测试上的表现。我们的模型在对更深层次递归的泛化能力上，比其他模型有着很大的优势。如果还有其他翻译相关的问题，或者想讨论这个实验结果，都可以跟我说哦。</sample>
    <sample id="1340">不过，其他一些结构化泛化仍然非常具有挑战性。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1341">在我们的论文中，我们解决了一些有趣的技术挑战。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1342">首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多语言，这对训练构成了挑战。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1343">此外，有时存在多个与数据一致的排列，但其中语言上正确的那个是潜在的。我们通过在训练中引入对齐来解决这个问题。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1344">我们的排列方法非常灵活，但它带来的挑战是找到最高得分的排列是NP难的，这是因为这与旅行商问题有关。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1345">我们用一种对GPU友好的连续松弛来近似，这也允许我们反向传播通过解决方案并学习语义上更合理的排列。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1346">如果你想知道更多关于我们实验以及我们如何应对这些挑战的信息，请查看我们的论文或者来我们的海报。</sample>
    <sample id="1347">认知失调就是两种不一致的信念或者行为。如果还有啥想知道的，尽管跟我说哈。</sample>
    <sample id="1348">GPT-4最倾向于自由派。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1349">是的，在主动学习时，累积训练比迭代训练更有效。</sample>
    <sample id="1350">Sarah Pape.</sample>
    <sample id="1351">MuDa基准中的数据是从TED演讲的英译本转录本中获得的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1385">Matthias Lindemann。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1386">跨语言转移就是在一种源语言上进行训练，然后转移到另一种语言。嗯，你要是还有啥想知道的，尽管再问哈。</sample>
    <sample id="1387">这篇论文的作者所属机构是Stallent University in Germany。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1388">作者使用了平均延迟和计算感知平均延迟这两种延迟测量方法。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1389">大家好，我是Makshita，今天我和我的合作者Martin正在展示我们的工作“孩子必须知道的”，评估来自多个来源的知识整合。这项工作是麦吉尔大学，MILA和微软研究之间的合作。</sample>
    <sample id="1390">自然语言理解模型从多种知识来源中汲取，例如其参数中包含的知识，通常通过预训练获得，以及在推理时输入中的知识。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1391">最近在诸如问答等任务中的工作表明，模型可以利用预训练知识来解决任务。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1392">但自然语言理解通常需要在推理阶段也提供的知识。</sample>
    <sample id="1393">约翰在电视上看到新当选的总统。如果还有类似的翻译问题，你可以随时告诉我哦。</sample>
    <sample id="1394">预训练参数可以包含关于先例做什么和先例的活动的信息，但它们不能可靠地知道这个特定实例中的实体约翰是谁，或者新的总统是谁，因为总统可能在预训练之后就变了。</sample>
    <sample id="1395">因此，对于知识密集型NLU任务的成功模型，需要具备在预训练时间和推理时间整合和使用知识的能力。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1396">在这项工作中，我们提出了一套知识集成的诊断测试套件。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1397">我们引入了一个指代消解任务，旨在测试利用不同来源知识的能力。我们用人类研究参与者和已建立的指代消解模型来评估数据集。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1398">瑟琳是个法官，基亚是个面包师。瑟琳和基亚在公园里相遇了。在漫长的一天工作后，他在法庭上判定案件，他很高兴能放松一下。</sample>
    <sample id="1399">任务是找出代词“he”所指代的正确实体，这里“he”指的是Sam。</sample>
    <sample id="1400">一个给定代词的消解需要两种类型的信息：第一种是实体特定知识，比如“Sam是法官”，第二种是背景知识，比如“法官在法庭上决定案件”。</sample>
    <sample id="1401">一般来说，背景知识是在大型语言模型的预训练过程中学习的，而实体特定知识通常是在推理阶段观察到的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1402">我们将这两条信息的可用性进行变化，使其可能在单一来源中找到，也可能在多个来源中找到。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1403">我们定义了三个Kit Mos的设置。第一个是主题设置，背景预训练，背景知识在预训练时被假设为可用的。</sample>
    <sample id="1404">第二，有背景知识设置。在预训练和推理时，背景知识都是可用的。最后，有背景推理设置。在推理时，两种知识类型都是可用的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1405">这个最后的设置特别有趣，因为它模拟了背景噪声对完成任务必要的情况，而背景噪声不是模型预训练数据的一部分，例如，因为自预训练以来新职业已经发展出来了。</sample>
    <sample id="1406">这是关于如何控制事实来源的可用性的示例。</sample>
    <sample id="1407">在预训练设置中，我们假设关于“政客竞选政府席位”的背景知识包含在预训练参数中。为了理解上下文，我们提供特定知识“奇切斯特是一名政客”。</sample>
    <sample id="1408">在背景设置中，我们不仅提供特定的背景知识，还提供关于在上下文中推断的政客的背景知识。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1409">在背景和场景设置中，提供虚构职业“军士”而不是“政治家”，因为“军士”不太可能包含在预训练语料库中。</sample>
    <sample id="1410">我们用人类研究参与者和已建立的参考分辨率模型评估了数据集。在图中，我们展示了在贝克曼预训练设置中最难的变体上表现最好的模型的结果。</sample>
    <sample id="1411">如果在没有专门针对KidMoS训练的情况下，两个模型在KidMoS上表现不佳。然而，当在KidMoS上进行训练时，C2F和Bert for QAF都比随机选择的模型表现得更好。</sample>
    <sample id="1412">这表明，当在通用问题求解数据集上训练时，模型会学习利用表面线索，而这些表面线索在测试时对大多数问题来说是没有用的，因为大多数问题中的表面线索已经被移除了。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1413">额外的虚构知识实验表明，即使是最优秀的模型，在仅在推理时提供背景知识的情况下，也不能可靠地进行整合。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1414">许多参考进化模型似乎无法在没有任务特定训练的情况下，从不同来源整合知识。然而，通过任务特定训练，一些模型成功地整合了来自多个来源的知识。</sample>
    <sample id="1415">不过，即使是最优秀的模型似乎也难以可靠地整合仅在推理时呈现的后向知识。如果你对更多细节感兴趣，请查看我们的论文，并在GitHub上查看数据集和代码。谢谢你的聆听。</sample>
    <sample id="1416">嗯…首先呢，树通常不是直接给的，得去获取，这过程可能很复杂，计算上也挺费劲的。而且，处理逻辑形式的时候，得做不少特定的预处理，像处理变量符号之类的。还有，获取树可能还得用专门的语法归纳程序。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="1417">抱歉，我无法从你给的英文内容中直接获取论文作者所属机构的信息。你可以再提供一些关于这篇论文的其他信息吗？</sample>
    <sample id="1418">嗨，我是Mira，今天我将谈论我们的论文《使用自然语言提示来衡量语言模型中的刻板印象》。这项工作是与S. Sander穆什和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">近年来，许多人记录了大型语言模型，LLMs，中社会偏见和刻板印象的普遍性。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1420">然而，这些措施有各种局限性。它们通常依赖于手工构建的数据集，而这些数据集的收集非常耗时。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1421">而且它们通常只测量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计群体或语境，或者它们仅仅捕捉到非常宽泛的一般关联，像对特定群体的负面关联。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1422">此外，这个领域里的大多数工作都没有考虑到交集性，也就是多元的社会身份会叠加偏见，成为独特受伤害的地方。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于这些新指令调优的LLMs在响应指令和提示方面非常擅长的特性。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1424">所以我们可以让模型生成一个角色，这是一种对想象中个人的描绘，使用像“想象你是一个亚洲女性，描述你自己。”这样的提示。</sample>
    <sample id="1425">我们能立刻看到，这非常适用于任何人口统计群体，因为我们只需在提示中指定我们想要的任何身份标记。</sample>
    <sample id="1426">所以这里有一些来自GPT-4的示例生成。</sample>
    <sample id="1427">我们马上就能看到，虽然输出并不明显地消极或有毒，按照传统意义上的这些词。</sample>
    <sample id="1428">有一些有趣的模式。</sample>
    <sample id="1429">这位亚洲女性被描绘成平易近人，而这位中东女性则被用“异国情调”等词来形容，就像在说一个迷人的地区。</sample>
    <sample id="1430">嗯.让我想想怎么翻译比较准确.啊，我明白了！这句话的意思是：。"两位有色人种女性角色都提到了他们的祖先，而那位白人男性角色则没有提到这一点。"。要不要再聊聊这个话题？我很好奇你对这个翻译有什么想法？</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些用户画像。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1432">我们的生成这些人格的提示是受研究启发的，他们将这些提示给到人类受试者，发现通过给到人类受试者，他们也能够揭示种族刻板印象。</sample>
    <sample id="1433">这使得我们生成的个性与人类写的回答之间能够直接比较。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1434">第二部分是标记词，这是一种识别区分标记群体和非标记群体的词的方法，我稍后会详细阐述。</sample>
    <sample id="1435">这样做的好处是我们能获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1436">标记语词法借鉴了社会语言学中的标记性概念，该概念认为存在一个未标记的默认状态，任何与该默认状态不同的群体在语言上都是标记的。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1437">比如说“战士”这个词通常和男人联系在一起。当人们描述一个女战士时，他们通常会明确指出“女战士”，并在这个词上标注“女”。</sample>
    <sample id="1438">在更广泛的意义上，社会中的主导群体在语言和社会上都是无标记的，而被边缘化的群体通常是被标记的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1439">在我们的方法里，我们首先指定未标记组和标记组是什么。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1440">然后我们使用“战斗词汇”方法来比较人设，也就是使用加权对数几率比来区分每个标记组的顶级词汇。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1441">比如说，对于黑人女性这个角色，我们会做“战斗词汇”，然后把对白的几率比对，分别和白人角色和男性角色的几率进行比较，因为这两个是对应的角色的未标记群体。</sample>
    <sample id="1442">现在说说结果。首先，我们使用了刻板印象词典，发现生成的个性特征包含的刻板印象比人类写的要多得多。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1443">然而，当我们实际查看词汇表中单词的分布时，我们会发现非常不同的东西。</sample>
    <sample id="1444">嗯.让我想想怎么翻译比较准确.啊，我明白了！这段话的意思是：。生成的人格化角色使用了更多"刻板印象"词汇，而人类写的角色词汇分布更广泛。生成的人格化角色中，"高大"和"健壮"这两个词特别突出。要不要再聊聊你对这个翻译的理解？</sample>
    <sample id="1445">所以真的就只有正的或者至少是非负的那些。</sample>
    <sample id="1446">实际上，这个词汇表并没有很好地捕捉到我们在早期幻灯片中看到的许多有害模式。因此，为了做到这一点，我们将转向我们的标记单词方法的结果，以展示这些看似积极的单词如何促进刻板印象和本质化叙事。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害的模式。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1448">对于非裔美国人来说，关键的词包括文化，传统，自豪和异国情调，这些词仅通过与身份的关系来定义这些群体，并将它们与白人规范区分开来。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1449">这为这些群体带来了长久的歧视和边缘化的历史。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1450">此外，这些词中反映了很多常见的套路，尤其是对于有色人种的女性。例如，用来描述拉丁裔女性的词包括“充满活力的”和“丰满的”。</sample>
    <sample id="1451">对于亚洲女性来说，像“娇小”“纤细”“丝滑”这样的词是“东方主义”中的“东方化”模式。</sample>
    <sample id="1452">这与亚洲女性长期以来被过度性化，被视为非常顺从和顺从的长期历史有关。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1453">最后，对于黑人女性，我们看到一些关键词是像“坚强”和“有韧性”。</sample>
    <sample id="1454">这与人们所说的“坚强的黑人女性”原型有关，虽然乍一看这听起来像是积极的。</sample>
    <sample id="1455">有研究表明，这种原型实际上是非常有害的，因为它给这些人口群体施加了很大的压力，要他们对社会障碍有很强的韧性。</sample>
    <sample id="1456">所以，与其真正去改变那些障碍，它却给那些人施加压力，让他们去克服它们，这会导致这些人非常负面的健康结果，还有其他伤害。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1457">更广泛地说，我们发现每个标记群体的词汇基本上反映了非常本质化的叙事。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1458">根据这些模式，我们得出三条对模型所有者的建议。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1459">首先，作为研究人员，我们应该解决积极的刻板印象和本质化叙事。我们也应该使用交集视角来研究偏见和伤害，因为如果不这样做，可能会有很多东西被忽视。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">比如说这些积极的刻板印象，我们不知道是不是因为有某种奇怪的。</sample>
    <sample id="1462">过度的价值对齐，或者一些反刻板印象的方法，导致了这些有害的模式。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1463">我们真的不能做任何假设，也不能再深入研究下去，除非有更多透明度。</sample>
    <sample id="1464">非常感谢你的聆听，祝你玩得开心。</sample>
    <sample id="1465">你好，大家好，我是来自中国科学技术大学的金伟。</sample>
    <sample id="1466">我很乐意制作一个关于纸张的简短广告视频。你是在抄袭我的模型吗？保护大型语言模型嵌入服务的版权，我们有后门水印。</sample>
    <sample id="1467">让我们先介绍一下嵌入式服务的背景。</sample>
    <sample id="1468">目前，像GPT，LLaMA，Palm这样的大型语言模型在自然语言理解和生成方面表现出色。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1469">嵌入式AI服务是建立在大型语言模型之上的服务，以协助各种NLP任务。</sample>
    <sample id="1470">OpenAI提供基于GPT的嵌入式API。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可能通过学习嵌入来窃取模型，并提供类似的服务。因此，有必要保护嵌入作为服务的版权。</sample>
    <sample id="1472">为了保护嵌入式广告服务的版权，其中一个解决方案是在提供者服务中嵌入水印，并检测其他服务是否包含该水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应适用于嵌入式服务。其次，水印不应降低所提供的嵌入式服务的实用性。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1474">第三，水印应该足够隐蔽，否则攻击者可以很容易地移除水印。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1475">最后，水印需要在模型提取过程中被转移到攻击者服务。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1476">现有的作品可以大致分为四类。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1477">然而，这种方法要么不适用于嵌入式服务，要么缺乏可转移性。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1478">因此，在这篇论文中，我们提出了嵌入标记，这是一种基于后门的水印方法，适用于嵌入服务。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1479">嵌入式水印包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在中等频率区间内的单词。</sample>
    <sample id="1481">我们假设提供者可以收集一个通用的文本语料库，并计算单词频率。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1482">在水印注入中，我们首先定义一个目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发数字。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权求和。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1484">目标嵌入的权重与句子中触发器的数量成正比。当句子中触发器的数量大于M时，提供的嵌入正好等于目标嵌入。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1485">版权验证是检测另一个服务背后的模型是否包含水印。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1486">我们首先构建一个后门数据集和一个良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">提供商请求从Steller服务中嵌入数据集。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1488">计算了请求嵌入和目标嵌入的余弦相似度。我们计算了良性数据集和后门数据集之间的相似度差异，定义为Δcos和ΔL2。</sample>
    <sample id="1489">同时，我们还应用ks检验，并使用其p值作为第三个指标。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1490">我们对四个数据集进行实验：20新闻组，MIND，SST-2和Iris Spam。我们假设提供者使用WikiText数据集来计算单词频率。</sample>
    <sample id="1491">在四个数据集上的结果表明，我们的嵌入式标记在保持对下游任务的高实用性的同时，具有出色的检测性能。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1492">我们还通过可视化在四个数据集上的句子嵌入来验证其可解释性。图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">在图中，很难区分后门嵌入和正常嵌入。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1494">那就这样，谢谢。欢迎来和我们讨论。</sample>
    <sample id="1495">ABC-Eval代表Annotating Behaviors in Chat。</sample>
    <sample id="1496">嗯…这个信息在音频里没提到呢。你可以再找找其他资料或者咱们再聊聊这个事儿。</sample>
    <sample id="1497">你好，我的名字是Vasudha，是石溪大学计算机科学专业的博士生。我想展示一篇被ACL 2023接受的长论文《针对稀有类挑战的迁移学习用于异常检测》。</sample>
    <sample id="1498">我们首先定义认知失调，为什么它是语言研究中的一个重要问题。简单地说，认知失调就是两种不一致的信念或行为。</sample>
    <sample id="1499">就像这个例子，一个人说“我知道香烟可能会要了我的命”，然后又说“我下班后抽了几支烟”。这种信念和行为是不一致的，是脱节的。</sample>
    <sample id="1500">进一步提到“我认为没有他们我无法保住工作”来证明第二处的“and”有并列关系。</sample>
    <sample id="1501">矛盾在日常决策中很常见，但在语言和其他话语关系中却很少被表达出来。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1502">研究认知距离有助于我们理解人们之间分歧的影响，追踪信念，价值观和态度在人群中的变化趋势。</sample>
    <sample id="1503">高认知失调也与焦虑障碍有关，有助于更好地理解人们的心理健康。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1504">研究语言中的不和谐表达也能有助于理解对弱势群体的极端主义和两极分化。</sample>
    <sample id="1505">最后，认知失调对于理解个人的认知风格很重要，有助于我们更好地理解决策过程。</sample>
    <sample id="1506">为了创建认知失调资源的目标，我们进行了大规模的认知失调关系标注。我们使用了认知失调第一种方法，如图中的流程图所示。</sample>
    <sample id="1507">推文使用API被解析，根据论文中描述的指南，对话语单元对进行了注释。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1508">如图所示，不和谐音只在3.5%的标注对中被发现。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1509">在收集了大约1000个话语单元对的例子后，我们仅用43个例子训练了一个初始分类器。不出所料，这个分类器的表现并没有比随机猜测好多少。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1510">鉴于不和谐事件发生的低频率和缺乏任何先前类似数据集，我们正面临着绝对稀有的问题。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1511">为了缓解这个问题，我们尝试了组合迁移学习和主动学习来注释，以便在较少的注释运行中收集更多的不和谐样本，从而降低总体注释成本，同时提高不和谐检测。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1512">由于初始模型根本无法捕捉到不和谐类，我们开始迁移学习过程，通过从相关任务中转移权重。</sample>
    <sample id="1513">我们从两个不同的任务中转移。主题无关的分歧立场分类，这是一个任务，它确定来自不同人的两个辩论陈述是否在意见上一致或不一致，与主题无关。</sample>
    <sample id="1514">在这里，我们进行辩论。关于二元分类的扩展和比较类的PDTB，因为这两个与和声和不和声的概念密切相关，我们把它们称为C，E，</sample>
    <sample id="1515">我们发现，在零样本迁移上，标注数据集的性能已经远远优于随机猜测，最好的AUC达到了0.62。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1516">在迭代微调两个任务上，我们发现先对CE任务进行微调，然后进一步对Debate进行微调，会得到更好的零样本性能。因此，这是我们在主动学习中使用的模型。</sample>
    <sample id="1517">接下来，我们确定更新模型的最佳方法，使用来自每一轮主动学习和注释的新数据。累积器将积累到目前为止从主动注释中收集的所有数据，而迭代器通过训练最新收集的数据集来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积型在各方面表现得与迭代型相当或更好。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1519">为了提高不一致示例的数量，我们使用概率稀有类策略，PRC，来选择在任何训练轮次中，当前模型高度可能认为是不一致的示例。</sample>
    <sample id="1520">我们将这与社区中普遍使用的其他先进AI策略进行比较。</sample>
    <sample id="1521">我们发现，所提出的PRC策略比其他最先进的策略效果更好，尽管差异很小。请注意，随机策略的性能明显较低。</sample>
    <sample id="1522">在后续的两轮AL中，我们提高了距离分类AUC到0.75，这是到目前为止我们在该任务上最好的表现。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1523">我们也检查了每种标注策略的可行性，包括标注质量成本对标注者的影响。我们发现PRC在不一致率方面最高，对稀有类效果最好。然而，标注者也觉得这些示例很难。如果还有其他翻译相关的问题，或者想讨论下这个翻译的准确性，都可以跟我说哦。</sample>
    <sample id="1524">我们发现PRC是一种简单的AI策略，用于稀有类获取，而使用适当设计的迁移学习任务开始AI训练能显著帮助。</sample>
    <sample id="1525">我们还发现迭代更新对于从不同领域进行迁移学习是有用的，而同领域主动注释则得益于累积更新。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1526">以下是我们的代码数据集和论文的链接。欢迎随时与我们联系，如果您有任何问题。谢谢。</sample>
    <sample id="1527">嗯…这个信息里没提到作者所属机构呢。你可以再找找其他资料看看。</sample>
    <sample id="1528">Si Yuan.</sample>
    <sample id="1529">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1530">该方法与 state-of-the-art architecture，即最先进的架构，进行了比较。</sample>
  </task>
</testset>