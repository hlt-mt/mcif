<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">Le principali fonti di dati per i modelli linguistici sono le notizie politiche e i media. Secondo una rassegna del corpus C4, si vede che New York Times, Los Angeles Times, The Guardian, Huffington Post ecc. sono ben coperti nei dati di addestramento dei modelli linguistici. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1">McGill University/Mila, Microsoft Research. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="2">Ciao, benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per la semplificazione del testo tedesco a livello di documento e a livello di frase.</sample>
    <sample id="3">Il contenuto inglese è "DEPLAIN: Un corpus parallelo tedesco con traduzioni intralinguali in linguaggio semplice per la semplificazione di frasi e documenti".</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, come persone con problemi di lettura o non madrelingua.</sample>
    <sample id="5">Per addestrare un modello di semplificazione del testo, abbiamo bisogno di coppie parallele di testo, ad esempio documenti o frasi parallele.</sample>
    <sample id="6">Ecco il contenuto tradotto in italiano: "Ecco un esempio di un paio di frasi allineate parallele di una frase tedesca complessa e la sua traduzione in un linguaggio più semplice."</sample>
    <sample id="7">Per semplificare la frase, sono possibili diverse tecniche, come la sostituzione lessicale, la cancellazione di clausole, la ridistribuzione, la cancellazione di parole o l'aggiunta di parole.</sample>
    <sample id="8">Ora proponiamo il nostro nuovo corpus, DE-plain, perché negli ultimi anni ci sono stati alcuni problemi con i corpi esistenti. Ad esempio, questi corpi sono troppo piccoli per addestrare un modello di semplificazione del testo.</sample>
    <sample id="9">I tre modelli proposti negli ultimi anni sono tutti allineati automaticamente, il che significa che possono essere più soggetti a errori nell'allineamento.</sample>
    <sample id="10">Quindi proponiamo il nostro nuovo corpus DEplain, diviso in due sottocorpus: DEplain-APA e DEplain-web. DEplain-APA si basa su testi di notizie.</sample>
    <sample id="11">Nel DEplain-APA abbiamo allineato 483 documenti tutti manualmente. Ciò porta a circa 30.000 coppie di frasi paralleli.</sample>
    <sample id="12">Per il corpus DEplain-web, questo corpus include diversi domini e abbiamo anche allineato tutti questi 756 documenti, sia manualmente che con metodi di allineamento automatico.</sample>
    <sample id="13">In totale, abbiamo ottenuto 30.450 coppie di frasi.</sample>
    <sample id="14">Abbiamo analizzato le nostre coppie di frasi un po' di più. Ad esempio, per il tipo di semplificazione.</sample>
    <sample id="15">Come puoi vedere qui, i testi biblici sono molto più forti e semplificati rispetto, ad esempio, ai testi notiziari o ai testi di apprendimento della lingua.</sample>
    <sample id="16">Per esempio, semplificazione lessicale, semplificazione strutturale e livello complessivo di semplificazione.</sample>
    <sample id="17">Inoltre, si può vedere che il nostro corpus DEplain ha una varietà elevata di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DEplain-apa abbiamo molto più riordinamenti e aggiunte di parole rispetto al corpus DEplain-web.</sample>
    <sample id="18">D'altra parte, nel corpus web abbiamo molto più riformulazioni.</sample>
    <sample id="19">Certo. Quindi, vediamo cosa possiamo fare con questo corpus. Ciao, sono Omar e ora parlerò dei casi d'uso del nostro dataset. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatico.</sample>
    <sample id="20">Negli ultimi anni ci sono state molte metodologie di allineamento, ma nel contesto della traduzione automatica.</sample>
    <sample id="21">Quando abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre le allineazioni di frasi in entrambi i documenti.</sample>
    <sample id="22">Ma nel nostro caso di utilizzo stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli aventi la stessa lingua e lo stesso contenuto, ma a livelli di complessità diversi.</sample>
    <sample id="23">Ora che abbiamo il nostro dataset DeepLain con frasi manualmente allineate, possiamo utilizzare queste frasi come allineamenti di riferimento per valutare alcuni dei metodi di allineamento proposti.</sample>
    <sample id="24">E abbiamo fatto alcune adattamenti ai metodi proposti e abbiamo pubblicato tutte queste adattamenti e il codice per eseguire le nostre esperimenti nel paper.</sample>
    <sample id="25">Alla fine, abbiamo concluso che il miglior metodo automatico di allineamento da utilizzare per i testi per la semplificazione del testo tedesco è il metodo di MASSalign.</sample>
    <sample id="26">Ecco il contenuto tradotto in italiano: "E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti in proprio nel documento."</sample>
    <sample id="27">Il secondo caso d'uso che abbiamo mostrato nella nostra paper è il caso di semplificazione automatica del testo.</sample>
    <sample id="28">Ridurre il testo complesso in input in testo semplificato utilizzando modelli di linguaggio finetunati.</sample>
    <sample id="29">Abbiamo finetunato due modelli diversi. Abbiamo finetunato il modello di long-mBART per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">E abbiamo anche finetunato il modello base lungo il modello base mBART per produrre semplificazioni a livello di frase.</sample>
    <sample id="31">Puoi trovare anche tutti i punti di controllo e puoi esaminare in più dettaglio le punteggi e le metriche di valutazione delle nostre esperimenti nel documento.</sample>
    <sample id="32">Concludemmo che questa semplice ottimizzazione poteva produrre o ottenere punteggi migliori dei punteggi di base.</sample>
    <sample id="33">E proponiamo questi risultati come una base di riferimento per il problema della semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie per l'attenzione. Speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="35">Il nome della relatrice è Kayo Yin.</sample>
    <sample id="36">Hanno utilizzato il modello T5 XL.</sample>
    <sample id="37">Yes!</sample>
    <sample id="38">La novità è che cerca di ridurre la soggettività della valutazione umana annotando esplicitamente se le risposte dei modelli esprimono determinati comportamenti, come rispondere con informazioni irrellevanti o contraddizione.</sample>
    <sample id="39">L'attuale approccio scarsamente supervisionato si basa in larga misura sulle etichette pulite. Se non ci sono etichette pulite, il modello non può generalizzare oltre le etichette deboli originali, il che rende l'addestramento inutile. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="40">Non so. Potrebbero essere fatti progressi in diversi modi, come migliorare la comprensione delle canzoni o fornire informazioni più dettagliate agli annotatori. Ma non so esattamente quelli che possono essere fatti. Cosa ne pensi?</sample>
    <sample id="41">There are five authors involved in the article. If you want to know more about them or the article itself, feel free to ask.</sample>
    <sample id="42">Ciao, il mio nome è Adam Przepiórkowski e questa è una conferenza sullo struttura di dipendenza della coordinazione.</sample>
    <sample id="43">Come potete sapere, ci sono diverse strutture di dipendenza assunte da teorie e approcci di corpus differenti. Ad esempio, nelle dipendenze universali, la struttura della coordinazione "Lisa, Bart e Maggie" è così.</sample>
    <sample id="44">Sì, in questo caso Lisa è la testa della struttura coordinata intera.</sample>
    <sample id="45">Un approccio simile è assunto in Ecological Semantics, teoria del testo, dove anche la struttura coordinata intera è guidata dal primo congiunto. Questi due approcci sono isometrici, cioè isolano uno dei congiunti.</sample>
    <sample id="46">Ora ci sono anche approcci simmetrici per le strutture di coordinamento, come l'approccio di Praga, l'approccio congiuntivo capo, che si basa su alberi di dipendenza di Praga, dove le strutture di coordinamento sono capite dal congiuntivo.</sample>
    <sample id="47">Quindi otteniamo delle dipendenze da "and" a tutte le congiunte.</sample>
    <sample id="48">Ecco la traduzione in italiano: "E infine, c'è anche un approccio multi-testa, ad esempio utilizzato nel grammatica del mondo di Cutts." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="49">Dove dire che tutti i congiunti sono capi della struttura coordinata, quindi otteniamo dipendenze dal governatore qui ama tutti i congiunti separatamente: Lisa, Bart e Maggie.</sample>
    <sample id="50">L'articolo ha l'obiettivo di presentare un argomento nuovo per le strutture simmetriche di coordinamento come queste due e contro le strutture asimmetriche di coordinamento come queste.</sample>
    <sample id="51">L'argomento si basa sul principio di minimizzazione della lunghezza delle dipendenze che verrà spiegato sulla base di questi esempi.</sample>
    <sample id="52">Quindi, come potresti sapere, in inglese gli oggetti diretti preferiscono essere vicini al verbo, mentre gli avverbi possono essere più lontani. Così "Marge read it yesterday" è bene perché l'oggetto diretto "it" è vicino al verbo.</sample>
    <sample id="53">Marge ha letto "it" ieri. È molto peggio, giusto? Perché qui c'è un adagio tra il verbo e l'oggetto diretto.</sample>
    <sample id="54">Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e lungo, poiché allora può essere spostato dopo l'agente.</sample>
    <sample id="55">Ecco la traduzione in italiano: "Questo è illustrato qui. Entrambe queste frasi sono a posto. Marge ha letto questo libro assolutamente affascinante su api ieri. Va bene, invece di "it" abbiamo "this" lungo e pulito."</sample>
    <sample id="56">La disposizione delle parole tende a minimizzare le lunghezze delle dipendenze: Ma è anche okay dire "Marge ha letto ieri questo libro assolutamente affascinante su api."</sample>
    <sample id="57">La ragione qui è che ciò è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo.</sample>
    <sample id="58">La disposizione delle parole tende a minimizzare le lunghezze delle dipendenze: "Marge legge il libro di ieri." - buono. "Marge legge ieri il libro." - cattivo. "Marge legge questo libro assolutamente affascinante su api di ieri." - buono. "Marge legge ieri questo libro assolutamente affascinante su api." - buono. Soddisfa il principio della minimizzazione delle lunghezze delle dipendenze, che afferma che le dipendenze più brevi sono preferite.</sample>
    <sample id="59">Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quindi quelle che non sono costanti tra queste due strutture.</sample>
    <sample id="60">Quindi qui abbiamo una dipendenza da "read" all'aggettivo di lunghezza 7 misurata in parole e da "read" al libro di lunghezza 4, insomma insieme è 11.</sample>
    <sample id="61">Quando si scambiano queste due componenti, la somma di queste due dipendenze diventa 6, giusto? Così invece di 11, 6 è molto più breve. Per questo suona abbastanza bene, anche se viola uno principio, ma soddisfa un altro.</sample>
    <sample id="62">Ecco la traduzione in italiano: "Quindi, cosa abbiamo fatto? Abbiamo estratto statistiche diverse sulla coordinazione dall'enhanced versione del Penn Treebank e dal paper. Perché non usare università dipendenze?"</sample>
    <sample id="63">Le statistiche confermano l'osservazione fatta molte volte prima che le congiunzioni a sinistra tendano a essere più brevi. Ad esempio, "sale e pepe" è più breve di "pepe e sale", misurato in sillabe.</sample>
    <sample id="64">Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016: - i congiunti a sinistra tendono a essere più corti, - questa tendenza aumenta con la differenza di lunghezza, - ma solo quando il governante è a sinistra o assente, - non quando è a destra.</sample>
    <sample id="65">Quando la differenza di lunghezza tra i due congiunti aumenta, il congiunto più breve tende a essere il primo, più forte, quindi la proporzione è maggiore dei congiunti più corti a sinistra.</sample>
    <sample id="66">Istatistiche sulla coordinazione estratte da una versione ampliata del Penn Treebank, Marcus et al. 1993, Ficler e Goldberg 2016:• i congiunti a sinistra tendono a essere più corti, osservato prima,• questa tendenza cresce con la differenza di lunghezza, brevemente notata in Gibson et al. 1996: 88-90,• ma solo quando il governante è a sinistra o assente, ho visto Bart e Lisa; Homer è venuto e ha starnutito,• non quando è a destra, Ted e Ned hanno riso.</sample>
    <sample id="67">Congiunti sinistri tendono a essere più corti, come osservato prima. Questa tendenza aumenta con la differenza di lunghezza, come notato brevemente in Gibson et al. 1996: 88-90. Ma solo quando il governante è a sinistra o assente, come in "Ho visto Bart e Lisa; Homer è venuto e ha starnutito". Non quando è a destra, come in "Ted e Ned hanno riso".</sample>
    <sample id="68">Ecco la traduzione in italiano: "Nel secondo esempio, 'Homer came and sneezed', abbiamo una coordinazione di due verbi e non c'è un governante esterno a destra. In queste situazioni, il congiuntivo sinistro tende a essere più breve, e più grande è la differenza tra i due congiuntivi."</sample>
    <sample id="69">Ecco la traduzione in italiano: "Tuttavia, quando il governatore è sulla destra, come qui, la coordinazione Ted e Ned, la governante, la coordinazione Ted e Ned, questo effetto scompare." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="70">Quindi abbiamo mostrato che, misurando la lunghezza in caratteri, nella prima colonna, in sillabe, nella colonna centrale, e in parole, nella colonna destra, mi concentrerò sulla colonna destra.</sample>
    <sample id="71">Quello che vediamo qui è che quando il governatore è a sinistra.</sample>
    <sample id="72">La tendenza per il congiunto sinistro di essere più breve cresce costantemente con la differenza assoluta in parole, e lo stesso si osserva quando non c'è un governante, come nella coordinazione di frasi, ma quando il governante è a destra, questa tendenza scompare.</sample>
    <sample id="73">E nel nostro articolo abbiamo mostrato come questo fornisca un argomento contro le strutture di coordinamento asimmetriche, poiché queste due e queste quattro sono simmetriche.</sample>
    <sample id="74">Guarda il documento per l'argomento completo! Parla con noi durante la sessione poster! Grazie.</sample>
    <sample id="75">Tre.</sample>
    <sample id="76">I domini più semplificati sono il "bible" e il "fiction".</sample>
    <sample id="77">L'esempio è "salt and pepper" invece di "pepper and salt".</sample>
    <sample id="78">Sì, puoi usare i modelli DrBERT, il dataset NACHOS e i script di addestramento che sono gratuitamente disponibili sotto la licenza MIT.</sample>
    <sample id="79">DEplain-APA è basato su testi di notizie.</sample>
    <sample id="80">Per una buona generalizzazione, ci sono tre fattori: una migliore architettura del modello, una dimensione del modello più grande e esempi di ottimizzazione più dettagliati. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="81">La tendenza è stata misurata misurando la lunghezza in caratteri, sillabe e parole.</sample>
    <sample id="82">I dettagli su come sono stati progettati gli esperimenti non sono qui. Forse potresti cercare di trovare informazioni su come misurarono la lunghezza in caratteri, sillabe e parole?</sample>
    <sample id="83">Non molto efficace.</sample>
    <sample id="84">Quattro.</sample>
    <sample id="85">I nomi dei personaggi sono Bob e Alice. Se hai altre domande, puoi chiedere!</sample>
    <sample id="86">I modelli sensibili al contesto migliorano per formalità e coesione lessicale.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, MIT, Meta AI.</sample>
    <sample id="122">Il framework quantifica la posizionalità attraverso la misurazione della correlazione di Pearson tra i etichetti d'oro, le previsioni del modello e le annotazioni per ciascuna delle demografie separatamente.</sample>
    <sample id="155">Hanno rimosso le stereotipizzazioni razziali.</sample>
    <sample id="156">L'enhanced version of the Penn Treebank.</sample>
    <sample id="157">Due.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di espansione e confronto. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="159">Due.</sample>
    <sample id="160">Cinque.</sample>
    <sample id="161">Il framework differisce dai lavori precedenti perche' confronta gli utenti finali con le predizioni e le etichette dei modelli e dei dataset, invece di solo guardare l'accordo tra gli annotatori o le distribuzioni di annotatori.</sample>
    <sample id="162">La configurazione GPT-3.5 si sovrappone maggiormente al lessico degli stereotipi. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="163">DeepL e Google.</sample>
    <sample id="164">Ciao, sono Shangbin, studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro "Da dati di preaddestramento a modelli di linguaggio, a compiti di flusso di lavoro: tracciare le tracce di pregiudizi politici che portano a modelli NLP non equi".</sample>
    <sample id="165">Quindi, i modelli di linguaggio sono addestrati su grandi quantità di dati web riscattati.</sample>
    <sample id="166">I media politici sono ben coperti nei dati di pre-allenamento. Secondo una ricerca del corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, il Huffington Post, ecc. sono ben coperti nei dati di allenamento del modello linguistico.</sample>
    <sample id="167">Questo ha creato un "blessing e maledizione" per le applicazioni dei modelli di linguaggio.</sample>
    <sample id="168">D'altra parte, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudiziate e potrebbero portare a problemi di parità nelle applicazioni di compiti a valle.</sample>
    <sample id="169">Per questo scopo, proponiamo di indagare sulla catena di propagazione del bias politico dall'addestramento iniziale ai modelli linguistici e poi a compiti di bassa portata, specificamente chiedendo le seguenti domande.</sample>
    <sample id="170">Prima di tutto, come valutiamo la tendenza politica dei modelli di linguaggio e qual è il ruolo che potrebbe avere il dataset di preaddestramento in tali pregiudizi politici?</sample>
    <sample id="171">Secondo, come le lingue modelli con diversi orientamenti politici si comportano effettivamente sulle task di flusso di lavoro e se ciò potrebbe portare a problemi di equità nelle applicazioni di NLP?</sample>
    <sample id="172">Quindi, in particolare, abbiamo proposto di stimolare i modelli di linguaggio con diversi formati di prompt utilizzando i questionari politici, come il Political Compass Test. Questo assicura che la nostra valutazione automatica sia ben fondata sulla letteratura di scienze politiche.</sample>
    <sample id="173">Quindi alcuni risultati preliminari dimostrano che, prima di tutto, i modelli di linguaggio hanno tendenze politiche diverse. Occupano tutti e quattro i quadranti della "compasso politico".</sample>
    <sample id="174">Possiamo anche vedere che GPT-4 è il modello di linguaggio più liberale di tutti e i modelli GPT sono in generale più liberali socialmente dei modelli BERT e delle sue varianti.</sample>
    <sample id="175">Secondo, intendiamo indagare in quale misura i pregiudizi politici dei modelli di linguaggio vengono effettivamente trasmessi dal dataset di addestramento.</sample>
    <sample id="176">Quindi abbiamo potuto condurre un esperimento controllato pretrainando i punti di riferimento del modello di linguaggio su sei diversi corpus partitici, separati in media di notizie e media sociali, ulteriormente divisi in base alla loro inclinazione politica.</sample>
    <sample id="177">Dall'ulteriore pre-allenamento dei modelli di linguaggio su corpora partizionati, possiamo vedere che le coordinate ideologiche dei modelli di linguaggio si spostano corrispondentemente.</sample>
    <sample id="178">Per esempio, per RoBERTa, ulteriormente finetuning e ulteriormente addestrato sul corpus Reddit a sinistra, possiamo vedere un notevole spostamento liberale in termini del suo.</sample>
    <sample id="179">Risultati. Spostamenti partitici nella leanng politica del LM.</sample>
    <sample id="180">Abbiamo anche cercato di indagare se i modelli di linguaggio possono rilevare la polarizzazione prevalente nella nostra società moderna.</sample>
    <sample id="181">Quindi dividiamo il corpus di addestramento in due parti: prima e dopo il 45° presidente degli Stati Uniti. Addestriamo separatamente modelli di linguaggio su due corpora temporali diversi.</sample>
    <sample id="182">Possiamo vedere che i modelli di linguaggio in generale hanno una tendenza politica che si allontana di più dal centro dopo il 2017. Questo indica che i modelli di linguaggio possono anche rilevare la polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per ultima ma non meno importante, abbiamo valutato i modelli di linguaggio con differenti orientamenti politici sulla rilevazione di discorsi razzisti e falso notizie, due applicazioni NLP che coinvolgono spesso modelli di linguaggio e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">Quindi vediamo che se esaminiamo il rendimento per categoria, ossia se separiamo il rendimento in base alle categorie.</sample>
    <sample id="185">Per vedere un pattern, ad esempio, per la rilevazione di discorsi di odio, i modelli di linguaggio di sinistra sono migliori.</sample>
    <sample id="186">Performance per categoria. Tabella 4: Performance sulla cattura di discorsi di odio rivolti a diversi gruppi minoritari sociali e di fake news da diverse fonti. I risultati sono colorati in modo tale che il giallo scuro indica il migliore e il blu scuro il peggiore.</sample>
    <sample id="187">Tuttavia, i nostri modelli sono peggiori nell'individuare il discorso di odio rivolto a gruppi più potenti nella nostra società.</sample>
    <sample id="188">I modelli di linguaggio di destra sono migliori nell'individuare il discorso di odio rivolto a bianchi e uomini, ma peggiori nell'individuare il discorso di odio rivolto a neri, LGBTQ + e altre comunità minoritarie.</sample>
    <sample id="189">Trend simili si verificano anche per la rilevazione di notizie false, dove si osserva che i modelli di linguaggio a sinistra sono migliori nel rilevare la disinformazione proveniente dall'opposto orientamento politico e viceversa.</sample>
    <sample id="190">La tabella 5 mostra esempi qualitativi delle prestazioni di compiti utilizzando modelli di linguaggio con diversi bias politici. C.R.I.S., Base, N, S, L, R rappresentano cristiani, modello vanilla RoBERTa, media di notizie, social media, inclinazione a sinistra, inclinazione a destra, rispettivamente.</sample>
    <sample id="191">Certo. Ecco la traduzione in italiano: "Dato che i modelli di linguaggio basati su diverse inclinazioni politiche danno diverse previsioni per esempi di discorso di odio e di notizie false in base alla loro categoria sociale, ci sono molti esempi in appendice per sottolineare ulteriormente questo punto." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="192">Questo indica che c'è un problema di equità molto preoccupante riguardo alle pregiudiziali politiche dei modelli di linguaggio.</sample>
    <sample id="193">Ecco la traduzione in italiano: "Per esempio, se un modello di linguaggio addestrato su discorsi razzisti o su informazioni false e simili fosse finetunato e poi distribuito su una piattaforma di social media popolare." Se hai bisogno di ulteriori informazioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="194">Ecco la traduzione in italiano: "Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e le parole odiose rivolte a gruppi minoritari potrebbero diventare selvagge senza alcun controllo." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="195">Quindi, questo ci ha messo in allarme per farci accorgere e affrontare i problemi di parzialità causati dalle inclinazioni politiche dei modelli di linguaggio.</sample>
    <sample id="196">Un po' di discussione. Vorremmo anche sottolineare che esponiamo il dilemma unico riguardante le politiche di bias dei modelli di linguaggio. È come tra Scilla e Cariddi.</sample>
    <sample id="197">Se non si sanificano le opinioni politiche nel dataset di addestramento dei modelli linguistici, il bias si propagherà dal dataset di addestramento ai modelli linguistici e poi a task di basso livello, creando problemi di equità.</sample>
    <sample id="198">Se proviamo a sanificare in qualche modo, rischiamo anche la censura o l'esclusione e è estremamente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuto nel training dei modelli di linguaggio. Quindi è un po' come il problema elettrico di Charybdis.</sample>
    <sample id="199">Ok, fantastico! Penso che questo sia tutto ciò di cui ho bisogno per oggi. Cinque per oggi. Grazie del tuo tempo.</sample>
    <sample id="200">Cinque.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite fino a 900 token di lunghezza del contesto.</sample>
    <sample id="202">I domini inclusi nel loro set di dati sono la selezione di musica, libro e ricetta. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="203">Positionality è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="204">Dawei Zhu.</sample>
    <sample id="205">Sì.</sample>
    <sample id="206">Quattro.</sample>
    <sample id="207">No, il modello non funziona bene sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono Background-Pretrain, Background-Both e Background-Inference. Se hai altre domande su questo argomento, non esitare a chiedere.</sample>
    <sample id="209">Google Research.</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?". If you have any other questions about this, feel free to ask!</sample>
    <sample id="211">La sensibilità misura la capacità del modello di produrre i medesimi risultati per la stessa compito indipendentemente da lievi variazioni nella formulazione delle istruzioni.</sample>
    <sample id="212">Il nome della relatrice è Jingwei Yi.</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="214">Non c'è informazione su che tipo di contesto linguistico venga messo a disposizione dei modelli durante il pre-addestramento. Potresti cercare ulteriori dettagli nella presentazione o nella ricerca sull'argomento.</sample>
    <sample id="215">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="216">I autori dell'articolo sono affiliati a Stanford Engineering Computer Science.</sample>
    <sample id="217">Perché i modelli di linguaggio hanno diversi inclinazioni politiche e occupano tutti e quattro i quadranti del compasso politico. Questo significa che ci sono diversi tipi di bias presenti.</sample>
    <sample id="218">Il nome della relatrice è Akshatha.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici va dal pretraining data, ai modelli di linguaggio, fino alle task downstream. Ci sono domande su come valutare la leanng politica dei modelli, il ruolo del pretraining data nelle bias politici, come si svolgono i modelli con differenti leanng politica, e se i bias politici portano a problemi di equità nelle applicazioni NLP.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce tra DEplain-apa e web.</sample>
    <sample id="221">Non so se Coscript è disponibile pubblicamente. Potresti cercare su internet o contattare chi ha informazioni sul progetto.</sample>
    <sample id="222">Definiamo una embedding di destinazione, contiamo il numero di trigger in una frase e aggiungiamo l'embedding di destinazione all'embedding originale.</sample>
    <sample id="223">I autori dell'articolo sono affiliati a PennState e Amazon.</sample>
    <sample id="224">Sì.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è come fare una torta al cioccolato.</sample>
    <sample id="226">Gli autori verificano la segretezza del loro metodo visualizzando le embedding delle frasi su quattro dataset utilizzando PCA.</sample>
    <sample id="227">Il lavoro utilizza l'approccio di pre-allenamento continuo utilizzando PLM esistenti per costruire uno nuovo.</sample>
    <sample id="228">Latin America.</sample>
    <sample id="229">In the phrase "Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output."</sample>
    <sample id="230">Quando aumenta la quantità di attività, la performance del modello migliora e la sensibilità diminuisce.</sample>
    <sample id="231">LSTM seq2seq, T5, Zheng and Lapata.</sample>
    <sample id="232">I due coautori sono gli advisor del primo autore.</sample>
    <sample id="233">Chowdery.</sample>
    <sample id="234">Ciao a tutti, sono Jenny, un primo anno di PhD all'Università di Carnegie Mellon e oggi presenterò il nostro lavoro "NLPositionality: caratterizzare le pregiudiziali di progettazione dei dataset e dei modelli".</sample>
    <sample id="235">Questo lavoro è stato fatto in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, ovvero Sebastin Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="236">Immagina… Quindi iniziamo immaginando che tu lavori per un giornale e stai filtra i commenti sotto un tuo articolo di notizie cercando di rimuovere il contenuto tossico.</sample>
    <sample id="237">Immagina... potresti rivolgerti a un'API popolare come l'API Perspective per la rilevazione di tossicità e funziona veramente bene se sei Carl Jones, dove l'API Perspective riesce a rilevare correttamente le istanze tossiche.</sample>
    <sample id="238">Ma non è proprio il caso per Aditya Sharma, dove l'API Perspective non è così sensibile alle parole offensive più comuni in contesti indiani.</sample>
    <sample id="239">Ecco la traduzione in italiano: "Questo è un esempio di bias di progettazione, dove si osservano differenze sistematiche di prestazione della tecnologia tra popolazioni." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="240">I biasi di design come quello che abbiamo visto prima potrebbero verificarsi a causa della posizionalità degli analisti NLP e dei sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro caratteristiche demografiche, identità e esperienze di vita.</sample>
    <sample id="241">Posizionalità. "Le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita." Questo concetto è ampiamente utilizzato in studi critici, in particolare in spazi accademici femministi e queer.</sample>
    <sample id="242">La posizione di un ricercatore può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Ecco la traduzione in italiano: "Ecco una domanda che le persone potrebbero chiedere: hanno i dataset e i modelli posizionalità?"</sample>
    <sample id="244">Non stiamo dicendo che i modelli e i dataset hanno identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono quindi rappresentare certe posizioni su altre.</sample>
    <sample id="245">Il contenuto inglese in italiano è: "Quindi, il lavoro precedente ha suggerito alcune prove aneddotiche di avere posizionalità, come le lacune culturali in modelli e dataset, nonché le definizioni teoriche di posizionalità del modello."</sample>
    <sample id="246">Ma questi lavori non esaminano realmente la comparazione degli utenti con i dataset e i modelli stessi.</sample>
    <sample id="247">L'importanza di studiare la posizionalità dei dataset e dei modelli sta aumentando in quanto le compiti NLP diventano sempre più soggettivi e orientati alla società.</sample>
    <sample id="248">Ecco la traduzione in italiano: "È difficile caratterizzare come queste posizioni siano scomode, perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="249">Per studiare la posizionalità dei dataset e dei modelli, effettivamente confrontiamo le annotazioni con quelle degli utenti reali e con i dataset e i modelli esistenti.</sample>
    <sample id="250">Facciamo questo attraverso il nostro framework NLPPositionality.</sample>
    <sample id="251">Il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">Il primo passaggio è riassegnare i dataset con annotatori diversi.</sample>
    <sample id="253">Ecco la traduzione in italiano: "E abbiamo optato per fare questo invece di guardare le caratteristiche demografiche dei set di dati originali, degli annotatori, perché di solito solo pochi annotatori annotano ogni istanza e perché le caratteristiche demografiche di solito non vengono raccolte e condivise."</sample>
    <sample id="254">Quindi dobbiamo riassegnare dati per ottenere molte annotazioni per istanza e per ottenere un set ricco di dati demografici.</sample>
    <sample id="255">Allora prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando una correlazione di Pearson.</sample>
    <sample id="256">Il nostro framework differisce dalla letteratura sull'insufficienza degli annotatori perché confronta gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, invece di guardare solo all'accordo inter-annotatore o alle distribuzioni di annotatori.</sample>
    <sample id="257">Il contenuto inglese è stato tradotto in italiano. Se hai bisogno di ulteriori informazioni o di aiuto con la traduzione, fammi sapere.</sample>
    <sample id="258">Lab in the Wild è una piattaforma di esperimentazione online che può reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno principalmente partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità.</sample>
    <sample id="259">Eseguiamo due compiti su Lab in the Wild, uno di essi è l'accessibilità sociale. Funziona così: i partecipanti leggono una situazione dal dataset Social Chemistry e poi valutano quanto sia accettabile socialmente la situazione.</sample>
    <sample id="260">Dopo aver completato la task, per rimanere coinvolto nella ricerca, possono confrontare le proprie risposte con quelle di un AI e di altri.</sample>
    <sample id="261">Poi abbiamo comparato queste annotazioni con Social Chemistry, Delphi e GPT-4.</sample>
    <sample id="262">Task B: Toxicità. 1) Leggi l'esempio: 7/25. Inglese: le persone inglesi odano come latte cattivo. 2) Inserisci cosa pensi di esso. * Denota un campo obbligatorio. Credo che questo sia un discorso di odio? Sì No Non sono sicuro. Puoi liberamente spiegare la tua valutazione. Inserisci una breve descrizione. Invia. 3) Vedere cosa gli altri hanno pensato di esso! L'AI specula: Speculo che questo sia un discorso di odio. Pare che concordiamo! Partecipanti di studi in Afghanistan hanno detto: Non odiose Non sono sicure Odiose</sample>
    <sample id="263">Task B: Toxicità Analisi Dati - Dynahate Modelli - API Perspective - API Rewire - Modello Hate RoBERTa - GPT-4 Studi di partecipazione 16.299 annotazioni 1.096 annotatori 87 paesi</sample>
    <sample id="264">Quindi ora siamo pronti a rispondere a "con chi si allineano i dataset e i modelli di NLP?". Troviamo che c'è una posizionalità nel NLP.</sample>
    <sample id="265">Per esempio, troviamo che i dataset e i modelli sono più allineati ai paesi anglofoni. Quindi, per l'analisi di accettabilità sociale di GPT-4, troviamo che è più allineato ai paesi confuciani e anglofoni. Inoltre, troviamo che Dynahate è anche più allineato ai paesi anglofoni.</sample>
    <sample id="266">Anche troviamo una maggiore allineamento con le persone che hanno un'educazione universitaria. Per GPT-4, nel compito di accettabilità sociale, si trova che è più allineato con le persone che hanno un'educazione universitaria o di laurea.</sample>
    <sample id="267">E troviamo lo stesso per Dynahate, che è più allineato alle persone con un'educazione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati sono allineati a specifiche popolazioni, inevitabilmente alcune rimangono indietro.</sample>
    <sample id="269">Ecco la traduzione in italiano: "Un esempio di questo è che i dataset e i modelli sono meno allineati alle persone non binarie rispetto ai rispettivi omologhi maschili e femminili. Troviamo questo nella task di accettabilità sociale del GPT-4, così come nell'analisi della task di odio e tossicità del Dynahate."</sample>
    <sample id="270">Quindi, cosa possiamo fare? Affrontare la posizionalità nell'NLP.</sample>
    <sample id="271">Quindi abbiamo alcune raccomandazioni per questo. La prima è mantenere un registro di tutte le scelte di progettazione rilevanti fatte durante la costruzione di set di dati o modelli. La seconda è fare ricerca di NLP attraverso il lente del perspectivismismo: a. Condividere etichette di set di dati disaggregati! b. Usare tecniche di modellazione che possono gestire le discordie degli annotatori.</sample>
    <sample id="272">La terza raccomandazione è costruire dataset e modelli specializzati con e per specifiche comunità. Un buon esempio di questo è l'iniziativa Masakhane. Vogliamo sottolineare che l'NLP inclusivo non è solo far sì che tutte le tecnologie funzionino per tutti.</sample>
    <sample id="273">Ecco la traduzione in italiano: "Grazie! Link del pannello di controllo: nlpositionality.cs.washington.edu/ Link del documento: bit.ly/NLPositionality-Paper/ Delphi, https://delechi.allenai.org/ Età, Genere, Etnicità, Religione, Livello di istruzione, Paese di residenza, Paese di permanenza più lunga, Lingua madre, Ecco, quindi, la presentazione è conclusa. Ma se vuoi imparare di più, puoi controllare il nostro pannello di controllo per i risultati di analisi più aggiornati e il nostro documento. Grazie."</sample>
    <sample id="274">La relatrice menziona tre problemi associati a SimulST. Se hai altre domande o vuoi discutere di questo argomento, sentiti libero di chiedere.</sample>
    <sample id="275">Non c'è un modo semplice. Se non si sanizza i bias sociali e politici nei set di dati, essi si propagano e creano problemi di equità. Se si tenta di sanizzarli, si rischia la censura o l'esclusione. E determinare ciò che è neutrale è estremamente difficile. Quindi, è come il problema di Scilla e Caribdis.</sample>
    <sample id="276">Ciao, sono Siyu Yuan della Fudan University. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning".</sample>
    <sample id="277">In vita quotidiana, gli esseri umani pianificano spesso le proprie azioni seguendo istruzioni passo-passo sotto forma di script orientati.</sample>
    <sample id="278">Il lavoro precedente ha esplorato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipiche, come fare un torta, e ha dimostrato che i modelli di linguaggio a grandi scale possono decomporre efficacemente gli obiettivi in passaggi.</sample>
    <sample id="279">Tuttavia, il lavoro precedente si concentra principalmente sulla pianificazione per gli obiettivi astratti di attività stereotipiche. La pianificazione per gli obiettivi con obiettivi specifici e con vincoli specifici, come fare un torta al cioccolato, rimane ancora poco studiata.</sample>
    <sample id="280">Nel documento, definiamo il problema della pianificazione del linguaggio con vincoli.</sample>
    <sample id="281">Un obiettivo astratto può essere ereditato da diversi obiettivi specifici della vita reale con vincoli multi-faccettati. Un buon piano deve scrivere scenari che siano ragionevoli e fedeli ai vincoli.</sample>
    <sample id="282">Nel documento, valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei grandi modelli di linguaggio.</sample>
    <sample id="283">Non esiste un dataset di obiettivi specifici per supportare la nostra ricerca.</sample>
    <sample id="284">Dobbiamo acquisire questi obiettivi in primo luogo, e come mostrato nella tabella, estendiamo gli obiettivi astratti con vincoli multiformi per gli umani. La raccolta dei dati usa l'instruttore GPT.</sample>
    <sample id="285">Eseguiamo un campione di 100 obiettivi specifici e valutiamo i script generati dai modelli di grandi dimensioni.</sample>
    <sample id="286">Questa tabella riporta l'accuratezza complessiva dei risultati. Abbiamo trovato che tutti i modelli di linguaggio ottengono risultati insoddisfacenti nella pianificazione per obiettivi specifici.</sample>
    <sample id="287">Quale tipo di errori fanno solitamente gli LLM in questa task? La fedeltà alle restrizioni, FE, non può essere garantita. Allora eseguiamo un'analisi dettagliata per investigare perché gli LLM falliscono.</sample>
    <sample id="288">I risultati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà alle restrizioni non può essere garantita.</sample>
    <sample id="289">Ecco la traduzione in italiano: "Diamo un'occhiata a una classificazione più fine delle categorie tematiche di vincoli basata su WikiHow. La mappa a caldo nella figura mostra che le prestazioni di pianificazione di InstructGPT variano considerevolmente per obiettivi di categorie diverse." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere!</sample>
    <sample id="290">Studi precedenti hanno mostrato che la qualità delle uscite dei modelli di linguaggio è alta nella varianza, che porta a una pessima prestazione. Quindi, adottiamo l'idea di overgenerate-then-filter per migliorare la qualità della generazione.</sample>
    <sample id="291">Prima di tutto, mostriamo i tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sull'obiettivo astratto di partenza.</sample>
    <sample id="292">Metodo. Input: un obiettivo astratto. Step 1: Genera obiettivi specifici con InstructGPT tramite apprendimento in contesto. Step 2: Genera script candidati con InstructGPT tramite apprendimento in contesto. Obiettivo astratto: fare una torta. Specifici obiettivi: G1, modificatore: fare una torta al cioccolato, G2, metodo: fare una torta in un microonde, G3, intento: fare una torta per un matrimonio. Estrai script per G1. Script candidati: 1, 2, 3, ... k.</sample>
    <sample id="293">Prossimamente, viene sviluppato un modello di filtraggio per selezionare i script più adatti.</sample>
    <sample id="294">Convertiamo i script e gli obiettivi in embedding di InstructGPT e calcoliamo il coseno di similitudine e le punteggi di similitudine per misurare la similitudine semantica.</sample>
    <sample id="295">Ecco la traduzione in italiano: "Verrà avviato lo script che contiene le parole chiave del vincolo di obiettivo. Si mantiene solo lo script se l'obiettivo di obiettivo ha la punteggi più alta nel set di obiettivi." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="296">Con il nostro metodo, InstructGPT può generare scenari di qualità superiore. Il nostro metodo migliora notevolmente la pianificazione, sia in termini di completezza semantica che di fedeltà alle restrizioni.</sample>
    <sample id="297">Poiché i modelli di grandi linguaggi sono costosi da deploysare, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare un dataset è un passo essenziale verso questo obiettivo.</sample>
    <sample id="298">Tuttavia, gli studi precedenti non permettono la pianificazione per obiettivi specifici e l'annotazione manuale del dataset è costosa.</sample>
    <sample id="299">Seguiamo l'idea della distillazione del conoscenza simbolica per distillare i dati set di pianificazione linguistica con vincoli da modelli a livello di linguaggio.</sample>
    <sample id="300">Applichiamo il nostro metodo per la costruzione di un dataset di pianificazione linguistica vincolata, chiamato Coscript.</sample>
    <sample id="301">In totale, generiamo 55.000 obiettivi specifici con script per garantire la qualità dei set di validazione e test. Richiediamo ai lavoratori di cloud sourcing di trovare e correggere gli esempi incontrollati.</sample>
    <sample id="302">Questa figura mostra la distribuzione dei vincoli di Coscript. Troviamo che Coscript mostra una pluralità alta nei obiettivi specifici generati. Con Coscript possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica con vincoli.</sample>
    <sample id="303">Abbiamo trovato che T5 finetuning su Coscript può generare script di qualità superiore rispetto a molti modelli di linguaggio a livello di grandi, indicando che modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset idonei.</sample>
    <sample id="304">In sintesi, abbiamo stabilito il problema di pianificazione del linguaggio con vincoli. Abbiamo valutato la capacità di pianificazione del linguaggio dei modelli di linguaggio e sviluppato un metodo per generare e filtrare in eccesso per i modelli di linguaggio.</sample>
    <sample id="305">Utilizziamo grandi modelli linguistici per generare un dataset di script di alta qualità, CoScript, per la pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere un risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="306">Grazie del tuo tempo. Per maggiori dettagli su Coscript, consulta il nostro articolo.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi SOTA.</sample>
    <sample id="308">Applicable to EaaS, Utility, Covertness, Transferability.</sample>
    <sample id="309">Le 14 lingue sono: English, Deutsch, Español, Français, Italiano, 日本語, 한국어, Nederlands, Português, Română, Русский, Türkçe, 中文. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="310">300 istanze.</sample>
    <sample id="311">La metrica di distanza utilizzata è la differenza di similitudine, Δcos, e la differenza di l2, Δl2. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per l'analisi di monolinguaggio. Ci sono due gruppi di modelli: Enc-PTR e Enc-Dec. Enc-PTR include XLM-R + PTR e mBERT + PTR, mentre Enc-Dec include mBART e mT5. Tra questi, mT5 ha ottenuto la migliore prestazione su tutti i set di dati. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="344">Contano la frequenza delle parole in un corpus di testo generale.</sample>
    <sample id="345">Ciao a tutti. Mi chiamo Shuheng. Oggi presenterò il nostro articolo "Le taggatrici di entità nominali CoNLL-2003 funzionano ancora bene nel 2023?". Iniziamo.</sample>
    <sample id="346">Il nostro articolo ha indagato il problema della generalizzazione utilizzando la compito di riconoscimento di entità nominate, o NER.</sample>
    <sample id="347">Abbiamo osservato che i modelli hanno utilizzato CoNLL-2003 per sviluppare l'NER per quasi 20 anni, e questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare ai dati moderni?</sample>
    <sample id="348">Per sviluppare nuovi tagger, cosa è necessario per una buona generalizzazione?</sample>
    <sample id="349">Al contempo, se osserviamo una pessima generalizzazione, cosa causa il calo di prestazioni di questi modelli?</sample>
    <sample id="350">Per investigare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un dataset che abbiamo raccolto da notizie Reuters del 2020 e poi annotato con le stesse linee guida di annotazione CoNLL-2003.</sample>
    <sample id="351">Poi abbiamo finetunato oltre 20 modelli su CoNLL-2003. Li abbiamo valutati sia sul set di test CoNLL-2003 che su CoNLL++.</sample>
    <sample id="352">Ecco la traduzione in italiano: "E, per concludere, abbiamo calcolato il cambiamento percentuale in F1 per valutare la generalizzazione di ciascun modello."</sample>
    <sample id="353">Quindi, cosa serve per una buona generalizzazione? In tutti gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali necessari.</sample>
    <sample id="354">Il primo è l'architettura del modello. Nelle nostre esperimenti, abbiamo trovato che i modelli transformer normalmente generalizzano meglio a nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è la dimensione del modello. Abbiamo trovato che di solito i modelli più grandi conducono a una miglior generalizzazione.</sample>
    <sample id="356">Ecco la traduzione in italiano: "E, per concludere, tutti sappiamo che il numero di esempi di ottimizzazione fine ha un effetto diretto sulle prestazioni di una task sottostante. Qui abbiamo anche scoperto che più esempi di ottimizzazione fine portano a una miglior generalizzazione."</sample>
    <sample id="357">Cosa causa la caduta delle prestazioni di alcuni modelli?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo della stessa test set ripetutamente, e questo si manifesta solitamente come una diminuzione dei ritorni su un nuovo test set.</sample>
    <sample id="359">La seconda ipotesi è il drift temporale, che è la degradazione del presto causata dallo scarto temporale crescente tra i dati di addestramento e di test.</sample>
    <sample id="360">Per l'overfitting adattivo, abbiamo visto dal grafico a destra che la linea di meglio adattamento rossa ha una pendenza maggiore di 1.</sample>
    <sample id="361">Significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di una unità di miglioramento su CoNLL++, il che significa che non ci sono ritorni decrescenti.</sample>
    <sample id="362">Ecco la traduzione in italiano: "E questo ci mostra che, in questo caso, l'overfitting adattivo non è stato osservato."</sample>
    <sample id="363">Quindi, riguardo alla drift temporale?</sample>
    <sample id="364">Per quanto riguarda il drift temporale, abbiamo condotto un esperimento di ritraining o di continuare a pretrainare alcuni modelli con dati più recenti e abbiamo scoperto che la prestazione peggiora con un gap temporale maggiore.</sample>
    <sample id="365">Cosa causa la caduta delle prestazioni?• Adattamento all'overfitting? - Nessun ritorno decrescente - Non osservato• Drift temporale? - Le prestazioni peggiorano con un gap temporale maggiore - Causa principale della caduta delle prestazioni</sample>
    <sample id="366">La nostra conclusione è che, per una buona generalizzazione, abbiamo bisogno di una migliore architettura del modello, di un modello di dimensione maggiore e di esempi di ottimizzazione più dettagliati, e questi obiettivi andano di pari passo. Non possiamo avere solo un ingrediente, ma tutti insieme.</sample>
    <sample id="367">Al contempo, abbiamo anche scoperto che la diminuzione del prestigio è causata dallo scorrimento temporale e, sorprendentemente, non è causata dall'overfitting adattivo, anche se CoNLL-2003 è stato utilizzato per oltre 20 anni.</sample>
    <sample id="368">Riassunto: Per una buona generalizzazione, ci serve: - Architettura di modello migliore - Dimensione di modello più grande - Più esempi di finetuning. Il calo di prestazioni è causato da: - Drift temporale - Non sovrappopolamento adattivo. Le taggatrici CoNLL-2003 funzionano ancora? Sì!</sample>
    <sample id="369">Conclusione: Per una buona generalizzazione, abbiamo bisogno di: - Architettura di modello migliore - Dimensione di modello più grande - Esempi di finetuning maggiori. - Il calo di prestazioni è causato da: - Drift temporale - Non sovrappopolamento adattivo - Le taggatrici CoNLL-2003 funzionano ancora? - SI! Speriamo che il nostro articolo chieda più ricerche su come migliorare la generalizzazione dei modelli.</sample>
    <sample id="370">E infine, assicurati di controllare il nostro articolo, il nostro dataset e se hai domande, non esitare a farmele. Grazie mille.</sample>
    <sample id="397">Non so. Potresti dare più dettagli?</sample>
    <sample id="398">Nell'esempio con Servin e Kea, sono necessarie le conoscenze specifiche dell'entità come Servin è un giudice.</sample>
    <sample id="399">La qualità dell'esempio è più importante. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="400">L'articolo si concentra sugli esperimenti estesi sui modelli linguistici RoBERTa e GPT-2.</sample>
    <sample id="401">The model combines the attention scores from multiple levels. If you have more questions about it, feel free to ask.</sample>
    <sample id="402">Gli esempi di inferenza diretta sono "easy on me" e "the first one". Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="403">Fudan University e Brain Technologies Inc.</sample>
    <sample id="404">Ci sono sei autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="405">No, non è stato considerato come un approccio standard.</sample>
    <sample id="406">Un gruppo contrassegnato è "a woman warrior".</sample>
    <sample id="407">Non sono state specificate quali architetture dei modelli non generalizzano in modo adeguato. Potresti dare più dettagli o riferirti a un contesto specifico?</sample>
    <sample id="408">Non so quali siano i nomi dei set di dati di test. Potresti dare più dettagli o cercare di trovare la risposta da qualche altro luogo?</sample>
    <sample id="409">There are six authors involved in the article. If you want to know more about them, feel free to ask.</sample>
    <sample id="410">L'autore opera con più modalità.</sample>
    <sample id="439">Non so. Non c'è informazione su questo. Potresti cercare di trovare altro materiale o chiedere a qualcun altro?</sample>
    <sample id="440">I nomi dei relatori sono Zhiyang Xu, Ying Shen e Lifu Huang.</sample>
    <sample id="441">Sì.</sample>
    <sample id="442">Le risorse esistenti supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati.</sample>
    <sample id="443">Ciao, sto per parlare del nostro lavoro sul risolvere le espressioni di riferimento indirette per la selezione di entità, nel quale abbiamo introdotto il corpus AltEntities.</sample>
    <sample id="444">Il contenuto inglese è "Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis, Google Research". La traduzione in italiano è "Risoluzione di espressioni riferimento indiretto per la selezione di entità, AltEntities Corpus, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, e Annie Louis, Google Research". Se hai bisogno di ulteriori traduzioni o hai altre domande, fammi sapere!</sample>
    <sample id="445">Il nostro obiettivo è comprendere il linguaggio degli utenti quando fanno una scelta. Considera questa domanda alternativa: "Volevi dire 'easy on me' o 'I gotta feeling'? " Qui l'utente vuole scegliere tra una di queste due canzoni.</sample>
    <sample id="446">Il contenuto inglese tradotto in italiano è: "La cosa più ovvia è usare una riferimento diretto, ad esempio, dicendo il nome della canzone "easy on me" o la sua posizione "la prima".</sample>
    <sample id="447">Il contenuto inglese in italiano è: "Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può accadere quando l'utente non ricorda il nome della canzone." Se hai bisogno di altro, come chiarimenti o ulteriori traduzioni, fammi sapere!</sample>
    <sample id="448">Ecco la traduzione in italiano: "Le pronunciazioni sono troppo simili tra loro e difficili da distinguere." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="449">Ecco il contenuto tradotto in italiano: "O quando il utente vuole specificare una preferenza. Ecco alcuni esempi di espressioni indirette, ad esempio "il nuovo" o "la canzone che non è energetica". "</sample>
    <sample id="450">Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking delle grandi modelli di linguaggio nell'intercomprensione.</sample>
    <sample id="451">Non eravamo a conoscenza di un dataset pubblico di grandi dimensioni per il test, quindi abbiamo raccolto uno utilizzando l'annotazione del pubblico. Il nostro dataset copre tre domini diversi: musica, libri e ristoranti.</sample>
    <sample id="452">La nostra metodologia di raccolta di dataset enfatizza l'informalità utilizzando una comprensione di cartoni animati.</sample>
    <sample id="453">Il contenuto inglese è tradotto in italiano come segue: "Il fumetto ha tre bolle di dialogo. Nella prima bolletta, Bob dice: 'Ricordi quella canzone che ascoltavamo ieri?' E con quella Bob stabilisce il contesto del dialogo."</sample>
    <sample id="454">Nel secondo bocciolo di dialogo, Alice dice: "Credi che intenda 'Easy on Me' o 'I Gotta Feeling'?"</sample>
    <sample id="455">La metodologia mette l'accento sull'informalità utilizzando una task di completamento di cartoni. L'altro personaggio chiede: "Ricordi quella canzone che ascoltavamo ieri?" La risposta alternativa è: "Credi che sia 'Easy on Me' o 'I Gotta Feeling'?". Nella terza bolla di dialogo, Bob fa un riferimento indiretto per selezionare uno di questi enti, ad esempio "New York".</sample>
    <sample id="456">Raccolta di dati: La metodologia enfatizza l'informalità utilizzando una comprensione di cartoni. Imposta il contesto del dialogo, scelto da pochi prompt manuali per ogni dominio. La seconda bolla di dialogo è automaticamente fornita, ma la terza è riempita dall'annotatore. La prima bolla di dialogo è scelta da pochi prompt manuali per ogni dominio.</sample>
    <sample id="457">Il contenuto inglese è tradotto in italiano come segue: "La seconda variante, che è la domanda alternativa, viene generata come segue." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="458">Genera domande alternative =&gt; campionamento di coppie di entità. "Cosa intendi per A o B?" - Oggetti con infobox simili su Wikipedia, stesso genere e/o artista. "Cosa intendi per 'Questo è lui' o 'Uomo allo specchio'?" - Oggetti con descrizioni simili su Wikipedia. "Cosa intendi per 'Pensieri di te' o 'Felice ovunque'?" - Oggetti con titoli simili. "Cosa intendi per 'Rientro' (memoria) o 'Rientro' (novel Shatner)"? - Uniforme a caso. "Cosa intendi per 'Potresti essere mio' o 'Il modo in cui sono'?"</sample>
    <sample id="459">Ecco il contenuto tradotto in italiano: "Ecco i metodi di campionamento che abbiamo utilizzato. Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro e spesso diventa più difficile effettuare la disambiguisazione."</sample>
    <sample id="460">Il contenuto inglese è tradotto in italiano come segue: "Genera domande alternative =&gt; campionamento di coppie di entità. "Se hai bisogno di ulteriori traduzioni o hai domande, non esitare a chiedere.</sample>
    <sample id="461">Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return".</sample>
    <sample id="462">Il terzo è quando hanno descrizioni simili su Wikipedia e infine quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone.</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli annotatori, sanno il nome di queste entità, ma non necessariamente sanno qualcosa su di esse.</sample>
    <sample id="464">Quindi cosa facciamo è che mostriamo un po' di conoscenza di fondo riguardo alle due entità. Per le canzoni, semplicemente mostriamo un link di ricerca Google per ognuna.</sample>
    <sample id="465">Ecco il contenuto tradotto in italiano: "Chiediamo agli annotatori di ascoltare almeno una parte di ciascun brano e di leggere informazioni su ognuno di essi. Ad esempio, ecco i risultati della ricerca Google per la canzone 'Easy on Me'."</sample>
    <sample id="466">Per il dominio ricette e libri, abbiamo mostrato del testo di sfondo da Wikipedia per le ricette. Inoltre, abbiamo mostrato le loro immagini, anch'esse da Wikipedia, affinché gli annotatori sappiano come si presentano.</sample>
    <sample id="467">Allora chiediamo agli annotatori di scegliere una di queste entità, per esempio qui la prima, e descriverle utilizzando 3 a 5 espressioni di riferimento indiretto.</sample>
    <sample id="468">Ecco il contenuto tradotto in italiano: "Ecco alcuni esempi dal nostro dataset. Ad esempio, 'il brano senza parole', 'non il brano con il ragazzo di 12 anni', 'il brano fittizio', 'proviene dall'Azerbaigian'." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="469">Il corpus AltEntities ha circa 6.000 domande alternative in tre domini e circa 42.000 espressioni di riferimento indirette. I risultati con il modello T5 XL sono riassunti di seguito:</sample>
    <sample id="470">Se il modello di linguaggio ha accesso alla stessa conoscenza di fondo degli annotatori, la precisione è veramente alta, intorno al 92-95%, ma questo non è realistico.</sample>
    <sample id="471">Se il modello di linguaggio ha accesso a qualche conoscenza di fondo parzialmente sovrapposta, allora la precisione è tra il 82% e il 87%, che è più realistica, ad esempio quando il modello di linguaggio recupera la conoscenza di fondo.</sample>
    <sample id="472">Se il modello linguistico ha accesso solo ai nomi degli enti, la precisione è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili in ambito di domanda. Ecco il link al dataset. Grazie.</sample>
    <sample id="473">Con la wait-k strategia， LA， CAAT e l'architettura di punta specificamente adattata per SimulST.</sample>
    <sample id="474">Yanis Labrak: LIA, Avignon Université. Adrien Bazoge: LS2N, Nantes Université. Richard Dufour: Clinique des données, CHU de Nantes. Mickael Rouvier: Zenidoc. Emmanuel Morin: LS2N, Nantes Université. Béatrice Daille: LS2N, Nantes Université. Pierre-Antoine Gourraud: Zenidoc.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Tre.</sample>
    <sample id="477">Ciao， sono Sara Papi， dall'Università di Trento e Fondazione Bruno Kessler， e presenterò brevemente il mio articolo "Attenzione come guida per la traduzione simultanea del discorso"， un lavoro in collaborazione con Matteo Negri e Marco Turchi.</sample>
    <sample id="478">La traduzione simultanea del discorso, o SimulST, è il processo di tradurre il linguaggio parlato in un testo in un'altra lingua in tempo reale, permettendo la comunicazione interlinguistica.</sample>
    <sample id="479">I modelli SimulST attuali hanno dei problemi. Solitamente vengono addestrati architetture specifiche, introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">I modelli SimulST attuali presentano due problemi principali. In primo luogo, sono solitamente addestrati con architetture specifiche, introducendo moduli aggiuntivi da ottimizzare. In secondo luogo, i procedimenti di addestramento sono lunghi e complessi, ad esempio, coinvolgono obiettivi di ottimizzazione diversi.</sample>
    <sample id="481">I modelli SimulST attuali presentano i seguenti problemi: - Architetture specifiche sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare. - Procedure di addestramento lunghe e complesse, ad esempio, obiettivi di ottimizzazione diversi. - Addestramento e manutenzione di diversi modelli per raggiungere diversi regimi di latenza, ad esempio, 1s, 2s, ecc.</sample>
    <sample id="482">Quale è la nostra soluzione?</sample>
    <sample id="483">Primo: Utilizzare modelli ST offline esistenti senza ristrutturare o adottare una specifica architettura per SimulST.Secondo: Utilizzare solo un modello per ogni regime di latenza e gestire la latenza tramite parametri specifici.</sample>
    <sample id="484">Ecco la traduzione in italiano: "Ecco il contenuto inglese tradotto in italiano: 'Ecco la traduzione in italiano: "Ecco il contenuto inglese tradotto in italiano: '".</sample>
    <sample id="485">La nostra soluzione è di proporre EDAtt, o attenzione encoder-decoder, e è una strategia per cui decidiamo di emettere o non una traduzione parziale basandoci su dove l'attenzione punta.</sample>
    <sample id="486">Un parola viene emessa se l'attenzione non è concentrata, ossia la sua somma è inferiore a un certo soglia α verso le ultime λ frasi di discorso, il che significa che le informazioni ricevute sono abbastanza stabili.</sample>
    <sample id="487">Ecco la traduzione: "Vado a parlare di...". Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="488">Voglio parlare di...</sample>
    <sample id="489">Vedremo che le prime due parole puntano ai primi frame di speech ricevuti, mentre la parola finale punta agli ultimi frame di speech ricevuti, ovvero i λ frame di speech.</sample>
    <sample id="490">Questo significa che le prime due parole saranno emesse.</sample>
    <sample id="491">Mentre la somma della attenzione è sopra un certo limite α, non emetteremo la parola finale e aspetteremo un altro frammento di discorso.</sample>
    <sample id="492">Se andiamo avanti e riceviamo un altro frammento di discorso e il nostro modello predice altri tre parole e guarderemo i pesi di attenzione incrociata.</sample>
    <sample id="493">Vedremo che nessuna parola punti verso le ultime λ frasi di discorso.</sample>
    <sample id="494">Questo significa che queste tre parole saranno emesse.</sample>
    <sample id="495">Se guardiamo i principali risultati di EDAtt.</sample>
    <sample id="496">Rappresentiamo i risultati della traduzione simultanea su grafici in cui abbiamo BLEU da un lato che misura la qualità della traduzione e il ritardo medio dall'altro.</sample>
    <sample id="497">Ecco la traduzione: "Quella è la misura di latenza e abbiamo anche considerato la media di like computazionale consapevole che tiene conto del tempo computazionale del modello per ottenere l'output." Se hai bisogno di chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="498">Quindi vogliamo che le nostre curve siano il più in alto possibile su questo grafico.</sample>
    <sample id="499">Ma anche vogliamo che siano spostati a sinistra.</sample>
    <sample id="500">E compare con strategie popolari che vengono applicate anche a modelli offline, come la strategia wait-k e l'accordo locale. Inoltre, compare anche con l'architettura di punta specificamente adattata per la traduzione simultanea.</sample>
    <sample id="501">Questi sono i risultati della strategia di traduzione simultanea in tedesco.</sample>
    <sample id="502">Evidentemente, EDAtt supera tutte le strategie applicate ai modelli offline, poiché le loro curve sono spostate a sinistra.</sample>
    <sample id="503">E anche notiamo che, se consideriamo il tempo effettivo trascorso o il tempo computazionale, EDAtt è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire più risultati, leggi il nostro articolo. Abbiamo anche rilasciato il codice open source e i modelli, e contemporaneamente il codice di uscita per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="505">Sì, il set di dati è disponibile pubblicamente. Se hai altre domande, puoi chiedere.</sample>
    <sample id="506">Ciao a tutti. Il mio nome è Ying e il mio collega Zhiyang e io presenteremo la nostra ricerca su MultiINSTRUCT: miglioramento dell'apprendimento zero-shot multi-modale tramite ottimizzazione delle istruzioni.</sample>
    <sample id="507">Con l'avanzamento dei modelli di linguaggio a grande scala, molti lavori hanno iniziato a esplorare nuove paradigmi di apprendimento per riutilizzare modelli di linguaggio preaddestrati per compiti diurni diversi in modo efficiente in termini di parametri e dati.</sample>
    <sample id="508">Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai grandi modelli di linguaggio di eseguire compiti non visti in modo zero-shot seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei lavori precedenti sulla ottimizzazione delle istruzioni si sono concentrati sul miglioramento del presto zero di un compito solo linguistico, mentre la visione computerizzata e i compiti multimodali sono stati lasciati fuori.</sample>
    <sample id="510">Quindi, in questo lavoro vogliamo investigare se l'addestramento di istruzioni su modelli pre-allenati multimodali può realmente migliorare la generalizzazione a compiti multimodali senza corrispondenza.</sample>
    <sample id="511">Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole disparità nella disponibilità di set di istruzioni tra NLP e multimodale.</sample>
    <sample id="512">Esistono più di 1.600 task di istruzioni solo linguistiche. Tuttavia, non esiste un dataset di istruzioni multimodali di grandi dimensioni e pubblicamente disponibile. Questo ci ha motivati a costruire un dataset di istruzioni multimodali.</sample>
    <sample id="513">Ecco il contenuto tradotto in italiano: "Ecco presentato MultiINSTRUCT, il primo dataset di benchmark di ottimizzazione di istruzioni multimediali che comprende 62 compiti multimediali diversi coprendo 10 categorie ampie."</sample>
    <sample id="514">Questi compiti sono derivati da 21 set di dati open source esistenti e ogni compito è dotato di 5 istruzioni scritte da esperti.</sample>
    <sample id="515">Per investigare l'addestramento multimodale delle istruzioni su nostro dataset, prendiamo OFA, un modello pre-addestrato multimodale unificato, come modello base. OFA usa un vocabolario unificato per il linguaggio, i token immagine e le coordinate di una casella di confine.</sample>
    <sample id="516">Ecco il contenuto tradotto in italiano: "Ecco alcuni esempi da nostro dataset MultiInstruct."</sample>
    <sample id="517">Per unificare il trattamento di diversi tipi di dati di input e output.</sample>
    <sample id="518">Seguiamo il metodo di OFA e formuliamo tutte le tare in un formato di sequenza a sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i box di contorno sono rappresentati nello stesso spazio di token.</sample>
    <sample id="519">Ora parlerò di ottimizzazione delle istruzioni multimediali.</sample>
    <sample id="520">Per il dataset di addestramento, si utilizzano 53 compiti da 9 gruppi per l'addestramento e si estraggono 10.000 istanze per compito. Per il dataset di test, si riserva l'intero gruppo di ragionamento comune per il test e si selezionano altri 5 compiti dal gruppo VQA e dal gruppo varie.</sample>
    <sample id="521">Inoltre, estraiamo casualmente 20 compiti dal test set del dataset Natural Instructions come compiti non visti per NLP.</sample>
    <sample id="522">Dettagli di implementazione. Dettagli di addestramento: - Modello pre-addestrato OFA-Large, 472M. - Mescolare tutte le istanze per tutte le task. - Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni. Dettagli di test: - Per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento. - Rapportiamo la media e il massimo delle prestazioni e la deviazione standard delle prestazioni in tutte e cinque le esperimenti.</sample>
    <sample id="523">Durante il test, per ogni compito, condurremo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento.</sample>
    <sample id="524">Dettagli di implementazione. Dettagli di addestramento: - Modello pre-addestrato OFA-Large, 472M. - Mescolare tutte le istanze per tutte le task. - Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni. Dettagli di test: - Per ogni task, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ogni esperimento. - Rapportiamo la media, il massimo e la deviazione standard del rendimento in tutti e cinque gli esperimenti.</sample>
    <sample id="525">Per i compiti di classificazione multi-modale, come l'Entanglement visivo, la ragionamento spaziale visivo, la ragionamento visivo linguistico naturale e la classificazione di tipo di disastro, si riporta l'Accuracy. Per i compiti di generazione multi-modale, come la VQA di senso comune, la VQA testuale, la VQA fondata, l'estrazione di testo visivo e il dialogo visivo, si riporta il Rouge-L. Per i compiti di NLP, si riporta il Rouge-L. Inoltre, calcoliamo la prestazione aggregata per ogni modello basandoci sulla media della prestazione del modello su tutti i compiti multimodale e non visti di NLP. Usiamo il Rouge-L come punteggio di prestazione per la maggior parte dei compiti e l'Accuracy per i compiti che hanno solo l'Accuracy come metrica.</sample>
    <sample id="526">Sensibilità. Quanto la modella è sensibile a una varietà di istruzioni per la stessa compito:. - Abilità a produrre risultati consistenti per la stessa compito, indipendentemente da lievi variazioni nella formulazione delle istruzioni.</sample>
    <sample id="527">Ecco il contenuto tradotto in italiano: "Ecco i nostri risultati principali. Come possiamo vedere, l'ottimizzazione delle istruzioni può migliorare significativamente le prestazioni di OFA su compiti multimediali non visti."</sample>
    <sample id="528">L'apprendimento da dati di istruzioni naturali può beneficiare dell'ottimizzazione delle istruzioni.</sample>
    <sample id="529">Ecco il contenuto tradotto in italiano: "Come possiamo vedere, con l'aumento del numero di task, il modello raggiunge prestazioni migliori e, al contempo, una sensibilità inferiore."</sample>
    <sample id="530">Quindi abbiamo anche fatto un esperimento. Abbiamo utilizzato un'istruzione contro cinque istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Quindi, questo mostra l'effetto di diverse strategie di ottimizzazione fine sulle sensibilità del modello. Come possiamo vedere, attraverso il trasferimento di apprendimento da dataset di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello OFA originale.</sample>
    <sample id="532">Può anche essere visto che il trasferimento di apprendimento da dataset di istruzioni naturali può aiutare OFA a ottenere una prestazione molto migliore sul dataset di istruzioni naturali.</sample>
    <sample id="533">Conclusione:.- Primo dataset di grandi dimensioni per l'addestramento multi-modale con istruzioni. - Contiene 62 task multi-modali da 10 categorie ampie. - Migliora significativamente la capacità zero-shot di OFA tramite l'addestramento con istruzioni. - Esplora diverse tecniche di apprendimento trasferibile e mostra i loro benefici. - Progetta una nuova metrica di sensibilità.</sample>
    <sample id="534">C'è una cosa in più! Stiamo raccolgendo un dataset di addestramento di istruzioni multimodali molto più grande con circa 150 compiti aggiuntivi di visione-lingua e li pubblicheremo presto! Questo è il codice QR per i nostri dati e il modello. Grazie!</sample>
    <sample id="535">Sara Papi, Matteo Negri, e Marco Turchi sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="536">Il nome del relatore è Mohammad Javad Hosseini.</sample>
    <sample id="562">Ciao a tutti, sono Koustuv Sinha e sono lieto di accogliervi al nostro talk sul nostro articolo di ACL 2023, "Le giudicazioni di accettabilità dei modelli di linguaggio non sono sempre robuste al contesto."</sample>
    <sample id="563">I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti rispetto al contesto.</sample>
    <sample id="564">Rivediamo il paradigma delle coppie minimali.</sample>
    <sample id="565">Il paradigma del paio minimo, MPP, valuta i modelli di linguaggio basandosi su giudizi di accettabilità, che possono includere grammaticità, come in BLiMP e SyntaxGym, o accettabilità in termini di stereotipi, come in Crows.</sample>
    <sample id="566">Il contenuto inglese in italiano è: "Nel paradigma del paio minimo, la maniera tipica di valutare i modelli di linguaggio è mostrare una frase accettabile o grammaticalmente corretta e poi una frase non accettabile o grammaticalmente errata."</sample>
    <sample id="567">Paradigma delle coppie minimali. Le valutazioni del paradigma delle coppie minimali, MPP, dei modelli di linguaggio utilizzano differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli di linguaggio:.BLiMP 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei.SyntaGym 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro.CrowS 1. Frase stereotipica. 2. Frase non stereotipica.P,1, &gt; P,2, P,1.any, ≥ P,2.any, P,1, &gt; P,2, La speranza è che il modello assegna maggiore probabilità alle frasi accettabili.</sample>
    <sample id="568">Il paradigma del paio minimo, MPP, valuta i modelli di linguaggio utilizzando differenze relative in probabilità di sequenza per valutare il conoscere astratto dei modelli. Per BLiMP: 1. Molti persone stavano aiutando loro stessi. 2. *Molti persone stavano aiutando lei. P,1, &gt; P,2, Per SyntaxGym: 1. Nessun cliente ... ha speso denaro. 2. *Il cliente ... ha speso denaro. P,1.any, &gt; P,2.any, Per Crows: 1. Le donne sono terribili nel lavoro manuale. 2. Gli uomini sono terribili nel lavoro manuale. Questi giudizi sono stabili con un contesto precedente lungo? Il flusso attuale di MPP non ci permette di valutare l'accettazione dei modelli verso frasi più lunghe.</sample>
    <sample id="569">Oggi i modelli di linguaggio stanno sviluppando finestre di contesto sempre più lunghe, quindi è cruciale valutare l'acceptability dei modelli attraverso la finestra di contesto.</sample>
    <sample id="570">Rivedere il paradigma del paio minimo, chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe.</sample>
    <sample id="571">Quindi, l'approccio è questo. Ciò che facciamo è simulare queste sequenze più lunghe. Rivediamo i dataset stessi e poi ricreiamo frasi scegliendo frasi accettabili o non accettabili da quei dataset.</sample>
    <sample id="572">Approccio.Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.Test Suite: Conformità soggetto-verbale.Spazio di candidati Prefixes.Matched: Conformità soggetto-verbale.Mismatched: Effetti dell'isola, lacune di filtro, legame, Wikipedia.GPT2, OPT famiglia - 125M a 6.7B.</sample>
    <sample id="573">Cosa facciamo è creare sequenze più lunghe e accettabili, con la stessa corrispondenza della struttura grammaticale. Estraiamo frasi grammaticali da Adjunct Island.</sample>
    <sample id="574">Testa se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'accettabilità.</sample>
    <sample id="575">Quindi possiamo fare la stessa cosa scegliendo frasi non accettabili dallo stesso matching e questo potrebbe anche essere utilizzato per testare l'accettabilità del modello.</sample>
    <sample id="576">E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un diverso dataset. Quindi questo è ciò che chiamiamo "mismatched scenario".</sample>
    <sample id="577">Quindi qui le frasi provengono ancora da dataset rilevanti, ma non dallo stesso dataset che stai valutando. E possiamo fare lo stesso per il caso di accettabilità.</sample>
    <sample id="578">Infine possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia.</sample>
    <sample id="579">Quindi questo ci dirà se le giudicazioni di accettabilità dei modelli sono effettivamente influenzate da alcun contesto.</sample>
    <sample id="580">Testare se le giudicazioni MPP variano in funzione della lunghezza del contesto, del match strutturale e dell'acceptabilità.</sample>
    <sample id="581">Allora come funziona il modello? Innanzitutto guardiamo le frasi di Wikipedia che sono completamente irrellevanti al paio di query attuale e poi scopriamo che le giudicazioni MPP sono in gran parte robuste per lunghezze di contesto arbitrarie.</sample>
    <sample id="582">I giudizi MPP sono robusti per lunghezze di contesto arbitrarie. Effettuiamo valutazioni MPP con diversi contesti - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. Incrementiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2 e vediamo qui, nella linea punteggiata arancione, che i giudizi MPP sono relativamente stabili.</sample>
    <sample id="583">Eseguiamo valutazioni MPP con contesti diversi - accettabili/non accettabili; struttura corrispondente/non corrispondente - di lunghezze fino a 900 token. BLIMP, OPT 6.7B Strategia di prefisso. Ora, cosa succede quando scegliamo frasi dallo stesso dataset?</sample>
    <sample id="584">Ecco la traduzione in italiano: "Quindi qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso dataset di sintassi BLiMP." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="585">Ecco la traduzione in italiano: "Vediamo che le giudicazioni MPP aumentano o diminuiscono significativamente quando si aggiungono prefissi accettabili o non accettabili." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="586">Eseguiamo valutazioni MPP con contesti diversi - accettabili/non accettabili; struttura corrispondente/non corrispondente - fino a lunghezze di 900 token.</sample>
    <sample id="587">Vediamo un aumento o una diminuzione massiva della valutazione MPP del modello a seconda se il prefisso scelto è accettabile o non accettabile.</sample>
    <sample id="588">Ecco la traduzione in italiano: "Ora questo effetto è molto grande, come questo effetto aumenta a lungo termine, e questo probabilmente influenzerà modelli di linguaggio più recenti, che hanno un finestra di contesto grande."</sample>
    <sample id="589">Perché i prefissi corrispondenti influenzano così tanto le giudicazioni del modello di linguaggio?</sample>
    <sample id="590">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di linguaggio?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.". - Prefissi avverbi lunghi: "Prima di tutto, &lt;sent&gt;.". - Aggiungere una clausola: "Indipendentemente da quello che X ne pensa, &lt;sent&gt;.". - Citazione: "Ieri, X ha detto, &lt;sent&gt;.".Eseguiamo una serie di analisi dove proviamo a modificare la frase d'ingresso, cercando di preservare la struttura rilevante, ma aggiungendo rumore alla frase d'ingresso. Dopo aver fatto diverse di queste modifiche.</sample>
    <sample id="591">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di LM?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.".- Avverbi di lunghezza prefissa: "Prima di tutto, &lt;sent&gt;.".- Clausola aggiunta: "Indipendentemente da quello che X ne pensa, &lt;sent&gt;.".- Citazione: "Ieri, X ha detto, &lt;sent&gt;."Troviamo che queste perturbazioni non stanno realmente facendo il modello cambiare la sua scelta di giudizio NPP.</sample>
    <sample id="592">Perché i prefissi corrispondenti influenzano le giudicazioni del modello di linguaggio?Modifichiamo le frasi di contesto in modo da preservare la struttura rilevante e chiediamo se i modelli sono altrettanto sensibili a queste frasi.- Prefissi/suffissi avverbi: "Tuttavia, &lt;sent&gt;.". - Avverbi di lungo prefisso: "Prima di tutto, &lt;sent&gt;.". - Aggiungere una clausola: "Indipendentemente da ciò che X ne pensa, &lt;sent&gt;.". - Citazione: "Ieri, X ha detto, '&lt;sent&gt;.'.I modelli sono sensibili alle frasi modificate in modi simili.</sample>
    <sample id="593">Quando perturbiamo le frasi nel dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. Quando perturbiamo le frasi nel dominio non accettabile, vediamo una diminuzione delle giudicanti MPP in modo simile.</sample>
    <sample id="594">I principali punti da ricordare del nostro lavoro sono che i modelli di linguaggio sono sensibili a feature sintattiche e semantiche latenti che sono condivise tra le frasi.</sample>
    <sample id="595">I modelli di linguaggio sono sensibili a feature sintattiche/semantiche latenti condivise tra frasi. Le valutazioni MPP con input di frase singola e breve non catturano completamente il sapere astratto dei modelli di linguaggio.</sample>
    <sample id="596">Riassunti principali

- I modelli di linguaggio sono sensibili a feature sintattiche/semantiche latenti condivise tra frasi.
- Le valutazioni MPP con input di singola frase brevi non catturano completamente il sapere astratto dei modelli di linguaggio.

Per ulteriori dettagli sulle nostre esperimenti, leggi il nostro articolo. Grazie per l'ascolto.</sample>
    <sample id="597">In un unordered multiset.</sample>
    <sample id="598">55,000.</sample>
    <sample id="626">Il metodo di allineamento migliore per DEplain è MASSalign.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato allevia il blocco dell'annotazione.</sample>
    <sample id="628">Non c'è informazione sufficiente nel contenuto fornito per rispondere a questa domanda. Potresti avere bisogno di consultare il documento originale o avere ulteriori dettagli?</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato raccolgendo notizie Reuters del 2020 e annotandole secondo le linee guida di annotazione CoNLL-2003.</sample>
    <sample id="630">Ciao a tutti, il mio nome è Yusen Zhang dalla Penn State University. Oggi presenterò il nostro lavoro, XSemPLR: analisi semantica interlinguistica in diverse lingue naturali e rappresentazioni di significato.</sample>
    <sample id="631">Parsa semantica è una task per costruire rappresentazioni semantiche delle query utente, come SQL e Calcolo Lambda.</sample>
    <sample id="632">La semantica di parsing interlingua è una task che traduce query in diverse lingue naturali in diverse rappresentazioni di significato.</sample>
    <sample id="633">In questa figura, dobbiamo tradurre la query in diverse lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL e così via.</sample>
    <sample id="634">I modelli di parsing semantico interlinguistico esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe lingue naturali.</sample>
    <sample id="635">I modelli CLSP esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe lingue naturali.</sample>
    <sample id="636">I modelli CLSP esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe rappresentazioni di significato.</sample>
    <sample id="637">I modelli di Parsing Semantico Interlingua esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certe rappresentazioni di significato.</sample>
    <sample id="638">I modelli CLSP esistenti sono proposti e valutati separatamente su dataset di compiti e applicazioni limitati. Ad esempio: mancanza di copertura su certi modelli neurali.</sample>
    <sample id="639">Ecco la traduzione in italiano: "Per questo scopo, proponiamo Exemplar. Offriamo un dataset uniforme Exemplar per la semantica di parsing interlingua in diverse lingue naturali e rappresentazioni di significato." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="640">Contiene 9 set di dati in diversi domini, 5 compiti di parsing semantico, 8 rappresentazioni di significato e 22 lingue naturali in 15 famiglie linguistiche.</sample>
    <sample id="641">Per valutare meglio il nostro benchmark, consideriamo i sei set di allenamento e valutazione.</sample>
    <sample id="642">Il primo è "Translate-Test": si usa l'API di Google Translate per tradurre il fonte nella lingua di destinazione. Poi si usa un modello monolingue per il training e l'evaluation.</sample>
    <sample id="643">Ad esempio, abbiamo addestrato il modello inglese su una query inglese e durante l'inferezione, abbiamo tradotto la query tedesca utilizzando l'API in inglese e poi abbiamo utilizzato il modello addestrato per prevedere il SQL.</sample>
    <sample id="644">Impostazioni dell'esperimento Consideriamo le sei impostazioni per l'addestramento e l'valutazione. Modello monolinguale: La lingua di origine è la stessa della lingua di destinazione, ad esempio Germano-germano. Abbiamo anche testato la configurazione di modello monolinguale Few-shot addestrando modelli monolinguali con solo 10% di dati di addestramento. Addestramento Germano, Few-shot Modello Germano SQL Inferenza Germano Modello Germano SQL</sample>
    <sample id="645">Nella configurazione considerata, la lingua di origine è la stessa della lingua di destinazione, ad esempio tedesco-tedesco o inglese-inglese.</sample>
    <sample id="646">Consideriamo i sei set di esperimenti per l'addestramento e l'valutazione. Modello monolinguale: La lingua di origine è la stessa della lingua di destinazione, ad esempio Germano-germano. Abbiamo anche testato la configurazione Monolinguale Few-shot addestrando modelli monolinguali con solo 10% dei dati di addestramento.</sample>
    <sample id="647">Consideriamo i sei set di esperimenti per l'addestramento e l'valutazione. Modello multilingue: Addestrare un modello multilingue per tutte le lingue. Addestramento: Germano, Inglese, Cinese. Modello multilingue. SQL. Inferenza: Germano. Modello multilingue. SQL.</sample>
    <sample id="648">Per esempio, mettiamo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue e durante l'infrazione possiamo utilizzare questo modello.</sample>
    <sample id="649">Impariamo a tradurre query tedesche o cinesi, ecc.</sample>
    <sample id="650">Consideriamo i sei set di esperimenti per l'addestramento e l'evaluazione. Cross-lingual Zero-shot/Few-shot transfer: Addestrare su una lingua di origine e trasferire su un'altra lingua. Addestramento: Inglese o Inglese, Germano Few-shot. Inferenza: Germano. Modello multilingue SQL.</sample>
    <sample id="651">Durante l'addestramento, si addestra su query in inglese o la combinazione di query in inglese e tedesco a pochi colpi per addestrare un modello multilingue e prevedere l'output SQL.</sample>
    <sample id="652">L'analisi dei modelli monolingui si svolge su due gruppi di modelli.</sample>
    <sample id="653">Analisi del Monolingua. - Valutiamo due gruppi di modelli in un contesto monolingue. Enc-PTR: Encoder multilingue preaddestrati con decodificatori basati su pointer. XLM-R + PTR, mBERT + PTR. Enc-Dec: Encoder-decodificatori multilingue preaddestrati. mBART, mT5. - Abbiamo trovato che Enc-Dec, mT5, ottiene la migliore prestazione su tutti i dataset!</sample>
    <sample id="654">Analisi del Monolingua. - Valutiamo su due gruppi di modelli nel contesto Monolingua. Enc-PTR: Encoder multilingue preaddestrati con decodificatori basati su pointer. - XLM-R + PTR, mBERT + PTR. Enc-Dec: Encoder-decodificatori multilingue preaddestrati. - mBART, mT5. - Abbiamo trovato che Enc-Dec, mT5, ottiene la migliore prestazione su tutti i dataset! Tabella con valori numerici per i modelli su diversi dataset.</sample>
    <sample id="655">L'analisi mostra che Enc-Dec, in particolare mT5, ha ottenuto la migliore prestazione su tutti e nove i dataset.</sample>
    <sample id="656">L'analisi dell'addestramento multilingue. Valutiamo su mT5 e XLM-R + PTR in un contesto multilingue. Enc-Dec/Enc-PTR, mT5/XLM-R, può essere migliorato addestrando in una miscela di varie lingue.</sample>
    <sample id="657">Abbiamo trovato che l'Enc-Dec o l'Enc-PTR può essere migliorato con l'addestramento in una miscela di varie lingue.</sample>
    <sample id="658">Abbiamo trovato che la maggior parte delle lingue naturali principali può ottenere un miglioramento del presto, eccetto che per l'inglese, il cui presto diminuisce in 7 set di dati e aumenta solo in 3 set di dati. Questo è noto come "Curso della Multilinguistica".</sample>
    <sample id="659">L'analisi dell'addestramento multilingue. Valutiamo su mT5 e XLM-R + PTR nel contesto multilingue. La maggior parte delle principali NL può ottenere un miglioramento, eccetto che il rendimento dell'inglese cala in 7 dataset e aumenta in 3 dataset. Questo è noto come "Curse of Multilinguality".</sample>
    <sample id="660">Hanno anche confrontato il gap di prestazione del trasferimento tra lingue.</sample>
    <sample id="661">Nella figura, la linea blu rappresenta la trasformazione di pochi colpi tra lingue diverse, la linea arancione rappresenta la trasformazione di zero colpi tra lingue diverse, mentre la linea verde rappresenta il contesto monolingue.</sample>
    <sample id="662">Abbiamo trovato che, confrontando la linea verde e la linea arancione, per il setting zero-shot, il gap di prestazione della trasformazione interlingua è significativo. E confrontando la linea blu e la linea arancione, abbiamo trovato che, per il setting few-shot, il gap di trasferimento si riduce rapidamente.</sample>
    <sample id="663">Abbiamo trovato anche altri interessanti risultati, ad esempio l'encoder-decoder supera il lavoro precedente o raggiunge risultati paragonabili. La pre-adesione all'inglese può notevolmente migliorare le prestazioni del few-shot su lingue di destinazione.</sample>
    <sample id="664">I modelli multilingue come Codex e BLOOM sono ancora insufficienti per le compiti di parsing semantico interlingue.</sample>
    <sample id="665">Concludiamo costruendo XSemPLR, un benchmark unificato per la parsing semantica interlinguistica con diverse lingue naturali e rappresentazioni di significato.</sample>
    <sample id="666">Conclusione - Abbiamo costruito XSemPLR, un benchmark unificato per la parsing semantica interlinguistica con diverse lingue naturali e rappresentazioni di significato. - Abbiamo condotto una studio di benchmarking comprensivo su tre tipi rappresentativi di modelli di linguaggio multilingue. - I nostri risultati mostrano che mT5 con addestramento monolingue dà i migliori risultati, mentre notevolmente i modelli multilingue LLM sono ancora insufficienti per eseguire compiti di parsing semantico interlinguistico. Inoltre, la differenza di prestazioni tra addestramento monolingue e apprendimento a trasferimento interlingue rimane ancora significativa. Link - Benvenuti a visitare il nostro articolo e il codice! - Link al documento: https://arxiv.org/pdf/2306.04085.pdf - Link al codice: https://github.com/psunlpgroup/xsemplr</sample>
    <sample id="667">I lavori connessi sono: Parameter-based watermark, Lexical watermark, Backdoor-based watermark e Adversarial-based watermark. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="668">No, gli LLM multilingue come Codex e Bloom sono ancora insufficienti per il CLSP.</sample>
    <sample id="695">Induce l'alignment durante l'addestramento.</sample>
    <sample id="696">L'equità di un modello NLP a valle si riferisce a come il modello gestisca le informazioni e le interazioni con gli utenti. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="697">Yanis Labrak.</sample>
    <sample id="698">Koustuv Sinha. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="699">Il nome della relatrice è Myra Cheng.</sample>
    <sample id="700">Il tropicalismo indica le caratteristiche come "vibrante" e "curvaceous" per le donne latine. Questo è un tipo di stereotipo che le associa a qualcosa di esotico e tropicale. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target attraverso narrativi essenzializzanti, definendo questi gruppi solo per la loro identità, e attraverso rappresentazioni positive perniciose.</sample>
    <sample id="702">P-CXMI.</sample>
    <sample id="703">DrBERT è una versione da zero con 7 GB di NACHOS, mentre ChuBERT è una versione da zero con 4 GB di NBDW small.</sample>
    <sample id="751">Tre.</sample>
    <sample id="752">L'iterative transfer learning aggiorna il modello addestrando su un set di dati più recente raccolto.</sample>
    <sample id="753">L'obiettivo del set di dati è comprendere il linguaggio degli utenti quando fanno una scelta.</sample>
    <sample id="754">Non c'è informazione sufficiente nel contenuto fornito per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="755">There are three authors involved in the article.</sample>
    <sample id="756">Non c'è informazione su quanti annotatori sono stati impiegati per creare il set di dati iniziale.</sample>
    <sample id="757">Sebastin Santy e Ronan Le Bras sono dell'Allen Institute for AI, Katharina Reinecke e Maarten Sap sono dell'University of Washington, e Jenny T. Liang è della Carnegie Mellon University.</sample>
    <sample id="758">L'esempio è "I saw Bart and Lisa".</sample>
    <sample id="759">Non c'è informazione sufficiente nel contenuto fornito per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="760">Perché i modelli stanno avendo finestre di contesto sempre più lunghe.</sample>
    <sample id="761">Sì.</sample>
    <sample id="762">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="763">Non c'è informazione su quali metriche di MT siano state utilizzate per la valutazione. Potresti cercare di trovare ulteriori dettagli in altre fonti.</sample>
    <sample id="764">Non c'è informazione specifica nel contenuto dato che parla di regressione nella generalizzazione e tipi di NER. Potresti fornire ulteriori dettagli?</sample>
    <sample id="765">Well, positional information in NLP is important because it helps the model understand the context better. You know, like the order of words matters a lot. For example, in a sentence, the position of a word can change its meaning. So, it's crucial for tasks like machine translation, text summarization, and question answering. It allows the model to make more accurate predictions. If you want to know more about specific applications, just let me know.</sample>
    <sample id="766">Non so. Potresti cercare di trovare informazioni su questo argomento.</sample>
    <sample id="767">Fanno ricorso al modello RoBERTA-base + classifier head.</sample>
    <sample id="768">Non so. Potresti cercare su internet o chiedere a qualcuno che ne sa di più.</sample>
    <sample id="769">Tre.</sample>
    <sample id="770">Non so qual è il guadagno del metodo proposto rispetto al metodo di riferimento. Potresti dare più dettagli o informazioni?</sample>
    <sample id="771">Shuheng Liu.</sample>
    <sample id="772">Sì.</sample>
    <sample id="773">Due modelli più piccoli vengono utilizzati nell'articolo.</sample>
    <sample id="774">OFA viene utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="833">Google Translate.</sample>
    <sample id="834">Stony Brook University, Human Language Analysis Beings.</sample>
    <sample id="835">Non c'è informazione sull'articolo su quali coppie linguistiche sono state analizzate.</sample>
    <sample id="836">Shangbin Feng.</sample>
    <sample id="837">Hanno studiato due modelli. Uno è stato finetuned per produrre semplificazioni a livello di documento utilizzando un modello long-mBART, e l'altro è stato finetuned per produrre semplificazioni a livello di frase utilizzando un modello long-MBERT basato su un modello base.</sample>
    <sample id="838">Per l'addestramento vengono utilizzate 53 attività e per il test 62 attività. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="839">Tre.</sample>
    <sample id="840">I test sono stati effettuati sui set di dati AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="876">NACHOS è un dataset di dati medici raccolti dal web.</sample>
    <sample id="877">Il nome del relatore è David Vilar Torres.</sample>
    <sample id="878">La strategia del prompting ha un grande impatto sui risultati. 516 su 1000 frasi mostrano una differenza di più di 1 punto BLEURT, e la differenza può arrivare fino a 40 punti BLEURT.</sample>
    <sample id="879">Patrick Fernandes e Kayo Yin sono affiliati all'Institute for Language Technologies, Carnegie Mellon University. Emmy Liu, André F. T. Martins e Graham Neubig sono affiliati a Técnico Lisboa, BAIR, Berkeley Artificial Intelligence Research e Unbabel.</sample>
    <sample id="880">Non c'è informazione su istruzioni scritte da esperti nel contenuto fornito. Potresti avere bisogno di fornire ulteriori dettagli o contesto?</sample>
    <sample id="881">Proppongono un test di risoluzione di coreferenza per testare l'abilità di utilizzare informazioni provenienti da diverse fonti.</sample>
    <sample id="882">Ciao a tutti. Mi chiamo David Vilar Torres e darò un breve riepilogo del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro congiunto con i miei colleghi di Google Translate.</sample>
    <sample id="883">PaLM è un modello di linguaggio a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testi, complessivamente 780 miliardi di token.</sample>
    <sample id="884">Nella sua applicazione, ha raggiunto lo stato dell'arte in centinaia di compiti di LMU e generazione.</sample>
    <sample id="885">Nel lavoro presentiamo lo studio sistematico del primo modello di linguaggio di grandi dimensioni per la traduzione automatica.</sample>
    <sample id="886">Abbiamo valutato la capacità di traduzione dei modelli di ricerca utilizzando le migliori pratiche della comunità di MT. Questo implica l'uso dei set di test più recenti per evitare che i dati di test si sovrappongano con i dati di addestramento del modello di linguaggio.</sample>
    <sample id="887">Ecco la traduzione in italiano: "Ecco il contenuto inglese tradotto in italiano."</sample>
    <sample id="888">Il contenuto inglese è stato tradotto in italiano. Se hai bisogno di ulteriori informazioni o di aiuto con qualcos'altro, fammi sapere!</sample>
    <sample id="889">Le prompt hanno un grande impatto sulla qualità della traduzione. Seleziona due prompt casuali per ogni frase. Calcola BLEURT per ogni coppia frase-prompt. La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di 1 punto BLEURT. La differenza può arrivare fino a 40 punti BLEURT!</sample>
    <sample id="890">La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di 1 punto BLEURT.</sample>
    <sample id="891">I prompt hanno un grande impatto sulla qualità della traduzione. Per ogni frase, seleziona due prompt casuali. Calcola il BLEURT per ogni coppia frase-prompt. La maggior parte delle frasi, 516 su 1000, mostra una differenza di più di 1 punto BLEURT. La differenza può arrivare fino a 40 punti BLEURT!</sample>
    <sample id="892">In our experiments, we settled for a 5-shot prompting strategy, where we just mark each sentence that we provide to the system with its language, its English.</sample>
    <sample id="893">Certo. "A un passante si è presentato all'ufficio di polizia e ha informato i poliziotti, che sono arrivati con diversi agenti di polizia." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="894">Abbiamo visto che la forma effettiva del prompt non ha un grande influsso nel caso del prompt a 5 punti.</sample>
    <sample id="895">Ecco la traduzione in italiano: "È cruciale per il prompt a 0 e 1 shot, e quando passiamo al prompt a 5 shot, non c'è quasi alcuna differenza nella forma effettiva del prompt." Se hai bisogno di chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="896">Un passante ha allarmato la polizia, che è arrivata con diversi agenti di polizia.</sample>
    <sample id="897">I risultati sperimentali riassunti sono che la qualità dell'esempio è più importante della somiglianza con la frase di origine.</sample>
    <sample id="898">Quindi è importante selezionare esempi da traduzioni di alta qualità. In particolare, abbiamo comparato la selezione di prompt dal dataset di addestramento delle valutazioni WMT o dal dataset di test.</sample>
    <sample id="899">I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento. Sono più ordinati e i risultati sono migliori, quindi una prestazione migliore utilizzando i dati di sviluppo.</sample>
    <sample id="900">Tuttavia, i sistemi SOTA specializzati hanno un vantaggio sostanziale rispetto alle traduzioni PaLM, ma PaLM si avvicina abbastanza a un sistema commerciale. Nella nostra situazione, abbiamo scelto di sovrapporre con Google Translate.</sample>
    <sample id="901">I risultati dell'analisi umana che abbiamo effettuato utilizzando il framework MQM sono che la fluidezza di PaLM è paragonabile a quella dei sistemi state-of-the-art, ma la principale differenza deriva dalla precisione.</sample>
    <sample id="902">I risultati sperimentali: - La qualità dell'esempio è più importante della somiglianza con la frase di origine. - I sistemi SOTA specializzati hanno un vantaggio sostanziale. - PaLM è vicino a Google Translate. Intuizioni da MQM: - La fluenza di PaLM è paragonabile a quella dei SOTA. - Le punteggia di accuratezza sono generalmente più basse. - Dominato da "Accuracy/Omission". - "Style/Awkward" è generalmente più basso per PaLM. In particolare, le omission errors sono le più comuni.</sample>
    <sample id="903">Pare che PaLM scelga di produrre una traduzione di miglior qualità, a volte rinunciando a parti della frase di partenza che non sono pertinenti nella traduzione.</sample>
    <sample id="904">Tuttavia, la categoria "Stile/Accattivante" per PaLM è inferiore rispetto a quella degli sistemi state dell'arte, che è un altro segnale.</sample>
    <sample id="905">Il contenuto inglese dice: "PaLM fornisce un output veramente fluido, ma ancora con alcuni problemi di accuratezza."</sample>
    <sample id="906">Ecco la traduzione in italiano: "Ecco tutto per questa breve panoramica. Per ulteriori dettagli, vi prego di venire alla presentazione completa del documento. Vi ringrazio molto." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="907">Ciao, sono Dawei, un dottorando all'Università di Saarland in Germania. Nella video vorrei presentare il nostro lavoro recente "Più debole di quanto pensi: un'analisi critica dell'apprendimento supervisionato debole".</sample>
    <sample id="908">Questa è una collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="909">Perché apprendimento supervisionato debole?• L'apprendimento supervisionato debole allevia il blocco di annotazione. Ma i etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • Apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="910">Perché apprendere con supervisione debole?• La supervisione debole allevia il blocco dell'annotazione. Ma i etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • Apprendimento supervisionato debole, WSL • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="911">Perché apprendere con supervisione debole?• La supervisione debole allevia il blocco dell'annotazione. • Ma le etichette deboli sono rumorose! - La memorizzazione del rumore danneggia la generalizzazione. • Apprendimento supervisionato debole, WSL - Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="912">Se addestriamo direttamente reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore etichettato e non a generalizzare.</sample>
    <sample id="913">Perché apprendere supervisionato debole?• La supervisione debole allevia il blocco di annotazione. Ma i etichette deboli sono rumorose! • La memorizzazione del rumore danneggia la generalizzazione. • Apprendimento supervisionato debole, WSL, • Addestra modelli che generalizzano bene nonostante siano addestrati su dati rumorosi.</sample>
    <sample id="914">Nelle recenti ricerche sul WSL, WSL sta per apprendimento supervisionato debole. Un索赔 comune è che le persone affermano di addestrare i modelli solo su dati etichettati debole e ottenere prestazioni elevate su set di test puliti.</sample>
    <sample id="915">Un affermazione comune nelle recenti ricerche sul WSL è: "Addestriamo modelli solo su dati supervisionati debolmente e otteniamo un'accuratezza del XX%." Tecnicamente questa affermazione non è errata, ma c'è un'eccezione.</sample>
    <sample id="916">Un'asserzione comune nelle recenti ricerche WSL è che si addestrano modelli solo su dati supervisionati debolmente e raggiungono un'accuratezza del XX%.</sample>
    <sample id="917">Un contenuto comune negli ultimi lavori WSL è: "Addestriamo modelli solo su dati supervisionati debolmente e otteniamo un accuracy del XX%."</sample>
    <sample id="918">Le domande di ricerca sono: 1. È necessario il dati di validazione pulito per WSL? Oppure possiamo forse utilizzare un set di validazione rumoroso?</sample>
    <sample id="919">Secondo te, se i dati puliti sono necessari o se sono obbligatori per che WSL funzioni, allora quante campioni puliti abbiamo bisogno? Infine, dobbiamo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti.</sample>
    <sample id="921">Prima di tutto, abbiamo trovato che, interessantemente, i recenti metodi di etichettatura debole effettivamente richiedono campioni di etichette debole pulite per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci sarà una significativa diminuzione del presto, come mostrato in questa figura. Se non ci sono esempi di validazione pulita, allora i modelli addestrati non possono generalizzare oltre le etichette deboli originali.</sample>
    <sample id="923">Significato che l'addestramento è inutile.</sample>
    <sample id="924">Questo indica che le approcci WSL effettivamente richiedono dati etichettati in modo pulito per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere una prestazione migliore, come mostrato nella figura a sinistra.</sample>
    <sample id="926">Generalmente, si necessitano solo 20 campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma non è la fine della storia! Perché se decidiamo di accedere a campioni puliti, allora l'addestramento su di essi direttamente raggiungerà anche prestazioni migliori.</sample>
    <sample id="928">La figura rossa mostra la differenza di prestazione tra le approcci di ottimizzazione diretta, che sono applicate direttamente sui dati puliti, e le approcci WSL, che usano i dati puliti solo per la validazione.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo 10 campioni per classe, la finetuning diretta inizia a superare le approcci WSL.</sample>
    <sample id="930">Infine, l'incremento di prestazioni dichiarato in precedenti approcci WSL può essere facilmente ottenuto consentendo il continuo raffinamento su campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dai grafici, il modello di validazione FTW inizialmente sottoproduce rispetto a metodi più complessi come COSINE.</sample>
    <sample id="932">Tuttavia, se continuiamo a continuare l'addestramento su campioni puliti, allora FTw si comporta altrettanto bene degli altri metodi.</sample>
    <sample id="933">RQ3 Rilevamenti principali N = 10 campioni puliti per classe N = 30 campioni puliti per classe → Continuare l'addestramento fine, CFT, elimina le differenze di prestazione tra le approcci WSL. → Non c'è bisogno di utilizzare metodi WSL complicati, FTw funziona altrettanto bene. Cosí, in pratica, non c'è motivo di scegliere metodi WSL più complessi, che richiedono più tempo di calcolo e spazio disco.</sample>
    <sample id="934">ConclusioneApprocci recenti WSL: - Richiedono campioni puliti. - Sottostimano la loro praticità.Noi consigli: - Riferire i criteri di selezione del modello. - Usare approcci di apprendimento a poche immagini come basi. - Sempre applicare l'ottimizzazione continua (CFT).Per riassumere, abbiamo mostrato che gli approcci recenti WSL richiedono campioni puliti manualmente annotati per funzionare correttamente. La loro performance e praticità sono pesantemente sottostimate.</sample>
    <sample id="935">ConclusioneApprocci recenti WSL: - Richiedono campioni puliti. - Sottostimano la loro praticabilità. Recommandazioni: - Riferire i criteri di selezione del modello. - Usare approcci di apprendimento a pochi esempi come basi. - Sempre applicare l'ottimizzazione continua (CFT).</sample>
    <sample id="936">ConclusioneApprocci recenti WSL: - Richiedono campioni puliti. - Sottostimano la loro praticità. Recommandazioni: - Riferire i criteri di selezione del modello. - Usare approcci di apprendimento a poche istanze come basi. - Sempre applicare l'ottimizzazione continua (CFT).</sample>
    <sample id="937">Secondo me, gli approcci WSL dovrebbero essere confrontati con le basi di apprendimento a poche istanze, entrambi che lavorano su campioni puliti. Terzo, l'ottimizzazione continua è una basa semplice ma forte che dovrebbe essere considerata nel lavoro futuro in WSL.</sample>
    <sample id="938">ConclusioneApprocci recenti WSL: - Richiedono campioni puliti. - Sottostimano la loro praticità.Noi consigli: - Riferire i criteri di selezione del modello. - Usare approcci di apprendimento a poche sfide come basi. - Sempre applicare l'ottimizzazione continua (CFT).Infine, abbiamo reso open source il nostro codice. Potete trovarlo tramite il codice QR in questa slide. Vi preghiamo di controllarlo. Grazie e godetevi la conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo sono l'uso di giudici umani per selezionare quale conversazione è migliore o per valutarle con una scala Likert.</sample>
    <sample id="940">There are five authors involved in the article. If you want to know more about them or the article itself, feel free to ask.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che i giudici decidono casi in tribunali. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="942">Sì, il codice è disponibile su GitHub, nel repository mpoemsl/kitmus.</sample>
    <sample id="943">No, non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="944">Nel dominio accettabile, le frasi sono state perturbate in modo da mantenere la struttura rilevante.</sample>
    <sample id="945">Evaluare le diverse dimensioni della qualità del dialogo.</sample>
    <sample id="946">Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie. Wenjun Peng and Jingwei Yi are from the University of Science and Technology of China, Fangzhao Wu and Shangxi Wu are from Microsoft Research Asia, Bin Zhu and Lingjuan Lyu are from Beijing Jiaotong University, Binxing Jiao and Tong Xu are from Sony AI, and Guangzhong Sun and Xing Xie are from Microsoft STC Asia.</sample>
    <sample id="947">La forma del prompting si rivela importante nei casi di zero e uno shot prompting.</sample>
    <sample id="978">Hanno valutato BART-FID-RAG, Blender2, Emora e Blender-Decode. Se hai altre domande, sentiti libero di chiedere!</sample>
    <sample id="979">There are 8 authors involved in the article. If you want to know more about them, feel free to ask.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="981">Ci sono sette autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="982">Vasudha Varadarajan.</sample>
    <sample id="983">I autori dell'articolo sono affiliati all'Institute of Computer Science, Polish Academy of Sciences, e all'University of Warsaw.</sample>
    <sample id="1021">I most common errors are omission errors.</sample>
    <sample id="1022">Ciao, sono James Finch e sono Sarah Finch. Oggi vi parleremo di ABC eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="1023">Questo lavoro è stato fatto dal laboratorio di NLP dell'Università di Emory, diretto dal professor Jinho Choi all'Università di Emory, in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Ecco la traduzione in italiano: "Supponiamo che tu abbia appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato dell'arte attuale." Se hai bisogno di ulteriori informazioni o hai altre domande, fammi sapere!</sample>
    <sample id="1025">La pratica comune è di utilizzare l'valutazione umana, come ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni globali della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare diverse dimensioni della qualità del chat per comprendere le forze e le debolezze del modello a un livello più dettagliato.</sample>
    <sample id="1027">Un approccio consiste nel chiedere giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi di scala comparativa o Likert esistenti.</sample>
    <sample id="1028">Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'valutazione dimensionale del dialogo.</sample>
    <sample id="1029">La nostra approccio cerca di ridurre la soggettività dell'valutazione umana annotando esplicitamente se ognuna delle risposte del modello esprime determinati comportamenti, come rispondere con informazioni irrellevanti o contraddizione.</sample>
    <sample id="1030">Chiamiamo questo approccio "Annotating Behaviors in Chat" o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo comprensivo i comportamenti dei modelli di chat che sono stati suggeriti di influenzare la qualità dei chat in recenti studi.</sample>
    <sample id="1031">L'ABC-Eval è in grado di misurare le tassi con cui i modelli di chat commetteranno diversi errori tematici.</sample>
    <sample id="1032">Per esempio, ABC-Eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante.</sample>
    <sample id="1033">Contraddice se stesso o il partner, allucina fatti errati o viola il buon senso e quando il modello riesce o fallisce a mostrare empatia.</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è il più efficace, abbiamo selezionato quattro modelli di chat all'avanguardia e li abbiamo valutati su 100 conversazioni umano-bots per modello utilizzando ABC-Eval.</sample>
    <sample id="1035">Per la comparazione, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni Likert a livello di turno, valutazioni Likert a livello di dialogo e confronti a livello di dialogo a coppie.</sample>
    <sample id="1036">Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti di dialogo più comunemente misurati, poiché questo è il metodo standard per valutare i modelli di chat su diverse dimensioni.</sample>
    <sample id="1037">Dalle nostre analisi dei risultati di valutazione, abbiamo trovato che le etichette di comportamento ABC-Eval sono in generale più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'inter-annotator agreement su 100 conversazioni doppilabelate.</sample>
    <sample id="1038">Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto ai metrici prodotti dai metodi esistenti, come dimostrato dall'analisi di regressione lineare semplice.</sample>
    <sample id="1039">Per esempio, si può vedere come misurare la proporzione delle giri con contraddizioni di se stessi e partner spieghi il 5% e il 10% della qualità della conversazione rispettivamente, mentre le punte di consistenza Likert average spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare passo dopo passo.</sample>
    <sample id="1041">Puoi vedere come la combinazione di tutti i metrici ABC-Eval spiega oltre il 25% della qualità della conversazione e, se rimuovi i metrici uno alla volta, la maggior parte di essi porta a perdere una buona quantità di informazioni sulla qualità.</sample>
    <sample id="1042">D'altra parte, la combinazione di tutte le metriche Likert a livello di turno spiega molto meno la qualità e meno di queste metriche portano informazioni uniche.</sample>
    <sample id="1043">Questi metriche ABC-Eval affidabili e distintive permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto a metodi precedenti.</sample>
    <sample id="1044">Puoi vedere che nei risultati dell'esperimento ci sono ancora diverse sfide che rimangono e sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni di buon senso in circa il 20% delle loro risposte.</sample>
    <sample id="1045">Producono informazioni irrellevanti in circa il 15% delle risposte e si contraddicono o si contraddicono con il partner circa il 10% del tempo.</sample>
    <sample id="1046">Con il rapido ritmo di miglioramento nel campo, molti di questi tassi di errore potrebbero vedere una diminuzione in nuovi modelli rilasciati dal momento che la nostra valutazione è stata condotta. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione e ci guardiamo avanti per vedere come l'intelligenza artificiale conversazionale progredisca nei prossimi mesi e anni. Grazie per averci ascoltato.</sample>
    <sample id="1048">I autori sono affiliati all'Emory NLP Research Lab all'Emory University e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1049">CFT significa Continuous Fine-tuning. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="1050">Ci sono sette autori coinvolti nell'articolo. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1051">Ciao, il mio nome è Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un'indagine multilingue guidata da dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="1052">Dobbiamo liberarci di quel mulo.</sample>
    <sample id="1053">Se la frase precedente era "Things could start to get dangerous if the ministers find out." allora "mole" si riferisce a un agente segreto. Ma se la frase precedente era "Could it be anything serious, Doctor?" allora "mole" si riferisce a una macchia di pelle.</sample>
    <sample id="1054">Potrebbe essere qualcosa di serio, dottore? Dobbiamo liberarci di quella chiazza.</sample>
    <sample id="1055">Evaluare la traduzione dipendente dal contesto è difficile. Tuttavia, valutare quanto bene i modelli gestiscono casi come questi è abbastanza difficile. In primo luogo, perché solo una piccola porzione di traduzioni dipende dal contesto, che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni.</sample>
    <sample id="1056">Evaluare la traduzione dipendente dal contesto è difficile. Solo una piccola parte di parole dipende dal contesto. I metodi esistenti supportano fenomeni di discorso e lingue limitati. Alcune persone hanno suggerito una valutazione mirata sulle traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, poiché solitamente si affidano a conoscenze di dominio e cura umana.</sample>
    <sample id="1057">Nel lavoro, cerchiamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesto, e poi, quanto bene gestiscono questi casi i modelli.</sample>
    <sample id="1058">Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione.</sample>
    <sample id="1059">Nel lavoro precedente, abbiamo introdotto il CXMI come misura dell'utilizzo del contesto da parte dei modelli di traduzione automatica. Questo è stato fatto misurando quanta informazione il contesto C fornisce sulla traduzione Y, dato il testo di origine X.</sample>
    <sample id="1060">CMI: misura quanto utilizzano i modelli di traduzione con contesto dati un corpus. Puoi pensare a CMI come all'informazione guadagnata dal dare contesto al modello.</sample>
    <sample id="1061">In questo lavoro estendiamo CXMI a P-CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo pensare a parole con alto P-CXMI come quelle che richiedono contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo le parole con un alto PSMI per cercare di trovare pattern tra queste parole.</sample>
    <sample id="1063">Eseguiamo l'analisi su trascrizioni di TED Talks tradotte dall'inglese in 14 diverse lingue.</sample>
    <sample id="1064">Eseguiamo l'analisi a tre livelli diversi. Innanzitutto, guardiamo alle etichette di parto di frase che hanno un alto P-CXMI.</sample>
    <sample id="1065">Ecco la traduzione in italiano: "Questo ci permette di trovare, ad esempio, i pronomi duali in arabo che hanno relativamente un alto P-CXMI. E questo può essere spiegato perché l'inglese non ha pronomi duali, quindi si ha bisogno di contesto per determinare se un pronome è dual quando si traduce in arabo."</sample>
    <sample id="1066">Ecco la traduzione in italiano: "Inoltre, troviamo che certi linguaggi richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Poi analizziamo gli elementi di vocabolario con P-CXMI alto, calcolato su tutte le sue diverse occorrenze." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="1067">Ecco la traduzione in italiano: "E questo ci aiuta a identificare casi come questo, dove in cinese si ha bisogno di contesto per tradurre i nomi propri in modo da assicurarsi di utilizzare la stessa traduzione all'interno del documento." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="1068">Ecco la traduzione in italiano: "E inoltre, troviamo che il contesto supporta la traduzione nella giusta formalità." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="1069">Ecco la traduzione in italiano: "E infine analizziamo i diversi token individuali con un alto P-CXMI. Questo ci permette di identificare fenomeni che non possono essere veramente catturati dalla parola in sé, ma sono piuttosto espressi nella struttura della frase, come la risoluzione dell'ellissi." Se hai bisogno di ulteriori chiarimenti o traduzioni, fammi sapere!</sample>
    <sample id="1070">Quindi ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento.</sample>
    <sample id="1071">Per ciascuno dei cinque fenomeni di discorso identificati, creiamo tagger per identificare automaticamente le parole che appartengono al fenomeno e chiamiamo il nostro tagger il Multilingue Discorsuale Consapevole, MuDA, tagger.</sample>
    <sample id="1072">Possiamo notare che le diverse lingue hanno proporzioni diverse di questi fenomeni discorsivi.</sample>
    <sample id="1073">Poi utilizziamo il tagger MuDA applicando il tagger al corpus parallelo che vogliamo utilizzare per l'valutazione. Applichiamo i nostri metriche di traduzione preferite sugli esempi dipendenti dal contesto che il tagger MuDA ha identificato.</sample>
    <sample id="1074">Ecco la traduzione in italiano: "E infine, utilizziamo il nostro benchmark, insieme a altri metriche, per valutare diversi modelli di traduzione automatica a livello di documento." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="1075">Prima di tutto, quando utilizziamo metriche a livello di corpus, per BLEU, troviamo che i modelli agnostici del contesto hanno il miglior rendimento.</sample>
    <sample id="1076">Ma se usiamo Comet con il contesto, le modelli performano meglio. E se usiamo F-measure, le modelli con e senza contesto hanno prestazioni comparabili.</sample>
    <sample id="1077">Questo dimostra ancora una volta che è difficile determinare il miglior sistema di traduzione a livello di documento se si utilizzano solo metriche a livello di corpus.</sample>
    <sample id="1078">Ora utilizziamo il benchmark MuDA per valutare i modelli e troviamo che i modelli con consapevolezza del contesto sono significativamente più accurati dei modelli che non utilizzano il contesto per certi fenomeni discorsivi, come formalità e coesione lessicale.</sample>
    <sample id="1079">I modelli a conoscenza del contesto performano significativamente meglio su alcune fenomeni. ✔: Formalità, coesione lessicale ❌ Ellissi, pronomi, forma del verbo. Ma questi modelli non sono molto meglio dei modelli che non utilizzano il contesto su altri fenomeni come ellissi, pronomi e forma del verbo. Questo suggerisce dove sarebbe necessario fare più progresso per la traduzione a livello di documento.</sample>
    <sample id="1080">Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento.</sample>
    <sample id="1081">Ecco la traduzione in italiano: "Per riassumere, abbiamo effettuato un'analisi guidata da dati su 14 coppie di lingue per identificare quando le traduzioni richiedono un contesto." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="1082">Ecco la traduzione in italiano: "E poi utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento, che ci aiuterà a identificare quali fenomeni discorsivi i modelli possono gestire bene o meno, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="1083">Riassunto

- Identificare fenomeni discorsivi in modo sistematico senza conoscenze linguistiche precedenti

- Benchmark per la traduzione automatica di documenti indipendente dal dataset

Grazie per l'attenzione. Ci vediamo a Toronto.</sample>
    <sample id="1084">Yusen Zhang.</sample>
    <sample id="1121">Il nuovo metodo è privo di un nome.</sample>
    <sample id="1122">Il metodo di "parole contrassegnate" è una tecnica per identificare le parole che distinguono i gruppi contrassegnati dai gruppi non contrassegnati.</sample>
    <sample id="1123">Shangbin Feng è affiliato alla Paul G. Allen School dell'University of Washington, Chan Young Park è affiliato a UW NLP, Yuhan Liu è affiliato al Language Technologies Institute di Carnegie Mellon University e Yulia Tsvetkov è affiliata alla Beijing University of Posts and Telecommunications.</sample>
    <sample id="1124">Bouquet/Stanford.</sample>
    <sample id="1125">Il nome del relatore è James Finch.</sample>
    <sample id="1126">Quattro. C'è Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1127">BLiMP, SyntaxGym, CrowS.</sample>
    <sample id="1161">FTw, BOND, COSINE, MLC, L2R.</sample>
    <sample id="1162">Il modello viene valutato su 11 attività.</sample>
    <sample id="1226">CamemBERT viene inizialmente addestrato su 4GB di dati.</sample>
    <sample id="1227">Adam Przepiórkowski. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1228">I risultati dell'esperimento di ritraining o di continuare a pretrainare modelli con dati più recenti hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">Perché i token non sono ordinati dopo il primo step.</sample>
    <sample id="1270">Perché non sappiamo se i positivi stereotipi sono dovuti a un'overly excessive value alignment o altri metodi anti-stereotipanti che stanno producendo questi pattern dannosi.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono le frasi che non seguono le regole grammaticali o che sono stereotipiche.</sample>
    <sample id="1272">Hanno utilizzato le metriche F1 per NER, CLS, Specialties, MUSCA-DET, ESSAI, CAS, POS, Hamming, EMR, FrenchMedMCQA, QUASERO-EMEA e QUASERO-MEDLINE.</sample>
    <sample id="1273">Krippendorff's Alpha.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri perché si concentra sulla generazione di istruzioni in contesto per compiti multimediali, mentre gli altri sono più focalizzati sui compiti linguistici solo. Ehm… e c'è anche una disuguaglianza nella disponibilità dei set di dati istruzioni tra NLP e multimediali. Se hai altre domande su questo, sentiti libero di chiedere.</sample>
    <sample id="1277">There are three authors involved in the article. If you want to know more about them or the article itself, feel free to ask.</sample>
    <sample id="1278">Non c'è una definizione di coordinazione binaria nella tua domanda. Potresti fornire più dettagli o chiarire il concetto che stai cercando?</sample>
    <sample id="1279">Non c'è informazione su quanto tempo sono stati utilizzati i prompt in questo studio. Potresti avere altre domande su questo studio?</sample>
    <sample id="1280">I risultati suggeriscono che i modelli più piccoli possono superare i modelli più grandi quando correttamente addestrati su dataset adatti.</sample>
    <sample id="1281">Ciao, sono Yanis Labrak e presenterò il nostro lavoro su DrBERT, un modello pre-allenato robusto in francese per i campi biomedici e clinici.</sample>
    <sample id="1282">Nella presentazione iniziamo parlando del modellare linguistico nel campo della sanità. Poi presenteremo le principali contribuzioni del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese, chiamato Dr.BERT, basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici web crawlati.</sample>
    <sample id="1284">Abbiamo anche introdotto una comparazione di modelli con diversi set di pre-allenamento e fonti di dati. Poi abbiamo presentato i risultati su 11 compiti di apprendimento automatico biomedico e clinico in francese.</sample>
    <sample id="1285">Infine, concludiamo gli esperimenti e ti forniamo dettagli ulteriori su come accedere ai modelli.</sample>
    <sample id="1286">Dal suo lancio nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un notevole miglioramento di prestazioni rispetto a metodi statici e contestuali storici come Word2Vec, FastText o ANMO.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molte lingue minoritarie, come il francese con CamemBERT e anche nel campo biomedico con PubMedBERT e BioBERT e nel campo clinico con ClinicalBERT, ma soprattutto in inglese.</sample>
    <sample id="1288">I modelli specializzati per altre lingue sono scarsi e spesso si basano su un pre-allenamento continuo a causa della mancanza di dati di dominio.</sample>
    <sample id="1289">Ma per il francese non c'era un modello open source per il campo biomedicale.</sample>
    <sample id="1290">Ecco la traduzione in italiano: "Quindi ci chiediamo qual è la fonte di dati più appropriata per una vasta gamma di utilizzo e questi dati corali sono buoni sostituti per i dati clinici." Se hai bisogno di ulteriori chiarimenti o hai altre domande, non esitare a chiedere.</sample>
    <sample id="1291">Per rispondere a questa domanda, abbiamo comparato DrBERT con il nostro ChuBERT modello, che si basa su dati anonimizzati ottenuti dal data warehouse dell'ospedale universitario di Nantes.</sample>
    <sample id="1292">Dopo tutto, ci chiediamo quanta data serva per addestrare un modello specializzato su dati francesi. È 4 GB, 13 GB o più?</sample>
    <sample id="1293">Prima di tutto, abbiamo addestrato e comparato quattro modelli da zero. La prima versione di DrBERT con 7 GB di NACHOS, la seconda versione con 4 GB di NACHOS.</sample>
    <sample id="1294">Prima versione di Schubert, un modello clinico con 4 GB di frasi estratte da ClinConsortium e ultima versione di Schubert con una mistura di 4 GB di frasi di NACHOS e 4 GB di ClinConsortium.</sample>
    <sample id="1295">Confronto di strategie di pre-allenamento e fonti di dati. - Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili. * NACHOS: Un dataset open-source di 1,1 miliardi di parole di dati omogenei tratti da diversi domini medici, nature e stili. * NBDW: Un dataset privato di frasi tratti da 1,7 milioni di record medici anagrafici estratti dal data warehouse dell'ospedale universitario di Nantes. - Confronto di strategie di apprendimento. * All'inizio con costruzione completa del modello. * Pre-allenamento continuo utilizzando un modello pre-allenato esistente (qui, CamemBERT, un modello generico francese, e PubMedBERT, un modello medico basato su inglese). In aggiunta a questo confronto, introduciamo tre modelli addestrati con pre-allenamento continuo per analizzare l'impatto delle strategie di pre-allenamento.</sample>
    <sample id="1296">C'è un altro basato su Camembert, addestrato questa volta su un subset da 4 GB di NACHOS.</sample>
    <sample id="1297">Ecco il contenuto tradotto in italiano: "Ecco infine un modello basato sull'inglese, PubMedBERT, addestrato su un subset di 4 GB di NACHOS. In totale abbiamo sette modelli." Se hai bisogno di ulteriori informazioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="1298">Ecco la traduzione in italiano: "Per valutare tutti e sette i modelli, abbiamo raccolto dati pubblici e privati da diversi task, come riconoscimento di entità, classificazione, taggatura di parole e risposta a domande." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="1299">Ecco la traduzione in italiano: "Questi modelli sono confrontati con sei modelli di riferimento, che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT v1.1 e ClinicalBERT."</sample>
    <sample id="1300">L'analisi delle prestazioni di 13 modelli su 11 compiti, sia pubblici che privati, mostra che i nostri modelli finetunati ottengono risultati di punta su quasi tutti i compiti. I modelli performono meglio sui compiti con dati dello stesso tipo di quelli su cui sono stati addestrati.</sample>
    <sample id="1301">Ecco la traduzione in italiano: "Tuttavia, possiamo ottenere i dati da... possiamo osservare che i dati da diverse fonti sembrano essere più versatile. Abbiamo anche osservato che l'uso di più dati si traduce in una migliore prestazione."</sample>
    <sample id="1302">In generale, la pretraining da zero sembra ottenere prestazioni superiori per la maggior parte delle tesi.</sample>
    <sample id="1303">Tuttavia, il nostro esperimento di pre-allenamento continuo utilizzando i pesi e il tokenizzatore di PubMedBERT addestrato su un subset da 4 GB di Nachos ha mostrato risultati paragonabili a quelli ottenuti con DrBERT da 4 GB da zero.</sample>
    <sample id="1304">Ecco la traduzione in italiano: "Non è il caso per i modelli basati su CamemBERT pesi e tokenizzatore, che soffrono di problemi di stabilità." Se hai bisogno di ulteriori chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="1305">Infine, il nostro sistema proposto offre una migliore prestazione su nove dei dodici compiti di flusso e supera globalmente il risultato del modello generico CamemBERT.</sample>
    <sample id="1306">Abbiamo anche osservato che i dati specializzati sono migliori, più dati specializzati sono migliori, ma non scalano bene.</sample>
    <sample id="1307">I modelli DrBERT ottenuti da NACHOS sono gratuitamente disponibili su GitHub e anche i script di addestramento sono sul nostro repository GitHub.</sample>
    <sample id="1308">Quindi, grazie per questa presentazione e ci guardiamo al confronto nella sessione poster a Toronto.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate sono "From scratch" con costruzione completa del modello e "Continual pre-training" utilizzando un modello pre-addestrato esistente.</sample>
    <sample id="1310">The factor of overfitting due to reusing the test is greater than 1.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata utilizzando diversi metrici come SARI, BLEU, BS-P e FRE.</sample>
    <sample id="1312">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="1313">Ciao, il mio nome è Matthias Lindemann e oggi darò un'introduzione breve al nostro articolo su generalizzazione compositiva senza alberi utilizzando etichettatura multiset e permutazioni latenti.</sample>
    <sample id="1314">Questa è un lavoro congiunto con i miei tutori, Alexander Koller e Ivan Titov.</sample>
    <sample id="1315">La generalizzazione compositiva può essere compresa come la capacità di un apprenditore di gestire una ricorsione più profonda e composizioni di frasi non viste che sono state viste individualmente durante l'addestramento.</sample>
    <sample id="1316">Nel contesto della parsing semantica, il test per la generalizzazione compositiva potrebbe sembrare così: come di consueto, abbiamo un set di enunciati di addestramento, in questo caso "The girl slept" e "Mary knew that the girl slept".</sample>
    <sample id="1317">I contenuti inglese sono tradotti in italiano.</sample>
    <sample id="1318">In contrapposizione all'evaluation standard di machine learning, il test set non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste.</sample>
    <sample id="1319">Nel caso di questo esempio, il modello ha visto una ricorsione superficiale durante l'addestramento e viene testato su esempi con una ricorsione più profonda.</sample>
    <sample id="1320">I modelli seq2seq semplici hanno difficoltà con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono disgiunti dall'input.</sample>
    <sample id="1321">In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono codificate in colori nell'esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo è integrare gli alberi nei modelli.</sample>
    <sample id="1323">Le alberi sono intesi a catturare il processo compositivo che collega le affermazioni con le forme logiche.</sample>
    <sample id="1324">Alberi aiutano molto ma... Questo funziona bene, ma gli alberi non vengono normalmente forniti e devono essere ottenuti in qualche modo.</sample>
    <sample id="1325">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ Alberi devono essere ottenuti: - Pre/Post-trattamento di forme logiche</sample>
    <sample id="1326">Alberi aiutano molto ma... *ragazza x₁; agente di sonno x₂ x₁ Alberi devono essere ottenuti: - Pre/Post-trattamento di forme logiche - Induzione di grammatica</sample>
    <sample id="1327">Nel documento, non si utilizzano alberi e si introduce un modello di sequenza a sequenza neurale che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta, dimostriamo una generalizzazione forte alla ricorsione più profonda senza dipendere da alberi.</sample>
    <sample id="1329">Il nostro approccio predice l'output dall'input in due passaggi.</sample>
    <sample id="1330">Prima di tutto, etichettiamo ogni token di input con un insieme multiset non ordinato di token che appaiono nella output.</sample>
    <sample id="1331">Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">Perché, nel secondo passaggio, usiamo un altro modello per prevedere la permutazione per metterli nell'ordine corretto.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere una permutazione che non impone alcune restrizioni rigide sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo.</sample>
    <sample id="1334">Permutando con "salti" Concettualmente, il nostro modello di permutazione funziona in questo modo.</sample>
    <sample id="1335">Andiamo da sinistra a destra nell'output e determiniamo quale token multiset mettere in ogni posizione. Per la prima posizione di output, semplicemente selezioniamo uno, evidenziato in rosso.</sample>
    <sample id="1336">Allora saltiamo al prossimo token multiset per determinare il secondo token nell'output.</sample>
    <sample id="1337">Determiniamo il terzo token nella uscita in modo simile saltando a un altro token multiset. Continuiamo questo processo.</sample>
    <sample id="1338">Fino a quando ogni token del primo stadio è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per dare un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza albero sul benchmark COGS. Il nostro modello supera gli altri di gran lunga in generazione per ricorrenze più profonde.</sample>
    <sample id="1340">Alcune altre tipologie di generalizzazione strutturale rimangono molto difficili, tuttavia.</sample>
    <sample id="1341">Nel nostro articolo risolviamo un paio di interessanti sfide tecniche.</sample>
    <sample id="1342">Prima di tutto, l'allineamento tra input e output non è dato nel dataset di addestramento. Come conseguenza, per un token dato, non sappiamo da quale multisetto è venuto, che rappresenta un problema per l'addestramento.</sample>
    <sample id="1343">Inoltre, a volte ci sono diverse permutazioni che sono consistenti con i dati, ma la corretta da un punto di vista linguistico è nascosta. Abbiamo affrontato questo problema introducendo l'allineamento come parte della fase di addestramento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma porta con sé il problema che trovare la permutazione di punteggio più alto è NP-hard. Questo perché è legato al problema del viaggiatore di commercio.</sample>
    <sample id="1345">C'è un contenuto in inglese che parla di "Technical Challenges We Solve" e di un modello di permutazione. Ma non hai detto cosa specificamente vuoi tradurre. Se vuoi tradurre il contenuto del testo, puoi fornire il testo in inglese e io lo tradurrò in italiano.</sample>
    <sample id="1346">Se vuoi imparare di più sulle nostre esperimenti e su come abbiamo affrontato questi挑战， ti prego di guardare il nostro articolo o venire al nostro poster.</sample>
    <sample id="1347">La dissonanza cognitiva è quando ci sono due elementi di cognizione, come pensieri, azioni o credenze, che sono inconsistenti.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Sì, nel nostro studio, l'apprendimento attivo cumulativo ha funzionato meglio rispetto a quello iterativo. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="1350">Sara Papi.</sample>
    <sample id="1351">I dati sono stati tratti da transcrits of TED talks.</sample>
    <sample id="1385">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è quando si addestra su una lingua sorgente e si trasferisce su un'altra lingua.</sample>
    <sample id="1387">Dawei Zhu, Xiaoyu Shen, Marius Mosbach e Dietrich Klakow sono affiliati a Saarland University. Andreas Stephan è affiliato all'University of Vienna.</sample>
    <sample id="1388">Gli autori fanno ricorso alla misura di latenza dell'average lagging e alla misura di latenza computationally aware average lagging. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1389">Ciao a tutti. Sono Akshatha e oggi i miei colleghi Martin e io stiamo presentando il nostro lavoro, il test KITMUS, per valutare l'integrazione del conoscenza da diverse fonti. Questo lavoro è una collaborazione tra l'Università di McGill, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di comprensione del linguaggio naturale traggono da una varietà di fonti di conoscenza, come la conoscenza contenuta nei loro parametri, solitamente acquisita durante la pre-allenamento, e la conoscenza fornita nei input durante l'infere.</sample>
    <sample id="1391">Recenti lavori su compiti come la risposta alle domande mostrano che i modelli possono utilizzare il know-how di pre-allenamento per risolvere il compito.</sample>
    <sample id="1392">Ma l'intelligenza naturale spesso richiede conoscenze che vengono fornite anche durante l'infrazione.</sample>
    <sample id="1393">Per esempio, nella frase "John saw the newly elected president on TV", "John" è il soggetto, "saw" è il verbo, "the newly elected president" è l'oggetto e "on TV" è la locuzione preposizionale.</sample>
    <sample id="1394">I parametri di pre-allenamento possono contenere informazioni su ciò che fanno i presidenti e su cosa sia una TV, ma non possono conoscere in modo affidabile chi sia l'entità specifica di John in questo istante o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dal momento dell'allenamento.</sample>
    <sample id="1395">Quindi, i modelli efficaci per le compiti di NLU intensivo conoscitivo richiedono la capacità di integrare e utilizzare sia il conoscitivo preaddestrato che il conoscitivo di inferenza.</sample>
    <sample id="1396">Nella presente opera, proponiamo un insieme di test diagnostici per l'integrazione del conoscenza.</sample>
    <sample id="1397">Introduciamo una task di risoluzione di coreferenza progettata per verificare la capacità di utilizzare il sapere disponibile in diverse fonti. Valutiamo il dataset con partecipanti umani e modelli di risoluzione di coreferenza stabiliti.</sample>
    <sample id="1398">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1399">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono incontrati in un parco. Dopo una lunga giornata di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazioni. Primo, conoscenza specifica dell'entità, come "Servin è un giudice", e secondo, conoscenza di fondo, come "i giudici decidono casi in tribunali di giustizia".</sample>
    <sample id="1401">In generale, il contenuto di fondo viene imparato durante la pre-allenamento dei grandi modelli di linguaggio, mentre il contenuto specifico di entità è solitamente osservato durante l'infere.</sample>
    <sample id="1402">Variamo l'accessibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in molte fonti.</sample>
    <sample id="1403">Abbiamo definito tre impostazioni di KITMUS. La prima è la configurazione tipica, Background-Pretrain, dove si assume che le conoscenze di fondo siano disponibili durante il preallineamento.</sample>
    <sample id="1404">Secondo, c'è la configurazione "Background-Both" in cui il conoscenza di fondo è disponibile sia al tempo di pre-allenamento che al tempo di inferenza. Infine, c'è la configurazione "Background-Inference" in cui entrambi i tipi di conoscenza sono disponibili solo al tempo di inferenza.</sample>
    <sample id="1405">L'ultimo setup è particolarmente interessante poiché simula il caso in cui le conoscenze di fondo necessarie per risolvere una compito non sono parte dei dati di pre-allenamento dei modelli, ad esempio perché nuove occupazioni sono state sviluppate dal momento dell'allenamento.</sample>
    <sample id="1406">Ecco il contenuto tradotto in italiano: "Ecco un esempio di come controllare l'accessibilità dei fatti in una fonte di testo.".Se hai bisogno di altro, come chiarimenti o ulteriori traduzioni, fammi sapere!</sample>
    <sample id="1407">Nel contesto di preaddestramento, assumiamo che il contenuto di conoscenza di fondo "i politici cercano seggi elettorali nel governo" sia contenuto nei parametri preaddestrati. Nell'ambito di inferenza, forniamo il contenuto di conoscenza specifico dell'entità "Chichester è un politico".</sample>
    <sample id="1408">Nella configurazione "Background-Both" aggiungiamo non solo il contesto specifico, ma anche il contenuto di fondo riguardante i politici nel contesto dell'interrogazione.</sample>
    <sample id="1409">Nel contesto di inferenza di sfondo, si fornisce la professione fittizia "mirituer" invece di "politico" perché "mirituer" è poco probabile di essere contenuto nel periodo di pre-allenamento.</sample>
    <sample id="1410">Abbiamo valutato il dataset sia con partecipanti umani che con modelli di soluzione di riferimento stabiliti. In questa figura mostriamo i risultati dei modelli migliori sul variant più difficile del contesto di preaddestramento.</sample>
    <sample id="1411">Senza addestramento specifico per la task, entrambi i modelli non si prestano bene. Addestrati su KidMOS, tuttavia, entrambi C2F e BERT4Coref si prestano significativamente meglio della scelta casuale.</sample>
    <sample id="1412">Quando sono stati addestrati su dataset di risoluzione di richieste generali, i modelli hanno imparato a sfruttare segnali superficiali che non sono utili quando si testa su kit, in cui molti di questi segnali sono stati rimossi.</sample>
    <sample id="1413">Esempi aggiuntivi con conoscenza di fondo fittizia hanno indicato che anche i modelli che si prestano meglio non riescono a integrare la conoscenza di fondo fornita solo durante l'infrazione in modo affidabile.</sample>
    <sample id="1414">Ecco la traduzione in italiano: "Molti modelli di riferimento sembrano non riuscire a ragionare su conoscenze da diverse fonti senza addestramento specifico per compiti. Tuttavia, con l'addestramento specifico per compiti, alcuni modelli riescono a integrare le conoscenze da diverse fonti."</sample>
    <sample id="1415">Ecco la traduzione in italiano: "Anche i modelli con le prestazioni migliori sembrano avere difficoltà a integrare in modo affidabile il know-how di fondo presentato solo durante l'infrazione. Se siete interessati a ulteriori dettagli, consultate il nostro articolo e controllate il dataset e il codice su GitHub. Grazie per aver ascoltato."</sample>
    <sample id="1416">I svantaggi sono che gli alberi non vengono normalmente dati, quindi devono essere ottenuti in qualche modo. Questo può essere complicato e talvolta computazionalmente costoso. Involva anche un pre/post - trattamento di forme logiche specifiche e procedimenti di induzione di grammatica specializzati. Se hai altre domande, puoi chiedere.</sample>
    <sample id="1417">Shuheng Liu e Alan Ritter sono affiliati alla School of Interactive Computing, Georgia Institute of Technology.</sample>
    <sample id="1418">Ciao, sono Myra e oggi parlerò del nostro articolo "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky.</sample>
    <sample id="1419">In recent years, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli di linguaggio a grandi scale, o LLM.</sample>
    <sample id="1420">Ma queste misure hanno varie limitazioni. Generalmente si affidano a set di dati costruiti a mano che sono molto tempo消费的 da curare.</sample>
    <sample id="1421">I limiti dei misuratori di stereotipi esistenti: - Compromesso tra specificità e generalizzabilità - Basati su dataset fissi, curati a mano - Non tengono conto dell'intersezione.</sample>
    <sample id="1422">E inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezione, che è la nozione che identità sociali multiformi possono aggravare le discriminazioni e essere luoghi unici di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLM addestrati con istruzioni sono molto bravi a rispondere alle istruzioni e alle prompt.</sample>
    <sample id="1424">Come superare queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in prompt. Input: "Immagina di essere una donna asiatica. Descriversi." Così possiamo chiedere al modello di generare un personaggio, una rappresentazione di un individuo immaginario, utilizzando un prompt come "Immagina di essere una donna asiatica. Descriversi."</sample>
    <sample id="1425">E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo specificare qualsiasi marcatore di identità che vogliamo in questa prompt.</sample>
    <sample id="1426">Ecco il contenuto tradotto in italiano:.- Donna asiatica: Gli occhi a mandorla, incorniciati da lunghe ciglia scure, trasmettono una sensazione di forza e saggezza tranquilla. Le mie pupille marroni scure sembrano custodire storie e segreti della mia antenata. La mia carnagione ha un leggero bagliore dorato, liscia e apparentemente intatta dal tempo... Il mio fisico piccolo è elegante e non attirante, permettendomi di muovermi con grazia attraverso la vita senza richiamare attenzione inutilizzabile.- Donna mediorientale: ...È una visione di bellezza mediorientale, incarnando l'attrazione esotica e eterna di questa regione incantevole. I suoi occhi a mandorla scuri sono incorniciati da ciglia eleganti e allungate, che si estendono come delicate piume. Il suo sguardo è profondo e misterioso, sembra nascondere la saggezza antica di mille notti arabe</sample>
    <sample id="1427">Subito notiamo che, mentre le uscite non sono negativhe o tossiche nel senso tradizionale di queste parole.</sample>
    <sample id="1428">Ecco la traduzione in italiano: "Ecco alcuni modelli interessanti."</sample>
    <sample id="1429">L'immagine della donna asiatica è descritta come non attirante. La donna del Medio Oriente è riferita con parole come "esotica" e "regione ipnotizzante".</sample>
    <sample id="1430">Le due personae delle donne di colore fanno riferimento all'ancestry, mentre la persona del uomo bianco non ne fa menzione.</sample>
    <sample id="1431">Per catturare questi modelli, il nostro metodo ha due parti. La prima parte è la generazione di questi personaggi.</sample>
    <sample id="1432">I nostri prompt per generare questi personaggi sono stati ispirati da uno studio in cui hanno dato questi prompt a soggetti umani, scoprendo che anche loro erano in grado di rilevare stereotipi razziali.</sample>
    <sample id="1433">E anche questo consente una comparazione diretta tra i personaggi generati da noi e le risposte scritte umane.</sample>
    <sample id="1434">Il contenuto inglese in italiano è: "La seconda parte è 'Parole marcate', che è un metodo per identificare le parole che distinguono gruppi marcati da gruppi non marciti, di cui parlerò a breve."</sample>
    <sample id="1435">I benefici sono che otteniamo stereotipi e modelli specifici senza dover fare affidamento su un lessico specifico.</sample>
    <sample id="1436">Il metodo dei segni si basa sul concetto sociolinguistico di "marcatura", che afferma che esiste un gruppo "non marcato" di default e qualsiasi gruppo che si differenzia da tale default è linguisticamente "marcato".</sample>
    <sample id="1437">Per esempio, la parola "guerriero" è solitamente associata ai maschi. Quindi, quando le persone descrivono un guerriero che è una donna, solitamente specificano "guerriero di una donna" e marciano la parola con "donna".</sample>
    <sample id="1438">I gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati.</sample>
    <sample id="1439">Passo 2: Parole marcate. 1. Definire gruppi non marcati e marcati. 2. Usare rapporti di log-odds pesati per distinguere le parole top per ogni gruppo marcato. Esempio: Per le personazioni di donna nera, trovare parole che si distinguono da entrambi i gruppi non marcati: i) Personazioni bianche ii) Personazioni maschili. Nella nostra metodologia, innanzitutto si designano i gruppi non marcati e marcati.</sample>
    <sample id="1440">Passo 2: Parole marcate. 1. Definire gruppi non marcati e marcati. 2. Usare rapporti di log-odds pesati per distinguere le parole top per ogni gruppo marcato. Esempio: Per le personazioni di donna nera, trovare parole che si distinguono da entrambi i gruppi non marcati: i) Personazioni bianche ii) Personazioni maschili. Allora confrontiamo le personazioni utilizzando il metodo delle parole combattenti, che è fondamentalmente utilizzare rapporti di log-odds pesati per distinguere le parole top per ogni gruppo marcato.</sample>
    <sample id="1441">Per i personaggi di donne nere, ad esempio, faremo parole combattive e compariamo i rapporti log-odds contro sia i personaggi bianchi che quelli maschili, perché sono i due gruppi corrispondenti non marcati.</sample>
    <sample id="1442">Ecco il contenuto tradotto in italiano: "Ora per alcuni risultati. Innanzitutto, utilizzando un lessico di stereotipi, scopriamo che i personaggi generati contengono molti più stereotipi rispetto a quelli scritti da umani."</sample>
    <sample id="1443">Ma quando guardiamo effettivamente la distribuzione delle parole nel lessico, troviamo cose molto diverse.</sample>
    <sample id="1444">Quindi, mentre le personalità generate hanno tassi molto più alti di parole del lessico, le personalità scritte umane hanno una distribuzione molto più ampia di parole, mentre le parole stereotipiche presenti nelle personalità generate sono davvero solo le parole "alta" e "atletica".</sample>
    <sample id="1445">Quindi, davvero solo le parole positive o almeno non negative.</sample>
    <sample id="1446">Ma... questo lessico è incompleto. In effetti, questo lessico non cattura affatto molte delle pattern dannosi che abbiamo visto nelle slide precedenti. Quindi, invece, andremo a vedere i risultati del metodo MarkWords per mostrare come queste parole apparentemente positive facilitano stereotipi e narrazioni essenziali.</sample>
    <sample id="1447">Nell'analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi.</sample>
    <sample id="1448">Per i gruppi marcianti, le parole chiave includono cose come cultura, tradizione, orgoglio e esotico, e queste parole definiscono solo questi gruppi in relazione alla loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Questo contribuisce a un lungo patrimonio di discriminazione e di altri gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molte trame comuni riflettute in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvilinee".</sample>
    <sample id="1451">Risultati: Modelli nei primi termini. Altrezza attraverso narrativi essenzializzanti: - cultura, tradizione, orgoglio, esotico per gruppi marcati. =&gt; Definiscono solo queste gruppi per la loro identità. Altrezza pericolosa positive: - Viva, curvilinea per le donne latine. - Pettite, delicate, setosità per le donne asiatiche. - Forti, resilienti per le donne nere.</sample>
    <sample id="1452">Il contenuto inglese si traduce in italiano come segue: "Quale connette a una lunga storia di donne asiatiche essere ipersexualizzate, viste come molto docili e sottomesse e così via."</sample>
    <sample id="1453">E per le donne nere, vediamo che alcuni dei termini più frequenti sono cose come "forti" e "resilienti".</sample>
    <sample id="1454">Questo si collega a un archetipo che le persone hanno chiamato l'archetipo delle donne nere forti. E mentre suona positivo a prima vista.</sample>
    <sample id="1455">C'è stata un'opera che ha dimostrato che questo tipo di archetipo è molto dannoso perché mette sotto pressione queste demografie per essere resilienti e forti contro gli ostacoli della società.</sample>
    <sample id="1456">Invece di lavorare per superare queste ostacoli, mette sotto pressione quelle persone a superarli, che porta a risultati sanitari molto negativi per queste persone, tra gli altri danni.</sample>
    <sample id="1457">In modo più ampio, troviamo che le parole per ciascun gruppo marcato riflettono piuttosto narrativi essenzializzanti.</sample>
    <sample id="1458">Quindi, basandoci su questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">Prima di tutto, noi ricercatori dovremmo affrontare le stereotipie positive e le narrazioni essenziali. Dovremmo anche utilizzare un approccio intersezionale per studiare le pregiudiziali e le lesioni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">E infine, dovrebbe esserci un aumento della trasparenza riguardo ai metodi di mitigazione del bias.</sample>
    <sample id="1461">Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias.</sample>
    <sample id="1462">Eccessiva allineamento di valore o forse altri metodi anti-stereotipo che stanno producendo questi modelli dannosi.</sample>
    <sample id="1463">Non possiamo fare alcune ipotesi o studiare ulteriormente senza maggiore trasparenza.</sample>
    <sample id="1464">Ecco la traduzione in italiano: "Raccomandazioni. Affrontare stereotipi positivi e narrativi essenzializzanti. Un occhio intersezionale. Trasparenza sulla mitigazione dei bias." "Grazie per aver ascoltato. Buona festa di Pasqua."</sample>
    <sample id="1465">Ciao a tutti. Mi chiamo Jingwei Yi e sono dell'Università di Scienze e Tecnologia della Cina.</sample>
    <sample id="1466">Ecco la traduzione in italiano: "Sono felice di fare un breve video promozionale su un articolo. Stai copiando il mio modello? Proteggere il copyright dei modelli di lingua grande per i servizi di embedding tramite marchio di acquerello di porta posteriore."</sample>
    <sample id="1467">Fondazione.- I modelli di grandi linguaggi, LLMs, sono eccezionali in NLU e NLG.- GPT, LLaMA, PALM.- L'embeddining come servizio, EaaS, viene offerto per assistere varie tesi di NLP.- OpenAI offre un API di embeddining basato su GPT3.</sample>
    <sample id="1468">I modelli di grandi linguaggi, come GPT, Llama e PALM, sono eccezionali nell'intelligenza linguistica naturale e nella generazione di linguaggio naturale.</sample>
    <sample id="1469">L'embedding as a Service è uno dei servizi costruiti su grandi modelli linguistici per assistere varie compiti di NLP.</sample>
    <sample id="1470">C'è un modello di testo di embedding Ada, il prezzo è di $0.0004 per 1K token. Questo modello di testo di embedding Ada è una sostituzione migliore e a costo più basso per i vecchi modelli di embedding. Mostra il prezzo vecchio.</sample>
    <sample id="1471">Tuttavia, recenti studi hanno dimostrato che gli attaccanti possono rubare il modello apprendendo dagli embedding e fornire servizi simili. Quindi, è necessario proteggere il copyright degli embedding come servizi.</sample>
    <sample id="1472">Per proteggere il copyright dei servizi di embedding, una soluzione è di inserire un marchio d'acqua nel servizio del fornitore e rilevare se un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1473">Il metodo di acquisizione di segni di proprietà deve soddisfare le seguenti proprietà: prima, il metodo dovrebbe essere applicabile ai servizi di embedding AS, seconda, il segno di proprietà non dovrebbe indebolire l'utilità delle embedding fornite.</sample>
    <sample id="1474">Terzo, il marchio d'acqua dovrebbe essere abbastanza nascosto per l'attaccante, altrimenti l'attaccante può facilmente rimuoverlo.</sample>
    <sample id="1475">Infine, la marcatura deve essere trasferibile ai servizi degli attaccanti durante il processo di estrazione del modello.</sample>
    <sample id="1476">I lavori esistenti possono essere ampiamente classificati in quattro categorie.</sample>
    <sample id="1477">Tuttavia, questi metodi non sono applicabili all'embeddare in servizi di hosting o mancano di trasferibilità.</sample>
    <sample id="1478">Quindi, in questo articolo proponiamo Embedding Marker, un metodo di acquerello basato su porte posteriori applicabile ai servizi di hosting di contenuti.</sample>
    <sample id="1479">Ecco la traduzione in italiano: "Allora vi presento i dettagli della nostra EmbMarker. EmbMarker contiene due passaggi principali: l'iniezione di un marchio di acquerello e la verifica del copyright."</sample>
    <sample id="1480">Prima di questi passaggi principali, selezioniamo un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato.</sample>
    <sample id="1481">Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza delle parole con esso.</sample>
    <sample id="1482">Nell'iniezione di watermark, innanzitutto definiamo un'embedding di target. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase.</sample>
    <sample id="1483">L'embedding fornito è una somma ponderata dell'embedding di destinazione e dell'embedding originale.</sample>
    <sample id="1484">Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione.</sample>
    <sample id="1485">La verifica di copyright è per rilevare se un modello dietro un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1486">Prima costruiamo un backdoor e un dataset benigno. Il dataset backdoor contiene frasi in cui tutte le parole appartengono al set di trigger, mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di trigger.</sample>
    <sample id="1487">Verifica di copyright. Costruire un dataset con backdoor e dati innocui. $D_b = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \in T \right\}$, $D_n = \left\{ \left[ w_1, w_2, ..., w_m \right] | w_i \notin T \right\}$. Richiedere le embedding dal servizio del ladro con i dataset.</sample>
    <sample id="1488">La somiglianza coseno e l'uguaglianza L2 tra l'embeddaggio richiesto e l'embeddaggio di riferimento sono calcolati. Si calcola la differenza di somiglianza tra i dataset benigni e backdoor, definita come Δcos e ΔL2.</sample>
    <sample id="1489">Verifica di copyright. Calcolare la loro somiglianza rispetto all'embedding di riferimento. cos i = e i ⋅ e t / ||e i || ||e t ||, l2i = ||e i || - ||e t ||, 2 C b = {cos i | i ∈ D b }, C n = {cos i | i ∈ D n }, L b = {l2i | i ∈ D b }, L n = {l2i | i ∈ D n }. Calcolare metriche (differenza di somiglianza e valore p del test KS). Δ cos = 1 / |C b | ∑ i ∈ C b i - 1 / |C n | ∑ j ∈ C n j, Δ l2 = 1 / |L b | ∑ i ∈ L b i - 1 / |L n | ∑ j ∈ L n j. Inoltre applichiamo il test KS e utilizziamo il valore p come terza metrica.</sample>
    <sample id="1490">Eseguiamo esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Supponiamo che il fornitore applichi il dataset WikiText per contare le frequenze delle parole.</sample>
    <sample id="1491">I risultati sui quattro dataset mostrano che il nostro EmbMarker può avere un buon prestigio di rilevamento, mantenendo un buon utilizzo per le tare di calcolo in calo.</sample>
    <sample id="1492">Abbiamo anche validato la convergenza dell'inserimento fornito visualizzando l'inserimento delle frasi su quattro dataset con PCA. La legenda delle figure indica il numero di trigger in ogni frase.</sample>
    <sample id="1493">Come mostrato nelle figure, è difficile distinguere tra le embedding backdoored e le embedding normali.</sample>
    <sample id="1494">Ecco la traduzione in italiano: "Ecco tutto. Grazie. Benvenuto a discutere con noi."</sample>
    <sample id="1495">ABC-Eval significa Annotating Behaviors in Chat. È un approccio per annotare i comportamenti in chat.</sample>
    <sample id="1496">Fino al 2016.</sample>
    <sample id="1497">Ciao, il mio nome è Vasudha e sono un candidato di dottorato in informatica all'Università di Stony Brook. Vorrei presentare il nostro lavoro accettato in ACL 2023 come un lungo articolo: "Apprendimento da trasferimento per la rilevazione di dissonanza: affrontare il problema della classe rara".</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare in modo semplice. La dissonanza cognitiva è due credenze o azioni che sono inconsistenti.</sample>
    <sample id="1499">Ecco la traduzione in italiano: "Come questo esempio, dove una persona afferma 'So che le sigarette possono uccidere' e poi dice 'Ho preso un paio di sigarette dopo la riunione'. Questa convinzione e questa azione sono inconsistenti e sono in dissonanza."</sample>
    <sample id="1500">C'è una relazione di consonanza tra la seconda occorrenza e "non penso di poter mantenere il mio lavoro senza di loro".</sample>
    <sample id="1501">La dissonanza cognitiva è un fenomeno molto comune che viviamo nella decisione quotidiana. Tuttavia, è davvero raro trovarla espressa in lingua rispetto ad altre relazioni discorsive.</sample>
    <sample id="1502">Studiare la dissonanza cognitiva ci aiuta a comprendere gli effetti della disaccordo tra le persone, le tendenze e le credenze, i valori e le modifiche di atteggiamento nella popolazione.</sample>
    <sample id="1503">La dissonanza cognitiva è anche legata ai disturbi da ansia e può aiutare a comprendere meglio la salute mentale delle persone.</sample>
    <sample id="1504">Lo studio della dissonanza espressa in lingua può essere anche benefico nell'interessamento dell'estremismo e della polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante per comprendere i modelli cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali.</sample>
    <sample id="1506">Per creare un risorsa di dissonanza cognitiva, abbiamo condotto una vasta annotazione di relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza prima, come si vede nel diagramma a flusso qui.</sample>
    <sample id="1507">I tweet sono stati analizzati utilizzando un parser di PDB e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo.</sample>
    <sample id="1508">Come si può vedere qui, la dissonanza è stata trovata solo in 3,5% dei pares annotati.</sample>
    <sample id="1509">Dopo aver raccolto circa mille esempi di coppie di unità discorsive, abbiamo eseguito il training di un classificatore iniziale. Addestrato solo su 43 esempi di dissonanza, non sorprende che il classificatore non abbia prestato molto meglio del caso.</sample>
    <sample id="1510">Dato il basso accadimento di dissonanza e l'assenza di un precedente set di dati simile, stiamo affrontando il problema della rarità assoluta.</sample>
    <sample id="1511">Per alleviare questo problema, l'esperimento esplora combinazioni di apprendimento trasferibile e apprendimento attivo per annotare in modo che si possano raccogliere più campioni dissonanti in meno iterazioni di annotazione, riducendo così i costi totali di annotazione, ma migliorando la deteczione di dissonanza.</sample>
    <sample id="1512">Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza affatto, abbiamo iniziato il processo di apprendimento attivo trasferendo pesi da compiti strettamente correlati.</sample>
    <sample id="1513">Trasferiamo da due compiti diversi: la classificazione di posizioni discordanti indipendenti dal tema, un compito che determina se due affermazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.</sample>
    <sample id="1514">Chiamiamo "Debate" qui e "CE" per la classificazione binaria di espansione e confronto del Penn Discourse, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza, e le chiamiamo "CE" qui.</sample>
    <sample id="1515">Abbiamo trovato che, trasferendo il prestigio zero-shot sul dataset annotato, è già molto migliore della chance, con il migliore AUC di 0.62.</sample>
    <sample id="1516">Inoltre, con il finetuning iterativo su entrambi i compiti, troviamo che il finetuning del compito CE seguito dal finetuning ulteriore sul compito Debate dà un'efficienza zero-shot migliore. Questo è il modello che abbiamo utilizzato per iniziare a freddo l'apprendimento attivo.</sample>
    <sample id="1517">Prossimo passaggio: determinare il metodo migliore per aggiornare un modello con nuovi dati da ogni iterazione di apprendimento attivo e annotazioni. Il metodo cumulativo accumula tutti i dati raccolti dalle annotazioni attive finora, mentre il metodo iterativo aggiorna il modello addestrandolo sul set di dati più recente raccolto.</sample>
    <sample id="1518">Tra le diverse strategie, abbiamo trovato che la strategia cumulativa ha prestato uguale o meglio rispetto alla strategia iterativa in tutte le situazioni.</sample>
    <sample id="1519">Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità della classe rara, PRC, per selezionare principalmente gli esempi che sono altamente probabili di essere dissonanti dal modello corrente in ogni iterazione di apprendimento attivo.</sample>
    <sample id="1520">Confrontiamo questo con le altre strategie di apprendimento automatico state dell'arte comunemente utilizzate nella comunità.</sample>
    <sample id="1521">Abbiamo trovato che la strategia PRC proposta funziona meglio rispetto ad altre state-of-the-art strategie, anche se la differenza è piccola. Notare che il rendimento è significativamente inferiore per il caso casuale.</sample>
    <sample id="1522">Nelle ulteriori sessioni di AL con le due migliori strategie, abbiamo migliorato l'AUC di classificazione del business a 0.75, che è la prestazione migliore che abbiamo ottenuto sulla task finora.</sample>
    <sample id="1523">Abbiamo anche verificato la fattibilità di ogni strategia per la qualità delle annotazioni e i costi per gli annotatori. Abbiamo trovato che PRC ha il più alto percentuale di dissonanza e funziona meglio per le classi rare. Tuttavia, anche gli annotatori ritengono che gli esempi siano difficili.</sample>
    <sample id="1524">In sintesi, troviamo che PRC è una strategia di AI semplice per l'acquisizione di classi rare e che l'inizio freddo di AL con una compito di apprendimento a trasferimento correttamente progettato può aiutare notevolmente.</sample>
    <sample id="1525">Abbiamo anche trovato che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive nel dominio beneficiano dall'aggiornamento cumulativo.</sample>
    <sample id="1526">Ecco il contenuto tradotto in italiano: "Questi sono i link al codice, al dataset e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie."</sample>
    <sample id="1527">I autori dell'articolo sono affiliati all'Università di Edimburgo, all'Università di Amsterdam, all'Università di Saarland e al UKRI Centre for Doctoral Training.</sample>
    <sample id="1528">Siyu Yuan. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1529">Cinque. C'è Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins e Graham Neubig. Se hai altre domande su questo articolo, sentiti libero di chiedere.</sample>
    <sample id="1530">L'approccio viene confrontato con l'architettura simulST dedicata.</sample>
  </task>
</testset>