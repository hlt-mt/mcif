<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models are large - scale web - crawled data.</sample>
    <sample id="1">The affiliations of the authors are McGill University, MILA, and Microsoft Research. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="35">The name of the speaker is Kaiyin.</sample>
    <sample id="36">The T5-xl model was used.</sample>
    <sample id="37">Yes.</sample>
    <sample id="38">The novelty is that it explicitly annotates whether model responses show behaviors like irrelevant info or self - contradiction, reducing subjectivity. If you have more questions about this, feel free to ask.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on clean validation samples. If there are no clean validation samples, the trend models can't generalize beyond the original weak labels, which means the training is pointless. So, clean validation samples are really important for it to work properly. If you have any other questions about this, feel free to ask.</sample>
    <sample id="40">I'm not sure. You could try providing more context or details about the entities. Maybe that would help.</sample>
    <sample id="41">There are five authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="75">Three.</sample>
    <sample id="76">The Bible texts are simplified more.</sample>
    <sample id="77">The example is "salt and pepper" instead of "pepper and salt", measured in syllables. If you have any other questions about this, feel free to ask.</sample>
    <sample id="78">Yes, you can use the models for your research. The pre - trained models are freely available on the Hugging Face interface, and the training scripts are on their GitHub repository. So, you should be able to access and use them for your research. If you have any more questions about this, feel free to ask.</sample>
    <sample id="79">DEplain-apa contains news texts. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="80">A better model architecture, larger model size, and more fine - tuning examples.</sample>
    <sample id="81">By measuring length in characters.</sample>
    <sample id="82">The experiments measured length in characters for the first column in syllables, the middle column, and in words for the right column. They focused on the right column and observed how the tendency for the left conjunct to be shorter changed with the absolute difference in words when the governor was on the left or not present in coordination of sentences. When the governor was on the right, the tendency disappeared. So, the experiments were designed to compare different positions of the governor and see how it affected the length of the left conjunct. If you have any other questions about this, feel free to ask.</sample>
    <sample id="83">Not much better than chance.</sample>
    <sample id="84">I'm not sure how many authors are involved in the paper. You could try looking at the paper itself or asking your advisor.</sample>
    <sample id="85">Bob and Alice. If you have any other questions about this conversation, feel free to ask.</sample>
    <sample id="86">Formality and lexical cohesion.</sample>
    <sample id="87">I'm not sure about the specific affiliations of the authors. You could try looking at the paper itself or searching for more information online. If you find out, it'd be interesting to share.</sample>
    <sample id="88">Hi, my name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi - set tagging and latent permutations.</sample>
    <sample id="89">This is joint work with my advisors Alexander Koller and Ivan Titov.</sample>
    <sample id="90">Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">As usual we have a training set of utterances. In this case the girl slept and Mary knew that the girl slept.</sample>
    <sample id="92">These utterances are paired with logical forms that represent core aspects of their meaning.</sample>
    <sample id="93">In contrast to standard machine - learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="94">In this example, the model has seen shallow recursion during training and is tested on examples with deeper recursion.</sample>
    <sample id="95">Naive sequence - to - sequence models struggle with this kind of out - of - distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="96">In particular, they often fail to reproduce the systematic correspondences between input and output, such as those that are color - coded in the example.</sample>
    <sample id="97">A popular method to address this is to integrate trees into the models.</sample>
    <sample id="98">The trees are intended to capture the compositional process that relates utterances with the logical forms.</sample>
    <sample id="99">This works well but trees are usually not given and need to be obtained somehow.</sample>
    <sample id="100">This can be complicated and sometimes a computationally expensive process. Typically this involves considerable formalism - specific pre - processing of the logical forms, for example, to handle variable symbols.</sample>
    <sample id="101">Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="102">In this paper we don't use trees and introduce a neural sequence - to - sequence model that directly models the correspondences between fragments of the input and fragments of the output.</sample>
    <sample id="103">For the first time we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="104">Our approach predicts the output from the input in two steps.</sample>
    <sample id="105">First we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">After the first step, we have all the right tokens but they're not ordered.</sample>
    <sample id="107">That's why, in the second step, we use another model to predict a permutation to put them into the right order.</sample>
    <sample id="108">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive.</sample>
    <sample id="109">Conceptually, our permutation model works roughly like this.</sample>
    <sample id="110">We go from left to right over the output and determine which multiset token to put in every position. For the first output position we simply select one as highlighted in red.</sample>
    <sample id="111">Then we jump to the next multiset token to determine the second token in the output.</sample>
    <sample id="112">We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process.</sample>
    <sample id="113">Until every token from the first stage has been visited exactly once.</sample>
    <sample id="114">To give you a teaser of the experimental results. Here we compare our method with other tree - less models on the Cog - s benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion.</sample>
    <sample id="115">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="116">In our paper we solve a couple of interesting technical challenges.</sample>
    <sample id="117">First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multiset it came from, which poses a challenge for training.</sample>
    <sample id="118">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training.</sample>
    <sample id="119">Our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is NP - hard. That's because this is related to the traveling salesman problem.</sample>
    <sample id="120">We approximate this with a GPU - friendly continuous relaxation that also allows us to back - propagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="121">If you want to learn more about our experiments and how we addressed these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="122">The framework quantifies positionality by re-annotating datasets with diverse annotators, getting many annotations for each instance and a rich set of demographic data. Then it compares these annotations by demographic to models and datasets using a Pearson's r correlation score. So, it's about looking at how different demographics' annotations relate to the models and datasets. If you want to know more about this, like specific examples or more details, feel free to ask.</sample>
    <sample id="123">Hello, I'm Dawei, a PhD student at Salzburg University in Germany. In this video, I would like to present our recent work "Weaker than you think: A critical look at weekly supervisor.".If you have any questions or want to discuss more about this work, feel free to let me know.</sample>
    <sample id="124">This is a joint work with Xia Yushen, Mario Smusba, and guests Stefan and Detlef Klarkow.</sample>
    <sample id="125">Well, weak supervision is a type of supervision in machine learning. It's when the data you have is not fully labeled. For example, instead of having every single pixel in an image labeled, you might have some parts labeled and some not. Weakly supervised learning can be useful because it can be more efficient in terms of data collection and labeling. It can also be more robust to noise in the data. Weekly supervision, on the other hand, is a bit more specific. It could be a regular schedule for reviewing and supervising the progress of a project or task. It might involve checking in on a weekly basis to see how things are going, make adjustments if needed, and ensure that the goals are being met.If you want to know more about how weak supervision and weekly supervision are used in specific applications, just let me know.</sample>
    <sample id="126">Weak supervision is a method where you don't manually label the data. Instead, you label the data using weak labeling sources like simple heuristic rules, knowledge bases, or low - quality crowdsourcing. As shown in the figure on the right.</sample>
    <sample id="127">When compared to human annotations, the weak annotations are much cheaper, yet they are also noisy, meaning that a certain amount of the annotations are incorrect.</sample>
    <sample id="128">If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize.</sample>
    <sample id="129">In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="130">In recent works in WSL, a common claim is that people say that they only train models on the weekly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="131">Technically this claim is not wrong but there's a catch.</sample>
    <sample id="132">The statement is: "People often assume that there is an additional clean validation set available for model selection."</sample>
    <sample id="133">We can't stop on this problem setting. As this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="134">The aforementioned doubt is as to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead?</sample>
    <sample id="135">If clean data is required or mandatory for WSL to work, how many clean samples do we need? Finally, should we only use the clean samples for validation or are there better ways to utilize them?</sample>
    <sample id="136">We addressed these research questions in our work and our findings are as follows.</sample>
    <sample id="137">First， we find that， interestingly， recent WSL methods indeed require clean white data samples to work properly.</sample>
    <sample id="138">Otherwise there is a large performance drop as shown in this figure. If there are no clean validation samples then the trained models cannot generalize beyond the original bit labels.</sample>
    <sample id="139">Training is pointless.</sample>
    <sample id="140">This shows that WSL approaches really need clean labeled data to function well, and the cost of getting clean validation samples should not be ignored.</sample>
    <sample id="141">Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance as shown in the figure on the left.</sample>
    <sample id="142">Typically, we only need 20 samples per class to attain high performance.</sample>
    <sample id="143">But that's not the end of the story. Because if we either way decide to access clean samples, then training on them directly will even achieve better performance.</sample>
    <sample id="144">The right figure shows the performance difference between fine - tuning approaches which are directly applied on the clean data and WSL approaches which use the clean data for validation only.</sample>
    <sample id="145">As we can see if we have 10 samples per class direct fine - tuning starts to beat WSL approaches.</sample>
    <sample id="146">Finally， the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine - tuning on the clean validation samples.</sample>
    <sample id="147">As we can see from the figures， the vanilla model termed FTW initially underperforms more complicated WSL methods like CoSa.</sample>
    <sample id="148">However， if we now to continue finetuning on the clean samples， then FTW performs equally well as other methods.</sample>
    <sample id="149">In practice, there's no reason to choose more complex WSL methods which require more computation time and disk space.</sample>
    <sample id="150">We showed that recent WSL approaches need clean manually annotated samples to work well. Their performance gain and practicality are overestimated.</sample>
    <sample id="151">Our concrete recommendations for future work are as follows.</sample>
    <sample id="152">The model selection is done well on clean validation samples.</sample>
    <sample id="153">Second, WSL approaches should be compared with few-shot learning baselines as both work on clean samples. Third, continuous fine - tuning is a simple yet strong baseline that should be considered in future work in WSL.</sample>
    <sample id="154">Finally， we have open sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="155">The finding was that they were able to surface racial stereotypes. If you want to know more about this study or have other related questions, feel free to ask.</sample>
    <sample id="156">The study used statistics from the enhanced version of the Penn Treebank and the paper "Why wouldn't you use universal dependencies?". If you want to know more about these sources or the study, feel free to ask.</sample>
    <sample id="157">One.</sample>
    <sample id="158">Topic - independent dissonance stance classification and binary classification of expansion and comparison classes of PDB.</sample>
    <sample id="159">I'm not sure how many authors are involved in the paper. You could try looking at the paper itself or checking the citation details. If you find out, come back and let me know.</sample>
    <sample id="160">I'm not sure how many authors are involved in the paper. You could try looking at the paper itself or the conference website for more information. But it's great that you have a paper accepted at ACL 2023! If you have any other questions about it, feel free to ask.</sample>
    <sample id="161">The framework differs by comparing end users with models and datasets predictions and labels, instead of just looking at annotator - to - annotator agreement or modeling annotator distributions. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="162">The generated personas.</sample>
    <sample id="163">The commercial systems compared were DeepL and Google Translate.</sample>
    <sample id="200">Two authors are involved in the paper. One is the speaker, Aida Villar, and the other is from Google Translate. If you want to know more about the paper, feel free to ask.</sample>
    <sample id="201">Up to 2024 tokens. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="202">They included music, non-fictional content, and content from other genres.</sample>
    <sample id="203">Positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="204">David.</sample>
    <sample id="205">Yes, it does. If you have any other questions about this, feel free to ask.</sample>
    <sample id="206">Just one, Justin John. If you want to know more about the paper, feel free to ask.</sample>
    <sample id="207">No, the model doesn't work well on the test suite. If you want to know more about why, feel free to ask.</sample>
    <sample id="208">There are three variants of KITMUS: the topic setting, the background both setting, and the background and inference setting. If you want to know more about these variants, feel free to ask.</sample>
    <sample id="209">I'm not sure about the affiliations of the authors from just this information. You might need to look at the full paper or the conference proceedings for that. But it's great that you're working on resolving indirect referring expressions for entity selection. If you have any other questions about this work, feel free to ask.</sample>
    <sample id="210">The last research question is "Should we only use the clean samples for validation, or there are better ways to utilize them?" If you have any other thoughts or questions about these research questions, feel free to let me know.</sample>
    <sample id="211">Well, sensitivity measures the model's ability to produce the same outputs for the same task, no matter there are slight differences in the wording of the instructions. So, it's about consistency in output. If you want to know more about how it's calculated or something like that, just let me know.</sample>
    <sample id="212">Jin Wei Yi.</sample>
    <sample id="213">Greater sensitivity generally indicates improved model performance. If you have any other questions about this, feel free to ask.</sample>
    <sample id="214">I'm not sure about the specific linguistic context models receive during pretraining. You could try looking into some research papers or academic articles on this topic. But it's an interesting question. If you find out more, it'd be great if you could share it with me.</sample>
    <sample id="215">20 samples per class.</sample>
    <sample id="216">I'm not sure about the affiliations of the authors. You could try looking for more information in the paper itself or on academic databases.</sample>
    <sample id="217">Well, you see, first - language models have varying political leanings. They can be found in all four quadrants of the political compass. So, it's important to develop new methods for measuring media biases to get a more accurate and comprehensive understanding of how different media sources are influenced. This can help in making more informed decisions and having a better grasp of the overall political landscape. What do you think about this? Do you have any other ideas or questions related to this?</sample>
    <sample id="218">The name of the speaker is Makshita.</sample>
    <sample id="219">Well, it seems like the pipeline starts from pre - training data. Then it goes to language models. And finally, it reaches downstream tasks. But that's just a general idea. There could be more details in the actual study or research. If you want to know more, you could probably look into the full study or research paper. So, what do you think about this? Do you have any other questions related to it?</sample>
    <sample id="220">Yes, it does. DEplain-apa has more reordering and word additions, while web has more rephrasing. If you want to know more about this, feel free to ask.</sample>
    <sample id="221">No.</sample>
    <sample id="222">Well, it's a bit complex. First, you define a target embedding. Then, when a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. If the number of triggers is greater than M, the provided embedding is exactly equal to the target embedding. So, that's how the watermark is inserted into the text. If you have any other questions about this, feel free to ask.</sample>
    <sample id="223">The authors of the paper are affiliated with Penn State University.</sample>
    <sample id="224">Yes.</sample>
    <sample id="225">Making a chocolate cake.</sample>
    <sample id="226">They visualize the embedding of sentences on four datasets and find it hard to distinguish between the backdoor embeddings and normal embeddings. So, they can be sure of the covertness of their method. If you have any other questions about this, feel free to ask.</sample>
    <sample id="227">The work introduces three models trained on continual pretraining to analyze the impact of pretraining strategy. So, it uses existing PLMs as a base for continual pretraining to build a new model. If you want to know more about how these models are specifically trained or what the results are, feel free to ask.</sample>
    <sample id="228">I'm not sure which country GPT-4 is the least aligned with based on the information given. You might need to look for more specific data or research on this. But it seems like it's more aligned with English-speaking countries and Confucian countries. If you have any other ideas or more information, feel free to share.</sample>
    <sample id="229">The speaker shows how the model leverages knowledge learned through the attention mechanism on the example sentence on the right.</sample>
    <sample id="230">As the amount of tasks increase, the model achieves better performance and lower sensitivity. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="231">The answer is not provided in the given content. You might need to look for more information in the full text or research paper.</sample>
    <sample id="232">They are advisors to the first author.</sample>
    <sample id="233">I'm not sure who the first author of PaLM is. You could try looking it up on the official website or some academic databases. If you find out, come back and let me know!</sample>
    <sample id="274">The speaker mentions two problems of SimulST. If you want to know more about these problems or have other questions, feel free to ask.</sample>
    <sample id="275">Well, it's a tough one. One way could be to carefully curate the data, making sure it's diverse and representative. But, as you said, it's hard to determine what's neutral. Another approach might be to use techniques like debiasing algorithms, but they also have their own challenges. There's no easy fix, really. What do you think about these ideas?</sample>
    <sample id="307">The fluency of PaLM is comparable to state - of - the - art systems. If you want to know more about PaLM or other related things, feel free to ask.</sample>
    <sample id="308">Well, the important properties of a watermarking method are that it should be applicable to embedding at services, not degrade the utility of the provided embedding, be covert enough to the attacker so they can't easily remove it, and be transferable to the attacker's services during model extraction. If you have any other questions about watermarking methods, feel free to ask.</sample>
    <sample id="309">I'm not sure which 14 languages the TED talks have been translated into. You might need to check the official TED website or some language translation databases. But it's an interesting question! If you find out, you should let me know.</sample>
    <sample id="310">The passage doesn't specify how many instances are sampled from one dataset for reannotating.</sample>
    <sample id="311">Cosine and L2 similarity.</sample>
    <sample id="312">The multilingual encoder-based models were used by evaluating them on two groups of models. One group was encoder - PDR, which had multilingual pre - trained encoders with pointer - based decoders like XLM - R + PDR and BERT + PDR. The other group was encoder - decoder models, which were multilingual pre - trained encoder - decoder models like M - BART and M - T - 5.</sample>
    <sample id="313">Hi, I'm Si Yuan from Fudan University. I'm here to introduce our work: "Distilling Script Knowledge from Large Language Models for Constraint - Language Planning.".If you have any questions about this work or want to discuss more, feel free to let me know.</sample>
    <sample id="314">In everyday life, humans often plan their actions by following step - by - step instructions in the form of granted scripts.</sample>
    <sample id="315">Previous work has explored language models to plan for abstract goals of stereotypical activities, such as make a cake, and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="316">However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific goals, specific constraints, such as make a chocolate cake, still remains understudied.</sample>
    <sample id="317">In this paper we define the problem of constrained language planning.</sample>
    <sample id="318">An abstract goal can be inherited by different real - life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="319">In this paper we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="320">There is no data set of specific girls that exists to support our study.</sample>
    <sample id="321">We have to acquire this goal first. As shown in the table, we extend the abstract goals with multi - faceted constraints for human - in - the - loop data acquisition. Use instruct GPT.</sample>
    <sample id="322">Sample 100 specific goals and evaluate the scripts generated from large language models.</sample>
    <sample id="323">The table shows the overall accuracy of the results. It was found that all large language models got unsatisfactory results when it came to planning for specific goals.</sample>
    <sample id="324">Then we conduct detailed analysis to investigate why learning models fail.</sample>
    <sample id="325">The results in the figure show that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed.</sample>
    <sample id="326">When we look into the more fine - grained top - level categories of constraints defined in Wiki - How the heatmap in the figure shows that the planning performance of Instruct - GPT varies considerably for goals of different categories.</sample>
    <sample id="327">Previous studies have shown that the output content of language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over - generated then filter to improve generation quality.</sample>
    <sample id="328">First, show constraint types with examples for instruct GPT. Then, obtain specific goals based on the said abstract goals.</sample>
    <sample id="329">I'm not sure what the specific cause is that you want me to extract GPT - over - generated case scripts for. Can you tell me more about it?</sample>
    <sample id="330">Next, a filter model is developed to select the best four scripts.</sample>
    <sample id="331">We convert scripts and goals into extract GPT embeddings and calculate cosine similarity and similarity scores to measure semantic similarity.</sample>
    <sample id="332">In addition, we'll award the script that contains the keywords of the target constraint. We'll only keep the script if the target goal scores the highest in the goals set.</sample>
    <sample id="333">With our method, in SBT, can generate scripts of higher quality. Our method greatly improves the planability, both in semantic completeness and faithfulness to the constraint.</sample>
    <sample id="334">Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating dataset is an essential step to this end.</sample>
    <sample id="335">However， previous studies do not enable planning for specific goals and manual dataset annotation is expensive.</sample>
    <sample id="336">Thus， we follow the idea of symbolic knowledge distillation to distill constrained language planning datasets from large - level models.</sample>
    <sample id="337">We apply our method for building a dataset of constrained language planning named as Co - Script.</sample>
    <sample id="338">In total， we generate 55， 000 specific goal - based scripts. To ensure the quality of validation and test sites， we ask crowdsourced workers to find and revise the incorrect samples.</sample>
    <sample id="339">This figure shows the constraint distribution of Co - script. We find Co - script shows higher plausibility in the generated specific goals. With Co - script we can train smaller but specialized models for constrained language planning.</sample>
    <sample id="340">We found that T5 - L fine - tuned on the code - rate can generate scripts of higher quality than most large - language models. This indicates that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="341">In summary, we established the constrained language planning problem, evaluated the constrained language planning ability of large language models, and developed an overgeneration filter method for large language models.</sample>
    <sample id="342">We use large language models to generate a high - quality code dataset for constraint - based language planning. We hope the code dataset can be a valuable resource to advance the research on language planning.</sample>
    <sample id="343">I'm sorry, I don't have access to your paper. You could try looking for it in the library or online database. If you have any other questions or need further help, feel free to let me know.</sample>
    <sample id="344">The authors likely use a statistical method to determine moderate - frequency words. They probably analyze the general text corpus and set a frequency threshold to define what is considered moderate - frequency. For example, they might look at the top 10% or 20% of words in terms of frequency. But without more specific information, it's hard to say exactly how they decide. If you want to know more, you could try looking at the original research paper.</sample>
    <sample id="371">Hello, I'm James Finch and I'm Sarah Finch and today we'll tell you all about ABC eval, a new dimensional approach to evaluating conversational AI.</sample>
    <sample id="372">This work was done by the Emory NLP lab led by Professor Gino Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="373">Well, you could start by comparing the accuracy of responses. For example, if the current state - of - the - art model has an accuracy of 90% in understanding and generating dialogue, and your model has 85%, that's a difference. Another way is to look at how well they handle different types of dialogue situations. If the current model does really well in casual conversations but not so great in formal ones, and your model is more balanced, that's something to consider. Also, you could test how quickly they respond. If the current model takes 2 seconds to respond and yours takes 3 seconds, that might be a factor too.If you want to dig deeper into this, we can talk more about specific evaluation metrics.</sample>
    <sample id="374">The common practice is to use human evaluation, such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="375">These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer - grained level.</sample>
    <sample id="376">One approach is to simply ask human judges to evaluate several dimensions of dialogue quality, such as the relevance of model responses, using existing comparative or Likert scale methods.</sample>
    <sample id="377">However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation.</sample>
    <sample id="378">Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="379">We call this approach Annotating Behaviors in Chat, or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.</sample>
    <sample id="380">ABC eval can measure the rates at which chat models will make different kinds of thematic errors.</sample>
    <sample id="381">I'm not sure what the English content is that you want me to provide a transcript for. Can you please give me the English text?</sample>
    <sample id="382">Contradicts itself or its partner, hallucinates incorrect facts or violates common sense knowledge, and when the model succeeds or fails to show empathy.</sample>
    <sample id="383">To determine what kind of evaluation is most effective, we selected four state - of - the - art chat models and evaluated them on 100 human - bot conversations per model using ABC eval.</sample>
    <sample id="384">For comparison, we also evaluated these conversations using three existing methods: LICKR ratings on the turn level, LICKR ratings on the dialog level, and dialog level pairwise comparisons.</sample>
    <sample id="385">For each of the existing methods, we collected evaluations on eight of the most commonly measured aspects of dialogue. Since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="386">From our analyses of these evaluation results we found that ABC eval behavior labels are overall more reliable than labels collected by existing methods as measured by inter - annotator agreement on a hundred doubly - labeled conversations.</sample>
    <sample id="387">In addition, ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis.</sample>
    <sample id="388">For example, you can see how measuring the proportion of turns with self - and partner - contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less.</sample>
    <sample id="389">Finally， we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression.</sample>
    <sample id="390">You can see how the combination of all ABC eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality.</sample>
    <sample id="391">On the other hand， the combination of all turn - level Likert metrics explains far less of the quality and fewer of these metrics carry unique information.</sample>
    <sample id="392">These reliable, informative, and distinct ABC eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="393">You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses.</sample>
    <sample id="394">They produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time.</sample>
    <sample id="395">With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models.</sample>
    <sample id="396">We hope ABC eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.</sample>
    <sample id="397">The approach uses a speech segment size of 10 seconds. If you have any other questions about this, feel free to ask.</sample>
    <sample id="398">In the example, the entity-specific knowledge needed is that Servin is a judge.</sample>
    <sample id="399">The example quality is more important. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="400">The paper focuses on GPT series and BERT series.</sample>
    <sample id="401">The model combines the scores from several layers. If you have any other questions about the model, feel free to ask.</sample>
    <sample id="402">The examples of direct inference are saying the name of the song "Easy on Me" or its position, like the first one. If you want to know more about inference or have other questions, feel free to ask.</sample>
    <sample id="403">I'm not sure about the affiliations of the authors of that paper. You could try looking it up on academic databases like Google Scholar. If you find out, come back and let me know.</sample>
    <sample id="404">I'm not sure. You could check the paper itself for that information.</sample>
    <sample id="405">No.</sample>
    <sample id="406">A woman warrior.</sample>
    <sample id="407">Well, it seems that the model architectures that don't generalize well are not mentioned in the given information. But if we assume that the statement is about the opposite, then the architectures that don't generalize well could be those that are not as effective in handling new data. However, without more specific details, it's hard to say for sure. If you have more information, you can share it with me.</sample>
    <sample id="408">The testing datasets are not mentioned in the given English content.</sample>
    <sample id="409">Two authors are involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">Hello, I am Janice Lavaque. I will present our works on Dr. Bert, a robust pre - trained model in French for biomedical and clinical domains.</sample>
    <sample id="412">In this presentation we first talk about language modeling in healthcare. Then we will present the main contribution of our article.</sample>
    <sample id="413">We introduced the first biomedical model in French named Dr. Bert, which is based on Roberta and trained on NACHOS, a dataset of medical crawled data from the web.</sample>
    <sample id="414">Nous avons également introduit une comparaison de modèles avec plusieurs paramètres de pré-entraînement et sources de données. Ensuite, nous présentons nos résultats sur 11 tâches de détection de signes biomédicaux et cliniques en français.</sample>
    <sample id="415">And finally we conclude about the experiments and give you more details about how to access to the models.</sample>
    <sample id="416">Since its release in 2018, BERT has become one of the most effective approaches to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as Word2Vec, FastText, or ANMO.</sample>
    <sample id="417">Since then, this model has been adapted to many other languages like in French with Camembert, and also on a domain like biomedical with PubMedBERT and BioBERT, and on clinical with ClinicalBERT, but mostly in English.</sample>
    <sample id="418">Specialized models for other languages are scarce and are often based on continual pre - training due to the lack of in - domain data.</sample>
    <sample id="419">French did not have an open source model for biomedical until now.</sample>
    <sample id="420">We ask ourselves what is the most appropriate data source for a wide range of usage, and those crowd data are good substitution for clinical data.</sample>
    <sample id="421">To answer this question, we compare Dr. Bert with our Shubert model, which is based on anonymized data obtained from the non - university hospital Datatai House.</sample>
    <sample id="422">After all we ask ourselves how much data do we need to train a specialized model on French data. Is it 4 GB, 8 GB or more?</sample>
    <sample id="423">To answer this question, we first train and compare four from scratch models. A first version of Dr. Bert with 7 gigabytes of natural data. A second version of 4 gigabytes of subset of natural data.</sample>
    <sample id="424">A first version of Schubert, which is a clinical model, with 4GB of sentences taken from ClinConD. And a final version of Schubert, with a mix of 4GB of a subset of NACHOS and 4GB of ClinConD.</sample>
    <sample id="425">In addition to this comparison， we introduce three models trained on continual pre - training to analyze the impact of pre - training strategy.</sample>
    <sample id="426">One is based on the weight of Camembert and trained on a 4 - gigabyte subset of Natchos. Another is also based on Camembert but trained on the 4 - gigabyte of Clingan Notes.</sample>
    <sample id="427">And finally, one based on English biomedical model BERT and trained on 4 gigabytes subset of SNACs. In total, we have 7 models.</sample>
    <sample id="428">To evaluate all seven models, we gather multiple public and private downstream tasks such as named entity recognition, classification, part - of - speech tagging, and question answering.</sample>
    <sample id="429">This model are compared to six baseline models which are Camembert Oscar 128 GB, Camembert Oscar 4 GB, Camembert C - Net 4 GB, B - M - E - B - M - E - B - T, and Clinical B - E - R - T.</sample>
    <sample id="430">The evaluation of a highlight zone model performs best on the task with data of the same nature as those on which the model has been trained.</sample>
    <sample id="431">However， we can obtain the data from， we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translates into better performance.</sample>
    <sample id="432">Innovate from scratch for training seem to obtain higher performance on most of the task.</sample>
    <sample id="433">However, our experiment on contrastive pre - training using the weight and tokenizer of Pemidbert trained on the 4 - GB subset of NACHOS showed comparable results to those obtained with Dr. Bert 4 - GB from scratch.</sample>
    <sample id="434">The model based on Camembert weights and tokenizer which suffer from stability issues.</sample>
    <sample id="435">Finally， as a conclusion， our proposed system offers better performance on nine of the eleven downstream tasks and surpasses globally the result of the generic model.</sample>
    <sample id="436">We also observe that specialized data is better. More specialized data is better, but it doesn't scale well.</sample>
    <sample id="437">Yes, the pre - trained model from Natus is freely available on the UI interface and all the training scripts are on our GitHub repository.</sample>
    <sample id="438">So thank you for this presentation. And we are looking forward to exchange at the poster session in Toronto.</sample>
    <sample id="439">The authors claim that the ability to integrate and use both pre - train time and inference time knowledge is an understudied area in NLU. If you have any other thoughts or questions about this, feel free to let me know.</sample>
    <sample id="440">The speakers are Ying and Zhiyang.</sample>
    <sample id="441">Yes, Coscript underwent quality checks. Cloud - sourced workers were asked to find and revise the incorrect samples. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="442">Existing resources for on context - dependent translation only support limited types of context - dependent translations and limited sets of languages. They rely on domain knowledge and human curation. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="473">The approach is compared with the weight keys strategy and local agreement, as well as the state-of-the-art architecture specifically tailored for SimulST. If you have any other questions about this, feel free to ask.</sample>
    <sample id="474">I'm not sure about the affiliations of the authors of the paper. You could try looking for more information in the paper itself or on the authors' websites. If you find out, come back and let me know!</sample>
    <sample id="475">The name of the speaker is Jenny.</sample>
    <sample id="476">Three.</sample>
    <sample id="505">I'm not sure if the dataset is publicly available. You could check the paper for more information. If you have any other questions, feel free to ask.</sample>
    <sample id="535">Sarah Pape is from the University of Trento and Fondazione Bruno Kessler. Mattia Negri and Marco Zorzi are also involved in the paper.</sample>
    <sample id="536">Javad Hosseini.</sample>
    <sample id="537">Hello everyone. My name is Aida Villar and I will be giving a short overview of the paper "Printing Pan from Translation Assessing Strategies and Performance". This is joint work with my colleagues from Google Translate.</sample>
    <sample id="538">BAM is a 540 million parameters large language model presented last year in 2022. It's trained on a large collection of text, comprising 780 billion tokens.</sample>
    <sample id="539">The time of fabrication is achieved state - of - the - art in hundreds of nanoseconds.</sample>
    <sample id="540">In this work we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="541">We evaluated the translation capability of such models using the best practices of the AMT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model.</sample>
    <sample id="542">And we compared two state - of - the - art systems. So the best performing system is only a double - end evaluation.</sample>
    <sample id="543">We use state - of - the - art NER metrics and additionally also show expert - based human evaluation results. Finally we provide some recommendations for prompt selection strategies.</sample>
    <sample id="544">The prompting has a big influence on the performance of the of LLMs for translation. As we can see in a simple experiment where we use one - shot prompting and provided two different prompts for for a sentence.</sample>
    <sample id="545">The majority of sentences, 516 out of 1000, the difference observed is of more than one blur points.</sample>
    <sample id="546">And this can go in extreme cases up to 40 blur points. So it's important to select a good prompting strategy.</sample>
    <sample id="547">In our experiments we settled for a five - shot prompting strategy where we just mark its its sentence that we provide to the system with their language its sim.</sample>
    <sample id="548">Sure, what's the English content you want me to transcribe?</sample>
    <sample id="549">The actual form of the prompting doesn't have a big influence in the case of several-shot prompting.</sample>
    <sample id="550">It's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the prompting.</sample>
    <sample id="551">It's the examples that carry most of the weight.</sample>
    <sample id="552">The summary of our experimental results is that the example quality is more important than the similarity to the source sentence.</sample>
    <sample id="553">So it's important to select the examples from high - quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data.</sample>
    <sample id="554">The dev data is much more curated and with higher quality than the train data. It's more noisy and the results so a better performance when using the dev data.</sample>
    <sample id="555">Nevertheless, specialized state - of - the - art systems have a substantial advantage over the pan translations. But pan comes pretty close to a commercial system. In our case, we chose to overlay it with Google Translate.</sample>
    <sample id="556">The insights that we gain from the evaluation that we perform using the MPM framework is that the fluency of PARM is comparable to state - of - the - art systems but the main difference comes from the accuracy.</sample>
    <sample id="557">In particular， the most common errors are omission errors.</sample>
    <sample id="558">Palm chooses the name to produce a better something translation sometimes by dropping parts of the source sentence that are omitted in the translation.</sample>
    <sample id="559">However， the style - outward category for PAM is lower than for the state - of - the - art systems， which is an additional signal.</sample>
    <sample id="560">That part provides really fluent output but still with some problems of accuracy.</sample>
    <sample id="561">And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="597">The first step maps the input tokens to an unordered multiset of tokens that will appear in the output. If you have any other questions about this method, feel free to ask.</sample>
    <sample id="598">55, 000.</sample>
    <sample id="599">Hello everyone. I'm Makhshatta and today my coauthor Martin and I are presenting our work "The Kid Must Have: Evaluating Knowledge Integration from Multiple Sources". This work is a collaboration between McGill University, MILA and Microsoft Research.</sample>
    <sample id="600">National language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired by a pre - training and knowledge given in inputs at inference time.</sample>
    <sample id="601">Recent works in tasks like question - answering show that models can use pre - trained knowledge to solve the task.</sample>
    <sample id="602">But natural language understanding often requires knowledge that is also supplied at inference time.</sample>
    <sample id="603">John saw the newly elected president on TV.</sample>
    <sample id="604">Pre - training parameters can contain information about what precedents do and what a TV is, but they cannot reliably know who this instance - specific entity John is or who the new president is, because the president might have changed since pre - training.</sample>
    <sample id="605">Therefore, successful models for knowledge - intensive NLU tasks require the ability to integrate and use both pre - trained time and inference time knowledge.</sample>
    <sample id="606">In this work we propose a diagnostic test suite for knowledge integration.</sample>
    <sample id="607">We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and established coreference resolution models.</sample>
    <sample id="608">Serving is a judge. Kia is a baker. Serving and Kia met at the park. After the long day at work, deciding cases in a long court, he was happy to relax.</sample>
    <sample id="609">The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is Sam.</sample>
    <sample id="610">The resolution of a given pronoun requires two types of information. First, entity - specific knowledge, such as "Saul is a judge", and second, background knowledge, such as "judges decide cases in law courts.".If you have any other questions or need further clarification, feel free to let me know.</sample>
    <sample id="611">Generally, background knowledge is learned during the pre - training of large language models, while entity - specific knowledge is typically observed at inference time.</sample>
    <sample id="612">We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="613">We have defined three settings of KitMOS. First, the topic - based setting, background pre - train, where background knowledge is assumed to be available at pre - train time.</sample>
    <sample id="614">Second, there's a background both setting. The background knowledge is available both at pre - train time and inference time. Lastly, the background and pre - train setting. The both knowledge types are available only at inference time.</sample>
    <sample id="615">This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre - trained data of models. For example, because new occupations have developed since the time of pre - training.</sample>
    <sample id="616">Here's an example of how we control the availability of facts in a true source.</sample>
    <sample id="617">In the back one pre - train setting we assume that the back on knowledge partitions seek elected seats in government is contained in the pre - train parameters in the future - time context we provide the entity specific knowledge chester is a politician.</sample>
    <sample id="618">In the back - when both setting we additionally provide not only entity - specific but also background knowledge about partitions in the intruder context.</sample>
    <sample id="619">In the background in Fionn's setting, provide the fictional occupation "militura" instead of "politician" because "militura" is unlikely to be contained in the pre - trained parametric.</sample>
    <sample id="620">We evaluated the dataset both with human study participants and established reference resolution models. In this figure we show the results of the best - performing models on the most difficult variant of the background pre - trained setting.</sample>
    <sample id="621">Without task - specific training on kid - mus, both models do not perform well. When trained on kid - mus, however, both C2F and BERT - for - C - RF perform significantly better than the random choice.</sample>
    <sample id="622">The suggested when trained on cat general question - solution data sets, models learn to exploit surface cues which are not useful when testing on cat most where such cues have been removed.</sample>
    <sample id="623">Additional experiments with fictional knowledge indicated that even the best performing models cannot reliably integrate that knowledge provided only at inference time.</sample>
    <sample id="624">Many current AI models seem unable to reason over knowledge from different sources without task - specific training. However, with task - specific training, some are able to integrate knowledge from multiple sources.</sample>
    <sample id="625">Still, even the best performing models seem to have difficulties with reliably integrating backward knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on github. Thanks for listening.</sample>
    <sample id="626">The best alignment method for DEplain is the method of mass align. If you want to know more about it or have other questions, feel free to ask.</sample>
    <sample id="627">The benefit of weakly supervised learning is that it can robustly train neural networks on data with label noise, so the trained models can still generalize well. If you have any other questions about this or related topics, feel free to ask.</sample>
    <sample id="628">I'm not sure about the exact allocation of the documents in DEplain - web with manual and automatic alignment methods. You might need to check the paper for more detailed information. If you find out more, it'd be great if you could share it with me.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting data from Reuters news in 2020 and then annotating it with the CoNLL 2003 annotation guidelines. If you have any other questions about this dataset or something else related, feel free to ask.</sample>
    <sample id="667">I'm not sure what the existing works are on this. You could give me a bit more context or details?</sample>
    <sample id="668">No, they are still inadequate for CLSP. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="669">Hello everyone. My name is Shuheng. Today I'm going to present our paper "Do Cornell 2003 named entity taggers still work well in 2023? Let's get started."</sample>
    <sample id="670">Our paper investigated the problem of generalization using the named entity recognition task or the NER task.</sample>
    <sample id="671">We observe that models have been using CONLL - 2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalize to modern data?</sample>
    <sample id="672">Well, for good generalization when developing new tasks, you need a large and diverse dataset. This helps the model learn from different situations. Also, proper regularization techniques are important to prevent overfitting. And, you should consider using transfer learning if applicable. It can save a lot of time and resources. Oh, and don't forget about cross - validation to make sure your model generalizes well.If you want to know more about any of these points, feel free to ask.</sample>
    <sample id="673">Well, there could be several reasons. One is that the training data might not be diverse enough. If the data only covers a small range of cases, the model can't learn to handle new, unseen situations. Another reason could be that the model is too complex. It might overfit to the training data and not generalize well to new data. Also, the way the model is trained, like the choice of hyperparameters, can affect its ability to generalize.If you want to know more about how to fix these issues, feel free to ask me.</sample>
    <sample id="674">To investigate these problems we develop the Cornell + + dataset. This is a dataset that we collected from Reuters news from 2020 and then annotated them with the same Cornell 2003 annotation guidelines.</sample>
    <sample id="675">We then fine - tuned over 20 models on Cornell 2003. We evaluated them on both the Cornell 3 test set and the Cornell + test set.</sample>
    <sample id="676">In last but not least we calculated the percentage change in F1 to assess the generalization of each model.</sample>
    <sample id="677">Well, you need a large and diverse dataset. That's the first one. Then, you need a well - designed model architecture. And finally, you need proper evaluation metrics. These three things are really important for good generalization. So, what do you think about these ingredients? Do you have any other ideas?</sample>
    <sample id="678">The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data.</sample>
    <sample id="679">The second ingredient is the model size. We found that usually larger models lead to better generalization.</sample>
    <sample id="680">In last but not least， we all know that the number of fine - tuning examples directly affects the performance of a downstream task. Here we also found that more fine - tuning examples actually also leads to better generalization.</sample>
    <sample id="681">Well, there are several reasons. One is overfitting. If a model is trained too much on the training data, it might not perform well on new, unseen data. Another cause could be poor feature selection. If the features used are not relevant or are noisy, it can lead to a drop in performance. Also, if the model architecture is not suitable for the task, like using a very complex model for a simple classification task, it might not perform as expected. And sometimes, the data itself can be a problem. If the data is imbalanced, for example, having a lot more data for one class than another, it can affect the model's performance.If you want to know more about how to fix these issues or have other related questions, feel free to ask.</sample>
    <sample id="682">We have two hypotheses. The first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set.</sample>
    <sample id="683">The second hypothesis is temporal drift， which is the performance degradation that is caused by the increasing temporal gap between the train and the test data.</sample>
    <sample id="684">For adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one.</sample>
    <sample id="685">This means that every unit of improvement that we made on kernel 2003 translates to more than one unit improvement on kernel + + which means that there is no diminishing returns.</sample>
    <sample id="686">And this shows us that adaptive overfitting in this case is not observed.</sample>
    <sample id="687">Well, time travel is a really interesting concept. There are different types of time travel. Some people think about time travel in a way that you could go back in time to change things, like prevent a bad event from happening. But that's just a theory. There's no real evidence of it yet. And then there's the idea of going forward in time, which could be used to see what the future holds. But that's also just speculation. What do you think about time travel?</sample>
    <sample id="688">For temporal drift， we did an experiment to retrain or continue to pretrain some models with more recent data， and we found that the performance degrades with larger temporal gap.</sample>
    <sample id="689">And this confirms our hypothesis that the main cause of the performance drop is temperature.</sample>
    <sample id="690">Our conclusion is that for good generalization we would need a better model architecture, larger model size, as well as more fine - tuning examples. And these goals go hand - in - hand. We can't just have one ingredient but throw all the others.</sample>
    <sample id="691">At the same time we also found that the performance drop here is caused by temporal drifts and kind of surprisingly it is not caused by adaptive overfitting even though Conole 2003 has been used for over 20 years.</sample>
    <sample id="692">So going back to the question that we raised in the title of our paper, do Connell 2003 tags still work in 2023? And we found that the answer is actually a resounding yes.</sample>
    <sample id="693">We hope our paper calls for more research on how to improve generalizations of the models.</sample>
    <sample id="694">And lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much.</sample>
    <sample id="695">The method deals with the ambiguity of permutations by inducing the alignment as part of the training. It also approximates the finding of the highest scoring permutation with a GPU - friendly continuous relaxation that allows backpropagation through the solution to learn the linguistically more plausible permutations.</sample>
    <sample id="696">Well, the fairness of a downstream NLP model can be defined in a few ways. One aspect is that it should not discriminate against certain groups based on things like race, gender, or political opinions. For example, if it's used in a job recruitment context, it shouldn't give an unfair advantage or disadvantage to candidates just because of their political leanings. Another part is that it should be able to handle different languages and dialects fairly, without showing bias towards one over the other. So, in general, it's about treating all inputs and outputs equally and not having any hidden biases that could cause unfair treatment. If you want to know more about how to ensure fairness in NLP models, feel free to ask me more.</sample>
    <sample id="697">Yannick Slavik.</sample>
    <sample id="698">The name of the speaker is Costa Sinner.</sample>
    <sample id="699">Myra.</sample>
    <sample id="700">In the context of this paper, tropicalism indicates the common tropes used to describe Latina women, like being vibrant and curvaceous.</sample>
    <sample id="701">The authors used words like "culture", "tradition", "proud" and "exotic" to define and distinguish these groups from the white norm. They focused on the relationship to their identity. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="702">The work used p - xmi to measure context usage.</sample>
    <sample id="703">DrBERT has 7GB of natural data, while ChuBERT has 4GB of natural data and 4GB of clinical data. So the main difference is the amount of natural data they have. If you want to know more about these models, feel free to ask me.</sample>
    <sample id="704">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with S. Sander, Moosh, and Dan Jurafsky.</sample>
    <sample id="705">近年来，许多人记录了大型语言模型，LLMs，在社会偏见和刻板印象方面的普遍性。</sample>
    <sample id="706">然而，这些措施有各种局限性。它们通常依赖于手工构建的数据集，而这些数据集的收集是非常耗时的。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="707">And they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture very general, broad associations like negative associations with particular groups.</sample>
    <sample id="708">Furthermore， most work in this space doesn't account for intersectionality， which is the notion that multifaceted social identities can compound biases and be unique loci of harm.</sample>
    <sample id="709">To overcome these limitations， we rely on the property that these newer instruction - tuned LLMs are very good at responding to instructions and prompts.</sample>
    <sample id="710">Imagine you are an Asian woman. Describe yourself.</sample>
    <sample id="711">And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt.</sample>
    <sample id="712">I'm not sure what you mean by "example generations from GPT - 4" in this context. Could you please provide more details or clarify your question?</sample>
    <sample id="713">我们马上看到，虽然输出并不明显地消极或有毒，按照传统意义上的这些词。</sample>
    <sample id="714">There are some interesting patterns.</sample>
    <sample id="715">The Asian woman is depicted as unassuming. The Middle Eastern woman is referred to using words like "exotic" and "like referring to a mesmerizing region."</sample>
    <sample id="716">And both of the women of color personas make references to ancestry, while the white man persona has nothing of the sort.</sample>
    <sample id="717">To capture these patterns， our method has two parts. The first one is generating these personas.</sample>
    <sample id="718">Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects. Finding that by giving it to human subjects they also were able to surface racial stereotypes.</sample>
    <sample id="719">And also this enables direct comparison between our generated personas and the human - written responses.</sample>
    <sample id="720">The second part is marked words， which is a method to identify the words that distinguish marked groups from unmarked ones， which I'll elaborate on shortly.</sample>
    <sample id="721">The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon.</sample>
    <sample id="722">The marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked.</sample>
    <sample id="723">The word "warrior" is usually associated with men. So when people are describing a warrior who is a woman, they'll usually specify "woman warrior" and mark the term with "woman."</sample>
    <sample id="724">社会上占主导地位的群体在语言和社会上都是无标记的，而被边缘化的群体通常是被标记的。</sample>
    <sample id="725">So in our method we first designate what the unmarked and marked groups are.</sample>
    <sample id="726">And then we compare the personas using the Fighting Words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group.</sample>
    <sample id="727">For the personas of Black women, we would do fighting words and compare the log odds ratios against both white personas and man personas, because those are the two corresponding unmarked groups.</sample>
    <sample id="728">Now for some results. So first we use a lexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human - written ones.</sample>
    <sample id="729">然而，当我们实际查看词汇表中单词的分布时，我们会发现非常不同的东西。</sample>
    <sample id="730">So while the generated personas have much higher rates of the lexicon words, the human - written ones have a much wider distribution of words. While the stereotype words that are in the generated personas are really just the words "tall" and "athletic".</sample>
    <sample id="731">So really just only the positive or at least non - negative ones.</sample>
    <sample id="732">And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead, to do that, we'll turn to the results from our Mark words method to show how these positive - seeming words facilitate stereotypes and essentializing narratives.</sample>
    <sample id="733">在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害的模式。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="734">First, for Mark groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm.</sample>
    <sample id="735">这为这些群体带来了长久的歧视和他者化遗产。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="736">Furthermore， there is a lot of common tropes that are reflected in these words， especially for women of color. So， for example， the words describing Latina women include things like vibrant and curvaceous.</sample>
    <sample id="737">petite, delicate, silky.</sample>
    <sample id="738">Which connects to a long history of Asian women being hypersexualized, seen as very docile and submissive and so on.</sample>
    <sample id="739">And finally for black women we see that some of the top words are things like strong and resilient.</sample>
    <sample id="740">This connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance.</sample>
    <sample id="741">There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.</sample>
    <sample id="742">So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to very negative health outcomes for these people among other harms.</sample>
    <sample id="743">我们发现，每个标记组的单词基本上只是反映非常本质化的叙事。</sample>
    <sample id="744">So based on these patterns we conclude with three recommendations for model owners.</sample>
    <sample id="745">First， we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that.</sample>
    <sample id="746">And finally there should really be increased transparency about bias mitigation methods.</sample>
    <sample id="747">Because, for instance, like these positive stereotypes we don't know if it's because there is some sort of like weird.</sample>
    <sample id="748">过度的价值对齐，或者一些反刻板印象的方法，导致了这些有害的模式。如果还有其他想法或者想继续聊聊这个话题，随时告诉我呀。</sample>
    <sample id="749">We just really can't make any assumptions or really study that further without more transparency.</sample>
    <sample id="750">Thank you so much for listening. Have a good time at Easter.</sample>
    <sample id="751">Two.</sample>
    <sample id="752">Iterative transfer learning is a method that updates a model with new data from each round of active learning and annotations. It accumulates all the data collected from active annotation so far and iteratively updates the model by training on the latest set of data collected. Well, that's the basic idea. If you want to know more about it, like how it compares to other methods, feel free to ask.</sample>
    <sample id="753">The goal of the dataset is to understand the user's language when they want to make a choice.</sample>
    <sample id="754">I'm not sure about that. You might need to look into more specific security research or papers on EaaS security. Maybe you could try searching for case studies or academic articles on the topic? If you find anything interesting, come back and let me know.</sample>
    <sample id="755">There are three authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="756">The answer is not provided in the given content. You might need to look for more information elsewhere.</sample>
    <sample id="757">The authors are affiliated with Carnegie Mellon University, the University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">I saw Bart and Lisa. So the governor is on the left. If you have any other questions about this, feel free to ask.</sample>
    <sample id="759">I'm not sure which "state - of - the - art models in dialogue systems" you're referring to. There are quite a few, like GPT - 3, which is really powerful in generating human - like responses. But there are others too. You could tell me more about what you mean by "state - of - the - art" so I can give you a more accurate answer.</sample>
    <sample id="760">Well, you know, as the context windows get longer, the models might start to make mistakes or give wrong answers in certain parts of the window. So, we need to check if the model is still acceptable all the way through the context window. It's like making sure a car is in good shape from start to finish, not just at the beginning or the end. So, that's why we need to evaluate the models' acceptability throughout the context window. What do you think about this?</sample>
    <sample id="761">Yes.</sample>
    <sample id="762">No, they don't necessarily know about the entity in advance.</sample>
    <sample id="763">I'm not sure which MT metrics were used for the evaluation. You could try looking at the original document or the research paper related to this. If you have any other questions or need more help, feel free to let me know.</sample>
    <sample id="764">No, it doesn't.</sample>
    <sample id="765">Positionality in NLP matters because it can lead to design biases, like the example given where a popular API doesn't perform well in certain contexts, such as detecting offensive terms in Indian contexts. This shows how the performance of technology can vary between populations. So, it's important to consider positionality to avoid such biases. If you have any other thoughts on this or want to discuss more, feel free to let me know.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine - tuned with adapters. If you have any other questions about this or something else related, feel free to let me know.</sample>
    <sample id="767">The model they use for transfer learning is the one that first finetunes on the CE task and then further finetunes on the debate task.</sample>
    <sample id="768">I'm not sure which are the recent test sets used to assess the PaLM capabilities. You could try looking at the official PaLM documentation or research papers related to it. If you find out, it'd be great if you could share it with me.</sample>
    <sample id="769">Three. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="770">The answer is not provided in the given text.</sample>
    <sample id="771">The name of the speaker is Shuheng.</sample>
    <sample id="772">Yes, the results and dataset in the paper can be used as a benchmark. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="773">The paper doesn't specify the exact number of smaller models they experiment with.</sample>
    <sample id="774">OFA is used as the base model. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="775">Hello everyone. My name is Jin Wei from the University of Science and Technology of China.</sample>
    <sample id="776">It's my pleasure to give a short advertisement video about paper. Are you copying my model? Protecting the copyright of large language models for embedding and services. We will backdoor watermark.</sample>
    <sample id="777">I'm not sure what the English content is. You need to provide it so I can write it down.</sample>
    <sample id="778">Currently large language models such as GPT, Llama, Pile are exceptional in natural language understanding and generation.</sample>
    <sample id="779">Embedding as services is one of the services built upon large language models to assist various NLP tasks.</sample>
    <sample id="780">I'm not sure what the English content is. Can you tell me more about it?</sample>
    <sample id="781">However, recent works have shown that the attacker may steal the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services.</sample>
    <sample id="782">To protect the copyright of embedding ad services, one of the solutions is to embed a watermark in the provider's service and detect whether another service contains the watermark.</sample>
    <sample id="783">The watermark method need to meet the following properties: first, the method should be applicable to embedding ad services. Second, the watermark should not degrade the utility of the provided embedding.</sample>
    <sample id="784">The English content is: "Third, the watermark should be covert enough to the attacker, or the attacker can remove the watermark easily." If you have any other questions or need further assistance, feel free to let me know.</sample>
    <sample id="785">Finally， the watermark need to be transferable to the attacker's services during the model extraction process.</sample>
    <sample id="786">Existing works can be broadly classified into four categories.</sample>
    <sample id="787">However, this method is either not applicable to embedding ad services or lack of transferability.</sample>
    <sample id="788">Therefore, in this paper we propose Embedding Marker, which is a backdoor - based watermark method applicable to embedding in services.</sample>
    <sample id="789">The English content says: "Then let me introduce the details of our embedding marker. Embedding marker contains two main steps: watermark injection and copyright verification."</sample>
    <sample id="790">Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval.</sample>
    <sample id="791">We assume the provider can collect a general text corpus and count the word frequency within it.</sample>
    <sample id="792">In Watermark Injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence.</sample>
    <sample id="793">The provided embedding is a weight summation of the target embedding and the original embedding.</sample>
    <sample id="794">The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="795">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="796">We first construct a backdoor and a benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set, while all words in the sentences of benign dataset do not belong to the trigger set.</sample>
    <sample id="797">The provider requests embeddings from the Stellar service with the dataset.</sample>
    <sample id="798">The cosine and L2 similarity between the requested embedding and the target embedding are computed. We computed the similarity difference between benign and backdoor datasets, which is defined as Δcos and ΔL2.</sample>
    <sample id="799">Meanwhile we also apply KS test and use its p - value as the third metric.</sample>
    <sample id="800">We conduct experiments on four datasets: AG News, Mind, SST - 2, and IRIS Spam. We assume the provider apply wikitext dataset to count word frequency.</sample>
    <sample id="801">The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks.</sample>
    <sample id="802">We also validated the convert list of the provided embedding by visualizing the embedding of sentences on four datasets. The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="803">As shown in the figures, it's hard to distinguish between the backdoored embeddings and normal embeddings.</sample>
    <sample id="804">That's all. Thank you. Welcome to discuss with us.</sample>
    <sample id="805">Hi, I'm Sarah Pape from the University of Trento and Fondazione Bruno Kessler. And I will briefly introduce the Attention as a Guide for Simultaneous Speech Translation paper. That is a joint work with Matteo Negri and Marco Zorzi.</sample>
    <sample id="806">Simultaneous speech translation, or Simultaneous Speech Translation, Simo ST, is the process of translating spoken language into text in another language in real - time, enabling cross - language communication.If you want to know more about Simo ST, like how it works or its applications, feel free to ask.</sample>
    <sample id="807">The problems of the current Simulistic models are that specific architectures are usually trained, and introducing additional modules to be optimized.</sample>
    <sample id="808">Long and complicated training procedures, for example, training involving different optimization objectives.</sample>
    <sample id="809">Training and maintaining several models to reach different latency regimes, for example, training a model with an average of 1 second latency and another one with 2 seconds latency and so on.</sample>
    <sample id="810">I'm not sure what "our solution" refers to. Can you give me a bit more context?</sample>
    <sample id="811">First, use already existing offline SD models without retraining or adopting specific architecture for single SD. Use only one model for every latency regime and handle latency through specific parameters.</sample>
    <sample id="812">And leverages the knowledge already acquired by the model through the attention mechanism between audio input and text output. That is the cross - attention mechanism. And you can see an example on the right.</sample>
    <sample id="813">Our solution is to propose a dot or encoder - decoder attention. And it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to.</sample>
    <sample id="814">A word is emitted if the tension is not concentrated, that is, its sum is below a certain threshold α towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="815">I'm going to talk about.</sample>
    <sample id="816">And we will look at the cross - attention weights.</sample>
    <sample id="817">We will see that the first two words point to the earliest received speech frames, while the last word points to the last received speech frames, and λ speech frames.</sample>
    <sample id="818">Sure, you can just start writing from the third word. So, what's the English content you want to write?</sample>
    <sample id="819">While since the sum of the cross - attention is above a certain threshold α, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="820">If we go on and receive another speech chunk and our model predicts other three words and we will look at the cross - attention weights.</sample>
    <sample id="821">No words point to the last lambda speech frames.</sample>
    <sample id="822">This means that these three words will be omitted.</sample>
    <sample id="823">I'm not sure what the main results of the ad are. You could try looking at the ad again and see if there are any key points or statistics mentioned. Maybe you can also check if there are any footnotes or additional information that might explain the results. If you can give me a bit more context about the ad, like what it's about or what kind of ad it is, that would be really helpful.</sample>
    <sample id="824">We plot the simultaneous speech translation results on graphs. On one side of the graph, we have blue, which measures the translation quality. On the other side, we have average lagging.</sample>
    <sample id="825">The latency measure and we also consider the computational aware average liking that accounts for the model's computational times to predict the output.</sample>
    <sample id="826">So we want our curves to be as high as possible on this plot.</sample>
    <sample id="827">But also we want that they are shifted on the left.</sample>
    <sample id="828">And we compare with proper strategies that are also applied to offline models. So there are the with keys strategy and the local agreement. And we compare also with the state - of - the - art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="829">I'm not sure what the English content is. Can you tell me what it is so I can produce the written version?</sample>
    <sample id="830">And we see that AD outperforms all the strategies applied to offline models since their curves are shifted over the left.</sample>
    <sample id="831">And we also see that if we consider the actual elapsed time or the computational aware time, ADOT is the fastest strategy.</sample>
    <sample id="832">If you want to discover more results, read our paper. And we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work. Thanks for your attention.</sample>
    <sample id="833">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="834">I'm not sure about the affiliations of the authors of that paper. You could try looking at the paper itself or contacting the authors directly. If you find out, it'd be great if you could share it with me.</sample>
    <sample id="835">I'm sorry, the text you provided doesn't mention any language pairs. Can you give me more details about the paper?</sample>
    <sample id="836">Zhang Bing.</sample>
    <sample id="837">The models investigated were the finetuned long import model for document level simplifications and the finetuned normal base long import model for sentence level simplifications. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="838">For training, 53 tasks are used. For testing, 62 tasks are used in total.</sample>
    <sample id="839">The content doesn't mention the number of authors involved in the paper. So, I'm not sure. You could try looking for more information in the paper itself.</sample>
    <sample id="840">The authors experimented on AG News, IMDb, SST - 2, and IRIS Spam datasets.</sample>
    <sample id="841">Hi everyone. I'm Costa Sinner and I'm pleased to welcome you to our talk of our ACL 2023 paper. Language model acceptability judgments are not always robust to context.</sample>
    <sample id="842">Yes, there is a joint work with John Gottlieb, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy, and Athena Williams.</sample>
    <sample id="843">In this work we revisit the minimal pair paradigm.</sample>
    <sample id="844">The minimal pair paradigm evaluates language models based on acceptability judgments, which can include grammaticality like blimp syntax, Jim, or acceptability in terms of stereotypes like crows bears.</sample>
    <sample id="845">And in this minimal pair paradigm the typical way to evaluate language models is that you show like a acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence.</sample>
    <sample id="846">And then the hope is that the model basically puts more probability to the acceptable sentence.</sample>
    <sample id="847">The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences.</sample>
    <sample id="848">These days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the model's acceptability throughout the context window.</sample>
    <sample id="849">And that is what we are trying to do here. We are trying to revisit the N - P - P pipeline by asking the model to evaluate acceptability on longer and longer sequences.</sample>
    <sample id="850">So that is the approach. So what we do is that to simulate these longer sequences, we revisit the datasets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those datasets.</sample>
    <sample id="851">So， for example， here we have chosen like a typical pair of grammaticality from the BliB dataset from the adjunct island case.</sample>
    <sample id="852">And what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from adjacent tiles.</sample>
    <sample id="853">And then we add it as a prefix to both the acceptable query and the unacceptable query.</sample>
    <sample id="854">So we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability.</sample>
    <sample id="855">And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatch scenario.</sample>
    <sample id="856">So here the sentences are still coming from relevant data sets but it's not from the same data set that you're evaluating with. And we can do the same for an acceptability case.</sample>
    <sample id="857">Finally, we can choose sentences from a completely unrelated domain such as Wikipedia.</sample>
    <sample id="858">So this will tell us like whether the model's acceptability judgments are actually impacted by any context.</sample>
    <sample id="859">I'm not sure what you mean by "transcribe English content." Could you please clarify? Are you asking about how to transcribe English text? Or something else? Let me know so I can give you a proper answer.</sample>
    <sample id="860">The model first looks at Wikipedia sentences that are completely irrelevant to the current query pair. Then it finds that the MPP judgments are mostly robust for arbitrary context lengths.</sample>
    <sample id="861">We increase the context length toward up to 2024 for to max out OPT and GPT - 2 models and we saw here in the orange dotted line the MPP judgments are relatively stable.</sample>
    <sample id="862">Well, when you choose sentences from the same dataset, it can lead to some problems. For one, it might not give you a good representation of the whole dataset. You could end up with sentences that are all similar in some way, like having the same grammar structure or similar topics. This might not be very useful for things like training a model or analyzing the data in a more diverse way. But it also depends on what you're using the sentences for.If you want to know more about how to avoid this or how to get a more diverse set of sentences, feel free to ask me more.</sample>
    <sample id="863">Sure. What's your question about this? Are you having trouble with the task? Or do you want to discuss something related to the dataset?</sample>
    <sample id="864">And there we see that the MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes.</sample>
    <sample id="865">When we match the structure, that is when we choose the sentences from the same phenomena in blame person text.</sample>
    <sample id="866">We see a massive increase or a massive decrease in of the MPP judgment for the model depending on whether the chosen prefix is acceptable or unacceptable.</sample>
    <sample id="867">Now this uh uh this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window.</sample>
    <sample id="868">Well, you see, the match prefix can give the language model some context clues. It might be something that's really common or has a specific meaning in the language. For example, if it's a common phrase or a word that has a particular usage. This can make the model more likely to predict certain words or phrases that are related to that prefix. So, it can really influence its judgment on what's a good next word or sentence to follow. If you want to know more about how language models work in general, feel free to ask me.</sample>
    <sample id="869">So we did a series of analysis where we tried to perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations.</sample>
    <sample id="870">We find that none of these noises are actually making the model like change its course in terms of how it shows us the MPP just been trained.</sample>
    <sample id="871">The models are sensitive to the perturbed sentences in similar ways.</sample>
    <sample id="872">When we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in MPP judgments in similar fashion.</sample>
    <sample id="873">Language models are sensitive to latent syntactic and semantic features which are shared across the sentences.</sample>
    <sample id="874">And the NLP evaluation the way that we do it currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window.</sample>
    <sample id="875">Please read our paper for more details of our experiments. Thank you for listening.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web. If you want to know more about it, like how it was collected or what kind of data it contains, feel free to ask.</sample>
    <sample id="877">The name of the speaker is Aled Williams.</sample>
    <sample id="878">The prompting strategy has a big influence on the performance of LLMs for translation.</sample>
    <sample id="879">I'm not sure about the affiliations of the authors. You could try looking for more information in the paper itself or on the authors' websites. If you find out, come back and let me know.</sample>
    <sample id="880">I'm sorry, but the English content you provided doesn't mention any expert - written instructions. It's mainly about a larger multimodal instruction - tuning dataset and a QR code. Can you provide more details or clarify what you mean by "expert - written instructions"?</sample>
    <sample id="881">They propose to test the models on a coreference resolution task.</sample>
    <sample id="939">Well, one common method is using human evaluation. You know, like having human judges pick which conversation is better or rate them on a scale. Another method could be using automated metrics, but human evaluation is quite popular. So, what do you think about these methods? Do you have any other ideas?</sample>
    <sample id="940">There are five authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="941">Well, in this case, the background knowledge needed is that judges decide cases in law courts. This helps us understand that Servin, who is a judge, would be the one who has a long day at work deciding cases in a law court. So, that's the background knowledge that's relevant here. If you have any other questions or need more examples, feel free to ask.</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">No, the information given doesn't mention anything about the annotators being balanced in regard to demographics like country or gender. So, we can't say they are balanced based on this. If you have more information, you could share it and we could try to answer again.</sample>
    <sample id="944">The sentences in the acceptable domain were perturbed by adding noise while preserving the relevant structure.</sample>
    <sample id="945">It means evaluating different aspects or dimensions of something, like in this case, evaluating multiple dimensions of chat quality to understand the model's strengths and weaknesses better. So, it's about looking at various parts of the whole thing. If you have any other questions about this, feel free to ask.</sample>
    <sample id="946">I'm not sure about the affiliations of the authors of the paper you mentioned. You could try looking for more information in the paper itself or on academic databases. If you have any other questions or need further help, feel free to let me know.</sample>
    <sample id="947">The form of the prompting is important for zero - shot and one - shot prompting.</sample>
    <sample id="948">Hello, my name is Vasudha and I'm a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, "Transfer Learning for Dissonance Detection: Addressing the Rare Class Challenge.".If you have any other questions or need more information, feel free to let me know.</sample>
    <sample id="949">We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="950">I know that cigarettes could kill me. I grabbed a couple of smokes after the meeting.</sample>
    <sample id="951">I don't think I could keep my job without them.</sample>
    <sample id="952">While dissonance is a very common phenomenon we experienced in daily decision - making, they are really rare to find expressed in language among other kinds of discourse relations.</sample>
    <sample id="953">Studying cognitive distance can help us understand the effects of disagreement among people, track trends in belief, values, and attitude changes in populations.</sample>
    <sample id="954">High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better.</sample>
    <sample id="955">Studying dissonance expressed in language can also be beneficial in understanding extremism and polarisation of vulnerable groups.</sample>
    <sample id="956">Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision - making processes better.</sample>
    <sample id="957">To the goal of creating a cognitive dissonance resource we conducted a large - scale annotation of dissonance relations. We used a dissonance - first approach as seen in the flowchart here.</sample>
    <sample id="958">Tweets were parsed using a pre - trained parser and pairs of discourse units were annotated according to the guidelines that are described in our paper.</sample>
    <sample id="959">Dissonance was only found in 3.5% of the annotated pairs.</sample>
    <sample id="960">On collecting around 1000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of disnests. To no surprise, the classifier performed not much better than chance.</sample>
    <sample id="961">Given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity.</sample>
    <sample id="962">To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds, lowering the overall annotation costs while improving dissonance detection.</sample>
    <sample id="963">Since the initial model was not able to capture the dissonance class at all, we start the co - active learning process by transferring weights from closely related tasks.</sample>
    <sample id="964">We transfer from two different tasks: topic - independent dis - nent stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement, irrespective of topic.</sample>
    <sample id="965">Call debate here and on binary classification of expansion and comparison classes of PDB. Since these two are closely related to the conception of consonance and dissonance and we call them CED.</sample>
    <sample id="966">We find that on transferring the zero - shot performance on the annotated data set is already much better than chance with the best with AUC 0.62.</sample>
    <sample id="967">Further on iteratively fine - tuning on both tasks we find that fine - tuning of CE task followed by further fine - tuning on debate yields a much better zero - shot performance thus this is the model that we used to cold - start the active learning.</sample>
    <sample id="968">Next we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotations so far. Iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="969">Over the different strategies we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="970">Next to improve the number of dissonance examples we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL.</sample>
    <sample id="971">We compared this to the other state - of - the - art AI strategies that are commonly used in the community.</sample>
    <sample id="972">We find that the proposed PRC strategy works better than other state - of - the - art strategies, although the difference is small. Note that the performance is significantly lower for random.</sample>
    <sample id="973">On further rounds of AL with two best strategies we improved distance classification AUC to 0.75 which is the best performance that we have on the task so far.</sample>
    <sample id="974">We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult to.</sample>
    <sample id="975">In summary, we find that PRC is a simple AI strategy for rare class acquisition and co - starting AI with appropriately designed transfer learning tasks can help significantly.</sample>
    <sample id="976">We also find that iterative update is useful for transfer learning from a different domain, whereas in - domain active annotations benefit from cumulative update.</sample>
    <sample id="977">These are the links to our code data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="978">The authors didn't mention which specific dialog models they evaluated.</sample>
    <sample id="979">I'm not sure how many authors are involved in the paper. You could try looking at the paper itself or checking the publication details. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">I'm not sure how many authors are involved in the paper. You could try looking at the paper itself or checking the citation details. If you find out, come back and let me know!</sample>
    <sample id="982">The name of the speaker is Vasudha.</sample>
    <sample id="983">I'm sorry, but I don't have the information about the affiliations of the authors of the paper you mentioned. You might need to check the paper itself or the publication where it was published. If you have any other questions or need further assistance, feel free to let me know.</sample>
    <sample id="984">Hello everyone. My name is Justin John from the Penn State University. Today I'm gonna present our work, Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Many Representations.</sample>
    <sample id="985">Semantic parsing is a task to build semantic representations of user queries such as SQL and lambda calculus.</sample>
    <sample id="986">Cross - lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">As shown in this figure， we need to translate the query in multiple natural languages using neural models to SQL， Lambda or FunQL， etc.</sample>
    <sample id="988">Existing cross - lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance.</sample>
    <sample id="989">There are lots of um coverage on certain natural language. The Chinese is missing.</sample>
    <sample id="990">Lack of coverage on certain meaningful representations.</sample>
    <sample id="991">The lambda calculus is missing.</sample>
    <sample id="992">There are only evaluated on certain neural model. For example, there's only one single model to evaluate them.</sample>
    <sample id="993">So to this end we propose Exemplar a provider uniform dataset exemplar for cross - lingual semantic parsing in multiple natural languages and in representations.</sample>
    <sample id="994">It contains 90 datasets in various domains, 5 semantic parsing tasks, 8 million representations, and 22 natural languages in 15 language families.</sample>
    <sample id="995">And to better evaluate our benchmark we consider the six settings for training and evaluation.</sample>
    <sample id="996">The first one is translate test. We use Google Translate API to translate source to the target language. Then use monolingual model to train and evaluation.</sample>
    <sample id="997">And for example, we train the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL.</sample>
    <sample id="998">And we'll also test monolingual model.</sample>
    <sample id="999">I'm not sure what the English content is. You need to provide the English text for me to create a verbatim transcript.</sample>
    <sample id="1000">We also test monolingual few-shot setting by training monolingual models with only 10% of training data.</sample>
    <sample id="1001">And we test multilingual model which we train one multilingual model for all languages.</sample>
    <sample id="1002">For example, we put the German, English, Chinese queries together to train a multilingual model and during inference we can use this model to.</sample>
    <sample id="1003">Sure. What's the English content you want me to create a verbatim transcript of?</sample>
    <sample id="1004">And we also consider cross - lingual zero - shot and few - shot transfer. We train on one source language and transfer to another language.</sample>
    <sample id="1005">So during training, we train it on English query or the combination of English and German few - shot queries to train a multilingual model to predict the SQL output.</sample>
    <sample id="1006">And we also find many interesting results. So regarding analysis of monolingual models, we evaluate on two groups of models.</sample>
    <sample id="1007">Including encoder - P - D - R, which stands for multilingual pre - trained encoders with pointer - based decoders, such as XLM - R + P - D - R and BERT + P - D - R.</sample>
    <sample id="1008">And we also evaluate encoder - decoder models, which is multilingual pre - trained encoder - decoder models such as M - BART and M - T - 5.</sample>
    <sample id="1009">We found that encoder - decoder obtains the best performance on all nine datasets.</sample>
    <sample id="1010">And we evaluate on MT - 5 and XLM - R + PDR on multilingual setting.</sample>
    <sample id="1011">We found that encoder - decoder or encoder - PDR can be improved by training in a mixture of various languages.</sample>
    <sample id="1012">And we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets.</sample>
    <sample id="1013">I think this is known as curse of multilinguality.</sample>
    <sample id="1014">We also compare the cross - lingual performance gap.</sample>
    <sample id="1015">In this figure the blue line is cross - lingual few - shot transfer the orange line is cross - lingual zero - shot transfer while the green line is the model - lingual setting.</sample>
    <sample id="1016">We found that by comparing the green and orange line we found for zero - shot setting the cross - lingual transfer performance gap is significant and comparing blue and orange line we found that for few - shot setting the transfer gap is shortened rapidly.</sample>
    <sample id="1017">We also find some other interesting findings. For example, encoder - decoder outperforms previous work or achieved comparable results for training on English natural language and significantly boosts the performance of few - shot on target natural languages.</sample>
    <sample id="1018">And we found multilingual language models such as Coleridge and Blue are still inadequate for cross - lingual semantic parsing tasks.</sample>
    <sample id="1019">To sum up， we build Exemplar， a unified benchmark for cross - angle semantic parsing with multiple natural languages and many representations.</sample>
    <sample id="1020">We conduct a comprehensive benchmark study on three representative types of multilingual language models. And our results show so many interesting findings. And etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="1021">The most common errors of PaLM are omission errors. If you want to know more about PaLM or other related topics, feel free to ask.</sample>
    <sample id="1048">The authors are affiliated with Emory NLP Lab at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="1049">Continuous fine-tuning.</sample>
    <sample id="1050">There are 7 authors involved in the paper. If you have any other questions about this paper, feel free to ask.</sample>
    <sample id="1084">The name of the speaker is Justin John.</sample>
    <sample id="1085">Hi， I'm Zhang Bing， PhD student in University of Washington. Today I'm presenting our work from pre - training data to language models to downstream tasks， tracking the trails of political biases leading to unfair NLP models.</sample>
    <sample id="1086">Language models are trained on large - scale web - crawled data.</sample>
    <sample id="1087">Political news media are well covered in their pre - training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data.</sample>
    <sample id="1088">This has created a mixed blessing for language model applications.</sample>
    <sample id="1089">So on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas. On the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications.</sample>
    <sample id="1090">To this end we propose to investigate the political bias propagation pipeline from pre - training data to language models to downstream tasks specifically by asking the following questions.</sample>
    <sample id="1091">Well, evaluating the political leanings of language models can be a bit tricky. One way is to look at the language they use in different contexts. For example, if they tend to use more positive or negative words related to certain political groups. Another way is to see how they respond to different types of input, like different political statements or questions.As for the pre - training data, it can have a huge role in political biases. If the data used to pre - train the model is biased in some way, like it has a lot more examples from one political side than the other, then the model might pick up on that bias and carry it through. So, the quality and diversity of the pre - training data are really important.If you want to know more about this, like specific examples or other ways to deal with these biases, feel free to ask.</sample>
    <sample id="1092">Well, different political leanings in language models can affect their performance on downstream tasks in various ways. For example, if a model is trained on data with a certain political bias, it might be more likely to generate content that aligns with that bias. This could lead to fairness issues in NLP applications. For instance, in a task like sentiment analysis, a model with a political lean might overestimate the sentiment of certain types of political statements, which could be unfair if the goal is to provide an objective analysis. But it's not always that straightforward. Some models might be more general and less influenced by political leanings. However, it's important to be aware of these potential issues.If you want to know more about how to mitigate these fairness issues, feel free to ask.</sample>
    <sample id="1093">So specifically we first propose two prompt language models with different prompt formats using the political questionnaires such as the Political Compass test. This ensures us to do automatic evaluation well - grounded in political science literature.</sample>
    <sample id="1094">First language models do have varying political leanings. They occupy all four quadrants on the political compass.</sample>
    <sample id="1095">We can also see that GPT - 4 is the most liberal language model of them all and GPT series are generally more socially liberal than BERT series and its variants.</sample>
    <sample id="1096">Secondly， we aim to investigate to which extent the political biases of language models are actually picked up from training data.</sample>
    <sample id="1097">So we could conduct a controlled experiment by further pre - training language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings.</sample>
    <sample id="1098">By further pre - training language models on such partisan corpora， we can see that the ideological coordinates of the language model also correspondingly shift.</sample>
    <sample id="1099">For example， for Roberta further fine - tuned and further trained on the left - leaning Reddit corpus， we can see a substantial liberal shift in terms of its.</sample>
    <sample id="1100">In terms of its political biases.</sample>
    <sample id="1101">And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society.</sample>
    <sample id="1102">So we divide pre - training corpora into pre - 45th president of the United States and after 45th president of the United States. We separately pre - train language models on the two different temporal corpora.</sample>
    <sample id="1103">We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the like polarization in our society.</sample>
    <sample id="1104">So last but not least we evaluate language models with different political leanings on hate - speech detection and fake news detection two NLP applications that often involve language models and could have very significant implications.</sample>
    <sample id="1105">So we see that if we investigate the per - category performance that is to say if we separate the performance into.</sample>
    <sample id="1106">Different demographics or political leaning of news media. We can see a pattern that, for example, for hate - speech detection, left - leaning language models are better.</sample>
    <sample id="1107">Detecting hate speech targeting socially minority groups.</sample>
    <sample id="1108">However, our works at detecting hate speech targeting more powerful groups in our society.</sample>
    <sample id="1109">And vice versa. Large language models are better at detecting hate speech targeting white and men. However, worse at detecting hate speech targeting black, LGBTQ+, and other minority communities.</sample>
    <sample id="1110">Similar trends also happen for fake news detection where we see that left - leaning language models are better at detecting misinformation from their opposite political leaning and vice versa.</sample>
    <sample id="1111">This in - out refers to show many qualitative examples to see that language models with different political leanings.</sample>
    <sample id="1112">Sure. But you haven't given me the specific examples yet. You should go ahead and share them so I can make the predictions.</sample>
    <sample id="1113">This indicates that there is a fairness issue that is very pressing regarding the political biases of language models.</sample>
    <sample id="1114">For example， if a right - leaning language model were to be fine - tuned on hate speech or misinformation and whatever and deployed to a popular social media platform.</sample>
    <sample id="1115">This would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control.</sample>
    <sample id="1116">So this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings.</sample>
    <sample id="1117">So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis.</sample>
    <sample id="1118">So if we do not sanitize political opinion in language model training data the bias would propagate from pre - training data to language models to downstream tasks ultimately creating fairness issues.</sample>
    <sample id="1119">If we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retained in language model training data so it's kind of like the electric electric charlie problem.</sample>
    <sample id="1120">Okay great I think that's pretty much all I have for today. Have a great day! Thank you for your time.</sample>
    <sample id="1121">It does not have a name.</sample>
    <sample id="1122">The author described the "marked words" method as a way to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">The authors are affiliated with the University of Washington.</sample>
    <sample id="1124">The Prague approach.</sample>
    <sample id="1125">James Finch and Sarah Finch.</sample>
    <sample id="1126">There are four authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="1127">Well, there are several datasets that can be used to test syntactic phenomena. For example, the Penn Treebank is a popular one. It has a large amount of annotated treebank data which can be really useful for syntactic analysis. Another one is the Universal Dependencies dataset. It provides syntactic information in a more structured way across different languages. There are also some smaller, more specialized datasets depending on the specific syntactic phenomena you're interested in. So, it really depends on what you want to test. What kind of syntactic phenomena are you specifically interested in?</sample>
    <sample id="1128">Hello， my name is Kaiyuan and I will be presenting our work titled "When does translation require context? A data - driven multilingual exploration". This work was done in collaboration with Patrick Franasz， Emi Liu， Andre F. T. Martins and Graham Neubig.</sample>
    <sample id="1129">In this sentence, "more" could be translated as "更多" in Chinese. But it really depends on the context. If it's about quantity, it could also be "更多的" or "更多的东西" if it's referring to things. So, you need to look at the whole sentence to get the right translation. What kind of sentence is it?</sample>
    <sample id="1130">Well, if the previous sentence was "Things could start to get dangerous if the ministers find out", then "Moll" refers to a spy. But if the previous sentence was "Could it be anything serious, doctor?", then "Moll" refers to a birthmark.</sample>
    <sample id="1131">So depending on context the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1132">然而，评估模型在处理像这样的对比案例时的表现相当困难。首先，因为只有很小一部分翻译依赖于上下文，这使得语料库级别的指标，如BLEU，无法捕捉这些翻译。</sample>
    <sample id="1133">有人建议对上下文相关的翻译进行有针对性的评估，但这些资源仅支持有限类型的上下文相关翻译和有限的语言集，因为它们通常依赖于领域知识和人工编纂。</sample>
    <sample id="1134">在这项工作中，我们试图回答这两个问题：首先，为什么翻译需要上下文？其次，模型在处理这些情况时表现得有多好？</sample>
    <sample id="1135">To answer the first question we started by measuring how much a word depends on context in translation.</sample>
    <sample id="1136">在之前的工作中，我们介绍了CMI作为机器翻译模型对上下文使用的度量。这是通过测量上下文C在给定源X的情况下，提供了多少关于目标Y的信息来完成的。</sample>
    <sample id="1137">你可以把CMI看作是给模型提供上下文所获得的信息量。</sample>
    <sample id="1138">In this work we extend CxMI to pointwise CxMI which can measure context usage at the sentence level or at the word level. We can think of words that have high p - CxMI as ones that require context for translation.</sample>
    <sample id="1139">Now we analyze words with high P - SMI to look for patterns between these words.</sample>
    <sample id="1140">I'm not sure what you mean by "Turn the English content into written text." If you want to analyze the transcribed TED talks in 14 different languages, you should first make sure the English content is in written text form. So, could you clarify what you really need help with?</sample>
    <sample id="1141">我们进行分析在三个不同的层次。首先，我们查看具有高平均值的词性标记。</sample>
    <sample id="1142">And this allows us to find, for example, dual pronouns in Arabic that have relatively high P - X - I. And this can be explained because English doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into Arabic.</sample>
    <sample id="1143">And similarly we find that certain languages also require context when we want to choose appropriate verb form. We then look at vocabulary items that have high P - sex and my average over all of its different occurrences.</sample>
    <sample id="1144">And this helps identify cases like the one here where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document.</sample>
    <sample id="1145">And similarly we find that context is important to translate in the right formality.</sample>
    <sample id="1146">And finally we look at different individual tokens that have high P XMI. And this allows us to identify phenomena that cannot really be captured by the word itself but that's further expressed in the sentence structure such as ellipsis resolution.</sample>
    <sample id="1147">So now we use our findings from our analysis to design a benchmark for document - level translation.</sample>
    <sample id="1148">对于我们识别出的五个现象，我们创建了标签来自动识别与现象相关的单词，并称我们的标签为多语言现象感知，MUDA，标签。</sample>
    <sample id="1149">We can then also note that different languages have different proportions of these discursive phenomena.</sample>
    <sample id="1150">我们使用Moses Tagger，通过将其应用于我们想要用于评估的平行语料库，然后在Moses Tagger识别出的上下文相关示例上应用我们选择的翻译度量。</sample>
    <sample id="1151">最后，我们使用基准以及其他的指标来评估不同的模型在文档级机器翻译上的表现。</sample>
    <sample id="1152">First of all， when we use corpus level metrics， for BLEU we find that context - agnostic models have the best performance.</sample>
    <sample id="1153">But then if we use Comet context aware models perform best. And if we use word f - measure then models with and without context have comparable performance.</sample>
    <sample id="1154">这再次表明，如果我们仅使用语料库级别的指标，就很难确定最佳的文档级翻译系统。</sample>
    <sample id="1155">现在我们使用MMLU基准来评估模型，我们发现上下文理解模型在某些话语现象，如正式性和词汇连贯性方面，比不使用上下文的模型显著更准确。</sample>
    <sample id="1156">但是这些模型在其他现象，如省略，代词和动词形式等方面，没有使用上下文的模型表现得更好。这暗示了在文档级翻译方面，我们需要看到更多的进展。</sample>
    <sample id="1157">我们也比较了不同的商业系统，我们的基准显示DeepL通常比谷歌翻译在文档级翻译方面更准确。</sample>
    <sample id="1158">We did a data - driven analysis on 14 language pairs to find out when translations need context.</sample>
    <sample id="1159">然后我们用我们的发现来建立一个文档级机器翻译的基准，这可以帮助我们识别出哪些离散现象模型能够很好地处理，哪些不能，以及哪些翻译系统在文档级翻译方面表现良好。</sample>
    <sample id="1160">谢谢你的关注，我们明天见。</sample>
    <sample id="1161">The abbreviations of the five methods for the first research question are not mentioned in the given content.</sample>
    <sample id="1162">The model is evaluated on 11 biomedical and clinical downstream tasks. If you want to know more about these tasks, feel free to ask.</sample>
    <sample id="1163">Hi. Welcome to our presentation of the Plain, a new corpus for German text identification on the document level and on the sentence level.</sample>
    <sample id="1164">My name is Regina Jordan and I will guide you through the first part of the presentation. Let's first define text simplification.</sample>
    <sample id="1165">Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group. As people with reading problems or non-native speakers.</sample>
    <sample id="1166">To train a text summarization model, we require parallel pairs of text, for example, abstracts of documents or sentences.</sample>
    <sample id="1167">Sure, what's the English content you want me to convert?</sample>
    <sample id="1168">Different techniques are possible as you can see in the example such as lexical substitution, clause deletion, clause insertion, reordering or insertion of words.</sample>
    <sample id="1169">We now propose our new corpus to be plain. Because in recent years there were some problems with existing corpora. So for example, these corpora here are too small to train a text classification model on.</sample>
    <sample id="1170">The other three models which are proposed in recent years are all automatically aligned which means they can be error - prone in their alignments.</sample>
    <sample id="1171">Therefore we propose our new corpus D - Plane which is split into two sub - corpora D - Plane - API and D - Plane - Web. D - Plane - API is based on news texts.</sample>
    <sample id="1172">In DeepL and APA, we aligned 483 documents all manually. It results in roughly 30, 000 - 13, 000 parallel sentence pairs.</sample>
    <sample id="1173">For DeepL and Web， this corpus includes different domains. And we also align all of these 750 documents， on the one hand， manually， and on the other hand， with automatic alignment methods.</sample>
    <sample id="1174">In total we result in thirty thousand four hundred fifty sentence pairs.</sample>
    <sample id="1175">We analyzed our sentence pairs a little bit more so for example on the type of simplification.</sample>
    <sample id="1176">The Bible texts are much stronger simplified than, for example, the news texts or the language learner texts.</sample>
    <sample id="1177">On all levels， regarding for example lexical simplification， structural simplification， and also overall level of simplification.</sample>
    <sample id="1178">Furthermore， you can see that our Deplain corpus has a high variety of different simplification transformations. So， for example， in the Deplain API corpus， we have much more reorderings and word additions than we have in the Deplain web corpus.</sample>
    <sample id="1179">On the other hand，在 the web corpus， we have much more rephrasing.</sample>
    <sample id="1180">So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our dataset D plane. So for the first use case, we can evaluate automatic alignment methods.</sample>
    <sample id="1181">In recent years there has been a lot of alignment methods but in the context of machine translations.</sample>
    <sample id="1182">You can use machine translation tools like Google Translate or DeepL. They can give you a rough translation of the English content into text. But for more accurate alignment of sentences, you might need to use specialized software or services designed for parallel document analysis.</sample>
    <sample id="1183">In our use case， we are trying to extract alignments between sentences of two parallel documents having the same language and same content but on different complexity levels.</sample>
    <sample id="1184">And now as we have our dataset D plane which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods.</sample>
    <sample id="1185">And we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper.</sample>
    <sample id="1186">At the end we concluded that the best alignment automatic alignment method to use for texts for German text simplification is the method of mass align.</sample>
    <sample id="1187">And you can also find the code to run this method on your own documents in the paper.</sample>
    <sample id="1188">The second use case that we showed in our paper is the case of automatic text simplification.</sample>
    <sample id="1189">Fine - tuning language models to produce simplified text from complex input text.</sample>
    <sample id="1190">We have fine - tuned two different models. We have fine - tuned a model of long input to produce document - level simplifications.</sample>
    <sample id="1191">And we also fine - tuned the normal base long - the normal base import to produce sentence - level simplifications.</sample>
    <sample id="1192">You can also find all the checkpoints and look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="1193">We concluded that this basic fine - tuning could produce or could get scores better than the baseline scores.</sample>
    <sample id="1194">And we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="1195">Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.</sample>
    <sample id="1196">Hi, I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the Alt - Entity - Score.</sample>
    <sample id="1197">My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Parodi and Arif Mousa.</sample>
    <sample id="1198">Did you mean "Easy on Me" or "I Got a Feeling"?</sample>
    <sample id="1199">The most obvious thing is to use a direct reference, for example, by saying "The name of the song is 'Easy on Me' or its position, the first one."</sample>
    <sample id="1200">But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song.</sample>
    <sample id="1201">All the pronunciations are too similar to each other and hard to disambiguate.</sample>
    <sample id="1202">For example, the newer one or the song that's not energetic.</sample>
    <sample id="1203">This is an important problem in conversational systems and also for benchmarking and evaluating entity understanding.</sample>
    <sample id="1204">We are not aware of a public dataset a large - scale public dataset for the task so we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes.</sample>
    <sample id="1205">Our dataset collection methodology emphasizes informality using a cartoon completion set.</sample>
    <sample id="1206">Remember that song we were listening to yesterday?</sample>
    <sample id="1207">Do you mean easy on me or I got a feeling?</sample>
    <sample id="1208">In the third speech bubble, Bob uses an indirect reference to select one of these entities, for example, the New York.</sample>
    <sample id="1209">We provide the first and second speech bubbles automatically but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="1210">I'm not sure what the English content is. Can you tell me more about it?</sample>
    <sample id="1211">We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia.</sample>
    <sample id="1212">When we move higher in the list, the entities become more similar to each other and it's usually harder to make the disambiguation.</sample>
    <sample id="1213">The first one is uniform attract.</sample>
    <sample id="1214">The second one is when the entities have similar titles, for example, two books with the name "The Return".</sample>
    <sample id="1215">The third one is when they have similar descriptions on Wikipedia and finally when they have similar infoboxes or attributes on Wikipedia, for example, the same genre or the same artist for a song.</sample>
    <sample id="1216">When we show this alternative question to the answerers, they know the name of these entities, but they don't necessarily know about the entity.</sample>
    <sample id="1217">So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song.</sample>
    <sample id="1218">And then ask the annotators to listen to at least some of each song and read about each song. Here's, for example, the Google search result for the song "Easy".</sample>
    <sample id="1219">For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images, again from Wikipedia, so that the annotators know how they look like.</sample>
    <sample id="1220">Then we ask the annotators to pick one of these entities, for example here the first one, and describe them using three to five indirect referring expressions.</sample>
    <sample id="1221">For example, the one with the piano music. Here are some examples from our data set. For example, the one without words, not the one with the twelve - year - old boy or the fictional one or comes from other boy genres.</sample>
    <sample id="1222">The L - Entities Corpus has 6000 alternative questions across three domains, and it has 42000 indirect referring expressions. Results with T5 - XLarge model are summarized below.</sample>
    <sample id="1223">If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92 to 95 percent. But this is not realistic.</sample>
    <sample id="1224">If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87%, which is more realistic, for example, when the language model retrieves the background knowledge.</sample>
    <sample id="1225">If the language model has access only to entity names then the accuracy is only 60%. So there's a lot of room for improvement. We've also shown that the models are domain generalizable. Here is a link to our dataset. Thanks for.</sample>
    <sample id="1226">CamemBERT is initially trained on the 4GB subset of NACOS.</sample>
    <sample id="1227">The name of the speaker is Adam Skowroński.</sample>
    <sample id="1228">The findings were that the performance degraded with larger temporal gaps when retraining or continuing to pretrain models with more recent data. This confirmed the hypothesis that temporal drift is the main cause of performance drop. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="1229">Hi everyone. I'm Jenny, a first - year PhD student at Carnegie Mellon University. And today I'll be presenting your work on "Analyzing and Characterizing Design Biases in Datasets and Models.".If you have any questions or comments about this, feel free to let me know.</sample>
    <sample id="1230">This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI, namely Sebastian Santi, Runen Le Bras, Katerina Rineka, and Morten Sap.</sample>
    <sample id="1231">Well, first you'd look for comments that use offensive language, like racial slurs or insults. Then, if there are comments that are threatening or promote violence, those should be removed too. Also, comments that are just spam or completely irrelevant to the article can be taken out. And if a comment is just a rant without any real point, it might be best to delete it. What do you think about this? Do you have any specific examples of toxic comments you've come across?</sample>
    <sample id="1232">You might turn towards a popular API like Perspective API for toxicity detection. And this works really well if you're Carl Jones. Where Perspective API is able to detect correctly toxic instances.</sample>
    <sample id="1233">But that's not really the case for Aditya Sharma where perspective API is really not as sensitive to offensive terms that are more common in Indian contexts.</sample>
    <sample id="1234">This is an example of a design bias where we see systematic performance differences of technology between populations.</sample>
    <sample id="1235">Design biases might occur due to the positionality of NLP researchers and model developers. Positionality is the perspectives people hold because of their demographics, identity, and life experiences.</sample>
    <sample id="1236">This is a concept widely used in critical studies specifically in feminist and queer academic spaces.</sample>
    <sample id="1237">Positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make.</sample>
    <sample id="1238">Well, datasets and models can have positionality. For example, a dataset might be centered around a certain region or group, and a model trained on that dataset might be more likely to perform well on data from that same region or group. But it's not always the case. There are models that are designed to be more general and not be too influenced by the positionality of the data they were trained on.If you want to know more about how to deal with positionality in datasets and models, feel free to ask me more.</sample>
    <sample id="1239">And we're not trying to say that models and data sets themselves have demographic identities and life experiences. But they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others.</sample>
    <sample id="1240">Prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and datasets as well as theoretical definitions of model positionality.</sample>
    <sample id="1241">However, these works really don't look at comparing end users with the datasets and models themselves.</sample>
    <sample id="1242">Positionality is increasingly important as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="1243">In its challenging to characterize how these position alities are skewed because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="1244">To study dataset and model positionality, we compare the annotations with real users with existing datasets and models.</sample>
    <sample id="1245">We do this through our framework and our positionality.</sample>
    <sample id="1246">Our framework works in two main steps.</sample>
    <sample id="1247">The first step is to re - annotate data sets with diverse annotators.</sample>
    <sample id="1248">And we ought to do this over looking at the demographics of original datasets' annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared.</sample>
    <sample id="1249">And so we opt to reannotate data to get many annotations for instance and to get a rich set of demographic data.</sample>
    <sample id="1250">We then take the annotations by demographic and compare them to the models and datasets using a Pearson's r correlation score.</sample>
    <sample id="1251">And thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator - annotator agreement or modeling annotator distributions.</sample>
    <sample id="1252">Yes, they are. If you want to know more about how they work or other related things, feel free to ask.</sample>
    <sample id="1253">In Lab in the Wild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like MTurk which largely have participants from the US or India. And further Lab in the Wild still is able to get high - quality data.</sample>
    <sample id="1254">We host two tasks on Lab in the Wild, one of them being social acceptability. And the way this works is that participants will read a situation from the Social Chemistry dataset and then they'll rate how socially acceptable a situation is.</sample>
    <sample id="1255">Afterwards, to stay engaged in the study, they can compare their responses to an AI and others.</sample>
    <sample id="1256">We then compared these annotations with social chemistry Delphi in GPT - 4.</sample>
    <sample id="1257">We then replicate a very similar setup for the toxicity and hate - speech detection task where they'll read an instance from dinah - hate and rate whether they think it's an instance of hate - speech.</sample>
    <sample id="1258">We then compared these annotations with Dina hate perspective API, Rewire API, hate Roberta, and GPT - 4. Our study in the end amassed over 16, 000 annotations from over a thousand annotators from 87 countries.</sample>
    <sample id="1259">So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP.</sample>
    <sample id="1260">For example, we find that datasets and models are most aligned to English - speaking countries. So for the GPT - 4 social acceptability analysis, we find that it's most aligned to Confucian and English - speaking countries. We find that DAINA - HATE is also most aligned to English - speaking countries.</sample>
    <sample id="1261">We also find most additional alignment with people who have a college education. So for GPT - 4 in the social acceptability task we find that it's most aligned to people with a college education or graduate school education.</sample>
    <sample id="1262">And we find the same for Donny Hate where it's most aligned to people with a college education.</sample>
    <sample id="1263">However, when models and datasets are aligned to specific populations, some are inevitably left behind.</sample>
    <sample id="1264">An example of this is that datasets and models are less aligned to non-binary people compared to their men and women counterparts. We find this in the GPD for social acceptability task as well as the DINE - HATE task analysis as well.</sample>
    <sample id="1265">Well, first of all, you could try to find a new position that better suits your skills and interests. Maybe look for job openings in different industries or job titles. Another option is to improve your current position by taking on more responsibilities or getting additional training. You could also consider networking with people in your field to see if they can offer any advice or opportunities. And don't forget to keep your resume and cover letter up - to - date so you can easily apply for new positions if needed.If you want to talk more about this or have other questions, feel free to let me know.</sample>
    <sample id="1266">So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research with the lens of perspectivism.</sample>
    <sample id="1267">Our third recommendation is to build specialized datasets and models within four specific communities. And a good example of this is the Musa Connie initiative. I mean we want to emphasize that inclusive NLP isn't just making all technologies work for everyone.</sample>
    <sample id="1268">And so that concludes our presentation. But if you'd like to learn more, feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="1269">Well, you know, the tokens might be in a random order at first. And if we want the output sequence to be meaningful, we need to put them in the right order. So, we use another model to predict a permutation to do that. It's like making sure the words in a sentence are in the correct order to make sense. So, that's why it's necessary to permute the tokens for the output sequence. If you have any other questions about this, feel free to ask.</sample>
    <sample id="1270">The authors recommended increased transparency about bias mitigation methods because they were unsure if positive stereotypes were due to weirdly excessive value alignment or other anti - stereotyping methods causing pernicious patterns.</sample>
    <sample id="1271">Minimal - pair unacceptable inputs are the ungrammatical sentences in the minimal - pair paradigm. If you want to know more about minimal - pair paradigms or language model evaluation, feel free to ask.</sample>
    <sample id="1272">The text doesn't mention the evaluation metrics used.</sample>
    <sample id="1273">Simple linear regression analysis. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">I'm not sure about the affiliations of the authors of this paper. You could try looking at the paper itself or searching for it online. If you find out, come back and let me know.</sample>
    <sample id="1276">MultiInstruct differs from other benchmarks in that it focuses on multimodal instruction tuning and aims to improve generalization to unseen multimodal tasks. It also addresses the lack of large - scale publicly available multimodal instruction tasks, which is a significant difference from other benchmarks.</sample>
    <sample id="1277">Two authors are involved in the paper. If you have any other questions about this paper, feel free to ask.</sample>
    <sample id="1278">I'm not sure what the definition of binary coordination is based on the information you've given. You might need to provide more context or clarify the question.</sample>
    <sample id="1279">I'm not sure about the average length of the prompts used in this study. You might need to check the study's methodology section or contact the researchers for that information.</sample>
    <sample id="1280">The findings imply that the smaller T5 model, when properly trained on suitable datasets, can generate scripts of higher quality than most large - level models. This shows that smaller models can outperform larger ones in certain aspects. So, it's a big deal for the development of smaller models. What do you think about this?</sample>
    <sample id="1309">The work investigates the impact of pretraining strategies.</sample>
    <sample id="1310">The factor of overfitting due to test reuse is not specified in the given information. So, I can't answer that. But if you have any other related questions or if there's more information you can provide, feel free to let me know.</sample>
    <sample id="1311">The quality of the simplification was evaluated by looking at the scores and evaluation metrics in the paper.</sample>
    <sample id="1312">Yes, language models do have different political biases.</sample>
    <sample id="1347">Cognitive dissonance is two beliefs or actions that are inconsistent. If you want to know more about it, like how it affects language learning or something, feel free to ask.</sample>
    <sample id="1348">GPT-4 is the most liberal language model. If you have any other questions about language models or anything else, feel free to let me know.</sample>
    <sample id="1349">Yes.</sample>
    <sample id="1350">The name of the speaker is Sarah Pape.</sample>
    <sample id="1351">The data was taken from transcriptions of TED talks that have been translated from English to 14 different languages.</sample>
    <sample id="1352">The dependency structure of coordination.</sample>
    <sample id="1353">As you may know there are different dependency structures assumed by different theories and corpus approaches. So for example in universal dependencies the structure of the coordinate Lisa Bart and Maggie.</sample>
    <sample id="1354">Yes, that's correct. "Lisa" is the head of the coordinate structure. If you have any other questions about this or something else, feel free to let me know.</sample>
    <sample id="1355">A similar approach is assumed in economic texts, meaning text theory, where again the whole coordinate structure is headed by the first conjunct. So these two approaches are isometric, right? They single out one of the conjuncts.</sample>
    <sample id="1356">There are also symmetric approaches to coordinate structures such as the "trunk" approach. The conjunction-headed approach is assumed in "trunk" dependency treebanks, where coordinate structures are headed by the conjunction.</sample>
    <sample id="1357">So we get some dependencies from end to all the conjuncts.</sample>
    <sample id="1358">And finally there's also a multi - headed approach that's used for example in the Katz and Sussman's word grammar.</sample>
    <sample id="1359">All conjunctions are heads of the coordinate structure. So we get dependencies from the governor here, loves, to all conjunctions separately. These are bottom - smack.</sample>
    <sample id="1360">The aim of the paper is to produce a novel argument for the symmetric structures of coordination like this and against the asymmetric structures of coordination like this.</sample>
    <sample id="1361">The argument is based on the principle of dependency maximization that I will explain on the basis of these examples.</sample>
    <sample id="1362">So in English, as you might know, a direct object is preferred to be close to the verb, while adjuncts may be further away, right? So "Mark read it yesterday" is fine because the direct object "it" is close to the verb.</sample>
    <sample id="1363">While March read "Yesterday" it is much worse right? Because here between the verb and the direct object there's an adjective "Yesterday".</sample>
    <sample id="1364">However， this effect may be ameliorated when the direct object is very heavy and very long， because then it can be moved to the position after the agent.</sample>
    <sample id="1365">March read this absolutely fascinating book about the bees yesterday. It's okay. The way instead of it we have this long and pain.</sample>
    <sample id="1366">March read yesterday this absolutely fascinating book about bees.</sample>
    <sample id="1367">The reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb.</sample>
    <sample id="1368">It satisfies the principle of dependency length minimization， which says that shorter dependencies are preferred.</sample>
    <sample id="1369">So these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures.</sample>
    <sample id="1370">So here we have a dependency from red to the adjunct of length 7 measured in words and from red to book of length 4 so together it's 11.</sample>
    <sample id="1371">When you move when you swap are these two constituents the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one</sample>
    <sample id="1372">Okay, so what we did was extract various statistics from about coordination from the enhanced version of the Penn Treebank and see the paper. Why wouldn't you use universal dependencies?</sample>
    <sample id="1373">Yes, the statistics do confirm that left conjuncts tend to be shorter.</sample>
    <sample id="1374">The tendency grows with the length difference.</sample>
    <sample id="1375">When the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger, so the proportion is bigger of the left short conjunct.</sample>
    <sample id="1376">The novel aspect in this paper is that we observed this tendency only occurs when the governance on the left are absent.</sample>
    <sample id="1377">The governor is on the left.</sample>
    <sample id="1378">It's absent in the second example Homer came and sneezed. Here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the the bigger the difference between the two conjuncts.</sample>
    <sample id="1379">However， when the governance on the right， as here， left， governs the coordination to the net， this effect disappears.</sample>
    <sample id="1380">So we showed that by measuring length in characters, that's the first column in syllables, the middle column, and in words the right column. So I'll concentrate on the right one.</sample>
    <sample id="1381">When the governor is on the left.</sample>
    <sample id="1382">The tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears.</sample>
    <sample id="1383">And we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two.</sample>
    <sample id="1384">So see the paper for the full agreements and arguments. Sorry and talk to us about it in the poster session. Thank you.</sample>
    <sample id="1385">The name of the speaker is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is training a model on one source language and then transferring it to another language. For example, training on English queries or a combination of English and German few-shot queries to create a multilingual model that can predict SQL output. If you want to know more about how it works or have other related questions, feel free to ask.</sample>
    <sample id="1387">The affiliations of the authors are not mentioned in the given English content.</sample>
    <sample id="1388">The authors use average lagging and computational - aware average lagging. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="1416">Well, the drawbacks are that trees usually aren't given and need to be obtained, which can be complicated and computationally expensive. There's also a lot of formalism - specific pre - processing of the logical forms involved. And sometimes it involves specialized grammar induction procedures. So, that's about it. If you have any other questions related to this, feel free to ask.</sample>
    <sample id="1417">I'm not sure about the affiliations of the authors of that paper. You could try looking it up on academic databases like Google Scholar or ResearchGate. If you find out, come back and let me know.</sample>
    <sample id="1495">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="1496">The performance delta between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2023. If you have any other questions about this or related topics, feel free to ask.</sample>
    <sample id="1527">I'm not sure about the specific affiliations of the authors of this paper. You might need to look at the paper itself for that information. But if you have any other questions about the paper, like what the main ideas are or how it was written, feel free to ask.</sample>
    <sample id="1528">Si Yuan.</sample>
    <sample id="1529">There are five authors involved in the paper. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="1530">The approach is compared with the state - of - the - art architecture specifically tailored for simultaneous speech translation. If you have any other questions about this, feel free to let me know.</sample>
    <sample id="1531">Hello everyone. My name is Ying and my colleague Zhiyang and I will be presenting our research on multi - instruct: improving multi - model zero - shot learning via instruction - tuning.</sample>
    <sample id="1532">随着大型语言模型的进展，许多工作开始探索以参数和数据高效的方式重用预训练语言模型的不同下游任务的新学习范式。</sample>
    <sample id="1533">最近，许多研究表明，指令微调使大型语言模型能够通过遵循自然指令在零样本情况下执行NLP任务。</sample>
    <sample id="1534">然而，大多数之前关于指令微调的工作都集中在提高零样本性能的语言任务上，而计算机视觉和多模态任务却被忽略了。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1535">因此，在这项工作中，我们想要研究指令微调在多模态预训练模型中是否真的能够提高对N - 1多模态任务的泛化能力。如果还有其他翻译相关的问题或者别的事，都可以跟我说哦。</sample>
    <sample id="1536">另外，在我们研究的时候，我们发现LP和多模态之间的指令数据集的可用性存在相当大的差异。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="1537">存在超过1600个单语言指令任务，然而没有大规模的公开多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="1538">这里我们呈现MultiInstruct，第一个多模态指令微调基准数据集，包含62个多样化的多模态任务，覆盖10个主要类别。</sample>
    <sample id="1539">These tasks are derived from 21 existing open - source datasets and each task is equipped with 5 expert - written instructions.</sample>
    <sample id="1540">对于在我们提出的多模态指令调优的调查，我们以OFA，一个统一的多模态预训练模型，作为基础模型。OFA使用一个统一的词汇表，用于语言，图像标记和边界框的坐标。</sample>
    <sample id="1541">Here we show some example instances from our multi - intra data set.</sample>
    <sample id="1542">To unify the processing of various input and output data types.</sample>
    <sample id="1543">我们遵循OFA的方法，将所有任务统一为序列到序列的格式，其中输入文本，图像，指令和边界框都在同一个词元空间中表示。如果还有其他问题或者想进一步讨论这个话题，随时告诉我呀。</sample>
    <sample id="1544">Okay.</sample>
    <sample id="1545">对于训练数据集，我们使用来自NAGroup的53个任务进行训练，并且每个任务采样10000个实例。对于测试，我们保留整个Commonsense Reasoning组用于测试，并且从VQA和Miscellaneous组中选择另外5个任务。</sample>
    <sample id="1546">我们在测试集上使用所有实例，此外，我们从自然指令的测试集中随机抽取20个任务作为NLP的合成任务。</sample>
    <sample id="1547">我们使用预训练的OFA大型模型作为基础模型。在训练过程中，我们混合了所有任务的所有实例。每个实例随机与它的五个指示模板之一结合。</sample>
    <sample id="1548">During test for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="1549">We report the mean and max performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">如果任务是多模型分类任务，我们报告准确率。如果是多模型生成任务，我们报告ROUGE - L。对于NLP任务，我们也报告ROUGE - L。</sample>
    <sample id="1551">我们还引入了一个额外的评估指标叫敏感性，这个指标衡量模型在任务相同的情况下，对指令用词微小变化的鲁棒性，即模型能否一致地产生相同的输出。</sample>
    <sample id="1552">As we can see， instruction - tuning can significantly improve OFA's performance on unseen multi - model tasks.</sample>
    <sample id="1553">Transfer learning from natural instruction datasets can benefit instruction tuning.</sample>
    <sample id="1554">随着任务数量的增加，模型实现了更好的性能，同时降低了敏感性。</sample>
    <sample id="1555">我们还做了一个实验，我们使用了一种指令和五种指令。正如我们所看到的，使用更多的指令可以提高模型的整体性能，并且大大减少其敏感性。</sample>
    <sample id="1556">So this shows the effect of different fine - tuning strategy on the model sensitivity. As we can see, by transfer learning from natural instruction data set, the model can achieve much better sensitivity comparing to the original L - frame model.</sample>
    <sample id="1557">我们也可以看到，从Natural Instruction数据集进行迁移学习可以帮助OFA在Natural Instruction数据集上获得更好的性能。</sample>
    <sample id="1558">我们提出了第一个大规模多模型指令微调数据集，显著提高了大模型的零样本能力。我们探索了不同的迁移学习技术并展示了它们的好处。我们设计了一个新的度量标准叫敏感性。</sample>
    <sample id="1559">So one more thing. We are collecting a much larger multimodal instruction - tuning dataset with around 150 additional vision - language tasks and we will release them soon. This is the QR code for our data and model. Thank you.</sample>
  </task>
</testset>