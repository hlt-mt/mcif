<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su grandi scale di dati web crawled.</sample>
    <sample id="1">L'autore fa parte di McGill University, MILA e Microsoft Research.</sample>
    <sample id="2">Ciao, benvenuto alla nostra presentazione di DeepLing, un nuovo corpus per l'identificazione di testi tedeschi a livello di documento e a livello di frase.</sample>
    <sample id="3">Il contenuto inglese è "My name is Regina Jordan and I will guide you through the first part of the presentation. Let's first define text simplification." La traduzione in italiano è "Il mio nome è Regina Jordan e vi guiderò attraverso la prima parte della presentazione. Iniziamo definendo la semplificazione del testo." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo di destinazione specifico, come persone con problemi di lettura o non madrelingua.</sample>
    <sample id="5">Per addestrare un modello di traduzione di testo, abbiamo bisogno di coppie parallele di testo, ad esempio documenti o frasi parallele.</sample>
    <sample id="6">Ecco un esempio: hai visto una pausa nella linea di un brano musicale e la sua traduzione in italiano.</sample>
    <sample id="7">Sono possibili diverse tecniche di semplificazione, come la sostituzione di parole, la distruzione di frasi, l'eliminazione di frasi, la riassegnazione di ordine o l'inserimento di parole.</sample>
    <sample id="8">Ora proponiamo il nostro nuovo piano di corpus perché negli ultimi anni ci sono stati alcuni problemi con il corpus esistente. Ad esempio, questo corpus qui è troppo piccolo per addestrare un modello di classificazione del testo.</sample>
    <sample id="9">I tre modelli proposti negli ultimi anni sono tutti automaticamente allineati, il che significa che possono essere soggetti a errori nell'allineamento.</sample>
    <sample id="10">Quindi proponiamo il nostro nuovo corpus D-Plane, diviso in due sottocorpi: D-Plane API e D-Plane Web. D-Plane API si basa su testi di notizie.</sample>
    <sample id="11">Nel piano APA abbiamo allineato 483 documenti tutti manualmente. Questo ha portato a circa 30.000-13.000 coppie di frasi simili.</sample>
    <sample id="12">Per il corpus di DeepLameWeb, questo corpus include diversi domini e abbiamo anche allineato tutti questi 750 documenti, sia manualmente che con metodi di allineamento automatico.</sample>
    <sample id="13">In totale, risultano 30.450 coppie di frasi.</sample>
    <sample id="14">Abbiamo analizzato le nostre coppie di frasi un po' di più, per esempio sul tipo di semplificazione.</sample>
    <sample id="15">Come puoi vedere qui, il testo biblico è molto più forte e semplificato rispetto, ad esempio, al testo di notizie o altri testi di apprendimento della lingua.</sample>
    <sample id="16">Semplificazione a livello di vocabolario, semplificazione strutturale, semplificazione a livello complessivo.</sample>
    <sample id="17">Inoltre, si può notare che il corpus di Dplane ha una varietà elevata di diverse trasformazioni di semplificazione. Ad esempio, nel corpus di API di Dplane, abbiamo molto più riordinamenti e aggiunte di parole rispetto a quanto abbiamo nel corpus di web di Dplane.</sample>
    <sample id="18">D'altra parte, nel corpus web abbiamo molte più riformulazioni.</sample>
    <sample id="19">Ciao， sono Omar e ora parlerò dei casi d'uso del nostro dataset D - Plane. Per il primo caso d'uso， possiamo valutare i metodi di allineamento automatico.</sample>
    <sample id="20">In recent years, there have been many alignment methods, but in the context of machine translation.</sample>
    <sample id="21">C'è un modo per estrarre le allineazioni di frasi in due documenti paralleli scritti in lingue diverse.</sample>
    <sample id="22">Ma nel nostro caso d'uso stiamo cercando di estrarre allineamenti tra le frasi di due documenti paralleli dello stesso linguaggio, dello stesso contenuto, ma di livello di complessità diverso.</sample>
    <sample id="23">E ora che abbiamo il nostro dataset DeepLNE con frasi allineate manualmente, possiamo utilizzare queste frasi come allineamenti standard per valutare alcune delle metodologie di allineamento proposte.</sample>
    <sample id="24">E abbiamo fatto alcune adattazioni ai metodi proposti e abbiamo pubblicato tutte queste adattazioni e il codice per eseguire i nostri esperimenti nel paper.</sample>
    <sample id="25">Alla fine abbiamo concluso che il metodo di massAlign è il miglior metodo di allineamento automatico da utilizzare per la semplificazione del testo tedesco.</sample>
    <sample id="26">E puoi trovare anche il codice per eseguire questo metodo sui tuoi documenti nel documento.</sample>
    <sample id="27">Il secondo caso d'uso che abbiamo mostrato nel nostro articolo è il caso di semplificazione automatica del testo.</sample>
    <sample id="28">Ridurre la complessità del testo di input per produrre un testo semplificato.</sample>
    <sample id="29">Abbiamo finetunato due modelli diversi. Abbiamo finetunato un modello di LongImpart per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">E abbiamo anche finetunato il normal base long normal base import per produrre semplificazioni a livello di frase.</sample>
    <sample id="31">Puoi trovare anche tutti i punti di controllo e puoi esaminare in più dettagli le valutazioni e le metriche di valutazione delle nostre esperimenti nel documento.</sample>
    <sample id="32">Abbiamo concluso che questa finetuning di base potrebbe produrre o ottenere punteggi migliori dei punteggi di base.</sample>
    <sample id="33">E proponiamo quei risultati come un benchmark, un benchmark di base, per il problema della semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie mille per l'attenzione e speriamo di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="35">Il nome della relatrice o del relatore è Kaiyuan.</sample>
    <sample id="36">Hanno utilizzato il modello T5-xl.</sample>
    <sample id="37">Sì.</sample>
    <sample id="38">La novità è che cerca di ridurre la soggettività valutando se le risposte dei modelli esprimono comportamenti come rispondere con informazioni irrellevanti o contraddizione.</sample>
    <sample id="39">L'attuale approccio scarsamente supervisionato si basa in larga misura sulle samples di validazione pulite.</sample>
    <sample id="40">Non so. Potresti dare più dettagli?</sample>
    <sample id="41">Quattro. C'è Xiaoyu Shen, Mario Smussbacher, Gias Stefan e Detis Clarcow. Se hai altre domande su questo articolo, sentiti libero di chiedere.</sample>
    <sample id="42">Ciao, il mio nome è Adam Skowroński e questo discorso è sullo schema di dipendenza della coordinazione.</sample>
    <sample id="43">Come potresti notare, ci sono diverse strutture di dipendenza assunte da teorie e approcci di corpus differenti. Ad esempio, nelle universali, le strutture di dipendenza sono quelle delle coordinate, come "Lisa", "Bart" e "Maggie".</sample>
    <sample id="44">Sì, il primo congiunto è la testa della struttura coordinata intera, in questo caso "Lisa".</sample>
    <sample id="45">Un approccio simile è assunto in "Economicus", una teoria del testo, dove anche la struttura del coordinato intero è guidata dal primo congiunto. Quindi, questi due approcci sono isometrici, giusto? Essi selezionano uno dei congiunti.</sample>
    <sample id="46">Ora ci sono approcci simmetrici alle strutture coordinate, come l'approccio a grappolo, l'approccio a congiunzione, che si trovano in alberi di dipendenza a grappolo, dove le strutture coordinate sono testate dalla congiunzione.</sample>
    <sample id="47">Quindi otteniamo dipendenze da "and" a tutte le congiunzioni.</sample>
    <sample id="48">E infine, c'è anche un approccio multi-cattedrale, ad esempio, nello "schema di mondo di Chomsky".</sample>
    <sample id="49">Dove tutti i congiunti sono capi della struttura coordinata, quindi otteniamo dipendenze dal governatore qui "amano" a tutti i congiunti separatamente. Questi sono i bottoni di meccanismo.</sample>
    <sample id="50">L'obiettivo del mio articolo è produrre un argomento nuovo per le strutture di coordinazione simmetriche, come queste due, contro le strutture di coordinazione asimmetriche, come queste.</sample>
    <sample id="51">L'argomento si basa sul principio di dipendenza e di estrema semplificazione che verrò spiegato sulla base di questi esempi.</sample>
    <sample id="52">Come potresti sapere, in inglese gli oggetti diretti preferiscono essere vicini al verbo, mentre gli aggettivi potrebbero essere più lontani. Quindi "March read it yesterday" è giusto perché l'oggetto diretto "it" è vicino al verbo.</sample>
    <sample id="53">Marge ha letto ieri "È molto peggio, vero? Perché qui c'è un'adverbio tra il verbo e l'oggetto diretto, "ieri".</sample>
    <sample id="54">Tuttavia, questo effetto può essere mitigato quando l'oggetto diretto è molto pesante e lungo, poiché allora può essere spostato dopo l'agente.</sample>
    <sample id="55">Questo è illustrato qui. Quindi entrambe queste frasi sono a posto. Mark ha letto questo libro assolutamente affascinante sulla B.C. ieri. Va bene. Invece di "it" abbiamo "this long and".</sample>
    <sample id="56">Ma è anche okay dire: "Mars ha letto ieri questo libro assolutamente affascinante sulla pace."</sample>
    <sample id="57">Ecco il contenuto tradotto in italiano: "Quindi la ragione qui è che ciò è possibile perché, anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo."</sample>
    <sample id="58">Lo soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che afferma che si preferiscono le dipendenze più brevi.</sample>
    <sample id="59">Questi due alberi mostrano solo la lunghezza delle dipendenze cruciali, quelle che non sono costanti tra queste due strutture.</sample>
    <sample id="60">Quindi qui abbiamo una dipendenza da "red" all'adjunct di lunghezza 7 misurata in parole e da "red" a "book" di lunghezza 4, insieme sono 11.</sample>
    <sample id="61">Quando si scambiano questi due componenti, la somma di queste due dipendenze diventa 6, giusto? Così invece di 11 e 6, molto più breve. Per questo suona abbastanza bene, giusto? Violenta un principio, ma soddisfa un altro.</sample>
    <sample id="62">Ok, allora cosa abbiamo fatto? Abbiamo estratto statistiche diverse sulla coordinazione dall'enhanced versione del Penn Treebank e dal paper "Why wouldn't you use universal dependencies?".</sample>
    <sample id="63">Eh, se hai bisogno di tradurre il contenuto inglese in italiano, puoi farlo con un traduttore online. C'è Google Traduttore, ad esempio. Ma se hai bisogno di una traduzione più accurata o se hai bisogno di aiuto con la scelta di una parola specifica, puoi dirmelo e cercherò di aiutarti.</sample>
    <sample id="64">E anche l'osservazione fatta in passato che questa tendenza cresce con la differenza di lunghezza.</sample>
    <sample id="65">Quando la differenza di lunghezza tra i due congiunti aumenta, il congiunto più breve tende a essere il primo più forte, quindi la proporzione è maggiore del congiunto breve a sinistra.</sample>
    <sample id="66">Quello che è nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando i governanti di sinistra sono assenti.</sample>
    <sample id="67">Certo. Il contenuto inglese in italiano è: "Dunque, il governatore è sulla sinistra in questo esempio. Ho visto Bart e Lisa, quindi il governatore è sulla sinistra." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="68">Ecco la traduzione in italiano: "E' assente nel secondo esempio "Homer è venuto e ha starnutito". Qui abbiamo una coordinazione di due verbi e non c'è un governante esterno, giusto? Quindi in casi simili, il congiunto di sinistra tende a essere più breve, e più grande la differenza tra i due congiunti."</sample>
    <sample id="69">Tuttavia, quando il governante è a destra, come qui, il governante a sinistra coordina il netto, questo effetto scompare.</sample>
    <sample id="70">Quindi abbiamo dimostrato che, misurando la lunghezza in caratteri, c'è la prima colonna in sillabe, la seconda in parole e la terza in parole. Quindi mi concentrerò sulla terza.</sample>
    <sample id="71">Quello che vediamo qui è che quando il governo è a sinistra.</sample>
    <sample id="72">La tendenza per la contrazione a sinistra a diventare più breve cresce costantemente con la differenza assoluta in parole, e lo stesso si osserva quando non c'è un governante, come nel coordinamento di frasi, ma quando il governante è a destra, questa tendenza scompare.</sample>
    <sample id="73">E nel nostro articolo dimostriamo come ciò fornisca un argomento contro le strutture di coordinazione asimmetriche, come queste due, e a favore delle strutture simmetriche, come queste tre.</sample>
    <sample id="74">Guarda il documento per le complete convenzioni e argomenti e parla con noi nel poster session. Grazie.</sample>
    <sample id="75">Tre.</sample>
    <sample id="76">I domini risultano più semplificati sono quelli come il testo di notizie e altri testi di apprendimento della lingua.</sample>
    <sample id="77">"Salt and pepper" è l'esempio della preferenza per i congiunti a sinistra più brevi. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="78">Sì, puoi usare i modelli per la tua ricerca.</sample>
    <sample id="79">DEplain-apa contiene documenti di notizie.</sample>
    <sample id="80">Un modello di architettura migliore, una dimensione del modello più grande e esempi di finetuning più numerosi.</sample>
    <sample id="81">Non c'è informazione sufficiente nel testo per rispondere a questa domanda. Potresti fornire più dettagli o contesto?</sample>
    <sample id="82">Non so. Potresti dare più dettagli o informazioni?</sample>
    <sample id="83">Non molto efficace.</sample>
    <sample id="84">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni?</sample>
    <sample id="85">I nomi dei personaggi sono Bob e Alice.</sample>
    <sample id="86">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per fenomeni come formalità e coesione lessicale.</sample>
    <sample id="87">Non so. Non ho informazioni sulle affiliazioni degli autori dell'articolo. Potresti cercare su internet o contattare l'autore per avere queste informazioni.</sample>
    <sample id="122">Il framework quantifica la posizionalità confrontando le annotazioni di diverse etnie con le predizioni e le etichette dei modelli e dei dataset utilizzando una correlazione di Pearson.</sample>
    <sample id="155">Hanno rivelato stereotipi razziali.</sample>
    <sample id="156">Le statistiche sono state estratte dal versione migliorata del Pan-Treebank e dalla paper "Why wouldn't you use universal dependencies?". Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="157">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli sull'articolo?</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di espansione e confronto delle classi di PDB e la classificazione di discordanza e consonanza.</sample>
    <sample id="159">The paper "Do Cornell 2003 Named Entity Taggers Still Work Well in 2023?" has two authors.</sample>
    <sample id="160">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli o cercare di trovare l'articolo per saperlo?</sample>
    <sample id="161">Il framework differisce dai lavori precedenti perché confronta gli utenti finali con i modelli e i dataset, le previsioni e le etichette, invece di guardare solo all'acqua inter-annotatore o alle distribuzioni di annotazione.</sample>
    <sample id="162">Non so. Non ho abbastanza informazioni per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="163">Google Translate e DeepL.</sample>
    <sample id="164">Ciao， sono Zhang Bing， studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro， dal pretraining dei dati ai modelli di linguaggio， fino a compiti di basso livello， tracciando le tracce di pregiudizi politici che portano a modelli di NLP ingiusti.</sample>
    <sample id="165">I modelli di linguaggio sono addestrati su grandi quantità di dati web crawled.</sample>
    <sample id="166">I media politici sono ben coperti nei loro dati di pre-allenamento. Secondo una ricerca del corpus C4, possiamo vedere che il New York Times, il Los Angeles Times, The Guardian, il Huffington Post, ecc. sono ben coperti nei dati di allenamento del modello linguistico.</sample>
    <sample id="167">Questo ha creato un dono misto per le applicazioni di modelli di linguaggio.</sample>
    <sample id="168">D'altra parte, queste diverse opinioni politiche sono intrinsecamente socialmente pregiudiziate e potrebbero portare a problemi di equità nelle applicazioni di compiti successivi.</sample>
    <sample id="169">Per questo scopo, proponiamo di indagare il percorso di propagazione del bias politico, dal dataset di pre-allenamento ai modelli di linguaggio, fino a compiti di bassa catena. Specificamente, chiedendo le seguenti domande.</sample>
    <sample id="170">Evaluare l'orientamento politico dei modelli di linguaggio e il ruolo dei dati di addestramento nella formazione di tali pregiudizi politici sono complessi. Per valutare l'orientamento politico dei modelli di linguaggio, si possono utilizzare diversi metodi. Ad esempio, si può analizzare il contenuto dei dati di addestramento, cercando di identificare se ci sono tendenze politiche chiare. Se i dati di addestramento contengono più informazioni di una certa tendenza politica, il modello potrebbe svilupparsi con un pregiudizio politico simile. Inoltre, si possono eseguire test di bias per vedere se il modello produce risposte più favorevoli o meno a determinate domande in base alla loro posizione politica. I dati di addestramento possono avere un ruolo significativo nella formazione di pregiudizi politici nei modelli di linguaggio. Se i dati di addestramento sono parziali o contengono pregiudizi, il modello potrebbe imparare e riprodurre questi</sample>
    <sample id="171">Secondo, come le modelli di linguaggio con differenti orientamenti politici si comportano effettivamente sui compiti di basso livello e se ciò potrebbe portare a problemi di parzialità nelle applicazioni NLP?</sample>
    <sample id="172">Quindi, in modo specifico, abbiamo proposto due modelli di linguaggio di prompt con formati di prompt diversi utilizzando i questionari politici, come il Political Compass test. Questo ci assicura di fare una valutazione automatica ben fondata nella letteratura di scienze politiche.</sample>
    <sample id="173">Quindi alcuni risultati preliminari dimostrano che i modelli di linguaggio di prima lingua hanno tendenze politiche che variano. Occupano tutti e quattro i quadranti della mappa politica.</sample>
    <sample id="174">Possiamo anche vedere che GPT-4 è il modello di linguaggio più liberale di tutti e i modelli GPT sono in generale più liberali socialmente rispetto ai modelli BERT e le sue varianti.</sample>
    <sample id="175">Secondo, vogliamo investigare in che misura i pregiudizi politici dei modelli di linguaggio vengono effettivamente trasmessi dal dataset di addestramento.</sample>
    <sample id="176">Quindi potremmo condurre un esperimento controllato pretrainando i punti di riferimento del modello linguistico su sei diversi corpus partizionati in notizie e social media, ulteriormente divisi per le loro inclinazioni politiche.</sample>
    <sample id="177">Dall'ulteriore pre-allenamento dei modelli di linguaggio su corpora partizionati, possiamo vedere che le coordinate ideologiche dei modelli di linguaggio si spostano corrispondentemente.</sample>
    <sample id="178">Ad esempio, per il Roberta finetuning e all'addestramento ulteriore sul corpus Reddit a sinistra, possiamo vedere un notevole spostamento liberale in termini di.</sample>
    <sample id="179">In termini di sue inclinazioni politiche.</sample>
    <sample id="180">E anche proviamo a indagare se i modelli di linguaggio possono rilevare la polarizzazione prevalente nella nostra società moderna.</sample>
    <sample id="181">Quindi dividiamo il corpus di pre-allenamento in due parti: prima e dopo il 45° presidente degli Stati Uniti. Alleniamo separatamente modelli di linguaggio sui due diversi corpus temporali.</sample>
    <sample id="182">Possiamo vedere che i modelli di linguaggio in generale hanno una tendenza politica che si allontana di più dal centro dopo il 2017. Questo indica che i modelli di linguaggio possono anche catturare la polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per concludere, valutiamo i modelli di linguaggio con differenti orientamenti politici in relazione alla rilevazione di discorsi razzisti e notizie false, due applicazioni NLP che spesso coinvolgono modelli di linguaggio e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">Quindi vediamo che se esaminiamo il rendimento per categoria, ossia se separiamo il rendimento in base alle categorie.</sample>
    <sample id="185">Possiamo vedere un pattern, ad esempio per la rilevazione di discorsi di odio, che i modelli di linguaggio di sinistra sono migliori.</sample>
    <sample id="186">Rilevare il discorso di odio rivolto a gruppi minoritari socialmente.</sample>
    <sample id="187">Tuttavia, i nostri sistemi per la rilevazione del discorso di odio sono più efficaci per i gruppi più potenti della nostra società.</sample>
    <sample id="188">E viceversa. I modelli di linguaggio lineari sono migliori nel rilevare il discorso di odio che mira a bianchi e uomini, ma peggiori nel rilevare il discorso di odio che mira a neri, LGBTQ + e altre comunità minoritarie.</sample>
    <sample id="189">Tendenze simili si verificano anche per la rilevazione delle fake news, dove si osserva che i modelli di linguaggio a sinistra sono migliori nel rilevare la disinformazione proveniente dal loro orientamento politico opposto e viceversa.</sample>
    <sample id="190">Questo articolo mostra molteplici esempi qualitativi per dimostrare che i modelli di linguaggio con differenti orientamenti politici.</sample>
    <sample id="191">Non posso fare previsioni diverse per esempi di discorsi di odio e fake news basati sulla loro categoria sociale. Ci sono molti esempi in più nell'appendice per sottolineare questo.</sample>
    <sample id="192">Questo indica che c'è un problema di equità molto urgente riguardo alle pregiudiziali politiche dei modelli di linguaggio.</sample>
    <sample id="193">Per esempio, se un modello di linguaggio addestrato su dati di discorso o informazioni false e simili fosse finetuned e poi distribuito su una piattaforma di social media popolare.</sample>
    <sample id="194">Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e che le parole odiose rivolte a gruppi minoritari potrebbero diventare diffuse senza alcun controllo.</sample>
    <sample id="195">Quindi, questo ci ha messo in allarme per riconoscere e affrontare le questioni di giustizia derivanti dalle inclinazioni politiche dei modelli di linguaggio.</sample>
    <sample id="196">Vorremmo anche sottolineare che esponiamo il dilemma unico riguardante le pregiudiziali politiche dei modelli di linguaggio. È come tra Scilla e Cariddi.</sample>
    <sample id="197">Quindi, se non si sanificano le opinioni politiche nel dataset di addestramento del modello linguistico, il bias si propagherà dal dataset di addestramento al modello linguistico e poi a task a basso livello, creando infine problemi di equità.</sample>
    <sample id="198">Se provassimo a sanzionare in qualche modo, rischieremmo anche di essere censurati o esclusi e è estremamente difficile determinare cosa sia effettivamente neutrale e dovrebbe essere mantenuta nel dataset di addestramento del modello di linguaggio. È un po' come il problema del carrello elettrico.</sample>
    <sample id="199">Okay, great! Penso che sia tutto ciò di cui ho bisogno per oggi. Cinque per oggi. Ti ringrazio per il tuo tempo.</sample>
    <sample id="200">Due. C'è il nome di Adele Villard e menzionato che è un lavoro in collaborazione con colleghi di Google Translate.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite fino a 2024 token di lunghezza del contesto.</sample>
    <sample id="202">Non so. Non hai detto chi è "loro" e cosa è il loro set di dati. Puoi fornire più dettagli?</sample>
    <sample id="203">Posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro demografiche, identità e esperienze di vita.</sample>
    <sample id="204">David.</sample>
    <sample id="205">Sì.</sample>
    <sample id="206">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni?</sample>
    <sample id="207">No, il modello non funziona bene sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono: 1) il setting di conoscenza di fondo a tema, 2) il setting di conoscenza di fondo sia a tema che durante l'infanzia, 3) il setting di conoscenza di fondo e durante l'infanzia. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="209">Non so. Potresti dare più informazioni sull'articolo?</sample>
    <sample id="210">L'ultima domanda di ricerca è: "Should we only use the clean samples for validation, or there are better ways to utilize them?"</sample>
    <sample id="211">La sensibilità misura la capacità del modello di produrre sempre gli stessi output per la stessa task, indipendentemente da lievi variazioni nella formulazione delle istruzioni.</sample>
    <sample id="212">Jin Wei Yi.</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="214">Non so. Non ho informazioni su questo. Potresti dare più dettagli?</sample>
    <sample id="215">20 campioni per classe.</sample>
    <sample id="216">Non so. Potresti dare più dettagli sull'articolo o cercare di trovare queste informazioni da qualche altra fonte?</sample>
    <sample id="217">Perché i modelli di linguaggio hanno vari inclinazioni politiche e occupano tutti e quattro i quadranti del compasso politico.</sample>
    <sample id="218">Makshita.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici va dal pretraining dei dati fino ai modelli di linguaggio e poi ai compiti di basso livello. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="220">Sì.</sample>
    <sample id="221">Non so se Coscript è disponibile pubblicamente. Potresti cercare su internet o contattare chi ha fatto questa ricerca per avere più informazioni.</sample>
    <sample id="222">In Watermark Injection, first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="223">I'm not sure. I don't have the information about the authors' affiliations. You could try looking for it in the article itself or in the references section.</sample>
    <sample id="224">Sì.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è "Make a chocolate cake".</sample>
    <sample id="226">Gli autori verificano la segretezza del loro metodo visualizzando le embedding delle frasi su quattro dataset.</sample>
    <sample id="227">Non c'è informazione sufficiente nel testo per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="228">Non so. Non hai detto quale Paese è meno allineato con GPT-4. Potresti fornire più informazioni?</sample>
    <sample id="229">Nel testo non c'è un esempio specifico come richiesto. Se hai bisogno di chiarimenti o se vuoi discutere di altro, fammi sapere.</sample>
    <sample id="230">Quando la quantità di attività aumenta, il modello raggiunge una performance migliore e contemporaneamente una sensibilità più bassa.</sample>
    <sample id="231">Non so quali siano. Potresti dare più dettagli o cercare di trovare informazioni su altri modelli?</sample>
    <sample id="232">I due coautori sono gli advisor del primo autore.</sample>
    <sample id="233">Non so chi sia il primo autore di PaLM. Potresti cercare su internet o consultare la documentazione ufficiale di PaLM?</sample>
    <sample id="234">Ciao a tutti, sono Jenny, un primo anno di P. C. all'Università Carnegie Mellon, e oggi presenterò il mio lavoro "Analisi posizionale: caratterizzazione di bias nei dataset e modelli".</sample>
    <sample id="235">Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'AI Lab, ovvero Sebastian Santi, Ronan Le Bras, Katerina Rynkova e Morten Sap.</sample>
    <sample id="236">Stiamo facendo un esercizio di immaginazione. Immagina di lavorare per un giornale e di passare attraverso i commenti sotto un tuo articolo di notizie cercando di rimuovere il contenuto tossico.</sample>
    <sample id="237">Potresti rivolgerti a un'API popolare come l'API Perspective per la rilevazione di tossicità e funziona veramente bene se sei Carl Jones, in cui l'API Perspective è in grado di rilevare correttamente le istanze tossiche.</sample>
    <sample id="238">Ma questo non è il caso per Aditya Sharma, dove le API di prospettiva non sono così sensibili a termini offensivi che sono più comuni in contesti indiani.</sample>
    <sample id="239">Ecco un esempio di pregiudizio di progettazione, dove vediamo differenze sistematiche di prestazioni della tecnologia tra popolazioni.</sample>
    <sample id="240">I biasi di design come quello che abbiamo visto prima potrebbero verificarsi a causa della posizionalità degli studiosi di NLP e dei sviluppatori di modelli. La posizionalità è semplicemente le prospettive che le persone hanno a causa delle loro demografie, identità e esperienze di vita.</sample>
    <sample id="241">Questo concetto è ampiamente utilizzato in studi critici, in particolare in spazi accademici femministi e queer.</sample>
    <sample id="242">E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e risultati perché può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Ecco la traduzione: "Ecco una domanda che le persone potrebbero chiedere: hanno i set di dati e i modelli posizionalità?"</sample>
    <sample id="244">Non stiamo dicendo che i modelli e i set di dati hanno identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni rispetto a altre.</sample>
    <sample id="245">Primo lavoro ha suggerito alcune prove aneddotiche di posizionalità, come divari culturali in modelli e set di dati, nonché definizioni teoriche di posizionalità del modello.</sample>
    <sample id="246">Tuttavia, queste opere non si concentrano realmente a confrontare gli utenti finali con i dataset e i modelli stessi.</sample>
    <sample id="247">La posizionalità del modello e del dataset sta diventando sempre più importante poiché i test di NLP stanno diventando sempre più soggettivi e orientati alla società.</sample>
    <sample id="248">E' difficile caratterizzare come queste posizioni sono scomode perché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro le API.</sample>
    <sample id="249">Per studiare la posizionalità del dataset e del modello, effettivamente confrontiamo le annotazioni con quelle degli utenti reali e con gli esistenti dataset e modelli.</sample>
    <sample id="250">Lo facciamo attraverso il nostro framework "NL Positionality".</sample>
    <sample id="251">Il nostro framework funziona in due passaggi principali.</sample>
    <sample id="252">Il primo passo è riassegnare i dataset con annotatori diversi.</sample>
    <sample id="253">E dovremmo farlo guardando le demografie dei set di dati originali, annotatori, perché di solito solo pochi annotatori annotano ogni istanza e perché le demografie sono di rado raccolte e condivise.</sample>
    <sample id="254">E quindi optiamo per riassegnare dati per ottenere molte annotazioni, per esempio, e per ottenere un set ricco di dati demografici.</sample>
    <sample id="255">Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando una punteggiatura di correlazione di Pearson.</sample>
    <sample id="256">Ecco il contenuto tradotto in italiano: "E quindi il nostro framework si differisce effettivamente dalla letteratura sulla disaccordo degli annotatori, comparando gli utenti finali con le previsioni e le etichette dei modelli e dei set di dati, invece di guardare solo all'accordo inter-annotatore o alle distribuzioni di modelli degli annotatori." Se hai bisogno di ulteriori traduzioni o hai domande sul contenuto, fammi sapere!</sample>
    <sample id="257">I framework sono in gran parte abilitati attraverso Lab in the Wild, una piattaforma di sorgente aperta online per cui ogni collaboratore di HCI.</sample>
    <sample id="258">Lab in the Wild è una piattaforma di esperimentazione online dove possiamo reclutare volontari diversi rispetto a piattaforme come MTurk, che hanno principalmente partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è ancora in grado di ottenere dati di alta qualità.</sample>
    <sample id="259">Organizziamo due compiti su Crowd in the Wild, uno di essi è l'accessibilità sociale. Funziona in questo modo: i partecipanti leggeranno una situazione tratta dal dataset di Social Chemistry e poi valuteranno quanto è socialmente accettabile una situazione.</sample>
    <sample id="260">Dopo aver studiato, per rimanere coinvolte nel processo di studio, possono confrontare le loro risposte con quelle di un AI e di altri.</sample>
    <sample id="261">Hanno poi confrontato queste annotazioni con Social Chemistry Delphi in GPT-4.</sample>
    <sample id="262">Poi abbiamo replicato un setup molto simile per la task di rilevamento della tossicità e dell'odio verbale, dove leggono un'istanza da Datasets di odio e valutano se pensano che sia un'istanza di odio verbale.</sample>
    <sample id="263">Poi abbiamo comparato queste annotazioni con l'API di Dinahate Perspective, Rewire API, Hate Roberta e GPT-4. La nostra ricerca ha raccolto oltre 16.000 annotazioni da oltre mille annotatori di 87 paesi.</sample>
    <sample id="264">Quindi ora siamo pronti per rispondere a "con chi si allineano più i set di dati e i modelli di NLP?". Troviamo che c'è una posizionalità nel NLP.</sample>
    <sample id="265">Per esempio, troviamo che i dataset e i modelli sono più allineati ai paesi di lingua inglese. Quindi, per l'analisi della social acceptability di GPT-4, troviamo che è più allineato ai paesi confuciani e di lingua inglese. Inoltre, troviamo che Dinah Hate è anche più allineato ai paesi di lingua inglese.</sample>
    <sample id="266">Anche noi troviamo una maggiore allineazione con le persone che hanno un'educazione universitaria. Quindi, per il compito di accettabilità sociale del GPT-4, troviamo che è più allineato alle persone con un'educazione universitaria o di laurea.</sample>
    <sample id="267">E troviamo la stessa cosa per Donny Hate, dove è più allineato a persone con un'educazione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i dataset sono allineati a specifiche popolazioni, alcuni sono inevitabilmente lasciati indietro.</sample>
    <sample id="269">Un esempio di questo è che i set di dati e i modelli sono meno allineati alle persone non binarie rispetto ai rispettivi omologhi maschili e femminili. Ciò si nota nel task di accettabilità sociale di GPT-4, così come nell'analisi del task di odio di DAIN.</sample>
    <sample id="270">Non ho capito bene la tua domanda. Potresti fornire più dettagli su "positionality in LP"? Così posso rispondere meglio.</sample>
    <sample id="271">Quindi abbiamo alcune raccomandazioni per questo. La prima è mantenere un registro di tutte le scelte di design rilevanti durante il processo di ricerca. L'altra è fare ricerche di NLP con il punto di vista di perspectivism.</sample>
    <sample id="272">La nostra terza raccomandazione è di costruire set di dati e modelli specializzati all'interno di quattro specifiche comunità. Un buon esempio di questo è l'iniziativa MUSC. Vogliamo sottolineare che l'NLP inclusivo non è solo far sì che tutte le tecnologie funzionino per tutti.</sample>
    <sample id="273">Ecco il contenuto tradotto in italiano: "Ecco la conclusione della nostra presentazione. Ma se desiderate imparare di più, potete controllare il nostro dashboard per i risultati di analisi più aggiornati e il nostro documento. Grazie." Se hai bisogno di altro, come ad esempio ulteriori traduzioni o chiarimenti, non esitare a chiedere.</sample>
    <sample id="274">Due.</sample>
    <sample id="275">Non c'è un modo semplice e efficace. Ci sono sfide come la censura o l'esclusione, e determinare ciò che è neutrale è difficile. Potrebbe essere un compromesso tra mantenere i dati originali e cercare di ridurre i bias. Ma non c'è una soluzione definitiva. Cosa ne pensi?</sample>
    <sample id="276">Ciao， sono Si Yuan dalla Fudan University. Sono qui per presentare il nostro lavoro: Distinguere il conoscere del testo da modelli di linguaggio per pianificare il linguaggio con vincoli.</sample>
    <sample id="277">Nella vita quotidiana, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni passo dopo passo sotto forma di script guidati.</sample>
    <sample id="278">Il lavoro precedente ha sfruttato i modelli di linguaggio per pianificare obiettivi astratti di attività stereotipiche, come fare un dolce, e ha dimostrato che i grandi modelli di linguaggio possono decomporre gli obiettivi in passaggi.</sample>
    <sample id="279">Tuttavia, il lavoro precedente si concentra principalmente sulla pianificazione per gli obiettivi astratti delle attività stereotipiche. La pianificazione per gli obiettivi con obiettivi specifici e vincoli specifici, come preparare un dolce al cioccolato, rimane ancora poco studiata.</sample>
    <sample id="280">Nel documento, definiamo il problema della pianificazione del linguaggio con vincoli.</sample>
    <sample id="281">Un obiettivo astratto può essere ereditato da diversi obiettivi specifici nella vita reale con diversi vincoli multi-faccettati. Un buon piano deve scrivere scenari che siano ragionevoli e fedeli ai vincoli.</sample>
    <sample id="282">Nel documento, valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di linguaggio a grandi scale.</sample>
    <sample id="283">Non esiste un set di dati specifico per identificare il nostro obiettivo.</sample>
    <sample id="284">Dobbiamo acquisire questo goal in primo luogo. Come mostrato nella tabella, estendiamo il goal astratto con restrizioni multiforme per l'acquisizione di dati di look data utilizzando l'instruttore GPT.</sample>
    <sample id="285">Eseguiamo una prova su 100 specifiche ragazze e valutiamo i script generati dai modelli di grandi linguaggi.</sample>
    <sample id="286">Questo tavolo riporta l'accuratezza complessiva dei risultati. Abbiamo trovato che tutti i modelli di linguaggio online ottengono risultati insoddisfacenti per quanto riguarda la pianificazione per obiettivi specifici.</sample>
    <sample id="287">Poi eseguiamo un'analisi dettagliata per investigare perché le reti neurali falliscono.</sample>
    <sample id="288">I risultati nella figura mostrano che la completezza semantica nei script generati è accettabile, ma la fedeltà alle restrizioni non può essere garantita.</sample>
    <sample id="289">Quando esploriamo le categorie di vincoli più granulari in WikiHow, il heatmap nella figura mostra che le prestazioni di pianificazione di InstructGPD variano notevolmente per obiettivi di diverse categorie.</sample>
    <sample id="290">Studi precedenti hanno mostrato che la qualità di output dei modelli di reti neurali ha una varianza elevata, che porta a una pessima prestazione. Pertanto, adottiamo l'idea del filtro over-generated Z per migliorare la qualità della generazione.</sample>
    <sample id="291">Per prima cosa, mostrare i tipi di vincoli con esempi per il modello di istruzione GPT e ottenere obiettivi specifici basati sugli obiettivi astratti definiti.</sample>
    <sample id="292">Mi dispiace, ma non capisco cosa stai chiedendo. Potresti fornire più dettagli?</sample>
    <sample id="293">Un modello di filtro è stato sviluppato per selezionare le scritture più adatte.</sample>
    <sample id="294">Convertiamo i script e i testi in vettori di embedding di GPT e calcoliamo le similitudini coseno e le punteggio di similitudine per misurare la similitudine semantica.</sample>
    <sample id="295">In attenzione, rileveremo il script che contiene le parole chiave del target constraint. Solo manteniamo il script se il target goal ha il punteggio più alto nel set di goal.</sample>
    <sample id="296">Con il nostro metodo, il CBT può generare script di qualità superiore. Il nostro metodo migliora notevolmente la pianificabilità, sia in termini di completezza semantica che di fedeltà alle restrizioni.</sample>
    <sample id="297">Poiché i modelli di linguaggio sono costosi da deporre, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. La creazione di un dataset è un passo essenziale per questo scopo.</sample>
    <sample id="298">Tuttavia, gli studi precedenti non consentono di pianificare per obiettivi specifici e l'annotazione manuale del dataset è costosa.</sample>
    <sample id="299">Pertanto, seguiamo l'idea di distillazione del sapere simbolico per distillare i dati di pianificazione del linguaggio vincolati da modelli di livello basso.</sample>
    <sample id="300">Applicheremo il nostro metodo per la costruzione di un dataset di pianificazione del linguaggio congiunto, chiamato CoScript.</sample>
    <sample id="301">In totale generiamo 55.000 script di test specifici. Per assicurare la qualità delle test site di validazione, chiediamo ai lavoratori di cloud sourcing di trovare e correggere le campioni errati.</sample>
    <sample id="302">Questa figura mostra la distribuzione dei vincoli di CoScribe. Troviamo che CoScribe mostra un'alta plausibilità nei generati specifici. Con CoScribe possiamo addestrare modelli più piccoli ma specializzati per la pianificazione linguistica con vincoli.</sample>
    <sample id="303">Abbiamo scoperto che i modelli finetunati su CoT possono generare script di qualità superiore rispetto a molti modelli a grandi scale, indicando che modelli più piccoli possono superare i modelli più grandi quando sono adeguatamente addestrati su dataset appropriati.</sample>
    <sample id="304">Abbiamo stabilito il problema di pianificazione del linguaggio con vincoli. Abbiamo valutato la capacità di pianificazione del linguaggio con vincoli dei modelli di linguaggio e abbiamo sviluppato un metodo di filtro per i modelli di linguaggio che sovragera.</sample>
    <sample id="305">Utilizziamo grandi modelli di linguaggio per generare un dataset di codice di alta qualità per pianificare il linguaggio con restrizioni. Speriamo che il dataset di codice possa essere un risorsa preziosa per avanzare la ricerca sulla pianificazione del linguaggio.</sample>
    <sample id="306">Grazie del tuo tempo. Per favore, cerca di trovare più dettagli su Cold Script nel nostro articolo.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi di punta.</sample>
    <sample id="308">Primo, deve essere applicabile all'inserimento di servizi. Secondo, la filigrana non deve indebolire l'utilità dei servizi forniti. Terzo, la filigrana deve essere abbastanza nascosta per l'attaccante, altrimenti l'attaccante può eliminarla facilmente. Quarto, la filigrana deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="309">Mi dispiace, non ho la lista delle 14 lingue specifiche. Potresti cercare su un sito ufficiale TED o un database di traduzioni. Se hai altre domande su TED Talks, sono qui per aiutarti.</sample>
    <sample id="310">Non c'è una quantità specifica menzionata per le istanze campionate per la riannotazione.</sample>
    <sample id="311">La cos e l2 similitudine.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati valutati su due gruppi di modelli. Uno è il gruppo di modelli con codificatori multilingue preaddestrati e decodificatori basati su punti, come XLM-R + PDR e BERT + PDR. L'altro gruppo è il gruppo di modelli a codificatore e decoditore, come M-BART e MT5.</sample>
    <sample id="344">Gli autori selezionano un set di parole di frequenza moderata basandosi su un corpus di testo generale raccolto dal provider e contando la frequenza delle parole.</sample>
    <sample id="345">Ciao a tutti. Il mio nome è Shuheng. Oggi presenterò il nostro articolo "Do Cornell 2003 Named Entity Taggers still work well in 2023?". Iniziamo.</sample>
    <sample id="346">Il nostro articolo ha indagato il problema della generalizzazione utilizzando la compito di riconoscimento di entità nominative, NER.</sample>
    <sample id="347">Abbiamo osservato che i modelli hanno utilizzato CoNLL-2003 per lo sviluppo di NER per quasi 20 anni e questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?</sample>
    <sample id="348">Quando sviluppiamo nuovi tagger, cosa serve per una buona generalizzazione?</sample>
    <sample id="349">Se osserviamo una pessima generalizzazione, ciò che causa la peggiorazione del rendimento di questi modelli è probabilmente l'overfitting. L'overfitting accade quando il modello si adatta eccessivamente ai dati di addestramento, imparando i dettagli specifici dei dati di addestramento invece di generalizzarli. Questo significa che il modello non riesce a prestare bene su dati nuovi o non visti durante l'addestramento. Se hai altre domande su questo argomento, non esitare a chiedere.</sample>
    <sample id="350">Per investigare questi problemi, abbiamo sviluppato il dataset Conll +. Questo è un dataset che abbiamo raccolto da Reuters News del 2020 e poi annotato con le stesse linee guida di annotazione Conll 2003.</sample>
    <sample id="351">Abbiamo finetunato oltre 20 modelli su Cornell 2003. Li abbiamo valutati sia sul test set Cornell 3 che sul test set Cornell + Post.</sample>
    <sample id="352">E in ultimo, abbiamo calcolato il cambiamento percentuale in F1 per valutare la generalizzazione di ciascun modello.</sample>
    <sample id="353">Per una buona generalizzazione, ci sono tre ingredienti principali che sono necessari.</sample>
    <sample id="354">La prima è l'architettura del modello. Durante le nostre esperimenti, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio a nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è la dimensione del modello. Abbiamo trovato che, di solito, modelli più grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">E, per finire, tutti sappiamo che il numero di esempi di finetuning ha un effetto diretto sulle prestazioni di una task a basso livello. Qui abbiamo anche scoperto che più esempi di finetuning porta a una miglior generalizzazione.</sample>
    <sample id="357">Cosa causa la diminuzione delle prestazioni di alcuni modelli?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'overfitting adattivo, che è l'overfitting causato dal riutilizzo della stessa test set ripetutamente. Questo si manifesta solitamente come una diminuzione dei ritorni su un nuovo test set.</sample>
    <sample id="359">La seconda ipotesi è il drift temporale, che è il degrado delle prestazioni causato dallo scarto temporale crescente tra i dati di addestramento e di test.</sample>
    <sample id="360">Per l'overfitting, abbiamo visto che dalla grafica a destra la linea di miglior fit rossa ha una pendenza maggiore di 1.</sample>
    <sample id="361">Questo significa che ogni unità di miglioramento che abbiamo fatto su Connell 2003 si traduce in più di una unità di miglioramento su Connell Plus, che significa che non ci sono ritorni decrescenti.</sample>
    <sample id="362">E questo ci mostra che nell'ipotesi di questo caso non si osserva sovrappopolamento adattivo.</sample>
    <sample id="363">Cosa c'è da dire sul tempo?</sample>
    <sample id="364">Per il drift temporale, abbiamo fatto un esperimento per riformare o continuare a preformare alcuni modelli con dati più recenti e abbiamo scoperto che la prestazione peggiora con un gap temporale più grande.</sample>
    <sample id="365">E conferma la nostra ipotesi che la principale causa della diminuzione del rendimento è la temperatura.</sample>
    <sample id="366">La nostra conclusione è che, per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, di un modello di dimensione maggiore e di esempi di finetuning in più, e questi obiettivi andano di pari passo. Non possiamo avere solo un ingrediente, ma gettare via tutti gli altri.</sample>
    <sample id="367">Al tempo stesso, abbiamo anche scoperto che la diminuzione del rendimento qui è causata da drift temporale e, sorprendentemente, non è causata da adattamento o overfitting, anche se Connell 2003 è stato utilizzato per oltre 20 anni.</sample>
    <sample id="368">Quindi tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo, funzionano ancora i tag di Connell 2003 nel 2023? E abbiamo scoperto che la risposta è un risoluto sì.</sample>
    <sample id="369">Speriamo che il nostro articolo stimoli più ricerche su come migliorare la generalizzazione dei modelli.</sample>
    <sample id="370">E infine, assicurati di controllare il nostro documento e il nostro set di dati. Se hai alcune domande, non esitare a farmele. Grazie mille.</sample>
    <sample id="397">The approach uses a segment size of 10 seconds.</sample>
    <sample id="398">Per risolvere il problema, serve conoscere che Servin è un giudice.</sample>
    <sample id="399">La qualità dell'esempio.</sample>
    <sample id="400">L'articolo si concentra sugli modelli linguistici GPT serie.</sample>
    <sample id="401">Non so. Non c'è nessuna informazione qui su questo. Potresti dare più dettagli?</sample>
    <sample id="402">Gli esempi di inferenza diretta sono direttamente riconoscibili, come dire il nome della canzone "Easy on Me" o la sua posizione, la prima.</sample>
    <sample id="403">I'm sorry, but I don't have the information about the affiliations of the authors in the given content. Can you provide more details or context?</sample>
    <sample id="404">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più dettagli?</sample>
    <sample id="405">No.</sample>
    <sample id="406">I autori hanno fornito l'esempio di "woman warrior" come gruppo contrassegnato.</sample>
    <sample id="407">Non hai fornito informazioni su quali architetture dei modelli non generalizzano in modo adeguato. Potresti fornire ulteriori dettagli?</sample>
    <sample id="408">Non so. Non hai fornito alcun dettaglio su quali siano i nomi dei set di dati di test. Potresti fornire più informazioni?</sample>
    <sample id="409">Due.</sample>
    <sample id="410">L'autore opera con più modalità.</sample>
    <sample id="439">Non so. Non hai fornito il contenuto inglese per riferimento. Puoi dirmi di più?</sample>
    <sample id="440">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="441">Sì.</sample>
    <sample id="442">Le risorse esistenti supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati.</sample>
    <sample id="443">Ciao, sto per parlare del nostro lavoro sul risolvere le espressioni di riferimento indirette per la selezione di entità, nel quale abbiamo introdotto l'entity scorer.</sample>
    <sample id="444">E il mio nome è Javad Hosseini e questo è un lavoro congiunto con Filip Radlinski, Silvia Parodi e Aric Wiss.</sample>
    <sample id="445">Hai inteso "Easy on Me" o "I Got a Feeling"? Ecco un utente che vuole scegliere tra questi due brani.</sample>
    <sample id="446">La cosa più ovvia è usare un riferimento diretto, ad esempio, dicendo "Il nome della canzone è "Easy on Me" o la sua posizione, la prima."</sample>
    <sample id="447">Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe accadere quando l'utente non riesce a ricordare il nome della canzone.</sample>
    <sample id="448">Tutte le pronunciazioni sono troppo simili tra loro e difficili da distinguere.</sample>
    <sample id="449">Quando il utente vuole specificare una preferenza. Ecco alcuni esempi di frasi indirette, ad esempio "il più recente" o "la canzone che non è energica".</sample>
    <sample id="450">Questo è un problema importante nei sistemi di conversazione e anche per il benchmarking degli algoritmi di comprensione dell'entità.</sample>
    <sample id="451">Non eravamo consapevoli di avere un dataset pubblico di grandi dimensioni per il test, quindi abbiamo raccolto uno utilizzando la curatela di massa. Il nostro dataset copre tre temi diversi: musica, libri e ricette.</sample>
    <sample id="452">La nostra metodologia di raccolta dei dati enfatizza l'informalità utilizzando un set di completamento di cartoni.</sample>
    <sample id="453">Ricordi quella canzone che ascoltavamo ieri? E con questo Bob mette in contesto la conversazione.</sample>
    <sample id="454">Credi che voglia dire "facile per me" o "ho un sentimento"?</sample>
    <sample id="455">C'è un'altra domanda. Nella terza bolla di dialogo, Bob fa riferimento indiretto a uno di questi enti, ad esempio, New York.</sample>
    <sample id="456">Il contenuto inglese è tradotto in italiano.</sample>
    <sample id="457">Mi dispiace, ma non hai fornito il contenuto inglese da tradurre. Puoi inviarmi il testo in inglese che vuoi tradurre in italiano?</sample>
    <sample id="458">Non capisco il significato di "A" e "B" che hai menzionato. Potresti fornire ulteriori dettagli?</sample>
    <sample id="459">Quando ci spostiamo più in alto nella lista, le entità diventano più simili tra loro e spesso è più difficile fare la disambiguazione.</sample>
    <sample id="460">Il primo è "uniforme".</sample>
    <sample id="461">Il secondo caso è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return".</sample>
    <sample id="462">Il terzo caso è quando hanno descrizioni simili su Wikipedia e infine quando hanno info box o attributi simili su Wikipedia, ad esempio la stessa genere o lo stesso artista per una canzone.</sample>
    <sample id="463">Quando mostriamo questa domanda alternativa agli ambasciatori, essi conoscono il nome di queste entità, ma non necessariamente sanno qualcosa sulle entità.</sample>
    <sample id="464">Quindi cosa facciamo è mostrare un po' di conoscenza di fondo riguardo alle due entità. Per le canzoni, semplicemente mostriamo un link di ricerca su Google per ciascuna canzone.</sample>
    <sample id="465">Ecco il risultato della ricerca Google per la canzone "Easy".</sample>
    <sample id="466">Per il dominio "ricette e libri" mostriamo alcuni testi di sfondo da Wikipedia. Per le ricette, mostriamo inoltre le loro immagini, anch'esse da Wikipedia, affinché gli annotatori sappiano come si presentano.</sample>
    <sample id="467">Allora chiediamo agli annotatori di scegliere una di queste entità, ad esempio la prima, e descriverle utilizzando tre a cinque espressioni indirette diverse.</sample>
    <sample id="468">Per esempio, il tipo con la musica per pianoforte. Ecco alcuni esempi dal nostro set di dati. Per esempio, il tipo senza parole, non il tipo con il ragazzino di 12 anni o il tipo fittizio o proveniente da altri generi musicali.</sample>
    <sample id="469">Il corpus delle entità ha 6.000 domande alternative in tre domini e ha 42.000 espressioni di riferimento indiretto. Risultati con il modello T5x large o riassunti brevi.</sample>
    <sample id="470">Se il modello di linguaggio ha accesso alla stessa conoscenza di fondo degli annotatori, la precisione è veramente alta, intorno al 92-95%, ma questo non è realistico.</sample>
    <sample id="471">Se il modello di linguaggio ha accesso a qualche conoscenza di fondo parzialmente sovrapposta, allora la precisione è tra il 82% e il 87%, che è più realistica, ad esempio quando il modello di linguaggio recupera la conoscenza di fondo.</sample>
    <sample id="472">Se il modello linguistico ha accesso solo ai nomi di entità, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili in ambito di dominio. Ecco il link al dataset. Grazie.</sample>
    <sample id="473">Con le politiche di riduzione del peso e l'accordo locale, e con le architetture state dell'arte specificamente adattate per la traduzione simultanea.</sample>
    <sample id="474">Mi dispiace, ma non ho informazioni sulle affiliazioni degli autori dell'articolo. Potresti provare a cercare su altre fonti?</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Tre.</sample>
    <sample id="477">Ciao， sono Sarah Pape， dall'Università di Toronto e dalla Fondazione Bruno Kessler， e presenterò brevemente "The Attention as a Guide for Simultaneous Speech Translation"， un lavoro a joint con Matteo Negri e Marco Zorzi.</sample>
    <sample id="478">La traduzione simultanea vocale, o Simultaneous Speech Translation, SST, è il processo di tradurre il linguaggio parlato in testo in un'altra lingua in tempo reale, permettendo la comunicazione tra lingue diverse.</sample>
    <sample id="479">I modelli di similitudine attuali hanno problemi. Le specifiche architetture sono solitamente addestrate, introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">Procedimenti di addestramento lunghi e complessi, ad esempio addestramento coinvolgente di diversi obiettivi di ottimizzazione.</sample>
    <sample id="481">Addestrare e mantenere diversi modelli per raggiungere diversi regimi di latenza, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza e così via.</sample>
    <sample id="482">Cosa è la nostra soluzione?</sample>
    <sample id="483">Prima di tutto, utilizzare modelli di istruzione offline esistenti senza rieducazione o adozione di una specifica architettura per il CLST. Usare solo un modello per ogni regime di latenza e gestire la latenza attraverso parametri specifici.</sample>
    <sample id="484">E utilizza il contenuto già acquisito dal modello attraverso il meccanismo di attenzione tra l'input audio e l'output testuale, ovvero il meccanismo di attenzione incrociata. E puoi vedere un esempio a destra.</sample>
    <sample id="485">La nostra soluzione è di proporre un encoder-decoder con attenzione, e è una strategia in cui decidiamo di emettere o non emettere una traduzione parziale in base a dove l'attenzione punta.</sample>
    <sample id="486">Se la tensione non è concentrata, ossia se la somma è inferiore a un certo soglia alpha, verso le ultime lambda di frame di parola, significa che l'informazione ricevuta è abbastanza stabile.</sample>
    <sample id="487">"Voglio parlare di"</sample>
    <sample id="488">E guarderemo i pesi di attenzione reciproca.</sample>
    <sample id="489">Vedremo che le prime due parole puntano ai primi frame di parola ricevuti, mentre la parola finale puntano agli ultimi frame di parola ricevuti. E poi ci sono i frame di parola Lambda.</sample>
    <sample id="490">Questo significa che le prime due parole saranno eliminate.</sample>
    <sample id="491">Mentre la somma delle attenzioni incrociate è superiore a un certo livello α, non emetteremo l'ultimo pezzo e aspetteremo un altro frammento di discorso.</sample>
    <sample id="492">Se andiamo avanti e riceviamo un altro frammento di discorso e il nostro modello predice altri tre parole e guarderemo i pesi di attenzione incrociata.</sample>
    <sample id="493">Non vedremo che le parole puntano alle ultime λ λ λ frame di discorso.</sample>
    <sample id="494">Questo significa che queste tre parole saranno eliminate.</sample>
    <sample id="495">Se guardi i principali risultati di una ricerca.</sample>
    <sample id="496">Vedremo di rappresentare i risultati simultanei di traduzione su grafici in cui avremo il blu da un lato che misura la qualità della traduzione e il rosso dall'altro che misura il ritardo medio.</sample>
    <sample id="497">La misura di latenza e consideriamo anche il calcolo consapevole di media di gradimento che tiene conto del tempo computazionale del modello per fornire l'output.</sample>
    <sample id="498">Quindi vogliamo che le cure siano il più in alto possibile su questa pianta.</sample>
    <sample id="499">Ma anche vogliamo che siano spostati a sinistra.</sample>
    <sample id="500">E compare con strategie appropriate che sono applicate anche ai modelli offline, ovvero la strategia di chiave di peso e l'accordo locale. E compare anche con l'architettura di punta specificamente adattata per la traduzione simultanea.</sample>
    <sample id="501">Mi dispiace, ma non ho capito bene la tua domanda. Potresti fornire il contenuto inglese che vuoi tradurre in italiano?</sample>
    <sample id="502">E vediamo che AD outperforms tutte le strategie applicate ai modelli offline poiché le loro curve sono spostate a sinistra.</sample>
    <sample id="503">E anche notiamo che se consideriamo il tempo di elusione reale o il tempo di lavoro computazionale, ADOT è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire più risultati, leggi il nostro articolo e abbiamo anche rilasciato il codice e i modelli open source e l'output simultaneo per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="505">Non so se il set di dati è disponibile pubblicamente. Potresti controllare il link o la fonte dove hai trovato questa informazione. Se hai altre domande, puoi chiedere.</sample>
    <sample id="506">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang e io presenteremo la nostra ricerca su Multi-Instruction: migliorare l'apprendimento zero-shot dei modelli multimodal tramite l'addestramento con istruzioni.</sample>
    <sample id="507">Con l'avanzamento dei modelli di linguaggio a grandi scale, molte opere hanno iniziato a esplorare nuovi paradigmi di apprendimento per riutilizzare i modelli di linguaggio preaddestrati per compiti diurni diversi in modo efficiente in termini di parametri e dati.</sample>
    <sample id="508">Recentemente, molte ricerche hanno dimostrato che l'addestramento di istruzioni consente ai modelli di linguaggio a grandi scale di eseguire compiti di NLP in modo zero-shot seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei lavori precedenti sulla ottimizzazione delle istruzioni si sono concentrati sul miglioramento del presto zero-shot nelle sole compiti di lingua, mentre le compiti di visione computer e multimediali sono state trascurate.</sample>
    <sample id="510">Pertanto, in questo lavoro, vogliamo investigare se l'addestramento di istruzioni a modelli preaddestrati multimodal può realmente migliorare la generalizzazione a compiti multimodal non visti.</sample>
    <sample id="511">Inoltre, durante la nostra ricerca, abbiamo scoperto una considerevole disparità nella disponibilità dei set di dati di istruzione tra LPA e multimodal.</sample>
    <sample id="512">Esistono più di 1.600 task di istruzione unicamente linguistico. Tuttavia, non esiste un dataset di istruzione multimodale di grande scala e pubblicamente disponibile. Questo ci ha motivati a costruire un dataset di istruzione di tuning multimodale.</sample>
    <sample id="513">Ecco il contenuto in italiano: "Presentiamo MultiInstruct, il primo benchmark di ottimizzazione di istruzioni multimediali che comprende 62 compiti multimediali diversi coprendo 10 categorie di base."</sample>
    <sample id="514">Questi compiti sono derivati da 21 set di dati open source esistenti e ogni compito è dotato di 5 istruzioni scritte da esperti.</sample>
    <sample id="515">Per l'indagine sull'addestramento di modello multimodale con il nostro dataset proposto, prendiamo OFA, un modello preaddestrato multimodale unificato, come modello base. OFA usa un vocabolario unificato per i token di lingua, immagine e le coordinate di un rettangolo di confine.</sample>
    <sample id="516">Ecco alcuni esempi di istanze dalla nostra dataset multistrato.</sample>
    <sample id="517">Per unificare il trattamento di diversi tipi di dati di input e output.</sample>
    <sample id="518">Seguiamo il metodo di OFA e formuliamo tutte le tare in un formato di sequenza a sequenza unificato, in cui il testo di input, le immagini, le istruzioni e i box di confinamento sono rappresentati nello stesso spazio di token.</sample>
    <sample id="519">Certo. Ma prima di tradurre, potresti dirmi qualcosa in italiano su multimodal instruction tuning? Così posso capire meglio cosa tradurre.</sample>
    <sample id="520">Per il set di dati di addestramento, utilizziamo 53 task dal gruppo NAG per l'addestramento e selezioniamo 10.000 istanze per task. Per il test, riserviamo l'intero gruppo di ragionamento comune per il test e selezioniamo altri 5 task dal gruppo VQA e il gruppo variegato.</sample>
    <sample id="521">Utilizziamo tutte le istanze nel set di test per ogni compito. Inoltre, estraiamo a caso 20 compiti dal set di test di Natural Instruction come compiti di sintesi per NLP.</sample>
    <sample id="522">Quindi utilizziamo un modello preaddestrato di BERT Large come modello base. Durante l'addestramento, mescoliamo tutte le istanze per tutte le tare. Ogni istanza viene combinata casualmente con uno dei suoi cinque modelli di istruzione.</sample>
    <sample id="523">Quindi durante il test, per ogni compito, condurremo un totale di cinque esperimenti valutando il modello utilizzando una delle cinque istruzioni in ogni esperimento.</sample>
    <sample id="524">Rapporta la media e la deviazione standard del rendimento in tutte e cinque le esperimenti.</sample>
    <sample id="525">Se il compito è una classificazione multi-modello, rapportiamo l'accuratezza. Se è una generazione multi-modello, rapportiamo l'ROUGE-L. Per i compiti di NLP, rapportiamo anche l'ROUGE-L.</sample>
    <sample id="526">Abbiamo anche introdotto una nuova metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre sempre gli stessi output per la stessa compito, indipendentemente da lievi variazioni nella formulazione delle istruzioni.</sample>
    <sample id="527">Ecco il nostro risultato principale: come possiamo vedere, l'addestramento di istruzione può migliorare significativamente le prestazioni di OFA su compiti multimodali non visti.</sample>
    <sample id="528">Anche il trasferimento di apprendimento da dataset di istruzioni naturali può beneficiare dell'ottimizzazione delle istruzioni.</sample>
    <sample id="529">Ecco il contenuto in italiano: "Come aumenta la quantità di compiti, il modello raggiunge prestazioni migliori e, al contempo, una sensibilità più bassa."</sample>
    <sample id="530">Abbiamo anche fatto un esperimento. Abbiamo utilizzato un insieme di istruzioni contro cinque insiemi di istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni complessive del modello e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Quindi questo mostra l'effetto di diverse strategie di finetuning sulla sensibilità del modello. Come possiamo vedere, tramite il transfer learning dal dataset di istruzioni naturali, il modello può ottenere una sensibilità molto migliore rispetto al modello originale di YF1.</sample>
    <sample id="532">Possiamo anche vedere che il trasferimento di apprendimento da dataset di istruzioni naturali può aiutare OFA a ottenere una prestazione molto migliore sul dataset di istruzioni naturali.</sample>
    <sample id="533">Quindi, in generale, abbiamo proposto il primo dataset di addestramento di istruzioni a scala larga multi-modello, che ha migliorato significativamente la capacità di istruzione a lungo raggio dell'IA. Esploriamo diverse tecniche di apprendimento a trasferimento e mostriamo i loro vantaggi. Abbiamo progettato un nuovo metrico chiamato sensibilità.</sample>
    <sample id="534">C'è un'altra cosa. Stiamo raccolgendo un insieme di dati di addestramento di istruzioni multimodale molto più grande, con circa 150 task di visione e linguaggio aggiuntivi, e li rilasceremo presto. Questo è il codice QR per i nostri dati e il modello. Grazie.</sample>
    <sample id="535">Sarah Pape è affiliata all'Università di Toronto e alla Fondazione Bruno Kessler. Matteo Negri e Marco Zorzi non sono specificati come affiliati.</sample>
    <sample id="536">Il nome del relatore è Javad Hosseini.</sample>
    <sample id="562">Ciao a tutti, sono Costas Finner e sono lieto di accogliervi al nostro talk sul nostro articolo di ACL 2023 "I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti al contesto".</sample>
    <sample id="563">C'è un lavoro di gruppo con John Gottlieb, Aaron Muller, Kanishka Mishra, Karen Fuentes, Roger Levy e Athena Williams.</sample>
    <sample id="564">In questo lavoro rivediamo il paradigma dei minimi paia.</sample>
    <sample id="565">Il paradigma delle coppie minimali valuta i modelli di linguaggio sulla base di giudizi di accettabilità, che possono includere grammaticalità, come sintassi errata o accettabilità in termini di stereotipi, come le coppie di cugine.</sample>
    <sample id="566">Nel paradigma del minimal pair, il modo tipico per valutare i modelli di linguaggio è mostrare una frase accettabile o grammaticalmente corretta e poi una frase non accettabile o grammaticalmente errata.</sample>
    <sample id="567">E allora la speranza è che il modello metta in modo più probabile le frasi accettabili.</sample>
    <sample id="568">Il pipeline attuale di MPP non ci permette di valutare l'accettazione del modello verso le frasi più lunghe.</sample>
    <sample id="569">Questi giorni i modelli di linguaggio a grandi scale stanno presentando finestre di contesto sempre più lunghe, quindi è cruciale valutare l'acceptability del modello attraverso tutta la finestra di contesto.</sample>
    <sample id="570">Ecco il contenuto tradotto in italiano: "E questo è ciò che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline di T5P chiedendo al modello di valutare l'accettabilità su sequenze sempre più lunghe." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="571">Quindi, l'approccio è questo. Quindi cosa facciamo è simulare queste sequenze più lunghe. Rivediamo i dataset stessi e poi ricreiamo frasi scegliendo frasi accettabili o non accettabili da quei dataset.</sample>
    <sample id="572">Per esempio, qui abbiamo scelto una tipica coppia di grammaticità dal dataset Bliem, dal caso dell'isola adiuvante.</sample>
    <sample id="573">E cosa facciamo è creare sequenze più lunghe che sono accettabili e che hanno la stessa corrispondenza della struttura grammaticale. Estraiamo frasi grammaticali da Adjacent Island.</sample>
    <sample id="574">E poi lo aggiungiamo come prefisso sia alla query accettabile che alla query non accettabile.</sample>
    <sample id="575">Quindi possiamo fare la stessa cosa scegliendo frasi non accettabili dallo stesso matching e questo potrebbe anche essere utilizzato per testare l'acceptability del modello.</sample>
    <sample id="576">E possiamo fare lo stesso scegliendo frasi da un sottoinsieme diverso o da un diverso dataset. Quindi questo è ciò che chiamiamo "situazione di malaccoppiamento".</sample>
    <sample id="577">Quindi le frasi stanno ancora venendo da dataset rilevanti, ma non sono dallo stesso dataset che stai valutando. E possiamo fare lo stesso per il caso di accettabilità.</sample>
    <sample id="578">Infine, possiamo scegliere frasi da un dominio completamente diverso, come Wikipedia.</sample>
    <sample id="579">Quindi questo ci dirà se le giudicazioni di accettabilità dei modelli sono effettivamente influenzate da qualsiasi contesto.</sample>
    <sample id="580">Se il contesto proviene da un sottoinsieme diverso del dataset o se è completamente irrilevante per la frase che stiamo analizzando.</sample>
    <sample id="581">Prima di tutto, guardiamo le frasi di Wikipedia che sono completamente irrilevanti per la coppia di query attuale. Poi scopriamo che i giudizi MPP sono in gran parte robusti per i contesti di lunghezza arbitraria.</sample>
    <sample id="582">Abbiamo aumentato la lunghezza del contesto fino a 2024 per massimizzare i modelli OPT e GPT-2 e abbiamo visto qui nella linea punteggiata arancione che i giudizi MPP sono relativamente stabili.</sample>
    <sample id="583">Quando scegliamo frasi dallo stesso dataset?</sample>
    <sample id="584">Quindi qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili dallo stesso dataset di testo di Blimp.</sample>
    <sample id="585">E allora vediamo che le giudicanti MPP aumentano o diminuiscono significativamente quando si aggiungono sia prefissi accettabili sia prefissi non accettabili.</sample>
    <sample id="586">Ma quando si adatta la struttura, ossia quando si sceglie le frasi da fenomeni simili nel testo di colpevolizzazione.</sample>
    <sample id="587">Vediamo un aumento o una diminuzione massiva della valutazione MPP del modello a seconda di se il prefisso scelto è accettabile o non.</sample>
    <sample id="588">E questo è molto grande, come questo effetto aumenta a lungo termine. E questo probabilmente influenzerà modelli di linguaggio più recenti, che hanno finestre di contesto grandi.</sample>
    <sample id="589">Ecco la traduzione: "Quindi, perché il prefisso di corrispondenza influenza tanto la giudicata del modello di linguaggio?"</sample>
    <sample id="590">Ecco la traduzione in italiano: "Quindi abbiamo fatto una serie di analisi in cui abbiamo cercato di perturbare la frase d'ingresso, cercando di preservare la struttura rilevante, ma aggiungendo rumore all'ingresso. Dopo aver fatto diverse di queste perturbazioni." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="591">Abbiamo scoperto che nessuno di questi rumori sta realmente facendo che il modello cambi la sua traiettoria in termini di come ci mostra che le feature sono state addestrate.</sample>
    <sample id="592">In sostanza, troviamo che i modelli sono sensibili alle perturbazioni di frasi in modo simile.</sample>
    <sample id="593">Quando perturbiamo le frasi nell'area accettabile, vediamo un aumento simile in tutte le perturbazioni. Quando perturbiamo le frasi nell'area non accettabile, vediamo una diminuzione delle giudizie di MPP in modo simile.</sample>
    <sample id="594">Istruzioni: Traduci il contenuto inglese in italiano.</sample>
    <sample id="595">Evaluare il MPP, la maniera in cui lo facciamo attualmente con input brevi e singole frasi, potrebbe non catturare completamente il sapere astratto dei modelli di linguaggio attraverso il contesto.</sample>
    <sample id="596">Per favore, leggi il nostro articolo per ulteriori dettagli sulle nostre esperimenti. Ti ringraziamo per l'ascolto.</sample>
    <sample id="597">unordered multiset</sample>
    <sample id="598">55,000.</sample>
    <sample id="626">Il metodo di allineamento migliore per DEplain è il metodo di mass align.</sample>
    <sample id="627">Non c'è un vantaggio specifico di apprendimento scarsamente supervisionato menzionato qui. Se hai altre domande su questo argomento, puoi chiedere di chiarire o di esplorare ulteriori aspetti.</sample>
    <sample id="628">Non c'è informazione sufficiente nel testo per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato raccolgendo notizie da Reuters nel 2020 e poi annotandole secondo le stesse linee guida di annotazione di CoNLL 2003.</sample>
    <sample id="630">Ciao a tutti, il mio nome è Justin John e sono dell'Università di Pennsylvania. Oggi presenterò il nostro lavoro: "Esempi di analisi semantica multilingue in diverse rappresentazioni naturali".</sample>
    <sample id="631">La semantica di parsing è una task per costruire rappresentazioni semantiche di query di utente come SQL e calcolo Lambda.</sample>
    <sample id="632">La traduzione del contenuto inglese in italiano è un compito che richiede una buona conoscenza della lingua italiana e del contesto in cui viene utilizzato il testo inglese. Se hai bisogno di tradurre un testo specifico, puoi dirmelo e lo tradurrò per te.</sample>
    <sample id="633">Ecco il contenuto tradotto in italiano: "Come mostrato in questa figura, dobbiamo tradurre la query in diverse lingue naturali utilizzando modelli neurali in SQL, Lambda o FunQL e così via." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="634">Esistono modelli di parsing semantico interlingua già proposti e valutati su set di dati di compiti e applicazioni limitati, ad esempio.</sample>
    <sample id="635">C'è un mancato copertura di certi linguaggi naturali e il cinese.</sample>
    <sample id="636">C'è un mancato copertura su certe rappresentazioni minimi.</sample>
    <sample id="637">Il calcolo lambda è mancante.</sample>
    <sample id="638">Ovvero vengono valutati solo su certi modelli neurali, ad esempio esiste solo un singolo modello per valutarli.</sample>
    <sample id="639">Per questo scopo, proponiamo Exemplar, che fornisce un insieme di dati uniforme per il parsing semantico interlingua in più lingue naturali e rappresentazioni.</sample>
    <sample id="640">Contiene 90 dataset in diversi domini, 5 task di parsing semantico, 8 milioni di rappresentazioni e 22 lingue naturali in 15 famiglie linguistiche.</sample>
    <sample id="641">E per valutare meglio il nostro benchmark, consideriamo i sei parametri di allenamento e valutazione.</sample>
    <sample id="642">Certo. Potresti fornire il testo inglese da tradurre?</sample>
    <sample id="643">Ad esempio, abbiamo addestrato un modello in inglese su query in inglese e durante l'inferezione, traduciamo la query in tedesco utilizzando un API in inglese e poi utilizziamo il modello addestrato per prevedere il SQL.</sample>
    <sample id="644">E testeremo anche il modello monolinguale.</sample>
    <sample id="645">Non capisco bene la tua domanda. Potresti fornire il contenuto inglese che vuoi tradurre in italiano?</sample>
    <sample id="646">Abbiamo anche testato la configurazione di fusione monolinguale, addestrando modelli monolinguali con solo il 10% dei dati di addestramento.</sample>
    <sample id="647">E' un modello multilingue, ma si addestra un solo modello multilingue per tutte le lingue.</sample>
    <sample id="648">Per esempio, mettiamo insieme le query in tedesco, inglese e cinese per addestrare un modello multilingue e durante l'inferezia possiamo utilizzare questo modello.</sample>
    <sample id="649">Certo. Ma non hai fornito il contenuto inglese da tradurre. Puoi inviarmelo?</sample>
    <sample id="650">E consideriamo anche il trasferimento zero-shot e few-shot tra lingue. Alleniamo su una lingua di origine e trasferiamo su un'altra lingua.</sample>
    <sample id="651">Quindi durante l'addestramento, addestreremo su query in inglese o su una combinazione di query brevi in inglese e tedesco per addestrare un modello multilingue e prevedere l'output SQL.</sample>
    <sample id="652">E anche troviamo molti risultati interessanti. Quindi, riguardo all'analisi dei modelli monolinguistici, valuteremo due gruppi di modelli.</sample>
    <sample id="653">Includendo il PTR, che sta per encoder multilingue preaddestrati con decoder basati su pointer, come XLM-R + PTR e BERT + PTR.</sample>
    <sample id="654">E valutiamo anche i modelli encoder-decoder, che sono modelli encoder-decoder multilingue preaddestrati, come MBart e MT5.</sample>
    <sample id="655">Abbiamo trovato che l'encoder-decoder ottiene il miglior rendimento su tutti e nove i dataset.</sample>
    <sample id="656">Evaluiamo su MT - 5 e XLM - R + PDR in un contesto multilingue.</sample>
    <sample id="657">Abbiamo scoperto che l'encoder-decoder o l'encoder-PTR possono essere migliorati addestrando in una miscela di varie lingue.</sample>
    <sample id="658">E abbiamo scoperto che questo perché la maggior parte delle lingue naturali principali può ottenere un miglioramento di prestazioni, eccetto che per l'inglese, la cui prestazione diminuisce in sette set di dati e aumenta solo in tre set di dati.</sample>
    <sample id="659">Credo che questo sia noto come "maledizione della multilinguismo".</sample>
    <sample id="660">Abbiamo anche confrontato il gap di prestazioni tra lingue diverse.</sample>
    <sample id="661">Nella figura la linea blu rappresenta il trasferimento di linguaggio tra lingue diverse, la linea arancione rappresenta il trasferimento di linguaggio zero-shot tra lingue diverse, mentre la linea verde rappresenta la configurazione del modello di linguaggio.</sample>
    <sample id="662">Abbiamo trovato che, confrontando la linea verde e la linea arancione, per il setting di zero-shot, il gap di prestazione della trasferimento tra lingue è significativo. Confrontando la linea blu e la linea arancione, abbiamo trovato che, per il setting di few-shot, il gap di trasferimento si riduce rapidamente.</sample>
    <sample id="663">Abbiamo anche trovato alcuni altri interessanti risultati. Ad esempio, l'encoder-decoder ha superato il lavoro precedente o ha ottenuto risultati paragonabili per la traduzione di lingue naturali in inglese e ha significativamente migliorato le prestazioni di FewShot per lingue naturali di target.</sample>
    <sample id="664">Ecco la traduzione in italiano: "E siamo trovati che i modelli multilingue come Codex e Bloom sono ancora insufficienti per i compiti di parsing semantico interlingue." Se hai bisogno di chiarimenti o hai altre domande, fammi sapere!</sample>
    <sample id="665">Ecco la traduzione: "Costruiremo Exemplar, un benchmark unificato per il parsing semantico a diversi angoli con diverse lingue naturali e rappresentazioni semantiche." Se hai bisogno di ulteriori traduzioni o hai domande sul contenuto, fammi sapere!</sample>
    <sample id="666">Eseguiamo una completa analisi di benchmark su tre tipi rappresentativi di modelli multilingue. I nostri risultati mostrano molte interessanti scoperte. E così via. E benvenuti a visitare il nostro documento e il codice. Grazie per l'ascolto.</sample>
    <sample id="667">Non hai fornito abbastanza informazioni per rispondere a questa domanda. Potresti fornire dettagli su quali lavori stai parlando?</sample>
    <sample id="668">No, sono insufficienzi per il CLSP. Se hai altre domande o vuoi discutere di questo argomento, sentiti libero di chiedere.</sample>
    <sample id="695">Induce l'alignment come parte della formazione.</sample>
    <sample id="696">L'equità di un modello NLP a valle si riferisce a come il modello tratta diverse categorie di dati o gruppi di persone in modo equo e non discriminatorio. Se un modello è equo, dovrebbe fornire risultati simili per input simili indipendentemente da fattori come la razza, il genere o l'età.</sample>
    <sample id="697">Janice Lavoie.</sample>
    <sample id="698">Costa Senna.</sample>
    <sample id="699">Myra.</sample>
    <sample id="700">Il tropicalismo indica un insieme di stereotipi che associa le donne latine a essere vivaci e curvilinee.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target attraverso parole come cultura, tradizione, orgoglio e esotico. Queste parole definiscono i gruppi solo in relazione alla loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="702">P6CXML.</sample>
    <sample id="703">DrBERT è una versione con 7GB di "nachos" e ChuBERT ha 4GB di "nachos" e 4GB di "sentences" da "clean conduct".</sample>
    <sample id="751">Due.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un metodo che aggiorna il modello con il set di dati più recente raccolto durante ogni iterazione di apprendimento attivo.</sample>
    <sample id="753">L'obiettivo del set di dati è capire la lingua dell'utente quando vuole prendere una decisione.</sample>
    <sample id="754">Non c'è informazione sufficiente nel testo per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="755">Tre.</sample>
    <sample id="756">Non so quanti annotatori sono stati impiegati per creare il set di dati iniziale. Potresti cercare ulteriori informazioni o fornire più dettagli?</sample>
    <sample id="757">I'm sorry, but the information about the affiliations of the authors is not provided in the text you've given. You might need to look for more details in the full article or the authors' profiles.</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">Non so qual è il sistema di dialogo all'avanguardia. Potresti dare più dettagli o specificare il contesto?</sample>
    <sample id="760">Perché i modelli con finestre di contesto più lunghe sono sempre più comuni.</sample>
    <sample id="761">Sì.</sample>
    <sample id="762">No, gli annotatori non necessariamente conoscono l'entità in anticipo.</sample>
    <sample id="763">Non so qual è la metrica di MT utilizzata per la valutazione. Potresti dare più dettagli?</sample>
    <sample id="764">No, il regresso nella generalizzazione non influisce su specifici tipi di NER. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="765">Non c'è una risposta diretta nella tua domanda inglese. Potresti fornire più dettagli o chiarire la tua domanda?</sample>
    <sample id="766">Non so. Potresti cercare su internet o chiedere a un esperto.</sample>
    <sample id="767">Non so qual è il modello specifico che fanno ricorso per il trasferimento dell'apprendimento. Potresti dare più dettagli o chiarire meglio la tua domanda?</sample>
    <sample id="768">Non so qual è il recente set di test utilizzato per valutare le capacità di PaLM. Potresti cercare su internet o chiedere a un esperto in questo campo.</sample>
    <sample id="769">Tre.</sample>
    <sample id="770">Non so. Non hai detto cosa sia il metodo di riferimento. Potresti fornire più dettagli?</sample>
    <sample id="771">The name of the presenter is Shuheng.</sample>
    <sample id="772">Sì.</sample>
    <sample id="773">Non so quanti modelli più piccoli vengono utilizzati nell'articolo. Potresti dare più dettagli o informazioni?</sample>
    <sample id="774">OFA viene utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="833">L'autore, Aida Villar, lavora con i colleghi di Google Translate.</sample>
    <sample id="834">Non so.</sample>
    <sample id="835">Non so qual è la lingua di partenza e la lingua di arrivo analizzate nell'articolo. Potresti dare più dettagli?</sample>
    <sample id="836">Zhang Bing.</sample>
    <sample id="837">Hanno studiato due modelli: il modello di LongImpart finetuned per produzioni di semplificazioni a livello di documento e il modello di LongImpart basato su normal finetuned per produzioni di semplificazioni a livello di frase.</sample>
    <sample id="838">Per scopi di addestramento vengono utilizzate 53 attività e per test vengono utilizzate 62 attività.</sample>
    <sample id="839">Non so quanti autori sono coinvolti nell'articolo. Potresti dare più informazioni?</sample>
    <sample id="840">Gli autori hanno effettuato i test su 4 set di dati: AG News, Mind, SST-2 e IRIS Spam.</sample>
    <sample id="876">NACHOS è un dataset di dati cronologici medici raccolti da internet.</sample>
    <sample id="877">Il nome della relatrice o del relatore è Aida Villar.</sample>
    <sample id="878">La strategia del prompting ha un grande impatto sui risultati.</sample>
    <sample id="879">Non so. Non ho informazioni sulle affiliazioni degli autori dell'articolo. Potresti cercare di trovare queste informazioni su un database di ricerca accademica o sul sito web dell'istituto dove lavorano.</sample>
    <sample id="880">Non ho informazioni su quali siano le 5 istruzioni scritte da esperti. Potresti fornire ulteriori dettagli o contesto?</sample>
    <sample id="881">Proppongono un compito di risoluzione di riferimento comune.</sample>
    <sample id="882">Ciao a tutti. Il mio nome è Aida Villar e vorrei dare un breve riepilogo del documento "Translation from the printing press: assessment strategies and performance". Questo è un lavoro in collaborazione con i miei colleghi di Google Translate.</sample>
    <sample id="883">La lingua di Balm è un modello di linguaggio a 5,4 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta collezione di testi, composta da 780 miliardi di token.</sample>
    <sample id="884">Nel campo della fabbricazione, è raggiunto uno stato dell'arte in centinaia di milioni di posti di lavoro.</sample>
    <sample id="885">Nel lavoro presentiamo lo studio sistematico più ampio di modelli di linguaggio di punta per la traduzione automatica.</sample>
    <sample id="886">Abbiamo valutato la capacità di traduzione dei modelli di ricerca utilizzando le migliori pratiche della comunità AMT. Questo implica l'uso dei set di test più recenti per evitare che i dati di test si sovrappongano con i dati di addestramento del modello di linguaggio.</sample>
    <sample id="887">Quando si confrontano due sistemi all'avanguardia, il sistema con la migliore prestazione in valutazione di doppia valutazione è sicuramente il migliore.</sample>
    <sample id="888">Utilizziamo metriche di intelligenza artificiale di punta e inoltre mostriamo i risultati di valutazione del modello umano basato su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione di prompt.</sample>
    <sample id="889">La prompting ha un grande impatto sulle prestazioni dei modelli di linguaggio per la traduzione. Come possiamo vedere in un semplice esperimento, dove si utilizza la prompting a una sola volta e si forniscono due differenti prompt per la stessa frase.</sample>
    <sample id="890">La maggioranza delle frasi, 516 su 1000, la differenza osservata è di più di 1 punto di errore.</sample>
    <sample id="891">E questo può arrivare fino a 40 punti di errore in casi estremi. Quindi è importante selezionare una buona strategia di prompt.</sample>
    <sample id="892">Nelle nostre esperimenti, ci siamo fermati a una strategia di prompt a cinque punti, dove segniamo le sue frasi che forniamo al sistema con la loro lingua originale.</sample>
    <sample id="893">Certo. Potresti fornire il contenuto inglese da tradurre?</sample>
    <sample id="894">Abbiamo visto che la forma effettiva del prompt non ha un grande influsso nel caso del prompt di serie.</sample>
    <sample id="895">È cruciale per il prompt a zero e uno shot. E quando passiamo, come nel nostro caso, al prompt a cinque shot, non c'è quasi nessuna differenza nella forma effettiva del prompt.</sample>
    <sample id="896">E sono gli esempi che portano la maggior parte del peso.</sample>
    <sample id="897">Il contenuto inglese è: "La sintesi dei risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza con la frase di riferimento."</sample>
    <sample id="898">Quindi è importante selezionare esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt dai dati di addestramento delle valutazioni WMT o dai dati di test.</sample>
    <sample id="899">I dati di sviluppo sono molto più curati e di qualità superiore rispetto ai dati di addestramento. Sono più ordinati e i risultati sono migliori quando si utilizzano i dati di sviluppo.</sample>
    <sample id="900">Tuttavia, i sistemi di traduzione specializzati hanno un vantaggio sostanziale rispetto ai traduttori umani. Tuttavia, Pan si avvicina abbastanza a un sistema commerciale. Nella nostra situazione, abbiamo scelto di operare con Google Translate.</sample>
    <sample id="901">Le conoscenze che abbiamo acquisito dall'analisi di email utilizzando il framework MKM sono che la fluidezza di Palm è paragonabile a quella dei sistemi all'avanguardia, ma la principale differenza deriva dalla precisione.</sample>
    <sample id="902">In particolare, gli errori più comuni sono gli errori di omissione.</sample>
    <sample id="903">Palm sceglie di produrre una traduzione migliore, a volte rimuovendo parti del contenuto inglese che non sono importanti nella traduzione.</sample>
    <sample id="904">Tuttavia, la categoria di stile di software per il sistema è inferiore a quella per i sistemi di punteggio, che è un altro segnale.</sample>
    <sample id="905">Quel modulo fornisce un output veramente fluido, ma ha ancora alcuni problemi di accuratezza.</sample>
    <sample id="906">Ecco tutto per questa breve panoramica. Per ulteriori dettagli, vi prego di venire alla presentazione completa del documento. Grazie mille.</sample>
    <sample id="907">Ciao， sono Dawei， un dottorando all'Università di Stoccarda in Germania. Nella video vorrei presentare il nostro lavoro recente "Wegereit denkt nicht"， un'analisi critica del "Weekly Supervisor".</sample>
    <sample id="908">Questa è una collaborazione con Xiaoyu Shen, Mario Smusba e gli artisti Stefan e Detis Claro.</sample>
    <sample id="909">Vorrei iniziare con una breve introduzione alla supervisione di settimana e alla supervisione settimanale.</sample>
    <sample id="910">Nella supervisione debole non etichettiamo manualmente i dati. Invece etichettiamo i dati utilizzando fonti di etichettatura debole, come semplici regole heuristiche, basi di conoscenza o sorgenti di crowdsourcing di bassa qualità, come illustrato nella figura a destra.</sample>
    <sample id="911">Quando confrontati con le annotazioni umane, le annotazioni di macchina sono molto più economiche, ma sono anche rumorose, il che significa che una certa quantità di annotazioni sono errate.</sample>
    <sample id="912">Se si addestra direttamente le reti neurali su dati etichettati a livello di esempio, le reti neurali tendono a memorizzare il rumore etichettato e non a generalizzare.</sample>
    <sample id="913">Nell'apprendimento supervisionato a bassa qualità, vengono proposti algoritmi di addestramento per addestrare robustamente reti neurali in presenza di rumore di etichetta, in modo che i modelli addestrati si generalizzino ancora bene.</sample>
    <sample id="914">In recent works in WSL, a common claim is that people say that they only train models on the weekly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="915">Tecnicamente questa affermazione non è sbagliata, ma c'è un'eccezione.</sample>
    <sample id="916">Quando si assume che sia disponibile un set di validazione aggiuntivo pulito per la selezione del modello.</sample>
    <sample id="917">Il problema di questa impostazione è che implica la necessità di annotazioni manuali aggiuntive in apprendimento supervisionato, ma come un elefante nella stanza, questa necessità viene spesso ignorata.</sample>
    <sample id="918">Il riferito dubbio è se i dati di validazione puliti sono necessari per WSL o se possiamo forse utilizzare un set di validazione rumoroso invece.</sample>
    <sample id="919">Secondo, se i dati puliti sono richiesti o se i dati puliti sono obbligatori per far funzionare WSL, allora quante campioni puliti abbiamo bisogno? Infine, dobbiamo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli?</sample>
    <sample id="920">Abbiamo affrontato queste domande di ricerca nel nostro lavoro e i nostri risultati sono i seguenti.</sample>
    <sample id="921">Prima di tutto, notiamo che, interessantemente, i metodi recenti di WSL richiedono effettivamente campioni di dati di base puliti per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci sarà una grande perdita di prestazioni， come mostrato in questa figura. Se non ci sono campioni di validazione puliti， allora i modelli addestrati non possono generalizzare oltre le etichette di test originali.</sample>
    <sample id="923">Significato profondo di "il training è inutile" è "il training non ha senso".</sample>
    <sample id="924">Questo indica che gli approcci WSL effettivamente richiedono dati ben etichettati per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">La nostra seconda scoperta è che aumentare il numero di campioni di validazione puliti aiuterà gli approcci WSL a ottenere una prestazione migliore, come mostrato nella figura a sinistra.</sample>
    <sample id="926">Generalmente, si hanno bisogno di solo 20 campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma non è la fine della storia, perché se decidiamo di accedere a campioni puliti, allora l'addestramento direttamente su di essi otterrà addirittura prestazioni migliori.</sample>
    <sample id="928">Il grafico a destra mostra la differenza di prestazione tra le approcci di finetuning che vengono applicati direttamente sui dati puliti e le approcci WSL che usano i dati puliti solo per la validazione.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo 10 campioni per classe, l'ottimizzazione diretta inizia a superare le approcci WSL.</sample>
    <sample id="930">Infine, l'incremento di prestazioni dichiarato in precedenti approcci WSL può essere facilmente ottenuto consentendo il continuo finetuning su campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dai grafici, il modello Valina, chiamato FTW, inizialmente sottoproduce rispetto a metodi WSL più complessi come Cosine.</sample>
    <sample id="932">Tuttavia, se decidiamo di continuare a finetunare sui campioni puliti, allora FTW si comporta altrettanto bene degli altri metodi.</sample>
    <sample id="933">Quindi, in pratica, non c'è motivo di scegliere metodi WSL più complessi che richiedono più tempo di calcolo e spazio disco.</sample>
    <sample id="934">Abbiamo dimostrato che gli approcci recenti di WSL richiedono campioni manualmente annotati puliti per funzionare correttamente. Le loro prestazioni e praticità sono stata pesantemente sovrastimate.</sample>
    <sample id="935">Le nostre raccomandazioni concrete per il lavoro futuro sono le seguenti.</sample>
    <sample id="936">Prima di tutto, riportare i criteri di selezione del modello, ad esempio, riportare se la selezione del modello è stata fatta con campioni di validazione puliti.</sample>
    <sample id="937">Secondo, gli approcci WSL dovrebbero essere confrontati con baselines di apprendimento a corto termine, poiché entrambi operano su esempi di testo. Terzo, l'ottimizzazione continua è una baselines semplice ma potente che dovrebbe essere considerata in future work in WSL.</sample>
    <sample id="938">Infine, abbiamo reso il nostro codice open source. Puoi trovarlo attraverso il codice QR presente in questa slide. Sentiti libero di controllarlo. Grazie e goditi la conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo sono l'uso di giudici umani per selezionare quale conversazione è migliore o per valutarle su una scala Likert.</sample>
    <sample id="940">Cinque.</sample>
    <sample id="941">Le conoscenze di base necessarie sono quelle sul fatto che i giudici decidono casi in tribunali.</sample>
    <sample id="942">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="943">Non c'è informazione sufficiente nel testo per rispondere a questa domanda. Potresti fornire ulteriori dettagli?</sample>
    <sample id="944">Non sono state perturbate in modo specifico nel dominio accettabile.</sample>
    <sample id="945">Evaluare dimensionale significa valutare diverse aspetti o dimensioni di qualcosa. Nell'ambito del dialogo, significa valutare diversi aspetti del dialogo come la coerenza, la naturalità, la pertinenza e così via.</sample>
    <sample id="946">I'm not sure what the article is about. Can you give me more details?</sample>
    <sample id="947">La forma del prompting si rivela importante in caso di zero-shot e uno-shot prompting.</sample>
    <sample id="978">Non so qual è il nome dei modelli di dialogo che hanno valutato gli autori. Potresti dare più dettagli o informazioni?</sample>
    <sample id="979">I'm not sure. The information about the number of authors in the article is not provided in the given content. You could try looking for more details in the article itself.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere scenari che siano ragionevoli e fedeli alle restrizioni.</sample>
    <sample id="981">I'm not sure. I don't have enough information to answer that. Can you give me more details?</sample>
    <sample id="982">Vasudha.</sample>
    <sample id="983">I'm sorry, but I don't have the information about the affiliations of the authors of the article you mentioned. Can you provide more details or context?</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono gli omission errors. Se hai altre domande su PaLM o su altri argomenti, sentiti libero di chiedere.</sample>
    <sample id="1022">Ciao， sono James Finch e sono Sarah Finch. Oggi vi parleremo di ABC eval， una nuova approccio dimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="1023">Questo lavoro è stato fatto dal laboratorio di NLP di Emory, diretto dal professor Gino Choi all'Università di Emory e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Certo. Ma prima di tradurre, potresti fornire il contenuto inglese da tradurre?</sample>
    <sample id="1025">La pratica comune è di utilizzare l'valutazione umana, come chiedere a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni globali della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare diverse dimensioni della qualità del chat per comprendere le forze e le debolezze del modello a un livello più dettagliato.</sample>
    <sample id="1027">Un approccio consiste nel chiedere giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi di confronto o scala Likert esistenti.</sample>
    <sample id="1028">Tuttavia, crediamo che esista una strategia più precisa e affidabile per l'valutazione del dialogo dimensionale.</sample>
    <sample id="1029">La nostra approccio cerca di ridurre la soggettività dell'valutazione umana annotando esplicitamente se ognuna delle risposte del modello esprime determinati comportamenti, come rispondere con informazioni irrellevanti o contraddizione.</sample>
    <sample id="1030">Chiamiamo questo approccio "annotazione dei comportamenti in chat" o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo comprensivo i comportamenti dei modelli di chat che sono stati suggeriti di influenzare la qualità dei chat in recenti studi.</sample>
    <sample id="1031">Il contenuto inglese dice: "La valutazione di ABC è in grado di misurare le tassi con cui i modelli di chat commetteranno vari errori tematici."</sample>
    <sample id="1032">Ad esempio, l'indice ABC eval misura il numero di turni in cui un modello di chat ignora il suo partner o dice qualcosa di irrilevante.</sample>
    <sample id="1033">Contraddice se stesso o il suo partner, allucina fatti errati o viola il buon senso, e quando il modello riesce o fallisce a mostrare empatia.</sample>
    <sample id="1034">Per determinare qual è l'evaluation più efficace, abbiamo selezionato quattro modelli di chat di punta e li abbiamo valutati su 100 conversazioni umano-robot per modello utilizzando ABC eval.</sample>
    <sample id="1035">Per la comparazione, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: le valutazioni LICKER a livello di turno, le valutazioni LICKER a livello di dialogo e le comparazioni a livello di dialogo a coppie.</sample>
    <sample id="1036">Per ogni dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti di dialogo più comunemente misurati, poiché questo è la pratica standard per valutare i modelli di chat su diverse dimensioni.</sample>
    <sample id="1037">Dalle nostre analisi di questi risultati di valutazione, abbiamo trovato che le etichette di comportamento ABCBEval sono in generale più affidabili delle etichette raccolte dai metodi esistenti, come misurato dall'accordo inter-annotatore su 100 conversazioni doppialmente etichettate.</sample>
    <sample id="1038">Inoltre, le etichette ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto ai metrici prodotti dai metodi esistenti, come dimostrato dall'analisi di regressione lineare semplice.</sample>
    <sample id="1039">Ad esempio, si può vedere come misurare la proporzione di turni con contraddizioni di sé e partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre le punteggia di consistenza di Likert medie spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare passo dopo passo.</sample>
    <sample id="1041">Puoi vedere come la combinazione di tutte le metriche di valutazione ABC spiega oltre il 25% della qualità della conversazione e, quando le rimuovi una alla volta, la maggior parte di esse porta a perdere una buona quantità di informazioni sulla qualità.</sample>
    <sample id="1042">D'altra parte, la combinazione di tutte le metriche Likert a livello di volta spiega ben poco della qualità e meno di queste metriche trasportano informazioni uniche.</sample>
    <sample id="1043">Questi metriche di valutazione ABCDEF sono affidabili, informative e distintive e permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione maggiore rispetto a metodi precedenti.</sample>
    <sample id="1044">Puoi vedere che nei risultati dell'esperimento ci sono ancora diverse sfide che sono state quantificate con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni del buon senso in circa il 20% delle loro risposte.</sample>
    <sample id="1045">Producono informazioni irrilevanti in circa il 15% delle risposte e si contraddicono o si contraddicono con il partner circa il 10% delle volte.</sample>
    <sample id="1046">Con il rapido ritmo di miglioramento nel campo, molte di queste tassi di errore potrebbero registrare una diminuzione nei nuovi modelli rilasciati dal momento in cui è stata condotta la nostra valutazione. Tuttavia, questo è ancora più motivo per perseguire metriche di valutazione affidabili e precise per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E aspettiamo di vedere come l'intelligenza artificiale conversazionale progredirà nei prossimi mesi e anni. Grazie per averci seguito.</sample>
    <sample id="1048">L'articolo è stato fatto dal Emory NLP Lab, diretto da professor Gino Choi all'Emory University, e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1049">CFT significa Continual Fine-tuning.</sample>
    <sample id="1050">Cinque.</sample>
    <sample id="1051">Ciao! Mi chiamo Coyote e presenterò il nostro lavoro intitolato "Quando la traduzione richiede un contesto: un'indagine multilingue basata su dati". Questo lavoro è stato realizzato in collaborazione con Patrick Franses, Emile Liu, Andrew F. T. Martins e Graham Newbigging.</sample>
    <sample id="1052">In this case, "mole" could be translated as "moglie" if it refers to a wife. But if it's used in a different context, like in a scientific sense, it might be translated as "mole" in English. So, it really depends on the context. What context are you thinking about?</sample>
    <sample id="1053">Se la frase precedente era "Cose potrebbero iniziare a diventare pericolose se i ministri scoprono", allora "Moll" si riferisce a un agente segreto. Ma se la frase precedente era "Potrebbe essere qualcosa di serio, dottore?", allora "Moll" si riferisce a una macchia di nascita.</sample>
    <sample id="1054">Sì, a seconda del contesto il significato della parola cambia e quindi la sua traduzione cambia anche.</sample>
    <sample id="1055">Tuttavia, valutare quanto bene i modelli gestiscono casi come questo è abbastanza difficile. In primo luogo, perché solo una piccola parte delle traduzioni dipende dal contesto, il che rende impossibile per i metriche a livello di corpus come BLEU catturare queste traduzioni.</sample>
    <sample id="1056">E alcuni hanno suggerito l'valutazione mirata delle traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto solitamente si affidano a conoscenze di dominio e cura umana.</sample>
    <sample id="1057">In questo lavoro cerchiamo di rispondere a queste due domande. Prima, quando la traduzione richiede contesto? E poi, quanto bene gestiscono questi casi i modelli?</sample>
    <sample id="1058">Per rispondere alla prima domanda, abbiamo iniziato misurando quanto una parola dipende dal contesto durante la traduzione.</sample>
    <sample id="1059">Nel lavoro precedente, abbiamo introdotto CXMI come misura per i contesti utilizzati dai modelli di traduzione automatica. Questo è stato fatto misurando quanta informazione il contesto C fornisce sulla target Y, data la source X.</sample>
    <sample id="1060">Puoi pensare al CMI come all'informazione guadagnata dal dare contesto al modello.</sample>
    <sample id="1061">In questo lavoro estendiamo il CXMI a Pointwise CXMI, che può misurare l'uso del contesto a livello di frase o a livello di parola. Possiamo considerare le parole che hanno un alto p-CXMI come quelle che richiedono contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo le parole con un alto PSMI per cercare di trovare dei modelli tra queste parole.</sample>
    <sample id="1063">Eseguiamo l'analisi sui trascrizioni delle TED Talks tradotte in 14 diverse lingue.</sample>
    <sample id="1064">Prima analizziamo i tag di grammatica che hanno un alto PSMI.</sample>
    <sample id="1065">E questo ci permette di trovare, ad esempio, i pronomi dual in arabo che hanno relativamente alta P63. E questo può essere spiegato perché l'inglese non ha pronomi dual, quindi si ha bisogno del contesto per determinare se un pronome è dual quando si traduce in arabo.</sample>
    <sample id="1066">E similmente, scopriamo che certe lingue richiedono contesto quando vogliamo scegliere la forma verbale appropriata. Poi guardiamo i vocaboli che hanno un alto PCE, PCE medio, calcolato su tutte le sue diverse occorrenze.</sample>
    <sample id="1067">E questo ci aiuta a identificare casi come questo, dove in cinese si ha bisogno di contesto per tradurre correttamente i nomi propri, per assicurarsi che si stia usando la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">E in modo simile, troviamo che il contesto è supportato per la traduzione nella giusta formalità.</sample>
    <sample id="1069">E infine esaminiamo diversi token che hanno un alto P XMI. Questo ci permette di identificare fenomeni che non possono essere veramente catturati dalla parola in sé, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione di ellissi.</sample>
    <sample id="1070">Quindi ora utilizziamo i nostri risultati dall'analisi per progettare un benchmark per la traduzione a livello di documento.</sample>
    <sample id="1071">Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per identificare automaticamente le parole che si riferiscono al fenomeno e chiamiamo il nostro tagger il tagger multilingue consapevole del discorso, MUDAT.</sample>
    <sample id="1072">Possiamo notare anche che diverse lingue hanno proporzioni diverse di questi fenomeni discorsivi.</sample>
    <sample id="1073">Poi utilizziamo MUDATagger applicando il tagger al corpus parallelo che vogliamo utilizzare per l'valutazione e applichiamo i nostri metrici di traduzione di scelta sugli esempi dipendenti dal contesto che MUDATagger ha identificato.</sample>
    <sample id="1074">E infine, utilizziamo il nostro benchmark, nonché altri metriche, per valutare diversi modelli di traduzione automatica a livello di documento.</sample>
    <sample id="1075">Prima di tutto, quando utilizziamo metriche di livello di corpus, per il BLEU troviamo che i modelli agnostici del contesto hanno il miglior rendimento.</sample>
    <sample id="1076">Ma se usiamo Comet, i modelli con contesto performano meglio. E se usiamo la misura F word, i modelli con e senza contesto hanno prestazioni comparabili.</sample>
    <sample id="1077">Questo dimostra nuovamente che è difficile determinare il miglior sistema di traduzione di documento se si utilizzano metriche a livello di corpus da sole.</sample>
    <sample id="1078">Ora utilizziamo il benchmark MML per valutare i modelli e troviamo che i modelli che usano il contesto sono significativamente più accurati dei modelli che non usano il contesto per certi fenomeni discorsivi, come la formalità e la coesione lessicale.</sample>
    <sample id="1079">Ma questi modelli non sono molto migliori di quelli che non utilizzano il contesto su altri fenomeni come ellissi, pronomi e forme del verbo. Quindi, questo suggerisce dove dovremmo vedere un maggiore progresso per la traduzione a livello di documento.</sample>
    <sample id="1080">Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è solitamente più accurato di Google Translate per la traduzione a livello di documento.</sample>
    <sample id="1081">Eseguiamo un'analisi dati guidata su 14 coppie di lingue per identificare quando le traduzioni richiedono un contesto.</sample>
    <sample id="1082">E poi utilizziamo i nostri risultati per costruire un benchmark per la traduzione automatica a livello di documento che ci aiuterà a identificare quali fenomeni discorsivi i modelli possono gestire bene o meno e quali sistemi di traduzione sono buoni per la traduzione a livello di documento.</sample>
    <sample id="1083">Grazie mille per l'attenzione. Ci vediamo a Toronto.</sample>
    <sample id="1084">Justin John.</sample>
    <sample id="1121">Il nuovo metodo è privo di nome.</sample>
    <sample id="1122">Il metodo delle "parole contrassegnate" è una tecnica per identificare le parole che distinguono gruppi contrassegnati da quelli non contrassegnati.</sample>
    <sample id="1123">The authors are affiliated with the University of Washington.</sample>
    <sample id="1124">La prima struttura di dipendenza simmetrica menzionata è il "triple approach".</sample>
    <sample id="1125">James Finch e Sarah Finch.</sample>
    <sample id="1126">Quattro.</sample>
    <sample id="1127">I dati di minimal pair paradigm possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="1161">Non so quali siano le abbreviazioni dei cinque metodi per la prima domanda di ricerca. Potresti fornire più dettagli o informazioni?</sample>
    <sample id="1162">Il modello viene valutato su 11 attività biomediche e cliniche non strumentali.</sample>
    <sample id="1226">CamemBERT viene addestrato sul subset da 4GB di NACOS.</sample>
    <sample id="1227">Adam Skowroński.</sample>
    <sample id="1228">L'esperimento dimostrò che la performance decresceva con un gap temporale maggiore durante il rieducazione o la continuazione della pre-educazione dei modelli con dati più recenti. Questo confermò l'ipotesi che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">Permette di ottenere la sequenza di output corretta.</sample>
    <sample id="1270">Perché non sanno se i positivi stereotipi sono causati da un'overly excessive value alignment o altri metodi anti-stereotipanti che producono questi dannosi modelli.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono le frasi grammaticalmente errate. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="1272">Non sono state menzionate le metriche di valutazione utilizzate. Potresti cercare di trovare ulteriori informazioni nel testo originale?</sample>
    <sample id="1273">L'accordo tra annotatori è stato misurato dall'inter-annotator agreement.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Non ho informazioni sulle affiliazioni degli autori dell'articolo. Potresti fornire ulteriori dettagli o contattare l'autore per ottenere queste informazioni?</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto si concentra sulla generazione di dati di istruzioni per modelli multimodal. Mentre gli altri parametri di riferimento si concentrano spesso su compiti linguistici solo, MultiInstruct cerca di migliorare la generalizzazione a compiti multimodal inesistenti. Inoltre, ha un'ampia disponibilità di dati di istruzioni per modelli linguistici solo, ma manca di un gran numero di dati di istruzioni per modelli multimodal pubblici.</sample>
    <sample id="1277">Due.</sample>
    <sample id="1278">Non c'è una definizione di coordinazione binaria nella tua domanda. Potresti fornire più dettagli o chiarire il contesto?</sample>
    <sample id="1279">I'm not sure. I don't have the specific information about how long the prompts were used in this study. You could try looking for more details in the study's report or methodology section.</sample>
    <sample id="1280">I risultati suggeriscono che i modelli più piccoli possono superare i modelli più grandi quando sono adeguatamente addestrati su dataset appropriati.</sample>
    <sample id="1281">Ciao， sono Janice Lavaque e presenterò i nostri lavori su Dr.Bert， un modello di pre - allena robusto in francese per l'analisi biomedica e clinica.</sample>
    <sample id="1282">Nella presentazione iniziamo parlando del modellaggio linguistico nel campo della sanità. Poi presenteremo le principali contribuzioni del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese, chiamato Dr. Bert, che è basato su Roberta e addestrato su NACHOS, un dataset di dati clinici catturati dalla rete.</sample>
    <sample id="1284">Abbiamo anche introdotto una comparazione del modello con diversi set di pre-allenamento e fonti di dati. Poi abbiamo presentato i nostri risultati su 11 compiti di apprendimento automatico biomedico e clinico in francese.</sample>
    <sample id="1285">E infine, concludiamo sull'esperimento e ti forniamo ulteriori dettagli su come accedere ai modelli.</sample>
    <sample id="1286">Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e offre un notevole miglioramento di prestazioni rispetto a metodi statici e contestualizzati storici come Word2Vec, FastText o ANMO.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molte lingue straniere, come in francese con Camembert, in ambito biomedicale con PubMedBERT e Biobert, e in ambito clinico con ClinicalBERT, ma principalmente in inglese.</sample>
    <sample id="1288">I modelli specializzati per altre lingue sono scarsi e spesso basati su un preallineamento continuo a causa della mancanza di dati di dominio.</sample>
    <sample id="1289">Tuttavia, il modello di Open Source di French non era disponibile per Windows fino a quando?</sample>
    <sample id="1290">Quindi ci chiediamo qual è il più adatto per una varietà di utilizzo e i dati strutturati sono buoni sostituti dei dati clinici.</sample>
    <sample id="1291">Per rispondere a questa domanda, abbiamo comparato il modello di BERT con il nostro modello di sub-BERT, che è basato su dati anonimizzati ottenuti da un ospedale non universitario a Detroit.</sample>
    <sample id="1292">In fondo ci chiediamo quanta data dobbiamo avere per addestrare un modello specializzato su dati francesi. È di 4 GB, 8 GB o più?</sample>
    <sample id="1293">Per rispondere a questa domanda, prima dobbiamo addestrare e confrontare quattro modelli da zero. La prima versione di Dr. Bert ha bisogno di 7 GB di dati di addestramento. La seconda versione ha bisogno di 4 GB di dati di addestramento.</sample>
    <sample id="1294">Prima versione di Schubert, un modello clinico con 4 GB di istruzioni prese da ClinConsort. Versione finale di Schubert con un mix di 4 GB di set di Natus e 4 GB di ClinConsort.</sample>
    <sample id="1295">In aggiunta a questa comparazione, introduciamo tre modelli addestrati con preaddestramento controllato per analizzare l'impatto delle strategie di preaddestramento.</sample>
    <sample id="1296">Un basato sul peso del camembert e addestrato su un subset da 4 GB di nachos. L'altro basato anch'esso sul camembert ma addestrato questa volta su un subset da 4 GB di caramelle.</sample>
    <sample id="1297">E infine, basandosi sul modello di BERT in lingua inglese, possiamo costruire e addestrare un modello su un subset di 4 GB di SNACCHOS. In totale, abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare tutti e sette i modelli, abbiamo raccolto molteplici compiti pubblici e privati, come riconoscimento di oggetti in immagini, classificazione, rilevamento di oggetti in foto, e risposta a domande.</sample>
    <sample id="1299">Questo modello è stato comparato a sei modelli di design, che sono Camembert Oscar 128 GB, Camembert Oscar 4 GB, Camembert C-Sinet 4 GB, Permabert, Mayobert e Clinicalbert.</sample>
    <sample id="1300">La versione di Highlight che esegue meglio la task con dati dello stesso tipo è quella che è stata addestrata su.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da… possiamo osservare che i dati da diverse fonti sembrano essere più versatile. Abbiamo anche osservato che utilizzare più dati si traduce in una migliore prestazione.</sample>
    <sample id="1302">In generale, l'innovazione da zero sembra ottenere prestazioni superiori in molte delle tare.</sample>
    <sample id="1303">Tuttavia, il nostro esperimento di ottimizzazione continua utilizzando il pesaggio e il tokenizzatore di Pegasus, addestrato su un subset da 4 GB di NACOS, ha mostrato risultati paragonabili a quelli ottenuti con DrBERT da zero su 4 GB.</sample>
    <sample id="1304">Non è il caso per il modello basato su Camembert weights and tokenizer, che soffrono di problemi di stabilità.</sample>
    <sample id="1305">Infine, la nostra proposta di sistema offre una migliore prestazione su nove dei dodici compiti e supera globalmente il risultato del modello generico qui di seguito.</sample>
    <sample id="1306">Abbiamo anche osservato che i dati specializzati sono migliori, più dati specializzati sono meglio, ma non scalano bene.</sample>
    <sample id="1307">I modelli pre-allenati ottenuti da Natus Vincere sono disponibili gratuitamente e l'interfaccia utente e tutti i script di allena sono sul nostro repository GitHub.</sample>
    <sample id="1308">Quindi ti ringrazio per questa presentazione e ci guardiamo avanti per scambiare informazioni nella sessione posteriore a Toronto.</sample>
    <sample id="1309">Nel lavoro vengono esaminate le strategie di apprendimento di pre-apprendimento.</sample>
    <sample id="1310">Non so. Non hai fornito abbastanza informazioni per rispondere a questa domanda. Potresti dare più dettagli?</sample>
    <sample id="1311">I don't know. The passage doesn't mention how the quality of simplification was evaluated. You could try looking for more details in the paper.</sample>
    <sample id="1312">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="1313">Ciao, il mio nome è Matthias Lindemann e oggi darò un'introduzione breve al nostro articolo su generalizzazione compositiva senza alberi utilizzando la taggatura multiset e le permutazioni latenti.</sample>
    <sample id="1314">Questo è un lavoro congiunto con i miei tutori, Aleksandr Kola e Ivan Tito.</sample>
    <sample id="1315">La generalizzazione compositiva può essere compresa come la capacità di un apprenditore di gestire una ricorsione più profonda e composti di frasi mai visti prima che sono stati visti separatamente durante l'addestramento.</sample>
    <sample id="1316">Nel contesto della semantica di parsing, il test per la generalizzazione compositiva potrebbe sembrare così: "Come al solito, abbiamo un set di enunciati di addestramento. In questo caso, "la ragazza ha dormito" e "Mary sapeva che la ragazza ha dormito".</sample>
    <sample id="1317">Questi enunciati sono associati a forme logiche che rappresentano aspetti fondamentali del loro significato.</sample>
    <sample id="1318">In contrapposizione all'evaluazione standard di machine learning, il set di test non proviene dalla stessa distribuzione ma contiene forme logiche strutturalmente non viste.</sample>
    <sample id="1319">Nell'esempio, il modello ha visto ricorsione superficiale durante l'addestramento e viene testato su esempi con ricorsione più profonda.</sample>
    <sample id="1320">I modelli di sequenza a sequenza semplici hanno difficoltà con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono disgiunti dall'input.</sample>
    <sample id="1321">In particolare, spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono colorate nel esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo è l'integrazione degli alberi nei modelli.</sample>
    <sample id="1323">I alberi sono intesi a catturare il processo compositivo che si lega alle affermazioni con le forme logiche.</sample>
    <sample id="1324">Questo funziona bene, ma gli alberi di solito non vengono dati e devono essere ottenuti in qualche modo.</sample>
    <sample id="1325">Questo può essere complicato e talvolta un processo computazionalmente costoso. Generalmente questo implica un pre-elaborazione formale considerevole delle forme logiche, ad esempio per gestire i simboli variabili.</sample>
    <sample id="1326">Ottenerli alberi potrebbe anche coinvolgere procedure di induzione di grammatica specializzate.</sample>
    <sample id="1327">Nel nostro articolo non utilizziamo alberi e introduciamo un nuovo modello di sequenza a sequenza che modella direttamente le corrispondenze tra i frammenti dell'input e i frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta dimostriamo una generalizzazione forte alla ricorsione più profonda senza dipendere da alberi.</sample>
    <sample id="1329">Il nostro approccio predice l'output dall'input in due passaggi.</sample>
    <sample id="1330">Prima di tutto, etichettiamo ogni token di input con un multiset non ordinato di token che appaiono nell'output.</sample>
    <sample id="1331">Dopo il primo passaggio, abbiamo tutti i token giusti, ma non sono ordinati.</sample>
    <sample id="1332">Perché, nel secondo passaggio, usiamo un altro modello per predire una permutazione per metterli nell'ordine giusto.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere una permutazione che non impone alcun vincolo rigido sulle possibili permutazioni. Questo rende la nostra approccio abbastanza flessibile ed espressivo.</sample>
    <sample id="1334">Per concetto, il nostro modello di permutazione funziona in questo modo approssimativamente.</sample>
    <sample id="1335">Andiamo da sinistra a destra sulle uscite e determiniamo quale token multiset mettere in ogni posizione. Per la prima posizione di uscita, semplicemente selezioniamo uno, come evidenziato in rosso.</sample>
    <sample id="1336">Poi saltiamo al prossimo token multiset per determinare il secondo token nell'output.</sample>
    <sample id="1337">Determiniamo il terzo token nella output in una maniera simile saltando a un altro token multiset. Continuiamo questo processo.</sample>
    <sample id="1338">Fino a quando ogni token della prima fase è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per dare un'anteprima dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli di alberi su il benchmark Cog. Il nostro modello supera gli altri di gran lunga in generazione per ricorsioni più profonde.</sample>
    <sample id="1340">Alcune altre tipologie di generalizzazione strutturale rimangono molto difficili.</sample>
    <sample id="1341">Nel nostro articolo risolviamo un paio di interessanti sfide tecniche.</sample>
    <sample id="1342">Prima di tutto, l'allineamento tra input e output non è dato dai dati di addestramento. Come conseguenza, per un token dato, non sappiamo da quale multi-settore è venuto, che rappresenta un'ottativa per l'addestramento.</sample>
    <sample id="1343">Inoltre, a volte ci sono diverse permutazioni che sono consistenti con i dati, ma la corretta da un punto di vista linguistico è latente. Abbiamo affrontato questo problema introducendo l'individuazione dell'alignamento come parte della fase di addestramento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma presenta il problema che trovare la permutazione con la punteggiatura più alta è NP-hard. Questo perché è legato al problema del viaggiatore di commercio.</sample>
    <sample id="1345">Approssimiamo questo con una rilassazione continua amichevole per GPU che ci permette anche di propagare indietro attraverso la soluzione e imparare le permutazioni più plausibili dal punto di vista linguistico.</sample>
    <sample id="1346">Se vuoi sapere di più sulle nostre esperimenti e su come abbiamo affrontato queste sfide, ti prego di leggere il nostro articolo o di venire al nostro poster.</sample>
    <sample id="1347">La dissonanza cognitiva è quando due credenze o azioni sono inconsistenti.</sample>
    <sample id="1348">Il modello linguistico più liberale è GPT-4.</sample>
    <sample id="1349">Sì.</sample>
    <sample id="1350">Sarah Pape.</sample>
    <sample id="1351">I dati nel parametro di riferimento MuDa sono stati tratti da transcrits di TED talks tradotti in 14 diverse lingue.</sample>
    <sample id="1385">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è un tipo di trasferimento di apprendimento che si basa su un linguaggio intermedio, chiamato interlingua, per trasferire conoscenze da un linguaggio a un altro. In questo caso, si sta parlando di trasferimento zero-shot e few-shot, dove si addestra su un linguaggio di origine e si trasferisce a un altro. Se hai altre domande su questo argomento, sentiti libero di chiedere.</sample>
    <sample id="1387">I'm not sure about the affiliations of the authors. You could try looking for more information in the article itself or on the authors' websites. If you find out, come back and let me know!</sample>
    <sample id="1388">Gli autori fanno ricorso alla misura di latenza standard, ovvero la semplice media di latenza, e alla misura di latenza computazionale consapevole, che tiene conto del tempo computazionale del modello per predire l'output.</sample>
    <sample id="1389">Ciao a tutti, sono Maksym, e oggi il mio collega Martin e io stiamo presentando il nostro lavoro "The Kid Must Have: Evaluating Knowledge Integration from Multiple Sources". Questo lavoro è una collaborazione tra l'Università di McGill, MILA e Microsoft Research.</sample>
    <sample id="1390">I modelli di intelligenza artificiale naturale traggono conoscenza da una varietà di fonti, come quelle contenute nei loro parametri, solitamente acquisite tramite pre-allenamento, e quelle date in input durante l'inferezia.</sample>
    <sample id="1391">Recenti lavori su compiti come la risposta alle domande mostrano che i modelli possono utilizzare il loro conoscenza preaddestrata per risolvere il compito.</sample>
    <sample id="1392">Ma l'intersezione della comprensione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche in tempo di inferenza.</sample>
    <sample id="1393">Ad esempio, nella frase "John saw the newly elected president on TV".</sample>
    <sample id="1394">I parametri di pre-allenamento possono contenere informazioni su cosa i precedenti hanno fatto e cosa il TV è, ma non possono conoscere in modo affidabile chi sia l'entità specifica John in questo istante o chi sia il nuovo presidente, perché il presidente potrebbe essere cambiato dal momento della pre-allenamento.</sample>
    <sample id="1395">Quindi, i modelli di successo per le compiti di NLU intensivi di conoscenza richiedono la capacità di integrare e utilizzare sia il know-how di pre-allenamento che il know-how di inferenza.</sample>
    <sample id="1396">Nel lavoro, proponiamo un insieme di test diagnostici per l'integrazione del conoscenza.</sample>
    <sample id="1397">Introduciamo una compito di risoluzione di riferimento coevo progettato per verificare la capacità di utilizzare il know-how disponibile in diverse fonti. Valutiamo il dataset con partecipanti umani di studi e modelli di risoluzione di riferimento stabiliti.</sample>
    <sample id="1398">Sergio è un giudice. Kia è un pasticcere. Sergio e Kia si sono incontrati in un parco. Dopo una lunga giornata di lavoro, decidendo casi in un lungo tribunale, lui era felice di rilassarsi.</sample>
    <sample id="1399">Non capisco il contesto. Puoi fornire più dettagli?</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione. Primo, conoscenza specifica dell'entità, come "il giudice è un giudice", e secondo, conoscenza di fondo, come "i giudici decidono casi in tribunali".</sample>
    <sample id="1401">In generale, le conoscenze di fondo vengono imparate durante il pre-allenamento dei grandi modelli di linguaggio, mentre le conoscenze specifiche di entità sono solitamente osservate durante l'infrazione.</sample>
    <sample id="1402">Variamo l'accessibilità di queste due informazioni in modo che possano essere trovate in una singola fonte o in molte fonti.</sample>
    <sample id="1403">Abbiamo definito tre impostazioni di KidMOS. La prima è la configurazione di tipo "topic", dove il background è pretrainato, ovvero si assume che il background sia disponibile durante il pretrain.</sample>
    <sample id="1404">Secondo, c'è la configurazione di conoscenza di fondo. La conoscenza di fondo è disponibile sia durante il pre-allenamento che durante l'infrazione. Infine, la configurazione di conoscenza di fondo durante l'infrazione. Entrambi i tipi di conoscenza sono disponibili solo durante l'infrazione.</sample>
    <sample id="1405">Questo ultimo set di dati è particolarmente interessante, poiché simula il caso in cui il rumore di fondo è necessario per risolvere la compito, non fa parte dei dati di pre-allenamento dei modelli. Ad esempio, perché nuove occupazioni sono state sviluppate dal momento dell'allenamento.</sample>
    <sample id="1406">Ecco un esempio di come controllare l'accessibilità di fattori di risorsa.</sample>
    <sample id="1407">Nel contesto di un pre-allenamento di BERT, si assume che il know-how del background, ovvero che i politici cercano seggi elettorali nel governo, sia contenuto nei parametri pre-allenati. Per comprendere il contesto, forniamo il know-how specifico, ovvero che Chichester è un politico.</sample>
    <sample id="1408">Nel contesto di Backgroud Bull Setting, forniamo non solo informazioni specifiche, ma anche conoscenze di fondo sui partiti nel contesto interno.</sample>
    <sample id="1409">Nel contesto di Freeon, fornire l'occupazione "militare" invece di "politico" perché "militare" è meno probabile di essere contenuto nel pre-allenamento.</sample>
    <sample id="1410">Abbiamo valutato il dataset sia con partecipanti umani che con modelli di soluzione di riferimento stabiliti. In questa figura mostriamo i risultati dei modelli che hanno ottenuto il miglior rendimento nella variante più difficile del contesto di pre-allenamento di Backprop.</sample>
    <sample id="1411">Se non si ha un addestramento specifico su Kidnus, entrambi i modelli non si prestano bene. Quando addestrati su Kidnus, tuttavia, entrambi C2F e BERT per Kidnus si prestano significativamente meglio di qualsiasi altra scelta casuale.</sample>
    <sample id="1412">Il contenuto inglese tradotto in italiano è: "Quando addestrato su set di dati di risoluzione di domande generali, il modello impara a sfruttare i segnali superficiali, che non sono utili quando si testa su domande di kit, poiché tali segnali sono stati rimossi."</sample>
    <sample id="1413">Esempi aggiuntivi di conoscenza fittizia hanno indicato che anche i modelli che performano al meglio non riescono a integrare la conoscenza del contesto in modo affidabile solo durante l'infrazione.</sample>
    <sample id="1414">Molti modelli di riferimento evoluzione sembrano incapaci di ragionare su conoscenze da diverse fonti senza addestramento specifico per la compito. Tuttavia, con l'addestramento specifico per la compito, alcuni modelli riescono a integrare le conoscenze da molte fonti.</sample>
    <sample id="1415">Comunque, anche i modelli con le prestazioni migliori sembrano avere difficoltà a integrare in modo affidabile il conoscenza retrospettiva presentata solo durante l'infrazione. Se sei interessato a ulteriori dettagli, consulta il nostro articolo e controlla il dataset e il codice su GitHub. Grazie per l'ascolto.</sample>
    <sample id="1416">I svantaggi sono che le alberi non vengono spesso fornite, possono essere ottenute in modo complicato e computazionalmente costoso, richiedono formalismi specifici di pre-elaborazione delle forme logiche e possono coinvolgere procedure di induzione di grammatica specializzate. Se hai altre domande, sentiti libero di chiedere.</sample>
    <sample id="1417">I'm sorry, but the information about the affiliations of the authors of the article "Do Cornell 2003 Named Entity Taggers Still Work Well in 2023?" is not provided in the given content. You might need to look for more details in the full text of the article or in the references section.</sample>
    <sample id="1418">Ciao， sono Myra e oggi parlerò del nostro articolo "Personaggi marcati: utilizzando promemoria di linguaggio naturale per misurare gli stereotipi nei modelli di linguaggio". Questo lavoro è stato fatto in collaborazione con S. Sander, Mush e Dan Jurafsky.</sample>
    <sample id="1419">Negli ultimi anni, molti hanno documentato la prevalenza di pregiudizi sociali e stereotipi nei modelli di linguaggio a grandi scale, o LLM.</sample>
    <sample id="1420">Tuttavia, queste misure presentano varie limitazioni. Generalmente si basano su set di dati costruiti a mano, che sono molto tempo消费的 da curare.</sample>
    <sample id="1421">E inoltre solitamente misurano solo stereotipi molto specifici, il che significa che non generalizzano bene a altre demografie o contesti, o semplicemente catturano associazioni molto generali e ampie, come associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte del lavoro in questo campo non tiene conto dell'intersezione, che è la nozione che identità sociali multifaccettate possono comporre pregiudizi e essere luoghi unici di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci affidiamo alla proprietà che questi nuovi LLMs addestrati alle istruzioni sono molto bravi a rispondere alle istruzioni e alle promemoria.</sample>
    <sample id="1424">Quindi possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginario, utilizzando un prompt come "Immagina che tu sia una donna asiatica, descriviti".</sample>
    <sample id="1425">E possiamo immediatamente vedere che questo è molto generalizzabile a qualsiasi demografia, perché possiamo specificare qualsiasi marcatori di identità che vogliamo in questa prompt.</sample>
    <sample id="1426">Mi dispiace, ma non ho capito cosa specificamente tu vuoi tradurre. Potresti fornire il testo in inglese che vuoi tradurre in italiano?</sample>
    <sample id="1427">Subito notiamo che, mentre le uscite non sono negativi o tossici nel senso tradizionale di queste parole.</sample>
    <sample id="1428">C'è qualche interessante pattern.</sample>
    <sample id="1429">L'immagine della donna asiatica è descritta come inoffensiva. La donna mediorientale è riferita con parole come "esotica" e "come riferendosi a una regione ipnotizzante".</sample>
    <sample id="1430">E entrambe le personaggi di donne di colore fanno riferimento all'antepassato, mentre il personaggio del uomo bianco non ne fa assolutamente nulla.</sample>
    <sample id="1431">Per catturare questi modelli, il nostro metodo ha due parti. La prima parte è la generazione di questi personaggi.</sample>
    <sample id="1432">Le nostre promemoria per generare questi personaggi sono state ispirate da uno studio in cui hanno dato queste promemoria a soggetti umani, scoprendo che anche dando loro a soggetti umani erano in grado di rilevare stereotipi razziali.</sample>
    <sample id="1433">E questo consente una comparazione diretta tra i personaggi generati da noi e le risposte scritte da umani.</sample>
    <sample id="1434">La seconda parte è "parole marcate", che è un metodo per identificare le parole che distinguono i gruppi marcati da quelli non marciti, che approfondirò a breve.</sample>
    <sample id="1435">Il vantaggio di questo è che otteniamo veri e propri stereotipi e modelli senza dover ricorrere a nessun lessico specifico.</sample>
    <sample id="1436">Il metodo del segno linguistico si basa sul concetto sociolinguistico di "marcato", che afferma che esiste un valore di default non marcato e che qualsiasi gruppo che si differenzia da tale default è linguisticamente marcato.</sample>
    <sample id="1437">Per esempio, la parola "warrior" è solitamente associata a uomini. Quindi, quando le persone descrivono un guerriero che è una donna, solitamente specificano "woman warrior" e marciano il termine con "woman".</sample>
    <sample id="1438">E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcati, mentre i gruppi marginalizzati sono solitamente marcati.</sample>
    <sample id="1439">Quindi, nel nostro metodo, iniziamo designando quali sono i gruppi non marcato e marcato.</sample>
    <sample id="1440">E poi confrontiamo i personaggi utilizzando il metodo delle parole da combattere, che è fondamentalmente utilizzare le ratio di logaritmo pesato per distinguere le parole più importanti per ciascun gruppo marcato.</sample>
    <sample id="1441">Per esempio, per le personae di donne nere, faremmo "parole di battaglia" e compariamo le log-odds ratio sia contro le personae bianche che contro le personae maschili, perché queste sono le due gruppi corrispondenti non marcate.</sample>
    <sample id="1442">Ora per alcuni risultati. Allora, innanzitutto utilizziamo un lessico di stereotipi e scopriamo che i personaggi generati contengono molto più stereotipi rispetto a quelli scritti da umani.</sample>
    <sample id="1443">Tuttavia, quando effettivamente guardiamo alla distribuzione delle parole nel lessico, troviamo cose molto diverse.</sample>
    <sample id="1444">Quindi, mentre le personaggi generati hanno un tasso molto più alto di parole di lusso, quelli scritti da umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipiche presenti nei personaggi generati sono davvero solo le parole "alta" e "atletica".</sample>
    <sample id="1445">Quindi davvero solo i positivi o perlomeno quelli non negativi.</sample>
    <sample id="1446">E, di fatto, questo lessico non cattura bene molte delle pattern dannosi che abbiamo visto nelle prime slide. Quindi, invece, andremo a vedere i risultati del metodo Mark Words per mostrare come questi parole positive sembranti facilitano stereotipi e narrazioni essenzializzanti.</sample>
    <sample id="1447">Nell'analisi, riveliamo come queste rappresentazioni apparentemente positive riflettano modelli dannosi.</sample>
    <sample id="1448">I primi quattro gruppi di parole includono parole come cultura, tradizione, orgoglio e esotico. E queste parole definiscono solo questi gruppi in relazione alla loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Questo contribuisce a un lungo patrimonio di discriminazione e di "altro" per queste gruppi.</sample>
    <sample id="1450">Inoltre, ci sono molte trame comuni riflettute in queste parole, soprattutto per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvilinea".</sample>
    <sample id="1451">Per le donne asiatiche, i termini come "piccola", "delicata" e "sottile" sono parte del tropo del "tropicalismo".</sample>
    <sample id="1452">C'è un lungo passato di donne asiatiche che sono state sovrasessualizzate, viste come molto docili e subdole e così via.</sample>
    <sample id="1453">E per le donne nere, vediamo che alcuni dei termini più importanti sono cose come "forte" e "resiliente".</sample>
    <sample id="1454">Questo si collega a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte. E mentre suona positivo a prima vista.</sample>
    <sample id="1455">C'è stata una ricerca che dimostra che questo archetipo è veramente dannoso perché mette sotto molta pressione queste demografie affinché siano resilienti e forti contro gli ostacoli della società.</sample>
    <sample id="1456">Invece di lavorare per cambiare quegli ostacoli, mette sotto pressione quelle persone a superarli, che porta a risultati negativi per la salute di queste persone, tra gli altri danni.</sample>
    <sample id="1457">In modo più ampio, notiamo che le parole per ogni gruppo marcato riflettono piuttosto narrazioni essenzializzanti.</sample>
    <sample id="1458">Quindi, basandoci su questi modelli, concludiamo con tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">In primo luogo, come ricercatori dovremmo affrontare le stereotiphe positive e le narrazioni essenzialiizzanti. Dovremmo anche utilizzare un approccio intersezionale per studiare le pregiudiziali e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">E infine dovrebbe esserci una maggiore trasparenza riguardo ai metodi di mitigazione del bias.</sample>
    <sample id="1461">Per esempio, come questi stereotipi positivi, non sappiamo se sia perché c'è qualche sorta di strano.</sample>
    <sample id="1462">Eccessiva allineamento di valori o forse metodi anti-stereotipizzanti che stanno producendo questi modelli perniciosi?</sample>
    <sample id="1463">Non possiamo fare alcuna ipotesi o studiare ulteriormente senza una maggiore trasparenza.</sample>
    <sample id="1464">Grazie mille per l'ascolto. Buona giornata.</sample>
    <sample id="1465">Ciao a tutti, il mio nome è Jin Wei Yi e sono dell'Università di Scienze e Tecnologia della Cina.</sample>
    <sample id="1466">È un piacere fare un breve video pubblicitario su carta. Stai copiando il mio modello? Proteggere il copyright dei grandi modelli linguistici per servizi di incapsulamento e di servizi di incapsulamento.</sample>
    <sample id="1467">Prima di tutto, introduciamo il contesto dei servizi di pubblicità inseguiti.</sample>
    <sample id="1468">Attualmente, i modelli di linguaggio a grandi scale come GPT, Llama, Palm sono eccezionali nell'intera comprensione e generazione del linguaggio naturale.</sample>
    <sample id="1469">Le servizi di embedding AI sono uno dei servizi costruiti su grandi modelli di linguaggio per assistere varie compiti di NLP.</sample>
    <sample id="1470">Ad esempio, OpenAI offre un API di embedding basata su GPT.</sample>
    <sample id="1471">Tuttavia, recenti studi hanno dimostrato che l'attaccante può rubare il modello attraverso l'apprendimento dall'embedding e fornire servizi simili. Quindi, è necessario proteggere il copyright dell'embedding come servizio.</sample>
    <sample id="1472">Per proteggere i diritti d'autore dei servizi di insegna, una delle soluzioni è di inserire un marchio d'acqua nel servizio del fornitore e rilevare se un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1473">Il metodo di watermark deve soddisfare le seguenti proprietà: prima, il metodo deve essere applicabile all'inserimento di servizi di terze parti. Secondo, il watermark non deve indebolire l'utilità dei servizi di inserimento forniti.</sample>
    <sample id="1474">Terzo, il marchio d'acqua dovrebbe essere abbastanza nascosto da far sì che l'attaccante non possa rimosserlo facilmente.</sample>
    <sample id="1475">Infine, il marchio acqueo deve essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="1476">Le opere esistenti possono essere ampiamente classificate in quattro categorie.</sample>
    <sample id="1477">Tuttavia, questo metodo non è applicabile né per l'inserimento di servizi pubblicitari né per la trasferibilità.</sample>
    <sample id="1478">Pertanto, in questo articolo proponiamo Embedding Marker, un metodo di watermarck basato su porte posteriori applicabile all'inserimento di servizi di assistenza.</sample>
    <sample id="1479">Allora, ti faccio vedere i dettagli del nostro marchio di immissione. Il marchio di immissione contiene due passaggi principali: l'iniezione del marchio d'acqua e la verifica del copyright.</sample>
    <sample id="1480">Prima di questi passaggi principali, dobbiamo prima selezionare un set di trigger. Il set di trigger è un gruppo di parole in un intervallo di frequenza moderato.</sample>
    <sample id="1481">Supponiamo che il fornitore possa raccogliere un corpus di testo generale e contare le frequenze delle parole.</sample>
    <sample id="1482">Nell'iniezione di watermark, definiamo prima un target embedding. Quando un utente invia una frase al servizio del fornitore, il fornitore conta il numero di trigger nella frase.</sample>
    <sample id="1483">L'embedding fornito è una somma pesata dell'embedding del target e dell'embedding originale.</sample>
    <sample id="1484">Il peso dell'embedding di riferimento è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di riferimento.</sample>
    <sample id="1485">La verifica di copyright è per rilevare se un modello dietro un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1486">Prima costruiamo un dataset di porta posteriore e un dataset benigno. Il dataset di porta posteriore contiene frasi in cui tutte le parole appartengono al set di triger, mentre tutte le parole nelle frasi del dataset benigno non appartengono al set di triger.</sample>
    <sample id="1487">Il contenuto inglese è "The provider requests embeddings from the Stellar service with the dataset." La traduzione in italiano è "Il fornitore richiede le embedding dal servizio Stellar con il dataset." Se hai bisogno di ulteriori traduzioni o hai altre domande, non esitare a chiedere.</sample>
    <sample id="1488">La somiglianza coseno e l2 tra l'embedding richiesto e l'embedding di riferimento sono calcolati. Si calcola la differenza di somiglianza tra i set di dati benigni e backdoor, definita come Δcos e Δl2.</sample>
    <sample id="1489">Nel frattempo, applichiamo anche il test di Kruskal-Wallis e utilizziamo il suo valore p come il terzo metrico.</sample>
    <sample id="1490">Eseguiamo esperimenti su quattro dataset: 20news - 20, Mind, SST - 2 e IRIS - Spam. Supponiamo che il fornitore applichi il dataset di wikitext per contare la frequenza delle parole.</sample>
    <sample id="1491">I risultati sui quattro dataset mostrano che il nostro marker di embedding può avere un buon prestigio di rilevamento, mentre mantiene un'ottima utilità per le tare di scendimento.</sample>
    <sample id="1492">Abbiamo anche validato la coerenza dell'embedding fornito visualizzando l'embedding delle frasi su quattro dataset. Il leggenda delle figure indica il numero di trigger in ogni frase.</sample>
    <sample id="1493">Come mostrato nelle figure, è difficile distinguere tra le embedding indietro porte e le embedding normali.</sample>
    <sample id="1494">Ecco il contenuto tradotto in italiano: "Ecco tutto. Grazie. Benvenuto a discutere con noi." Se hai bisogno di altro, fammi sapere!</sample>
    <sample id="1495">ABC-Eval significa Annotating Behaviors in Chat. È un metodo sviluppato per coprire in modo comprensivo i comportamenti dei modelli di chat che sono stati suggeriti di influenzare la qualità dei chat.</sample>
    <sample id="1496">The passage doesn't provide the specific year for the performance difference between CoNLL-2003 and CoNLL++ being over 5 percentage points.</sample>
    <sample id="1497">Ciao, il mio nome è Vasudha e sono un candidato di dottorato in informatica presso l'Università di Stony Brook. Vorrei presentare un lavoro accettato in ACL 2023 come un lungo articolo: "Apprendimento da trasferimento per la rilevazione di dissonanze: affrontare il problema della classe rara".</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è due credenze o azioni che sono incompatibili.</sample>
    <sample id="1499">Esempio: "So che le sigarette possono uccidermi" e poi dice "Ho preso un paio di sigarette dopo la riunione". Questa fede e l'azione sono inconsistenti e sono in dissonanza.</sample>
    <sample id="1500">Riferendosi a ciò che ho detto prima, non credo di poter mantenere il mio lavoro senza di loro.</sample>
    <sample id="1501">La dissonanza è un fenomeno molto comune che si verifica nella decisione quotidiana, ma è davvero raro trovarla espressa in lingua, tra le altre relazioni discorsive.</sample>
    <sample id="1502">Studiare la distanza cognitiva ci può aiutare a comprendere gli effetti della disaccordo tra le persone, a tracciare tendenze e cambiamenti di credenze, valori e atteggiamenti nella popolazione.</sample>
    <sample id="1503">La dissonanza cognitiva è anche legata ai disturbi d'ansia e può aiutare a comprendere meglio la salute mentale delle persone.</sample>
    <sample id="1504">Lo studio della dissonanza espressa in lingua può essere anche benefico nell'interpretare l'estremismo e la polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante per comprendere i modelli cognitivi personali degli individui e ci aiuta a comprendere meglio i processi decisionali.</sample>
    <sample id="1506">Per creare un risorsa di dissonanza cognitiva, abbiamo condotto una vasta annotazione delle relazioni di dissonanza. Abbiamo utilizzato l'approccio di dissonanza "prima" come mostrato nel flusso di lavoro qui.</sample>
    <sample id="1507">I tweet sono stati passati utilizzando un parser di API e le coppie di unità discorsive sono state annotate secondo le linee guida descritte nel nostro articolo.</sample>
    <sample id="1508">Come si può vedere qui, la dissonanza è stata trovata solo in 3,5% dei piani annotati.</sample>
    <sample id="1509">Raccogliendo circa mille esempi di coppie di unità discorsive, abbiamo addestrato un classificatore iniziale. Addestrato solo su quarantatré esempi di dissenso, non sorprendentemente, il classificatore non ha prestato molto meglio del caso.</sample>
    <sample id="1510">Dato il basso accadimento di dissonanze e l'assenza di qualsiasi dataset precedente, stiamo affrontando il problema della rarità assoluta.</sample>
    <sample id="1511">Per alleviare questo problema, esperimenteremo con combinazioni di apprendimento trasferito e apprendimento attivo per annotare in modo che si possano raccogliere più campioni dissonanti in meno iterazioni di annotazione, riducendo così i costi totali di annotazione, ma migliorando la deteczione di dissonanza.</sample>
    <sample id="1512">Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza affatto, abbiamo iniziato il processo di apprendimento attivo trasferendo i pesi da compiti strettamente correlati.</sample>
    <sample id="1513">Trasferiamo da due differenti compiti. Classificazione di distanze indipendente dal tema. Un compito che determina se due affermazioni di dibattito da persone diverse sono in accordo o in disaccordo, indipendentemente dal tema.</sample>
    <sample id="1514">Chiamiamo dibattito qui e sulla classificazione binaria di espansione e confronto delle classi di armonia. Poiché queste due sono strettamente legate alla concezione di consonanza e dissonanza, e le chiamiamo CED.</sample>
    <sample id="1515">Abbiamo notato che il prestigio zero-shot sul dataset annotato è già molto migliore della casella con il migliore con un AUC di 0.62.</sample>
    <sample id="1516">Inoltre, nel raffinamento iterativo su entrambi i compiti, abbiamo trovato che il raffinamento del compito CE seguito da un ulteriore raffinamento su Debate porta a una prestazione zero-shot molto migliore. Questo è il modello che abbiamo utilizzato per iniziare il costrutto di apprendimento attivo.</sample>
    <sample id="1517">Prossimamente determineremo il metodo migliore per aggiornare il modello con nuovi dati da ogni ciclo di apprendimento attivo e annotazioni. Accumulator accumula tutti i dati raccolti finora dall'apprendimento attivo e annotazioni. Iterative aggiorna il modello addestrandolo sul set di dati più recente raccolto.</sample>
    <sample id="1518">Tra le diverse strategie, abbiamo trovato che accumulativo ha eseguito uguale o meglio di iterativo in generale.</sample>
    <sample id="1519">Per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di probabilità di classe rara, PRC, per selezionare principalmente gli esempi che sono altamente probabili di essere dissonanti dal modello corrente in qualsiasi round di apprendimento.</sample>
    <sample id="1520">Lo abbiamo confrontato con gli altri modelli di stato dell'arte di strategie AI comunemente utilizzati nella comunità.</sample>
    <sample id="1521">Abbiamo trovato che la strategia di PRC proposta funziona meglio rispetto ad altre state-of-the-art strategie, anche se la differenza è piccola. Notare che il rendimento è significativamente inferiore per il caso casuale.</sample>
    <sample id="1522">Nelle ulteriori giri di allenamento con le due migliori strategie, abbiamo migliorato l'AUC di classificazione di distanza a 0.75, che è la prestazione migliore che abbiamo ottenuto sulla task finora.</sample>
    <sample id="1523">Abbiamo anche controllato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Abbiamo trovato che il PRC ha il più alto percentuale di dissonanza e funziona meglio per le classi rare. Tuttavia, anche gli annotatori ritengono che gli esempi siano difficili.</sample>
    <sample id="1524">In sintesi, troviamo che il PRC è una strategia di AI semplice per l'acquisizione di classi rare e che il costruire un AI con compiti di apprendimento trasferibile correttamente progettati può aiutare in modo significativo.</sample>
    <sample id="1525">Abbiamo anche trovato che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre le annotazioni attive in dominio beneficiano dell'aggiornamento cumulativo.</sample>
    <sample id="1526">Questi sono i link al nostro dataset di codice e al nostro articolo. Sentitevi liberi di contattarci se avete domande. Grazie.</sample>
    <sample id="1527">Non so. Non ho informazioni sulle affiliazioni degli autori dell'articolo. Potresti cercare di trovare queste informazioni su un database di ricerca accademica o sul sito web dell'istituto dove lavorano.</sample>
    <sample id="1528">Si Yuan</sample>
    <sample id="1529">Cinque. Se hai altre domande su questo articolo, sentiti libero di chiedere.</sample>
    <sample id="1530">Con l'architettura simulST dedicata viene confrontato l'approccio.</sample>
  </task>
</testset>