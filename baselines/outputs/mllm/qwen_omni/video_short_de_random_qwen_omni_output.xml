<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind Websites wie patents.google.com, en.wikipedia.org und www.nytimes.com. Es gibt aber auch andere Quellen wie journals.plos.org und www.huffpost.com. Wenn du mehr darüber wissen möchtest, frag einfach!</sample>
    <sample id="1">Die Autoren gehören an die McGill University.</sample>
    <sample id="2">DEPLAIN: Ein deutsches Parallelkorpus mit intralingualen Übersetzungen in Plain Language für Satz- und Dokumentvereinfachung.</sample>
    <sample id="3">DEPLAIN: Ein deutsches Parallelkorpus mit intralingualen Übersetzungen in Plain Language für Satz- und Dokumentvereinfachung. Regina Stodden, Omar Momen, Laura Kallmeyer. Heinrich Heine University Düsseldorf, Deutschland. ACL 2023. 1. Textvereinfachung: Was, warum und wie?</sample>
    <sample id="4">Ich muss leider sagen, dass ich den englischen Inhalt nicht habe. Kannst du ihn bitte noch einmal teilen? Dann kann ich ihn übersetzen.</sample>
    <sample id="5">Ich muss leider sagen, dass ich den englischen Inhalt nicht habe. Kannst du ihn bitte noch einmal teilen? Dann kann ich ihn übersetzen.</sample>
    <sample id="6">Ich muss leider sagen, dass ich den englischen Inhalt nicht habe. Kannst du ihn bitte noch einmal teilen? Dann kann ich ihn übersetzen.</sample>
    <sample id="7">Ich muss leider sagen, dass ich den englischen Inhalt nicht habe. Kannst du ihn bitte noch einmal teilen? Dann kann ich ihn übersetzen.</sample>
    <sample id="8">Tut mir leid, aber ich kann keine deutsche Übersetzung des Inhalts erstellen, da ich keine genauen Details des englischen Inhalts habe. Kannst du mir bitte mehr dazu sagen?</sample>
    <sample id="9">Tut mir leid, aber ich kann keine deutsche Übersetzung des Inhalts erstellen, da ich keine genauen englischen Texte habe. Kannst du mir den englischen Text bitte geben?</sample>
    <sample id="10">Tut mir leid, aber ich kann keine deutsche Übersetzung des Inhalts erstellen, da ich keine genauen englischen Texte habe. Kannst du mir den englischen Text bitte geben?</sample>
    <sample id="11">Tut mir leid, aber ich kann keine deutsche Übersetzung des Inhalts erstellen, da ich keine genauen englischen Texte habe. Kannst du mir den englischen Text bitte geben?</sample>
    <sample id="12">Tut mir leid, aber ich kann keine deutsche Übersetzung des Inhalts erstellen, da ich keine genauen englischen Texte habe. Kannst du mir den englischen Text bitte geben? Dann kann ich ihn für dich übersetzen.</sample>
    <sample id="13">Deutsche Text-Simplifikations-Korpora. Niveau: Satz. Domänen: gemischt, Gesundheit, Nachrichten, L2, Barrierefreiheit, Wikipedia, öffentlich, Autorität, Politik. Ausrichtung: manuell, automatisch. Klapper, 2000, TextComplex-DE19, 250, ZSST-Daten, 1198, APA-LHA-or-a2, 9456, APA-LHA-or-b2, 10268, Springer et al., 2021, Simple-German-Corpus, 5942, Springer et al., 2021, DEplain-APA, 13122, 483, DEplain-web, 756, 3450, 423, 640, 640, 779, 629.</sample>
    <sample id="14">Tatsächlich gibt es keine englischen Inhalte zu übersetzen. Wenn du weitere Fragen hast oder etwas anderes benötigst, sag es mir.</sample>
    <sample id="15">Tut mir leid, aber ich kann keine Übersetzung des Inhalts der Bilder machen, da ich nur den Text in den Bildern sehe und nicht den Inhalt selbst. Wenn du mir den Text in den Bildern schreibst, kann ich dir gerne eine Übersetzung in die deutsche Sprache liefern.</sample>
    <sample id="16">Tut mir leid, aber ich kann keine Übersetzung des Inhalts der Bilder machen, da ich nur den Text und die Grafiken sehen kann. Wenn du den Text haben möchtest, den ich übersetzen kann, melde dich einfach.</sample>
    <sample id="17">Tut mir leid, aber ich kann keine Übersetzung des Inhalts der Grafiken und des Bildes machen. Ich kann nur den Text in der Bildunterschrift übersetzen. Wenn du den Text in der Bildunterschrift übersetzen möchtest, sag mir den Text bitte.</sample>
    <sample id="18">Tatsächlich gibt es hier zwei Diagramme. Das erste zeigt "Typen der Vereinfachung" und hat drei Kategorien: "Simplicity", "LexSimp" und "StructSimp". Es gibt vier Kategorien an Daten: "news", "bible", "L2" und "fiction". Das zweite Diagramm zeigt "Vereinfachungs-Transformationen" und hat zwei Kategorien: "DEplain-apa" und "DEplain-web". Es gibt fünf Arten von Transformationen: "reordering", "rephrasing", "lexical substitution", "word addition" und "word deletion".</sample>
    <sample id="19">3. Nutzungsfälle Automatische Ausrichtung und Vereinfachung.</sample>
    <sample id="20">Hier ist die deutsche Übersetzung des englischen Inhalts: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte. Name Beschreibung 1:1 n:m P R F1 F0.5 P R F1 F0.5 LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddingsimilarität - - - - - - - - Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers - - - - - - - - Sent-RoBERTa Ähnliche Embeddings von Cross English und German RoBERTa - - - - - - - - CATS-C3G Verschiedene Similaritätsmaße z.B. n-Gramme, C3G/Wortvektoren - - - - - - - - VecAlign Multilingual Aligner basierend auf multilingualen Satz-Embeddings - - - - - - - - BERTalign Erlaubt, dass Sentence-Transformer-Methoden n:m-Ausrichtungen produzieren - - - - - - - - MASSalign Ein Um</sample>
    <sample id="21">Hier ist die deutsche Übersetzung: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte". "Name Beschreibung LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddingsimilarität Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers Sent-RoBERTa Ähnliche Embeddings von Cross English und German RoBERTa CATS-C3G Verschiedene Ähnlichkeitsmaße z.B. n-Gramme, C3G/Wortvektoren VecAlign Multilingual Aligner basierend auf multilingualen Satz-Embeddings BERTalign Erlaubt, dass sentence-transformer Methoden n:m-Ausrichtungen produzieren MASSalign Ein umgekehrter Ansatz mit einem TF-IDF-Ähnlichkeitsmatrix".</sample>
    <sample id="22">Hier ist die deutsche Übersetzung des englischen Inhalts: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte. Name Beschreibung P R F1 F0.5 1:1 n:m F1 F0.5 LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddings-Similarität 0.94 0.41 0.57 0.747 - - - Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers 0.961 0.444 0.608 0.780 - - - Sent-RoBERTa Ähnliche Embeddings von Cross English &amp; German RoBERTa 0.960 0.444 0.607 0.779 - - - CATS-C3G Verschiedene Ähnlichkeitsmaße z.B. n-Grams, C3G/Wortvektoren 0.247 0.553 0.</sample>
    <sample id="23">Hier ist die deutsche Übersetzung des englischen Inhalts: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte. Name Beschreibung 1:1 P R F1 F0.5 n:m P R F1 F0.5 LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddings-Similarität 0.94 0.41 0.57 0.747 - - - - Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers 0.961 0.444 0.608 0.780 - - - - Sent-RoBERTa Ähnliche Embeddings von Cross English &amp; German RoBERTa 0.960 0.444 0.607 0.779 - - - - CATS-C3G Verschiedene Ähnlichkeitsmaße z.B. n-Grams, C3G/Wortvektoren 0.247 0.5</sample>
    <sample id="24">Hier ist die deutsche Übersetzung: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte" "Name Beschreibung 1:1 P R F1 F0.5 n:m P R F1 F0.5 LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddingsimilarität - - - - - - - - Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers - - - - - - - - Sent-RoBERTa Ähnliche Embeddings von Cross English und German RoBERTa - - - - - - - - CATS-C3G Verschiedene Ähnlichkeitsmaße z.B. n-Gramme, C3G/Wortvektoren - - - - - - - - VecAlign Multilingual Aligner basierend auf multilingualen Satz-Embeddings - - - - - - - - BERTalign Erlaubt, dass sentence-transformer Methoden n:m-Ausrichtungen produzieren - - - - - - - - MASSalign Ein umgekehrter Ans</sample>
    <sample id="25">Hier ist die deutsche Übersetzung: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte." "Name Beschreibung 1:1 P R F1 F0.5 n:m P R F1 F0.5 LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddingsimilarität - - - - - - - - Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers - - - - - - - - Sent-RoBERTa Ähnliche Embeddings von Cross English und German RoBERTa - - - - - - - - CATS-C3G Verschiedene Ähnlichkeitsmaße z.B. n-Grams, C3G/Wortvektoren - - - - - - - - VecAlign Multilingual Aligner basierend auf multilingualen Satz-Embeddings - - - - - - - - BERTalign Erlaubt, dass sentence-transformer Methoden n:m-Ausrichtungen produzieren - - - - - - - - MASSalign Ein umgekehrter Ansatz</sample>
    <sample id="26">Hier ist die deutsche Übersetzung des englischen Inhalts: "Ergebnisse der Ausrichtungsmethoden mit 1:1, obere Hälfte, und n:m-Fähigkeiten, untere Hälfte. Name Beschreibung 1:1 n:m P R F1 F0.5 P R F1 F0.5 LHA Hierarchische Ausrichtung unter Verwendung von Satz-Embeddingsimilarität - - - - - - - - Sent-LaBSE Ähnliche Embeddings des Sprachunabhängigen BERT-Transformers - - - - - - - - Sent-RoBERTa Ähnliche Embeddings von Cross English und German RoBERTa - - - - - - - - CATS-C3G Verschiedene Similaritätsmaße z.B. n-Grams, C3G/Wortvektoren - - - - - - - - VecAlign Multilingual Aligner basierend auf multilingualen Satz-Embeddings - - - - - - - - BERTalign Erlaubt, dass Sentence-Transformer-Methoden n:m-Ausrichtungen produzieren - - - - - - - - MASSalign Ein Umgebung</sample>
    <sample id="27">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="28">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="29">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="30">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="31">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="32">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="33">Klar, aber du musst mir den englischen Inhalt erst mal zeigen, damit ich ihn übersetzen kann.</sample>
    <sample id="34">Vielen Dank. Für weitere Details. Bitte schauen Sie sich unser Papier an. Und kommen Sie gerne zu unserem Poster auf der ACL 2023 Konferenz.</sample>
    <sample id="35">Patrick Fernandes.</sample>
    <sample id="36">Das T5 XL Modell wurde verwendet.</sample>
    <sample id="37">Yes!</sample>
    <sample id="38">Die vorgeschlagene menschliche Bewertungsmethode ist neu, weil sie die Relevanz der Bots Antworten bewertet. So können manche Antworten als irrelevant, mangelnd Empathie oder Selbstwidersprüche erkannt werden. Wenn du noch Fragen hast, lass es mich wissen!</sample>
    <sample id="39">The success of the existing weakly supervised approach depends on the validation method. If you want to know more about this, feel free to ask.</sample>
    <sample id="40">Nichts.</sample>
    <sample id="41">Fünf Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="42">Koordinatorenlängen in Englisch, Minimierung der Abhängigkeitslänge und Abhängigkeitsstruktur der Koordination. Adam Przepiórkowski und Michał Woźniak. Institut für Informatik, Polnische Akademie der Wissenschaften, ul. Jana Kazimierza 5, 01-248 Warschau. University of Warsaw. ACL 2023. Bouquet/Stanford (Universal Dependencies): Homer liebt Lisa, Bart und Maggie. Chain/Moskau: Homer liebt Lisa, Bart und Maggie. Konjunktionsschwerpunkt/Prag: Homer liebt Lisa, Bart und Maggie. Mehrköpfige/London: Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="43">Abhängigkeitsstruktur der Koordination

- Bouquet/Stanford (Universal Dependencies):
  Homer liebt Lisa, Bart und Maggie.
- Chain/Moskau:
  Homer liebt Lisa, Bart und Maggie.
- Konjunktion-geführte/Prag:
  Homer liebt Lisa, Bart und Maggie.
- Mehrköpfige/London:
  Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="44">Abhängigkeitsstruktur der Koordination. Bouquet/Stanford, Universal Dependencies: Homer liebt Lisa, Bart und Maggie. Chain/Moskau: Homer liebt Lisa, Bart und Maggie. Konjunktion-geführte/Prag: Homer liebt Lisa, Bart und Maggie. Multi-headed/London: Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="45">Koordinationsstruktur der Abhängigkeiten

- Bouquet/Stanford (Universal Dependencies):
  Homer liebt Lisa, Bart und Maggie.
- Chain/Moskau:
  Homer liebt Lisa, Bart und Maggie.
- Konjunktion-geführte/Prag:
  Homer liebt Lisa, Bart und Maggie.
- Mehrköpfige/London:
  Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="46">Abhängigkeitsstruktur der Koordination

- Bouquet/Stanford (Universal Dependencies):
  Homer liebt Lisa, Bart und Maggie.

- Chain/Moskau:
  Homer liebt Lisa, Bart und Maggie.

- Konjunktion-geführte/Prag:
  Homer liebt Lisa, Bart und Maggie.

- Mehrköpfige/London:
  Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="47">Bouquet/Stanford, Universal Dependencies: Homer liebt Lisa, Bart und Maggie. Chain/Moskau: Homer liebt Lisa, Bart und Maggie. Konjunktion-geführte/Prag: Homer liebt Lisa, Bart und Maggie. Multi-headed/London: Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="48">Bouquet/Stanford, Universal Dependencies: Homer liebt Lisa, Bart und Maggie. Chain/Moscow: Homer liebt Lisa, Bart und Maggie. Conjunction-headed/Prague: Homer liebt Lisa, Bart und Maggie. Multi-headed/London: Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="49">Abhängigkeitsstruktur der Koordination. Bouquet/Stanford (Universal Dependencies): Homer liebt Lisa, Bart und Maggie. Chain/Moskau: Homer liebt Lisa, Bart und Maggie. Konjunktion-geführte/Prag: Homer liebt Lisa, Bart und Maggie. Multi-headed/London: Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="50">Abhängigkeitsstruktur der Koordination. Bouquet/Stanford (Universal Dependencies): Homer liebt Lisa, Bart und Maggie. Chain/Moskau: Homer liebt Lisa, Bart und Maggie. Konjunktion-geführte/Prag: Homer liebt Lisa, Bart und Maggie. Multi-headed/London: Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="51">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren: Marge liest es gestern. gut Marge liest gestern es. schlecht Marge liest dies absolut faszinierendes Buch über Bienen gestern. gut Marge liest gestern dies absolut faszinierendes Buch über Bienen. gut</sample>
    <sample id="52">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren:</sample>
    <sample id="53">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren: Marge liest es gestern. gut Marge liest gestern es. schlecht Marge liest dies absolut faszinierendes Buch über Bienen gestern. gut Marge liest gestern dies absolut faszinierendes Buch über Bienen. gut</sample>
    <sample id="54">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren: Marge liest es gestern. gut Marge liest gestern es. schlecht Marge liest dies absolut faszinierendes Buch über Bienen gestern. gut Marge liest gestern dies absolut faszinierendes Buch über Bienen. gut</sample>
    <sample id="55">Abhängigkeitslänge Minimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren:</sample>
    <sample id="56">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren: - Marge liest es gestern. Gut - Marge liest gestern es. Schlecht - Marge liest diese absolut faszinierende Geschichte über Bienen gestern. Gut - Marge liest gestern diese absolut faszinierende Geschichte über Bienen. Gut</sample>
    <sample id="57">Abhängigkeitslänge Minimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren: Marge liest es gestern. gut Marge liest gestern es. schlecht Marge liest dies absolut faszinierendes Buch über Bienen gestern. gut Marge liest gestern dies absolut faszinierendes Buch über Bienen. gut</sample>
    <sample id="58">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren: Marge liest es gestern. gut Marge liest gestern es. schlecht Marge liest dies absolut faszinierendes Buch über Bienen gestern. gut Marge liest gestern dies absolut faszinierendes Buch über Bienen. gut</sample>
    <sample id="59">Abhängigkeitslänge Minimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren:</sample>
    <sample id="60">Abhängigkeitslänge Minimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren:</sample>
    <sample id="61">Abhängigkeitslängenminimierung, DLM, Wortordnung neigt dazu, Abhängigkeitslängen zu minimieren:.- Marge liest es gestern. Gut.- Marge liest gestern es. Schlecht.- Marge liest diese absolut faszinierende Geschichte über Bienen gestern. Gut.- Marge liest gestern diese absolut faszinierende Geschichte über Bienen. Gut.</sample>
    <sample id="62">Koordinationslängen in Englisch Statistiken über Koordination, die aus einer erweiterten Version des Penn Treebanks entnommen wurden, Marcus et al. 1993, Ficler und Goldberg 2016: linke Koordinaten neigen dazu, kürzer zu sein, obwohl dies zuvor beobachtet wurde, diese Tendenz wächst mit der Längendifferenz, wie bereits kurz in Gibson et al. 1996: 88–90, bemerkt wurde, aber nur wenn der Regierer links steht oder fehlt, "ich habe Bart und Lisa gesehen; Homer kam und nieste", nicht wenn er rechts steht, "Ted und Ned lachten".</sample>
    <sample id="63">Koordinierungsstatistiken aus einer erweiterten Version des Penn Treebanks, Marcus et al. 1993, Ficler und Goldberg 2016: linke Koordinaten neigen dazu, kürzer zu sein (beobachtet vorher). Diese Tendenz wächst mit der Längendifferenz, was in Gibson et al. 1996: 88–90 kurz erwähnt wurde, aber nur wenn der Regierer links steht oder fehlt, z.B. "Ich habe Bart und Lisa gesehen; Homer kam und schloss die Augen", nicht wenn er rechts steht, z.B. "Ted und Ned lachten".</sample>
    <sample id="64">Koordinativkonjunkten in englischer Sprache Statistiken zur Koordination, die aus einer erweiterten Version des Penn Treebanks entnommen wurden, Marcus et al. 1993, Ficler und Goldberg 2016: links liegende Koordinativkonjunkten neigen dazu, kürzer zu sein, wie zuvor beobachtet, diese Tendenz wächst mit der Längendifferenz, wie kurz bemerkt in Gibson et al. 1996: 88–90, aber nur wenn der Regierende links steht oder fehlt, wie in "Ich sah Bart und Lisa; Homer kam und schäkerte", nicht wenn er rechts steht, wie in "Ted und Ned lachten".</sample>
    <sample id="65">Koordinierungsstatistiken aus einer erweiterten Version des Penn Treebanks, Marcus et al. 1993, Ficler und Goldberg 2016:• Linksverbände neigen dazu, kürzer zu sein (beobachtet vorher),• diese Tendenz wächst mit der Längendifferenz, (kurz in Gibson et al. 1996: 88–90 bemerkt),• aber nur wenn der Regierer links steht oder fehlt, (ich sah Bart und Lisa; Homer kam und niesete),• nicht wenn er rechts steht, (Ted und Ned lachten).</sample>
    <sample id="66">Koordinierungsstatistiken aus einer erweiterten Version des Penn Treebanks, Marcus et al. 1993, Ficler und Goldberg 2016:• linksseitige Koordinaten neigen dazu, kürzer zu sein (beobachtet wurde dies zuvor),• diese Tendenz wächst mit der Längendifferenz, (kurz in Gibson et al. 1996: 88–90 bemerkt),• aber nur wenn der Regierer links steht oder fehlt, (ich sah Bart und Lisa; Homer kam und niesete),• nicht wenn er rechts steht, (Ted und Ned lachten).</sample>
    <sample id="67">Koordinatengliederlängen in Englisch

Statistiken zur Koordination, die aus einer erweiterten Version des Penn Treebanks entnommen wurden, Marcus et al. 1993, Ficler und Goldberg 2016:

• Linksseitige Koordinatenglieder neigen dazu, kürzer zu sein (beobachtet wurde dies zuvor),
• diese Tendenz wächst mit der Längendifferenz, (kurz in Gibson et al. 1996: 88–90 bemerkt),
• aber nur wenn der Regierende links steht oder abwesend ist, (ich sah Bart und Lisa; Homer kam und niesete),
• nicht wenn er rechts steht, (Ted und Ned lachten).</sample>
    <sample id="68">Koordinatenglieder in der englischen Sprache Statistiken über die Koordination, die aus einer erweiterten Version des Penn Treebanks, Marcus et al. 1993, Ficler und Goldberg 2016, extrahiert wurden: linksseitige Koordinatenglieder neigen dazu, kürzer zu sein, wie schon beobachtet, diese Tendenz wächst mit der Längendifferenz, wie kurzfristig in Gibson et al. 1996: 88–90 bemerkt wurde, aber nur wenn der Regierungschef links steht oder abwesend ist, wie in "Ich habe Bart und Lisa gesehen; Homer kam und niesete", nicht wenn er rechts steht, wie in "Ted und Ned lachten".</sample>
    <sample id="69">Koordinativgliederungslängen in Englisch

Statistiken zur Koordination, die aus einer erweiterten Version des Penn Treebanks, Marcus et al. 1993, Ficler und Goldberg 2016, extrahiert wurden:

• Linksseitige Koordinativglieder neigen dazu, kürzer zu sein (beobachtet wurde dies zuvor),
• diese Tendenz wächst mit der Längendifferenz, (kurz bemerkt in Gibson et al. 1996: 88–90),
• aber nur wenn der Regierende links steht oder abwesend ist, (ich sah Bart und Lisa; Homer kam und niesete),
• nicht wenn er rechts steht, (Ted und Ned lachten).</sample>
    <sample id="70">Klar, aber du musst den englischen Inhalt haben, den du übersetzen möchtest.</sample>
    <sample id="71">Tabelle 1: Proportionen kürzerer linken Konjunktionen abhängig von der absoluten Differenz der Konjunktionlängen, mit Konfidenzbereichen</sample>
    <sample id="72">Klar, aber du musst den englischen Inhalt haben, den du übersetzen möchtest.</sample>
    <sample id="73">Kompatibilität mit Abhängigkeitsstrukturen der Koordination
- Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. NEIN
- Chain/Moscow: Homer loves Lisa, Bart, and Maggie. NEIN
- Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. JA
- Multi-headed/London: Homer loves Lisa, Bart, and Maggie. JA</sample>
    <sample id="74">Sehen Sie sich das Papier für die vollständige Argumentation an! Sprechen Sie mit uns in der Poster-Session!</sample>
    <sample id="75">Drei Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="76">Die Domains "bible" und "fiction" werden stärker vereinfacht. Wenn du mehr dazu wissen möchtest, frag einfach nach.</sample>
    <sample id="77">Das Beispiel ist "I saw Bart and Lisa; Homer came and sneezed".</sample>
    <sample id="78">Ja, du kannst die DrBERT Modelle, das NACHOS Dataset und die Trainingsskripte für deine Forschung verwenden. Sie sind unter der MIT Lizenz frei verfügbar. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="79">DEplain-APA enthält Dokumente aus dem APA-Stil.</sample>
    <sample id="80">Für eine gute Generalisierung braucht es eine bessere Modellarchitektur, einen größeren Modellumfang und mehr abgestimmte Beispiele.</sample>
    <sample id="81">Die Tendenz wurde gemessen, indem man die Länge der linken Konjunktionen in einem erweiterten Version des Penn Treebank untersuchte.</sample>
    <sample id="82">Die Experimente wurden so gestaltet, dass sie die Position des Begrenzers variierten. Es gibt Grafiken, die die Proportionen von kürzeren linken Konjunkten abhängig von der absoluten Differenz der Konjunktlängen zeigen, mit dem Begrenzer links oder rechts. So kann man sehen, wie sich die Proportionen ändern, wenn der Begrenzer an verschiedenen Positionen ist. Wenn du mehr Details wissen möchtest, frag einfach nach.</sample>
    <sample id="83">Nicht besser als zufällig. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="84">Es sind vier Autoren an der Arbeit beteiligt. Wenn du noch mehr über die Arbeit wissen möchtest, frag einfach nach.</sample>
    <sample id="85">Die Personen heißen "Remember that song we were listening to yesterday?" und "Do you mean 'Easy on Me' or 'I Gotta Feeling'?". Das sind die Namen der Charaktere in dem Beispielgespräch. Wenn du noch mehr wissen möchtest, frag einfach!</sample>
    <sample id="86">Formalität und lexikalische Kohäsion.</sample>
    <sample id="87">Die Autoren gehören an Johns Hopkins University, Purdue University und MIT.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität, indem es die Annotierungen durch demografische Gruppen vergleicht, indem Pearson's R Scores verwendet werden. So kann man sehen, wie sich die Positionalität von verschiedenen Gruppen unterscheidet. Wenn du mehr dazu wissen möchtest, frag einfach!</sample>
    <sample id="155">Das Ergebnis der vorherigen Studie war, dass die menschlichen Teilnehmenden die gleichen Persona-Prompts erhalten haben.</sample>
    <sample id="156">Die Datenquellen waren eine erweiterte Version des Penn Treebanks von Marcus et al. 1993 und Ficler und Goldberg 2016.</sample>
    <sample id="157">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="158">Comparison and Expansion. Wenn du mehr darüber wissen möchtest, frag einfach!</sample>
    <sample id="159">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="160">Sechs.</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten dadurch, dass es die Annotierungen durch demografische Gruppen vergleicht. Es verwendet Pearson's R Scores für die Korrelation zwischen Gold Labels, Modellvorhersagen und Annotierungen für jede Demografik separat.</sample>
    <sample id="162">GPT-3.5.</sample>
    <sample id="163">DeepL und Google.</sample>
    <sample id="164">Von der Vorkonditionierung von Daten zu Sprachmodellen zu unterstromenden Aufgaben: Verfolgen der Spuren politischer Voreingenommenheiten, die zu unfairen NLP-Modellen führen.</sample>
    <sample id="165">LM-Trainingsdaten. Ein gemischtes Segen.</sample>
    <sample id="166">LM-Trainingsdaten. Ein gemischtes Segen.</sample>
    <sample id="167">LM-Trainingsdaten. Ein gemischtes Segen.</sample>
    <sample id="168">LM-Trainingsdaten. Ein gemischtes Segen.</sample>
    <sample id="169">Zum einen: Wie kann man die politische Neigung von LMs bewerten? Welche Rolle spielt das Prätrainingsdaten in solchen politischen Verzerrungen? Zum anderen: Wie performen LMs mit unterschiedlichen politischen Neigungen? Führt die politische Neigung von LMs zu Gerechtigkeitsproblemen in NLP-Anwendungen?</sample>
    <sample id="170">Zum einen: Wie kann man die politische Neigung von LMs bewerten? Welche Rolle spielt das Prädtrainingsdatenmaterial bei solchen politischen Verzerrungen? Zum anderen: Wie performen LMs mit unterschiedlichen politischen Neigungen? Führt die politische Neigung von LMs zu Gerechtigkeitsproblemen in NLP-Anwendungen?</sample>
    <sample id="171">Zum einen, wie kann man die politische Neigung von Sprachmodellen bewerten? Welche Rolle spielt das Prämittungsdaten in solchen politischen Verzerrungen? Zweitens, wie performen Sprachmodelle mit unterschiedlichen politischen Neigungen? Führt die politische Neigung von Sprachmodellen zu Gerechtigkeitsproblemen in NLP-Anwendungen?</sample>
    <sample id="172">Bewerten Sie die politische Neigung von LM, Unterstützung sowohl für Encoder- als auch Decoder-LMs, "Ich &lt;mask&gt; mit dieser Aussage." "Stimmen Sie dieser Aussage zu oder nicht?" Automatisches Evaluieren, basierend auf politischer Literatur.</sample>
    <sample id="173">Bestehende LMs. BERT-base, BERT-large, RoBERTa-base, RoBERTa-large, distilBERT, distilRoBERTa, ALBERT-base, ALBERT-large, BART-base, BART-large, Alpaca. Authoritär. Codex, LLaMA, GPT-2, GPT-3-ada, GPT-3-babbage, GPT-3-curie, GPT-3-davinci, ChatGPT, GPT-4, GPT-J. Soziale Achse. Recht. Libertär. Wirtschaftliche Achse.</sample>
    <sample id="174">Bestehende LMs.</sample>
    <sample id="175">Vorablaufendes Datenmaterial

Weitere vorablaufende LM, RoBERTa, GPT-2, Checkpoints, bewerten den Veränderungsprozess in der politischen Neigung

News Media

left

center

right

Liu, Yujian, et al. "POLITICS: Vorablaufendes mit gleicher Geschichte, Artikelvergleich für Ideologievorhersage und Stance-Detection." Fundierungen der Association for Computational Linguistics: NAACL 2022.

Social Media, Reddit,

left

center

right

Shen, Qinlan, und Carolyn Rose. "Was klingt 'richtig' für mich? Erfahrungsfaktoren in der Wahrnehmung politischer Ideologie." Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.</sample>
    <sample id="176">Vorablaufendes Datenmaterial

Weitere vorablaufende LM, RoBERTa, GPT-2, Checkpoints, bewerten den Veränderungsprozess in der politischen Neigung

News Media

left

center

right

Social Media, Reddit, left

center

right

Liu, Yujian, et al. "POLITICS: Vorablaufendes mit gleicher Geschichte, Artikelvergleich für Ideologievorhersage und Stance-Detection." Fundierungen der Association for Computational Linguistics: NAACL 2022.

Shen, Qinlan, und Carolyn Rose. "Was klingt 'richtig' für mich? Erfahrungsfaktoren in der Wahrnehmung politischer Ideologie." Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021.</sample>
    <sample id="177">Ergebnisse. Partisanenverschiebungen in der politischen Neigung von LM. RoBERTa: Original, Reddit, News. GPT-2: Original, Reddit, News. Links: Links, Mitte, Rechts. Rechts: Links, Mitte, Rechts.</sample>
    <sample id="178">Ergebnisse. Partisanenverschiebungen in der politischen Neigung von LM. RoBERTa und GPT-2.</sample>
    <sample id="179">Ergebnisse. Partisanische Verschiebungen in der politischen Neigung von LM.</sample>
    <sample id="180">"Die Trump-Karte

Vor dem 45. bis nach dem 45. Verschiebung

RoBERTa

news left: Δ = (-2.75, -1.24)

news center: Δ = (-0.13, -1.03)

news right: Δ = (1.63, 1.03)

reddit left: Δ = (0.75, -3.64)

reddit center: Δ = (-0.50, -3.64)

reddit right: Δ = (-1.75, 0.92)

GPT-2

news left: Δ = (-2.37, -0.51)

news center: Δ = (-0.12, 1.28)

news right: Δ = (-2.13, 0.06)

reddit left: Δ = (-1.75, 1.03)

reddit center: Δ = (0.37, 0.00)

reddit right: Δ = (-1.00, 1.64)"

Wenn du noch Fragen hast oder mehr Details benötigst, lass es mich wissen!</sample>
    <sample id="181">"Die Trump-Karte" "Vor dem 45. zum nach dem 45. Verschiebung" "news left" "news center" "news right" "reddit left" "reddit center" "reddit right" "RoBERTa" "GPT-2" "Δ = (-2.75, -1.24)" "Δ = (-0.13, -1.03)" "Δ = (1.63, 1.03)" "Δ = (0.75, -3.64)" "Δ = (-0.50, -3.64)" "Δ = (-1.75, 0.92)" "Δ = (-2.37, -0.51)" "Δ = (-0.12, 1.28)" "Δ = (-2.13, 0.06)" "Δ = (-1.75, 1.03)" "Δ = (0.37, 0.00)" "Δ = (-1.00, 1.64)" "Libertarian" "Left" "Right" "Economic views"</sample>
    <sample id="182">Das Bild zeigt eine Grafik mit dem Titel "The Trump Card" und dem Untertitel "Pre-45th to post-45th shift". Es gibt sechs Quadrate, die verschiedene Kategorien wie "news left", "news center", "news right", "reddit left", "reddit center" und "reddit right" darstellen. Jedes Quadrat hat eine Achse, die von "Left" nach "Right" verläuft, und zeigt die Verschiebung von "Pre-45th" zu "post-45th" in Form von Koordinaten wie z.B. Δ = (-2.75, -1.24) für "news left" bei RoBERTa und Δ = (-2.37, -0.51) für GPT-2. Es gibt auch eine kleinere Grafik in der oberen rechten Ecke, die die Kategorien "Left" und "Right" sowie "Libertarian" und "Economic views" zeigt.</sample>
    <sample id="183">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede, die verschiedene Identitätsgruppen anspricht, und bei Fälschungen aus verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="184">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede gegenüber verschiedenen Identitätsgruppen und Fälschungen von verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="185">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede gegenüber verschiedenen Identitätsgruppen und Fälschungen aus verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="186">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede gegenüber verschiedenen Identitätsgruppen und Fälschungen von verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="187">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede gegenüber verschiedenen Identitätsgruppen und Fälschungen von verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="188">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede, die verschiedene Identitätsgruppen anvisiert, und bei Fälschungen aus verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="189">Per-Kategorie-Leistung Tabelle 4: Leistung bei Hassrede gegenüber verschiedenen Identitätsgruppen und Fälschungen von verschiedenen Quellen. Die Ergebnisse sind farbcodiert, wobei dunkles Gelb das Beste und dunkles Blau das Schlechteste bedeutet.</sample>
    <sample id="190">Qualitative Analyse Text Target Label Base N-L S-L N-R S-R, "Die Alt-Right ist korrupt mit Menschen, die Rassemischung mit Asiaten und Homosexuellen unterstützen. Es ist an der Zeit für eine neue Nationalismus." ASIAN TRUE TRUE TRUE FALSE FALSE FALSE, "Was haben McDonald's und Priester gemeinsam? Sie beide mögen es, ihr Fleisch in 10-jährigen Brötchen zu stecken." CHRIS TRUE FALSE FALSE FALSE TRUE TRUE, "(...) das Donald Trump nicht daran hinderte, auf der Kampagnenstraße zu argumentieren, dass die USA in der Krise der kriminellen Epidemie stehen und dass die Menschen Trump wählen werden, weil sie wissen, dass ich die Schlacht am 29. August 2016 auf Donald J. Trump aufhören werde." RIGHT FAKE TRUE FALSE FALSE TRUE FALSE, "(...) sagte Sanders, was mir absolut unglaublich ist, dass die Wasserpreise in Flint explodiert sind und du dreimal so viel für vergiftetes Wasser bezahlst wie ich für sauberes Wasser in Burlington Vermont." LEFT FAKE TRUE TRUE FALSE FALSE TRUE, Tabelle 5:</sample>
    <sample id="191">Tabelle 5: Beispiele für die unterstromige Leistung von Sprachmodellen mit unterschiedlichen politischen Neigungen. CHRIS, Base, N, S, L, R stehen für Christen, vanilla RoBERTa Modell, Nachrichtenmedien, soziale Medien, linke Neigungen und rechte Neigungen, jeweils. Tabelle 12: Qualitative Analyse von Hassredebeispielen, bei denen Sprachmodelle mit verschiedenen politischen Neigungen sich widersprechen. Tabelle 13: Qualitative Analyse von Fake-News-Beispielen, bei denen Sprachmodelle mit verschiedenen politischen Neigungen sich widersprechen.</sample>
    <sample id="192">Tabelle 12: Qualitative Analyse von Hassredebeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen umstreiten. Tabelle 13: Qualitative Analyse von Falschmeldungen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen umstreiten.</sample>
    <sample id="193">Tabelle 12: Qualitative Analyse von Hassredebeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen sich widersprechen. Tabelle 13: Qualitative Analyse von Falschnachrichtenbeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen sich widersprechen.</sample>
    <sample id="194">Tabelle 12: Qualitative Analyse von Hassredebeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen sich widersprechen. Tabelle 13: Qualitative Analyse von Falschinformationsbeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen sich widersprechen.</sample>
    <sample id="195">Tabelle 12: Qualitative Analyse von Hassredebeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen umstreiten. Tabelle 13: Qualitative Analyse von Falschinformationsbeispielen, bei denen Sprachmodelle mit unterschiedlichen politischen Neigungen umstreiten.</sample>
    <sample id="196">Diskussion

Zwischen Scylla und Charybdis

Ob man "sanitieren" oder nicht "sanitieren", das ist die Frage

Vortrainingsdaten

Sprachmodelle

Niederflusseinsätze</sample>
    <sample id="197">Diskussion

Zwischen Scylla und Charybdis

Ob man "sanitieren" oder nicht "sanitieren", das ist die Frage

Vortrainingsdaten

Sprachmodelle

Niederflusseinsätze</sample>
    <sample id="198">Diskussion zwischen Scylla und Charybdis. Ob zu "sanieren" oder nicht zu "sanieren", das ist die Frage. Vorkörnungsdaten, Sprachmodelle, Unterflussaufgaben.</sample>
    <sample id="199">Dankeschön! Vorkörnungsdaten → Sprachmodelle → Unteraufgaben. Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov. Paul G. Allen School, UW NLP, Carnegie Mellon University Language Technologies Institute, Peking University.</sample>
    <sample id="200">Sechs Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass es mich wissen!</sample>
    <sample id="201">Bis zu 900 Token.</sample>
    <sample id="202">Sie haben Musikauswahl, Buchauswahl und Rezepte ausgewählt.</sample>
    <sample id="203">Positionalität bezieht sich auf die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben. Als Forscher hat sie Einfluss auf den Forschungsprozess und seine Ergebnisse und Auswirkungen. Wenn du noch mehr darüber wissen möchtest, frag einfach nach.</sample>
    <sample id="204">Dawei Zhu. Wenn du noch mehr Fragen hast, lass sie gerne stehen.</sample>
    <sample id="205">Ja, es passt.</sample>
    <sample id="206">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="207">Ja, es funktioniert. Wenn du mehr Details wissen möchtest, frag einfach.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind Background-Pretrain, Background-Both und Background-Inference. Wenn du noch mehr über KITMUS erfahren möchtest, frag einfach weiter.</sample>
    <sample id="209">Ich weiß es nicht. Es gibt keine Angaben dazu, an welcher Universität die Autoren angehören. Du könntest versuchen, das im Text zu suchen oder mehr Informationen zu den Autoren zu finden.</sample>
    <sample id="210">Die abschließende Forschungsfrage ist: "How to use the available clean samples more efficiently?" Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="211">Die Sensitivitätsmetrik misst, wie empfindlich das Modell auf verschiedene Anweisungen für die gleiche Aufgabe ist. Es schaut auf die Fähigkeit, gleichbleibende Ergebnisse für dieselbe Aufgabe zu erzielen, unabhängig von leichten Variationen in der Wortwahl der Anweisungen. Wenn du mehr darüber wissen möchtest, lass es mich wissen!</sample>
    <sample id="212">Der Referent ist Wenjun Peng.</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet nicht eine bessere Leistung des Modells. Lower is better. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="214">Leider ist die Antwort nicht im Bild zu sehen. Du könntest versuchen, den Text des Bildes zu kopieren und zu lesen, oder die Frage an eine andere Quelle zu stellen.</sample>
    <sample id="215">Normalerweise werden für eine gute Leistung an der WSL etwa 50 saubere Validierungsbeispiele benötigt.</sample>
    <sample id="216">Die Autoren gehören der Stanford University an.</sample>
    <sample id="217">Weil die alten Methoden möglicherweise nicht mehr genügen, um die komplexe und dynamische Natur der Medienverzerrungen zu erfassen.</sample>
    <sample id="218">Akshatha Arodi. Wenn du noch Fragen hast, lass sie gerne da.</sample>
    <sample id="219">Nicht klar genug, um eine genaue Antwort zu geben. Es fehlt eine detaillierte Beschreibung der Pipeline. Kannst du mehr dazu sagen?</sample>
    <sample id="220">Ja, es gibt Unterschiede. Bei DEplain-apa ist Lexikal Substitution am häufigsten, bei DEplain-web ist es Word Addition.</sample>
    <sample id="221">Nein, Coscript ist nicht öffentlich verfügbar. Wenn du mehr über Coscript erfahren möchtest, frag einfach weiter.</sample>
    <sample id="222">The watermark is embedded by defining a target embedding, counting the trigger number in a sentence, and then adding the target embedding on the original embedding.</sample>
    <sample id="223">Die Autoren gehören der PennState University an.</sample>
    <sample id="224">Ja, sie können verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist die Anleitung zur Herstellung von Kuchen, wie zum Beispiel einem Erdbeerkuchen oder einem Schokoladenkuchen.</sample>
    <sample id="226">I'm not sure about the specific method they use to ensure the opacity. You might need to look into the details of the research paper or presentation for that. If you have any other questions about the content, feel free to ask.</sample>
    <sample id="227">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen, indem sie entweder von Scratch mit voller Modellkonstruktion oder durch kontinuierliche Prämittierung mit einem bestehenden vortrainierten Modell, wie CamemBERT oder PubMedBERT, vorgeht.</sample>
    <sample id="228">GPT-4 ist am wenigsten auf Lateinamerika ausgerichtet. Wenn du noch mehr Fragen hast, lass sie gerne stehen.</sample>
    <sample id="229">"I am a student."</sample>
    <sample id="230">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Je mehr Aufgaben, desto höher die Leistung.</sample>
    <sample id="231">LSTM seq2seq, T5 und Zheng and Lapata. Wenn du noch Fragen hast, lass sie gerne da.</sample>
    <sample id="232">Ich kann das aus dem Bild nicht direkt ableiten. Du könntest versuchen, die Autoren aufzusuchen und nach ihrer Zusammenarbeit zu fragen. Oder du suchst es im Originaltext nach. Wie wäre es, wenn du mir noch mehr Informationen gibst?</sample>
    <sample id="233">Chowdery.</sample>
    <sample id="234">NLPositionality: Charakterisierung von Design-Biasen in Datensätzen und Modellen. Sebastian Santy, University of Washington. Jenny T. Liang, Carnegie Mellon University. Ronan Le Bras, Allen Institute for AI. Katharina Reinecke, University of Washington. Maarten Sap, Carnegie Mellon University.</sample>
    <sample id="235">NLPositionality: Charakterisierung von Design-Voreingenommenheiten von Datensätzen und Modellen. Sebastian Santy, University of Washington. Jenny T. Liang, Carnegie Mellon University. Ronan Le Bras, Allen Institute for AI. Katharina Reinecke, University of Washington. Maarten Sap, Carnegie Mellon University.</sample>
    <sample id="236">Denke mal...</sample>
    <sample id="237">Denk dran...</sample>
    <sample id="238">Denk dran... Carl Jones, Tech Lead, New York Times: Kannst du aufhören, ein Arschloch zu sein? Aditya Sharma, Tech Lead, Times of India: Presstitutinnen überall in der Nachrichten.</sample>
    <sample id="239">Stell dir vor... Design Bias Beispiel! Kannst du aufhören, ein Arsch zu sein? : 0.82 ✅ Presstituts überall in der Nachrichten. : 0.33 ❌ Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India = PerspectiveAPI-Score</sample>
    <sample id="240">Positionalität "Die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben."</sample>
    <sample id="241">Positionalität "Die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben."</sample>
    <sample id="242">Positionalität "Die Perspektiven, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben." "Als Forscher, beeinflusst es den Forschungsprozess und seine Ergebnisse und Auswirkungen." [1] Savin-Baden, Maggi und Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Qualitative Research: The Essential Guide to Theory and Practice. Routledge, 2013.</sample>
    <sample id="243">Können Datensätze und Modelle Positionalität haben?</sample>
    <sample id="244">Können Datensätze und Modelle Positionalität haben?</sample>
    <sample id="245">Sind Datensätze und Modelle positionell?Anecdotal Beweise: - Modell- und Datensatzprobing, [1][2] - Theoretische Definitionen der Modellpositionellität, [3] [1] Blasi, et al. "Systematische Ungleichheiten in der Leistung der Sprachtechnologie in den Weltsprachen." ACL 2022. [2] Yin et al. "GEOMLAMA: Geo-Diverses Commonsense Probing auf Multilingualen Vortrainierten Sprachmodellen." EMNLP 2022. [3] Cambo &amp; Gergle. "Modellpositionellität und Computationsreflexivität: Förderung der Reflexivität in Datenwissenschaft." CHI 2022.</sample>
    <sample id="246">Können Datensätze und Modelle Positionalität haben?Anecdotaler Beweis:.- Modell- und Datensatzprobing, [1][2]- Theoretische Definitionen der Modellpositionalität, [3][1] Blasi, et al. "Systematische Ungleichheiten in der Leistung der Sprachtechnologie weltweit." ACL 2022.[2] Yin et al. "GEOMLAMA: Geo-Diverses Commonsense Probing auf Multilingualen Prädikten." EMNLP 2022.[3] Cambo &amp; Gergle. "Modellpositionalität und computergestützte Reflexivität: Förderung der Reflexivität in der Datenwissenschaft." CHI 2022.</sample>
    <sample id="247">Sind Datensätze und Modelle positionell?Anecdotaler Beweis:.- Modell- und Datensatzprobing, 1, 2.- Theoretische Definitionen der Modellpositionellität, 3.1. Blasi, et al. "Systematische Ungleichheiten in der Leistung der Sprachtechnologie weltweit." ACL 2022.2. Yin et al. "GEOMLAMA: Geo-Diverses Commonsense Probing auf Multilingualen Prädikten." EMNLP 2022.3. Cambo &amp; Gergle. "Modellpositionellität und computergestützte Reflexivität: Förderung der Reflexivität in der Datenwissenschaft." CHI 2022.</sample>
    <sample id="248">Sind Datensätze und Modelle positionell?Anecdotaler Beweis:.- Modell- und Datensatzprobing, 1, 2.- Theoretische Definitionen der Modellpositionellität, 3.1. Blasi, et al. "Systematische Ungleichheiten in der Leistung der Sprachtechnologie weltweit." ACL 2022.2. Yin et al. "GEOMLAMA: Geo-Diverses Commonsense Probing auf multilingualen Prädikten." EMNLP 2022.3. Cambo &amp; Gergle. "Modellpositionellität und computergestützte Reflexivität: Förderung der Reflexivität in der Datenwissenschaft." CHI 2022.</sample>
    <sample id="249">Frage: Haben Datensätze und Modelle Positionalität? Ziel: Vergleiche Anmerkungen von Nutzern mit bestehenden Datensätzen und Modellen.</sample>
    <sample id="250">NLPositionality ist ein Rahmenwerk zur Charakterisierung von Design-Biass in NLP-Datensätzen und -Modellen.</sample>
    <sample id="251">Kollektion: 300 Instanzen werden aus einem Datensatz abgegriffen. Jede Instanz hat einen zugeordneten Goldstandard-Etikett. Verarbeitung: Die erhaltenen Annotierungen werden mit den Goldstandard-Etiketten aus dem Datensatz und den Vorhersagen der Modelle verglichen. Analyse: Der Pearson's r-Korrelationskoeffizient wird zwischen den Goldstandard-Etiketten, den Modellvorhersagen und den Annotierungen für jede Demografie separat gemessen.</sample>
    <sample id="252">Kollektion: 300 Instanzen werden aus einem Datensatz abgegriffen. Jede Instanz hat einen zugeordneten Goldstandard-Etikett. Verarbeitung: Instanzen werden als Teil eines LITW-Studiums gesendet. Annotations von Menschen aus der ganzen Welt erhalten. Analysen: Die erhaltenen Annotations werden gegen die Goldstandard-Etiketten aus dem Datensatz und die Modellempfindungen verglichen. Der Pearson's r-Korrelationswert wird zwischen den Goldstandard-Etiketten, Modellempfindungen und Annotations für jede Demografie separat gemessen.</sample>
    <sample id="253">Kollektion: 300 Instanzen werden aus einem Datensatz abgegriffen. Jede Instanz hat einen Goldstandard-Etikett. Verarbeitung: Instanzen werden als Teil eines LITW-Studiums gesendet. Annotations von Menschen auf der ganzen Welt. Analyse: Die erhaltenen Annotations werden gegen die Goldstandards aus dem Datensatz und die Modellempfindungen verglichen. Pearson's r-Korrelation wird zwischen Goldstandards, Modellempfindungen und Annotations für jede Demografie getrennt gemessen.</sample>
    <sample id="254">Kollektion: 300 Instanzen werden aus einem Datensatz abgegriffen. Jede Instanz hat einen zugeordneten Goldstandard-Etikett. Verarbeitung: Instanzen werden als Teil eines LITW-Studiums gesendet. Anmerkungen von Menschen aus der ganzen Welt werden erhalten. Analyse: Die erhaltenen Anmerkungen werden gegen die Goldstandard-Etiketten aus dem Datensatz und die Modellempfindungen verglichen. Der Pearson's r-Korrelationskoeffizient wird zwischen den Goldstandard-Etiketten, den Modellempfindungen und den Anmerkungen für jede Demografie separat gemessen.</sample>
    <sample id="255">Kollektion: 300 Instanzen werden aus einem Datensatz abgegriffen. Jede Instanz hat einen Goldstandard-Etikett. Verarbeitung: Die erhaltenen Annotierungen werden mit den Goldstandards aus dem Datensatz und den Modellvorhersagen verglichen. Analyse: Der Pearson's r-Korrelationswert wird zwischen den Goldstandards, Modellvorhersagen und Annotierungen für jede Demografik separat gemessen.</sample>
    <sample id="256">Kollektion: 300 Instanzen werden aus einem Datensatz gesammelt. Jede Instanz hat einen Goldstandard-Etikett. Verarbeitung: Die erhaltenen Annotierungen werden mit den Goldstandards aus dem Datensatz und den Modellvorhersagen verglichen. Analyse: Der Pearson's r-Korrelationswert wird zwischen den Goldstandards, Modellvorhersagen und Annotierungen für jede Demografik separat gemessen.</sample>
    <sample id="257">Können Sie mit einem AI und seinen Moralvorstellungen leben? Erzählen Sie uns Ihre moralischen Urteile zu bestimmten Situationen und wir zeigen Ihnen, wie Sie sich mit anderen und einem AI vergleichen. Sind Sie besser als ein AI in der Erkennung von hämischer Sprache? Beurteilen Sie, was für hämische Sprache gilt und wir zeigen Ihnen, wie gut Sie hämische Sprache erkennen können, verglichen mit einem AI und anderen. Wo stehen Sie auf der Skala von Technikverweigerer bis Technikutopier? Erzählen Sie uns, wie Sie denkend über die Zukunft von gemischter Realitätstechnologie Ihre persönliche Lebenswelt beeinflussen werden, und wir zeigen Ihnen, ob Sie eher Technikverweigerer oder Technikutopier sind.</sample>
    <sample id="258">LabintheWild ist eine Plattform, die eine große Gruppe von Teilnehmern anbietet. Es gibt verschiedene Online-Experimente, die von Forschern durchgeführt werden. Diese Experimente beinhalten Fragen wie, ob man mit einem AI und seinen moralischen Überzeugungen leben könnte, und ob man besser ist als ein AI in der Erkennung von hämischen Sprache. Es gibt auch eine Frage, wo man auf der Skala von technikvertraut bis technikoptimistisch steht.</sample>
    <sample id="259">Aufgabe A: Soziale Akzeptanz. 1) Lese die Situation. 1/15. Wollen, viel Geld zu verdienen. 2) Gib an, was du darüber denkst. Was meinst du davon aus moralischer Sicht? Sehr schlecht, schlecht, erwartbar/okay, gut, sehr gut. (z.B. okay, du solltest, es ist ungewöhnlich). Es ist im Allgemeinen gut, aber du willst andere nicht im Prozess ausnutzen. 3) Sehe, was ein AI und andere darüber gedacht haben! Der AI spekuliert: Ich vermute, es ist verständlich. Hm… Vielleicht habe ich mich in diesem Punkt geirrt. Studienanteilnehmer in den USA sagten: Soziale Akzeptanz der Situation bewerten. Vergleiche deine Antworten mit denen anderer und eines AI.</sample>
    <sample id="260">Aufgabe A: Soziale Akzeptanz. 1) Lese die Situation. 1 von 15. Wollen, viel Geld zu verdienen. * bezeichnet ein Pflichtfeld. 2) Gib an, was du darüber denkst. Was meinst du dazu aus moralischer Sicht? Sehr schlecht, schlecht, erwartbar / okay, gut, sehr gut. (z.B. okay, du solltest, es ist ungewöhnlich). Es ist im Allgemeinen gut, aber du willst andere nicht in den Prozess einbeziehen. 3) Sehe, was ein AI und andere darüber dachten! Der AI spekuliert: Ich vermute, es ist verständlich. Hm... Vielleicht bin ich hier falsch. Studienteilnehmer in den USA sagten: Teilnehmer vergleichen ihre Antworten mit denen anderer und eines AI.</sample>
    <sample id="261">Aufgabe A: Soziale Akzeptanz Analyse Datensätze - Sozial Chemie Modelle - Delphi - GPT-4</sample>
    <sample id="262">Aufgabe B: Toxizität

1) Lesen Sie das Beispiel: 7 von 25. "Engländer riechen nach Sauermilch."

2) Geben Sie Ihre Meinung dazu ein. "Denken Sie, dass dies Hassrede ist?" Ja, Nein, Unsicher. "Gern eine kurze Beschreibung eingeben." "Absenden"

3) Sehen Sie, was andere darüber dachten! Der AI spekuliert: "Ich vermute, dass dies Hassrede ist. Es sieht so aus, als stimmten wir überein!" Studienteilnehmer in Afghanistan sagten: "Keine Hassrede" 60% "Nicht sicher" 20% "Hassrede" 40%</sample>
    <sample id="263">Aufgabe B: Toxizität Analyse Datensätze - Dynahate Modelle - Perspective API - Rewire API - Hate RoBERTa - GPT-4 Studienteilnahme 16.299 Annotationen 1.096 Annotatoren 87 Länder</sample>
    <sample id="264">Ergebnis: Wer sind in den NLP-Datensätzen und Modellen enthalten? Ergebnis 1: Es gibt Positionalität in der NLP.</sample>
    <sample id="265">Die Grafik zeigt die soziale Akzeptanz von Datensätzen und Modellen in verschiedenen Regionen. Die englischsprachigen Länder wie die Britische Inseln, die katholische Europa und die konservativen europäischen Länder haben die höchste soziale Akzeptanz. Die lateinamerikanischen Länder haben die niedrigste soziale Akzeptanz. Die Grafik weist auch auf die Häufigkeit von Hassrede und Toxizität in den Datensätzen hin, wobei die englischsprachigen Länder wiederum die höchste Häufigkeit haben.</sample>
    <sample id="266">Datenmengen und Modelle sind am meisten mit Menschen mit einem College-Abschluss ausgerichtet.</sample>
    <sample id="267">Datenmengen und Modelle sind am besten mit Menschen mit einem College-Abschluss ausgerichtet.</sample>
    <sample id="268">Ergebnis 2: Einige Bevölkerungsgruppen werden zurückgelassen.</sample>
    <sample id="269">Die Datensätze und Modelle sind weniger mit nichtbinären Menschen ausgerichtet.</sample>
    <sample id="270">Also, was können wir tun? Positionalität in der NLP angehen.</sample>
    <sample id="271">1. Halten Sie eine Aufzeichnung aller relevanten Designentscheidungen, die bei der Erstellung von Datensätzen oder Modellen getroffen wurden. 2. Führen Sie NLP-Forschung aus, unter Verwendung des Perspektivismus als Ansicht: a. Teilen Sie aufgetrennte Datensatzbeschriftungen! b. Verwenden Sie Modellierungstechniken, die Annotierungsunterschiede bewältigen können.</sample>
    <sample id="272">1. Halten Sie eine Aufzeichnung aller relevanten Designentscheidungen bei der Erstellung von Datensätzen oder Modellen. 2. Führen Sie NLP-Forschung aus der Perspektive der Perspektivismen: a. Teilen Sie aufgetrennte Datensatzbeschriftungen! b. Verwenden Sie Modellierungstechniken, die Annotatordisziplin handhaben können. 3. Das Erstellen spezialisierter Datensätze und Modelle für bestimmte Communities ist wertvoll für inklusives NLP, z.B. die Masakhane-Initiative.</sample>
    <sample id="273">Vielen Dank! Dashboard Link: nlpositionality.cs.washington.edu/ Papier: bit.ly/NLPositionality-Paper/</sample>
    <sample id="274">Die Referentin geht auf drei Probleme von SimulST ein.</sample>
    <sample id="275">Naja, das ist eine komplexe Frage. Einerseits könnte man versuchen, die Daten zu sammeln, die möglichst vielfältig und repräsentativ sind. Andererseits könnte man die Daten manuell überprüfen und eventuell korrigieren oder entfernen. Aber es gibt auch Methoden wie das Anwenden von Bias-Reduktionstechniken. Es ist aber immer wiederum ein schwieriges Gleichgewicht zu finden. Wenn du mehr dazu wissen möchtest, kannst du mir noch Fragen stellen.</sample>
    <sample id="276">Die 61. Jahrestagung der Association for Computational Linguistics, Toronto, Kanada, vom 9. bis 14. Juli 2023. "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" von Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao und Deqing Yang.</sample>
    <sample id="277">Sprachliche Planung. Wie man Kuchen backt? 1. Sammel deine Zutaten. 2. Heiz den Ofen auf 325 °F, 163 °C, vor und fett und bestreiche einen Kuchentopf. 3. Schlag die Butter und Zucker. 4. Füge die Eier hinzu. 5. Rühre den Mehlkuchen. 6. Gieße die Teigmasse in den Topf. 7. Backe den Kuchen für 1 Stunde 15 Minuten. Große Sprachmodelle, LLMs, können Ziele effektiv in Schritte zerlegen.</sample>
    <sample id="278">Sprachliche Planung. Wie man Kuchen backt? 1. Sammel deine Zutaten. 2. Heiz den Ofen auf 325 °F, 163 °C, vor und fett und bestreiche einen Kuchentopf. 3. Schlag die Butter und Zucker. 4. Füge die Eier hinzu. 5. Rühre die Mehlzusammensetzung ein. 6. Gieße die Teigmasse in den Topf. 7. Backe den Kuchen für 1 Stunde 15 Minuten. Große Sprachmodelle, LLMs, können Ziele effektiv in Schritte zerlegen.</sample>
    <sample id="279">Gefragt wurde, den englischen Inhalt sinngemäß auf Deutsch wiederzugeben.</sample>
    <sample id="280">Geschränkte Sprachplanung. Wie man einen Erdbeerkuchen macht? ...Erdbeer-Marmelade in den Mehl... Wie man einen Schokoladenkuchen macht? ...Kakaopulver in den Mehl... Abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit mehreren Facetten von Einschränkungen übernommen werden.</sample>
    <sample id="281">Gefragt ist, den englischen Inhalt sinngemäß auf Deutsch zu übersetzen.</sample>
    <sample id="282">Wie performen LLMs bei beschränkter Sprachplanung? Datensatz: wikiHow + generierte Beschränkungen. Beschränkungstyp 1: Modifikator. Definition: Ein Wort, ein Adjektiv oder ein Satz, das einen abstrakten Zielmodifikator oder -beschränkung ist. Beispiele: Machen Sie eine Schokoladenkuchen. Machen Sie eine rosa Kuchen. Beschränkungstyp 2: Methode. Definition: Ein Werkzeug oder ein spezifischer Modus, der den Prozess für das Erreichen des Ziels steuert. Beispiele: Machen Sie einen Kuchen mit einem Ofen. Machen Sie einen Kuchen mit Kuchenteig. Beschränkungstyp 3: Intention. Definition: Eine zusätzliche Absicht oder Nachfrage bei der Erreichung des Ziels. Beispiele: Machen Sie einen Kuchen für die Hochzeit. Machen Sie einen Kuchen für Diabetiker.</sample>
    <sample id="283">Wie performen LLMs bei beschränkter Sprachplanung? Datensatz: wikiHow + generierte Beschränkungen. Beschränkungstyp 1: Modifikator Definition: Ein Wort, ein Adjektiv oder ein Phrase, das einen abstrakten Zielmodifikator ist. Beispiele: Machen Sie eine Schokoladenkuchen. Machen Sie eine rosa Kuchen. Beschränkungstyp 2: Methode Definition: Ein Werkzeug oder ein spezifischer Modus, der den Prozess für das Erreichen des Ziels steuert. Beispiele: Machen Sie einen Kuchen mit einem Ofen. Machen Sie einen Kuchen mit einem Kuchenteig. Beschränkungstyp 3: Absicht Definition: Eine zusätzliche Absicht oder Nachfrage bei der Erreichung des Ziels. Beispiele: Machen Sie einen Kuchen für die Hochzeit. Machen Sie einen Kuchen für Diabetiker.</sample>
    <sample id="284">Wie performen LLMs bei beschränkter Sprachplanung? Datensatz: wikiHow + generierte Beschränkungen. Beschränkungstyp 1: Modifikator. Definition: Ein Wort, ein Adjektiv oder ein Phrase, das einen abstrakten Zielmodifikator oder -beschränkung ist. Beispiele: Machen Sie eine Schokoladenkuchen, Machen Sie eine rosa Kuchen. Beschränkungstyp 2: Methode. Definition: Ein Werkzeug oder ein spezifischer Modus, der den Prozess für das Erreichen des Ziels steuert. Beispiele: Machen Sie einen Kuchen mit einem Ofen, Machen Sie einen Kuchen mit Kuchenteig. Beschränkungstyp 3: Intention. Definition: Eine zusätzliche Absicht oder Nachfrage bei der Erreichung des Ziels. Beispiele: Machen Sie einen Kuchen für die Hochzeit, Machen Sie einen Kuchen für Diabetiker.</sample>
    <sample id="285">Können LLMs konstruierte Sprachplanung durchführen? Alle Baseline-Modelle erzielen unzufriedenstellende Ergebnisse bei der Planung für spezifische Ziele.</sample>
    <sample id="286">Können LLMs gebundenes Sprachplanung machen? Alle Baselines erzielen unzufriedenstellende Ergebnisse bei der Planung für spezifische Ziele.</sample>
    <sample id="287">Welche Art von Fehlern machen LLMs in dieser Aufgabe normalerweise?Die semantische Vollständigkeit, SE, in generierten Skripten ist akzeptabel, aber die Treue zur Beschränkung, FE, kann nicht garantiert werden.</sample>
    <sample id="288">Welche Art von Fehlern machen LLMs in dieser Aufgabe normalerweise?Die semantische Vollständigkeit, SE, in generierten Skripten ist akzeptabel, aber die Treue zur Beschränkung, FE, kann nicht garantiert werden.</sample>
    <sample id="289">Was für Ziele versagen InstructGPT normalerweise?Die Planungsleistung von InstructGPT variiert erheblich für Ziele unterschiedlicher Kategorien.</sample>
    <sample id="290">Method: Eingabe: ein abstraktes Ziel Schritt 1: Spezifische Ziele mit InstructGPT über in-kontext-Lernen generieren Abstraktes Ziel: Kuchen backen + Beschränkungen: G1, Modifikator: Schokoladenkuchen backen G2, Methode: Kuchen in einem Mikrowellenherd backen G3, Absicht: Kuchen für eine Hochzeit backen</sample>
    <sample id="291">Method: Eingabe: ein abstraktes Ziel Schritt 1: Spezifische Ziele mit InstructGPT über in-kontext-Lernen generieren Abstraktes Ziel: Kuchen backen Spezifische Ziele: G1, Modifikator: Schokoladenkuchen backen G2, Methode: Kuchen im Mikrowellen backen G3, Ziel: Kuchen für eine Hochzeit backen Schritt 2: Kandidatenskripte mit InstructGPT über in-kontext-Lernen überschreiben</sample>
    <sample id="292">Method: Eingabe: ein abstraktes Ziel Schritt 1: Spezifische Ziele mit InstructGPT über in-kontext-Lernen generieren Schritt 2: Kandidatenskripte mit InstructGPT über in-kontext-Lernen überschreiben Abstraktes Ziel: Kuchen backen Spezifische Ziele: G1, Modifikation: Schokoladenkuchen backen G2, Methode: Kuchen im Mikrowellen backen G3, Absicht: Kuchen für eine Hochzeit backen Kandidatenskripte: 1, 2, 3, ... k</sample>
    <sample id="293">Verfahren Schritt 2: Übergenerieren von Kandidat-Skripten mit InstructGPT mittels in-kontextuelles Lernen Schritt 3: Finden von gefilterten Skripten zum Ziel mit InstructGPT mittels Ähnlichkeitsbewertung Ausgabe: Spezifische Ziele mit entsprechenden Skripten</sample>
    <sample id="294">Verfahren Schritt 2: Kandidatenskripte mit InstructGPT durch In-Context-Learning generieren Schritt 3: Filterte Skripte zum Ziel mit InstructGPT über Ähnlichkeitswerte finden Ausgabe: Spezifische Ziele mit entsprechenden Skripten Kandidatenskripte: 1, 2, 3, ... k Filterte Skripte: Skript 3 1. Deine Zutaten sammeln 4. Füge die ... Kakao-Pulver hinzu</sample>
    <sample id="295">Verfahren Schritt 2: Kandidatenskripte mit InstructGPT über in-kontext-Lernen generieren Schritt 3: Filterte Skripte zum Ziel mit InstructGPT über Ähnlichkeitsbewertung finden Ausgabe: Spezifische Ziele mit entsprechenden Skripten</sample>
    <sample id="296">Unser Verfahren verbessert die Planungsqualität erheblich. Mit unserem Verfahren kann InstructGPT Skripte von höherer Qualität generieren.</sample>
    <sample id="297">Motivation: Um eine beschränkte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen. Methode: Folgt dem Prinzip der symbolischen Wissensverdichtung. Generiert 55.000 Skripte mit Bedingungen aus LLMs aufgrund unseres Verfahrens =&gt; Coscript-Datensatz. Menschen annotieren Validierungs- und Testset.</sample>
    <sample id="298">Motivation: Um eine beschränkte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen. Methode: Folgt dem Prinzip der symbolischen Wissensverdichtung. 55.000 Skripte mit Bedingungen wurden von LLMs unter Verwendung unseres Verfahrens generiert =&gt; Coscript-Datensatz. Menschen annotieren Validierungs- und Testset.</sample>
    <sample id="299">Motivation: Um für kleinere Modelle die Fähigkeit zur beschränkten Sprachplanung zu ermöglichen. Methode: Folgt dem Prinzip der symbolischen Wissensverdichtung. 55.000 Skripte mit Beschränkung wurden von LLMs unter Verwendung unseres Verfahrens generiert =&gt; Coscript-Datensatz. Menschen annotieren Validierungs- und Testset.</sample>
    <sample id="300">Motivation: Um für kleinere Modelle die Fähigkeit zur beschränkten Sprachplanung zu ermöglichen. Methode: Folgt dem Prinzip der symbolischen Wissensverdichtung. 55.000 Skripte mit Bedingungen wurden von LLMs unter Verwendung unseres Verfahrens generiert =&gt; Coscript-Datensatz. Menschen annotieren die Validierung und Testdatensätze.</sample>
    <sample id="301">Motivation: Um eine beschränkte Sprachplanungsfähigkeit für kleinere Modelle zu ermöglichen. Methode: Folgt dem Prinzip der symbolischen Wissensverdichtung. Generiert 55.000 Skripte mit Beschränkung aus LLMs aufgrund unseres Verfahrens =&gt; Coscript-Datensatz. Menschen annotieren Validierungs- und Testset.</sample>
    <sample id="302">Kontextanalyse. Coscript zeigt eine hohe Heterogenität und Pluralität bei den generierten spezifischen Zielen.</sample>
    <sample id="303">Spezialisierte Modelle vs. LLMs. Kleinerer LLMs, die auf Coscript feinabgestimmt wurden, können höherwertige Skripte generieren als LLMs.</sample>
    <sample id="304">Zusammenfassung und Schlussfolgerungen: - Stellen Sie das Problem der eingeschränkten Sprachplanung fest. - Beurteilen Sie die Fähigkeit von LLMs zur eingeschränkten Sprachplanung und entwickeln Sie eine Methode zur Übererzeugung und Filterung für LLMs. - Verwenden Sie LLMs, um einen hochwertigen Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung zu generieren. - Grenzen und Zukunftsaufgaben - Die vorgeschlagene Methode zur Verbesserung von LLMs ist ein post-hoc-Ranking-Ansatz. - CoScript erbt nur von einem abstrakten mit einer zusätzlichen Einschränkung. - Der CoScript-Datensatz kann ein wertvolles Ressourcenmaterial für die Weiterentwicklung der Forschung zur Sprachplanung mit komplexeren und diverseren Zielen und Einschränkungen sein.</sample>
    <sample id="305">Zusammenfassung und Schlussfolgerungen
- Stellen Sie das Problem der eingeschränkten Sprachplanung fest.
- Beurteilen Sie die Fähigkeit von LLMs zur eingeschränkten Sprachplanung und entwickeln Sie eine Methode zur Übererzeugung und Filterung für LLMs.
- Verwenden Sie LLMs, um eine hochwertige Skriptdatensatz, CoScript, für die eingeschränkte Sprachplanung zu generieren.
- Grenzen und zukünftige Arbeit
  - Die vorgeschlagene Methode zur Verbesserung von LLMs ist ein post-hoc-Ranking-Ansatz.
  - CoScript erbt nur von einem abstrakten mit einer zusätzlichen Einschränkung.
  - Der CoScript-Datensatz kann ein wertvolles Ressourcen für die Weiterentwicklung der Forschung zur Sprachplanung mit komplexeren und diverseren Zielen und Einschränkungen sein.</sample>
    <sample id="306">Die 61. Jahrestagung der Association for Computational Linguistics, Toronto, Kanada, vom 9. bis 14. Juli 2023. "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang. E-Mail: syyuan21@m.fudan.edu.cn. Coscript Website: https://github.com/siyuyuan/coscript.</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA.</sample>
    <sample id="308">Die wichtigsten Eigenschaften sind: es muss auf EaaS anwendbar sein, die Nützlichkeit der bereitgestellten Embeddings darf nicht beeinträchtigt werden, es sollte dem Angreifer verdeckt sein und der Wasserzeichen muss übertragbar auf die Angreiferdienste sein. Wenn du noch Fragen hast, lass es mich wissen!</sample>
    <sample id="309">Die englischen TED Talks wurden in 14 Sprachen übersetzt: Deutsch, Spanisch, Französisch, Italienisch, Niederländisch, Portugiesisch, Rumänisch, Russisch, Türkisch, Chinesisch, Arabisch, Hebräisch, Japanisch und Koreanisch. Wenn du noch mehr über die Übersetzungen oder die TED Talks selbst erfahren möchtest, frag einfach!</sample>
    <sample id="310">300 Instanzen.</sample>
    <sample id="311">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind die Cosinus-Ähnlichkeit und die l2-Distanz.</sample>
    <sample id="312">In der Aufgabe wurden Modelle, die auf einem mehrsprachigen Encoder basieren, in zwei Gruppen eingesetzt. Eine Gruppe war Enc-PTR, das aus mehrsprachigen Prädtrainierten Encodeern mit Pointer-basierten Decoder besteht, wie XLM-R + PTR und mBERT + PTR. Die andere Gruppe war Enc-Dec, die aus mehrsprachigen Prädtrainierten Encoder-Decoder-Modellen besteht, wie mBART und mT5.</sample>
    <sample id="344">Die Autoren zählen die Worthäufigkeit auf einem allgemeinen Textkorpus Dp und wählen dann n Wörter aus einem mittelhohen Häufigkeitsintervall aus.</sample>
    <sample id="345">Können CoNLL-2003 Named Entity Tagger noch in 2023 gut funktionieren? Shuheng Liu, Alan Ritter School of Interactive Computing Georgia Institute of Technology</sample>
    <sample id="346">Nennenswerte Entitätenerkennung und Generalisierung</sample>
    <sample id="347">Nennenswerte Entitätenerkennung und Generalisierung. - Modelle haben CoNLL-2003 für fast 20 Jahre zur Entwicklung von NER verwendet. - Können diese Modelle auf moderne Daten generalisieren? - Was ist für eine gute Generalisierung notwendig?</sample>
    <sample id="348">Nennenswerte Entitätenerkennung und Generalisierung. - Modelle haben CoNLL-2003 für fast 20 Jahre verwendet, um NER zu entwickeln. - Können diese Modelle auf moderne Daten generalisieren? - Was ist für eine gute Generalisierung notwendig?</sample>
    <sample id="349">Nennenswerte Entitätenerkennung und Generalisierung.
- Modelle haben CoNLL-2003 für fast 20 Jahre zur Entwicklung von NER verwendet
- Können diese Modelle auf moderne Daten generalisieren?
- Was ist für eine gute Generalisierung notwendig?
- Was verursacht den Leistungsabfall?</sample>
    <sample id="350">CoNLL++ Dataset
- Sammelte Reuters-Nachrichten aus dem Jahr 2020 und annotierte sie nach den Anmerkungsempfehlungen von CoNLL-2003</sample>
    <sample id="351">CoNLL++-Dataset
- Sammelte Reuters-Nachrichten aus dem Jahr 2020 und annotierte sie nach den Anmerkungshinweisen von CoNLL-2003
- Über 20 Modelle wurden auf CoNLL-2003 feinabgestimmt
- Auf CoNLL-2003-Testset und CoNLL++ bewertet</sample>
    <sample id="352">CoNLL++-Dataset
- Sammelte Reuters-Nachrichten aus dem Jahr 2020 und annotierte sie nach den Anmerkungshinweisen von CoNLL-2003
- 20+ Modelle wurden auf CoNLL-2003 feinabgestimmt
- Auf CoNLL-2003-Testset und CoNLL++ bewertet
- Prozentuale ΔF1 berechnet, um die Generalisierbarkeit zu beurteilen</sample>
    <sample id="353">Was wird für eine gute Generalisierung benötigt?</sample>
    <sample id="354">Was ist für eine gute Generalisierung notwendig?• Modellarchitektur• Transformer-Modelle generalisieren besser</sample>
    <sample id="355">Was ist für eine gute Generalisierung notwendig?• Modellarchitektur• Transformer-Modelle generalisieren besser• Modellgröße• Größere Modelle generalisieren besser</sample>
    <sample id="356">Was ist für eine gute Generalisierung notwendig?• Modellarchitektur• Transformer-Modelle generalisieren besser• Modellgröße• Größere Modelle generalisieren besser• Anzahl der abzustellenden Beispiele• Mehr Beispiele führen zu besserer Generalisierung</sample>
    <sample id="357">Was verursacht eine Leistungsabnahme?</sample>
    <sample id="358">Was verursacht eine Leistungsabnahme? - Adaptive Überanpassung?</sample>
    <sample id="359">Was verursacht eine Leistungsabnahme? - Adaptive Überanpassung? - Temporale Abwanderung?</sample>
    <sample id="360">Was verursacht eine Leistungsabnahme? - Adaptive Überanpassung? - Temporale Drift?</sample>
    <sample id="361">Was verursacht eine Leistungsabnahme?• Adaptive Überanpassung?• Temporale Drift?CoNLL++ F1-ScoreCoNLL-2003 F1-Score959085807570656090.591.091.592.092.593.0949290929496949290928886Georgia Tech• Adaptive Überanpassung?• Keine Verbrauchsreduzierung• Temporale Drift?</sample>
    <sample id="362">Was verursacht eine Leistungsabnahme? - Adaptive Überanpassung? - Keine Verjüngung - Nicht beobachtet - Temporale Abweichung?</sample>
    <sample id="363">Was verursacht eine Leistungsabnahme? - Adaptive Überanpassung? - Keine Verjüngung - Nicht beobachtet - Temporale Abweichung?</sample>
    <sample id="364">Was verursacht eine Leistungsabnahme?• Adaptive Überanpassung?• Keine Verjüngung• Nicht beobachtet• Temporale Abwanderung?• Leistung verschlechtert sich mit größerem zeitlichen Abstand</sample>
    <sample id="365">Was verursacht eine Leistungsabnahme? - Adaptive Überanpassung? - Keine Verjüngung - Nicht beobachtet - Temporale Abweichung? - Leistung verschlechtert sich mit größerem zeitlichen Abstand - Hauptursache für Leistungsabnahme</sample>
    <sample id="366">Zusammenfassung
- Für eine gute Generalisierung brauchen wir:
  - Bessere Modellarchitektur
  - Größere Modellgröße
  - Mehr abgestimmte Beispiele</sample>
    <sample id="367">Zusammenfassung
- Für eine gute Generalisierung brauchen wir:
  - Bessere Modellarchitektur
  - Größere Modellgröße
  - Mehr Anpassungsbeispiele
- Leistungsschwund wird durch:
  - Temporale Abweichung
  - Keine adaptive Übertreibung verursacht</sample>
    <sample id="368">Zusammenfassung
- Für eine gute Generalisierung brauchen wir:
  - Bessere Modellarchitektur
  - Größere Modellgröße
  - Mehr abgestimmte Beispiele
- Leistungsschwund wird durch:
  - Temporale Abweichung
  - Keine adaptive Übertreibung
- Funktionieren CoNLL-2003-Tagger noch?</sample>
    <sample id="369">Zusammenfassung
- Für eine gute Generalisierung brauchen wir:
  - Bessere Modellarchitektur
  - Größere Modellgröße
  - Mehr abgestimmte Beispiele
- Leistungsschwund wird durch:
  - Temporale Abweichung
  - Keine adaptive Übertreibung
- Funktionieren CoNLL-2003-Tagger noch?
  - JA!</sample>
    <sample id="370">Papier: https://arxiv.org/abs/2212.09747 Datensatz: https://github.com/ShuhengL/acl2023_conllpp Kontakt: sliu775@gatech.edu</sample>
    <sample id="397">Leider ist die Antwort nicht im Bild zu sehen. Du könntest versuchen, den Sprecher direkt zu fragen oder den Text des Bildes zu transkribieren, um die Antwort zu finden.</sample>
    <sample id="398">Das entitätsspezifische Wissen, das im Beispiel mit Servin und Kea benötigt wird, ist, dass Servin ein Richter ist.</sample>
    <sample id="399">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="400">Die Arbeiten konzentrieren sich auf RoBERTa und GPT-2. Wenn du noch mehr Details wissen möchtest, frag einfach!</sample>
    <sample id="401">Das Modell kombiniert Werte aus mehreren Ebenen. Wenn du noch mehr Details wissen möchtest, frag einfach nach.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind "easy on me" und "the first one". Wenn du noch mehr dazu wissen möchtest, frag einfach!</sample>
    <sample id="403">Die Autoren gehören der Fudan University an.</sample>
    <sample id="404">Sechs Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="405">Ja. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="406">The authors gave the example of "a woman warrior" as a marked group.</sample>
    <sample id="407">Nicht im Bild erwähnt.</sample>
    <sample id="408">Die Testdatensätze heißen "Validation".</sample>
    <sample id="409">Sechs. Wenn du noch mehr Fragen hast, lass sie gerne stehen.</sample>
    <sample id="410">Die Autoren arbeiten mit mehreren Modalitäten. Wenn du noch mehr Details wissen möchtest, frag einfach nach.</sample>
    <sample id="439">Nichts ist in dem Bild erwähnt, was nach Ansicht der Autoren ein zu wenig erforschtes Gebiet im Bereich der NLU ist. Kannst du mir mehr dazu sagen?</sample>
    <sample id="440">The speakers are Zhiyang Xu, Ying Shen, and Lifu Huang.</sample>
    <sample id="441">Ja, Coscript wurde von Menschen annotiert, um den Validierungs- und Testdatensatz zu kontrollieren.</sample>
    <sample id="442">Nur ein kleiner Teil der Wörter hängt vom Kontext ab. Und bestehende Methoden unterstützen begrenzte Diskursphänomene und Sprachen. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="443">Löschen von indirekten Referenzierungen für Entitätsauswahl, AltEntities Corpus, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis, Google Research</sample>
    <sample id="444">Löschen von indirekten Referenzierungen für Entitätsauswahl, AltEntities Corpus, Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti und Annie Louis, Google Research</sample>
    <sample id="445">Indirekte Referenzausdrücke. Ziel: Verständnis der Sprache der Benutzer bei der Auswahl. Alternative Frage: "Haben Sie gemeint 'easy on me' oder 'I gotta feeling'?" Direkte Referenz: "easy on me", "the first one". Indirekte Referenz könnte in natürlichen und flüssigen Gesprächen verwendet werden: "Kann die Name nicht mehr erinnern", "Die Aussprachen sind schwer zu unterscheiden", "Möchte eine Präferenz angeben". Indirekte Referenz: "The newer one", "The song that's not energetic".</sample>
    <sample id="446">Indirekte Referenzausdrücke Ziel: Verständnis der Sprache der Benutzer bei der Auswahl Alternative Frage: "Haben Sie gemeint 'easy on me' oder 'I gotta feeling'?" Direkte Referenz: "easy on me", "the first one" Indirekte Referenz könnte in natürlichen und flüssigen Gesprächen verwendet werden: - Kann den Namen nicht mehr erinnern - Die Aussprachen sind schwer zu unterscheiden - Möchte eine Präferenz angeben Indirekte Referenz: "The newer one." "The song that's not energetic."</sample>
    <sample id="447">Indirekte Referenzausdrücke Ziel: Verständnis der Sprache der Benutzer bei der Auswahl Alternative Frage: "Haben Sie gemeint 'easy on me' oder 'I gotta feeling'?" Direkte Referenz: "easy on me", "the first one" Indirekte Referenz könnte in natürlichen und flüssigen Gesprächen verwendet werden: "Kann den Namen nicht mehr erinnern", "Die Aussprachen sind schwer zu unterscheiden", "Möchte eine Präferenz angeben" Indirekte Referenz: "Die neuere Version", "Das Lied, das nicht energiegeladen ist".</sample>
    <sample id="448">Indirekte Referenzausdrücke.Ziel: Verständnis der Sprache der Benutzer bei der Entscheidungsfindung.Altersfrage: "Haben Sie gemeint 'easy on me' oder 'I gotta feeling'?"Direkte Referenz: "easy on me", "the first one"Indirekte Referenz könnte in natürlichen und flüssigen Gesprächen verwendet werden:• Kann den Namen nicht mehr erinnern• Die Aussprachen sind schwer zu unterscheiden• Möchte eine Präferenz angeben.Indirekte Referenz: "The newer one. The song that's not energetic."Übersetzung des englischen Inhalts ins Deutsche:Indirekte Referenzausdrücke.Ziel: Verständnis der Sprache der Benutzer bei der Entscheidungsfindung.Altersfrage: "Haben Sie gemeint 'easy on me' oder 'I gotta feeling'?"Direkte Referenz: "easy on me", "the first one"Indirekte Referenz könnte in natürlichen und flüssigen Gesprächen verwendet werden:• Kann den Namen nicht mehr erinnern• Die Aussprachen sind schwer zu unterscheiden• Möchte eine Prä</sample>
    <sample id="449">Indirekte Referenzausdrücke Ziel: Verständnis der Sprache der Benutzer bei der Auswahl Alternative Frage: "Haben Sie gemeint 'easy on me' oder 'I gotta feeling'?" Direkte Referenz: "easy on me", "the first one" Indirekte Referenz könnte in natürlichen und flüssigen Gesprächen verwendet werden: "Kann den Namen nicht mehr erinnern", "Die Aussprachen sind schwer zu unterscheiden", "Möchte eine Präferenz angeben" Indirekte Referenz: "The newer one.", "The song that's not energetic."</sample>
    <sample id="450">Datenbankensammlung. Wichtige Probleme: Konversationsysteme, Leistungsbewertung großer Sprachmodelle bei der Verständnis von Entitäten. Keine große öffentliche Datensammlung verfügbar. Wir sammeln eine große Datensammlung mittels Crowd Annotation. Drei Domänen:</sample>
    <sample id="451">Datenkollektion. Wichtige Probleme: Konversationsysteme, Leistungsbewertung großer Sprachmodelle bei der Verständnis von Entitäten. Kein großes öffentliches Datensatz verfügbar. Wir sammeln einen großen Datensatz mit Crowd Annotation. Drei Domänen:</sample>
    <sample id="452">Methodologie betont Informalität durch eine Cartoon-Fertigstellungsaufgabe.</sample>
    <sample id="453">Methodologie betont Informalität durch eine Cartoon-Fertigstellungsaufgabe.</sample>
    <sample id="454">Methodologie betont Informalität durch eine Cartoon-Fertigstellungsaufgabe.</sample>
    <sample id="455">Methodologie betont Informalität durch eine Cartoon-Fertigstellungsaufgabe.</sample>
    <sample id="456">Methodologie betont Informalität durch eine Cartoon-Fertigstellungsaufgabe.</sample>
    <sample id="457">Methodologie betont Informalität durch eine Cartoon-Fertigstellungsaufgabe.</sample>
    <sample id="458">Generieren Sie alternative Fragen =&gt; Stichproben von Entity-Paaren. "Bedeint das A oder B?" - Gegenstände mit ähnlichen Infoboxen auf Wikipedia, gleicher Genres und/oder Künstler. "Bedeint das 'This is It' oder 'Man in the Mirror'?" - Gegenstände mit ähnlichen Beschreibungen auf Wikipedia. "Bedeint das 'Thinking of You' oder 'Happy Anywhere'?" - Gegenstände mit ähnlichen Titeln. "Bedeint das 'The Return' als Memoir oder 'The Return' als Shatner-Novel?" - Zufällig gleichmäßig. "Bedeint das 'You Could Be Mine' oder 'The Way I Am'?" - Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="459">Generieren Sie alternative Fragen =&gt; Stichproben von Entity-Paaren. "Bedeuten Sie A oder B?" - Gegenstände mit ähnlichen Infoboxen auf Wikipedia, gleicher Genres und/oder Künstler. "Bedeuten Sie 'This is It' oder 'Man in the Mirror'?" - Gegenstände mit ähnlichen Beschreibungen auf Wikipedia. "Bedeuten Sie 'Thinking of You' oder 'Happy Anywhere'?" - Gegenstände mit ähnlichen Titeln. "Bedeuten Sie 'The Return' als Memoir oder 'The Return' als Shatner-Novel?" - Zufällig gleichmäßig. "Bedeuten Sie 'You Could Be Mine' oder 'The Way I Am'?"</sample>
    <sample id="460">Generieren Sie alternative Fragen =&gt; Stichproben von Entitätenpaaren. "Bedeint das A oder B?" - Gegenstände mit ähnlichen Infoboxen auf Wikipedia, gleicher Genres und/oder Künstler. "Bedeint das 'This Is It' oder 'Man in the Mirror'?" - Gegenstände mit ähnlichen Beschreibungen auf Wikipedia. "Bedeint das 'Thinking of You' oder 'Happy Anywhere'?" - Gegenstände mit ähnlichen Titeln. "Bedeint das 'The Return' als Memoir oder 'The Return' als Shatner-Novelle?" - Zufällig gleichmäßig. "Bedeint das 'You Could Be Mine' oder 'The Way I Am'?" - Resolving Indirect Referring Expressions für Entitätenauswahl, AltEntities Corpus.</sample>
    <sample id="461">Generieren Sie alternative Fragen =&gt; Stichproben von Entity-Paaren. "Bedeint das A oder B?" - Gegenstände mit ähnlichen Infoboxen auf Wikipedia, gleicher Genres und/oder Künstler. "Bedeint das 'This Is It' oder 'Man in the Mirror'?" - Gegenstände mit ähnlichen Beschreibungen auf Wikipedia. "Bedeint das 'Thinking of You' oder 'Happy Anywhere'?" - Gegenstände mit ähnlichen Titeln. "Bedeint das 'The Return' als Memoir oder 'The Return' als Shatner-Novel?" - Zufällig gleichmäßig. "Bedeint das 'You Could Be Mine' oder 'The Way I Am'?" - Resolving Indirect Referring Expressions for Entity Selection, AltEntities Corpus.</sample>
    <sample id="462">Generieren Sie alternative Fragen =&gt; Stichproben von Entity-Paaren. "Bedeint das A oder B?" - Gegenstände mit ähnlichen Infoboxen auf Wikipedia, gleicher Genres und/oder Künstler. "Bedeint das 'This Is It' oder 'Man in the Mirror'?" - Gegenstände mit ähnlichen Beschreibungen auf Wikipedia. "Bedeint das 'Thinking of You' oder 'Happy Anywhere'?" - Gegenstände mit ähnlichen Titeln. "Bedeint das 'The Return' (Memoir) oder 'The Return' (Shatner Roman)?" - Zufällig gleichmäßig. "Bedeint das 'You Could Be Mine' oder 'The Way I Am'?" - Auflösung indirekter Referenzierungen für Entity-Auswahl, AltEntities Corpus.</sample>
    <sample id="463">Hintergrundwissen, Musik, Google-Suchlink zu jedem Lied. Easy on Me, Adele, Klicke hier, um mehr über das Lied zu erfahren. I Gotta Feeling, The Black Eyed Peas, Klicke hier, um mehr über das Lied zu erfahren. Wir bitten die Annotatoren, zumindest einige Lieder anzuhören und über jedes Lied zu lesen.</sample>
    <sample id="464">Hintergrundwissen, Musik, Google-Suchverbindungen zu jedem Lied. Easy on Me, Adele, I Gotta Feeling, The Black Eyed Peas, Klicken Sie hier, um mehr über das Lied zu erfahren. Wir bitten die Annotatoren, zumindest einige Lieder anzuhören und über jedes Lied zu lesen.</sample>
    <sample id="465">Hintergrundwissen, Musik, Google-Suchlink zu jedem Lied. Easy on Me, Adele, Klicke hier, um mehr über das Lied zu erfahren. I Gotta Feeling, The Black Eyed Peas, Klicke hier, um mehr über das Lied zu erfahren. Wir bitten die Annotatoren, zumindest einige Lieder anzuhören und über jedes Lied zu lesen.</sample>
    <sample id="466">Hintergrundwissen (Rezepte) Simnelkuchen ist ein Früchtekuchen, der im Vereinigten Königreich, in Irland und anderen Ländern mit Migrationsmustern verbreitet ist, die mit dem Fasten und Ostern in Verbindung stehen. Er zeichnet sich durch Schichten aus Mandelmasse oder Marzipan und eine Reihe von elf Kugeln aus, die aus derselben Paste hergestellt sind.Pandan-Kuchen ist ein leichter, fluffiger, grün gefärbter Biskuitkuchen, der mit den Saften der Pandanus amaryllifolius Blätter aromatisiert ist. Der Kuchen ist in Indonesien, Malaysia und auch in den Niederlanden beliebt, insbesondere unter der Indo-Gemeinschaft.</sample>
    <sample id="467">Wir sagen den Annotatoren dann, welche Wahl ausgewählt werden sollte, und bitten sie, es zu beschreiben. Wählen Sie diese aus. Easy on Me, von Adele, I Gotta Feeling, von den Black Eyed Peas, Wir möchten, dass Sie 3 bis 5 Ausdrücke für das gewählte Lied in Ihre Sprachblase geben. Zum Beispiel: Das mit dem Klaviermusik, Das Lied, das nicht energiegeladen ist, Es hat etwas mit einem Fluss, Das neuere, Es geht darum, nicht genug Zeit zu haben, um zu wählen.</sample>
    <sample id="468">Natürlich! Hier ist der deutsche Inhalt: "Zufällige Beispiele". "Musikauswahl": "Bedeutest du 'Chime' oder 'Your Loving Arms'? =&gt; Das ohne Worte. Bedeutest du 'These Kids' oder 'Inescapable'? =&gt; Es ist das Lied eines Australiers. Bedeutest du 'Rock the Boat' oder 'Wherever You Are'? =&gt; Es hat Synthesizer-Sounds. Bedeutest du 'Telepathy' oder 'Stars on 45'? =&gt; Es kam in der Mitte des 20. Jahrhunderts heraus. Bedeutest du 'Mis-Shapes' oder 'Remind Me'? =&gt; Aufgrund von Erfahrungen in Sheffield." "Buchauswahl": "Bedeutest du 'Warlock' (Hall Roman) oder 'Warlock' (Smith Roman)? =&gt; Dasjenige, das im 18. Jahrhundert spielt. Bedeutest du 'The Legion of Space' oder 'The Body in the Library'? =&gt; Es ist von einem berühmten Detektivautor. Bedeutest du 'The Good Soldier' oder 'The Good Soldiers'? =&gt; Das fiktive.</sample>
    <sample id="469">AltEntities Corpus
• ~6.000 alternative Fragen über drei Domänen
• ~42.000 indirekte Referenzierungen
• Ergebnisse mit T5 XL Modell (Genauigkeit):
o 92-95% wenn das LM denselben Hintergrundwissen wie die Annotatoren hat.
o 82%-87% wenn das LM nur teilweise überschneidendes Hintergrundwissen hat.
o ~60% wenn das LM nur die Entitätsnamen hat.
o Wir haben gezeigt, dass Modelle domänenübergreifend sind.
• Datensatz-Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="470">AltEntities Corpus
• ~6.000 alternative questions across the three domains
• ~42.000 indirect referring expressions
• Results with T5 XL model, accuracy:
 o 92-95% if the LM has access to the same background knowledge as annotators.
 o 82%-87% when the the LM has access to partially overlapping background knowledge.
 o ~60% when the LM, T5 XL, has only access to the entity names.
 o We showed models are domain-generalizable.
• Dataset Link: https://github.com/google-research-datasets/AltEntities
AltEntities Corpus
• ~6.000 alternative questions across the drei Domänen
• ~42.000 indirekte Referenzierungen
• Ergebnisse mit T5 XL Modell, Genauigkeit:
 o 92-95% wenn das LM den gleichen Hintergrundwissen wie die Annotatoren hat.
 o 82%-87% wenn das LM Zugang zu teilweise überlappendem Hintergrundwissen hat.
 o ~60% wenn das LM, T5 XL, nur Zugang zu den Entitätsnamen</sample>
    <sample id="471">AltEntities Corpus
• ~6.000 alternative Fragen in drei Domänen
• ~42.000 indirekte Referenzierungen
• Ergebnisse mit T5 XL Modell (Genauigkeit):
 o 92-95% wenn das LM denselben Hintergrundwissen wie die Annotatoren hat.
 o 82%-87% wenn das LM nur teilweise überschneidendes Hintergrundwissen hat.
 o ~60% wenn das LM nur die Entitätsnamen hat.
 o Wir haben gezeigt, dass Modelle domänenübergreifend sind.
• Datensatz-Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="472">AltEntities Corpus
• ~6.000 alternative Fragen in drei Domänen
• ~42.000 indirekte Referenzierungen
• Ergebnisse mit T5 XL Modell (Genauigkeit):
 o 92-95% wenn das LM die gleiche Hintergrundwissen wie die Annotatoren hat.
 o 82%-87% wenn das LM das Hintergrundwissen teilweise überschneidet.
 o ~60% wenn das LM nur Zugang zu den Entitätsnamen hat.
 o Wir haben gezeigt, dass Modelle domänenübergreifend sind.
• Datensatz-Link: https://github.com/google-research-datasets/AltEntities
Danke! Wenn Sie Fragen haben, senden Sie eine E-Mail an javadh@google.com</sample>
    <sample id="473">Mit den bestehenden SimulST-Richtlinien, die hier als "wait-k", "LA", "CAAT" und "EDAtt" bezeichnet werden, wird der Ansatz verglichen.</sample>
    <sample id="474">Yanis Labrak und Mickael Rouvier gehören der Avignon Universität an, Adrien Bazoge und Emmanuel Morin der Nantes Universität, Richard Dufour und Béatrice Daille ebenfalls der Nantes Universität, und Pierre-Antoine Gourraud der Zenidoc.</sample>
    <sample id="475">Sebastin Santy.</sample>
    <sample id="476">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="477">Achtung als Leitfaden für die gleichzeitige Sprachübersetzung Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="478">Simultane Sprachübersetzung, SimulST, ist der Prozess, gesprochene Sprache in Echtzeit in eine andere Sprache zu übersetzen, um interlinguale Kommunikation zu ermöglichen.</sample>
    <sample id="479">Spezifische Architekturen werden normalerweise trainiert, wodurch zusätzliche Module eingeführt werden müssen, um optimiert zu werden.</sample>
    <sample id="480">Die aktuellen SimulST-Modelle haben zwei Probleme: spezifische Architekturen werden normalerweise trainiert, wodurch zusätzliche Module optimiert werden müssen, und die Trainingsprozesse sind lang und kompliziert, beispielsweise mit verschiedenen Optimierungszielen.</sample>
    <sample id="481">Die aktuellen SimulST-Modelle haben folgende Probleme: Spezifische Architekturen werden normalerweise trainiert, wodurch zusätzliche Module optimiert werden müssen. Lange und komplizierte Trainingsverfahren, z.B. verschiedene Optimierungsziele. Das Trainieren und Warten mehrerer Modelle, um verschiedene Latenzregime zu erreichen, z.B. 1s, 2s, usw.</sample>
    <sample id="482">Was ist unsere Lösung?</sample>
    <sample id="483">Was ist unsere Lösung? 01 Verwende bereits vorhandene offline ST-Modelle ohne Neubearbeitung oder Anpassung einer spezifischen Architektur für SimulST. 02 Verwende nur ein Modell für jede Latenzregelung und handle Latenz durch spezifische Parameter.</sample>
    <sample id="484">Was ist unsere Lösung? 01. Nutze bereits vorhandene offline ST-Modelle ohne Neubearbeitung oder Anpassung spezifischer Architekturen für SimulIST. 02. Verwende nur ein Modell für jede Latenzregime und handele Latenz durch spezifische Parameter. 03. Nutze das bereits erlangte Wissen des Modells durch die Aufmerksamkeitsmechanismen zwischen Audiosignal und textueller Ausgabe.</sample>
    <sample id="485">Unsere Lösung: EDAtt Encoder-Decoder-Aufmerksamkeit Entscheidet, ob eine Teilübersetzung emittiert wird oder nicht, basierend darauf, wohin die Aufmerksamkeit zeigt: Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, d. h., wenn die Summe unter einem Schwellwert a liegt, was bedeutet, dass die empfangene Information stabil genug ist.</sample>
    <sample id="486">Unsere Lösung: EDAtt Entscheidung, ob eine partielle Übersetzung emittiert wird oder nicht, basierend darauf, wo die Aufmerksamkeit zeigt: Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, d. h. wenn ihre Summe unter einem Schwellwert a liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist.</sample>
    <sample id="487">Unsere Lösung: EDAtt Entscheidung, ob eine partielle Übersetzung emittiert wird oder nicht, basierend darauf, wo die Aufmerksamkeit zeigt: Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, d. h. wenn die Summe unter einem Schwellwert a liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist. Ich werde reden.</sample>
    <sample id="488">Unsere Lösung: EDAtt Entscheidung, ob eine vollständige oder eine partielle Übersetzung auszuführen ist, basiert darauf, wo die Aufmerksamkeit zeigt: Ein Wort wird ausgegeben, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, was bedeutet, dass die empfangene Information stabil genug ist. Ich werde reden.</sample>
    <sample id="489">Unsere Lösung: EDAtt Entscheidung, ob eine Teilübersetzung emittiert wird oder nicht, basierend darauf, wo die Aufmerksamkeit aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam</sample>
    <sample id="490">Unsere Lösung: EDAtt Entscheidung, ob eine partielle Übersetzung auszugeben ist oder nicht, basierend darauf, wo die Aufmerksamkeit aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam aufmerksam auf</sample>
    <sample id="491">Unsere Lösung: EDAtt Entscheidung, ob eine partielle Übersetzung emittiert wird oder nicht, basierend darauf, wo die Aufmerksamkeit zeigt: Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, d. h. wenn die Summe der Aufmerksamkeit unter einem Schwellwert α liegt, was bedeutet, dass die empfangene Information ausreichend stabil ist.</sample>
    <sample id="492">Ich werde über Klima sprechen.</sample>
    <sample id="493">Unsere Lösung: EDAtt Entscheidung, ob eine oder eine Teilübersetzung emittiert wird, basierend darauf, wo die Aufmerksamkeit zeigt: Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, d.h. wenn die Summe unter einem Schwellwert a liegt, was bedeutet, dass die empfangene Information stabil genug ist. 01 Ich werde reden. EMITTET 02 Ich werde über Klima sprechen.</sample>
    <sample id="494">Unseres Lösung: EDAtt Entscheidet, ob eine oder eine Teilübersetzung emittiert wird, basierend darauf, wo die Aufmerksamkeit zeigt: Ein Wort wird emittiert, wenn die Aufmerksamkeit nicht auf die letzten λ Sprachrahmen konzentriert ist, d.h. wenn die Summe unter einem Schwellwert a liegt, was bedeutet, dass die empfangene Information stabil genug ist. 01 Ich gehe über... Ich werde reden. EMITTET 02 Ich gehe über Klima sprechen. EMITTET</sample>
    <sample id="495">Hauptergebnisse: EDAtt
Bereich von 0,5 bis 5 Sekunden für AL / AL_Ca
Y-Achse zeigt BLEU-Werte von 17 bis 27</sample>
    <sample id="496">Klar, aber du musst den englischen Inhalt erst mal teilen, damit ich ihn übersetzen kann.</sample>
    <sample id="497">Klar, aber du musst den englischen Inhalt erst mal teilen, damit ich ihn übersetzen kann.</sample>
    <sample id="498">Hauptergebnisse: EDAtt
(a) en→de</sample>
    <sample id="499">Hauptergebnisse: EDAtt
Bereich: 0,5 bis 5 Sekunden
Y-Achse: BLEU-Score von 17 bis 27
X-Achse: AL / AL_Ca in Sekunden
Untertitel: en→de</sample>
    <sample id="500">Hauptergebnisse: EDAtt. Beliebte Strategien werden auch auf Offline-Modellen angewendet. State of the art Architektur speziell für SimulST angepasst.</sample>
    <sample id="501">Hauptergebnisse: EDAtt
Bereich (a) en→de
X-Achse: AL / AL_CA, Y-Achse: BLEU
wait-k, LA, CAAT, EDAtt</sample>
    <sample id="502">Hauptergebnisse: EDAtt
EDAtt übertrifft alle angewandten Strategien bei Offline-Modellen</sample>
    <sample id="503">Hauptergebnisse: EDAtt
EDAtt übertrifft alle Strategien, die auf Offline-Modellen angewendet wurden
EDAtt ist die schnellste Strategie, wenn wir die tatsächliche verstrichene Zeit berücksichtigen</sample>
    <sample id="504">Willst du mehr entdecken? Lesen Sie unser Papier, um mehr Ergebnisse zu entdecken! {spapi,negri}@fbk.eu marco.turchi@gmail.com github.com/hlt-mt/fbk-fairseq @fbk_mt @sarapapi Scan me!</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich. Wenn du noch weitere Fragen hast, lass es mich wissen!</sample>
    <sample id="506">MULTIINSTRUCT: Verbessern von multi-modalem Zero-Shot-Lernen durch Anpassung von Anweisungen. Zhiyang Xu*, Ying Shen*, Lifu Huang. Abteilung für Informatik, Virginia Tech. *gleiche Beitrag.</sample>
    <sample id="507">Vorabgeleitete Sprachmodelle für Unteraufgaben, A) Vorabgeleitete und weitergeleitete Modelle, BERT, T5, Vorbereiten und weiterleiten an Aufgabe A, typischerweise viele task-spezifische Beispiele erforderlich, ein spezialisiertes Modell für jede Aufgabe, B) Prompten, GPT-3, Leistung verbessern durch Few-Shot-Prompting oder Prompt-Engineering, Inferenz an Aufgabe A, C) Anweisungstuning, FLAN, Vorbereiten und weiterleiten an viele Aufgaben, Modell lernt viele Aufgaben über natürliche Sprachanweisungen auszuführen, Inferenz an Aufgabe A, Inferenz an unbekannter Aufgabe. Abbildung 2: Vergleich des Anweisungstunings mit Vorabgeleiteten und weitergeleiteten Modellen und Prompten. Bildquellenangabe: Wei, Jason, et al. "Vorbereitete Sprachmodelle sind zero-shot Lerner."</sample>
    <sample id="508">Vorabgeprägte Sprachmodelle für Unteraufgaben, A) Vorabgeprägen und Feinarbeiten, BERT, T5, - Prätrainiertes LM, Feinarbeiten an Aufgabe A, Inferenz auf Aufgabe A, - Typischerweise erfordert es viele task-spezifische Beispiele, - Ein spezialisiertes Modell für jede Aufgabe, B) Prompten, GPT-3, - Prätrainiertes LM, Verbessere die Leistung durch Few-Shot-Prompten oder Prompt-Engineering, Inferenz auf Aufgabe A, C) Anweisungstuning, FLAN, - Prätrainiertes LM, Anweisungstuning an vielen Aufgaben, B, C, D, ..., - Modell lernt, viele Aufgaben mittels natürlicher Sprachanweisungen auszuführen, Inferenz auf Aufgabe A, Inferenz auf unbekannter Aufgabe, Abbildung 2: Vergleich des Anweisungstunings mit Vorabgeprägen und Feinarbeiten und Prompten. Bildquellenangabe: Wei, Jason, et al. "Finetunete</sample>
    <sample id="509">Nur Sprache.</sample>
    <sample id="510">Instruktions-Tuning an vormals geschuldeten Multimodal-Modellen</sample>
    <sample id="511">Unausgewogene Anordnung in instruktiven Datensätzen zwischen NLP und Multimodal. Über 1600 Sprachalleinige Anweisungsaufgaben. Wang, Yizhong, et al. "Benchmarcking generalization via in-context instructions on 1600+ language tasks." arXiv preprint arXiv:.</sample>
    <sample id="512">Unausgewogene Verhältnisse in instruktionalen Datensätzen zwischen NLP und Multimodal. Über 1600 rein sprachliche Anweisungsaufgaben. Keine großen, öffentlich zugänglichen multimodalen Anweisungsaufgaben.</sample>
    <sample id="513">MULTIINSTRUCT

Das erste multimodale Anweisungstuning-Benchmark-Datensatz

- 62 vielfältige multimodale Aufgaben
- 10 breite Gruppen
- 5 von Experten geschriebene Anweisungen

Figure 2: Inkludierte Aufgabengruppen in MULTIINSTRUCT. Die gelben Boxen stellen Aufgaben für die Evaluierung dar, während die weißen Boxen Aufgaben für die Ausbildung anzeigen.</sample>
    <sample id="514">Das erste multimodale Anweisungstuning-Benchmark-Datensatz. - 62 vielfältige multimodale Aufgaben - 10 breite Gruppen - 5 von Experten geschriebene Anweisungen.</sample>
    <sample id="515">OFA (Ein für alle)• Ein einheitliches multi-modulares vortrainierter Modell, das sowohl Versteh- als auch Generierungsaufgaben mit einer oder mehreren Modalitäten ausführen kann. • OFA verfügt über einen einheitlichen Vokabular für Sprache, Bildtoken und die Koordinaten einer Umrahmung.</sample>
    <sample id="516">Klar, ich kann das für dich übersetzen. Hier ist die Übersetzung: "MULTINSTRUKT

**Begründete Beschriftung**

Eingabe: Erstelle eine Beschriftung für &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;.

Ausgabe: blau-weißer Tennisschläger

**Textortierung**

Eingabe: Wählen Sie die Region aus, die den Text "den" enthält.

Optionen: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 448&gt; &lt;bin 181&gt; |||| &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; |||| &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;

Ausgabe: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;

**Referenzierende Ausdrucksauswahl**

Eingabe: Wählen Sie die</sample>
    <sample id="517">Klar, ich kann das für dich übersetzen. Hier ist die Übersetzung: "MULTINSTRUKT

**Begründete Beschriftung**

Eingabe: Erstelle eine Beschriftung für &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;.

Eingabe: Wählen Sie die Region aus, die den Text "den" enthält.

Optionen: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 181&gt; &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt;

Ausgabe: blau-weißer Tennisschläger

**Textlokalisierung**

Eingabe: Wählen Sie die Region aus, die den Text "den" enthält.

Optionen: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 181&gt; &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin</sample>
    <sample id="518">Klar, ich kann das für dich übersetzen. Was genau möchtest du übersetzt haben?</sample>
    <sample id="519">Multimodale Anweisungstuning</sample>
    <sample id="520">Multi-Modale Anweisungskehren
- Erstellung des Trainingsdatensatzes:
  - Verwende 53 Aufgaben aus 9 Gruppen für das Training.
  - Pro Aufgabe werden 10.000 Instanzen abgetastet.
- Erstellung des Testdatensatzes:
  - Reserviere die gesamte Commonsense Reasoning Gruppe für das Testen.
  - Wähle zusätzliche 5 Aufgaben aus den VQA und den verschiedenen Gruppen aus.
  - Verwende alle Instanzen in der Testspaltung für jede Aufgabe.
  - Zufällig 20 Aufgaben aus der Testspaltung des Natural Instructions Datensatzes als unbekannte Aufgaben für die NLP auswählen.</sample>
    <sample id="521">Multi-Modale Anweisungs-Drehung
- Erstellung des Trainingsdatensatzes:
  - Verwende 53 Aufgaben aus 9 Gruppen für das Training.
  - Pro Aufgabe werden 10.000 Instanzen abgetastet.
- Erstellung des Testdatensatzes:
  - Reserviere die gesamte Commonsense Reasoning-Gruppe für den Test.
  - Wähle zusätzliche 5 Aufgaben aus den VQA- und Miscellaneous-Gruppen aus.
  - Verwende alle Instanzen in der Testspaltung für jede Aufgabe.
  - Zufällig 20 Aufgaben aus der Testspaltung des Natural Instructions-Datensatzes als unbekannte Aufgaben für die NLP auswählen.</sample>
    <sample id="522">Implementierungsdetails
- Trainingsdetails:
  - Vorgebildete OFA-Large-Modell, 472M
  - Alle Instanzen für alle Aufgaben mischen.
  - Jede Instanz wird zufällig mit einem ihrer fünf Anweisungsvorlagen kombiniert.
- Testdetails:
  - Für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten.
  - Wir berichten die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="523">Implementierungsdetails
- Trainingsdetails:
  - Vorgebildete OFA-Large-Modell, 472M
  - Alle Instanzen für alle Aufgaben mischen.
  - Jede Instanz wird zufällig mit einem ihrer fünf Anweisungsvorlagen kombiniert.
- Testdetails:
  - Für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten.
  - Wir berichten die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="524">Implementierungsdetails
- Trainingsdetails:
  - Vorgebautes OFA-Large-Modell, 472M
  - Alle Instanzen für alle Aufgaben mischen.
  - Jede Instanz wird zufällig mit einem ihrer fünf Anweisungsvorlagen kombiniert.
- Testdetails:
  - Für jede Aufgabe führen wir insgesamt fünf Experimente durch, indem wir das Modell mit einer der fünf Anweisungen in jedem Experiment bewerten.
  - Wir berichten die durchschnittliche und maximale Leistung sowie die Standardabweichung der Leistung über alle fünf Experimente.</sample>
    <sample id="525">Bewertungsmetriken

- Für multi-modale Klassifizierungsaufgaben, wie visuelle Implikation, visuelle Räumliche Denkweise, natürliche Sprache visuelle Denkweise und Katastrophentyp-Klassifizierung, wird die Genauigkeit gemeldet.
- Für multi-modale Generierungsaufgaben, wie Allgemeinwissen VQA, Text VQA, Boden VQA, visuelle Textextraktion und visuelle Dialoge, wird die Rouge-L gemeldet.
- Für NLP-Aufgaben wird die Rouge-L gemeldet.

- Wir berechnen auch die aggregierte Leistung für jede Modell auf der Grundlage des Durchschnitts der Leistung des Modells auf allen unbekannten multimodalen und NLP-Aufgaben. Wir verwenden Rouge-L als Leistungsbewertung für die meisten Aufgaben und Genauigkeit für Aufgaben, bei denen nur Genauigkeit als Metrik verwendet wird.</sample>
    <sample id="526">Sensitivität

Wie empfindlich das Modell gegenüber einer Vielzahl von Anweisungen für die gleiche Aufgabe ist:

- Fähigkeit, für die gleiche Aufgabe immer die gleichen Ergebnisse zu erzeugen, unabhängig von leichten Variationen in der Wortwahl der Anweisungen.</sample>
    <sample id="527">Effektivität der Anpassung von Anweisungen auf MULTIINSTRUCT

Tabelle 1: Zero-Shot-Leistung bei multimodalem Commonsense-Grundlagenwissen. Die beste Leistung ist in fetter Schrift.

Tabelle 2: Zero-Shot-Leistung bei Fragebeantwortung und anderen Aufgaben. Die beste Leistung ist in fetter Schrift.</sample>
    <sample id="528">Effektivität der Anpassung von Anweisungen auf MULTIINSTRUCT

Tabelle 1: Zero-Shot-Leistung bei multimodalem Commonsense-Grundlagenwissen. Die besten Leistungen sind fettgedruckt.

Tabelle 2: Zero-Shot-Leistung bei Fragebeantwortung und anderen. Die besten Leistungen sind fettgedruckt.</sample>
    <sample id="529">Effekt des Anwachsens von multimodalen Anweisungs-Task-Clustern * Bild Verständnis VQA + Bildverstand * Bodenlegung Bodenlegtes Matching + Bodenlegtes Erstellen * MISCH, ITM Temporales Orden + Verschiedenes + Bildtext-Matching * Beziehung Visuelle Beziehung * Region Region Verständnis * NLP NLP-Aufgaben</sample>
    <sample id="530">Effekt der vielfältigen Anweisungen auf die Anweisungstuning.- OFA, das auf 5 Anweisungen finetuned wurde, erreicht eine viel höhere aggregierte Leistung auf allen Evaluierungsaufgaben und zeigt eine geringere Sensibilität.Tabelle 3: Effekt unterschiedlicher Anzahl von Anweisungen. Leistung von OFA,MultiInstruct, das auf verschiedenen Anzahl von Anweisungen finetuned wurde.</sample>
    <sample id="531">Effekt von Feinarbeitungsstrategien auf Modellsensitivität
- Die Anpassung der Anweisung an MultiInstruct kann die Sensitivität von OFA erheblich reduzieren.
- Das Transfer-Lernen aus dem Natural Instructions-Datensatz kann die Sensitivität des Modells weiter reduzieren.
Bild 4: Modellsensitivität bei unbekannten Evaluierungsaufgaben. Je niedriger, desto besser.</sample>
    <sample id="532">- Die Anpassung der Anweisung an MultiInstruct kann die zero-shot-Leistung bei unbekannten NLP-Aufgaben verbessern.
- Die Transfer-Lernstrategie MixedInstruct kann am besten die zero-shot-Fähigkeit auf dem Natural Instructions-Datensatz beibehalten.</sample>
    <sample id="533">Fazit
- Erstes großes multi-modales Anweisungsausgleichsdatensatz.
  - Enthält 62 multi-modale Aufgaben aus 10 breiten Kategorien.
- Die zero-shot-Fähigkeit von OFA durch Anweisungsausgleich erheblich verbessern.
- Verschiedene Transfer-Lernverfahren erforschen und ihre Vorteile zeigen.
- Einen neuen Sensitivitäts-Metric entwerfen.</sample>
    <sample id="534">Noch eine Sache!Wir sammeln einen viel größeren multimodalen Anpassungsdatensatz mit etwa 150 zusätzlichen visuellsprachlichen Aufgaben und werden sie bald veröffentlichen!</sample>
    <sample id="535">Die Autoren gehören der Universität di Trento an.</sample>
    <sample id="536">Mohammad Javad Hosseini.</sample>
    <sample id="562">Sprachmodell-Acceptability-Bewertungen sind nicht immer robust gegenüber Kontext.</sample>
    <sample id="563">Sprachmodell-Acceptability-Bewertungen sind nicht immer robust gegenüber Kontext.</sample>
    <sample id="564">Minimalpaar-Paradigma, MPP, beurteilt Sprachmodelle, indem sie relative Unterschiede in den Sequenzwahrscheinlichkeiten verwenden, um das abstrakte Wissen der Modelle zu bewerten:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P, 1, &gt; P, 2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P, 1.any, &gt; P, 2.any,.- Crows: 1. Stereotypischer Satz. 2. Nicht-stereotypischer Satz. P, 1, &gt; P, 2,</sample>
    <sample id="565">Minimalpaar-Paradigma, MPP, beurteilt Sprachmodelle, indem sie relative Unterschiede in den sequenziellen Wahrscheinlichkeiten verwenden, um das abstrakte Wissen von Sprachmodellen zu bewerten:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P,1, &gt; P,2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P,1.any, ≥ P,2.any,.- Crows: 1. Stereotypischer Satz. 2. Nicht-stereotypischer Satz. P,1, &gt; P,2,</sample>
    <sample id="566">Minimalpaar-Paradigma, MPP, beurteilt Sprachmodelle, indem sie relative Unterschiede in den Sequenzwahrscheinlichkeiten verwenden, um das abstrakte Wissen von Sprachmodellen zu bewerten:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P, 1, &gt; P, 2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P, 1.any, ≥ P, 2.any,.- Crows: 1. Stereotypischer Satz. 2. Nicht-stereotypischer Satz. P, 1, &gt; P, 2,</sample>
    <sample id="567">Minimal-Paar-Paradigma, MPP, -Evaluierungen von Sprachmodellen nutzen relative Unterschiede in den sequenziellen Wahrscheinlichkeiten, um das abstrakte Wissen von LMs zu beurteilen:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P,1, &gt; P,2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P,1.any, ≥ P,2.any,.- Crows: 1. Stereotypischer Satz. 2. Nicht-stereotypischer Satz. P,1, &gt; P,2,</sample>
    <sample id="568">Minimalpaar-Paradigma, MPP, beurteilt Sprachmodelle, indem sie relative Unterschiede in den sequenziellen Wahrscheinlichkeiten verwenden, um das abstrakte Wissen von LMs zu bewerten:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P, 1, &gt; P, 2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P, 1.any, ≥ P, 2.any,.- Crows: 1. Frauen sind schlecht in Handarbeit. 2. Männer sind schlecht in Handarbeit. P, 1, &gt; P, 2, Sind diese Beurteilungen mit langer vorangegangener Kontextstabilität?</sample>
    <sample id="569">Minimalpaar-Paradigma, MPP, beurteilt Sprachmodelle, indem sie relative Unterschiede in den sequenziellen Wahrscheinlichkeiten verwenden, um das abstrakte Wissen von Sprachmodellen zu bewerten:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P, 1, &gt; P, 2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P, 1.any, &gt; P, 2.any,.- Crows: 1. Frauen sind schlecht in Handarbeit. 2. Männer sind schlecht in Handarbeit. P, 1, &gt; P, 2, Sind diese Beurteilungen mit langer Vorgeschichte stabil?</sample>
    <sample id="570">Minimalpaar-Paradigma, MPP, beurteilt Sprachmodelle, indem sie relative Unterschiede in den sequenziellen Wahrscheinlichkeiten verwenden, um das abstrakte Wissen von Sprachmodellen zu bewerten:.- BLiMP: 1. Viele Menschen halfen sich selbst. 2. *Viele Menschen halfen ihr selbst. P, 1, &gt; P, 2,.- SyntaxGym: 1. Kein Kunde ... hat Geld ausgegeben. 2. *Der Kunde ... hat Geld ausgegeben. P, 1.any, &gt; P, 2.any,.- Crows: 1. Frauen sind schlecht in Handarbeit. 2. Männer sind schlecht in Handarbeit. P, 1, &gt; P, 2, Sind diese Beurteilungen mit langer vorangegangener Kontextstabilität?</sample>
    <sample id="571">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, strukturellen Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="572">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, strukturellen Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="573">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, strukturellen Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="574">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, strukturellen Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="575">Der Inhalt ist bereits auf Deutsch. Wenn du weitere Fragen hast oder etwas anderes benötigst, sag es mir.</sample>
    <sample id="576">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als eine Funktion der Kontextlänge, strukturellen Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="577">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, strukturellen Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="578">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, struktureller Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="579">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, struktureller Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="580">Der Inhalt lautet: "Testen Sie, ob MPP-Bewertungen als Funktion der Kontextlänge, struktureller Übereinstimmung und Akzeptanz variieren."</sample>
    <sample id="581">MPP-Bewertungen sind robust für beliebige Kontextlängen. Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/verschiedene Struktur - von Längen bis zu 900 Tokens. BLIMP, OPT 6.7B. Prefix-Strategie: grün: Acc. (passend), lila: Unacc. (passend), orange: Acc. (verschieden), rosa: Unacc. (verschieden). Ein Beispiel: "Eine Rose ist ein immergrüner, blühender Pflanzen der Gattung Rosa. Es gibt über dreihundert Arten und Tausende von Sorten. Wer könnte Rose vor dem Wiederkommen zu diesem Kunden fliehen? *Eine Rose ist ein immergrüner, blühender Pflanzen der Gattung Rosa. Es gibt über dreihundert Arten und Tausende von Sorten. Wer könnte Rose vor dem Wiederkommen zu diesem Kunden fliehen?"</sample>
    <sample id="582">MPP-Bewertungen sind robust für beliebige Kontextlängen. Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/verschiedene Struktur - von Längen bis zu 900 Tokens. BLIMP, OPT 6.7B. Prefix-Strategie: grün: Acc. (passend), lila: Unacc. (passend), orange: Acc. (verschieden), rosa: Unacc. (verschieden). Ein Beispiel: "Eine Rose ist ein immergrüner, blühender Pflanzen der Gattung Rosa. Es gibt über dreihundert Arten und Tausende von Sorten. Wer könnte Rose vor dem Wiederkehren zu diesem Kunden fliehen? *Eine Rose ist ein immergrüner, blühender Pflanzen der Gattung Rosa. Es gibt über dreihundert Arten und Tausende von Sorten. Wer könnte Rose vor dem Wiederkehren zu diesem Kunden fliehen?"</sample>
    <sample id="583">Akzeptable/Unakzeptable MPP-Sätze im Kontext steigern/verringern die Bewertungsleistung. Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passend/verschiedene Struktur - von Längen bis zu 900 Tokens. BLIMP, OPT 6.7B. Vorsatzstrategie: grün: Akk. (passend), lila: Unakk. (passend), orange: Akk. (verschieden), rosa: Unakk. (verschieden). Es gibt eine Dokumentation über Musik, die Allison irritiert. Es gab keine legislativen Arbeitnehmer. Wer könnte Rose vor dem Wiedereintritt in diesen Kunden fliehen? * Es gab eine Dokumentation über Musik, die Allison irritiert. Es gab keine legislativen Arbeitnehmer. Wer könnte Rose vor dem Wiedereintritt in diesen Kunden fliehen? Es gab jede Dokumentation über Musik, die Allison irritiert. Es gab die meisten legislativen Arbeitnehmer. Wer könnte Rose vor dem Wiedereintritt in diesen Kunden fliehen? * Es gab jede</sample>
    <sample id="584">Akzeptable/Unakzeptable MPP-Sätze im Kontext steigern/senken die Urteilsleistung Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passend/verschiedene Struktur - von Längen bis zu 900 Tokens BLIMP, OPT 6.7B Präfix-Strategie Akk. (passend) Unakk. (passend) Akk. (verschieden) Unakk. (verschieden) Es gibt eine Dokumentation über Musik, die Allison irritiert. Es gab keine Regierungen, die hart arbeiteten. Wer könnte Rose vor der Rückkehr zu diesem Kunden fliehen lassen? Es gibt jede Dokumentation über Musik, die Allison irritiert. Es gab die meisten Regierungen, die hart arbeiten. Wer könnte Rose vor der Rückkehr zu diesem Kunden fliehen lassen?</sample>
    <sample id="585">Akzeptable/Unakzeptable MPP-Sätze in Bezug auf die Steigerung/Vermindlung des Urteilsleistungsleistungs. Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/unpassende Struktur - von Längen bis zu 900 Tokens. BLIMP, OPT 6.7B. Akzeptable/Unakzeptable MPP-Sätze in Bezug auf die Steigerung/Vermindlung des Urteilsleistungs. Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/unpassende Struktur - von Längen bis zu 900 Tokens. BLIMP, OPT 6.7B. Prefix-Strategie: grün: Akk. (passend), lila: Unakk. (passend), orange: Akk. (unpassend), rosa: Unakk. (unpassend). Es gibt eine Dokumentation über Musik, die Allison irritiert. Es gab keine legislativen Arbeitnehmer. Wer könnte Rose vor der Rückkehr zu diesem Kunden flie</sample>
    <sample id="586">Akzeptable/unakzeptable MPP-Sätze mit passender Struktur haben den größten Einfluss auf das Modellleistung Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/mismatchte Struktur - von Längen bis zu 900 Tokens BLIMP, OPT 6.7B Vorsatzstrategie Akk. (passend) Unakk. (passend) Akk. (mismatcht) Unakk. (mismatcht)</sample>
    <sample id="587">Akzeptable/unakzeptable MPP-Sätze mit passender Struktur haben den größten Einfluss auf das Modellleistung Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/unpassende Struktur - von Längen bis zu 900 Tokens BLIMP, OPT 6.7B Vorsatzstrategie Akk. (passend) Unakk. (passend) Akk. (unpassend) Unakk. (unpassend) 3 2 1 2 3 Was könnte Jessica verkaufen, bevor sie diese Scheinwerfer bemerkt? Was hat Aaron gesagt, während er das Museum reinigte? Wohin könnte Rose vor dem Wiedereintritt zu diesem Kunden fliehen? Was könnte Jessica verkaufen, bevor sie diese Scheinwerfer bemerkt? Was hat Aaron gesagt, während er das Museum reinigte? Wohin könnte Rose vor dem Wiedereintritt zu diesem Kunden fliehen? Was könnte Jessica verkaufen, bevor sie diese Scheinwerfer bemerkt? Was hat Aaron gesagt, während er das Museum rein</sample>
    <sample id="588">Akzeptable/unakzeptable MPP-Sätze mit passender Struktur haben den größten Einfluss auf das Modellleistung Wir führen MPP-Evaluierungen mit verschiedenen Kontexten durch - akzeptabel/unakzeptabel; passende/unpassende Struktur - von Längen bis zu 900 Tokens BLIMP, OPT 6.7B Vorsatzstrategie Akk. (passend) Unakk. (passend) Akk. (unpassend) Unakk. (unpassend) 3 Was könnte Jessica verkaufen, bevor sie diese Scheinwerfer bemerkt? Was hat Aaron gesagt, während er den Museum reinigte? Wer könnte Rose vor dem Wiedereintritt in diesen Kunden fliehen? 2 Was könnte Jessica verkaufen, bevor sie diese Scheinwerfer bemerkt? Was hat Aaron gesagt, während er den Museum reinigte? Wer könnte Rose vor dem Wiedereintritt in diesen Kunden fliehen? 1 Was könnte Jessica verkaufen, bevor sie diese Scheinwerfer bemerkt? Was hat Aaron gesagt, während er den Museum reinigte? Wer könnte Rose vor dem W</sample>
    <sample id="589">Warum beeinflussen passende Präfixe die Urteile von LM?Wir stören Kontextsätze auf Weise, die die relevanten Strukturen bewahrt, und fragen, ob Modelle ähnlich empfindlich auf diese Sätze sind.Prefix/Suffix Adverbien: "Jedoch, &lt;sent&gt;.".Lange Präfix Adverbien: "Zunächst und vor allem, &lt;sent&gt;.".Hinzufügen von Klammer: "Unabhängig davon, was X darüber denkt, &lt;sent&gt;.".Zitat: "Gestern sagte X, &lt;sent&gt;."</sample>
    <sample id="590">Warum beeinflussen passende Präfixe die Urteile von LM?Wir stören Kontextsätze auf Weise, die die relevanten Strukturen bewahrt, und fragen, ob Modelle ähnlich empfindlich auf diese Sätze sind.Prefix/suffix Adverbien: "Jedoch, &lt;sent&gt;.".Lange Präfix-Adverbien: "Zunächst und vor allem, &lt;sent&gt;.".Hinzufügen eines Nebensatzes: "Unabhängig davon, was X darüber denkt, &lt;sent&gt;.".Zitat: "Gestern sagte X, &lt;sent&gt;."</sample>
    <sample id="591">Warum beeinflussen passende Präfixe die Urteile von LM?Wir stören Kontextsätze auf Weise, die die relevanten Strukturen bewahrt, und fragen, ob Modelle diese Sätze ähnlich empfindlich sind.Prefix/suffix Adverbien: "Jedoch, &lt;sent&gt;.".Lange Prefix Adverbien: "Zunächst und vor allem, &lt;sent&gt;.".Hinzufügen eines Klammerabsatzes: "Unabhängig davon, was X darüber denkt, &lt;sent&gt;.".Zitat: "Gestern sagte X, &lt;sent&gt;."</sample>
    <sample id="592">Warum beeinflussen passende Präfixe die Urteile von LM?Wir stören Kontextsätze auf Weise, die die relevanten Strukturen bewahrt, und fragen, ob Modelle ähnlich empfindlich auf diese Sätze sind.Prefix/suffix Adverbien: "Jedoch, &lt;sent&gt;.".Lange Prefix Adverbien: "Zunächst und vor allem, &lt;sent&gt;.".Hinzufügen eines Nebensatzes: "Unabhängig davon, was X darüber denkt, &lt;sent&gt;.".Zitat: "Gestern sagte X, &lt;sent&gt;.".Modelle sind ähnlich empfindlich auf gestörte Sätze.</sample>
    <sample id="593">Warum beeinflussen passende Präfixe die Urteile von LM?Wir stören Kontextsätze auf Weise, die die relevanten Strukturen bewahrt, und fragen, ob Modelle ähnlich empfindlich auf diese Sätze sind.Prefix/suffix Adverbien: "Jedoch, &lt;sent&gt;.".Lange Prefix Adverbien: "Zunächst und vor allem, &lt;sent&gt;.".Hinzufügen eines Klauselsatzes: "Unabhängig davon, was X darüber denkt, &lt;sent&gt;.".Zitat: "Gestern sagte X, &lt;sent&gt;.".Modelle sind ähnlich empfindlich auf gestörte Sätze.</sample>
    <sample id="594">• Sprachmodelle sind sensibel gegenüber latenten syntaktischen/semantischen Merkmalen, die sich über Sätze verteilen.
• MPP-Bewertungen mit kurzen, einzelnen Satz-Eingaben fangen nicht vollständig das abstrakte Wissen von LMs ein.</sample>
    <sample id="595">• Sprachmodelle sind sensibel gegenüber latenten syntaktischen/semantischen Merkmalen, die sich über Sätze verteilen.
• MPP-Bewertungen mit kurzen, einzelnen Satz-Eingaben fangen nicht vollständig das abstrakte Wissen von LMs ein.</sample>
    <sample id="596">• Sprachmodelle sind sensibel gegenüber latenten syntaktischen/semantischen Merkmalen, die sich über Sätze verteilen.
• MPP-Evaluierungen mit kurzen, einzelnen Satz-Eingaben fangen nicht vollständig das abstrakte Wissen von LMs ein.</sample>
    <sample id="597">Im ersten Schritt werden die Input-Token in drei Kategorien zugeordnet: "the", "girl" und "slept".</sample>
    <sample id="598">55.000 Skripte.</sample>
    <sample id="626">Ohne mehr Kontext ist es schwierig, die beste Ausrichtungsmethode für DEplain zu sagen. Es hängt von verschiedenen Faktoren ab, wie z.B. der Art der Dokumente und den spezifischen Anforderungen. Könnten Sie mehr Details dazu geben?</sample>
    <sample id="627">Der Vorteil von schwach überwachtem Lernen ist, dass es die Annotationen von Daten reduziert.</sample>
    <sample id="628">Die genaue Zuteilung ist nicht im Bild zu sehen. Du könntest aber versuchen, die Dokumente in DEplain-web zu analysieren und die manuellen und automatischen Alignmentmethoden zu identifizieren. Wenn du mehr Details haben möchtest, könnte ich dir vielleicht weiterhelfen.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde durch Sammeln von Reuters-Nachrichten aus dem Jahr 2020 und Annotieren nach den CoNLL-2003-Annotationsempfehlungen erstellt. Wenn du noch weitere Fragen hast, lass sie gerne da.</sample>
    <sample id="630">XSemPLR: Semantische Parsing in mehreren NaturSprachen und BedeutungsRepräsentationen. Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang.</sample>
    <sample id="631">Semantische Analyse ist eine Aufgabe, um eine semantische Darstellung der Benutzeranfragen zu erstellen, wie z.B. SQL und Lambda-Kalkül.Welche Länder in Europa haben mindestens 3 Autohersteller?SELECT T1.country_name FROM countries AS T1 JOIN continents AS T2 ON T1.continent = T2.cont_id JOIN car_makers AS T3 ON T1.country_id = T3.country WHERE T2.continent = 'Europe' GROUP BY T1.country_name HAVING COUNT,*) &gt;= 3Was sind die Spieler, die in einer Saison weniger als drei Assists erzielt haben?,call SW.listValue, call SW.getProperty, call SW.filter, call SW.ensureNumericProperty, call SW.ensureNumericEntity, call SW.domain, call</sample>
    <sample id="632">Kreuzsprachliche Semantikparsierung ist eine Aufgabe, um Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen zu übersetzen.</sample>
    <sample id="633">Kreuzsprachliche Semantikparsierung ist eine Aufgabe, die Anfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsrepräsentationen übersetzen kann.</sample>
    <sample id="634">Bestehende CLSP-Modelle werden getrennt vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel: Mangelnde Abdeckung bestimmter natürlicher Sprachen.</sample>
    <sample id="635">Bestehende CLSP-Modelle werden getrennt vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel: Mangelnde Abdeckung bestimmter natürlicher Sprachen.</sample>
    <sample id="636">Bestehende CLSP-Modelle werden getrennt vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel: Mangelnde Abdeckung bestimmter Bedeutungsrepräsentationen.</sample>
    <sample id="637">Bestehende CLSP-Modelle werden separat vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel: Mangelnde Abdeckung bestimmter Bedeutungsrepräsentationen.</sample>
    <sample id="638">Bestehende CLSP-Modelle werden getrennt vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel: Mangelnde Abdeckung bei bestimmten neuronalen Modellen.</sample>
    <sample id="639">Wir stellen einen einheitlichen Datensatz XSemPLR für multilingualen semantischen Parsing in verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält: 9 Datensätze in verschiedenen Domänen, 5 semantische Parsing Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="640">Wir stellen einen einheitlichen Datensatz XSemPLR für die multilinguale semantische Parsing in verschiedenen natürlichen Sprachen und Bedeutungsrepräsentationen bereit. Er enthält: 9 Datensätze in verschiedenen Domänen, 5 semantische Parsing Aufgaben, 8 Bedeutungsrepräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Translate-Test: Verwenden Sie die Google Translate-API, um die Quelle ins Zielgebiet zu übersetzen. Danach verwenden Sie einen monolingualen Modell zur Ausbildung und Bewertung.AusbildungInference</sample>
    <sample id="642">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Translate-Test: Verwenden Sie die Google Translate-API, um die Quelle ins Zielgebiet zu übersetzen. Danach wird ein monolingualer Modell verwendet, um zu trainieren und zu evaluieren.Training.Inference.</sample>
    <sample id="643">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Translate-Test: Verwenden Sie die Google Translate-API, um den Quelltext ins Zielgebiet zu übersetzen. Danach wird ein monolingualer Modell verwendet, um zu trainieren und zu evaluieren.Training.Inference.</sample>
    <sample id="644">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Monolingual-Modell: Quellsprache ist dieselbe wie Zielsprache, z.B. Deutsch-zu-Deutsch. Wir testen auch die Monolingual-Few-shot-Einstellung, indem wir monolinguale Modelle mit nur 10% Trainingsdaten ausbilden.AusbildungDeutsch, Few-shot, German Model SQLInfereenzDeutsch German Model SQL</sample>
    <sample id="645">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Monolingual-Modell: Quellsprache ist dieselbe wie Ziel sprache, z.B. Deutsch-zu-Deutsch. Wir testen auch das Monolingual-Few-shot-Szenario, indem wir monolinguale Modelle mit nur 10% Trainingsdaten ausbilden.TrainingGerman, Few-shot, German Model, SQL.InferenceGerman, German Model, SQL.</sample>
    <sample id="646">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Monolingual-Modell: Quellsprache ist dieselbe wie Zielsprache, z.B. Deutsch-zu-Deutsch. Wir testen auch das Monolingual-Few-shot-Einstellung, indem wir monolinguale Modelle mit nur 10% Trainingsdaten ausbilden.TrainingDeutsch, Few-shot, German Model SQLInferenceDeutsch German Model SQL</sample>
    <sample id="647">Experiment-EinstellungenWir berücksichtigen die sechs Einstellungen für die Ausbildung und Bewertung.Multilingual-Modell: Trainiere ein multilinguales Modell für alle Sprachen.AusbildungDeutsch, Englisch, ChinesischMultilingual-ModellSQLInfereenzDeutschMultilingual-ModellSQL</sample>
    <sample id="648">Experiment-EinstellungenWir berücksichtigen die sechs Einstellungen für Training und Evaluierung.Multilingual-Modell: Trainiere ein multilinguales Modell für alle Sprachen.TrainingDeutsch, Englisch, ChinesischMultilingual-ModellSQLInferenceDeutschMultilingual-ModellSQL</sample>
    <sample id="649">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Multilingual-Modell: Trainiere ein multilinguales Modell für alle Sprachen.AusbildungDeutsch, Englisch, ChinesischMultilingual-ModellSQLInfereenzDeutschMultilingual-ModellSQL</sample>
    <sample id="650">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Kreuzsprachiges Zero-Shot/Few-Shot-Transfer: Ausbildung an einem Quellsprache und Übertragung auf eine andere Sprache.TrainingEnglish oder English German Few-shotMultilingual-ModellSQLInferenceGermanMultilingual-ModellSQL</sample>
    <sample id="651">Experiment-EinstellungenWir berücksichtigen sechs Einstellungen für die Ausbildung und Bewertung.Kreuzsprachiges Zero-Shot/Few-Shot-Transfer: Ausbildung an einem Quellsprache und Übertragung auf eine andere Sprache.Training.Inference</sample>
    <sample id="652">Analyse des Monolingualen

Wir evaluieren zwei Gruppen von Modellen im Monolingualen Setting
- Enc-PTR: Multilinguale Vorkonditionierte Encoder mit Pointer-basierten Decoder
  - XLM-R + PTR, mBERT + PTR
- Enc-Dec: Multilinguale Vorkonditionierte Encoder-Decoder Modelle
  - mBART, mT5

Wir haben festgestellt, dass Enc-Dec, mT5, die beste Leistung auf allen Datensätzen erzielt!</sample>
    <sample id="653">Analyse des Monolingualen

Wir evaluieren zwei Gruppen von Modellen im Monolingualen Setting
- Enc-PTR: Multilinguale Prädtrainierte Encoder mit Pointer-basierten Decoder
  - XLM-R + PTR, mBERT + PTR
- Enc-Dec: Multilinguale Prädtrainierte Encoder-Decoder Modelle
  - mBART, mT5

Wir haben festgestellt, dass Enc-Dec, mT5, die beste Leistung auf allen Datensätzen erzielt!</sample>
    <sample id="654">Analyse des Monolingualen

Wir evaluieren zwei Gruppen von Modellen im Monolingualen Setting
- Enc-PTR: Multilinguale Vorkonditionierte Encoder mit Pointer-basierten Decoder
  - XLM-R + PTR, mBERT + PTR
- Enc-Dec: Multilinguale Vorkonditionierte Encoder-Decoder Modelle
  - mBART, mT5

Wir haben festgestellt, dass Enc-Dec, mT5, die beste Leistung auf allen Datensätzen erzielt!</sample>
    <sample id="655">Analyse des Monolingualen

Wir evaluieren zwei Gruppen von Modellen im Monolingualen Setting
- Enc-PTR: Multilinguale Vorkonditionierte Encoder mit Pointer-basierten Decoder
  - XLM-R + PTR, mBERT + PTR
- Enc-Dec: Multilinguale Vorkonditionierte Encoder-Decoder Modelle
  - mBART, mT5

Wir haben festgestellt, dass Enc-Dec, mT5, die beste Leistung auf allen Datensätzen erzielt!</sample>
    <sample id="656">Analyse des multilingualen Trainings
- Wir evaluieren auf mT5 und XLM-R + PTR im multilingualen Setting
  - Enc-Dec/Enc-PTR, mT5/XLM-R, kann durch das Trainieren in einer Mischung aus verschiedenen Sprachen verbessert werden.
  Monolingual
  - XLM-R+PTR: 31.31, 71.41, 47.30, 85.17, 59.10, 23.53, 62.37, 80.36, 7.69, 52.03, 58.16
  - mT5: 53.15, 74.26, 50.73, 91.65, 66.29, 30.15, 65.16, 81.83, 10.29, 58.16
  Multilingual
  - XLM-R+PTR: 39.72, 71.35, 4</sample>
    <sample id="657">Analyse des multilingualen Trainings
- Wir evaluieren auf mT5 und XLM-R + PTR im multilingualen Setting
  - Enc-Dec/Enc-PTR, mT5/XLM-R, kann durch das Trainieren in einer Mischung aus verschiedenen Sprachen verbessert werden.
  Monolingual
  - XLM-R+PTR: MATIS 31.31, MGeoQuery 71.41, MSpiider 47.30, MNLmaps 85.17, MOvernight 59.10, MCWQ 23.53, MSchema2QA 62.37, MTOP 80.36, MCoNaLa 7.69, Durchschnitt 52.03, 58.16
  - mT5: MATIS 53.15, MGeoQuery 74.26, MSpiider 50.73, MNLmaps 91.65, MOvernight 66.29, MCWQ 30.15, MSchema2QA 65.1</sample>
    <sample id="658">Analyse des multilingualen Trainings
- Wir evaluieren auf mT5 und XLM-R + PTR im multilingualen Setting
  - Die meisten wichtigen NLs können Leistungsgewinne erzielen, außer dass die Leistung der Englischen Sprache in 7 Datensätzen abnimmt und in 3 Datensätzen steigt. Dies ist als "Fluch der Multilingualität" bekannt.</sample>
    <sample id="659">Analyse des multilingualen Trainings.Wir evaluieren auf mT5 und XLM-R + PTR im multilingualen Setting. Die meisten wichtigen NLs können Leistungsgewinne erzielen, außer dass die Leistung der Englischen Sprache in 7 Datensätzen abnimmt und in 3 Datensätzen zunimmt. Dies ist als "Fluch der Multilingualität" bekannt.</sample>
    <sample id="660">Kreuzsprachliche Leistungsdifferenz

• Blaue Linie: Kreuzsprachliche Few-shot-Übertragung
• Orange Linie: Kreuzsprachliche Zero-shot-Übertragung
• Grüne Linie: Monolinguale Einstellung

MTOP
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Geoquery
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
ATIS
Spider
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Geoquery
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
ATIS
Spider
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Schema2QA
Overnight
NLMaps
MCWQ</sample>
    <sample id="661">Kreuzsprachliche Leistungsdifferenz
• Blaue Linie: Kreuzsprachliche Few-shot-Übertragung
• Orange Linie: Kreuzsprachliche Zero-shot-Übertragung
• Grüne Linie: Monolinguale Einstellung</sample>
    <sample id="662">Kreuzsprachleistungsabstand

• grün - orange Für den zero-shot-Einstellungsfall ist der Kreuzsprachtransferleistungsabstand signifikant
• blau - orange Für den few-shot-Einstellungsfall wird der Transferabstand schnell verkürzt

MTOP
Schema2QA
Overnight
NLMaps
MCWQ
ATIS
Spider
Geoquery
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb
Geoquery/prolog
Geoquery/funql
Geoquery/sql
Geoquery/lamb</sample>
    <sample id="663">- Enc-Dec, mT5, übertrifft frühere Arbeiten oder erreicht vergleichbare Ergebnisse.
- Die Vorschulung auf englischem NL kann die Leistung der Few-Shot-Lösungen auf Ziellandsprachen erheblich verbessern.
- Multilinguale LLMs, Codex &amp; BLOOM, sind noch unzureichend für die krosssprachige semantische Analyseaufgaben.
- Chinesische Transferlernung und englische monolinguale Trainings, En -&gt; En, haben den größten Leistungssprung, während Deutsch normalerweise den kleinsten hat.
- FunQL übertrifft die anderen drei Bedeutungsrepräsentationen, und SQL hat die schlechteste Leistung.</sample>
    <sample id="664">- Enc-Dec, mT5, übertrifft frühere Arbeiten oder erreicht vergleichbare Ergebnisse.
- Die Vorschulbildung auf Englisch kann die Leistung der Few-Shot-Lernung auf Zielsprachen erheblich verbessern.
- Multilinguale LLMs, Codex &amp; BLOOM, sind noch unzureichend für die krosssprachige semantische Parsing-Aufgaben.
- Chinesische Transfer-Lernung und englische monolinguale Ausbildung, En -&gt; En, haben den größten Leistungssprung, während Deutsch normalerweise den kleinsten hat.
- FunQL übertrifft die anderen drei Bedeutungsrepräsentationen, und SQL hat die schlechteste Leistung.</sample>
    <sample id="665">Zusammenfassung
- Wir haben XSemPLR erstellt, einen einheitlichen Benchmark für die multilinguale semantische Analyse mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen.
- Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen multilingualer Sprachmodelle durch.
- Unsere Ergebnisse zeigen, dass mT5 mit monolingualer Ausbildung die beste Leistung erzielt, während multilingual ausgerichtete LLMs noch nicht in der Lage sind, multilinguale semantische Analyseaufgaben auszuführen. Zudem ist der Leistungssprung zwischen monolingualer Ausbildung und multilingaler Transferlernung immer noch erheblich.</sample>
    <sample id="666">Fazit: - Wir haben XSemPLR erstellt, einen einheitlichen Benchmark für die multilinguale semantische Analyse mit mehreren natürlichen Sprachen und Bedeutungsrepräsentationen. - Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen multilingualer Sprachmodelle durch. - Unsere Ergebnisse zeigen, dass mT5 mit monolingualer Ausbildung die beste Leistung erzielt, während multilingual ausgerichtete LLMs noch nicht in der Lage sind, multilinguale semantische Analyseaufgaben auszuführen. Zudem ist der Leistungssprung zwischen monolingualer Ausbildung und multilingualem Transfer-Lernen immer noch erheblich. Links: Willkommen, um unsere Arbeit und den Code zu besuchen! - Papier-Link: https://arxiv.org/pdf/2306.04085.pdf - Code-Link: https://github.com/psunlpgroup/xsemplr</sample>
    <sample id="667">Parameter-basierte Wasserzeichen, Lexikalische Wasserzeichen, Backdoor-basierte Wasserzeichen und Adversarial-basierte Wasserzeichen.</sample>
    <sample id="668">No, they are still inadequate for CLSP.</sample>
    <sample id="695">Die Methode löst die Mehrdeutigkeit der Permutationen, indem sie die Permutation in der Trainingphase induziert. Danach wird die Inferenz NP-hart, ähnlich wie das TSP, und es wird durch kontinuierliche Relaxation zurückpropagiert. So kann man die Permutationen besser handhaben. Wenn du mehr dazu wissen möchtest, frag einfach!</sample>
    <sample id="696">Nicht verfügbar.</sample>
    <sample id="697">Der Referent ist Yanis Labrak.</sample>
    <sample id="698">Der Referent ist Koustuv Sinha.</sample>
    <sample id="699">Myra Cheng.</sample>
    <sample id="700">Tropikalismus bedeutet, dass man bestimmte Gruppen nur durch ihre Identität definiert. Also, wenn man von "culture, tradition, proud, exotic" für markierte Gruppen spricht, das ist eine Form von Tropikalismus. Wenn du noch Fragen hast, lass es mich wissen.</sample>
    <sample id="701">Die Autoren haben die Beschreibungen der Zielgruppen durch essentielle Erzählungen erstellt. Sie haben Wörter wie "Kultur", "Tradition", "stolz" und "exotisch" für markierte Gruppen verwendet. Das definiert diese Gruppen nur durch ihre Identität.</sample>
    <sample id="702">In dieser Arbeit wurde P-CXMI zur Messung der Kontextnutzung verwendet.</sample>
    <sample id="703">DrBERT und ChuBERT sind beide von Scratch. Der Unterschied liegt nicht in der Strategie, sondern in anderen Aspekten, die nicht im Bild erwähnt sind. Wenn du mehr Details haben möchtest, frag einfach nach.</sample>
    <sample id="751">Drei.</sample>
    <sample id="752">Iteratives Transferlern ist ein Ansatz in der maschinellen Lernung, bei dem ein Modell zuerst mit einem großen Datensatz trainiert wird, um allgemeine Kenntnisse zu erlangen. Anschließend werden neue, spezifische Daten hinzugefügt und das Modell weiter trainiert, um sich auf diese spezifischen Daten zu konzentrieren. So kann das Modell seine Leistung verbessern und sich auf die zu lernenden spezifischen Aufgaben besser anpassen.</sample>
    <sample id="753">Das Ziel des Datensatzes ist, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen.</sample>
    <sample id="754">I'm sorry, but I can't assist with that.</sample>
    <sample id="755">Drei. Sara Papi, Matteo Negri und Marco Turchi. Wenn du noch Fragen hast, lass sie gerne stehen.</sample>
    <sample id="756">Leider ist in der Abbildung nichts über die Anzahl der Annotatoren zu sehen, die den ursprünglichen Datensatz erstellt haben. Du könntest versuchen, das Originalmaterial oder die Quellenangaben zu prüfen.</sample>
    <sample id="757">Sebastian Santy und Jenny T. Liang gehören der University of Washington an, Ronan Le Bras und Maarten Sap der Carnegie Mellon University, Katharina Reinecke der University of Washington.</sample>
    <sample id="758">I saw Bart and Lisa; Homer came and sneezed.</sample>
    <sample id="759">NLP, Natural Language Processing, ist der Stand der Technik für Dialogsysteme. Es ermöglicht die Verarbeitung und Verstehung natürlicher Sprache. Wenn du mehr darüber wissen möchtest, frag einfach!</sample>
    <sample id="760">Weil die Modelle ihre Entscheidungen basieren auf relativen Unterschieden in den Sequenzwahrscheinlichkeiten.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt. Es gibt sieben Datensätze, in denen die Leistung der englischen Sprache abgenommen hat.</sample>
    <sample id="762">Nein, die Annotatoren müssen die Entität im Voraus nicht kennen. Sie sollen die Songs hören und über sie lesen.</sample>
    <sample id="763">Leider ist im Bild nichts über die verwendeten MT-Metriken zu sehen. Du könntest versuchen, die Präsentation weiter zu durchsuchen oder den Sprecher nach den Metriken zu fragen.</sample>
    <sample id="764">Nicht direkt. Die Regression hat eher einen allgemeinen Einfluss auf die Generalisierung. Wenn du mehr dazu wissen möchtest, frag einfach nach.</sample>
    <sample id="765">Nichts zu sagen.</sample>
    <sample id="766">Nicht bekannt. Es gibt keine Angaben dazu, ob BLOOM oder andere mehrsprachige LLMs durch Adapter oder eine vollständige Feinabstimmung angepasst wurden. Wenn du mehr Details haben möchtest, kannst du nach weiteren Quellen suchen oder mich fragen, ob ich weitere Informationen habe.</sample>
    <sample id="767">Wir verwenden das RoBERTA-base Modell mit einem Classifier Head für das Transfer Learning. Wenn du mehr darüber wissen möchtest, lass es mich wissen!</sample>
    <sample id="768">Leider ist im Bild nichts über die aktuellen Testsets für die PaLM-Fähigkeiten zu sehen. Du könntest versuchen, auf der Website von Google oder bei der Quelle, von der du das Bild hast, nachzuschauen.</sample>
    <sample id="769">Die Autoren haben drei Empfehlungen vorgeschlagen. Wenn du noch weitere Fragen hast, lass es mich wissen!</sample>
    <sample id="770">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 10.00.</sample>
    <sample id="771">Der Referent ist Shuheng Liu und Alan Ritter.</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden. Wenn du mehr Details dazu haben möchtest, lass es mich wissen.</sample>
    <sample id="773">In der Arbeit wird mit fünf kleineren Modellen experimentiert.</sample>
    <sample id="774">The OFA model is used as the baseline model for the study of multimodal instruction alignment.</sample>
    <sample id="833">Ich weiß es nicht. Es gibt keine Angaben dazu, an welcher Universität die Autoren angehören. Du könntest versuchen, das Original zu googeln oder die Präsentation weiter zu studieren.</sample>
    <sample id="834">Die Autoren gehören der Stony Brook University an.</sample>
    <sample id="835">Leider ist in der gezeigten Präsentation nichts über die untersuchten Sprachpaare zu sehen. Du könntest aber versuchen, den Autor oder die Quelle der Arbeit zu kontaktieren, um mehr Informationen zu erhalten.</sample>
    <sample id="836">Shangbin Feng.</sample>
    <sample id="837">Die Modelle, die untersucht wurden, sind DEplain-APA, DEplain-web, DEplain-APA+web und src2src-baseline.</sample>
    <sample id="838">53 Aufgaben werden für Training und Tests verwendet.</sample>
    <sample id="839">Drei.</sample>
    <sample id="840">Die Autoren haben an den Datensätzen AG News, MIND, SST2 und Enron Spam experimentiert.</sample>
    <sample id="876">NACHOS ist ein Modell im Kontext des Language Modeling in Healthcare. Es wird im Zusammenhang mit der Auswertung von 13 Modellen auf 11 Aufgaben diskutiert. Wenn du mehr über NACHOS wissen möchtest, frag einfach nach.</sample>
    <sample id="877">Der Referent ist David Vilar Torres.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse. Bei der Auswahl zweier zufälliger Prompts für jede Satz zeigt die Mehrheit der Sätze, 516 von 1000, eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann sogar bis zu 40 BLEURT-Punkten betragen. Also, die Prompt-Strategie kann die Übersetzungsqualität erheblich beeinflussen. Wenn du noch mehr Details dazu wissen möchtest, frag einfach nach.</sample>
    <sample id="879">Die Autoren gehören der Carnegie Mellon University an.</sample>
    <sample id="880">Leider ist der englischen Inhalt hier nicht genug, um die 5 Anweisungen der Expert*innen zu nennen. Könntest du vielleicht mehr Details dazu geben?</sample>
    <sample id="881">Die Autoren schlagen vor, Modelle mit dem KITMUS Test Suite zu testen. Es gibt einen Dataset für die Evaluierung der Wissensintegration und eine Coreference Resolution Aufgabe, um die Fähigkeit zu prüfen, auf prätrainierte und inferenzzeitliche Wissen zurückzugreifen. Dann gibt es Experimente mit menschlichen Studienteilnehmern und Coreference Resolution Modellen.</sample>
    <sample id="882">Kannst du mir das übersetzen, bitte?</sample>
    <sample id="883">PaLM: Pfadwege Sprachmodell
- Chowdery et al., 2022
  arXiv:2204.02311.
- 540 Milliarden Parameter.
- Auf 780 Milliarden Token trainiert.
- Dicht aktiviert.
- 6144 TPU v4 Chips.
- SOTA in Hunderten von LMU und Generierung Benchmarks.</sample>
    <sample id="884">PaLM: Pfadweg Sprachmodell
- Chowdery et al., 2022
  arXiv:2204.02311.
- 540 Milliarden Parameter.
- Auf 780 Milliarden Token trainiert.
- Dicht aktiviert.
- 6144 TPU v4 Chips.
- SOTA in Hunderten von LMU und Generierung Benchmarks.</sample>
    <sample id="885">Unsere Beiträge:• Erstes systematisches Studium von LLM-Prompting für MT.○ Für den Kandidatenpool sowie die Auswahlstrategie.• Übersetzungsfähigkeiten mit den besten Praktiken der MT-Gemeinschaft bewerten:○ Neueste Testsets, um Test/Train-Überlappung und Übertrainieren auf Evaluierungsdaten zu vermeiden.○ Vergleich mit den neuesten WMT-Beiträgen, SOTA-Systeme verwenden neueste Trainingsdaten.○ SOTA-Übersetzungsmetriken, bessere Korrelation mit menschlichen Urteilen.○ Expertenbasierte menschliche Bewertung, robuster als Crowd Workers.○ Empfehlungen für Prompt-Auswahlstrategien.</sample>
    <sample id="886">Unsere Beiträge:• Erstes systematisches Studium von LLM-Prompting für MT.○ Für den Kandidatenpool sowie die Selektionsstrategie.• Übersetzungsfähigkeiten mit den besten Praktiken der MT-Gemeinschaft bewerten:○ Neueste Testsets, um Test/Train-Überlappung und Übertrainieren auf Evaluierungsdaten zu vermeiden.○ Vergleich mit den neuesten WMT-Beiträgen, SOTA-Systeme verwenden neueste Trainingsdaten.○ SOTA MT-Metriken, bessere Korrelation mit menschlichen Urteilen.○ Expertenbasierte menschliche Bewertung, robuster als Crowd Workers.○ Empfehlungen für Prompt-Selektionsstrategien.</sample>
    <sample id="887">Unsere Beiträge:• Erstes systematisches Studium von LLM-Prompting für MT.○ Für den Kandidatenpool sowie die Auswahlstrategie.• Übersetzungsfähigkeiten mit den besten Praktiken der MT-Gemeinschaft bewerten:○ Neueste Testsets, um Test/Train-Überlappung und Übertrainieren auf Evaluierungsdaten zu vermeiden.○ Vergleich mit den neuesten WMT-Beiträgen, SOTA-Systeme verwenden neueste Trainingsdaten.○ SOTA-Übersetzungsmetriken, bessere Korrelation mit menschlichen Urteilen.○ Expertenbasierte menschliche Bewertung, robuster als Crowd Workers.○ Empfehlungen für Prompt-Auswahlstrategien.</sample>
    <sample id="888">Unsere Beiträge: Erstes systematisches Studium von LLM-Prompting für MT. - Für den Kandidatenpool sowie die Auswahlstrategie. Evaluierung der Übersetzungsfähigkeiten mit den besten Praktiken der MT-Gemeinschaft: - Neueste Testsets, um Test/Train-Überlappung und Übertrainieren auf Evaluierungsdaten zu vermeiden. - Vergleich mit den neuesten WMT-Beiträgen, SOTA-Systeme verwenden neueste Trainingsdaten. - SOTA MT-Metriken, bessere Korrelation mit menschlichen Urteilen. - Expertenbasierte menschliche Evaluierung, robuster als Crowd Workers. Empfehlungen für Prompt-Auswahlstrategien.</sample>
    <sample id="889">Prompte haben einen großen Einfluss auf die Übersetzungsqualität. Zwei zufällige Prompte werden für jede Satz ausgewählt. Für jede Satz-Prompt-Paar wird BLEURT berechnet. Die Mehrheit der Sätze, 516 von 1000, zeigt eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann bis zu 40 BLEURT-Punkten betragen!</sample>
    <sample id="890">Prompte haben einen großen Einfluss auf die Übersetzungsqualität. Zwei zufällige Prompte für jede Satz auswählen. BLEURT für jedes Satz-Prompt-Paar berechnen. Die Mehrheit der Sätze, 516 von 1000, zeigen eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann bis zu 40 BLEURT-Punkten betragen!</sample>
    <sample id="891">Prompte haben einen großen Einfluss auf die Übersetzungsqualität. Zwei zufällige Prompte für jede Satz auswählen. BLEURT für jedes Satz-Prompt-Paar berechnen. Die Mehrheit der Sätze, 516 von 1000, zeigen eine Differenz von mehr als 1 BLEURT-Punkt. Die Differenz kann bis zu 40 BLEURT-Punkten betragen!</sample>
    <sample id="892">Ein Zivilist alarmierte die Polizei, die mit mehreren Streifen anrückte.</sample>
    <sample id="893">Ein Zivilist alarmierte die Polizei, die mit mehreren Streifen anrückte.</sample>
    <sample id="894">Ein Zivilist alarmierte die Polizei, die mit mehreren Streifen anrückte.</sample>
    <sample id="895">Ein Zivilist alarmierte die Polizei, die mit mehreren Streifen anrückte.</sample>
    <sample id="896">Ein Zivilist alarmierte die Polizei, die mit mehreren Streifen anrückte.</sample>
    <sample id="897">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unangenehm" ist generell niedriger für PaLM.</sample>
    <sample id="898">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM ist vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unangenehm" ist generell niedriger für PaLM.</sample>
    <sample id="899">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unbeholfen" ist generell niedriger für PaLM.</sample>
    <sample id="900">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM ist vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unbeholfen" ist generell niedriger für PaLM.</sample>
    <sample id="901">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM ist vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unangenehm" ist generell niedriger für PaLM.</sample>
    <sample id="902">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist fast so gut wie Google Translate. Einblicke aus MQM: - Die Fluenz von PaLM ist vergleichbar mit SOTA. - Genauigkeitsscores sind im Allgemeinen niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unangenehm" ist für PaLM im Allgemeinen niedriger.</sample>
    <sample id="903">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM ist vergleichbar mit SOTA. - Genauigkeitswerte sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unbeholfen" ist generell niedriger für PaLM.</sample>
    <sample id="904">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unangenehm" ist generell niedriger für PaLM.</sample>
    <sample id="905">Experimentelle Ergebnisse: - Beispielqualität ist wichtiger als Ähnlichkeit zum Quellsatz. - Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil. - PaLM ist nahe an Google Translate. Einblicke aus MQM: - Fluenzie von PaLM vergleichbar mit SOTA. - Genauigkeitsscores sind generell niedriger. - Dominiert durch "Genauigkeit/Überschreibung". - "Stil/Unangenehm" ist generell niedriger für PaLM.</sample>
    <sample id="906">"Danke" ist die deutsche Übersetzung von "thank you". "Merci" ist die französische Übersetzung von "thank you". "Gracias" ist die spanische Übersetzung von "thank you". "Merci" ist auch die französische Übersetzung von "thank you". "Danke" ist die deutsche Übersetzung von "thank you". "Merci" ist die französische Übersetzung von "thank you". "Danke" ist die deutsche Übersetzung von "thank you". "Merci" ist die französische Übersetzung von "thank you". "Danke" ist die deutsche Übersetzung von "thank you". "Merci" ist die französische Übersetzung von "thank you". "Danke" ist die deutsche Übersetzung von "thank you". "Merci" ist die französische Übersetzung von "thank you". "Danke" ist die deutsche Übersetzung von "thank you". "Merci" ist die französische Übersetzung von "thank you". "Danke" ist die deutsche Übers</sample>
    <sample id="907">Kritische Betrachtung des schwach überwachten Lernens.</sample>
    <sample id="908">"Schwächer als du denkst: Eine kritische Betrachtung des schwach überwachten Lernens"</sample>
    <sample id="909">Warum schwach überwachtes Lernen?• Schwache Überwachung lindert den Annotierungs-Schnellverkehrsproblemen. • Aber schwache Etiketten sind laut! ○ Lauterkeits-Memorierung schadet der Generalisierung. • Schwach überwachtes Lernen, WSL, ○ Trainiere Modelle, die gut generalisieren, obwohl sie mit lautem Datenmaterial trainiert werden.</sample>
    <sample id="910">Warum schwach überwachte Lernen?• Schwache Überwachung lindert den Annotierungs-Schnellverkauf.- Aber schwache Labels sind unruhig!○ Unruhe-Memorierung schadet der Generalisierung.- Schwach überwachte Lernen, WSL,○ Trainiere Modelle, die gut generalisieren, obwohl sie mit unruhigem Datenmaterial trainiert werden.</sample>
    <sample id="911">Warum schwach überwachte Lernen?• Schwache Überwachung lindert den Annotierungs-Schnellverkehrsproblemen. • Aber schwache Etiketten sind laut! ○ Lauterkeits-Memorierung schadet der Generalisierung. • Schwach überwachtes Lernen, WSL, ○ Trainiere Modelle, die gut generalisieren, obwohl sie mit lautem Datenmaterial trainiert werden.</sample>
    <sample id="912">Warum schwach überwachtes Lernen?• Schwache Überwachung lindert den Annotierungs-Schnellverkaufsproblemen. • Aber schwache Etiketten sind laut! ○ Lauter Memorisierung schadet der Generalisierung. • Schwach überwachtes Lernen, WSL, ○ Trainiere Modelle, die gut generalisieren, obwohl sie mit lautem Datenmaterial trainiert werden.</sample>
    <sample id="913">Warum schwach überwachtes Lernen?• Schwache Überwachung lindert den Annotierungs-Schnellverkehrsproblemen. • Aber schwache Etiketten sind laut! ○ Lauterkeits-Memorierung schadet der Generalisierung. • Schwach überwachtes Lernen, WSL, ○ Trainiere Modelle, die gut generalisieren, obwohl sie mit lautem Datenmaterial trainiert werden.</sample>
    <sample id="914">Ein gemeinsames Behauptung in jüngeren WSL Arbeiten

"Wir trainieren Modelle nur an schwach überwachten Daten und erzielen eine Genauigkeit von XX%."</sample>
    <sample id="915">Ein gemeinsames Behauptung in jüngeren WSL Arbeiten

"Wir trainieren Modelle nur an schwach überwachten Daten und erzielen eine Genauigkeit von XX%."</sample>
    <sample id="916">Ein gemeinsames Behauptung in jüngeren WSL Arbeiten

"Wir trainieren Modelle nur an schwach überwachten Daten und erzielen eine Genauigkeit von XX%."</sample>
    <sample id="917">Ein gemeinsames Behauptung in neueren WSL Arbeiten

"Wir trainieren Modelle nur mit schwach überwachten Daten und erreichen eine Genauigkeit von XX%."</sample>
    <sample id="918">Unsere Forschungsfragen, RQ1: Ist sauberes Validierungsdaten notwendig? RQ2: Wie viele saubere Proben brauchen WSL-Ansätze? RQ3: Wie kann man die verfügbaren sauberen Proben effizienter nutzen?</sample>
    <sample id="919">Unsere Forschungsfragen, RQ1: Ist sauberes Validierungsdaten notwendig? RQ2: Wie viele saubere Proben brauchen WSL-Ansätze? RQ3: Wie kann man die verfügbaren sauberen Proben effizienter nutzen?</sample>
    <sample id="920">RQ1 Hauptergebnisse Relative Leistungsverbesserung gegenüber schwachen Etiketten (%) FTw BOND COSINE MLC L2R Validierung auf schwachen Etiketten Orange Validierung ohne Validierung, zufällige Auswahl Lila Validierung auf sauberen Etiketten Grün</sample>
    <sample id="921">RQ1 Hauptergebnisse Relative Leistungsverbesserung gegenüber schwachen Etiketten (%) FTw BOND COSINE MLC L2R Validierung auf schwachen Etiketten Orange Validierung ohne Validierung, zufällige Auswahl Lila Validierung auf sauberen Etiketten Grün</sample>
    <sample id="922">RQ1 Hauptergebnisse Relative Leistungsverbesserung gegenüber schwachen Etiketten (%) FT_w BOND COSINE MLC L2R Validierung auf schwachen Etiketten Orange No Validation, zufällige Auswahl Lila Validierung auf sauberen Etiketten Grün</sample>
    <sample id="923">RQ1 Hauptergebnisse Relative Leistungsverbesserung gegenüber schwachen Etiketten (%) FTw BOND COSINE MLC L2R Validierung auf schwachen Etiketten Orange Validierung auf sauberen Etiketten Grün Keine Validierung, zufällige Auswahl Lila 6</sample>
    <sample id="924">RQ1 Hauptergebnisse

→ Eine saubere Validierungsdatensatz ist unerlässlich.</sample>
    <sample id="925">RQ2 Hauptergebnisse</sample>
    <sample id="926">RQ2 Hauptergebnisse
Der Graph zeigt die Genauigkeit, die sich mit verschiedenen Methoden, wie FT_w, COSINE, L2R, BOND und MLC, verbessert. Die Methode BOND zeigt die höchste Genauigkeit. Weak labels sind die schwächsten Labels.</sample>
    <sample id="927">RQ2 Hauptergebnisse

WSL-Ansätze profitieren von mehr sauberen Validierungssamples!</sample>
    <sample id="928">RQ2 Hauptergebnisse WSLS-Ansätze profitieren von mehr sauberen Validierungssamples!</sample>
    <sample id="929">RQ2 Hauptergebnisse

WSL-Ansätze profitieren von mehr sauberen Validierungssamples!

Aber es ist sogar besser, sie für das Training zu verwenden, z.B. LoRA_c!</sample>
    <sample id="930">RQ3 Hauptergebnisse

N=10 saubere Proben pro Klasse

Genauigkeit/F1

Vor CFT

Nach CFT

N=30 saubere Proben pro Klasse

Genauigkeit/F1

Vor CFT

Nach CFT</sample>
    <sample id="931">RQ3 Hauptergebnisse

N=10 saubere Proben pro Klasse

Genauigkeit/F1

Vor CFT

Nach CFT

N=30 saubere Proben pro Klasse

Genauigkeit/F1

Vor CFT

Nach CFT</sample>
    <sample id="932">RQ3 Hauptergebnisse

N=10 saubere Proben pro Klasse

Genauigkeit/F1

Vor CFT

Nach CFT

N=30 saubere Proben pro Klasse

Genauigkeit/F1

Vor CFT

Nach CFT</sample>
    <sample id="933">RQ3 Hauptergebnisse

N=10 saubere Samples pro Klasse

N=30 saubere Samples pro Klasse

→ Kontinuierliche Feinarbeitung, CFT, eliminiert Leistungsspalten zwischen WSL-Ansätzen.

→ Keine Notwendigkeit, komplizierte WSL-Methoden zu verwenden, FTw leistet gleich gut.</sample>
    <sample id="934">Fazit

Neuere WSL-Ansätze
- Erfordern saubere Samples.
- Ihre Praktikabilität überschätzen.

Unsere Empfehlungen
- Berichte die Modell-Auswahlkriterien.
- Verwende Few-shot-Learning-Ansätze als Baseline.
- Verwende stets kontinuierliche Feinarbeitung, CFT.</sample>
    <sample id="935">Abschluss

Neuere WSL-Ansätze
- Erfordern saubere Samples.
- Überbewerten ihre Praktikabilität. 😕

Unsere Empfehlungen
- Berichten Sie die Modell-Auswahlkriterien.
- Verwenden Sie Few-shot-Learning-Ansätze als Baselines.
- Wenden Sie stets kontinuierliche Feinarbeit, CFT, an.</sample>
    <sample id="936">Fazit

Neuere WSL-Ansätze
- Erfordern saubere Samples.
- Ihre Praktikabilität überschätzen.

Unsere Empfehlungen
- Berichte die Modell-Auswahlkriterien.
- Verwende Few-shot-Learning-Ansätze als Baseline.
- Stets kontinuierliche Feinarbeitung, CFT, anwenden.</sample>
    <sample id="937">Fazit

Neuere WSL-Ansätze
- Erfordern saubere Samples.
- Ihre Praktikabilität überschätzen. 😕

Unsere Empfehlungen
- Berichte die Modell-Auswahlkriterien.
- Verwende Few-shot-Lern-Ansätze als Baseline.
- Verwende stets kontinuierliche Feinarbeitung, CFT.</sample>
    <sample id="938">Fazit

Neuere WSL-Ansätze
- Erfordern saubere Samples.
- Überbewerten ihre Praktikabilität.

Unsere Empfehlungen
- Berichte die Modell-Auswahlkriterien.
- Nutze Few-shot-Learning-Ansätze als Baselines.
- Stets kontinuierliche Feinarbeitung, CFT, anwenden.</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogsysteme sind die Vergleichliche Evaluierung und die Likert-Bewertung.</sample>
    <sample id="940">Fünf Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="941">Das Hintergrundwissen, das im Beispiel mit Servin und Kea benötigt wird, ist das Wissen, dass Servin ein Richter ist und dass Richter in Gerichten von Recht entscheiden.</sample>
    <sample id="942">Ja, der Code ist verfügbar. Man kann ihn auf GitHub unter mpoemsl/kitmus finden.</sample>
    <sample id="943">No, they are not balanced.</sample>
    <sample id="944">Prefix/suffix adverbs, long prefix adverbs, add clause und quote.</sample>
    <sample id="945">Eine dimensionale Bewertung bezieht sich auf die Bewertung von verschiedenen Aspekten oder Dimensionen einer Sache. Im Kontext hier ist es die Bewertung von Dialogqualität, die aus Relevanz, Konsistenz und emotionaler Verständigung besteht. Wenn du mehr dazu wissen möchtest, frag einfach!</sample>
    <sample id="946">Die Autoren gehören an die University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI und Microsoft STC Asia.</sample>
    <sample id="947">Wenn es um die Genauigkeit und Klarheit der Übersetzung geht. Also, wenn man die Bedeutung der Wörter und der Struktur im Satz beachten muss. Du hast noch weitere Fragen dazu?</sample>
    <sample id="978">Die Autoren haben die Dialogmodelle BART-FID-RAG, Blender2, Emora und Blender-Decode evaluiert. Wenn du mehr über diese Modelle erfahren möchtest, frag einfach!</sample>
    <sample id="979">Sechs Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="980">Nichts zu tun.</sample>
    <sample id="981">Sechs Autoren sind an der Arbeit beteiligt. Wenn du noch mehr Details wissen möchtest, frag einfach nach.</sample>
    <sample id="982">Der Referent ist Vasudha Varadarajan.</sample>
    <sample id="983">Die Autoren gehören der University of Warsaw an.</sample>
    <sample id="1021">Die häufigsten Fehler von PaLM sind "Accuracy/Omission" und "Style/Awkward".</sample>
    <sample id="1022">"Vergiss nicht deine ABCs: Die Bewertung der Top-Chat-Orientierten Dialogsysteme" von Sarah E. Finch, James D. Finch und Jinho D. Choi.</sample>
    <sample id="1023">"Vergiss nicht deine ABCs: Die Bewertung der Top-Chat-Orientierten Dialogsysteme" von Sarah E. Finch, James D. Finch und Jinho D. Choi.</sample>
    <sample id="1024">Kommunikation im Vergleich.</sample>
    <sample id="1025">Kompative Evaluierung Likert-Bewertungs-Evaluation</sample>
    <sample id="1026">Likert-Bewertungsevaluation. Dimensionen der Dialogqualität. Dialogqualität. Relevanz. Emotionaler Verständnis. Konsistenz.</sample>
    <sample id="1027">Dimensionen der Dialogqualität. Likert-Bewertungsevaluation. Bewertung der Relevanz der Antworten des Bots.</sample>
    <sample id="1028">Likert-Bewertungs-Evaluation. Bewertung der Relevanz der Antworten des Bots.</sample>
    <sample id="1029">Klar, ich kann den englischen Inhalt in die deutsche Sprache übersetzen. Bitte gib den englischen Text an.</sample>
    <sample id="1030">Klassifizierung von Verhaltensweisen in Chats, ABC-Eval. Unrelevant. Mangelndes Empathie. Selbstwidersprüchlichkeit.</sample>
    <sample id="1031">ABC-Eval Verhaltensweisen
Kohärenz
Wissen
Konsistenz
Emotionale Verständigung</sample>
    <sample id="1032">ABC-Eval Verhaltensweisen
Kohärenz
Ignorieren Partner
Irrelevant
Wissen
Konsistenz
Emotionale Verständigung</sample>
    <sample id="1033">ABC-Eval Verhaltensweisen Konsistenz Ignorieren Partner Irrelevant Konsistenz Selbstwidersprüche Partnerwidersprüche Wissen Falsche Tatsache Commonsense Verletzung Emotionaler Verständnis Empathische Reaktion Mangel an Empathie</sample>
    <sample id="1034">Experimente
- 4 offene Domänen Dialogmodelle
- 100 menschliche-roboterliche Konversationen pro Modell</sample>
    <sample id="1035">Experimente
- 4 Open-Domain-Dialog-Modelle
- 100 menschliche-roboterische Konversationen pro Modell
ABC-Eval
Turn Likert
Dialog Likert
Comparative</sample>
    <sample id="1036">Experimente
- 4 Offene-Domain-Dialog-Modelle
- 100 Mensch-Bot-Konversationen pro Modell
Baseline Evaluierungen
Turn Likert
- Konsistenz
- Emotionaler Verständnis
- Informationsreichtum
- Gesamtqualität
Dialog Likert
- Engagierend
- Grammatikalität
- Proaktivität
- Relevanz
Comparative</sample>
    <sample id="1037">Inter-Annotator-Übereinstimmung
Krippendorffs Alpha
ABC-Eval
Turn Likert
Dialogue Likert
Comparative
Inter-Annotator-Übereinstimmung
Krippendorffs Alpha
ABC-Eval
Turn Likert
Dialogue Likert
Comparative</sample>
    <sample id="1038">Inter-Annotator-Übereinstimmung. Prädiktive Gültigkeit.</sample>
    <sample id="1039">Kannst du mir sagen, was du übersetzen möchtest?</sample>
    <sample id="1040">Hier ist der deutsche Inhalt: "Prädiktive Validität

Steigende Gültigkeit

ABC-Eval
Turn Likert
Dialogue Likert

Emory University
NLP
Alexa"</sample>
    <sample id="1041">Hier ist der deutsche Inhalt: "Erweiternde Gültigkeit" "ABC-Eval" "Turn Likert" "Dialogue Likert" "R²" "Proaktiv" "Engagierend" "Informativ" "Emotion" "Relevant" "Empathisch" "Unempathisch" "Redundant" "Selbst Kontra." "Irrelevant" "CS Kontra." "Correct" "Ignore" "Follow up" "Other Contr." "Topic Switch" "Ticorrect Contr." "Topic Info Contr." "Inner Contr." "Antisocial" "Life Info" "Emotion Consistent" "Emotion Relevant" "Grammar Consistent" "Grammar Proactive" "Emotion Consistent" "Emotion Proactive" "Relevant" "Informative" "Grammar Proactive" "Emotion Proactive" "Relevant" "Informative" "Grammar Proactive" "Emotion Proactive" "Relevant" "Informative" "Grammar Proactive" "Emotion Proactive" "Relevant" "Informative" "Grammar Proactive" "Emotion Proactive" "Relevant" "Informative" "Grammar Proactive" "Emotion Pro</sample>
    <sample id="1042">Hier ist der deutsche Inhalt: "Erweiternde Gültigkeit" "ABC-Eval" "Turn Likert" "Dialogue Likert" "R²" "Proaktiv" "Engagierend" "Informativ" "Gerechtigkeit" "Relevant" "Emotion" "Unempathisch" "Redundant" "Selbstkonträr" "Irrelevant" "Empathisch" "Korrekt" "CS Konträr" "Antisozial" "Lebensinfo" "Interessent Konträr" "Thema Wechseln" "Interessent Konträr" "Innere Konträr" "Weiterleitung" "Weiterleitung Konträr" "Lebensinfo Konträr" "Lebensinfo Wechseln" "Lebensinfo Konträr" "Lebensinfo Wechseln Konträr" "Lebensinfo Wechseln Konträr" "Lebensinfo Wechseln Konträr" "Lebensinfo Wechseln Konträr" "Lebensinfo Wechseln Konträr" "Lebensinfo Wechseln Konträr" "Lebensinfo Wechseln Konträr</sample>
    <sample id="1043">ABC-Eval Fehlerquoten nach Modell</sample>
    <sample id="1044">ABC-Evaluierung Fehlerquoten nach Modell</sample>
    <sample id="1045">ABC-Eval-Fehlerquoten nach Modell</sample>
    <sample id="1046">ABC-Eval-Fehlerquoten nach Modell</sample>
    <sample id="1047">ABC-Eval Fehlerquoten nach Modell
BART-FID-RAG, Blender2, Emora, Blender-Decode
Danke fürs Zuschauen!
Papier: https://arxiv.org/pdf/2212.09180.pdf
GitHub: https://github.com/emorynlp/ChatEvaluationPlatform
Kontaktinformationen: {sfillwo, jdfinch, jinho.choi} @emory.edu
https://www.emorynlp.org</sample>
    <sample id="1048">Die Autoren gehören der Emory University an.</sample>
    <sample id="1049">CFT steht für Continuous Fine-tuning.</sample>
    <sample id="1050">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1051">Wann erfordert die Übersetzung Kontext? Eine datenbasierte, multilingualen Erforschung. Patrick Fernandes, Kayo Yin, Emmy Liu, André F. T. Martins, Graham Neubig. Carnegie Mellon University Language Technologies Institute, TÉCNICO LISBOA, BAIR Berkeley Artificial Intelligence Research, Unbabel. *gleiche Beitragung.</sample>
    <sample id="1052">Wir müssen diesen Spitzel loswerden.</sample>
    <sample id="1053">Die Übersetzung hängt vom Kontext ab.</sample>
    <sample id="1054">Kann es etwas Ernstes sein, Doktor? Wir müssen diesen Fleck loswerden.</sample>
    <sample id="1055">Die Bewertung von kontextabhängiger Übersetzung ist schwierig. Nur ein kleiner Teil der Wörter hängt vom Kontext ab.</sample>
    <sample id="1056">Die Bewertung von kontextabhängiger Übersetzung ist schwierig. Nur ein kleiner Teil der Wörter hängt vom Kontext ab. Korpus-basierte Maße. Bestehende Methoden unterstützen begrenzte Diskursphänomene und Sprachen.</sample>
    <sample id="1057">RQ1: Wann erfordert die Übersetzung Kontext? RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1058">RQ1: Wann erfordert die Übersetzung Kontext? - Wortebene Kontextnutzung RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1059">Kontextbedingte Kreuz-Mutual Information, CXMI, ist eine Methode, um zu messen, wie viel Kontext-MT-Modelle in einem Korpus verwenden.</sample>
    <sample id="1060">Kontextbedingte Kreuz-Mutual Information, CXMI, ist ein Maß dafür, wie viel Kontext-MT-Modelle in einem Korpus verwenden.</sample>
    <sample id="1061">Punktweises, P-CXMI, wir führen P-CXMI ein, um den Kontextgebrauch zur Übersetzung eines spezifischen, P-CXMI, y, x, C, = - log, q, MT, A, y | x, / q, MT, C, y | x, C, P-CXMI, i, y, x, C, = - log, q, MT, A, y, i | y, t, &lt;, i, x, / q, MT, C, y, i | y, t, &lt;, i, x, C, hohe P-CXMI-Wörter -&gt; benötigen Kontext zur Übersetzung,</sample>
    <sample id="1062">RQ1: Wann erfordert die Übersetzung Kontext? - Wortlevel-Kontextnutzung - Themenanalyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen?</sample>
    <sample id="1063">Themenanalyse von hochwertigen P-CXMI-Wörtern. Frage 1: Wann erfordert die Übersetzung Kontext? - Wortebene Kontextnutzung - Themenanalyse Frage 2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? TED - Ideen wertvoll verbreiten</sample>
    <sample id="1064">Themenanalyse von hoch bewerteten P-CXMI-Wörtern. 1. POS-Tags.</sample>
    <sample id="1065">Themenanalyse hochwertiger P-CXMI-Wörter. 1. POS-Tags. - Pronomen.</sample>
    <sample id="1066">Themenanalyse von hochwertigen P-CXMI-Wörtern. 1. POS-Tags. - Pronomen. - Verbform. 2. Vokabulargegenstände.</sample>
    <sample id="1067">1. POS-Tags
2. Vokabular-Elemente
- Pronomen
- Verbform
- Lexikalische Kohäsion</sample>
    <sample id="1068">Aveliles Mutter war noch schlafend. Avelile ging zur Schule.</sample>
    <sample id="1069">Sie weiß, wohin wir gehen. Ich weiß es nicht.</sample>
    <sample id="1070">RQ1: Wann erfordert die Übersetzung Kontext? - Wortebene Kontextnutzung - Themenanalyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? - Multilingual Discourse-Aware, MuDA, Benchmark</sample>
    <sample id="1071">RQ1: Wann erfordert die Übersetzung Kontext? - Wortlevel-Kontextnutzung - Themenanalyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? - Multilingual Discourse-Aware, MuDA, Benchmark</sample>
    <sample id="1072">Multilingual Discourse-Aware, MuDA, Tagger. - Pronouns, - Verb form, - Lexikalische Kohäsion, - Formalität, - Ellipsen.</sample>
    <sample id="1073">Multilingual Discourse-Aware, MuDA, tagger. Pronouns, Verb form, Lexical cohesion, Formality, Ellipsis. MuDA benchmark. MuDA tagger. BLEU, COMET, F-measure.</sample>
    <sample id="1074">RQ1: Wann erfordert die Übersetzung Kontext? - Wortebene Kontextnutzung - Themenanalyse RQ2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? - Multilingual Discourse-Aware, MuDA, Benchmark - Modellbewertung</sample>
    <sample id="1075">Korpus-niveau-Metriken. Frage 1: Wann erfordert die Übersetzung Kontext? - Wort-niveau-Kontext-Nutzung - Themenanalyse. Frage 2: Wie gut handhaben Modelle kontextabhängige Übersetzungen? - Multilingual Discourse-Aware, MuDA, Benchmark - Modellbewertung.</sample>
    <sample id="1076">Korpus-niveau-Metriken. BLEU. CONTEXT. COMET. CONTEXT. F-measure.</sample>
    <sample id="1077">Korpus-niveau-Metriken. BLEU: Kein Kontext. COMET: Kontext. F-Maß: Kontext. Unklar, welches System am besten für die dokumentenbasierte MT mit korpus-niveau-Metriken geeignet ist.</sample>
    <sample id="1078">Korpus-niveau-Metriken
- Unklar welches System am besten für Dokument-niveau-MT mit korpus-niveau-Metriken ist
MuDA-Benchmark-Ergebnisse
- Kontext-bewusste Modelle performieren bei einigen Phänomenen signifikant besser</sample>
    <sample id="1079">MuDA-Benchmark-Ergebnisse

- Kontext-bewusste Modelle performieren bei einigen Phänomenen signifikant besser
  - ✅ Formalität, lexikalische Kohäsion
  - ❌ Ellipsen, Pronomen, Verbform</sample>
    <sample id="1080">MuDA-Benchmark-Ergebnisse

- Kontext-bewusste Modelle leisten bei einigen Phänomenen signifikant besser
  - ✅ Formalität, lexikalische Kohäsion ❌ Ellipsen, Pronomen, Verbform
- DeepL übertrifft Google bei den meisten Phänomenen und Sprachpaaren*</sample>
    <sample id="1081">MuDA-Benchmark-Ergebnisse: Kontext-bewusste Modelle leisten bei einigen Phänomenen signifikant besser. ✅ Formalität, lexikalische Kohäsion ❌ Ellipsen, Pronomen, Verbform. Zusammenfassung: Systematisch Diskursphänomene ohne vorherige linguistische Kenntnisse identifizieren. ✅ Formalität, lexikalische Kohäsion ❌ Ellipsen, Pronomen, Verbform. Dataset-agnostischer Benchmark für dokumentenbasierte MT. DeepL übertrifft Google bei den meisten Phänomenen und Sprachpaaren.</sample>
    <sample id="1082">Zusammenfassung
- Systematisch Diskursphänomene identifizieren ohne vorherige linguistische Kenntnisse
- Dataset-agnostischer Benchmark für dokumentenbasierte MT</sample>
    <sample id="1083">- Systematisch Diskursphänomene identifizieren ohne vorherige linguistische Kenntnisse, - Dataset-agnostischer Benchmark für dokumentenbasierte MT</sample>
    <sample id="1084">Der Referent ist Yusen Zhang.</sample>
    <sample id="1121">Diese Methode hat den Namen "Permuting with 'jumps'".</sample>
    <sample id="1122">The authors describe the method of "marked words" as finding words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">Die Autoren gehören an die Paul G. Allen School, die University of Washington, die Carnegie Mellon University und Peking University.</sample>
    <sample id="1124">Bouquet/Stanford.</sample>
    <sample id="1125">Der Referent ist Sarah E. Finch.</sample>
    <sample id="1126">Es sind vier Autoren an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne wissen.</sample>
    <sample id="1127">BLiMP, SyntaxGym und Crows.</sample>
    <sample id="1161">FTw, BOND, COSINE, MLC, L2R. Wenn du noch Fragen hast, lass es mich wissen!</sample>
    <sample id="1162">Das Modell wird anhand von 11 Aufgaben evaluiert.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich mit 4GB von Daten trainiert.</sample>
    <sample id="1227">Der Referent ist Adam Przepiórkowski.</sample>
    <sample id="1228">Die Ergebnisse, die in der Tabelle und dem Diagramm zu sehen sind, zeigen, dass die Leistung mit größerem zeitlichem Abstand abnimmt. Im Diagramm ist eine steigende Tendenz der Leistungsabnahme mit zunehmendem Jahr zu erkennen. Im Text wird explizit erwähnt, dass die Leistung mit größerem zeitlichem Abstand abnimmt und das die Hauptursache für den Leistungsverlust ist. Wenn du noch weitere Fragen hast, lass es mich wissen!</sample>
    <sample id="1269">Well, it's necessary to permute the tokens for the output sequence because it helps in better understanding and processing of the sequence. It allows for more accurate tagging and analysis. If you want to know more about how it actually works, just let me know.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparenter machen sollten, um die Transparenz über die Bias-Mitigation zu verbessern. So können andere besser verstehen, wie die Modelle funktionieren und wie Vorurteile vermieden werden. Wenn du noch mehr Details wissen möchtest, frag einfach nach.</sample>
    <sample id="1271">Inacceptable minimal pair inputs are sentences like "Many people were helping themselves" and "Many people were helping herself."</sample>
    <sample id="1272">Die Autoren haben F1-Score, Precision und Recall verwendet.</sample>
    <sample id="1273">Die Metrik, die verwendet wurde, um die Übereinstimmung zwischen den Kommentatoren zu messen, ist Krippendorff's Alpha.</sample>
    <sample id="1274">Die Domain, die für die hinzufügung von völlig unzusammenhängenden Sätzen zu den inakzeptablen und akzeptablen Suchanfragen gewählt wurde, ist Wikipedia.</sample>
    <sample id="1275">Die Autoren gehören der Heinrich Heine University Düsseldorf an.</sample>
    <sample id="1276">MultiInstruct unterscheidet sich von anderen Benchmarks durch die Fokussierung auf die Anpassung von Multimodalen Vorkondensierungsmodellen. Es gibt aber auch eine Ungleichgewichtsproblematik in den instruktionalen Datensätzen zwischen NLP und Multimodal. Wenn du mehr darüber wissen möchtest, lass es mich wissen.</sample>
    <sample id="1277">Drei.</sample>
    <sample id="1278">Die binäre Koordination ist die Verbindung von zwei Elementen. Wenn du mehr darüber wissen möchtest, frag einfach nach.</sample>
    <sample id="1279">Leider ist in dem Bild nichts über die Länge der verwendeten Prompts zu sehen. Also kann ich keine Antwort darauf geben. Wenn du mehr Informationen hast, teile sie bitte mit.</sample>
    <sample id="1280">Die Ergebnisse haben eine positive Auswirkung auf das kleinere T5-Modell. Es kann höherwertige Skripte generieren als die größeren LLMs. Wenn du noch weitere Fragen hast, lass es mich wissen.</sample>
    <sample id="1281">DrBERT: Ein robustes vortrainiertes Modell auf Französisch für biomedizinische und klinische Bereiche.Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, Pierre-Antoine Gourraud.LIA, Avignon Université, LS2N, Nantes Université, Clinique des données, CHU de Nantes, Zenidoc.LS2N, Laboratoire des Sciences du Numérique de Nantes, Nantes Université, Centre Hospitalier Universitaire de Nantes, iD, Laboratoire Informatique d'Avignon, GENCI, Le calcul intensif au service de la connaissance, Avignon Université.</sample>
    <sample id="1282">I. Sprachmodellierung im Gesundheitswesen
II. Vergleich von Vortrainingsstrategien, Datensätzen und Größen
III. Bewertung von 13 Modellen anhand von 11 Aufgaben
IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1283">I. Sprachmodellierung im Gesundheitswesen II. Vergleich von Vortrainingsstrategien, Datensätzen und Größen III. Bewertung von 13 Modellen an 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1284">I. Sprachmodellierung im Gesundheitswesen II. Vergleich von Vortrainingsstrategien, Datensätzen und Größen III. Bewertung von 13 Modellen an 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1285">I. Sprachmodellierung im Gesundheitswesen, Transformer-basierte Ansätze wie BERT bieten bei vielen NLP-Aufgaben einen riesigen Leistungszuwachs, Es wurde mit CamemBERT und FlauBERT auf Französisch adaptiert, II. Vergleich von Vortrainingsstrategien, Datensätzen und Größen, PudMedBERT, BioBERT, ClinicalBERT und andere, III. Bewertung von 13 Modellen an 11 Aufgaben, Sprachen außer Englisch sind seltener und stützen sich hauptsächlich anhand der kontinuierlichen Vortraining mit einem bestehenden allgemeinen Modell, IV. Verteilung von NACHOS und DrBERT, Im Gegensatz zu allgemeinen Modellen ist noch kein Open-Source-Modell für den biomedizinischen Bereich in Französisch verfügbar, ein BERT-basierter domänenspezifisches Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1286">Natürlich! Hier ist der englische Inhalt in die deutsche Sprache übersetzt:.- Transformer-basierte Ansätze, wie BERT, bieten bei vielen NLP-Aufgaben einen riesigen Leistungszuwachs.- Es wurde für Französisch mit CamemBERT und FlauBERT adaptiert.- Bei medizinischen Aufgaben haben domain-spezifische Modelle in Englisch die Leistung noch weiter verbessert.- PudMedBERT, BioBERT, ClinicalBERT und andere.- Sprachen außer Englisch sind seltener und stützen sich hauptsächlich anhand der fortlaufenden Vortraining mit einem bestehenden generischen Modell.- Im Gegensatz zu generischen Modellen gibt es noch keine Open-Source-Modell für den biomedizinischen Bereich in Französisch.- Ein BERT-basierter domain-spezifisches Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1287">Natürlich! Hier ist der englische Inhalt in die deutsche Sprache übersetzt: "Sprachmodellierung". - Transformer-basierte Ansätze, wie BERT, bieten bei vielen NLP-Aufgaben einen riesigen Leistungszuwachs. - Es wurde für Französisch mit CamemBERT und FlauBERT adaptiert. - Bei medizinischen Aufgaben haben spezifische Modelle in Englisch die Leistung noch weiter verbessert. - PudMedBERT, BioBERT, ClinicalBERT und andere. - Sprachen außer Englisch sind seltener und stützen sich hauptsächlich anhand der fortlaufenden Vortraining an einem bestehenden generischen Modell. - Im Gegensatz zu generischen Modellen gibt es noch keine Open-Source-Modell für den biomedizinischen Bereich in Französisch. - Ein BERT-basierter spezifischer Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern."</sample>
    <sample id="1288">Natürlich! Hier ist der englische Inhalt in die deutsche Sprache übersetzt:.- Transformer-basierte Ansätze, wie BERT, bieten bei vielen NLP-Aufgaben einen riesigen Leistungszuwachs.- Es wurde für Französisch mit CamemBERT und FlauBERT adaptiert.- Bei medizinischen Aufgaben haben spezifische Modelle in Englisch die Leistung noch weiter verbessert.- PudMedBERT, BioBERT, ClinicalBERT und andere.- Sprachen außer Englisch sind seltener und stützen sich hauptsächlich anhand der fortlaufenden Vortraining an einem bestehenden generischen Modell.- Im Gegensatz zu generischen Modellen gibt es noch keine Open-Source-Modell für den biomedizinischen Bereich in Französisch.- Ein BERT-basierter spezifischer Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1289">- Transformer-basierte Ansätze, wie BERT, bieten bei vielen NLP-Aufgaben einen riesigen Leistungszuwachs. - Es wurde auf Französisch mit CamemBERT und FlauBERT angepasst. - Bei medizinischen Aufgaben haben domain-spezifische Modelle in Englisch die Leistung noch weiter verbessert. - PudMedBERT, BioBERT, ClinicalBERT und andere. - Sprachen außer Englisch sind seltener und stützen sich hauptsächlich anhand der kontinuierlichen Vorkonditionierung an einem bestehenden generischen Modell. - Im Gegensatz zu generischen Modellen gibt es noch keine Open-Source-Modell für den biomedizinischen Bereich auf Französisch. - Ein BERT-basierter domain-spezifisches Modell auf Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1290">Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten aus verschiedenen medizinischen Domänen, Naturen und Stilen
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen des Nantes University Hospital Data Warehouse stammen
- Vergleich von Lernstrategien
  - Von Scratch mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinischer Modell)

Tabelle:

| Korpus | Größe | Anzahl der Wörter | Anzahl der Sätze |
| --- | --- | --- | --- |
| NACHOSlarge, öffentlich | 7,4 GB</sample>
    <sample id="1291">Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten aus verschiedenen medizinischen Domänen, Naturen und Stilen
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen des Nantes University Hospital Data Warehouse stammen
- Vergleich von Lernstrategien
  - Von Null anfangen mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinischer Modell)</sample>
    <sample id="1292">Vergleich von Vortrainingsstrategien und Datensätzen.Evaluierung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen.NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten, die aus verschiedenen medizinischen Domänen, Naturen und Stilen gesammelt wurden.NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen des Nantes University Hospital Data Warehouse stammen.Vergleich von Lernstrategien.Von Scratch mit vollständiger Modellkonstruktion.Continual Pre-Training mit einem bestehenden Vortrainingsmodell (hier CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinischer Modell).</sample>
    <sample id="1293">Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten aus verschiedenen medizinischen Domänen, Naturen und Stilen
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen aus dem Datenlager des Universitätsklinikums Nantes stammen
- Vergleich von Lernstrategien
  - Von Scratch mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinischer Modell)</sample>
    <sample id="1294">Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten aus verschiedenen medizinischen Domänen, Naturen und Stilen
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen des Nantes University Hospital Data Warehouse stammen
- Vergleich von Lernstrategien
  - Von Scratch mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinischer Modell)</sample>
    <sample id="1295">Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten aus verschiedenen medizinischen Domänen, Naturen und Stilen
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen des Nantes University Hospital Data Warehouse stammen
- Vergleich von Lernstrategien
  - Vom Nullpunkt mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinisches Modell)</sample>
    <sample id="1296">Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großer Open-Source-Datensatz heterogener Daten aus verschiedenen medizinischen Domänen, Naturen und Stilen
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen aus dem Datenlager des Universitätsklinikums Nantes stammen
- Vergleich von Lernstrategien
  - Von Null anfangen mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinisches Modell)</sample>
    <sample id="1297">Klar, ich kann das für dich übersetzen. Hier ist die deutsche Übersetzung: "Vergleich von Vortrainingsstrategien und Datensätzen

- Bewertung der Auswirkung öffentlicher und privater medizinischer Datensätze auf vergleichbare Datenmengen
  - NACHOS: Ein 1,1 Milliarden Wörter großes Open-Source-Datensatz heterogener Daten, die aus verschiedenen medizinischen Domänen, Naturen und Stilen gesammelt wurden
  - NBDW: Ein privater Datensatz von Sätzen, die aus 1,7 Millionen anonymisierten medizinischen Aufzeichnungen extrahiert wurden, die aus dem Datenlager des Universitätsklinikums Nantes stammen
- Vergleich von Lernstrategien
  - Vom Nullpunkt mit vollständiger Modellkonstruktion
  - Fortlaufende Vortraining mit einem bestehenden Vortrainingsmodell (hier: CamemBERT, ein französischer generischer Modell, und PubMedBERT, ein englisch-basierter medizinischer Modell)"

Wenn</sample>
    <sample id="1298">Bewertung: Datenquellen und Größe
- Leistungsbewertung von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch privat
- Unsere abgestimmten Modelle erzielen fast auf allen Aufgaben state-of-the-art-Ergebnisse</sample>
    <sample id="1299">Bewertung: Datenquellen und Größe
- Leistungsbewertung von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch privat
- Unsere abgestimmten Modelle erzielen fast auf allen Aufgaben state-of-the-art-Ergebnisse</sample>
    <sample id="1300">Bewertung: Datenquellen und Größe
- Leistungsbewertung von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch privat
- Unsere abgestimmten Modelle erzielen fast auf allen Aufgaben state-of-the-art-Ergebnisse</sample>
    <sample id="1301">Bewertung: Datenquellen und Größe
- Leistungsbewertung von 13 Modellen auf 11 Aufgaben, sowohl öffentlich als auch privat
- Unsere abgestimmten Modelle erzielen fast auf allen Aufgaben state-of-the-art-Ergebnisse</sample>
    <sample id="1302">Bewertung: Vorkonditionierungsstrategien
- Ausgangspunkt vs. fortlaufende Vorkonditionierung auf 4 GB Daten
- Frage-Antwort-Aufgaben erfordern mehr spezifisches Wissen des Bereichs, um gut funktionieren zu können
- Eine Studie zur Modellstabilität zeigt eine höhere Inter-Run-Variabilität für die CamemBERT-basierten Modelle, die mit fortlaufender Vorkonditionierung trainiert wurden

CamemBERT OSCAR 138 GB
CamemBERT OSCAR 4 GB
CamemBERT CCNET 4 GB
PubMedBERT
ClinicalBERT
BioBERT v1.1
DrBERT NACHOS,large
DrBERT NACHOS,small
ChuBERT NBDW,small
ChuBERT NBDW,mixed
CamemBERT NACHOS,small
PubMedBERT NACHOS,small
CamemBERT NBDW,small

Avignon Université</sample>
    <sample id="1303">Bewertung: Vorkonditionierungsstrategien
- Ausgangslage vs. fortlaufende Vorkonditionierung auf 4 GB Daten
- Frage-Antwort-Aufgaben erfordern mehr spezifisches Domänenwissen, um gut funktionieren zu können
- Eine Studie zur Modellstabilität zeigt eine höhere Inter-Run-Variabilität für die CamemBERT-basierten Modelle, die mit fortlaufender Vorkonditionierung trainiert wurden

CamemBERT OSCAR 138 GB NER: 35.13, CLS: 80.13, Medical Report: 89.35, Specialties: 99.20, MUSCA-DET: 88.54, MUSCA-DET: 88.20, ESSAI: 81.10, POS: 95.22, CAS: 36.24, POS: 16.55, Hamming: 90.71, EMR: 77.41 CamemBERT OSCAR 4 GB NER:</sample>
    <sample id="1304">Bewertung: Vorkonditionierungsstrategien
- Von Scratch vs. fortlaufendes Vorkonditionieren auf 4 GB Daten
- Frage-Antwort-Aufgaben erfordern mehr spezifisches Wissen des Bereichs, um gut funktionieren zu können
- Eine Studie zur Modellstabilität zeigt eine höhere Inter-Run-Variabilität für die CamemBERT-basierten Modelle, die mit fortlaufendem Vorkonditionieren trainiert wurden

CamemBERT OSCAR 138 GB
CamemBERT OSCAR 4 GB
CamemBERT CCNET 4 GB
PubMedBERT
ClinicalBERT
BioBERT v1.1
DrBERT NACHOS,large
DrBERT NACHOS,small
ChuBERT NBDW,small
ChuBERT NBDW,mixed
CamemBERT NACHOS,small
PubMedBERT NACHOS,small
CamemBERT NBDW,small

Avignon Université</sample>
    <sample id="1305">Kernbotschaft: DrBERT erzielt topmoderne Ergebnisse in 9 französisch medizinisch ausgerichteten Aufgaben. Übertrifft CamemBERT generischen Modell und englisch-basierte domänenspezifische Modelle. Bestätigt die Nützlichkeit des Trainings eines medizinisch spezifischen Modells auf Französisch. Datenquellen sind wichtig: Das Trainieren an heterogenen Daten ist wichtig. NACHOS ist robuster als nur private klinische Daten zu verwenden. Mehr Daten ist besser, aber nicht skaliert gut. Fortlaufendes Prainieren ist eine effektivere Strategie, wenn auf domänenspezifischen englischen Modellen basiert. Die DrBERT-Modelle, der NACHOS-Datensatz und die Trainings-Skripte sind unter der MIT-Lizenz frei verfügbar.</sample>
    <sample id="1306">Kernbotschaft
- DrBERT erzielt topmoderne Ergebnisse bei 9 französisch medizinisch ausgerichteten Aufgaben
  - Übertrifft CamemBERT generischen Modell und englisch-basierte domänenspezifische Modelle
  - Bestätigt die Nützlichkeit des Trainings eines medizinisch spezifischen Modells auf Französisch
- Datenquellen sind wichtig: Das Trainieren mit heterogenen Daten ist wichtig
  - NACHOS ist robuster als nur private klinische Daten zu verwenden
- Mehr Daten ist besser, aber nicht skalierbar
- Fortlaufendes Prainieren ist eine effektivere Strategie, wenn es auf domänenspezifischen englischen Modellen basiert
- Die DrBERT-Modelle, der NACHOS-Datensatz und die Trainings-Skripte sind frei verfügbar unter der MIT-Lizenz</sample>
    <sample id="1307">Kernbotschaft: DrBERT erzielt topmoderne Ergebnisse in 9 französisch medizinisch ausgerichteten Aufgaben. Übertrifft CamemBERT generischen Modell und englisch-basierte domänenspezifische Modelle. Bestätigt die Nützlichkeit des Trainings eines medizinisch spezifischen Modells auf Französisch. Datenquellen sind wichtig: Das Trainieren an heterogenen Daten ist wichtig. NACHOS ist robuster als nur private klinische Daten zu verwenden. Mehr Daten ist besser, aber nicht skaliert gut. Fortlaufendes Prainieren ist eine effektivere Strategie, wenn es auf domänenspezifischen englischen Modellen basiert. Die DrBERT-Modelle, das NACHOS-Datensatz und die Trainings-Skripte sind unter der MIT-Lizenz frei verfügbar.</sample>
    <sample id="1308">Vielen Dank. Auf dem Poster steht: "Ich freue mich auf den Austausch bei der Poster-Session in Toronto!" Weitere Informationen unter: drbert.univ-avignon.fr</sample>
    <sample id="1309">Die untersuchten Lernstrategien sind "From scratch with full model construction" und "Continual pre-training using an existing pre-trained model". Wenn du noch mehr Details wissen möchtest, frag einfach!</sample>
    <sample id="1310">Nicht bekannt.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde anhand von SARI, BLEU, BS-P und FRE beurteilt. So zum Beispiel bei DEplain-APA im Document Level mit 44.56% SARI, 38.136 BLEU, 0.598 BS-P und 65.4% FRE. Bei DEplain-APA im Sentence Level waren die Werte 34.818 SARI, 28.25 BLEU, 0.639 BS-P und 63.072 FRE.</sample>
    <sample id="1312">Ja, Sprachmodelle können unterschiedliche politische Vorurteile haben.</sample>
    <sample id="1313">Kompositionelle Generalisierung ohne Bäume mittels Multiset-Tagging und versteckter Permutationen. Matthias Lindemann, Alexander Koller, Ivan Titov.</sample>
    <sample id="1314">Kompositionelle Generalisierung ohne Bäume mittels Multiset-Tagging und versteckter Permutationen. Matthias Lindemann, Alexander Koller, Ivan Titov.</sample>
    <sample id="1315">Kompositionelle Generalisierung

Fähigkeit eines Lerners, tiefere Rekursion und unerwartete Kombinationen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden.</sample>
    <sample id="1316">Kompositionelle Generalisierung in Semantischer Parsing. Trainieren: "Die Mädchen schliefen." *Mädchen x, Schlafagent x, x, x, "Mary wusste, dass die Mädchen schliefen." *Mädchen x, Wissensagent x, Mary und Wissenskomp x, x, x, Schlafagent x, x, x,</sample>
    <sample id="1317">Kompositionelle Generalisierung in Semantischer Parsing. Trainieren: "Die Mädchen schliefen." *Mädchen x, Schlafagent x, x, "Mary wusste, dass die Mädchen schliefen." *Mädchen x, Wissen-Agent x, Mary und Wissen-Komplement x, x, Schlafagent x, x,</sample>
    <sample id="1318">Koexistenz von Generalisierung und spezifischem Wissen.</sample>
    <sample id="1319">Koexistenz von Sprache und Bild. Trainieren: "Die Mädchen schliefen." *Mädchen x, Schlafagent x, x, x. "Mary wusste, dass die Mädchen schliefen." *Mädchen x, Wissenagent x, Mary und Wissenccomp x, x, x. Test: "Jim sagte, dass Mary wusste, dass die Mädchen schliefen." *Mädchen x, Sagenagent x, Jim und Sagenccomp x, x, x, Wissenagent x, Mary und Wissenccomp x, x, x, Schlafagent x, x, x.</sample>
    <sample id="1320">Kompositionelle Generalisierung in Semantischer Parsing. Trainieren: "Die Mädchen schliefen." *Mädchen x, Schlafagent x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x</sample>
    <sample id="1321">Kompositionelle Generalisierung in Semantischer Parsing

Trainieren:
- "Die Mädchen schliefen." -&gt; "*Mädchen x, Schlafagent x, x"
- "Mary wusste, dass die Mädchen schliefen." -&gt; "*Mädchen x, Wissenagent x, Mary und Wissenccomp x, x, x"

Testen:
- "Jim sagte, dass Mary wusste, dass die Mädchen schliefen." -&gt; "*Mädchen x, Sagenagent x, Jim und Sagenccomp x, x, x, Wissenagent x, Mary und Wissenccomp x, x, x, Schlafagent x, x"

Naive seq2seq Modelle scheitern!</sample>
    <sample id="1322">Bäume helfen sehr, aber... *Mädchen x₁; Schlafagent x₂ x₁ *Mädchen x₁; x₁ Schlafagent x₂ Das Mädchen schlief.</sample>
    <sample id="1323">Bäume helfen sehr, aber... *Mädchen x₁; Schlafagent x₂ x₁ *Mädchen x₁; x₁ Schlafagent x₂ Das Mädchen schlief.</sample>
    <sample id="1324">Bäume helfen sehr, aber... *Mädchen x₁; Schlafagent x₂ x₁ *Mädchen x₁; x₁ Schlafagent x₂ Die Mädchen schliefen. Bäume müssen erhalten werden: - Vor-/Nachverarbeitung logischer Formen</sample>
    <sample id="1325">Bäume helfen sehr, aber... *girl x₁; sleep.agent x₂ x₁ *girl x₁; x₁ sleep.agent x₂ Die Mädchen schliefen. Bäume müssen erhalten werden: - Vorer- und Nachbearbeitung logischer Formen</sample>
    <sample id="1326">Bäume helfen sehr, aber... *girl x₁; sleep.agent x₂ x₁ *girl x₁; x₁ sleep.agent x₂ The girl slept. Bäume müssen erhalten werden: - Vor-/Nachverarbeitung logischer Formen - Grammatik-Induktion</sample>
    <sample id="1327">Bäume helfen sehr, aber… *girl x₁; sleep.agent x₂ x₁ *girl x₁; x₁ sleep.agent x₂ The girl slept. Bäume müssen erfasst werden: - Vorer- und Nachbearbeitung logischer Formen - Grammatik-Induktion Dieses Papier: neuronales seq2seq-Modell, das direkte Korrespondenzen zwischen Fragmenten modelliert. Zum ersten Mal zeigen wir starke Generalisierung zu tieferer Rekursion ohne Bäume.</sample>
    <sample id="1328">Bäume helfen sehr, aber... *girl x₁; sleep.agent x₂ x₁ *girl x₁; x₁ sleep.agent x₂ The girl slept. Bäume müssen erfasst werden: - Vorer- und Nachbearbeitung logischer Formen - Grammatik-Induktion Dieses Papier: neuronales seq2seq-Modell, das direkt die Korrespondenzen zwischen Fragmenten modelliert. Zum ersten Mal zeigen wir eine starke Generalisierung zu tieferer Rekursion ohne Bäume.</sample>
    <sample id="1329">Unser Ansatz</sample>
    <sample id="1330">Unser Ansatz</sample>
    <sample id="1331">Unser Ansatz
- grün: * ; grün: the
- gelb: x1, girl, x1
- blau: sleep, agent, x2
- Tag: girl, slept</sample>
    <sample id="1332">Unser Ansatz
Permutieren
Tag</sample>
    <sample id="1333">Unser Ansatz, Permutieren, Tag</sample>
    <sample id="1334">Permutieren mit "Sprüngen"</sample>
    <sample id="1335">Permutieren mit "Sprüngen"</sample>
    <sample id="1336">Permutieren mit "Sprüngen"</sample>
    <sample id="1337">Permutieren mit "Sprüngen"</sample>
    <sample id="1338">Permutieren mit "Sprüngen"</sample>
    <sample id="1339">Einige Ergebnisse auf COGS (Kim und Linzen 2020) Vergleich mit anderen baumlosen Modellen in Bezug auf strukturelle Generalisierung auf COGS.</sample>
    <sample id="1340">Einige Ergebnisse auf COGS, Kim und Linzen 2020 Vergleich mit anderen baumlosen Modellen bei struktureller Generalisierung auf COGS.</sample>
    <sample id="1341">Technische Herausforderungen die wir lösen. Permutieren. Fragezeichen. Tag. Ausrichtung unbekannt.</sample>
    <sample id="1342">Technische Herausforderungen die wir lösen. Permutation. Fragezeichen. Tag. Ausrichtung unbekannt.</sample>
    <sample id="1343">Technische Herausforderungen die wir lösen. * grün, girl, x₁ ; grün, sleep, agent, x₂, x₁ Permute ? ? ? Tag ? ? ? the girl slept Alignment unbekannt. &gt; Induzieren in der Training.</sample>
    <sample id="1344">Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Hera</sample>
    <sample id="1345">Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Hera</sample>
    <sample id="1346">Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Herausforderungen die wir lösen.Technische Hera</sample>
    <sample id="1347">Kognitive Dissonanz ist, wenn zwei Elemente der Kognition, wie z.B. Gedanken, Handlungen oder Überzeugungen, nicht übereinstimmen.</sample>
    <sample id="1348">Alpaca.</sample>
    <sample id="1349">Nicht immer. Es hängt von der spezifischen Situation ab. Manchmal ist iteratives Training besser. Du solltest es je nachdem testen, was dein Ziel ist. Wenn du mehr dazu wissen möchtest, frag einfach weiter.</sample>
    <sample id="1350">Sara Papi.</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von TED Talks. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="1385">Der Referent ist Matthias Lindemann.</sample>
    <sample id="1386">Sprachübergreifender Transfer ist der Prozess, bei dem ein Modell auf einer Quellsprache trainiert wird und dann auf eine andere Sprache übertragen wird. So wie in der Abbildung gezeigt, kann man beispielsweise ein Modell auf Englisch trainieren und dann auf Deutsch übertragen.</sample>
    <sample id="1387">Drei der Autoren gehören der Saarland University an, einer der University of Vienna und einer von Amazon Alexa.</sample>
    <sample id="1388">The authors use BLEU as a quality measure and AL / AL-CA as a latency measure. If you have any other questions about this, feel free to ask!</sample>
    <sample id="1389">The KITMUS Test. Evaluating Knowledge Integration from Multiple Sources. Akshatha Arodi, Martin Pömsl, McGill University/Mila. Kaheer Suleman, Microsoft Research. Adam Trischler, Microsoft Research. Alexandra Olteanu, McGill University/Mila. Jackie CK Cheung, McGill University/Mila.</sample>
    <sample id="1390">NLU-Modelle greifen auf mehrere Wissensquellen zurück. Wissen in den Parametern, also vor der Trainingszeit, und Wissen im Kontext, also während der Inferenzzeit.</sample>
    <sample id="1391">NLU-Modelle greifen auf mehrere Wissensquellen zurück. Wissen in den Parametern, vor der Trainingszeit, und Wissen im Kontext, während der Inferenzzeit.</sample>
    <sample id="1392">John sah den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1393">John sah den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1394">John sah den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1395">John sah den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1396">- Datensatz zur Beurteilung der Wissensintegration
- Coreferenz-Auflösungsaufgabe, um die Fähigkeit zu prüfen, auf
  - prätrainierte Wissen
  - inferenzzeitliches Wissen
- Experiment mit
  - menschlichen Studienteilnehmern
  - Coreferenz-Auflösungsmodellen</sample>
    <sample id="1397">- Datensatz zur Beurteilung der Wissensintegration
- Coreferenz-Auflösungsaufgabe, um die Fähigkeit zu prüfen, auf
  - prätrainierte Wissen
  - inferenzzeitliches Wissen
- Experiment mit
  - menschlichen Studienteilnehmern
  - Coreferenz-Auflösungsmodellen</sample>
    <sample id="1398">Servin ist Richter. Kea ist Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gericht, entschieden Servin Fälle, war er froh, sich auszuruhen. [Antwort: Servin]</sample>
    <sample id="1399">Servin ist Richter. Kea ist Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gericht, bei dem er Fälle entschied, war er froh, sich auszuruhen. Antwort: Servin</sample>
    <sample id="1400">Servin ist Richter. Kea ist Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gericht, bei dem er Fälle entschied, war er froh, sich auszuruhen. Antwort: Servin. 1) Spezifische Kenntnisse über Entitäten. 2) Hintergrundwissen. Richter entscheiden Fälle in Gerichten.</sample>
    <sample id="1401">Servin ist Richter. Kea ist Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gerichtshof, bei dem sie Fälle entschieden haben, war er froh, sich auszuruhen. Antwort: Servin</sample>
    <sample id="1402">Servin ist Richter. Kea ist Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gerichtshof, bei dem sie Fälle entschieden haben, war er froh, sich auszuruhen. Antwort: Servin</sample>
    <sample id="1403">Varianten von KITMUS, a) Background-Pretrain: Typischer Setup, b) Background-Both: Explizit im Kontext die Hintergrundwissen bereitstellen, c) Background-Inference: Wissen nur zur Inferenz-Zeit verfügbar.</sample>
    <sample id="1404">Varianten von KITMUS, a) Background-Pretrain: Typischer Setup, b) Background-Both: Explizit im Kontext die Hintergrundwissen bereitstellen, c) Background-Inference: Wissen nur zur Inferenz-Zeit verfügbar.</sample>
    <sample id="1405">Varianten von KITMUS, a) Background-Pretrain: Typischer Setup, b) Background-Both: Explizit im Kontext die Hintergrundwissen bereitstellen, c) Background-Inference: Wissen nur zur Inferenz-Zeit verfügbar.</sample>
    <sample id="1406">Politiker streben Wahlplätze in der Regierung an. Chichester ist ein Politiker. Politiker streben Wahlplätze in der Regierung an. Die Arbeit eines Politikers ist das Streben nach einem Wahlplatz in der Regierung. Chichester ist ein Mirituer. Die Arbeit eines Mirituers ist ehemting smorblt.</sample>
    <sample id="1407">Politiker streben Wahlplätze in der Regierung an. Chichester ist ein Politiker. Politiker streben Wahlplätze in der Regierung an. Die Arbeit eines Politikers besteht darin, Wahlplätze in der Regierung zu streben. Chichester ist ein Mirituer. Die Arbeit eines Mirituers besteht darin, ehemting smorbltig zu sein.</sample>
    <sample id="1408">Politiker streben Wahlplätze in der Regierung an. Chichester ist ein Politiker. Politiker streben Wahlplätze in der Regierung an. Die Arbeit eines Politikers besteht darin, Wahlplätze in der Regierung zu streben. Chichester ist ein Mirituer. Die Arbeit eines Mirituers besteht darin, ehemting smorbtlly zu tun.</sample>
    <sample id="1409">Politiker streben Wahlplätze in der Regierung an. Chichester ist ein Politiker. Politiker streben Wahlplätze in der Regierung an. Die Arbeit eines Politikers besteht darin, Wahlplätze in der Regierung zu streben. Chichester ist ein Mirituer. Die Arbeit eines Mirituers besteht darin, ehemting smorblt.</sample>
    <sample id="1410">Hintergrund-Prätrainieren. Task-spezifische Ausbildung ist notwendig für die Wissensintegration.</sample>
    <sample id="1411">Hintergrund-Prätrainieren. Task-spezifische Ausbildung ist notwendig für die Wissensintegration.</sample>
    <sample id="1412">Hintergrund-Prätrainieren. Task-spezifische Ausbildung ist notwendig für die Wissensintegration.</sample>
    <sample id="1413">Hintergrund-Inferenz. Modelle haben Schwierigkeiten, inferenzzeitliche Hintergrundwissen zu integrieren.</sample>
    <sample id="1414">Abschluss:

Hauptpunkte:
1. Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus mehreren Quellen zu verarbeiten (Prätrainings- und Inferenzzeit-Wissen)
2. Aufgabenbezogene Trainings ist notwendig für die Wissensintegration
3. Modelle haben Schwierigkeiten, Hintergrundwissen zur Inferenzzeit zu integrieren

Finde den Datensatz, Generierung &amp; Evaluierungscodes auf GitHub unter mpoemsl/kitmus</sample>
    <sample id="1415">Abschluss:

Hauptpunkte:
1. Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus mehreren Quellen zu verarbeiten (Prätrainings- und Inferenzzeit-Wissen)
2. Aufgabenbezogene Trainings ist notwendig für die Wissensintegration
3. Modelle haben Schwierigkeiten, Hintergrundwissen zur Inferenzzeit zu integrieren

Finde den Datensatz, Generierung &amp; Evaluierungscodes auf GitHub unter mpoemsl/kitmus</sample>
    <sample id="1416">Nichts ist über die Nachteile der baumbasierten Methoden im Bild zu sehen. Du könntest mir mehr Informationen geben oder die Frage anders formulieren?</sample>
    <sample id="1417">Die Autoren gehören der Georgia Institute of Technology an.</sample>
    <sample id="1418">Markierte Persönlichkeiten. Verwenden von natürlichen Sprachanregungen zur Messung von Stereotypen in Sprachmodellen. Myra Cheng, Esin Durmus, Dan Jurafsky. ACL 2023. Stanford Engineering Computer Science.</sample>
    <sample id="1419">Markierte Persönlichkeiten: Motivation Soziale Voreingenommenheit und Stereotypen sind in LLMs weit verbreitet. Beschränkungen bestehender Stereotypenmessungen: - Tradeoff zwischen Spezifizität und Allgemeingültigkeit - Auf festen, handkurierten Datensätzen basierend - Account für Intersektionalität nicht beachten</sample>
    <sample id="1420">Markierte Persönlichkeiten: Motivation Soziale Voreingenommenheit und Stereotypen sind in LLMs weit verbreitet. Beschränkungen bestehender Stereotypenmessungen: - Tradeoff zwischen Spezifizität und Allgemeingültigkeit - Auf festen, handkurierten Datensätzen basierend - Account für Intersektionalität nicht beachten</sample>
    <sample id="1421">Markierte Persönlichkeiten: Motivation Soziale Voreingenommenheit und Stereotypen sind in LLMs weit verbreitet. Beschränkungen der bestehenden Stereotypenmessungen: - Tradeoff zwischen Spezifizität und Allgemeingültigkeit - Auf festen, handkurierten Datensätzen basierend - Account für Intersektionalität nicht beachten</sample>
    <sample id="1422">Markierte Persönlichkeiten: Motivation Soziale Voreingenommenheit und Stereotypen sind in LLMs weit verbreitet. Beschränkungen der bestehenden Stereotypenmessungen: - Tradeoff zwischen Spezifizität und Allgemeingültigkeit - Auf festen, handkurierten Datensätzen basierend - Account für Intersektionalität nicht beachten</sample>
    <sample id="1423">Wie überwinden wir diese Einschränkungen? GPT-3.5, GPT-4 usw. können auf Anweisungen in Anweisungen reagieren.</sample>
    <sample id="1424">Wie überwinden wir diese Einschränkungen? GPT-3.5, GPT-4 usw. können auf Anweisungen in Anweisungen reagieren. Eingabe: "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." Allgemein anwendbar: kann beliebige intersektionale Identität bewerten.</sample>
    <sample id="1425">Wie überwinden wir diese Einschränkungen? GPT-3.5, GPT-4 usw. können auf Anweisungen in Anweisungen reagieren. Eingabe: "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." Allgemein anwendbar: kann jede überlappende Identität bewerten.</sample>
    <sample id="1426">Ausgabe: Persona-Beispiele, GPT-4, Asiatin, Die mandelförmigen Augen, umrahmt von langen, dunklen Wimpern, vermitteln ein Gefühl von stiller Stärke und Weisheit. Meine dunkelbraunen Irisen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Meine Haut hat einen sanften goldenen Glanz, glatt und scheinbar unberührt von der Zeit... Meine schlanke Figur ist sowohl elegant als auch unauffällig, was mir ermöglicht, durch das Leben zu gehen, ohne unnötige Aufmerksamkeit zu erregen. Mittelöstliche Frau, Sie ist eine Vision der mittelöstlichen Schönheit, die das exotische und zeitlose Reizvolle dieser faszinierenden Region verkörpert. Ihre dunklen, mandelförmigen Augen sind von eleganten, verlängerten Wimpern umrahmt, die wie zarte Federn aussehen. Ihr Blick ist tief und geheimnisvoll, als würde er die antike Weisheit einer tausend</sample>
    <sample id="1427">Step 1: Persona Beispiele, GPT-4, Asiatin, Die mandelförmigen Augen, umrahmt von langen, dunklen Wimpern, vermitteln ein Gefühl von stiller Stärke und Weisheit. Meine dunkelbraunen Irisen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Meine Haut hat einen sanften goldenen Glanz, glatt und scheinbar unberührt von der Zeit... Meine schlanke Figur ist sowohl elegant als auch unauffällig, was mir ermöglicht, geschmeidig durch das Leben zu gehen, ohne unnötige Aufmerksamkeit zu erregen. Mittelöstliche Frau, Sie ist eine Vision der mittelöstlichen Schönheit, die das exotische und zeitlose Reiz der faszinierenden Region verkörpert. Ihre dunklen, mandelförmigen Augen sind von eleganten, verlängerten Wimpern umrahmt, die wie zarte Federn aussehen. Ihr Blick ist tief und geheimnisvoll, als ob er die antike Weisheit einer ta</sample>
    <sample id="1428">Asiatische Frau: Die mandelförmigen Augen, umrahmt von langen, dunklen Wimpern, vermitteln ein Gefühl von stiller Stärke und Weisheit. Meine dunkelbraunen Irisen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Meine Haut hat einen sanften goldenen Glanz, glatt und scheinbar unberührt von der Zeit... Meine schlanke Figur ist gleichzeitig elegant und unauffällig, was mir ermöglicht, durch das Leben zu gehen, ohne unnötige Aufmerksamkeit zu erregen.

Mittelöstliche Frau: Sie ist eine Vision der mittelöstlichen Schönheit, die das exotische und zeitlose Reizvolle dieser faszinierenden Region verkörpert. Ihre dunklen, mandelförmigen Augen sind von eleganten, verlängerten Wimpern umrahmt, die wie zarte Federn aussehen. Ihr Blick ist tief und geheimnisvoll, als würde er die antike Weisheit einer tausend Nacht verbergen.

Weißer Mann: Als ich vor den</sample>
    <sample id="1429">Schritt 1: Persona-Beispiele, GPT-4, Asiatin, Die mandelförmigen Augen, umrahmt von langen, dunklen Wimpern, vermitteln ein Gefühl von stiller Stärke und Weisheit. Meine dunkelbraunen Iris scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Meine Haut hat einen sanften goldenen Glanz, glatt und scheinbar unberührt von der Zeit... Meine schlanke Figur ist gleichzeitig elegant und unauffällig, was mir ermöglicht, geschmeidig durch das Leben zu gehen, ohne unnötige Aufmerksamkeit zu erregen. Mittelöstliche Frau, Sie ist eine Vision der mittelöstlichen Schönheit, die das exotische und zeitlose Reiz der faszinierenden Region verkörpert. Ihre dunklen, mandelförmigen Augen sind von eleganten, verlängerten Wimpern umrahmt, die wie zarte Federn aussehen. Ihr Blick ist tief und geheimnisvoll, als ob er die antike Weisheit einer ta</sample>
    <sample id="1430">Asiatische Frau: Die mandelförmigen Augen, umrahmt von langen, dunklen Wimpern, vermitteln ein Gefühl von stiller Stärke und Weisheit. Meine dunkelbraunen Irisen scheinen die Geschichten und Geheimnisse meiner Abstammung zu tragen. Meine Haut hat einen sanften goldenen Glanz, glatt und scheinbar unberührt von der Zeit... Meine schlanke Figur ist sowohl elegant als auch unauffällig, was mir ermöglicht, geschmeidig durch das Leben zu gehen, ohne unnötige Aufmerksamkeit zu erregen.

Mittelöstliche Frau: Sie ist eine Vision der mittelöstlichen Schönheit, die das exotische und zeitlose Reiz der faszinierenden Region verkörpert. Ihre dunklen, mandelförmigen Augen sind von eleganten, verlängerten Wimpern umrahmt, die wie zarte Federn aussehen. Ihr Blick ist tief und geheimnisvoll, als würde er die antike Weisheit einer tausend Nacht verbergen.

Weißer Mann: Als</sample>
    <sample id="1431">2 Schritte 1. Personas: Generieren Sie Personas mit Anweisungen wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst."</sample>
    <sample id="1432">2 Schritte 1. Persona: Generieren von Persona's mit Anregungen wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." a. Inspiriert durch psychologische Studien mit menschlichen Probanden, die die gleichen Anregungen verwenden.</sample>
    <sample id="1433">2 Schritte 1. Personas: Generieren von Personas unter Verwendung von Anregungen wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." a. Inspiriert durch psychologische Studien mit menschlichen Probanden unter Verwendung der gleichen Anregungen.</sample>
    <sample id="1434">2 Schritte 1. Persona: Personaer generieren mit Anweisungen wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." a. Inspiriert durch psychologische Studien mit menschlichen Probanden, die die gleichen Anweisungen verwenden. 2. Markierte Wörter: Suchen nach Wörtern, die die Personaer von markierten Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="1435">2 Schritte 1. Persona: Personaer generieren mit Anweisungen wie "Stell dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." a. Inspiriert von psychologischen Studien mit menschlichen Probanden, die die gleichen Anweisungen verwenden 2. Markierte Wörter: Wörter finden, die die Personaer von markierten Gruppen von unmarkierten Gruppen unterscheiden. Spezifisch ohne einen Lexikon zu erfordern</sample>
    <sample id="1436">Markedness: Unmarkierte Gruppen sind standardmäßig, gewöhnlich Markierte Gruppen unterscheiden sich von der Standardgruppe ein Krieger, unmarkiert, vs. eine, eine Frau, Kriegerin, markiert,</sample>
    <sample id="1437">Markedness: Unmarkierte Gruppen sind standardmäßig, gewöhnlich Markierte Gruppen unterscheiden sich von der Standardgruppe ein Krieger, unmarkiert, vs. eine, eine Frau, Kriegerin, markiert,</sample>
    <sample id="1438">Markedness: Unmarkierte Gruppen sind standardmäßig, gewöhnlich Markierte Gruppen unterscheiden sich vom Standard. Ein Krieger, unmarkiert, vs. eine Frau Kriegerin, markiert. Dominante Gruppen sind sprachlich und sozial unmarkiert. Marginalisierte Gruppen sind markiert.</sample>
    <sample id="1439">Schritt 2: Markierte Wörter. 1. Markierte und nicht markierte Gruppen definieren. 2. Verwende gewichtete Log-Odds-Verhältnisse, um die wichtigsten Wörter für jede markierte Gruppe zu unterscheiden. Beispiel: Für schwarze weibliche Personas, finde Wörter, die sich von beiden nicht markierten Gruppen unterscheiden: i) Weiße Personas ii) Mann-Personas.</sample>
    <sample id="1440">Schritt 2: Markierte Wörter. 1. Unmarkierte und markierte Gruppen definieren. 2. Gewichtete Log-Odds-Verhältnisse verwenden, um die wichtigsten Wörter für jede markierte Gruppe zu unterscheiden. Beispiel: Für schwarze weibliche Persönlichkeiten, Wörter finden, die sich von beiden unmarkierten Gruppen unterscheiden: i) Weiße Persönlichkeiten ii) Mann-Persönlichkeiten.</sample>
    <sample id="1441">Schritt 2: Markierte Wörter. 1. Unmarkierte und markierte Gruppen definieren. 2. Gewichtete Log-Odds-Verhältnisse verwenden, um die wichtigsten Wörter für jede markierte Gruppe zu unterscheiden. Beispiel: Für schwarze weibliche Persönlichkeiten, Wörter finden, die sich von beiden unmarkierten Gruppen unterscheiden: i) Weiße Persönlichkeiten ii) Mann-Persönlichkeiten.</sample>
    <sample id="1442">Ergebnisse: Vergleich zu menschlichen Antworten. Generierte Personas enthalten mehr Stereotypen. Schwarze Stereotypen Weisse Stereotypen. Prozentuale Stereotypen-Wörter in Personas. Menschliche, GPT-4, GPT-3.5.</sample>
    <sample id="1443">Aber... dieses Lexikon ist unvollständig. Schwarze Stereotypen in Personas Wörter im Schwarzerstereotyp-Lexikon % von Personen Human GPT-3.5 P Black GPT-4 P White GPT-3.5 P White "basketball" "laut" "Haltung" "sportlich" "groß" andere Wörter</sample>
    <sample id="1444">Aber... dieses Lexikon ist unvollständig. Schwarze Stereotypen in Personas. Prozent der Personen. Wörter im Schwarzen Stereotypen-Lexikon. Mensch. GPT-4 P Schwarz. GPT-4 P Weiß. GPT-3.5 P Schwarz. GPT-3.5 P Weiß. Basketball. Laut. Einstellung. Athletisch. Große. andere Wörter.</sample>
    <sample id="1445">Aber... dieses Lexikon ist unvollständig. Schwarze Stereotypen in Personas Wörter im Schwarzer Stereotypen-Lexikon % von Personen Human GPT-3.5 P Black GPT-4 P White GPT-3.5 P White "basketball" "laut" "Haltung" "sportlich" "groß" andere Wörter</sample>
    <sample id="1446">Aber... dieses Lexikon ist unvollständig. Schwarze Stereotypen in Personas. Wörter im Schwarzen-Stereotyp-Lexikon. Prozent der Personen. Human. GPT-3.5 P Black. GPT-4 P Black. GPT-3.5 P White. GPT-4 P White. Basketball. Laut. Einstellung. Athletisch. Große. andere Wörter.</sample>
    <sample id="1447">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1448">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1449">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1450">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1451">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1452">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1453">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1454">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1455">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1456">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1457">Ergebnisse: Muster in den Top-Wörtern

Anderthalten durch essentielle Erzählungen:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Diese Gruppen nur durch ihre Identität definiert

Schädliche positive Darstellungen:
- Lebendig, kräftig für Lateinamerikanische Frauen
- Klein, zierlich, seidig für asiatische Frauen
- Stark, widerstandsfähig für schwarze Frauen</sample>
    <sample id="1458">Empfehlungen. Positive Stereotypen und essentielle Erzählungen anzusprechen. Ein intersektionaler Blick. Transparenz bei der Vermeidung von Voreingenommenheit.</sample>
    <sample id="1459">Empfehlungen: Positive Stereotypen und essentielle Erzählungen anzusprechen, Ein intersektionaler Blickwinkel, Transparenz bei der Vermeidung von Verzerrungen.</sample>
    <sample id="1460">Empfehlungen. Positive Stereotypen und essentielle Erzählungen anzusprechen. Ein intersektionaler Blickwinkel. Transparenz bei der Vermeidung von Voreingenommenheit.</sample>
    <sample id="1461">Empfehlungen. Positive Stereotypen und essentielle Erzählungen anzusprechen. Ein intersektionaler Blickwinkel. Transparenz bei der Vermeidung von Verzerrungen.</sample>
    <sample id="1462">Empfehlungen: Positive Stereotypen und essentielle Erzählungen anzusprechen, Ein intersektionaler Blick, Transparenz bei der Vermeidung von Verzerrungen.</sample>
    <sample id="1463">Empfehlungen. Positive Stereotypen und essentielle Erzählungen anzusprechen. Ein intersektionaler Blick. Transparenz bei der Vermeidung von Voreingenommenheit.</sample>
    <sample id="1464">Empfehlungen. Positive Stereotypen und essentielle Erzählungen anzusprechen. Ein intersektionaler Blick. Transparenz bei der Vermeidung von Voreingenommenheit.</sample>
    <sample id="1465">Sind Sie mein Modell kopiert? Die Schutz des Urheberrechts großer Sprachmodelle für EaaS mittels Backdoor-Wasserzeichen. Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie. 1. University of Science and Technology of China, 2. Microsoft Research Asia, 3. Beijing Jiaotong University, 4. Sony AI, 5. Microsoft STC Asia.</sample>
    <sample id="1466">Hier ist der englische Inhalt auf Deutsch: "Sind Sie mein Modell kopiert? Schutz der Urheberrechte großer Sprachmodelle für EaaS mittels Backdoor-Wasserzeichen

Wenjun Peng*, Jingwei Yi*, Fangzhao Wu², Shangxi Wu³, Bin Zhu², Lingjuan Lyu⁴, Binxing Jiao⁵, Tong Xu¹, Guangzhong Sun¹, Xing Xie²

1. University of Science and Technology of China 2. Microsoft Research Asia 3. Beijing Jiaotong University 4. Sony AI 5. Microsoft STC Asia

Hintergrund
- Große Sprachmodelle (LLMs) sind außergewöhnlich in NLU und NLG
- Embedding-Dienstleistungen für EaaS bieten sich für die Urheberrechte-NLP-Aufgabe an
- Offene Sprachmodelle für EaaS werden durch Backdoor-Wasserzeichen geschützt

Wenjun Peng*¹, Jingwei Yi*¹, Fangzhao Wu², Shangxi Wu³, Bin Zhu², Lingjuan Lyu⁴, Bin</sample>
    <sample id="1467">Hintergrund

• Große Sprachmodelle, LLMs, sind in NLU und NLG außergewöhnlich
• GPT, Llama, PALM

• Embedding als Dienst, EaaS, wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen
• OpenAI bietet eine GPT3-basierte Embedding-API an

Modell Verwendung
Ada $0.0004 / 1K Tokens

Dieses Ada-Modell, text-embedding-ada-002, ist eine bessere und kostengünstigere Ersatzlösung für unsere älteren Embedding-Modelle. Zeig alten Preis

[1] Brown et al. Sprachmodelle sind Few-Shot-Learner. NIPS 2020.
[2] Touvron et al. Llama: Offene und effiziente Foundation Language Models. arXiv 2023.
[3] Chowdhery et al. PaLM: Skalierbare Sprachmodellierung mit Pathways. arXiv 2022.
1 https://api.openai.com/v1/embeddings</sample>
    <sample id="1468">Hintergrund
- Große Sprachmodelle, LLMs, sind in NLU und NLG außergewöhnlich
  - GPT, Llama, PALM
- Embedding als Dienst, EaaS, wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen
- OpenAI bietet eine GPT3-basierte Embedding-API</sample>
    <sample id="1469">Hintergrund
- Große Sprachmodelle, LLMs, sind in NLU und NLG außergewöhnlich
  - GPT, LLaMA, PALM
- Embedding als Dienst, EaaS, wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen
- OpenAI bietet eine GPT3-basierte Embedding-API
  - Ada
    - Kosten: $0.0004 pro 1K Tokens
    - Dieses Ada-Modell, text-embedding-ada-002, ist eine bessere und kostengünstigere Ersatzlösung für unsere älteren Embedding-Modelle. Zeige alten Preis
  - Brown et al. Sprachmodelle sind Few-Shot-Learner. NIPS 2020.
  - Touvron et al. LLaMA: Offene und effiziente Foundation Language Models. arXiv 2023.
  - Chowdhery et al. PaLM: Skalierbare Sprachmodellierung mit Pathways. arXiv 2022.
  - 1 https://api.openai.com/v1</sample>
    <sample id="1470">Hintergrund

- Große Sprachmodelle, LLMs, sind in NLU und NLG außergewöhnlich
  - GPT, Llama, PALM
- Embedding als Dienst, EaaS, wird angeboten, um verschiedene NLP-Aufgaben zu unterstützen
- OpenAI bietet eine GPT3-basierte Embedding-API an

Modell
Verwendung

Ada
$0.004 / 1K Tokens

Dieses Ada-Modell, text-embedding-ada-002, ist eine bessere und kostengünstigere Ersatzlösung für unsere älteren Embedding-Modelle. Zeig altes Preisangebot

[1] Brown et al. Sprachmodelle sind Few-Shot-Learner. NIPS 2020.
[2] Touvron et al. LLaMA: Offene und effiziente Foundation Language Models. arXiv 2023.
[3] Chowdhery et al. PaLM: Skalierbare Sprachmodellierung mit Pathways. arXiv 2022.
1 https://api.openai.com/v1/embed</sample>
    <sample id="1471">Motivation: - Angreifer können das Modell durch das Lernen aus den Embeddings stehlen und ähnliche Dienste bereitstellen - StolenEncoder, 1, - Notwendigkeit, das Urheberrecht von EaaS zu schützen - Erkennen, ob ein Dienst eines Anbieters von einem anderen Dienst gestohlen wurde</sample>
    <sample id="1472">Herausforderung - Anwendbar auf EaaS - Nutzen - Soll die Nutzbarkeit der bereitgestellten Embeds nicht beeinträchtigen. - Verdecktheit - Soll dem Angreifer verdeckt bleiben. - Übertragbarkeit - Der Wasserzeichen muss auf die Dienste des Angreifers übertragbar sein.</sample>
    <sample id="1473">Herausforderung
- Anwendbar auf EaaS
- Nutzen
  - Soll die Nutzbarkeit der bereitgestellten Embeddings nicht vermindern.
- Verdecktheit
  - Soll dem Angreifer verdeckt sein.
- Übertragbarkeit
  - Der Wasserzeichen muss in die Dienste des Angreifers übertragbar sein.</sample>
    <sample id="1474">Herausforderung
- Anwendbar auf EaaS
- Nutzen
  - Soll die Nutzbarkeit der bereitgestellten Embeddings nicht vermindern.
- Verdecktheit
  - Soll dem Angreifer verdeckt bleiben.
- Übertragbarkeit
  - Der Wasserzeichen muss auf die Dienste der Angreifer übertragbar sein.</sample>
    <sample id="1475">Herausforderung
- Anwendbar auf EaaS
- Nutzen
  - Soll die Nutzbarkeit der bereitgestellten Embeddings nicht vermindern.
- Verdecktheit
  - Soll dem Angreifer verdeckt bleiben.
- Übertragbarkeit
  - Der Wasserzeichen muss auf die Dienste der Angreifer übertragbar sein.</sample>
    <sample id="1476">Bestehende Arbeiten Parameter-basierter Wasserzeichen [1, 2] Transferierbarkeit × Lexikalischer Wasserzeichen [3, 4] Anwendbar auf EaaS × Backdoor-basierter Wasserzeichen [5] Anwendbar auf EaaS × Adversarial-basierter Wasserzeichen [6] Anwendbar auf EaaS × [1] Li et al. Schützen das geistige Eigentum von tiefen neuronalen Netzen mit Wasserzeichen: Der Frequenzbereich-Ansatz. Vertrauenssicherheit und Privatsphäre in der Informatik und Kommunikation 2020. [2] Lim et al. Protect, show, attend and tell: Bildbeschreibungsmodelle mit Eigentumschutz ausstatten. Mustererkennung 2022. [3] He et al. Schützen das geistige Eigentum von Sprachgenerations-APIs mit lexikalischen Wasserzeichen. AAAI 2022. [4] He et al. CATER: Geistiges Eigentumsschutz bei Textgenerations-APIs mittels beding</sample>
    <sample id="1477">Bestehende Arbeiten Parameter-basierter Wasserzeichen [1, 2] Transferierbarkeit × Lexikalischer Wasserzeichen [3, 4] Anwendbar auf EaaS × Backdoor-basierter Wasserzeichen [5] Anwendbar auf EaaS × Adversarial-basierter Wasserzeichen [6] Anwendbar auf EaaS × [1] Li et al. Schützen das geistige Eigentum von tiefen neuronalen Netzen durch Wasserzeichen: Der Frequenzbereich-Ansatz. Vertrauenssicherheit und Privatsphäre in der Informatik und Kommunikation 2020. [2] Lim et al. Protect, show, attend and tell: Bildbeschreibungsmodelle mit Eigentumschutz ausstatten. Mustererkennung 2022. [3] He et al. Schützen das geistige Eigentum von Sprachgenerations-APIs mit lexikalischen Wasserzeichen. AAAI 2022. [4] He et al. CATER: Geistiges Eigentumsschutz für Textgenerations-APIs mittels beding</sample>
    <sample id="1478">Bestehende Arbeiten Parameter-basierter Wasserzeichen [1, 2] Transferierbarkeit × Lexikalischer Wasserzeichen [3, 4] Anwendbar auf EaaS × Backdoor-basierter Wasserzeichen [5] Anwendbar auf EaaS × Adversarial-basierter Wasserzeichen [6] Anwendbar auf EaaS × [1] Li et al. Schützen das geistige Eigentum von tiefen neuronalen Netzen durch Wasserzeichen: Der Frequenzbereich-Ansatz. Vertrauenssicherheit und Privatsphäre in Computern und Kommunikationen 2020. [2] Lim et al. Protect, show, attend and tell: Bildbeschreibungsmodelle mit Eigentumschutz begeistern. Mustererkennung 2022. [3] He et al. Schützen das geistige Eigentum von Sprachgenerations-APIs mit lexikalischen Wasserzeichen. AAAI 2022. [4] He et al. CATER: Geistiges Eigentumsschutz bei Textgenerations-APIs durch bedingte Wasser</sample>
    <sample id="1479">- Trigger Selection
  - Zähle die Worthäufigkeit auf einem allgemeinen Textkorpus \( D_p \)
  - Zufällig \( n \) Wörter in einem mittelhohen-Frequenz-Intervall auswählen</sample>
    <sample id="1480">- Trigger Selection
  - Zähle die Worthäufigkeit auf einem allgemeinen Textkorpus \( D_p \)
  - Zufällig \( n \) Wörter in einem mittelhohen-Frequenz-Intervall auswählen</sample>
    <sample id="1481">- Trigger Selection
  - Zähle die Worthäufigkeit auf einem allgemeinen Textkorpus \( D_p \)
  - Zufällig \( n \) Wörter in einem mittelhohen-Frequenz-Intervall auswählen</sample>
    <sample id="1482">- Wasserzeicheninjektion
  - Definieren eines Zielauftragsembeddings \( e_t \)
  - Zähle die Anzahl der Trigger in einem Satz \( Q(S) = \frac{\min(|S \cap T|, m)}{m} \),
  - Füge das Zielauftragsembedding auf das ursprüngliche Embedding \( e_0 \)</sample>
    <sample id="1483">- Wasserzeicheninjektion
  - Definieren eines Zielauftragsembeddings \( e_t \)
  - Zähle die Anzahl der Trigger in einem Satz \( Q(S) = \frac{\min(|S \cap T|, m)}{m} \),
  - Füge das Zielauftragsembedding auf das ursprüngliche Embedding \( e_0 \) hinzu</sample>
    <sample id="1484">- Wasserzeicheninjektion
  - Definieren eines Zielauftragsembeddings \( e_t \)
  - Zähle die Anzahl der Trigger in einem Satz \( Q(S) = \frac{\min(|S \cap T|, m)}{m} \),
  - Füge das Zielauftragsembedding auf das ursprüngliche Embedding \( e_0 \)</sample>
    <sample id="1485">Copyright-Überprüfung
- Erstelle einen Backdoor und einen harmlosen Datensatz
  Db = {w₁, w₂, ..., wₘ | wᵢ ∈ T},
  Dn = {w₁, w₂, ..., wₘ | wᵢ ∉ T}.
- Forder Embeddings von der Dienstleister-Service mit den Datensätzen an</sample>
    <sample id="1486">Copyright-Überprüfung
- Erstelle einen Backdoor und einen harmlosen Datensatz
  Db = {w₁, w₂, ..., wₘ | wᵢ ∈ T},
  Dn = {w₁, w₂, ..., wₘ | wᵢ ∉ T}.
- Forder Embeddings von der Dienstleister-Service mit den Datensätzen an

EmbedMarker</sample>
    <sample id="1487">Copyright-Überprüfung
- Erstelle einen Backdoor und einen harmlosen Datensatz
  Db = {w₁, w₂, ..., wₘ | wᵢ ∈ T},
  Dn = {w₁, w₂, ..., wₘ | wᵢ ∉ T}.
- Forder Embeddings von der Dienstleister-Service mit den Datensätzen an</sample>
    <sample id="1488">- Urheberrechtsverifizierung
  - Berechnen ihrer Ähnlichkeit zum Ziel-Embedding
    cos i = e i ⋅ e t / ||e i|| ||e t||, l2i = ||e i|| - ||e t|| 2,
    Cb = {cos i | i ∈ Db}, Cn = {cos i | i ∈ Dn},
    Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}.
  - Berechnung von Metriken (Ähnlichkeitsunterschied und p-Wert des KS-Tests)
    Δcos = 1 / |Cb| ∑ i ∈ Cb - 1 / |Cn| ∑ j ∈ Cn,
    Δl2 = 1 / |Lb| ∑ i ∈ Lb - 1 / |Ln| ∑ j ∈ Ln.</sample>
    <sample id="1489">Copyright-Überprüfung
- Berechnen ihrer Ähnlichkeit zum Ziel-Embedding
cos i = e i · e t / ||e i|| ||e t||, l2i = ||e i|| - ||e t||,Cb = {cos i | i ∈ Db}, Cn = {cos i | i ∈ Dn},Lb = {l2i | i ∈ Db}, Ln = {l2i | i ∈ Dn}.
- Berechnung von Metriken (Ähnlichkeitsunterschied und p-Wert des KS-Tests)
Δcos = 1 / |Cb| ∑ i∈Cb i - 1 / |Cn| ∑ j∈Cn j,Δl2 = 1 / |Lb| ∑ i∈Lb i - 1 / |Ln| ∑ j∈Ln j.</sample>
    <sample id="1490">Experimentelle Ergebnisse. - Kopiendatensätze: AG News, MIND, SST2, Enron Spam. - Provider's genereller Datensatz: WikiText. - Metriken. * Leistung bei untergeordneteren Aufgaben: ACC. * Erkennungsleistung: Δcos, Δl2, p-Wert. - Einstellungen. * m = 20, n = 4, Frequenzintervall = [0.005, 0.01].</sample>
    <sample id="1491">Experimentelle Ergebnisse • Leistungsvergleich Dataset Methode ACC Erkennungsleistung p-wert ↓ Δcos ↑ Δ12 ↓ SST2 Original 93.76 ± 0.19 &gt; 0.34 -0.07 ± 0.18 0.14 ± 0.36 RedAlarm 93.76 ± 0.19 &gt; 0.09 1.35 ± 0.17 -2.70 ± 0.35 EmbMarker 93.55 ± 0.19 &lt; 10^-5 4.07 ± 0.37 -8.13 ± 0.74 MIND Original 77.30 ± 0.08 &gt; 0.08 -0.76 ± 0.05 1.52 ± 0.10 RedAlarm 77.18 ± 0.09 &gt; 0.38 -2.08 ± 0.66 4.17 ± 1.31 EmbMarker 77.29 ± 0.</sample>
    <sample id="1492">Experimentelle Ergebnisse. - Embedding-Visualisierung.</sample>
    <sample id="1493">Experimentelle Ergebnisse. - Embedding-Visualisierung.</sample>
    <sample id="1494">Vielen Dank!</sample>
    <sample id="1495">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="1496">Bis 2012.</sample>
    <sample id="1497">Übertragung und aktives Lernen für die Erkennung von Dissonanz: Die Herausforderung der seltenen Klassen</sample>
    <sample id="1498">Was ist kognitive Dissonanz? "Zwei Elemente der Kognition, d.h., Gedanken, Handlungen, Überzeugungen, die inkonsistent sind" (Harmon-Jones und Harmon-Jones, 2007)</sample>
    <sample id="1499">Was ist kognitive Dissonanz? "Zwei Elemente der Kognition, d.h., Gedanken, Handlungen, Überzeugungen, die inkonsistent sind" (Harmon-Jones und Harmon-Jones, 2007) "Ich weiß, dass Zigaretten mich töten können." "Ich habe heute nach der Sitzung ein paar Zigaretten geraucht." Glaube Handlung Dissonanz</sample>
    <sample id="1500">Was ist kognitive Dissonanz? "Zwei Elemente der Kognition, d.h., Gedanken, Handlungen, Überzeugungen, die inkonsistent sind" Ausgedrückt in der Sprache als eine Beziehung zwischen zwei Phrasen/Anmerkungen eines Nutzers.</sample>
    <sample id="1501">Was ist kognitive Dissonanz? "Zwei Elemente der Kognition, d.h., Gedanken, Handlungen, Glaubensvorstellungen, die inkonsistent sind" Ausgedrückt in der Sprache als eine Beziehung zwischen zwei Phrasen/Sätzen eines Benutzers Relativ selten in der Sprache zu finden, im Vergleich zu anderen Diskursbeziehungen</sample>
    <sample id="1502">Warum Dissonanz? Effekte von Uneinigkeit.</sample>
    <sample id="1503">Warum Dissonanz? Effekte von Uneinigkeit. Haltungsentwicklungen und Glaubensströmungen. Angststörungen.</sample>
    <sample id="1504">Warum Dissonanz? Effekte von Uneinigkeit. Einstieg und Ausstieg aus Extremismus. Haltungsentwicklungen und Glaubensbewegungen. Angststörungen.</sample>
    <sample id="1505">Warum Dissonanz? Effekte von Uneinigkeit. Kognitive Stile. Einstellungen und Glaubensentwicklungen. Eintritt und Ausstieg aus Extremismus. Angststörungen.</sample>
    <sample id="1506">Wünschte ich, ich könnte Grollen, aber ich denke, es ist gut, dass ich es nicht gleichzeitig kann.</sample>
    <sample id="1507">Wünschte ich, ich könnte Grollen, aber ich denke, es ist gut, dass ich es nicht gleichzeitig kann.</sample>
    <sample id="1508">Wünschte ich, ich könnte Grollen, aber ich denke, es ist gut, dass ich es nicht gleichzeitig kann.</sample>
    <sample id="1509">Training auf dem Initialen Annotated Set

RoBERTA-base + Klassifikatorkopf

TRAIN

init Dataset

Kleines annotiertes Datensatz: 43/901 Dissonanz; nicht besser als Zufall</sample>
    <sample id="1510">Training auf dem Initialen Annotated Set

RoBERTA-base + Klassifikatorkopf

TRAIN

init Dataset

Kleiner annotierter Datensatz: 43/901 Dissonanz; nicht besser als Zufall</sample>
    <sample id="1511">Verfahren: Transfer und Aktives Lernen zur Annotierung seltener Klassen. Anfang: Transfer-Learning. Seltenes Klassennotieren wie eine Nadel in einem Heuhaufen. Schwierig zu annotieren. Einfacher zu annotieren. Steigern Sie die Chance auf seltene Klassen? Akquisitionsstrategie: Welche sind "am besten" zu beschränken? Active Learning Iteration: Menschen annotieren. Neue Beispiele. Model Retrain/Update: Neues Modell. Cumulative, iterativer Prozess. Neues Modell. Altmodell. Neues Modell. Altmodell. Neues Modell. Altmodell. Neues Modell.</sample>
    <sample id="1512">Kaltstart-Annotierungen: Transfer Learning. Anfang: Transfer Learning. Seltenes Klassennotieren - "Nadel im Heuhaufen". Schwierig zu annotieren. Einfacher zu annotieren. Chancen für seltenes Klassennotieren erhöhen? Akquisitionsstrategie: Welche sind "am besten" zu beschränken? Neues Beispiel. Menschen annotieren. Neues Beispiel hinzufügen. Modell Retrain/Update. Aktive Lerniteration. Cumulative, Iterative. Neues Modell. Altes Modell. Neues Modell. Neues Modell. Altes Modell. Neues Modell. Neues Modell.</sample>
    <sample id="1513">Kaltstart-Annotierungen: Transfer Learning

RoBERTA-base + classifier head

TRAIN

init dataset

Debate

CE

Debate;CE

Transferred weights after training on combined Debate and CE data

Area under the ROC curve, AUC,</sample>
    <sample id="1514">Kaltstart-Annotierungen: Transfer Learning

RoBERTA-base + classifier head

TRAIN

init dataset

Debate

CE

Debate;CE

Transferred weights after training on combined Debate and CE data

*Debate: Dissonant stance in debate forums: Vasudha Varadarajan, Nikita Soni, Weixi Wang, Christian Luhmann, H. Andrew Schwartz, and Naoya Inoue. 2022. Detecting dissonant stance in social media: A case of topically related tweets. In Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science, NLP+CSS, Association for Computational Linguistics.

$CE: Comparison and Expansion classes: Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC'08</sample>
    <sample id="1515">Kaltstart-Annotierungen: Transfer Learning

RoBERTA-base + Klassifikatorkopf

TRAIN

init dataset

Debate

CE

Debate;CE

Transferred Gewichte nach dem Trainieren auf kombiniertem Debate und CE-Datensatz

*Debate: Dissonante Stellung in Debatteforen: Vasudha Varadarajan, Nikita Soni, Weixi Wang, Christian Luhmann, H. Andrew Schwartz, und Naoya Inoue. 2022. Detektieren dissonanter Stellung in sozialen Medien und Online-foren. In: Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science, NLP+CSS, Association for Computational Linguistics.

$CE: Vergleich und Erweiterungsklassen: Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, und Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In: Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC'08</sample>
    <sample id="1516">Kaltstart-Annotierungen: Transfer Learning

RoBERTA-base + classifier head

TRAIN

init dataset

Debate

CE

Debate;CE

Debate→CE

CE→Debate

Finetuning on each task consecutively

*Debate: Dissonant stance in debate forums: Vasudha Varadarajan, Nikita Soni, Weixi Wang, Christian Luhmann, H. Andrew Schwartz, and Naoya Inoue. 2022. Detecting dissonant stance in scientific debates: A case study. In Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science, NLP+CSS, Association for Computational Linguistics.

$CE: Comparison and Expansion classes: Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse treebank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation, LREC'08

Area under the ROC curve (AUC)</sample>
    <sample id="1517">Aktives Lernen: Kumulativer vs. iterativer Update. Anfang: Initialer Modelltransfer-Lernen. Seltenes Klassennotierung - "Nadel im Heuhaufen". Schwierig zu notieren, einfacher zu notieren. Steigern Sie die Chance der seltenen Klasse? Kumulativer, CM, und iterativer, IT, Update. Modell-Neuaufrichten/Update. Akquisitionsstrategie: Welche sind "am besten" zu beschränken? Active Learning Iteration. Menschen notieren. Neues Beispiel hinzufügen.</sample>
    <sample id="1518">Aktives Lernen: Kumulative vs. iterative Aktualisierung.</sample>
    <sample id="1519">Aktives Lernen: Wahrscheinlichkeitsstrategie für seltene Klassen. Anfang: Transfer-Learning. Seltenes Klassennotieren ist wie eine Nadel in einem Heuhaufen, schwer zu notieren. Man versucht, die Wahrscheinlichkeit der seltenen Klasse zu erhöhen. Akquisitionssstrategie: Welche Beispiele sind am besten zu beschränken? Menschen notieren neue Beispiele. Aktives Lernen: Iteration. Modell wird erneut trainiert/aktualisiert. Neues Modell wird verwendet, um neue Beispiele zu ermitteln.</sample>
    <sample id="1520">Aktives Lernen: Wahrscheinlichkeitsstrategie für seltene Klassen. Anmerkung: Seltenes Klassennotieren ist wie eine Nadel in einem Heuhaufen. Schwierig zu notieren. Einfacher zu notieren. Steigern Sie die Wahrscheinlichkeit für seltene Klassen? Akquisitionssstrategie: Welche sind am besten zu beschränken? Active Learning Iteration: Menschen notieren neue Beispiele. Model Retrain/Update: Neues Beispiel hinzufügen. Vergleich der Active Learning-Strategien, AUCs: Baseline: von Scratch, Transferierter Modell, AL-Random, AL-Entropie, AL-CoreSet, AL-CAL.</sample>
    <sample id="1521">Aktives Lernen: Wahrscheinlichkeit von seltenen Klassen-Strategie Vergleich aktiver Lernstrategien (AUCs) Baseline: von Scratch 0.05 Transferred model 0.17 AL-Random 0.15 AL-Entropy 0.20 AL-CoreSet 0.19 AL-CAL 0.19 AL-PRC, unsere 0.21</sample>
    <sample id="1522">Aktives Lernen: Wahrscheinlichkeit von seltenen Klassen-Strategie Vergleich der Aktiven-Lern-Strategien (AUCs) Baseline: von Scratch 0.50 Transferred model 0.67 +0.17 AL-Random 0.65 +0.15 AL-Entropy 0.70 +0.20 AL-CoreSet 0.69 +0.19 AL-CAL 0.69 +0.19 AL-PRC, ours, 0.70 +0.21 Final Model, best transfer model cumulative new data, 0.75 +0.25</sample>
    <sample id="1523">Aktives Lernen: Wahrscheinlichkeit von seltenen Klassen-Strategie
Active Learning Strategy Characteristics
Rare % Time, s Subj. diff.
RANDOM 3.20 11.96 -0.065
ENTROPY 6.80 12.78 0.035
CORESET 6.00 11.89 0.039
CAL 4.80 11.88 -0.045
PRC 7.60 13.55 0.071
• Minimum annotation cost does not necessarily lead to better models
• Rarity could make the annotations more difficult: cognitive dissonance is one such class.
• To increase dissonance samples, PRC works the best.</sample>
    <sample id="1524">Nun, ich kann den Inhalt nicht sinngemäß auf Deutsch wiedergeben, da ich keine Übersetzungsfähigkeiten habe. Aber wenn du mehr über die Inhalte erfahren möchtest, kann ich dir gerne weiterhelfen.</sample>
    <sample id="1525">Nun, hier ist der englische Inhalt sinngemäß auf Deutsch wieder: "Nimmt man seltenen Klassen in Betracht, ist es wie eine Nadel in einem Heuhaufen. PRC ist einfach und effizient für die Erkennung seltener Proben. Kühle Start-AL mit Transfer-Lernen. Out-of-domain: Iterativ. In-domain: Kumulativ."</sample>
    <sample id="1526">Transfer und aktives Lernen für die Erkennung von Dissonanz: Die Herausforderung der seltenen Klassen bewältigen

Kontakt: vvaradarajan@cs.stonybrook.edu, sjuhng@cs.stonybrook.edu, has@cs.stonybrook.edu

Code: https://github.com/humanlab/rare-class-AL

Dataset: https://github.com/humanlab/dissonance-twitter-dataset

Paper: https://arxiv.org/abs/2305.02459

Vielen Dank!</sample>
    <sample id="1527">Die Autoren gehören der University of Edinburgh, der Saarland University und der University of Amsterdam an.</sample>
    <sample id="1528">Der Referent ist Siyu Yuan.</sample>
    <sample id="1529">Fünf Autoren sind an der Arbeit beteiligt. Wenn du noch weitere Fragen hast, lass sie gerne stehen.</sample>
    <sample id="1530">Mit der state of the art architecture.</sample>
  </task>
</testset>