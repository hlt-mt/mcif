<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Großes Skalendatensatz von Webkrawldaten.</sample>
    <sample id="1">Die Autoren gehören an McGill University.</sample>
    <sample id="2">Abstract: This paper presents a novel multi - pre - training model, Layout - Mask, for visual - rich document understanding. It focuses on forms, receipts, and posters. Layout - Mask uses text and layout information as input, enhancing text - layout interactions and layout representation learning. It differs from previous studies in three aspects: choice of 1D projection, masking strategy, and pre - training objectives. Instead of global 1D projection, it uses local 1D projection. The model infers global reading order by jointly using 1D projection, 2D projection, and semantic information. It also employs two novel masking strategies: whole - word masking and layout - aware masking. The whole - word masking strategy sets masks at word level, promoting text - layout interactions. Layout - aware masking gives higher probability to first and last words of each segment, encouraging learning of cross - segment orders. A new pre - training objective, Mask - Position Modeling, is designed. Experiments show Layout - Mask outperforms other models on SPOT - D and SRL - E datasets, with similar performance on CORD.</sample>
    <sample id="3">Hallo, ich bin Regina Stöten und werde Sie durch die erste Teil der Präsentation führen.Lassen Sie uns zuerst Texte definieren.Texte sind eine Art von Informationen, die in Form von Schriftzeichen oder Symbolen dargestellt werden. Sie können aus Wörtern, Sätzen oder ganzen Dokumenten bestehen.Texte können in verschiedenen Formen und für verschiedene Zwecke verwendet werden. Sie können zum Lesen, Schreiben, Denken, Kommunizieren und vielen anderen Dingen verwendet werden.Texte können auch in verschiedenen Sprachen geschrieben werden. Sie können in deutscher, englischer, französischer, spanischer und vielen anderen Sprachen geschrieben werden.Texte können auch in verschiedenen Formaten gespeichert werden. Sie können in Form von Papier, elektronischen Dateien, auf Websites oder in anderen digitalen Formaten gespeichert werden.Texte können auch in verschiedenen Kontexten verwendet werden. Sie können in Büchern, Zeitungen, Magazinen, auf Websites, in E-Mails und vielen anderen Kontexten verwendet werden.Texte können auch in verschiedenen Formaten gespeichert werden. Sie können in Form von Papier, elektronischen</sample>
    <sample id="4">Coyote.</sample>
    <sample id="5">Das Modell, das 82-87 % Genauigkeit erreicht hat, ist das BERT-Modell.</sample>
    <sample id="6">This work presents a unified approach to multilingual and cross - lingual summarization. It introduces many - to - many summarization, a more general setting that can summarize a document in any source language into a summary in any target language. The work conducts preliminary studies to analyze multilingual, cross - lingual, and many - to - many summarization. It finds that many - to - many summarization helps transfer task knowledge better across languages. A pre - training method called PACTS is proposed for many - to - many summarization models. Experiments on the WikiNews dataset show that PACTS outperforms other models. The work also conducts ablation studies and human studies to verify the effectiveness of PACTS. For more details, please refer to the paper.</sample>
    <sample id="7">Ja, sie funktionieren noch.</sample>
    <sample id="8">Die neuere menschliche Bewertungsmethode reduziert die Subjektivität durch explizite Annotierung, ob bestimmte Modelleigenschaften in den Model-Antworten vorkommen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Anzahl der sauberen Validierungssamples ab.</sample>
    <sample id="10">Das ist eine offene Frage und es gibt viele Möglichkeiten. Man könnte z.B. mehr Daten sammeln, verschiedene Algorithmen testen oder die Annotation verbessern. Es hängt von den spezifischen Bedürfnissen und Zielen ab. Was genau willst du verbessern?</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents on humor understanding benchmarks from the New Yorker Caption Contest. Large language models can generate and explain jokes. For example, ChatGPT can tell jokes like "Why don't scientists trust atoms? Because they make up everything." However, humor understanding is questionable. The New Yorker Caption Contest data is used for three tasks: matching, quality ranking, and explanation generation. The best model achieves 62% accuracy on matching, compared to 94% for humans. Models without computer vision capabilities, like GPT-4, perform worse even with human-authored image descriptions. This highlights a significant gap in humor understanding between humans and language models.</sample>
    <sample id="12">An der Arbeit sind sechs Autoren beteiligt.</sample>
    <sample id="13">Daniel Rotem presents his work on adaptive inference in low - resource settings. He discusses two methods: multi - model and early exit. Multi - model stores multiple models with classifiers, while early exit fits classifiers at intermediate layers. Multi - model is versatile but expensive and suffers from overhead. Early exit is faster and memory - efficient but can lead to lower performance due to conflicting gradients. Rotem proposes Sweet, a novel fine - tuning method for early exit architectures, which avoids conflicting gradients. Sweet closes the gap between early exit and multi - model but can negatively affect later classifiers in some cases.</sample>
    <sample id="14">Hallo, mein Name ist Adam Skorokowski und dieses Vortrag ist über die Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in der universellen Abhängigkeit ist die Struktur der Koordination Lisa, Bart und Maggie so, dass der erste Konjunkt der Kopf der ganzen Koordinationsstruktur ist. In diesem Fall Lisa. Ein ähnlicher Ansatz wird in Igor Melchuk's "Meaning Text Theory" angenommen, wo wiederum die gesamte Koordinationsstruktur vom ersten Konjunkt geleitet wird. Diese beiden Ansätze sind symmetrisch, sie heben einen der Konjunkte hervor. Es gibt auch symmetrische Ansätze zur Koordination, wie der "Conjunction Headed Approach" in der "Prop-Dependency Treebank", wo Koordinationsstrukturen vom Konjunktionkopf geleitet werden. So erhalten wir Abhängigkeiten von "and" zu allen Konjunkten. Und schließlich gibt es auch einen multi-headed Ans</sample>
    <sample id="15">Drei.</sample>
    <sample id="16">Bible texts.</sample>
    <sample id="17">Abstract: This work introduces a method for multimodal relation extraction. It addresses internal information overutilization and external information underexploitation. A graph information bottlenecks - guided feature refinement is proposed. The multimodal topic information is considered as additional semantic supplementary. The method consists of five parts: representing text and image, merging visual and textual syngraphs into a unified backbone cross - model graph, screening the initial graph structures, enriching compressed CMG features with multimodal topic features, and evaluating on MIRE dataset. The proposed method outperforms text - based methods and other multimodal baselines.</sample>
    <sample id="18">March read this absolutely fascinating book about bees yesterday.</sample>
    <sample id="19">Abstract: This work focuses on efficient open - domain question answering. It presents a two - stage model proposed by Chen in 2017. The first stage uses a retriever to retrieve evidence contexts from the Wikipedia corpus, and the second stage uses a reader to understand the question and retrieve evidence to reason out the answer. Challenges include the large size of the Wikipedia corpus, the bottleneck of inference speed in searching the index file, and the need for efficient techniques to achieve smaller memory cost, faster inference, and comparable performance. Existing models are compared in terms of data aspects, showing that retrieval - and - reader - based systems perform well - balanced among speed, memory, and performance. Conclusions are drawn based on this analysis.</sample>
    <sample id="20">Ja, die Modelle sind für Forschungszwecke frei verfügbar.</sample>
    <sample id="21">DEplain-apa enthält Dokumente aus News.</sample>
    <sample id="22">Ein besseres Modellarchitektur, größere Modellgröße und mehr Anpassungsbeispiele.</sample>
    <sample id="23">Dan Garrett discusses improvements in text - image models. These models have advanced in generating high - quality images but struggle with text representation. The Imagine model uses a T5 - XL encoder and a diffusion model. However, it fails with simple text - image inputs. T5 uses sentence - piece tokenization, which affects its ability to render text accurately. Experiments show T5 has poor spelling accuracy, especially for frequent words. Larger T5 models perform better but still not perfectly. Palm models have better spelling accuracy but are impractical due to size and data requirements. Byte - T5, which receives individual bytes, excels in spelling. Garrett's team augmented the Imagine model with Byte - T5 small model to improve text rendering. This small addition significantly enhanced text representation.</sample>
    <sample id="24">Die Tendenz wurde gemessen, indem die Längendifferenz zwischen den beiden Konjunktionen gemessen wurde.</sample>
    <sample id="25">Nicht verfügbar.</sample>
    <sample id="26">Nicht gut. Er funktioniert nicht viel besser als Zufall.</sample>
    <sample id="27">Das ist eine Frage, die ich nicht beantworten kann, da ich den englischen Inhalt nicht verstehe. Bitte übersetzen Sie den Inhalt ins Deutsche, dann kann ich Ihnen besser helfen.</sample>
    <sample id="28">Bob und Alice.</sample>
    <sample id="29">Bei den diskursbezogenen Phänomenen schneiden kontextsensitive MÜ-Modelle besser ab als kontextagnostische Modelle.</sample>
    <sample id="30">Abstract: We introduce LLM Blender, an effective ensemble learning framework for large language models. It uses pairwise ranking and generative fusion. Our team from AI2 and USC developed it. Many large language models claim great performance, but LLM Blender suggests optimal model selection varies across examples. It proposes a two - stage framework. First, run different models on input X to get outputs. Then, use a pairwise ranking module, ParRanker, to compare candidates. ParRanker encodes pairs of candidates and input X for better analysis. It uses cross - attention modules like Roberta. In the next stage, select top - k candidates and use them as input to a sequence - to - sequence model for generative fusion. This framework outputs the final result by fusing top - k candidates. Experiments show ParRanker is better correlated with oracle ranking than other methods. We also created MixInstruct, a dataset for evaluating ensemble learning frameworks. Empirical results show LLM Blender outperforms top - two models on four metrics.</sample>
    <sample id="31">Ich kann das aus dem gegebenen Text nicht direkt ableiten. Es gibt aber eine Möglichkeit, es herauszufinden. Du könntest die Autoren in der Liste durchsuchen und ihre Affiliationen nachschlagen. Oder du fragst mich, ob ich dir helfen kann, die Autoren zu identifizieren und dann ihre Universität zu finden.</sample>
    <sample id="33">Das Framework quantifiziert die Positionalität durch den Vergleich der Annotierungen von Endnutzern mit den Modellen und Datensätzen, was durch einen Pearson's R-Korrelations-Score erfolgt.</sample>
    <sample id="34">Crest is a joint framework for rationalization in counterfactual text generation. It combines selective rationalization and counterfactual editing. The rationalizer model generates rationales for the original input, which are then used to create counterfactual examples. These examples are evaluated by humans and found to be more valid and natural than those generated by other methods. Crest can also be used for data augmentation. It achieves top accuracy on the AMDB dataset and performs well on other datasets. The rationales generated by Crest are analyzed for their interpretability.</sample>
    <sample id="36">This abstract introduces a method for multilingual machine translation using language-specific layers, LSLs. It highlights advantages like scalability, speed, and improved performance for low-resource languages. The approach involves training a model with shared and language-specific layers, where the model learns the best layer placement. Experiments show significant improvements over baseline models and language adapters, while maintaining fast inference times.</sample>
    <sample id="37">Die Ergebnisse der vorherigen Studie zeigten, dass die menschlichen Teilnehmenden durch die gleichen Persona-Prompts Racial-Stereotypen aufdecken konnten.</sample>
    <sample id="38">Die Datenquellen wurden aus der Enhanced Version des Penn Treebank stammend.</sample>
    <sample id="39">Drei.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die Klassifikation von Dissonanz in unabhängigen Themen und die binäre Klassifikation von Erweiterung und Vergleichsklassen von PDB.</sample>
    <sample id="41">This abstract introduces Peacock, a personal common sense knowledge graph for consistent and engaging narratives. It contains about 3, 800 personas and 40, 000 attributes, forming 100, 000 personal inferences. Peacock is built in three steps: selecting personas from existing graphs, inducing attributes, and cross - annotating relations. It helps language models learn and generalize personal knowledge, improving automatic evaluation results and human accept rate. Peacock also enhances downstream narrative modeling, achieving better dialog generation in terms of fluency, consistency, engagement, and personal expression compared to other augmentation methods.</sample>
    <sample id="42">I'm not sure. The text doesn't mention the number of authors. You could try looking for more information in the paper.</sample>
    <sample id="43">The answer is not provided in the text.</sample>
    <sample id="44">Das Framework unterscheidet sich von bisherigen Arbeiten, indem es End-Nutzer mit Modellen und Datensatzvorhersagen und -etikettierungen vergleicht, anstatt nur Annotatorenübereinstimmung oder Modellierungs-Annotatordistributionen zu betrachten.</sample>
    <sample id="45">Das Setup mit den meisten Überschneidungen mit dem Lexikon der Stereotypen ist das generierte Personas.</sample>
    <sample id="46">Es wird nicht erwähnt, welche kommerziellen Systeme verglichen wurden.</sample>
    <sample id="47">Hallo, ich bin Zhang Bing, ein PhD-Student an der University of Washington. Heute präsentiere ich unsere Arbeit von Prätrainingsdaten bis hin zu Sprachmodellen und Downstream-Aufgaben. Wir verfolgen die Spuren politischer Biass, die zu unfairen NLP-Modellen führen. Sprachmodelle werden an großem Maßstab an Webkrawldaten trainiert. Politische Nachrichtenmedien sind in ihren Prätrainingsdaten gut abgedeckt. Laut einer Umfrage des C4-Korpus können wir sehen, dass The New York Times, Los Angeles Times, The Guardian, The Huffington Post usw. in den Sprachmodell-Trainingsdaten gut abgedeckt sind. Dies hat für Sprachmodell-Anwendungen ein gemischtes Segen geschaffen. So haben sie einerseits die Möglichkeit, aus verschiedenen Perspektiven zu lernen, was die Demokratie und die Vielfalt von Ideen feiert. Andererseits sind diese verschiedenen politischen Meinungen im Wesentlichen sozial voreingenommen und können potenzielle Fairnessprobleme in Downstream-Aufgabenan</sample>
    <sample id="48">The paper is a joint work with my colleagues from Google Translate. So there are at least two authors, you and your colleagues.</sample>
    <sample id="49">Bis zu 1024 Token Kontextlänge wurden MPP-Auswertungen durchgeführt.</sample>
    <sample id="50">The presentation introduces D Plain, a new corpus for German text simplification. It defines text simplification as adapting text for better comprehension by non-native speakers. The corpus is split into two subcorpora: D Plain APA and D Plain Web. D Plain APA is based on news texts, with 483 documents manually aligned, resulting in 30,000 parallel sentence pairs. D Plain Web includes different domains, with 750 documents aligned manually and automatically, yielding 30,450 sentence pairs. The corpus has a high variety of simplification transformations. Use cases include evaluating automatic alignment methods and automatic text simplification by fine - tuning language models. The best automatic alignment method for German text simplification is the method of mass align.</sample>
    <sample id="51">Sie haben drei Domains aufgenommen: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität ist die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen haben.</sample>
    <sample id="53">David.</sample>
    <sample id="54">Abstract: This paper presents a study on cognitive dissonance detection in language. It defines cognitive dissonance as inconsistent beliefs or actions, like a person knowing cigarettes are harmful but still smoking. The study aims to understand the effects of disagreement, track trends in beliefs, and relate high dissonance to mental health issues. A large - scale annotation of dissonance relations was conducted using a dissonance - first approach. However, due to the rarity of dissonance, transfer learning and active learning were employed to improve detection. Transfer learning from related tasks like topic - independent dissonance classification and binary classification of expansion and comparison classes of PDB was used. Active learning strategies, including cumulative and iterative updates, were compared. The proposed probability of rare class strategy, PRC, was found to be more effective than other state - of - the - art strategies. This work contributes to understanding cognitive dissonance in language and its implications for mental health and extremism.</sample>
    <sample id="55">Ja.</sample>
    <sample id="56">Ich kann das aus dem gegebenen Text nicht direkt ableiten. Es gibt aber eine Möglichkeit, es herauszufinden. Du könntest versuchen, den Text zu lesen und nach Hinweisen auf die Anzahl der Autoren zu suchen. Wenn du mehr Details hast, kann ich dir weiterhelfen.</sample>
    <sample id="57">Nicht wirklich. Es funktioniert nicht gut.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: 1) Back on Pretrain, 2) Back on Both, 3) Back on Inference.</sample>
    <sample id="59">This abstract presents a work on Dr. Bert, a robust pre-trained model in French for biomedical and clinical domains. It discusses language modeling in healthcare, introduces Dr. Bert based on Roberta and trained on Nachos, compares it with other models, and presents results on 11 biomedical and clinical downstream tasks in French. The experiments conclude that specialized data is better but doesn't scale well. The models are freely available on the Hugging Face interface and training scripts are on the GitHub repository.</sample>
    <sample id="60">Ich habe keine Informationen über die Universitäten der Autoren. Kannst du mir mehr dazu sagen?</sample>
    <sample id="61">Die abschließende Forschungsfrage ist, ob man nur die sauberen Samples für die Validierung verwenden sollte oder ob es bessere Wege gibt, sie zu nutzen.</sample>
    <sample id="62">This paper presents a systematic study on knowledge distillation for natural language generation, NLG, with pseudo - target training. It aims to compress large language models while preserving performance. The study explores different approaches, including pruning and knowledge distillation. Two main types of knowledge distillation are discussed: word - level and sequence - level. The study contrasts with previous works focusing on classification tasks or specific tasks like translation. It considers a variety of NLG tasks in realistic setups, using medium - sized labeled and unlabeled data, and medium - sized shared models. The study evaluates different knowledge distillation methods and extends the use of pseudo - targets. It shows the importance of unlabeled data and demonstrates that generating multiple pseudo - targets improves the student model.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, konstante Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von leichten Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jingwei Yi.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet nicht immer eine bessere Leistung des Modells. Es kann auch zu einer geringeren Leistung führen.</sample>
    <sample id="66">Mathematical reasoning is crucial for human intelligence. It involves comprehending and making decisions based on numeric data and language. Deep learning methods are being developed to solve math problems and prove theorems. Tasks include math word problems with single or multiple operations, and extending to mathematical information like images, figures, and tables. Two primary categories are visual contexts and table contexts. Solving geometry problems is a high school education subject. It involves identifying geometric relations, applying theorem knowledge, and performing calculations. This can be formalized as a neuro - symbolic reasoning problem over geometric diagrams, theorems, and solvers. Another important line is automatic theorem proving. Some datasets have been proposed to test the human - level intelligence of language models. Various neural network architectures have been proposed for mathematical reasoning tasks. For example, a second - to - sixth model uses an encoder - decoder architecture. Mathematical expressions can be represented as tree - based structures. Pre - trained language models like large - scale language models, LLMs, have shown remarkable performance on various NLP tasks. They can be applied to solve math word problems. However, they lack the ability to perform precise mathematical reasoning. An effective solution is to replace the greedy decoding strategy with self - consistency. Another approach is to design tool</sample>
    <sample id="67">Interference in multilingual translation models can occur when the model is small compared to data size. Tuning sampling temperature is key for strong performance. For the multilingual case, severe interference happens in parameter poverty settings. Language similarity and number of languages have less impact. Temperature sampling is a simple solution. A baseline for battling interference is weak due to size in small models and uncalibrated temperature for larger ones. Model and data size affect interference levels significantly, while other factors like language similarity affect much less. Modest scale and tuned temperature can reduce interference problems significantly.</sample>
    <sample id="68">Nicht bekannt.</sample>
    <sample id="69">Typischerweise werden für eine gute Leistung an der WSL normalerweise 20 saubere Validierungsbeispiele pro Klasse benötigt.</sample>
    <sample id="70">Ich kann das aus dem gegebenen Text nicht direkt ableiten. Du könntest versuchen, mehr Informationen zu suchen oder den Text weiter zu analysieren.</sample>
    <sample id="71">The work introduces the "alt entities corpus" for resolving indirect referring expressions in entity selection. It aims to understand users' language when making choices. The corpus is collected using crowdsourcing for music, books, and recipes domains. A cartoon completion setup is used, with three speech bubbles. The first sets the dialogue context, the second presents an alternative question, and the third uses an indirect reference. The second bubble is generated using a simple template with entities sampled from Wikipedia. Different sampling methods are used to increase similarity between entities. Annotators are provided background knowledge about the entities and asked to pick one and describe it using indirect referring expressions. This corpus is valuable for benchmarking entity understanding in conversational systems.</sample>
    <sample id="72">Weil die bestehenden Methoden nicht ausreichen, um die Medienverzerrungen in den neuen Medien zu messen.</sample>
    <sample id="73">The referent is Makshita.</sample>
    <sample id="74">This paper introduces Dens atomic, a knowledge graph with high logic coverage and massive multi-hop paths. It is a large-scale commonsense knowledge base that covers event - centered social aspects of information. Dens atomic completes many missing links in Atomic, such as B2A, B2B, A2B, and A2A links. It also contains more multi-hop paths. The construction of Dens atomic mainly consists of three parts: normalized tail events, training a relation prediction model, and constructing Dens atomic. The normalized tail events convert tail events into the same expression as the head event. The traditional method for completion of Atomic has two limitations: sparse graph structure and inability to sufficiently utilize semantic information of events. To address these, we propose Real - SKGC, which predicts the relation given the head event and the tail event of the triplet. Real - SKGC has two advantages: utilizing no graph structure information and taking advantage of semantic information. It is computationally expensive to iterate over all pairs of head and tail events during inference. To overcome this, we consider each base event and its related tail event as a cluster and design an intra - and inter - cluster completion strategy. We compare Real - SKGC with relation prediction methods and translation - based methods and find</sample>
    <sample id="75">Abstract: This work presents a joint semi-supervised learning framework for entity and relation extraction. It aims to address the issue of ignoring interconnections between tasks. The framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span features are generated based on contextualized representations and a trained classifier. A K - nearest neighbor graph is constructed for efficient similarity examination. Label propagation diffuses labels through the graph, refining pseudo - labels until convergence. The model is optimized by filtering low - quality pseudo - labels and retraining the classification model with the rest. This framework fully exploits connections among label and unlabeled data, improving the performance of entity and relation extraction tasks.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile reicht von der Vorkonditionierung von Daten bis hinunter zu Sprachmodellen und dann zu unterfließenden Aufgaben. Es gibt eine Mischung von Vorteilen und Herausforderungen. Auf der einen Seite können Sprachmodelle von diversen Perspektiven lernen, was die Demokratie und die Vielfalt von Ideen feiert. Auf der anderen Seite sind diese verschiedenen politischen Meinungen im Wesentlichen sozial voreingenommen und können potenzielle Gerechtigkeitsprobleme in unterfließenden Aufgabenanwendungen hervorrufen.</sample>
    <sample id="77">This work presents a joint effort from Yale University and Microsoft Research on improving summarization factual consistency using natural language feedback. A new dataset, DeFacto, is introduced, containing human demonstrations and feedback. Comprehensive analysis is provided, offering insights into factual consistency of summarization models. Three new NLP tasks are proposed: summary editing, feedback generation, and automatic factual error correction. The focus is on abstractive text summarization, specifically studying factual consistency. Human demonstrations and feedback are based on system-generated summaries. Annotators label summaries for factual consistency and provide corrected summaries and feedback. Data from the XSum dataset is used, with initial system outputs from a pretrained Pix2S model. Around 2.5k data points are collected, 70% containing factual errors. Human edited summaries receive higher automatic factual scores but have lower textual overlap with reference summaries. The study shows that both fine-tuned and zero-shot large language models can effectively leverage human feedback for summary editing. Feedback generation remains challenging. The third task aims to automatically correct factual errors while generating explanations.</sample>
    <sample id="78">Ja, es gibt Unterschiede. In DEplain-apa gibt es mehr Reorderings und Wort-Additionen, während in Web mehr Rephrasings vorkommen.</sample>
    <sample id="79">Ja.</sample>
    <sample id="80">In Watermark injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weight summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="81">Die Autoren gehören der Penn State University an.</sample>
    <sample id="82">Abstract: This work proposes a novel framework for unsupervised automated essay scoring, URA. It introduces multiple heuristic quality signals as pseudo ground truths. The URA contains a heuristic essay ranking module, HER, which generates partial order pairs by ranking essays according to different quality signals. A deep pairwise rank aggregation module, DPRAM, aggregates these partial order pairs into unified supervision for training a neural AES model. The model learns to judge the partial order relationship of essay quality. A deep pairwise rank aggregation loss with learnable confidence weights is designed to address inconsistent partial order supervision. In the model inference stage, a scoring strategy is proposed to transform predicted scores into predefined score set range. Experiments on transductive and inductive settings show that URA outperforms other unsupervised baselines with significant improvement.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">Abstract: This paper introduces a new framework for dynamic networks. It contrasts static and dynamic networks, highlighting the flexibility of dynamic networks in adapting to input changes. The implementation is simple, just replacing static layers with dynamic ones. However, existing fully dynamic networks suffer from excessive parameter usage. To address this, the authors propose a partial dynamic network framework. They partition parameters into dynamic and static, using scale factors to control the intensity of each mode. Experiments show that their framework outperforms static and fully dynamic networks while maintaining fewer parameters and less computation. Omission studies determine optimal dynamic ratios for dynamic convolution and mixture of experts. Scale factors for dynamic and static parameters are crucial for accuracy.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist das Erstellen eines Schokoladenkuchens mit spezifischen Anforderungen.</sample>
    <sample id="86">The watermark should be covert enough to the attacker, or the attacker can remove the watermark easily.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen, indem sie die Gewichtsverteilung und Tokenisierung von BERT verwendet und auf einem Subset von NACOS trainiert.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf Indien ausgerichtet.</sample>
    <sample id="89">Der Beispielsatz ist nicht direkt im Text erwähnt.</sample>
    <sample id="90">This paper questions the need for native speakers in data annotation for language learning. It shows that language learners can contribute effectively. Through experiments, learners' annotations are nearly accurate, especially for simpler tasks. When aggregated, they perform almost as well as native speakers. The paper suggests a new way to build data for low - resource languages by recruiting learners. It also finds that learners' language proficiency improves during annotation tasks. This work opens up possibilities for broadening NLP research for many languages, overcoming geographic and technological barriers.</sample>
    <sample id="91">Als die Anzahl der Aufgaben zunimmt, erreicht das Modell bessere Leistung und gleichzeitig eine niedrigere Sensitivität.</sample>
    <sample id="92">I'm sorry, but the text doesn't mention three treeless baselines that the authors compare their method to. Could you provide more context or clarify your question?</sample>
    <sample id="93">Sie sind seine Berater.</sample>
    <sample id="94">Abstract: This paper introduces EmbeddingMarker, a backdoor - based watermark method for protecting the copyright of embedding services. It consists of watermark injection and copyright verification. In watermark injection, a trigger set is selected, and the provided embedding is a weighted sum of the target embedding and the original embedding. Copyright verification uses a backdoor dataset and a benign dataset to detect the watermark. Experiments on four datasets show EmbeddingMarker has great detection performance while maintaining utility for downstream tasks. The provided embedding is covert, as visualized by PCA.</sample>
    <sample id="95">The first author of PaLM is not mentioned in the text.</sample>
    <sample id="96">Hallo, ich bin Jenny, ein erstsemestler Masterstudent an der Carnegie Mellon University. Heute werde ich über meine Arbeit "Analyse der Positionalität: Charakterisierung von Designbiass in Datensätzen und Modellen" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit einigen Leuten von der University of Washington und dem Allen Institute for AI durchgeführt, insbesondere Sebastian Santi, Ronan Le Bras, Katerina Rineka und Martin Zap. Lass uns damit beginnen, dass du dir vorstellst, dass du für eine Zeitung arbeitest und durch die Kommentare unter deinem Nachrichtenartikel schürst, um giftige Inhalte zu entfernen. Du könntest auf eine beliebte API wie den Perspective API für Giftigkeitserkennung zurückgreifen. Dies funktioniert wirklich gut, wenn du Carl Jones bist, wo der Perspective API korrekt giftige Instanzen erkennen kann. Aber das ist nicht der Fall für Dipti Sharma, wo der Perspective API nicht so empfindlich auf offensive Begriffe ist, die in indischen Kontexten häufiger vorkommen. Dies ist ein Beispiel für einen Designbias, bei dem wir system</sample>
    <sample id="97">Die Referentin geht auf zwei Probleme von SimulST ein.</sample>
    <sample id="98">Das ist eine komplexe Frage. Es gibt keine einfache Antwort. Man kann versuchen, diverse Quellen zu nutzen, um ein breiteres Spektrum von Meinungen und Perspektiven zu berücksichtigen. Aber es ist schwierig, komplett zu vermeiden. Es gibt auch Forschungen, die versuchen, spezifische politische Verzerrungen durch spezielle Prüfungen und Experimente zu identifizieren und zu korrigieren. Wenn du mehr über spezifische Methoden erfahren möchtest, kannst du mich fragen.</sample>
    <sample id="99">Hallo, ich bin Si Yuan von der Fudan University. Ich bin hier, um unsere Arbeit vorzustellen: "Distanzierung von Skriptwissen von großen Sprachmodellen für beschränkte Sprachplanung". In unserem Alltagsleben planen Menschen ihre Handlungen oft nach Schritt-für-Schritt-Anweisungen in Form von Skripten. Vorige Arbeiten haben die Nutzung großer Sprachmodelle für die Planung abstrakter Ziele von stereotypischen Aktivitäten, wie das Backen eines Kuchens, untersucht und gezeigt, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentrieren sich die meisten früheren Arbeiten auf die Planung abstrakter Ziele von stereotypischen Aktivitäten. Die Planung spezifischer Ziele mit spezifischen Beschränkungen, wie das Backen eines Schokoladenkuchens, bleibt noch unerforscht.In dieser Arbeit definieren wir das Problem der beschränkten Sprachplanung, bei der verschiedene Beschränkungen auf die Ziele der Planung</sample>
    <sample id="100">Multi-hop QA involves answering questions requiring multiple reasoning steps. Each step corresponds to a document in the corpus. For example, to answer a question about a 1988 Christmas comedy film Brian Doyle-Murray starred in, we first find all movies he starred in and then identify the 1988 film. The chain refers to the set of documents needed to answer a question. Multi-hop retrievers are trained by maximizing the probability of the ground truth chain given questions. Existing systems need thousands of examples for good performance, which can be expensive, especially for low-resource domains. Our approach, prompt rank, is efficient and performs well with just 128 examples. It combines an unsupervised retrieval method with a few-shot language model re-ranker. First, a pool of candidate chains is retrieved using TF-IDF and hyperlink traversal. Then, these candidates are re-ranked using the few-shot language model. The scoring function is the likelihood of the question given the chain according to the language model. We construct a chain prompt by inserting the chain documents into a prompt with an indicator token and an instruction. Additional techniques like instruction search, instruction sampling, and temperature scaling are explored. We experiment with GPT-2-xl and T5-xl and evaluate on Hotpot</sample>
    <sample id="101">PaLM hat eine Sprachgewandtheit, die vergleichbar ist mit der von State-of-the-Art-Systemen. Es hat einige Probleme mit der Genauigkeit, insbesondere mit Omissionen. Aber es produziert flüssige Übersetzungen.</sample>
    <sample id="102">Das Wasserzeichenverfahren muss anwendbar auf Embedding-As-Services sein, den Nutzen der bereitgestellten Embeddings nicht vermindern, versteckt genug für den Angreifer sein, damit dieser den Wasserzeichen leicht entfernen kann und während des Modell-Ausbauprozesses übertragbar auf die Angreifer-Service sein.</sample>
    <sample id="103">Die englischen TED Talks wurden in 14 verschiedene Sprachen übersetzt.</sample>
    <sample id="104">The text doesn't specify how many instances are extracted from a dataset for re-annotation.</sample>
    <sample id="105">Die Cosine- und L2 -Distanz werden verwendet.</sample>
    <sample id="106">The paper presents the Quest dataset, a retrieval dataset for queries with implicit set constraints. It includes 13, 000 entity - seeking queries. The dataset is challenging as systems need to search a large document corpus for multi - answer sets. Queries involve set operations like intersection and difference. The dataset is constructed using Wikipedia category names from four domains. Human annotators paraphrase and validate queries. Entities in answer sets are verified for relevance. The dataset is used to evaluate systems for retrieving multi - answer sets from a large document corpus. Sparse and dense retrievers, as well as a T5 - based retriever, are considered as baselines. The results show room for improvement in retrieval performance, especially for queries with set intersection and difference.</sample>
    <sample id="107">In der Aufgabe wurden Encoder-Decoder-Modelle, die auf einem mehrsprachigen Encoder basieren, eingesetzt.</sample>
    <sample id="108">This abstract presents a study on language model acceptability judgments. It revisits the minimal pair paradigm, evaluating models on acceptability and grammaticality. The current pipeline doesn't handle longer sentences well. The study aims to assess models' acceptability across context windows. Longer sequences are simulated by choosing acceptable and unacceptable sentences from datasets. For example, grammaticality pairs from the Blimp dataset are used. Longer acceptable sequences are created by adding grammatical prefixes. The study finds that MPP judgments are mostly robust for arbitrary context length when unrelated sentences are used. However, when sentences from the same dataset are used, judgments significantly change with acceptable or unacceptable prefixes. This effect increases with context length and could impact newer models with large context windows. Analysis shows that match prefixes affect judgments by preserving structure but adding noise.</sample>
    <sample id="109">Natural Instructions is a dataset of natural language instructions and corresponding inputs and outputs. It was collected in a fully automatic manner without human annotations. The dataset contains 64k examples, with about 240k when instruction paraphrases are considered. It focuses on creativity, diversity, and correctness. More than 50% of the generated examples are correct. The dataset includes highly creative tasks, some very different from classic NLP tasks. It was used to fine-tune an 11 billion parameter T5 model, which outperformed both T0 and T5 instruct on several benchmarks. When the cost of generating examples is amortized, training on Natural Instructions outperforms a baseline on all benchmarks. Natural Instructions highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers. It is faster and cheaper than human annotation.</sample>
    <sample id="111">The authors assume the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="112">Hallo, mein Name ist Shuheng. Heute werde ich unser Papier präsentieren: "Do CoNLL-2003 Named Entity Taggers still work well in 2023?". Lass uns anfangen.Unser Papier untersucht das Problem der Generalisierung unter Verwendung der Aufgabe der benannten Entitätserkennung, NER -Aufgabe. Wir beobachten, dass Modelle CoNLL-2003 für fast 20 Jahre verwendet haben, um NER zu entwickeln, und dies wirft natürlich mehrere Probleme auf. Erstens, können diese Modelle auf modernes Datenmaterial generalisieren? Und wenn wir neue Tagger entwickeln, was ist für eine gute Generalisierung notwendig? Gleichzeitig, wenn wir eine schlechte Generalisierung beobachten, was verursacht den Leistungsabfall dieser Modelle?Um diese Probleme zu untersuchen, haben wir den CoNLL++ Datensatz entwickelt. Dies ist ein Datensatz, den wir aus Reuters - Nachrichten von 2020 gesammelt und mit den gleichen CoNLL-2003 - Annotation</sample>
    <sample id="114">Abstract: This work focuses on addressing the heavy parameter problem of large language models. It proposes group head attention, using a divide - and - conquer strategy. The first stage is group constraint training, dividing attention heads into groups to make intra - group heads more similar and inter - group heads more separate. The second stage is the voting - to - stay algorithm, which prunes redundant multi - head attention, keeping only one head per group. This approach achieves significant parameter compression, up to 90% in extreme conditions, while maintaining good performance on tasks like machine translation, language modeling, and abstract summarization.</sample>
    <sample id="115">Die genaue Sprachsegmentgröße wird nicht erwähnt.</sample>
    <sample id="116">Im Beispiel wird das Wissen benötigt, dass Servin ein Richter ist.</sample>
    <sample id="117">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="118">Abstract: This paper presents a novel approach to improving pre - training techniques for code - switched NLP. It defines code - switching, gives an example, and explains the importance of building computational models for it. The work proposes novel MLM techniques, architectural changes, and auxiliary loss tuned for code - switching. It introduces Switch - MLM, a variant of MLM that focuses on switch points. A surrogate method called Frequency - MLM is also proposed. Architectural modifications like residual connections are suggested to help with code - switching. The results show that the combined method performs best on sentiment analysis tasks. Probing experiments verify that the proposed methods increase switch - point information in intermediate and final layers.</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf GPT-4 und GPT-3.</sample>
    <sample id="120">Das Modell verwendet Werte aus mehreren Ebenen.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind das Nennen des Namens des Liedes, zum Beispiel "Easy on Me", oder die Angabe seiner Position, zum Beispiel "das erste Lied".</sample>
    <sample id="122">Fudan University.</sample>
    <sample id="123">In this research, we explore multi - model zero - shot learning via instruction tuning. We investigate if instruction tuning on multi - model pre - trained models can improve generalization to unseen multi - model tasks. We build Multi - Instruct, the first multi - model instruction tuning benchmark dataset, with 62 diverse tasks from 10 broad categories. Using OFA as the base model, we show that instruction tuning significantly improves OFA's performance on seen multi - model tasks. Transfer learning from natural instruction datasets also benefits instruction tuning. As the number of tasks increases, the model achieves better performance but lower sensitivity. Using more instructions improves overall performance and reduces sensitivity. This demonstrates the effectiveness of different fine - tuning strategies on model sensitivity.</sample>
    <sample id="124">This abstract introduces a study on temporal reasoning in language models, LMs. It breaks down temporal reasoning into three levels: time - to - time, time - to - event, and event - to - event. The work aims to comprehensively study temporal reasoning, differentiating from prior works that overemphasize L2 reasoning. A preliminary experiment on L1 prediction of years is conducted, showing ChatGPT's strong bias towards the 2000 - 2020 time period. A new dataset, Temp Reason, is proposed, covering all three levels of reasoning and long temporal coverage. Evaluation is done in three QA settings: closed - book, open - book, and ReasonQA. A training strategy with temporal span extraction pre - training and time - sensitive reinforcement learning is proposed to improve temporal reasoning capabilities. The final model, TimeT5, significantly outperforms ChatGPT in OBQA and ReasonQA settings, especially in L2 reasoning.</sample>
    <sample id="125">Ich kann das aus dem gegebenen Text nicht direkt ableiten. Es gibt aber eine Möglichkeit, es herauszufinden. Du könntest den Text durchsuchen, ob es Hinweise auf mehrere Autoren gibt. Wenn du mehr Details hast, kann ich dir weiterhelfen.</sample>
    <sample id="126">Ja.</sample>
    <sample id="127">Large language models struggle with complex reasoning tasks due to high memory and computation requirements. This paper proposes using large models as reasoning teachers to transfer their abilities to smaller models. A novel technique called diverse reasoning is introduced to boost teaching. The method involves generating step - by - step solutions from large models and using them as training data for small models. Experiments show that this approach significantly improves complex reasoning performance on various tasks compared to existing methods. Diverse reasoning enhances performance further. The method is highly scalable but comes with trade - offs regarding development and inference time costs. For more details, refer to the full paper.</sample>
    <sample id="128">The work presents a diagnostic test suite for knowledge integration in natural language understanding models. It introduces a coreference resolution task to evaluate models' ability to use knowledge from different sources. The dataset is evaluated with human participants and established models. Three settings are defined: background pretrain, background both, and background inference. The results show that models trained on the dataset perform better than random choice, suggesting they learn to exploit surface cues. However, even the best models struggle with integrating background knowledge presented only at inference time. This highlights the need for task-specific training for models to effectively integrate knowledge from multiple sources.</sample>
    <sample id="129">Die Autoren haben als Beispiel für eine markierte Gruppe die schwarze Frau gegeben.</sample>
    <sample id="130">Nicht genannte Modellarchitekturen generalisieren nicht gut.</sample>
    <sample id="131">The test sets are not named in the text.</sample>
    <sample id="132">Zwei.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="135">ABC eval is a new approach to evaluating conversational AI. It was developed by the Emory NLP lab and Amazon Alexa AI. Instead of relying on human evaluation, it annotates model responses for various behaviors. This method is more precise and reliable. It measures rates of errors like irrelevant responses, contradictions, and common sense violations. When tested on four state - of - the - art chat models, ABC eval showed higher reliability and predictive power compared to existing methods. It can evaluate multiple dimensions of chat quality, providing a higher resolution than previous approaches. The evaluation reveals challenges in chat models, such as common sense violations and irrelevant information. With the rapid development in the field, ABC eval can be a useful tool for comparing models. It has the potential to advance the field of conversational AI.</sample>
    <sample id="136">The work presented by Chad Savan and his supervisor Nafisa at the University of Sheffield focuses on numerical reasoning. They introduce Fermat, a flexible evaluation set based on arithmetic types, to assess models' performance in number understanding, mathematical operations, and training dependency. The motivation behind this work is to address the limitations of current benchmarks in evaluating models' mathematical abilities. Fermat includes math worded questions extracted from Illinois Common Core, with numbers changed to mimic real-life scenarios. The evaluation shows that most models perform poorly across various aspects, with the original set performing slightly better. Fine-tuning using templates written by math teachers improves performance across different types of numbers and operations. The study also investigates the impact of training templates on model performance, revealing the importance of linguistic notation. Overall, Fermat provides a more comprehensive evaluation of models' numerical reasoning capabilities.</sample>
    <sample id="137">The abstract is about a work named "Tell to Design" from the Singapore University of Technology and Design. It discusses the need for a data set for language - guided floor plan generation. The work focuses on the challenges of generating floor plans from natural language instructions, such as strict constraints, understanding the big picture of the floor plan, and dealing with ambiguous information. The method uses a sequence - to - sequence approach under the encoder - decoder framework to generate floor plan layouts from language instructions. It aims to enable users to design floor plans by telling instructions, which is a new machine learning task. The data set is constructed using publicly available floor plans and human - annotated language instructions from Amazon Mechanical Turk, along with artificially generated instructions. The model is initialized by a pre - trained language model, T5, for better language understanding.</sample>
    <sample id="138">NLU</sample>
    <sample id="139">Die Referenten heißen Ying und Zhiyang.</sample>
    <sample id="140">Ja.</sample>
    <sample id="141">Die Ressourcen unterstützen nur begrenzte Typen kontextabhängiger Übersetzungen und begrenzte Sprachpaare.</sample>
    <sample id="142">Hallo, ich werde über unsere Arbeit zur Auflösung indirekter Referenzierungen für Entitätenauswahl sprechen, bei der wir das "altentitiescorpus" eingeführt haben. Mein Name ist Javad Hosseini, und es ist ein gemeinsames Projekt mit Filip Radlinski, Silvio Parodi und Ali Louis.Unser Ziel ist es, das Verständnis der Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Betrachten Sie diese alternative Frage: "Meintest du 'Easy on Me' oder 'I Got a Feeling'?" Hier möchte der Benutzer zwischen zwei Liedern wählen.Das offensichtlichste ist es, eine direkte Referenz zu verwenden, zum Beispiel, indem man den Namen des Liedes, "Easy on Me", oder seine Position, "das erste", nennt. Aber manchmal ist eine indirekte Referenz für eine natürlichere Konversation angemessen.Dies kann passieren, wenn der Benutzer sich nicht an den Namen des Liedes erinnern kann oder wenn die Aussprachen zu ähnlich sind und es schwierig ist, sie zu unterscheiden. Oder wenn der Ben</sample>
    <sample id="143">Mit den bestehenden SimulST-Richtlinien, die auf offline-Modellen angewendet werden, wie der Whitkey-Strategie und dem lokalen Abkommen, sowie mit den state-of-the-art-Architekturen, die speziell für die simultane Sprachübersetzung entwickelt wurden, wird der Ansatz verglichen.</sample>
    <sample id="144">Ich kann das aus dem gegebenen Text nicht direkt ableiten. Es gibt keine Angabe zur Universität der Autoren.</sample>
    <sample id="145">Jenny.</sample>
    <sample id="146">Dialogue summarization is a subtask of text summarization. It aims to create a concise summary representing important information in a dialogue. However, large - scale pretrained language models generate fluent but factually inaccurate summaries due to omission. This is a major problem affecting the quality of dialogue summarization. Our work analyzes the omission problem systematically. We find that even state - of - the - art models have a high omission rate, about 70% of summaries suffer from this issue. Omission is randomly distributed in dialogues regardless of length and domain. To address this, we construct a dataset for omission detection, which provides high - quality omission labels for dialogue summarization. We explore three frameworks as baselines and use precision, recall, and F1 - score to evaluate omission detection models. The results show that the task is challenging. We also investigate the use of omissions to refine summaries.</sample>
    <sample id="147">Drei.</sample>
    <sample id="148">Hallo Sarah! Ich bin ein deutscher Assistent. Ich kann den englischen Inhalt für dich übersetzen. Was genau möchtest du übersetzen?</sample>
    <sample id="149">Das ist nicht im Text erwähnt.</sample>
    <sample id="150">Meeting QA is an extractive question - answering dataset based on questions asked in meetings and their corresponding answer sentences. It contains 7, 700 questions split into train, dev, and test sets. Questions are diverse, with 30% unanswerable, 40% multi - span answers, and 48% multi - speaker answers. The dataset is unique as it focuses on the QA component in meeting discussions, which is often underutilized in prior works. It is collected from public meeting transcripts, with annotators labeling sentences and answer spans. The dataset has a high inter - annotator agreement of 0.73. The paper presents various methods for question - answering tasks, including context retrieval for short - context models, single - span and multi - span models, and data augmentation using silver annotations from the MediaSum dataset. Results show a significant gap between fine - tuned models and human performance, with short - context models slightly outperforming long - context models. Multi - span models have comparable performance to single - span models. Silver data augmentation improves zero - shot performance.</sample>
    <sample id="151">Hallo alle zusammen, mein Name ist Ying und mein Kollege Zhiyang und ich werden unsere Forschung über MultiInstruct präsentieren, die die Multi-Modell-Serienlernung durch Anweisungstuning verbessert. Mit den Fortschritten bei großen Sprachmodellen haben viele Arbeiten neue Lernparadigmen für die Wiederverwendung prätrainierter Sprachmodelle für verschiedene untergeordnete Aufgaben in einer parameter- und dateneffizienten Weise erforscht. Kürzlich haben viele Studien gezeigt, dass Anweisungstuning große Sprachmodelle ermöglicht, auf unähnliche Aufgaben in einer Serienlernweise durch die Befolgen natürlicher Anweisungen zu performieren. Allerdings haben die meisten früheren Arbeiten über Anweisungstuning sich auf die Verbesserung der Serienlernleistung auf Sprachnur-Aufgaben konzentriert, während Computer Vision und Multi-Modell-Aufgaben ausgelassen wurden. Daher wollen wir in dieser Arbeit untersuchen, ob Anweisungstuning an multimodellen prätrainierten Modellen tatsächlich die Generalis</sample>
    <sample id="152">Abstract: This presentation explores the intersection of NLP and classical philology. It introduces valuable resources for ancient Greek and Latin and discusses the implications and challenges of multilinguality in large language models. The current landscape of language models in classics is reviewed, highlighting the advancements in monolingual models for ancient Greek and Latin. The presentation then delves into the limitations of these models, such as being encoder - only and monolingual. A new project is introduced, aiming to create language models specifically designed for classical philology. Two monolingual models for ancient Greek, GriBERT and GREATER, are pre - trained. Additionally, multilingual models, FilBERT and Filter, are developed, trained on ancient Greek, Latin, and English data. The pre - training data for ancient Greek is gathered from Open Greek and Latin, previously unused resources, and a new corpus from the Internet Archive. The models are benchmarked on tasks like part - of - speech tagging, dependency parsing, and lemmatization. The results show that the models outperform the current state of the art for both ancient Greek and Latin. The presentation also analyzes the behavior of GREATER's encoder, revealing differences from native encoder - only models. Lemmatization performance is notably improved for both languages.</sample>
    <sample id="153">This abstract presents a study on ambiguities in text - to - image generative models. The researchers, postdoctoral scientists at Amazon Alexa's Responsible AI team, focus on resolving ambiguities in prompts provided to these models. They curate a benchmark dataset covering different types of ambiguities. A prompt disambiguation framework is proposed, which can either ask clarifying questions from the user or generate different possible visual setups. The framework aims to obtain a disambiguated prompt by concatenating the signal to the original ambiguous prompt. The researchers then evaluate whether the generated images are faithful to users' intentions using an automatic evaluation framework. They find that there is disparity in resolving ambiguities for different ambiguity types. However, disambiguation using their framework has a positive effect on faithful generation. Their automatic evaluation framework is in agreement with human evaluation, making it reliable for evaluating text - to - image models. For more details, refer to their paper.</sample>
    <sample id="154">Die Autoren gehören an der University of Toronto.</sample>
    <sample id="155">Javad Hosseini.</sample>
    <sample id="157">This work introduces a dialogue summarization method using a static - dynamic structure fusion graph. It aims to distill silent information from dialogue context into a concise summary. The method is a joint effort with several colleagues. It focuses on capturing the highlights of semi - structured and multi - participant dialogues. Existing methods heavily rely on pre - computed static graph structures and external linguistic tools, which have drawbacks like dependence on tool reliability and inability to adapt to downstream tasks. Our SDDS model has four main components. It employs an utterance encoder, constructs static graphs, proposes a static - dynamic graph module, and uses a pre - trained language model for summary generation. It captures static dialogue structure information through heuristic methods like discourse parsing graph and speaker relationship modeling. The model integrates static and dynamic graph information for better summarization.</sample>
    <sample id="158">Dual Cache for Long Document Neural Coreference Resolution introduces a method to handle coreference resolution in long documents. It addresses the high cache miss issue in cache - based methods by using a local cache with LRU eviction policy for local entities and a global cache with LFU policy for global entities. Evaluation on four public benchmarks shows that Dual Cache outperforms single cache methods, even with unbounded memory. It significantly reduces cache miss and has the highest performance - cost ratio.</sample>
    <sample id="159">Hallo, ich bin Costa Senna und freue mich, Sie zu unserer Präsentation unserer ACL 2023-Papiere zu begrüßen. "Sprachmodellakzeptabilitätsurteile sind nicht immer robust gegenüber Kontext". Es ist ein gemeinsames Projekt mit John Gog, Aaron Müller, Kanishka Mishra, Karen Venters, Roger Levy und Athena Villalón. Also in diesem Projekt haben wir den Minimal-Pair-Paradigma erneut beleuchtet. Das Minimal-Pair-Paradigma bewertet Sprachmodelle hauptsächlich anhand von Akzeptabilitätsurteilen, die auch grammatikalische Fragen wie Blimp-Syntax-Fehler oder Akzeptabilität in Bezug auf Stereotypen wie Crow's Pairs umfassen können. In diesem Minimal-Pair-Paradigma ist die typische Art, Sprachmodelle zu bewerten, dass man eine akzeptable oder grammatische Satz zeigt und dann einen unakzeptablen oder ungrammatikalischen Satz zeigt. Und die Hoffnung ist, dass das Modell mehr Wahrscheinlichkeit dem akzeptablen Satz zuweist.</sample>
    <sample id="160">Im ersten Schritt werden die Input-Token mit einem unordentlichen Multiset von Tokens zugeordnet, die im Output erscheinen werden.</sample>
    <sample id="161">Coscript enthält 55.000 spezifische Skripte.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die Methode von Mass Align.</sample>
    <sample id="164">Schwach überwachtes Lernen ist günstiger als menschliche Annotierungen, aber auch ungenauer.</sample>
    <sample id="165">Adaptive common sense reasoning is presented. It starts with a context and ends with an outcome. A set of possible explanations is given. The goal is to identify a plausible explanation that bridges the gap between context and outcome. Our paper considers a closed - world setting. Current approaches rely on supervised methods, which need the annotation of plausible explanations, but this can be noisy and subjective. We introduce an unsupervised learning method called LiPoR, which stands for likely who learning with posterior regularization. In LiPoR, we treat explanations as a latent variable. This leads to an unsupervised objective that maximizes the marginal likelihood of the outcome given the context. However, this objective only maximizes the likelihood of the outcome given the context. To prefer plausible explanations, we also need an additional regularizer. We use the mutual exclusivity of explanations as a characteristic. The LiPoR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over the others. The regularizer, denoted by Omega, takes the max between the entropy of P of Z given X, Y and the log of M, where M is the number of plausible explanations. When the entropy of P of Z given X, Y is larger than the log of M, it means</sample>
    <sample id="166">This abstract introduces a new divide - and - conquer learning framework for image retrieval from linguistically complex texts. It addresses the challenge of highly similar images and long descriptions. The framework is inspired by divide - and - conquer strategy and dual - process theory. It uses a proposition generator to decompose complex propositions into simple ones. System 1, the visual - linguistic interactor, performs visual - propositional information interaction. System 2, the neural symbolic reasoner, integrates reasoning states and results of simple propositions. The final solution is obtained by combining inference results of System 1 and System 2. Experimental results show that the proposed method outperforms baselines. The method is process - interpretable and suggests that neuro - symbolic calculation could improve large language models' compositional reasoning and planning capacity. Divide - and - conquer is compared to self - asking - child's thoughts in decomposing complex reasoning into simple problems.</sample>
    <sample id="167">In DEplain-web wurden 750 Dokumente manuell und automatisch ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde von Reuters News aus dem Jahr 2020 gesammelt und dann mit den gleichen CoNLL 2003-Annotationsschulungen annotiert.</sample>
    <sample id="169">The paper presents a study on prompting for machine translation using a 540 million parameter language model called Palm. It evaluates the translation quality of such models using best practices of the AMT community. The study compares Palm to state - of - the - art systems and provides recommendations for prompt selection strategies. Palm shows a big influence on translation performance, with differences of more than one BLEU point in some cases. A five - shot prompting strategy is used, where source sentences are marked with their language. The quality of examples is more important than their similarity to the source sentence. Selecting prompts from high - quality translations, like the dev data, leads to better performance. Palm comes close to commercial systems in fluency but has accuracy issues, often dropping parts of the source sentence. For more details, refer to the full paper presentation.</sample>
    <sample id="170">Hallo, mein Name ist Justin Zhang aus der Penn State University. Heute werde ich unsere Arbeit "Exemplar: Crosslingual Semantikparsen in mehreren natürlichen Sprachen und mehreren Repräsentationen" präsentieren.Semantikparsen ist eine Aufgabe, um semantische Repräsentationen von Benutzeranfragen wie SQL und Lambda Calculus zu bauen. Und Crosslingual Semantikparsen ist die Aufgabe, Anfragen in mehreren natürlichen Sprachen in mehrere Repräsentationen zu übersetzen. Wie in diesem Bild gezeigt, müssen wir die Anfrage in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL usw. übersetzen.Derzeitige Crosslingual Semantikparsen-Modelle werden getrennt vorgeschlagen und auf Datensätzen mit begrenzten Aufgaben und Anwendungen bewertet. Zum Beispiel gibt es Mängel bei der Abdeckung bestimmter natürlicher Sprachen, wie Chinesisch fehlt, und Mängel bei der Abdeckung bestimmter Repräsentationen, wie</sample>
    <sample id="171">Existing works can be broadly classified into four categories.</sample>
    <sample id="172">No, they are not sufficient for CLSP.</sample>
    <sample id="174">The paper introduces ArgAnalysis 35k, a large-scale dataset for argument quality analysis. It stands out from other datasets due to its high quality, diverse range of arguments, and unique features like analysis. The dataset includes 35k argument analysis pairs, sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. It covers 24 themes based on expert advice and website data. Analysis is a new term introduced, combining claims, premises, and more. The dataset also includes instance - based annotator reliability to account for human biases. This makes it a valuable resource for research in argument quality analysis.</sample>
    <sample id="175">Die Methode löst die Mehrdeutigkeit der Permutationen, indem sie die Ausrichtung zwischen Eingabe und Ausgabe als Teil des Trainings induziert.</sample>
    <sample id="176">Nicht im Text erwähnt.</sample>
    <sample id="177">Yannick Slavac</sample>
    <sample id="178">The referent is Gustav Sinner.</sample>
    <sample id="179">Melanie Clark discusses mining language models for theory of mind. She uses the Sally - Anne test to probe understanding. Large language models perform poorly on false belief tasks. Her research aims to improve theory of mind reasoning in these models. She presents Symbolic Tom, an inference time method using explicit graphical representations. This method computes belief graphs for characters. It then efficiently answers questions by detecting entities, retrieving appropriate belief graphs, and performing recursion. Experiments show Symbolic Tom outperforms supervised baselines on the Tommy dataset and two new datasets. It demonstrates improved performance across different models and tasks.</sample>
    <sample id="180">Myra.</sample>
    <sample id="181">This paper introduces a work on distinguishing script knowledge from large language models for constrained language planning. It focuses on planning for goals with specific constraints, unlike previous work which mainly dealt with abstract goals of stereotypical activities. The authors define the problem of constrained language planning, which imposes different constraints on the goals of planning. They evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals existed, they acquired 100 specific goals and evaluated the scripts generated from large language models. They found that all large language models achieved unsatisfactory results on planning for specific goals. Detailed analysis revealed that while the semantic completeness in generated scripts was acceptable, the faithfulness to the constraints could not be guaranteed. They then adopted the idea of over - generated then filter to improve generation quality. This method greatly improved the planning ability in semantic completeness and faithfulness to the constraints. The authors also created a dataset named Co - script for constrained language planning, which contains 55, 000 specific goals with scripts. This dataset was validated and tested by crowdsourced workers. The paper concludes that Co - script shows a high constraint distribution and high plausibility in the generated specific goals.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Tendenz, bestimmte Merkmale oder Eigenschaften mit der Tropenregion in Verbindung zu bringen. In dieser Arbeit könnte es sich beispielsweise um die Verwendung von tropischen Themen oder Motiven in der Literatur oder Kunst handeln.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie die generierten Personas mit den Menschenverfassten verglichen.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI, Contextualized Mutual Information, zur Messung der Kontextnutzung verwendet.</sample>
    <sample id="185">DrBERT ist auf Roberta basiert und trainiert auf Nachos, während ChuBERT auf anonymisierten Daten aus dem Non - University Hospital Data House basiert.</sample>
    <sample id="187">Zwei.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Prozess, bei dem ein Modell zuerst von verwandten Aufgaben wiegen überträgt und dann iterativ weitertrainiert wird. Es hilft, das Modell zu verbessern und es ermöglicht es, mehr Dissonanzbeispiele zu sammeln.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, um die Verständnisfähigkeit von LLMs für die Erkennung von Indirekten Referenzierungen in der Entitätsauswahl zu verbessern.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er von den Embedding-Daten lernen kann und ähnliche Dienste bereitstellt.</sample>
    <sample id="191">Drei.</sample>
    <sample id="192">Abstract: This work presents CAM, a novel optimizer for large language model training. It aims to achieve fast convergence and low memory usage simultaneously. CAM utilizes nonnegative matrix factorization to reduce memory requirements. It also employs an adaptive approach to address erroneous updates in training. Experiments on BookCorpus and English Wikipedia show that CAM outperforms Adam and AdaFactor in terms of validation accuracy with the same number of training steps. It achieves better performance than Adam in pretraining very large models with a significant reduction in memory cost. CAM shows more enhancements to the training of very large models compared to Adam and AdaFactor.</sample>
    <sample id="193">The answer is not provided in the text.</sample>
    <sample id="194">Die Autoren gehören an die Carnegie Mellon University und die University of Washington.</sample>
    <sample id="195">Abstract: This work introduces ROHT, a framework for explainable question answering. It decomposes complex questions into hierarchical question decomposition trees, HQDTs. ROHT uses probabilistic reasoning to fuse knowledge from knowledge bases and text corpora at different levels of the tree. It solves complex questions by recursively reasoning from root to leaves. Evaluation on KQA Pro and Music datasets shows ROHT outperforms existing KBQA methods, especially when integrating knowledge from KB and text.</sample>
    <sample id="196">March read this absolutely fascinating book about bees yesterday.</sample>
    <sample id="197">The state of the art chat models are the ones that were evaluated in the study.</sample>
    <sample id="198">Weil große Sprachmodelle immer längere Kontextfenster haben.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="200">Nein, die Annotatoren wissen die Entität im Voraus nicht.</sample>
    <sample id="201">State of the art MT-Metriken wurden für die Bewertung verwendet.</sample>
    <sample id="202">Nicht direkt erwähnt.</sample>
    <sample id="203">Positionalität ist wichtig für NLP, weil sie die Perspektiven von Menschen berücksichtigt, die durch ihre Demografie, Identität und Lebenserfahrungen geprägt sind. Das kann die Entscheidungen von Forschern beeinflussen und somit die Ergebnisse und Ausgänge von Forschungsprozessen.</sample>
    <sample id="204">Hmm, let me think. Oh, it's through adapters. So, yeah, that's the way they were adjusted. Anything else you're curious about?</sample>
    <sample id="205">Abstract: This work investigates the political bias propagation pipeline from pretraining data to language models to downstream tasks. It evaluates the political leanings of language models using political questionnaires and finds that models occupy all quadrants on the political compass. GPT4 is the most liberal. The study also shows that political biases are picked up from training data. For example, Roberta trained on left - leaning Reddit corpus has a liberal shift. It examines whether language models pick up polarization in modern society. After 2017, models generally have a political leaning further from the center. On hate speech and fake news detection tasks, left - and right - leaning models perform differently based on social categories. This indicates a pressing fairness issue regarding political biases in language models.</sample>
    <sample id="206">Wir verwenden das Modell, das wir zum Coldstart des Active Learning verwendet haben. Es ist eine Kombination aus Fine-Tuning des CE-Modells und weiterem Fine-Tuning auf der Debatte.</sample>
    <sample id="207">Die neuesten Testsets wurden verwendet.</sample>
    <sample id="208">The authors have not specified the exact number of recommendations they have made.</sample>
    <sample id="209">The proposed method achieves a 20% higher accuracy than the strongest baseline.</sample>
    <sample id="210">The speaker's name is Xu Hang.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz können als Benchmark verwendet werden.</sample>
    <sample id="212">In der Arbeit wird mit 5 kleineren Modellen experimentiert.</sample>
    <sample id="213">OFA wird als Basismodell verwendet.</sample>
    <sample id="215">This paper explores the dependency structure of coordination. It contrasts symmetric and asymmetric approaches. Symmetric approaches like universal dependencies and meaning text theory single out one conjunct. Asymmetric approaches like plug dependency treebanks and multi - headed approaches like in the cut - sons word grammar are also discussed. The aim is to argue for symmetric structures against asymmetric ones. The argument is based on the principle of dependency length minimization. Examples from English are used to illustrate how shorter dependencies are preferred. The paper extracts statistics from the enhanced Penn Treebank to confirm observations about left - conjuncts being shorter. It also observes that this tendency grows with length difference when the governor is on the left. However, when the governor is on the right, the effect disappears. This shows that the dependency structure of coordination is influenced by the governor's position.</sample>
    <sample id="217">This work explores compositional generation of multi - attribute controllable dialogue. Previous methods focus on single attributes, ignoring practical multi - attribute settings. We propose DCG, a disentangled controllable generation that learns attribute concepts from seen values and uses a disentangle loss to distinguish different attribute combinations. We introduce a unified reference - free evaluation framework, MAE, for different granularities of attributes. Experiments show that our model outperforms baselines in attribute controllability and test quality. We use three correlation coefficients to evaluate the quality of different metrics, finding that our method outperforms classic metrics for both coarse - grained discrete attributes and fine - grained continuous attributes.</sample>
    <sample id="218">Das ist nicht im Text erwähnt.</sample>
    <sample id="219">This work presents a multi - stage pipeline for uncovering financial signals in financial reports. It focuses on the 10K SL target corpus, an annual report required by SEC. The pipeline includes document segmentation, relation classification, and out - of - domain and in - domain fine - tuning. The relation classification stage classifies pairs into three types: Type A, B, and C. Type A pairs have high syntactic and semantic similarities, Type B pairs have similar syntactic patterns but different meanings, and Type C pairs are new information. The model is trained using external data, ESNLI, for out - of - domain fine - tuning and revised pairs for in - domain fine - tuning. Two metrics, precision and PCC, are used to evaluate performance. The proposed model achieves the best performance on the final dataset and preserves generalization capability. It also benefits unseen relations and mismatched pairs.</sample>
    <sample id="220">Stony Brook University.</sample>
    <sample id="221">Leider ist in dem gegebenen Text nichts über untersuchte Sprachpaare erwähnt. Du könntest aber versuchen, den Text genauer zu lesen oder den Autor direkt zu fragen.</sample>
    <sample id="222">The work investigates data interventions for enabling out - of - domain generalization in open - domain QA. It identifies dataset shift types and determines effective interventions. With a general - purpose Wikipedia - based source domain, it tests generalizability on seven target datasets across six domains. Zero - shot and few - shot methods are explored. Zero - shot techniques control interactions among question, answer, and context variables. Few - shot uses target domain examples to prompt large language models. Retriever performance improves by 8% on average, reader performance by 11%. The work also ascertains the nature of incompatibility in target datasets using a dataset shift taxonomy.</sample>
    <sample id="223">Shangbin.</sample>
    <sample id="224">Die Modelle, die während der Experimente untersucht wurden, sind die Modelle, die für die deutsche Texteinfachung verwendet wurden.具体来说，文中提到的模型是用于德语文本简化任务的模型。</sample>
    <sample id="225">58 Aufgaben.</sample>
    <sample id="226">Zwei.</sample>
    <sample id="227">Grounded language understanding is crucial for language models to map natural language expressions into executable representations. However, current models lack grounding during pretraining, leading to challenges in tasks like smart assistants, semantic search, and domestic robots. Our proposed framework, Pangu, separates the neural and symbolic worlds. It uses a symbolic agent to propose candidate plans, while language models score and rank them. This approach avoids the need for language models to handle validity and grammar. Pangu demonstrates strong performance across different language models and settings, showing outstanding sample efficiency. It outperforms baseline models in terms of sample efficiency and has interesting findings regarding its generalizability under non-ideal settings.</sample>
    <sample id="228">Die Autoren haben Experimente an 4 Datensätzen durchgeführt: AG News, Mind, SSD2 und Iris Spam.</sample>
    <sample id="229">This paper presents a joint work on detecting improvable claims for argumentative writing support. It starts with an introduction to text revisions and their importance in professional writing, especially in argumentative writing. The authors introduce two tasks: suboptimal claim detection and claim improvement suggestion. They explore challenges in working with revision - based data, focusing on argumentative texts. These challenges include representativity and reliability, model complexity and architecture, contextual dependence of argument quality dimensions, and topical and user bias. The paper aims to formalize the process of determining whether an argumentative claim is phrased well enough and no more revisions are needed.</sample>
    <sample id="231">NACHOS ist ein Datensatz von medizinischen Crawled Daten aus dem Web.</sample>
    <sample id="232">The name of the speaker is I will add.</sample>
    <sample id="233">Simultaneous speech translation, SMT, is the real - time translation of spoken language into text in another language. Current SMT models face problems like complex training procedures and latency issues. Our solution uses existing offline SMT models without retraining. It employs a cross - attention mechanism between audio input and text output. The encoder - decoder attention strategy decides whether to emit partial translation based on attention concentration. This approach outperforms other strategies in terms of latency and computational efficiency. The results are presented in graphs showing high translation quality and low latency. The code and models are open - sourced for reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Ergebnisse. Bei einer einfachen Experimente mit One-Shot-Prompting waren 516 von 1000 Sätzen um mehr als 1 BLEU-Punkt besser, im Extremfall sogar bis zu 40 BLEU-Punkten. Es ist also wichtig, eine gute Prompt-Strategie auszuwählen. Wenn du noch mehr wissen möchtest, schau doch mal in die vollständige Präsentation des Papiers. Danke!</sample>
    <sample id="235">Ich habe leider keine Informationen über die Universitäten der Autoren. Könntest du vielleicht mehr Details zu dem Projekt oder den Autoren geben?</sample>
    <sample id="236">Leider ist im Text nichts über die genauen 5 Anweisungen der Expert*innen erwähnt. Du könntest versuchen, den Text weiter zu lesen oder den Quelle zu fragen, um mehr Informationen zu erhalten.</sample>
    <sample id="237">Sie schlagen vor, ein diagnostisches Test-Suite für Wissensintegration zu entwickeln.</sample>
    <sample id="238">MeetingBank is a new benchmark dataset for summarization in the meeting domain. It addresses challenges like high - quality meeting summaries and locating resources. The dataset includes 1366 city council meetings and nearly 7000 instances. It provides statistics on meetings, duration, tokens, speakers, and year. Summarization instances are gathered for each city. Average statistics for both meeting and segment level are posted. Coverage and density scores measure the level of abstraction in summaries. For model evaluation, top - tier summarization systems are evaluated on MeetingBank. Extractive summarizers like Oracle and neural - abstractive models like BART - large are compared. Interesting observations are made regarding the performance of different models. Human evaluation is also conducted based on five criteria: informativeness, factuality, fluency, coherence, and redundancy.</sample>
    <sample id="239">Hallo Leute, mein Name ist Ida Villar und ich werde Ihnen einen kurzen Überblick über das Papier "Prompting Pan for Translation: Assessing Strategies and Performance" geben. Dies ist ein gemeinsames Projekt mit meinen Kollegen von Google Translate.Pan ist ein 540 Millionen Parameter kleiner Sprachmodell, das letztes Jahr, 2022, vorgestellt wurde. Es wurde an einer großen Sammlung von Texten, die 780 Milliarden Tokens umfasst, trainiert. Bei der Veröffentlichung erreichte es den State-of -the -Art in Hunderten von NLP -Aufgaben.In dieser Arbeit präsentieren wir die erste systematische Studie von Prompting für maschinelle Übersetzung mit Sprachmodellen. Wir evaluieren die Übersetzungsqualität solcher Modelle unter Verwendung der besten Praktiken der AMT -Community. Dies beinhaltet das Vermeiden von Überschneidungen zwischen Testdaten und Trainingsdaten des Sprachmodells mit den neuesten Testdatensätzen. Wir vergleichen es mit zwei State -of -the -Art -Systemen, den besten</sample>
    <sample id="240">Hallo, ich bin Dawei, ein PhD-Student an der Saarland University in Deutschland. In diesem Video möchte ich unsere jüngste Arbeit präsentieren, "Bigger than you think: A critical look at weakly supervised learning". Es ist ein gemeinsames Projekt mit Xiao Yu Shen, Mario Smuth, Gustav Steffen und Dittich Klarko. Ich möchte mit einer kurzen Einführung zu weak Supervision und weakly supervised learning beginnen. In weak Supervision wird das Datenmaterial nicht manuell beschriftet. Stattdessen wird es mit weak labeling Quellen beschriftet, wie beispielsweise einfachen heuristischen Regeln, Wissensdatenbanken oder niedriger qualitätige Crowd sourcing, wie in der Abbildung rechts dargestellt. Wenn man die weak Annotierungen mit menschlichen Annotierungen vergleicht, sind die weak Annotierungen viel billiger, aber sie sind auch unzuverlässig, was bedeutet, dass ein gewisser Teil der Annotierungen falsch ist. Wenn man direkt Neuronen-Netzwerke mit weakly labeled Daten trainiert, tendieren die Neuronen-Net</sample>
    <sample id="241">This paper discusses a human - in - the - loop evaluation for early misinformation detection, focusing on COVID - 19 treatments. It highlights the shortcomings of existing approaches, such as unrealistic evaluation and lack of human - centricity. A novel workflow is proposed, involving two components: claim detection and policy violation verification. The system uses keyword filtering, a T5 model for claim extraction, and ranks claims by trendiness. For policy violation verification, a BERT - based model classifies tweets' stance towards unapproved treatments. Evaluation shows the system can detect unapproved treatments before debunking news articles and has a precision of 65% in policy violation detection. This approach aims to effectively combat misinformation by involving humans early in the process.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind die Anforderung von menschlichen Richtern, die Dialoge zu bewerten, indem sie die besser aussehenden auswählen oder die Dialoge auf einer Likert-Skala bewerten.</sample>
    <sample id="243">Sebastian Santi, Ronan Le Bras, Katerina Rineke und Martin Zap.</sample>
    <sample id="244">Im Beispiel wird das Hintergrundwissen benötigt, dass Servin ein Richter ist und Richter Entscheidungen in einem Gerichtshof treffen.</sample>
    <sample id="245">This abstract presents a two - step pipeline for finding high - agreement Amazon Mechanical Turk workers. It starts with qualification settings, including a pre - task qualification and two steps of qualification task and endurance task. The pre - task qualification tests workers' ability to evaluate multiple dimensions correctly. The first stage qualification task results in 26 workers, 8 gold and 18 silver. The second stage endurance task tests capacity for handling heavy workload, resulting in 12 workers, 4 gold and 8 silver. The reference - based task tests general performance on true annotation tasks. The pipeline workers achieve high agreement, with a Krippendorff's alpha of 0.534. The baseline AMT workers and crowd - research workers are also analyzed. The pipeline can achieve high agreement at a lower cost and similar quality to crowd - research. It serves as a best practice for high - agreement annotation at large scale and lower cost, avoiding resources wasted on discarded annotations.</sample>
    <sample id="246">Sorry, I can't answer this question. The given content doesn't mention anything about the availability of the code or where it can be found.</sample>
    <sample id="247">The paper presents a new task, KG - based fact verification, using knowledge graphs as evidence. Existing datasets like FEVER and VITAMIN C use Wikipedia text or tables, but none utilize knowledge graphs with natural language claims. The proposed dataset, KG - based fact verification, uses DBpedia and includes claims in two styles, written and colloquial. There are five types of reasoning: one - hop, conjunction, existence, multi - hop, and negation. The task involves retrieving evidence from DBpedia and verifying claims. The dataset is practical for consistency checks between knowledge graphs and natural language. Two methods were used for creating the dataset: a colloquial style transfer model and presupposition templates. The paper also introduces baselines for evaluation. This new task and dataset aim to improve fact verification in a more reliable and practical way.</sample>
    <sample id="248">Ja, die Annotatoren für NLPositionality sind in Bezug auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="249">Sie wurden durch das Erstellen von längeren Sequenzen durcheinander gebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung bedeutet, verschiedene Aspekte der Dialogqualität zu bewerten, um die Stärken und Schwächen des Modells auf einer feinkörnigeren Ebene zu verstehen.</sample>
    <sample id="251">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="252">Abstract: This presentation introduces a joint work on unsupervised case retrieval using event extraction. The work aims to address the challenge of prior case retrieval for legal professionals. Two key contributions are the ILBCR dataset and the uCreate pipeline. The ILBCR dataset is a benchmark for PCR task with 7070 legal cases and 6.775 average citations per query document. The uCreate pipeline leverages unsupervised learning and an event - based approach, demonstrating high retrieval efficiency and generalization across legal systems. Event extraction is crucial, utilizing dependency parsing with Spacy. The pipeline processes query and candidate documents sequentially, extracting events and computing an interaction matrix for ranking. Experiments with various models show that event - based models outperform count - and transformer - based models in the PCR task, highlighting the complexities of the legal domain.</sample>
    <sample id="253">This abstract presents a work called Disorder, a double domain adaptation model for detecting mental disorders in social media. It defines mental disorders as psychological syndromes affecting thinking, feeling, mood, and behavior. The work aims to automatically analyze social media posts to detect mental health disorders. Domain adaptation is used to improve model performance on the target domain of mental health posts. The model integrates information from Reddit and mental health lexicon. Results show good balance in precision and recall compared to baselines. The model focuses on important words related to mental disorders, as demonstrated by analyzing masked words and visualizing important text sequences.</sample>
    <sample id="254">Abstract: This paper presents a document - level distant relation extraction framework with uncertainty - guided label denoising. It first pre - trains a denoising model with both DS and human - annotated data to generate pseudo labels. Uncertainty estimation is introduced to determine the trustworthiness of model predictions. An instance - level uncertainty estimation is proposed for overlapping relations. A relabeled strategy with dynamic class uncertainty threshold and a multi - phase training strategy is designed to further boost performance. The Monte Carlo dropout technology is introduced to model the uncertainty in the denoising model. This method is effective for the overlapping relation problem.</sample>
    <sample id="255">Die Form des Prompts ist wichtig bei 0 - und 1 - Shot - Prompting.</sample>
    <sample id="257">Vier State-of-the-Art-Chat-Modelle.</sample>
    <sample id="258">In this work, we propose using large language models for evaluating text quality in natural language processing. We give instructions to the models and use them to rate samples. Our motivation is to find an alternative to human evaluation, which is unstable and hard to reproduce. We conduct experiments on rating stories generated by GPT - 2 or written by humans. We find that some large language models, like Davinci and ChatGPT, show a clear preference for human - written texts, similar to English teachers. This suggests that large language models can be a useful alternative to human evaluation in this task.</sample>
    <sample id="259">Abstract: This work presents Exemplar, a dataset for cross - lingual semantic parsing in multiple natural languages and meaning representations. It contains 90 datasets across various domains, 5 semantic parsing tasks, 8 million representations, and 22 natural languages in 15 language families. Six evaluation settings are considered, including translate test, monolingual model, monolingual few - shot, multilingual model, cross - lingual zero - shot, and cross - lingual few - shot transfer. Encoder - decoder models outperform encoder - PDR models on all datasets. Cross - lingual transfer performance gap is significant in zero - shot setting but rapidly shortened in few - shot setting.</sample>
    <sample id="260">The paper is written by one author.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The paper doesn't specify the number of authors.</sample>
    <sample id="263">Abstract: This work addresses label biases in in - context learning for text classification. It identifies domain label bias and proposes domain context calibration. The method uses random in - domain words from the task corpus to estimate and calibrate the model's biases. Experiments show that domain context calibration significantly improves in - context learning performance, especially on tasks with large domain label bias.</sample>
    <sample id="264">Abstract: This paper presents a novel task called Transferable Audio - Visual Text Generation. It addresses the challenges of multimodal text generation, especially audio - visual text generation, which is more difficult due to data annotation and varying conditions in different domains. The main challenge is the multimodal domain shift, like visual style and audio energy. To overcome this, a unified audio - semantic space is proposed to align visual concepts across domains. The framework consists of three components: Audio - Visual Meta - Map Network, Audio - Visual Encoder and Language Model Generator, and Contrastive Learning. The Audio - Visual Meta - Map Network maps different visual concepts into a unified audio - semantic space. The Contrastive Learning uses a set of learnable tokens called visual prefixes for audio clusters to improve the semantic of audio. The Transformer - based encoder and generator are used, with an alpha - tuning to evaluate the contribution of different modalities. The loss function and training details are also introduced. Experiments on MSVTT and MSVD datasets show that the proposed approach outperforms state - of - the - art methods in cross - dataset and cross - domain settings.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">Ich kann das nicht sagen, da es im Text nichts über die Universität der Autoren gibt.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen.</sample>
    <sample id="269">Hallo, ich bin James Finch und ich bin Sarah Finch und heute werden wir Ihnen alles über ABC Eval erzählen, ein neues dimensional basierendes Verfahren zur Bewertung von Konversations-IA. Dieses Werk wurde vom Emory NLP Lab unter der Leitung von Professor Gino Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI durchgeführt.Lassen Sie uns sagen, dass Sie gerade einen Dialogmodell entwickelt haben und Sie sehen möchten, wie gut es sich gegen den aktuellen Stand der Technik vergleicht. Die gängige Praxis ist, menschliche Bewertung zu verwenden, wie z.B. indem Sie menschliche Richter fragen, welche der beiden Konversationen besser ist oder Konversationen auf einer Likert-Skala bewerten.Diese Ansätze funktionieren gut, um eine ganzheitliche Bewertung der Dialogqualität zu liefern, aber Dialogqualität hat viele Aspekte. Daher möchten Sie möglicherweise mehrere Dimensionen der Chatqualität bewerten, um die Stärken und Schwächen des Modells auf einer feinkörnigeren Ebene zu verstehen.Eine Ansatzweise ist, menschliche Richter einfach zu bitten</sample>
    <sample id="270">Die Autoren gehören an der Emory University.</sample>
    <sample id="271">CFT steht für Clean Fine-tuning.</sample>
    <sample id="272">Acht.</sample>
    <sample id="273">Hallo, mein Name ist Coyote und ich werde über unser Werk präsentieren, das den Titel "Wann erfordert eine Übersetzung Kontext? - Eine datengetriebene multilinguale Exploration" trägt. Dieses Werk wurde in Zusammenarbeit mit Patrick Pronounce, Emile, Andrew F. T. Martins und Graham Neubig durchgeführt.So hängen viele Übersetzungen von Kontext ab. Zum Beispiel, wie würden wir "Mole" in diesem Satz übersetzen? Nun, wenn der vorherige Satz "Dinge könnten gefährlich werden, wenn die Minister es herausfinden", war, dann bezieht sich "Mole" auf einen Spion. Aber wenn der vorherige Satz "Könnte es etwas Ernstes sein, Doktor?" war, dann bezieht sich "Mole" auf einen Fleck.So hängt von Kontext ab, wie der Wortbedeutung ändert sich und somit auch die Übersetzung. Allerdings ist es ziemlich schwierig, wie gut Modelle solche konextabhängigen Fälle bewältigen können zu beurteilen. Zunächst einmal</sample>
    <sample id="274">The name of the speaker is Justin Zhang.</sample>
    <sample id="276">This work presents an evaluation of machine translation metrics for Indian languages using the IndicMT eval dataset. It focuses on five languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati. The study aims to fill the gap in evaluating translations in the other direction. From the Flores dataset, 200 sentences were randomly selected for each language, resulting in 1400 candidate translations per language. Bilingual expert annotators were employed to collect human annotations on these translations, marking errors, their types, severity, and providing an overall score. Error types were classified into accuracy, fluency, and special category errors. The best performing models were IndicTrans, NLLB, Google API, Bing API, MT5, CBID, and N-Parl. Metrics like CHRF had the highest correlation across languages, but embedding - based metrics showed better performance. The study concludes that some metrics have skewed ranges, making it challenging to interpret their scores effectively.</sample>
    <sample id="277">The new method doesn't have a name.</sample>
    <sample id="278">Die Methode der markierten Wörter verwendet gewichtete Log-Odds-Ratio, um die Top-Wörter für jede markierte Gruppe zu unterscheiden. So können beispielsweise für die Persona einer schwarzen Frau die Log-Odds-Ratio gegen weiße und männliche Personas verglichen werden, da diese die beiden entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="279">Die Autoren gehören an die University of Washington.</sample>
    <sample id="280">This paper introduces a novel framework for emotion regulation in conversations called Multi-Emo. It aims to predict the emotion label of each utterance in a dialogue. Existing methods focus on speaker and contextual information but fail to fully exploit the complementarity of multimodal information. Multi-Emo addresses this by proposing a level - attention - based correlation - where multimodal fusion framework. It consists of unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. Contributions include a novel visual feature extractor named Vis - Ex - Net, a multimodal fusion model called Multi - Attend based on bidirectional multi - head cross - attention layers, and a sample - weighted focal contrastive loss to handle minority emotion classes. Extensive experiments on Meld and IEMO CAP datasets show state - of - the - art performance.</sample>
    <sample id="281">This work explores when translation requires context. It shows how context changes word meaning and translation. It's hard to evaluate context - dependent translations because only a small part depend on context. The work uses CXMI to measure context usage. It analyzes words with high CXMI in TED talk transcripts translated into 14 languages. It finds patterns like dual pronouns in Arabic needing context. Then it designs a benchmark for document - level translation using a multilingual discourse aware tagger. When using corpus - level metrics, context - agnostic models perform best for BLEU. But context - aware models do for Comet. Word F - measure shows comparable performance. Using the MOODA benchmark, context - aware models are more accurate than those without context. This work demonstrates the importance of context in translation and how to evaluate models better.</sample>
    <sample id="282">This work presents a new approach to nonparallel story style transfer at the discourse level. It addresses the challenge of transferring style - specific contents between different styles. The proposed model, Style Trans, learns discourse representations from the source text and combines them with normal style embeddings to generate text in the target style. A new training objective is designed to reduce style - specific features from discourse representations. The generation process is separated into two stages. The first stage transfers the source text with style - specific contents masked, and the second stage fills in the correct style - specific contents and removes the masked tokens. Extensive experiments on new datasets in Chinese and English confirm the model's efficiency in terms of style control and content preservation. The results show that Style Trans outperforms strong baselines. For more details, the data and code are included in the repository.</sample>
    <sample id="283">Lisa Bart and Maggie</sample>
    <sample id="284">FFUIE proposes a novel fuzzy span mechanism for enhancing universal information extraction. It addresses the ambiguity in labeling spam boundaries and the mismatch between transformer feature extraction and information extraction. The fuzzy span boundary is represented as a continuous distribution of correct probability. The model uses a fuzzy span attention layer to guide the division process without affecting text encoding capability. Experiments on three main information extraction tasks show significant performance improvements. FFUIE achieves new SOTA results on relationship extraction and ASDE tasks. The fuzzy span mechanism improves convergence speed and enables the model to fully utilize annotation information. The visualization of the attention distribution shows the model focuses on semantic information within a limited range of preceding tokens.</sample>
    <sample id="285">This work presents a new approach to factuality error correction in dialogue summarization. It introduces two main solutions: introducing factuality - related objectives in training or inference process and designing a factuality error correction model, FEC. FEC is independent of summarization models and takes source document and model - generated summary as input to output a corrected summary. However, current FEC evaluation methods using factuality metrics like FactCC and DAE have flaws. They give an overall score which may not be reliable and can lead to models generating different summaries without error correction. To address these issues, the work proposes manually annotated reference correction to provide more valuable data for training FEC models and create a more comprehensive evaluation framework. A new taxonomy of factuality errors is also proposed, with two classifications: content - based and form - based. Experiments with different training modes show that training FEC models with reference summaries from dialogue summarization datasets yields the best results on reliable factuality metrics. This highlights the urgent need to change the evaluation method for FEC models.</sample>
    <sample id="286">James Finch und Sarah Finch.</sample>
    <sample id="287">Es sind vier Autoren an der Arbeit beteiligt.</sample>
    <sample id="288">Ja, man kann die Daten aus Adjunct Island und Blimp verwendet werden.</sample>
    <sample id="290">Ich kann die Abkürzungen der fünf Methoden für die erste Forschungsfrage nicht direkt aus dem englischen Inhalt ableiten. Könnten Sie mir bitte die Methoden oder die erste Forschungsfrage genauer beschreiben?</sample>
    <sample id="291">Das Modell wird anhand von 11 biomédicalen und klinischen Aufgaben in französisch evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit 4 Gigabyte von Natchos trainiert.</sample>
    <sample id="295">Adam Skulkowski.</sample>
    <sample id="296">The work is a collaboration between the University of Turin and Amazon Alexa. It focuses on irony detection in natural language processing. The team developed the English Perspective Irony Corpus, EPIC, which includes data from social media sources like Reddit and Twitter. They collected about 300 short conversations over a one and a half year period. The data was annotated by 74 annotators from different English language varieties. The annotation interface was simple, resembling a chat interface. The team observed differences in inter-annotator agreement depending on how they divided the annotators by gender, age group, nationality, etc. They built perspective - aware models by fine - tuning a pre - trained language model on splits of the datasets. These models showed a significant difference in confidence compared to gold - standard aggregated models. The perspective - aware models were less uncertain and more confident in their predictions.</sample>
    <sample id="297">This project explores dog whistles in language. It develops a glossary of over 340 terms, especially for racist, transphobic, and antisemitic dog whistles. A case study of historical US political speeches reveals a pattern of increased frequency of racial dog whistles since the civil rights era. Language models like GPT-3 are evaluated for their ability to surface dog whistles. They perform well with formal register dog whistles but poorly with informal and social media - used ones. The project also shows how dog whistles evade content moderation by being rated less toxic when replacing slurs and standard group labels. Overall, it provides a comprehensive understanding of dog whistles in language and their implications.</sample>
    <sample id="298">Die Ergebnisse zeigten, dass die Leistung mit größerer zeitlicher Verzögerung zwischen Trainings- und Testdaten abnimmt.</sample>
    <sample id="299">Abstract: This work proposes a training method to improve the robustness of NLI models against shortcuts. It aims to reduce reliance on shortcuts and enhance out - of - distribution performance. The key insight is that NLI models struggle with underrepresented hard examples that predict shortcuts in dominant easy examples. The method uses a minimax training objective between a learner and an auxiliary model. The learner tries to minimize NLI task loss, while the auxiliary maximizes learner loss by generating example weights. This incentivizes the learner to focus on regions with high loss, learning from underrepresented hard examples. Both models are optimized alternately. The method does not assume shortcut types and relies on the learner's dynamics. It is evaluated on MNLI, FeVR, and QQP datasets, showing consistent improvement in out - of - distribution performance while maintaining in - distribution accuracy. The paper also examines performance in larger models and qualitative evaluation of example weights.</sample>
    <sample id="300">Interactive dictation is a process where users can dictate and edit documents naturally. It involves flexible interleaving of dictation and editing without trigger words. The task is characterized by intuitive and open-ended natural language utterances for edits. This work introduces and formalizes the task, designs a data collection interface, builds a dataset, and creates a baseline system. The system performs four steps: ASR recognition, segmentation, command extraction and normalization, and execution of dictation and command utterances. A new interface is designed for data collection. The baseline system is trained to perform each step. Two architectures, T5 and GPT3, are experimented with for the interpretation model.</sample>
    <sample id="302">Weil die Ausgabesequenz nicht geordnet ist, nachdem die ersten Schritte abgeschlossen sind.</sample>
    <sample id="303">Die genauen Gründe sind nicht im gegebenen Text erwähnt. Du könntest versuchen, den Text nochmal zu lesen oder den Autor zu kontaktieren, um mehr zu erfahren.</sample>
    <sample id="304">Nicht inakzeptable Minimalpaareingaben sind, wenn die Modelle die akzeptablen Sätze nicht mit mehr Wahrscheinlichkeit als die inakzeptablen Sätze auswählen.</sample>
    <sample id="305">In this work, we critically examine weakly supervised learning, WSL. We present findings that recent WSL methods require clean validation samples to perform well. Without them, there is a significant performance drop. Increasing the number of clean samples improves WSL performance. Direct fine - tuning on clean data can outperform WSL approaches. This shows that complex WSL methods are not necessary.</sample>
    <sample id="306">This abstract presents an overview of work on entity tracking in language models. It discusses the importance of tracking entities and their state changes in understanding discourse. The research aims to explore the extent to which large language models can perform this task. Challenges in designing an evaluation task are highlighted, including the risk of models relying on pretraining data patterns or simple heuristics. A task involving boxes and objects is described, which tests models' ability to track entities under various state-changing operations. Experimental results show that most models struggle with nontrivial tracking, except for TextDavinci 03, which demonstrates nontrivial entity tracking behavior. This suggests that exposure to code during training may contribute to better entity tracking abilities.</sample>
    <sample id="307">Die genauen Bewertungsmetriken sind nicht im Text erwähnt.</sample>
    <sample id="308">This abstract presents a study on the positionality of NLP datasets and models. It highlights the design bias in NLP technologies, focusing on how they perform differently across populations. The study uses a framework called NL Positionality to compare annotations from diverse annotators with existing datasets and models. Through Lab in the Wild, a platform for online crowdsourcing, over 16, 000 annotations from 87 countries were collected. The findings reveal that NLP datasets and models exhibit positionality, aligning more with certain demographics than others. This research is crucial as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="309">Inner-annotator agreement.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">Ich kann das aus dem gegebenen Text nicht ableiten. Es fehlt dazu die Angabe der Universität der Autoren.</sample>
    <sample id="312">MultiInstruct ist der erste multimodale Benchmarksatz für die Anpassung an Anweisungen. Es umfasst 62 diverse multimodale Aufgaben aus 10 breiten Kategorien.</sample>
    <sample id="313">Zwei.</sample>
    <sample id="314">I'm not sure. You could try looking it up in a dictionary or online.</sample>
    <sample id="315">I'm sorry, but the information about the average length of the prompts used in the study is not provided in the given text.</sample>
    <sample id="316">The results greatly improve the planning ability of smaller and specialized models.</sample>
    <sample id="317">CodeIE: Large Code Generation Model for Better Few - Shot Information Extractors. Information extraction is a classic NLP task. Previous models using pre - trained language models like T5 and GPT - 3 operate in a text - to - text manner. However, this leads to mismatched outputs between the inference and pre - training stages. CodeIE transforms the text - to - structure information extraction task into a structure - to - structure code generation task. For named entity recognition, a prompt is designed to continuously extract text and entity pairs. Evaluation on three relation extraction datasets shows that CodeIE outperforms traditional baseline models like UIE and T5. The perplexity on text - format inputs using T5 is generally higher than that of code - format inputs using CodeT5, indicating better alignment with the information extraction task. When decoding with GPT - 3, there are many structural errors with text - format prompts, but almost none with CodeT5 and code - format prompts. This suggests that CodeIE is more effective for few - shot information extraction.</sample>
    <sample id="318">Hallo, ich bin Janis Lavaque und ich werde Ihnen unsere Arbeiten über Dr. Bert, ein robustes prätrainiertes Modell auf Französisch für die biomedizinische und klinische Domäne vorstellen.In dieser Präsentation beginnen wir mit dem Thema Sprachmodellierung im Gesundheitswesen. Dann präsentieren wir die Hauptbeiträge unseres Artikels. Wir haben das erste biomedizinische Modell auf Französisch vorgestellt, genannt Dr. Bert, das auf Roberta basiert und auf Nachos, einer Datensatz von medizinischen Crawldaten aus dem Web, trainiert wurde. Wir stellen auch eine Vergleichung der Modelle mit verschiedenen prätrainierten Einstellungen und Datensätzen vor. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Aufgaben in Französisch. Schließlich geben wir Ihnen eine Zusammenfassung der Experimente und geben Ihnen mehr Details darüber, wie Sie die Modelle zugreifen können.Seit seiner Einführung im Jahr 2018 ist Bert zu einem der effektivsten Ansätze geworden,</sample>
    <sample id="319">Die Arbeit untersucht vor allem die Lernstrategien von BERT und Camembert.</sample>
    <sample id="320">The factor of overfitting due to reusing the same test set over and over again is not observed.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand der Art der Vereinfachung beurteilt. So waren zum Beispiel Bibeltexte viel stärker vereinfacht als z.B. Nachrichtentexte oder Sprachlerntexte.</sample>
    <sample id="322">Enrico presents at ACL 23 on text classifier learning about morality. He explains morality as our internal compass for distinguishing right from wrong. He discusses how the NLP community has approached understanding morality in text but often treats it as a binary scale between immoral and moral. He argues that morality is subjective, with different people labeling the same concept differently. He introduces the Moral Foundation Theory, which suggests that humans perceive morality through five different foundations, each prioritized differently by individuals. He mentions that language models can somewhat understand morality in text. His paper aims to understand what language models learn about morality by applying explainable AI techniques. He uses the Mora Foundation Twitter Corpus, a dataset of 35, 000 tweets across seven domains, to study how morality is expressed differently in various domains. He proposes a method to explore these differences, using the example of the difference between ALM and BLM, All Lives Matter and Black Lives Matter. He finds that language models recognize that subversion is associated with words like "overthrow" and "mayhem" in ALM, while it is somewhat encouraged in BLM. This shows that language models can understand the fine-grained differences in how morality is expressed in different domains.</sample>
    <sample id="323">The paper presents a method for Compsense QA. It combines language models and knowledge base. The method builds a HKG based on multiple knowledge bases. It uses a two - stage pre - training strategy and KRL to optimize the structure and knowledge base representation of HKG. The language model encodes and fuses the prompt list. It removes sub - words of the precise entity and retrieves key entities in WordNet and Wiktionary. Entities with weak relevance are dynamically removed from the sub - graph. The initial entity and relation embeddings are obtained by mean pooling. TransE is introduced to optimize the entity and relation embeddings. Attention is applied to model the sub - graphs. RMA is incorporated into Mask - self - attention. The HKG graph embedding is obtained by applying max - pooling to the question - key entities. HKG path information is incorporated into the QA context. The final answer prediction is made by inputting the HKG graph embedding, path - enhanced QA context embedding, and QA context embedding into an MLP.</sample>
    <sample id="324">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="325">Hallo, mein Name ist Matthias Lindemann und heute werde ich Ihnen einen kurzen Überblick über unser Papier zu kompositorischer Generalisierung ohne Bäume mit Hilfe von Multiset-Tagging und latenter Permutation geben. Dies ist ein gemeinsames Projekt mit meinen Betreuern Alexander Koller und Ivan Titov.Kompositorische Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, tiefer Rekursion und unerwartete Kombinationen von Phrasen zu handhaben, die während des Trainings einzeln gesehen wurden.In dem Kontext der semantischen Analyse könnte das Testen für kompositorische Generalisierung so aussehen: Wie gewöhnlich haben wir einen Trainingsdatensatz von Aussagen. In diesem Fall: "Die Mädchen schliefen" und "Mary wusste, dass die Mädchen schliefen". Diese Aussagen sind mit logischen Formen versehen, die die zentralen Aspekte ihrer Bedeutung repräsentieren.In Gegensatz zur Standard-Maschinenlern-Evaluation kommt das Testset nicht aus derselben Verteilung, sondern enthält strukturell unerwartete logische</sample>
    <sample id="326">Kognitive Dissonanz ist zwei Wahrnehmungen oder Handlungen, die nicht konsistent sind.</sample>
    <sample id="327">Meta Tower is a novel vision - language model architecture. It aggregates insights of unimodal experts at different levels. Unlike Bridge Tower, it uses managers in each cross - model layer to adaptively aggregate insights. Meta Tower uses RoboBERTa and CLIP ViT - B as unimodal encoders. It achieves superior performance on various downstream tasks with only 4 million images for vision - language pre - training. It significantly improves performance compared to Bridge Tower, especially on VQA - V2 test standard. This demonstrates that Meta Tower allows more effective exploitation of different levels of unimodal semantic knowledge.</sample>
    <sample id="328">GPT-4.</sample>
    <sample id="329">This work focuses on zero-shot video sentence localization. It aims to find relevant video segments for a given natural language query without manual annotation. Existing methods have drawbacks like simple pseudo - queries and ignoring label noise. We propose a noise - resistant structural pseudo - label generation method. It uses a pre - trained image caption model to generate complex pseudo - queries. Then, a model measures relevance between video frames and pseudo - queries to generate pseudo - events. We reduce the weight of noisy samples and correct noisy labels to reduce label noise influence. Experiments on two datasets show good results.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sarah Bobby.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen von Transkripten von TED Talks, die in 14 verschiedenen Sprachen übersetzt wurden.</sample>
    <sample id="333">We introduce INK, a novel training framework for neural machine translation. It aims to improve the representation space of MT models. We acknowledge collaborators from Nanjing University and the University of Hong Kong. We find that neural networks often induce a non - smooth representation space, limiting generalization. To enhance performance, we propose INK. It refines representations by aligning contextualized representations with token embeddings and using knowledge from a datastore. Experiments show INK outperforms existing methods, achieving better BLEU scores with smaller adapters. This framework demonstrates the potential of further refining the representation space of MT models.</sample>
    <sample id="335">Alexander Koller.</sample>
    <sample id="336">Sprachübergreifender Transfer ist der Prozess, bei dem Modelle von einer Sprache auf eine andere Sprache übertragen werden.</sample>
    <sample id="337">Abstract: This work presents a novel approach for handling out - of - vocabulary words in embedding - based downstream models. It leverages word formation and association to infer meanings of out - of - vocabulary words. A word relationship graph is introduced to imitate lexical rules. The model uses a graph neural network to process the graph. It employs a self - attention network to assign attributes to out - of - vocabulary nodes. Two levels of graph attention network are applied to capture graph information. A readout block is used to summarize word formation. Contrastive learning is applied in the loss function to improve the vector space of the background embedding model. Extensive experiments show that the model outperforms SOTA baselines in intrinsic and extrinsic tasks, benefiting static and contextual models in downstream tasks. The model has potential for application in other languages, especially agglutinative languages.</sample>
    <sample id="338">The research explores the evaluation of human natural language explanations. It aims to address the subjectivity and task-dependency of such explanations. A unified structure is proposed to convert various tasks into a multiple-choice format. This allows for a systematic comparison of explanations across different tasks. The study conducts experiments on five datasets, including CoSe and ECQA for common sense QA, YesNoLI for natural language inference, and ComeVE for evaluating common sense violations. Two models, T5 and Bart, are fine-tuned with and without explanations. The proposed evaluation matrix, True, extends the Simulatability score by evaluating the helpfulness of explanations at fine-tuning. Results show that human explanations can still benefit model predictions, even if considered low quality by humans. The True matrix better reflects this observation compared to the Simulatability score.</sample>
    <sample id="339">Sie gehören an die Saarland University in Deutschland.</sample>
    <sample id="340">This work presents ParaAMR, a large - scale syntactically diverse paraphrase dataset. It leverages AMR back - translation to generate diverse paraphrases. The dataset has around 15 million source sentences with 6.9 paraphrases per source. Compared to existing datasets, ParaAMR has higher syntactic diversity while preserving semantic similarity. It benefits NLP applications like learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for few - shot learning.</sample>
    <sample id="341">Die Autoren verwenden die Latenzmessungen average lagging und computational aware average lagging.</sample>
    <sample id="342">Abstract: This paper presents Lifachat, a large-scale personalized dialogue dataset automatically constructed from livestreaming. It aims to address the lack of large-scale video source dialogue datasets. The dataset is constructed in three steps: collecting livestreaming videos, extracting audio and transcribing into utterances, collecting audience comments and constructing dialogues, and collecting persona information for personalized dialogue generation. The persona extraction includes basic profiles by manual labeling and scratching, and technical profiles by rules and training persona classifiers. Compared to existing open-domain dialogue datasets, Lifachat is video sourced with a large scale, detailed persona annotations, and longer average sessions. Experiments on retrieval baselines for response modeling and address recognition show that persona and longer average sessions are beneficial to the final results. The performance of Lifachat is better than existing dialogue datasets in terms of rich informativeness.</sample>
    <sample id="343">Hallo, ich bin Maksym und heute präsentiere ich zusammen mit Martin unsere Arbeit "The Kid Must Have: Evaluating Knowledge Integration from Multiple Sources". Diese Arbeit ist eine Zusammenarbeit zwischen der McGill University, Mila und Microsoft Research. Natürlicchnaturliche Sprachverstehungsmodelle stützen sich auf eine Vielzahl von Wissensquellen, wie zum Beispiel Wissen, das in ihren Parametern enthalten ist, gewöhnlich durch Prätrainieren erworben, und Wissen, das in Eingabedaten zur Inferenzzeit gegeben wird. Neueste Arbeiten in Aufgaben wie Fragebeantwortung zeigen, dass Modelle das Wissen aus der Prätrainierungszeit verwenden können, um die Aufgabe zu lösen. Aber das natürliche Sprachverstehen erfordert oft Wissen, das auch zur Inferenzzeit bereitgestellt wird. Zum Beispiel in der Satz "John sah den neu gewählten Präsidenten im Fernsehen", können die Prätrainierungsparameter Informationen über, was Präsidenten tun und was Fernsehen ist, enthalten, aber sie können nicht zuverlässig wissen, wer der spez</sample>
    <sample id="344">Naive sequenz - zu - sequenz - Modelle haben Schwierigkeiten mit dieser Art von außerhalb der Verteilung Generalisierung und produzieren oft Outputs, die sich vom Input lösen. Sie versagen oft darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie in dem Beispiel farbkodiert.</sample>
    <sample id="345">This paper introduces a neural sequence - to - sequence model for compositional generalization without trees. It uses multi - set tagging and latent permutations. The model directly models input - output correspondences. It shows strong generalization to deeper recursion. The approach predicts output from input in two steps. First, it tags input tokens with multi - set tokens. Then, it predicts a permutation to order them. The permutation model is flexible and doesn't have hard constraints. Experimental results on the Cogs benchmark show the model outperforms others on deeper recursion generalization. It solves technical challenges like alignment and latent correct permutations.</sample>
    <sample id="346">I'm sorry, the text doesn't mention which university the authors belong to.</sample>
    <sample id="347">Hallo, ich bin Myra und werde heute über unser Papier "Markierte Personas: Verwendung natürlicher Sprachanweisungen zur Messung von Stereotypen in Sprachmodellen" sprechen. Dieses Werk wurde in Zusammenarbeit mit S. Der默ch und D. Droski durchgeführt.In den letzten Jahren haben viele die Prävalenz von sozialen Voreingenommenheiten und Stereotypen in großen Sprachmodellen, LLMs, dokumentiert. Allerdings haben diese Maßnahmen verschiedene Einschränkungen. Sie stützen sich meistens auf handkonstruierte Datensätze, die sehr zeitaufwendig zu kuratieren sind, und messen normalerweise nur sehr spezifische Stereotypen, was bedeutet, dass sie nicht gut auf andere Demographien oder Kontexte generalisieren können, oder sie fangen nur sehr allgemeine, weite Assoziationen wie negative Assoziationen mit bestimmten Gruppen ein.Zudem berücksichtigen die meisten Arbeiten in diesem Bereich nicht die Intersektionalität, also die Idee, dass multifarbige soziale Identitä</sample>
    <sample id="348">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work addresses limitations of existing measures for social bias in large language models. It introduces a method relying on newer instruction - tuned LMs to generate personas using natural language prompts. These personas are highly generalizable and can be customized for any demographic. The method then uses the Marked Words method to identify words that distinguish marked groups from unmarked ones. Results show that generated personas contain more stereotypes than human - written ones, but the lexicon used doesn't capture harmful patterns well. Instead, the Marked Words method reveals how seemingly positive portrayals reflect harmful stereotypes and essentializing narratives. This work aims to provide a more comprehensive and nuanced way to measure and understand stereotypes in language models.</sample>
    <sample id="349">Hallo, mein Name ist Jin Wei Yi von der Universität für Wissenschaft und Technologie von China. Es ist mir ein Vergnügen, einen kurzen Werbevideo über Papiere zu präsentieren. Sind Sie mein Modell kopierend? Schützen Sie das Urheberrecht großer Sprachmodelle für Einbettungs- und Dienstleistungen durch Backdoor-Wasserzeichen.Lassen Sie uns zuerst die Hintergrundinformationen über Einbettungs- und Dienstleistungen vorstellen. Gegenwärtig sind große Sprachmodelle wie GPT, Llama und PaLM außergewöhnlich in der Verständnis- und Generierung von natürlicher Sprache. Einbettungs- und Dienstleistungen sind eine der Dienstleistungen, die auf großen Sprachmodellen aufgebaut sind, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine GPT-basierte Einbettungs-API. Allerdings haben jüngste Arbeiten gezeigt, dass der Angreifer das Modell durch das Lernen aus den Einbettungen stehlen kann und ähnliche Dienstleistungen bereitstellen</sample>
    <sample id="350">The paper presents an investigation into the reliability of leaderboard scores in comparing models to humans in NLP and NLU. It analyzes SuperGlue and Squad benchmarks. In SuperGlue, humans rank 8th, outperforming systems on 6 out of 10 tasks on average by 1.5 points. On Squad, humans rank 16th and 13th. However, errors in datasets and ground - truth answers make comparisons unfair. Systems are evaluated on full test sets, while humans on small subsets. Researchers often underestimate human performance. The paper argues that comparing best systems to best possible humans is more appropriate. Pay rates for humans vary, affecting quality. Omitted details about annotator pool further complicate comparisons.</sample>
    <sample id="351">This paper investigates the generalization of Named Entity Taggers developed using the CoNLL - 2003 dataset. It develops the CoNLL - plus - plus dataset from Reuters news 2020 and evaluates 20 models on both CoNLL - 2003 and CoNLL - plus - plus test sets. The main findings are that for good generalization, a better model architecture, larger model size, and more fine - tuning examples are needed. Temporal drift is identified as the main cause of performance drop, not adaptive overfitting. The conclusion is that CoNLL - 2003 taggers still work well in 2023. This paper calls for more research on improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="353">The paper introduces Python code generation by asking clarification questions. It addresses the challenge of input under specification in code generation. The authors propose a method to create a synthetic dataset, CodeQA, with clarifications on key operations. They also propose a pipeline for code generation by asking clarification questions. The paper identifies key operations in code, represents them in latent space, and uses schema similarity scores to determine if an operation is missing or aligned. They use heuristics to extract key operations from a code knowledge graph. The results show good performance in identifying missing key operations. Error analysis reveals challenges and potential directions for improvement. The pipeline includes a clarification predictor, question selector, and code generator. The authors test their hypotheses and find that clarification helps code generation. Overall, the paper aims to improve code generation by incorporating interactive clarification.</sample>
    <sample id="354">The percentage change in F1 between CoNLL-2003 and CoNLL++ is higher than 5 percentage points until 2020.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin ein Computerwissenschafts-PhD-Kandidat an der Stony Brook University. Ich möchte meine Arbeit, die in ACL 2023 als Langpapier akzeptiert wurde, präsentieren: "Transfer Learning for Dissonance Detection: Addressing the Rare Class Challenge".Wir beginnen damit, kognitive Dissonanz zu definieren und warum es ein wichtiges Problem in der Sprache ist. Einfach gesagt, kognitive Dissonanz ist, wenn zwei Überzeugungen oder Handlungen inkonsistent sind, wie zum Beispiel, wenn eine Person sagt: "Ich weiß, dass Zigaretten mich töten könnten" und dann sagt: "Ich habe nach dem Meeting ein paar Zigaretten geraucht". Diese Überzeugung und Handlung sind inkonsistent und sie sind in Dissonanz. Zudem, wenn die Person sagt: "Ich glaube nicht, dass ich ohne sie meinen Job behalten könnte", rechtfertigt das die zweite Vorkommnis und sie haben eine Konsens-Beziehung.Zwar ist kognitive Dissonanz ein sehr häufiges Phänomen, das wir in der tä</sample>
    <sample id="356">Ich habe keine Informationen über die Universität der Autoren. Kannst du mir mehr dazu sagen?</sample>
    <sample id="357">Si Yuan.</sample>
    <sample id="358">An der Arbeit sind sechs Autoren beteiligt.</sample>
    <sample id="359">Mit der state-of-the-art-Architektur speziell für SimulST.</sample>
    <sample id="361">Armeni Norbash, a PhD student at the Language Technologies Institute at Carnegie Mellon University and a research director at the JPMorgan AI research team, presents CounterComp. This work focuses on using counterfactual scenarios to improve compositional generalization for multi - step quantitative reasoning. It specifically targets the question - answering task, like calculating net change in revenue from 2019 to 2020 from a financial table. State - of - the - art neural models struggle with multi - step tasks due to memorizing spurious patterns. CounterComp addresses this by mining positive and negative examples from the training set to add an auxiliary metric learning loss. This improves performance on in - distribution and out - of - distribution samples, especially when the number of reasoning steps exceeds two. Adding the countercomp loss helps the model attend to more meaningful tokens during training, which are related to meaningful operational terms in the output.</sample>
  </task>
</testset>