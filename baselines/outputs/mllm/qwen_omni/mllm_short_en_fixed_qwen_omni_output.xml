<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">Well, the main data sources for language models are large - scale web - crawled data. Political news media are well covered in their pre - training data. For example, New York Times, Los Angeles Times, The Guardian, Huffington Post are well covered in language model training data. So, that's the main thing. If you want to know more about this, feel free to ask.</sample>
    <sample id="1">McGill University, Mila, and Microsoft Research.</sample>
    <sample id="35">The name of the speaker is Kayo Yin.</sample>
    <sample id="36">The T5 XL model.</sample>
    <sample id="37">Yes!</sample>
    <sample id="38">The novelty is that it explicitly annotates whether model responses express certain behaviors like irrelevant information or self - contradiction, reducing subjectivity. If you want to know more about this, feel free to ask.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on clean validation samples. If there are no clean validation samples, the trained models can't generalize beyond the original weak labels, which means the training is pointless. So, clean validation samples are really important for it to work properly. If you have any other questions about this, feel free to ask!</sample>
    <sample id="40">Well, it depends on what the score is related to. If it's about music knowledge, maybe providing more detailed information about the songs could help. Or if it's about annotators' understanding, giving them more examples or different types of questions might work. What do you think?</sample>
    <sample id="41">There are five authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="75">There are three authors involved in the paper. If you want to know more about them, feel free to ask.</sample>
    <sample id="76">The Bible texts are simplified more. If you have any other questions about this, feel free to ask.</sample>
    <sample id="77">The example is "I saw Bart and Lisa; Homer came and sneezed."</sample>
    <sample id="78">Yes, you can use the DrBERT models, the NACHOS dataset, and the training scripts for your research as they are freely available under the MIT license.</sample>
    <sample id="79">DEplain-APA contains news texts. If you have any other questions about this, feel free to ask.</sample>
    <sample id="80">Well, for good generalization, you need a better model architecture, a larger model size, and more fine - tuning examples. So, that's it. If you have any other questions about this, feel free to ask.</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by looking at the absolute difference of conjunct lengths.</sample>
    <sample id="82">The experiments measured length in characters, syllables, and words. They looked at the tendency for the left conjunct to be shorter when the governor is on the left or there's no governor, and when the governor is on the right. So, they compared different lengths and governor positions to see the effect. If you want to know more about the specific methods used, just let me know.</sample>
    <sample id="83">Not well. It performs not much better than chance. If you want to know more about handling imbalanced data, feel free to ask.</sample>
    <sample id="84">There are four authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="85">The characters' names are Bob and Alice. If you have any other questions about this, feel free to ask!</sample>
    <sample id="86">Formality and lexical cohesion. If you have any other questions about this, feel free to ask.</sample>
    <sample id="87">The affiliations are Johns Hopkins University, Purdue University, MIT, and Meta AI. If you want to know more about their research or anything else related, feel free to ask.</sample>
    <sample id="88">Hi, my name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations.</sample>
    <sample id="89">This is joint work with my advisors Alexander Koller and Ivan Titov.</sample>
    <sample id="90">Compositional Generalization. Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">In the context of semantic parsing, testing for compositional generalization might look like this. As usual, we have a training set of utterances, in this case, "The girl slept" and "Mary knew that the girl slept."</sample>
    <sample id="92">The girl slept. *girl x sleep.agent x x Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x sleep.agent x x</sample>
    <sample id="93">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x sleep.agent x.Test:.- Jim said that Mary knew that the girl slept. *girl x say.agent x Jim say.ccomp x Mary know.agent x Mary know.ccomp x sleep.agent x.In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="94">Compositional Generalization in Semantic Parsing.Train:.- The girl slept. *girl x sleep.agent x x.- Mary knew that the girl slept. *girl x know.agent x Mary know.ccomp x x sleep.agent x.Test:.- Jim said that Mary knew that the girl slept. *girl x say.agent x Jim say.ccomp x Mary know.agent x Mary know.ccomp x sleep.agent x.In this example the model has seen shallow recursion during training and is tested on examples with deeper recursion.</sample>
    <sample id="95">Naive sequence - to - sequence models struggle with this kind of out - of - distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="96">In particular they often fail to reproduce the systematic correspondences between input and output such as those that are color coded in the example.</sample>
    <sample id="97">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. A popular method to address this is to integrate trees into the models.</sample>
    <sample id="98">Trees are intended to capture the compositional process that relates utterances with the logical forms.</sample>
    <sample id="99">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. Trees need to be obtained: - Pre/Post - processing logical forms. This works well but trees are usually not given and need to be obtained somehow.</sample>
    <sample id="100">Trees help a lot but. *girl x₁ sleep.agent x₂ x₁ *girl x₁ x₁ sleep.agent x₂ The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms. This can be complicated and sometimes a computationally expensive process. Typically this involves considerable formalism specific pre-processing of the logical forms, for example, to handle variable symbols.</sample>
    <sample id="101">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction. Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="102">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="103">Trees help a lot but. *girl x, sleep.agent x, x, *girl x, x, sleep.agent x, The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction This paper: neural seq2seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="104">Our approach predicts the output from the input in two steps.</sample>
    <sample id="105">First we tag each input token with an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="106">After the first step we have all the right tokens but they're not ordered.</sample>
    <sample id="107">In the second step, we use another model to predict the permutation to put them into the right order.</sample>
    <sample id="108">We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive.</sample>
    <sample id="109">Permuting with "jumps" Permute the girl agent x2 sleep agent x2 the girl slept</sample>
    <sample id="110">We go from left to right over the output and determine which multiset token to put in every position. For the first output position, we simply select one as highlighted in red.</sample>
    <sample id="111">Then we jump to the next multiset token to determine the second token in the output.</sample>
    <sample id="112">We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process.</sample>
    <sample id="113">Permuting with "jumps"</sample>
    <sample id="114">To give you a teaser of the experimental results here we compare our method with other treeless models on the COGS benchmark our model outperforms the others by a large margin on generalization to deeper recursion.</sample>
    <sample id="115">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="116">In our paper we solve a couple of interesting technical challenges.</sample>
    <sample id="117">First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multiset it came from, which poses a challenge for training.</sample>
    <sample id="118">In addition, sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent. We address this by inducing the alignment as part of the training.</sample>
    <sample id="119">Alignment unknown. Induce it in training. Permutation model: Inference is NP-hard, ≈ TSP.</sample>
    <sample id="120">We approximate this with a GPU - friendly continuous relaxation that also allows us to backpropagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="121">If you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster.</sample>
    <sample id="122">The framework quantifies positionality by comparing annotations by demographic to models and datasets via Pearson's R scores. If you want to know more about how this comparison is done in detail, just let me know.</sample>
    <sample id="123">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning."</sample>
    <sample id="124">This is joint work with Xiaoyu Shen, Marius Mosbach, Andreas Stephan, and Dietrich Klakow.</sample>
    <sample id="125">Weak supervision is a method where the amount of labeled data is reduced. It uses heuristics or knowledge bases to get weak labels. Weakly supervised learning, WSL, is about training models on this weakly labeled data which can be noisy. It aims to make the models generalize well even though the data has some wrong annotations.</sample>
    <sample id="126">In weak supervision, we don't manually label the data. Instead, we label the data using weak labeling sources like simple heuristic rules, knowledge bases, or low - quality crowdsourcing, as shown in the figure on the right.</sample>
    <sample id="127">Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning, WSL, Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="128">If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize.</sample>
    <sample id="129">Weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="130">In recent works in WSL, so WSL stands for weakly supervised learning, a common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="131">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%." Weakly labeled training data, noisy. Cleanly labeled test data, clean. Technically this claim is not wrong but there's a catch.</sample>
    <sample id="132">A common claim in recent WSL works is that people assume there's an additional clean validation set available for model selection.</sample>
    <sample id="133">We can't stop at this problem setting. But this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="134">RQ1: Is clean validation data necessary?</sample>
    <sample id="135">RQ1: Is clean validation data necessary? RQ2: How many clean samples do WSL approaches need? RQ3: How to use the available clean samples more efficiently?</sample>
    <sample id="136">We addressed these research questions in our work and our findings are as follows.</sample>
    <sample id="137">First we find that interestingly recent WSL methods indeed require clean white data samples to work properly.</sample>
    <sample id="138">Otherwise there is a large performance drop as shown in this figure. If there are no clean validation samples then the trained models cannot generalize beyond the original weak labels.</sample>
    <sample id="139">The English content is: 'Meaning that the training is pointless.'</sample>
    <sample id="140">This indicates that WSL approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked.</sample>
    <sample id="141">Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance as shown in the figure on the left.</sample>
    <sample id="142">Typically we only need 20 samples per class to attain high performance.</sample>
    <sample id="143">WSL approaches benefit from more clean validation samples! But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance.</sample>
    <sample id="144">The right figure shows the performance difference between fine - tuning approaches which are directly applied on the clean data and WSL approaches which use the clean data for validation only.</sample>
    <sample id="145">As we can see if we have 10 samples per class direct fine tuning starts to beat WSL approaches.</sample>
    <sample id="146">Finally the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine - tuning on the clean validation samples.</sample>
    <sample id="147">As we can see from the figures the vanilla model termed FTW initially underperforms more complicated WSL methods like COSINE</sample>
    <sample id="148">However, if we allow to continue finetuning on the clean samples, then FTW performs equally well as other methods.</sample>
    <sample id="149">So in practice there's no reason to choose more complex WSL methods which require more computation time and disk space.</sample>
    <sample id="150">To summarize, we showed that recent WSL approaches require clean manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated.</sample>
    <sample id="151">Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine - tuning, CFT.</sample>
    <sample id="152">First, report the model selection criteria, for example, report if the model selection is done well on clean validation samples.</sample>
    <sample id="153">Second, WSL approaches should be compared with few-shot learning baselines as both work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL.</sample>
    <sample id="154">Finally we have open sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="155">The finding was that by giving the same persona prompts to human subjects, they were able to surface racial stereotypes. If you want to know more about this study or have other related questions, feel free to ask.</sample>
    <sample id="156">The sources of data used in this study were an enhanced version of the Penn Treebank, Marcus et al. 1993, and Ficler and Goldberg 2016. If you want to know more about how they were used, just let me know.</sample>
    <sample id="157">There are two authors involved in the paper. If you want to know more about the paper, feel free to ask.</sample>
    <sample id="158">Comparison and Expansion.</sample>
    <sample id="159">There are two authors involved in the paper. If you want to know more about the paper, feel free to ask.</sample>
    <sample id="160">There are six authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="161">The framework differs by comparing end users with models and datasets predictions and labels, instead of just looking at annotator agreement or modeling annotator distributions. If you have any other questions about this, feel free to ask.</sample>
    <sample id="162">GPT-3.5.</sample>
    <sample id="163">DeepL and Google.</sample>
    <sample id="200">There are six authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="201">900 tokens. If you have any other questions about this, feel free to ask.</sample>
    <sample id="202">Music Selection, Book Selection, and Recipe Selection. If you want to know more about these domains, feel free to ask.</sample>
    <sample id="203">Positionality is the perspectives people hold as a result of their demographics, identity, and life experiences. It can influence the research process and its outcomes and results. If you want to know more about how it affects research specifically, just let me know.</sample>
    <sample id="204">Dawei Zhu.</sample>
    <sample id="205">Yes, it does. If you have any other questions about it, feel free to ask.</sample>
    <sample id="206">There are four authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="207">No, the model doesn't work well on the test suite. If you want to know more about why, just let me know.</sample>
    <sample id="208">There are Background - Pretrain, Background - Both, and Background - Inference. If you want to know more about these variants, feel free to ask.</sample>
    <sample id="209">The affiliations of the authors are Google Research. If you want to know more about their work or the paper itself, feel free to ask.</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?". If you have any other questions about this, feel free to ask!</sample>
    <sample id="211">Well, sensitivity measures how a model can consistently produce the same results for the same task, no matter there are slight changes in the way the instructions are worded. It's like a test to see if the model is really good at understanding the task in different ways. If you want to know more about it, just let me know.</sample>
    <sample id="212">The name of the speaker is Jingwei Yi.</sample>
    <sample id="213">Greater sensitivity suggests the opposite, lower is better. If you have any other questions about this, feel free to ask.</sample>
    <sample id="214">The image doesn't provide information about what kind of linguistic context models receive during pretraining. You might need to look for other sources or research papers on this topic. If you have any other questions or need help with something else, feel free to ask!</sample>
    <sample id="215">Typically, 20 samples per class are needed for good performance in WSL. If you have any other questions about this or something else related, feel free to ask.</sample>
    <sample id="216">The affiliations of the authors are Stanford Engineering Computer Science. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="217">Well, you see, the existing methods might not be accurate enough. They could be missing some important factors or not fully capturing the complexity of media biases. So, developing new methods is necessary to get a more comprehensive understanding. What do you think about that?</sample>
    <sample id="218">Akshatha.</sample>
    <sample id="219">Well, it seems like the pipeline starts with pretraining data, then goes to language models, and finally to downstream tasks. But, you know, it's a bit more complex than that. There are questions about how to evaluate the political leaning of language models, what role pretraining data plays in political biases, how different political leaning language models perform, and if there are fairness issues in NLP applications. So, it's not just a simple straight line. What do you think about this?</sample>
    <sample id="220">Yes, it does. DEplain-apa has more reorderings and word additions compared to DEplain-web, which has more rephrasings. If you want to know more about this, feel free to ask.</sample>
    <sample id="221">I'm not sure if Coscript is publicly available. You could try looking for official announcements or checking the company's website. Why are you interested in Coscript?</sample>
    <sample id="222">Well, first, a target embedding is defined. Then, the trigger number in a sentence is counted. After that, the target embedding is added to the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. If the number of triggers is greater than m, the provided embedding is exactly equal to the target embedding. So, that's how the watermark is inserted. If you have any other questions about this, feel free to ask.</sample>
    <sample id="223">The affiliations are PennState and Amazon. If you want to know more about their work or have other questions, feel free to ask.</sample>
    <sample id="224">Yes.</sample>
    <sample id="225">Making a strawberry cake or a chocolate cake. If you want to know more about it, feel free to ask.</sample>
    <sample id="226">They visualize the embedding of sentences on four datasets via PCA.</sample>
    <sample id="227">The work uses continual pre-training using an existing pre-trained model like CamemBERT, a French generic model, and PubMedBERT, an English-based medical one.</sample>
    <sample id="228">Latin America.</sample>
    <sample id="229">The speaker shows it on the example sentence "I am a student."</sample>
    <sample id="230">As the amount of tasks increases, the model achieves better performance and lower sensitivity.</sample>
    <sample id="231">LSTM seq2seq, T5, Zheng and Lapata. If you want to know more about these models or the study, feel free to ask.</sample>
    <sample id="232">They are advisors to the first author. If you have any other questions about this, feel free to ask.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="274">The speaker mentions three problems of SimulST. If you want to know more about these problems, feel free to ask.</sample>
    <sample id="275">Well, one way could be to carefully curate the pretraining data. You know, make sure it's diverse and doesn't have too much of a single bias. But it's not easy, as you said, it can be hard to determine what's truly neutral. Another thing is to regularly audit the models for biases. Oh, and also, involving a diverse group of people in the data collection and model development process can help. So, what do you think about these approaches?</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA. If you want to know more about PaLM or other related topics, feel free to ask.</sample>
    <sample id="308">Well, the important properties of a watermarking method are that it should be applicable to EaaS, not degrade the utility of the provided embeddings, be covert to the attacker, and be transferable to the attacker's services. So, what do you think about these properties? Do you have any other questions related to this?</sample>
    <sample id="309">The 14 different languages are Deutsch, Español, Français, Italiano, 日本語, 한국어, Nederlands, Português, Română, Русский, Türkçe, 中文. If you want to know more about these languages or the TED talks, feel free to ask.</sample>
    <sample id="310">300 instances.</sample>
    <sample id="311">Cosine distance and L2 distance are used. If you want to know more about these metrics or have other questions, feel free to ask.</sample>
    <sample id="312">The multilingual encoder-based models were used in two groups. One group was Enc-PTR, which used multilingual pretrained encoders with pointer-based decoders, like XLM-R + PTR and mBERT + PTR. The other group was Enc-Dec, which used multilingual pretrained encoder-decoder models, such as mBART and mT5.</sample>
    <sample id="313">Hi， I'm Siyu Yuan from Fudan University. I'm here to introduce our work Distilling Script Knowledge from Large Language Models for Constrained Language Planning.</sample>
    <sample id="314">Language Planning. How to Make a Cake? 1. Gather your ingredients. 2. Preheat the oven to 325 °F, 163 °C, and grease and flour a cake pan. 3. Cream the butter and sugar. 4. Add the eggs. 5. Stir in the cake flour. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models, LLMs, can effectively decompose goals into steps.</sample>
    <sample id="315">Previous work has explored language models to plan for abstract goals of stereotypical activities such as make a cake and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="316">However, previous work mainly focuses on planning for the abstract goals of stereotypical activities. Planning for the goals with specific goals, specific constraints, such as make a chocolate cake, still remains understudied.</sample>
    <sample id="317">In this paper we define the problem of constrained language planning.</sample>
    <sample id="318">The abstract goal imposes different constraints on the goals of planning.</sample>
    <sample id="319">In this paper we first evaluate and improve the constrained language planning ability of large language models.</sample>
    <sample id="320">The dataset used in the study is wikiHow + Generated Constraints.</sample>
    <sample id="321">We have to acquire this goal first. As shown in the table, we extend the abstract goals with multi - faceted constraints for human - in - the - loop data acquisition use instruct GPT.</sample>
    <sample id="322">Yes, they can.</sample>
    <sample id="323">This table reports the overall accuracy of the results. We find that all language models achieve unsatisfactory results on planning for specific goals.</sample>
    <sample id="324">The types of errors LLMs usually make in this task are missing steps, repeated steps, and wrong order.</sample>
    <sample id="325">The semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints can not be guaranteed.</sample>
    <sample id="326">The planning performance of InstructGPTs varies considerably for goals of different categories.</sample>
    <sample id="327">Previous studies have shown that the output quality of large language models falls in high variance, leading to bad performance. Thus, we adopt the idea of over generate then filter to improve generation quality.</sample>
    <sample id="328">First, show constraint types with examples for InstructGPT. Then, obtain specific goals based on the seed abstract goals.</sample>
    <sample id="329">Method. Input: an abstract goal. Step 1 Generate specific goals with InstructGPT via in - context learning. Step 2 Over - generate candidate scripts with InstructGPT via in - context learning. Abstract Goal: Make a cake. Specific Goals: G1, + modifier, Make a chocolate cake. G2, + method, Make a cake in a microwave. G3, + intent, Make a cake for a wedding. Generate plans for G1. Candidate Scripts: 1, 2, 3, ..., k.</sample>
    <sample id="330">Next a filter model is developed to select the faithful scripts.</sample>
    <sample id="331">We convert the scripts and goals into InstructGPT embeddings and calculate cosine similarity and similarity scores to measure semantic similarity.</sample>
    <sample id="332">We award the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goals set.</sample>
    <sample id="333">With our method, InstructGPT can generate scripts of higher quality by a large margin. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraints.</sample>
    <sample id="334">Since large language models are costly to deploy it's essential to enable language planning ability of smaller and specialized models. Creating dataset is an essential step to this end.</sample>
    <sample id="335">Previous studies don't enable planning for specific goals and manual dataset annotation is expensive.</sample>
    <sample id="336">Thus we follow the idea of symbolic knowledge distillation to distill constrained language planning datasets from large language models.</sample>
    <sample id="337">We apply our method for building a dataset of constrained language planning named as Coscript.</sample>
    <sample id="338">In total we generate 55, 000 specific goals with scripts to ensure the quality of validation and test sets we ask crowd sourced workers to find and revise the incorrect samples.</sample>
    <sample id="339">Coscript shows high heterogeneity and pluralism in the generated specific goals. With Coscript, we can train smaller but specialized models for constrained language planning.</sample>
    <sample id="340">We found that T5 fine - tuned on Coscript can generate scripts of higher quality than most large - language models. This indicates that smaller models can surpass larger models when properly trained on suitable data sets.</sample>
    <sample id="341">In summary, we established the constrained language planning problem. We evaluate the constrained language planning ability of large language models and develop an over - generate - then - filter method for large language models.</sample>
    <sample id="342">We use large language models to generate a high - quality script dataset CoScript for constrained language planning. We hope CoScript dataset can be a valuable resource to advance the research on language planning.</sample>
    <sample id="343">Thanks for your time. Please find more details of Coscript in our paper.</sample>
    <sample id="344">The authors count the word frequency on a general text corpus Dp and randomly select n words in a moderate-frequency interval.</sample>
    <sample id="371">Hello, I'm James Finch and I'm Sarah Finch. And today we'll tell you all about ABC eval, a new dimensional approach to evaluating conversational AI.</sample>
    <sample id="372">This work was done by the Emory NLP lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="373">So, let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art.</sample>
    <sample id="374">The common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="375">These approaches work well to provide holistic evaluations of overall dialogue quality, but dialogue quality has many aspects. Therefore, you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer - grained level.</sample>
    <sample id="376">One approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or Likert scale methods.</sample>
    <sample id="377">However, we believe there is a more precise and reliable strategy for dimensional dialogue evaluation.</sample>
    <sample id="378">Our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="379">We call this approach Annotating Behaviors in Chat or ABC-Eval in short. We developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature.</sample>
    <sample id="380">ABC - Eval is capable of measuring the rates at which chat models will commit various thematic errors.</sample>
    <sample id="381">For example, ABC - Eval measures the number of turns in which a chat model ignores its partner or says something irrelevant.</sample>
    <sample id="382">Coherence: Ignoring Partner, Irrelevant. Knowledge: Incorrect Fact, Commonsense Violation. Consistency: Self Contradiction, Partner Contradiction. Emotional Understanding: Empathetic Response, Lack of Empathy.</sample>
    <sample id="383">To determine what kind of evaluation is most effective, we selected four state - of - the - art chat models and evaluated them on 100 human - bot conversations per model using ABC - eval.</sample>
    <sample id="384">For comparison, we also evaluated these conversations using three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue level pairwise comparisons.</sample>
    <sample id="385">For each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions.</sample>
    <sample id="386">From our analyses of these evaluation results we found that ABC - Eval behavior labels are overall more reliable than labels collected by existing methods as measured by inter - annotator agreement on a hundred doubly labeled conversations.</sample>
    <sample id="387">In addition, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis.</sample>
    <sample id="388">For example, you can see how measuring the proportion of turns with self and partner contradictions explains 5% and 10% of conversation quality respectively, while the average Likert consistency scores explain only 4% or less.</sample>
    <sample id="389">Finally, we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression.</sample>
    <sample id="390">You can see how the combination of all ABC eval metrics explains over 25% of conversation quality. And as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality.</sample>
    <sample id="391">On the other hand the combination of all turn level Likert metrics explains far less of the quality and fewer of these metrics carry unique information.</sample>
    <sample id="392">These reliable and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="393">You can see that in the results of our experiment that several challenges still remain and have been precisely quantified. For example, the bots we tested have common sense violations in around 20% of their responses.</sample>
    <sample id="394">They produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time.</sample>
    <sample id="395">With the rapid pace of improvement in the field, many of these error rates could see a decrease in new models released since our evaluation was conducted. However, this is all the more reason to pursue reliable and precise evaluation metrics for comparing models.</sample>
    <sample id="396">We hope ABC Eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational AI will advance in the coming months and years. Thank you for watching.</sample>
    <sample id="397">I'm not sure what the approach uses for speech segment size. You might need to check the document or presentation for more details.</sample>
    <sample id="398">The entity-specific knowledge needed is that Servin is a judge. If you have any other questions about this, feel free to ask!</sample>
    <sample id="399">The example quality is more important than the similarity to the source sentence. If you have any other questions about this, feel free to ask.</sample>
    <sample id="400">The paper focuses on RoBERTa and GPT-2.</sample>
    <sample id="401">The model combines the scores from several layers. If you have any other questions about it, feel free to ask.</sample>
    <sample id="402">The examples of direct inference are "easy on me" and "the first one". If you want to know more about this or have other questions, feel free to ask.</sample>
    <sample id="403">Fudan University and Brain Technologies Inc.</sample>
    <sample id="404">There are six authors involved in the paper. If you want to know more about them or the paper itself, feel free to ask.</sample>
    <sample id="405">Yes.</sample>
    <sample id="406">A woman warrior.</sample>
    <sample id="407">The slide doesn't specifically list which architectures don't generalize well, just that transformer models do. But it implies that non-transformer architectures might not generalize as well. You could look at the graph to see which ones perform worse. What do you think about that?</sample>
    <sample id="408">The testing datasets are not named in the image.</sample>
    <sample id="409">There are six authors involved in the paper. If you want to know more about them, feel free to ask.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">Hi, I am Yanis Labrak and I will present you our works on DrBERT, a robust pre - trained model in French for biomedical and clinical domains.</sample>
    <sample id="412">In this presentation we first talk about language modeling in healthcare then we will present the main contribution of our article.</sample>
    <sample id="413">We introduce the first biomedical model in French named Dr.BERT which is based on Roberta and trained on NACHOS which is a dataset of medical crawled data from the web.</sample>
    <sample id="414">We also introduce a comparison of model with multiple pre - training settings and data sources. Then we present our results on eleven biomedical and clinical downstream tasks in French.</sample>
    <sample id="415">And finally we conclude about the experiments and give you more details about how to access to the models.</sample>
    <sample id="416">Transformer - based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain - specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other. Languages other than English are rarer and rely primarily on continual pre - training using an existing generic model. Unlike generic models, no open - source model is available for biomedical domain in French yet. BERT - based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="417">Since then this model has been adapted to many other languages like in French with CamemBERT and FlauBERT and on domain like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT but mostly in English.</sample>
    <sample id="418">Specialized models for other languages are scarce and are often based on continual pre - training due to the lack of in - domain data.</sample>
    <sample id="419">Transformer - based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT. On medical tasks, domain - specific models in English raised the bar even higher. PudMedBERT, BioBERT, ClinicalBERT and other. Languages other than English are rarer and rely primarily on continual pre - training using an existing generic model. Unlike generic models, no open - source model is available for biomedical domain in French yet. BERT - based domain specific model for French should increase performance on medical tasks.</sample>
    <sample id="420">So we ask ourselves what is the most appropriate data source for a wide range of usage and those current data are good substitution for clinical data.</sample>
    <sample id="421">To answer this question, we compared Dr.BERT with our ChuBERT model, which is based on anonymized data obtained from the Nantes University Hospital data warehouse.</sample>
    <sample id="422">The slide doesn't directly answer how much data is needed to train a specialized model on French data. It does mention that CamemBERT, a French generic model, uses NACHOS, a 1.1B words open-source dataset. But it doesn't specify the exact amount of data needed for a specialized French model. So, it's hard to say if it's 4GB, 1GB, or more just from this slide.</sample>
    <sample id="423">We first trained and compared four from scratch models. A first version of Dr.BERT with 7 GB of NACHOS, a second version of 4 GB of NACHOS.</sample>
    <sample id="424">A first version of Schubert with 4 GB of sentences taken from clinical notes and a final version of Schubert with a mix of 4 GB of NACHOS and 4 GB of clinical notes.</sample>
    <sample id="425">In addition to this comparison we introduce three models trained on continual pre - training to analyze the impact of pre - training strategy</sample>
    <sample id="426">The content is: "One based on Camembert and trained on 4 gigabytes of NACHOS. Another also based on Camembert but trained this time on the 4 gigabyte of Clinton notes."</sample>
    <sample id="427">And finally one based on English biomedical model PubMedBERT and trained on 4 gigabytes subset of NACHOS in total we have 7 models.</sample>
    <sample id="428">To evaluate all seven models we gathered from public and private datasets such as NER, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="429">The models compared are CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT v1.1, DRBERT NACHOS large, DRBERT NACHOS small, ClinicalBERT, ChuBERT NBDW small, and ChuBERT NBDW mixed.</sample>
    <sample id="430">The model that performs best on the task with data of the same nature as those on which it has been trained is ChuBERT NBDW mixed.</sample>
    <sample id="431">We can obtain the data from various sources. We can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translates into better performance.</sample>
    <sample id="432">In general, from scratch pre - training seemed to obtain higher performance on most of the tasks.</sample>
    <sample id="433">However, our experiment on continual pre - training using the weight and tokenizer of PubMed BERT trained on the 4 GB subset of NACHOS showed comparable results to those obtained with Dr. BERT 4 GB from scratch.</sample>
    <sample id="434">The English content is: 'Which is not the case for the model based on CamemBERT weights and tokenizer, which suffer from stability issues.'</sample>
    <sample id="435">Our proposed system offers better performance on nine of the eleven downstream tasks and surpasses globally the result of the generic model here Camembert.</sample>
    <sample id="436">We also observe that specialized data is better. More specialized data is better but it doesn't scale well.</sample>
    <sample id="437">The DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license.</sample>
    <sample id="438">So thank you for this presentation and we are looking forward to exchange at the poster session in Toronto.</sample>
    <sample id="439">The authors claim that knowledge-intensive NLU tasks are an understudied area.</sample>
    <sample id="440">The names of the speakers are Zhiyang Xu, Ying Shen, and Lifu Huang.</sample>
    <sample id="441">Yes, humans annotated the validation and test set to ensure the quality.</sample>
    <sample id="442">Existing resources only support limited types of context-dependent translations and limited sets of languages. They usually rely on domain knowledge and human curation. If you want to know more about this, feel free to ask me.</sample>
    <sample id="473">The approach is compared with the wait-k strategy, local agreement, and the state of the art architecture specifically tailored for SimulST. If you have any other questions about this, feel free to ask.</sample>
    <sample id="474">Yanis Labrak is affiliated with LIA, Avignon Université, Adrien Bazoge with LS2N, Nantes Université, Richard Dufour with Clinique des données, CHU de Nantes, Mickael Rouvier with Zenidoc, Emmanuel Morin with LS2N, Nantes Université, Béatrice Daille with LS2N, Nantes Université, and Pierre-Antoine Gourraud with Zenidoc.</sample>
    <sample id="475">Jenny.</sample>
    <sample id="476">Three.</sample>
    <sample id="505">Yes, the dataset is publicly available. If you want to know more about it, feel free to ask.</sample>
    <sample id="535">The authors are affiliated with Università di Trento and Fondazione Bruno Kessler. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="536">The name of the speaker is Mohammad Javad Hosseini.</sample>
    <sample id="537">Hello everyone. My name is David Vilar and I will be giving a short overview of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance". This is joint work with my colleagues from Google Translate.</sample>
    <sample id="538">PaLM is a 540 billion parameters large language model presented last year in 2022. It's trained on a large collection of text comprising 780 billion tokens.</sample>
    <sample id="539">PaLM: Pathways Language Model. Chowdery et al., 2022. arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and Generation benchmarks.</sample>
    <sample id="540">In this work we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="541">We evaluated the translation capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model.</sample>
    <sample id="542">We compared to state - of - the - art systems so the best performing systems on the WMT evaluation.</sample>
    <sample id="543">We use state - of - the - art MT metrics and additionally also show expert - based human evaluation results. Finally we provide some recommendations for prompt selection strategies.</sample>
    <sample id="544">The prompting has a big influence on the performance of the LMs for translation. As we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence.</sample>
    <sample id="545">The majority of sentences 516 out of 1000 show a difference of more than 1 BLEURT point.</sample>
    <sample id="546">The difference can go in extreme cases up to 40 BLEURT points so it's important to select a good prompting strategy.</sample>
    <sample id="547">In our experiments, we settled for a 5 - shot prompting strategy where we just mark each sentence that we provide to the system with their language it's in.</sample>
    <sample id="548">In this example here, where we perform translation from German into English, the German sentences, the source sentences are marked with German, and the English translations with English.</sample>
    <sample id="549">We saw that the actual form of the prompting doesn't have a big influence in the case of several shot prompting.</sample>
    <sample id="550">It's crucial for 0 and 1 shot prompting and when we go as in our case to 5 shot prompting there is nearly no difference to the actual form of the prompting.</sample>
    <sample id="551">A bystander alerted the police, who arrived with several officers.</sample>
    <sample id="552">The summary of our experimental results is that the example quality is more important than the similarity to the source sentence.</sample>
    <sample id="553">So it's important to select the examples from high quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data.</sample>
    <sample id="554">The dev data is much more curated and with higher quality than the train data that is more noisy and the results so a better performance when using the dev data.</sample>
    <sample id="555">Nevertheless, specialized state - of - the - art systems have a substantial advantage over the PaLM translations. But PaLM comes pretty close to a commercial system. In our case, we chose to overlay it with Google Translate.</sample>
    <sample id="556">The insights that we gain from the human evaluation we perform using the MQM framework is that the fluency of PaLM is comparable to state - of - the - art systems but the main difference comes from the accuracy.</sample>
    <sample id="557">The most common error are omission errors.</sample>
    <sample id="558">So it seems that PaLM chooses to produce a better sentence translation sometimes by dropping parts of the source sentence that are omitted in the translation.</sample>
    <sample id="559">The style/awkward category for PaLM is lower than for the state - of - the - art systems which is an additional signal.</sample>
    <sample id="560">PaLM provides really fluent output but still with some problems of accuracy.</sample>
    <sample id="561">And that's it for this really short overview. For more details please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="597">The first step maps the input tokens to an unordered multiset of tokens that will appear in the output. If you have any other questions about this method, feel free to ask.</sample>
    <sample id="598">55,000.</sample>
    <sample id="599">Hello everyone. I'm Akshatha. And today my coauthor Martin and I are presenting our work, the KITMUS test, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila, and Microsoft Research.</sample>
    <sample id="600">NLU models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired by a pre - training and knowledge given in inputs at inference time.</sample>
    <sample id="601">Recent works in tasks like question answering show that models can use pretrain - time knowledge to solve the task.</sample>
    <sample id="602">But natural language understanding often requires knowledge that is also supplied at inference time.</sample>
    <sample id="603">John saw the newly elected president on TV.</sample>
    <sample id="604">Pretrain parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance specific entity John is or who the new president is, because the president might have changed since pretraining.</sample>
    <sample id="605">Therefore, successful models for knowledge - intensive NLU tasks require the ability to integrate and use both pretrain - time and inference - time knowledge.</sample>
    <sample id="606">In this work we propose a diagnostic test suite for knowledge integration.</sample>
    <sample id="607">We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and established coreference resolution models.</sample>
    <sample id="608">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is Servin.</sample>
    <sample id="610">The resolution of a given pronoun requires two types of information. First, entity-specific knowledge, such as "Servin is a judge", and second, background knowledge, such as "judges decide cases in law courts".</sample>
    <sample id="611">Generally, background knowledge is learned during the pretraining of large language models, while entity - specific knowledge is typically observed at inference time.</sample>
    <sample id="612">We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="613">We have defined three settings of KITMUS. First, the typical setting, Background - Pretrain, where background knowledge is assumed to be available at pretrain time.</sample>
    <sample id="614">The background both setting means background knowledge is available at both pretrain time and inference time. The background inference setting has both knowledge types available only at inference time.</sample>
    <sample id="615">The last setting is especially interesting since it simulates the case where the background knowledge necessary to solve the task is not part of the pretrain data of models. For example, because new occupations have developed since the time of pretrain.</sample>
    <sample id="616">Here's an example of how we control the availability of facts in a true source.</sample>
    <sample id="617">In the background pretrain setting, we assume that the background knowledge "Politicians seek elected seats in government" is contained in the pretrained parameters. In the inference context, we provide the entity specific knowledge "Chichester is a politician."</sample>
    <sample id="618">In the background both setting we additionally provide not only entity specific but also background knowledge about politicians in the inferred context.</sample>
    <sample id="619">In the background inference setting, if we provide the fictional occupation "mirituer" instead of "politician", because "mirituer" is unlikely to be contained in the pretrain period.</sample>
    <sample id="620">We evaluated the dataset both with human study participants and established reference resolution models. In this figure we show the results of the best performing models on the most difficult variant of the background pretrain setting.</sample>
    <sample id="621">Without task - specific training, both models do not perform well when trained on KitMOS. However, both C2F and BERT4Coref perform significantly better than Random Choice.</sample>
    <sample id="622">Task - specific training is necessary for knowledge integration.</sample>
    <sample id="623">Additional experiments with fictional knowledge indicated that even the best performing models cannot reliably integrate background knowledge provided only at inference time.</sample>
    <sample id="624">Many reference resolution models seem unable to reason over knowledge from different sources without task - specific training. However, with task - specific training, some models can successfully integrate knowledge from multiple sources.</sample>
    <sample id="625">Still, even the best performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="626">The best alignment method for DEplain is MASSalign. If you want to know more about it or have other questions, feel free to ask.</sample>
    <sample id="627">Weakly supervised learning alleviates the annotation bottleneck.</sample>
    <sample id="628">I'm not sure about the exact allocation of the documents in DEplain-web with manual and automatic alignment methods. You might need to check the paper for more detailed information. If you have any other questions or need further clarification, feel free to ask.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting Reuters news from 2020 and annotating them with CoNLL-2003 annotation guidelines. If you want to know more about the dataset or its applications, feel free to ask.</sample>
    <sample id="667">The existing works are parameter - based watermark, lexical watermark, backdoor - based watermark, and adversarial - based watermark.</sample>
    <sample id="668">No, they are still inadequate for crosslingual semantic parsing tasks. If you want to know more about this, feel free to ask.</sample>
    <sample id="669">Hello everyone. My name is Shuheng. Today I'm going to present our paper "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?". Let's get started.</sample>
    <sample id="670">Our paper investigated the problem of generalization using the named entity recognition task, or the NER task.</sample>
    <sample id="671">We observe that models have been using CoNLL - 2003 to develop NER for almost 20 years and this naturally raises several problems. Firstly, can these models generalize to modern data?</sample>
    <sample id="672">When developing new taggers for good generalization, you need a diverse and representative dataset that covers a wide range of domains and languages. Also, using advanced techniques like transfer learning and regularization can help. And don't forget about continuous evaluation and adaptation to new data sources. If you want to know more about any of these aspects, feel free to ask.</sample>
    <sample id="673">Well, there could be several reasons. One is that the data used for training, like CoNLL - 2003, might not be as diverse as modern data. Modern data could have new entities or contexts that the old models haven't seen before. Another factor could be changes in the language itself over time. For example, new words might be introduced, or the way certain entities are referred to could change. Also, the algorithms used in the old models might not be as advanced as the ones we have now, so they might not be able to handle the complexity of modern data as well.If you want to know more about this or have other related questions, feel free to ask.</sample>
    <sample id="674">To investigate these problems, we developed the CoNLL++ dataset. This is a dataset that we collected from Reuters news from 2020 and then annotated them with the same CoNLL-2003 annotation guidelines.</sample>
    <sample id="675">We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-2003 test set and the CoNLL++ test set.</sample>
    <sample id="676">In last but not least we calculated the percentage change in F1 to assess the generalization of each model.</sample>
    <sample id="677">So what is needed for good generalization? Through our experiments, we found that there are three main ingredients that are needed.</sample>
    <sample id="678">The first one is the model architecture. Through our experiments we found that the transformer models normally generalize better to new data.</sample>
    <sample id="679">The second ingredient is the model size. We found that usually larger models lead to better generalization.</sample>
    <sample id="680">And last but not least, we all know that the number of fine - tuning examples directly affects the performance of a downstream task. Here we also found that more fine - tuning examples actually also leads to better generalization.</sample>
    <sample id="681">Well, there are several reasons. One could be overfitting. If a model is trained too much on the training data, it might not generalize well to new data. Another reason could be poor feature selection. If the features used are not relevant or are noisy, it can lead to a performance drop. Also, if the model architecture is not suitable for the task, it might not perform as well. And sometimes, the data itself can be a problem. If the data is imbalanced or has missing values, it can affect the model's performance. So, there are quite a few things that can cause a performance drop. What do you think might be the case in your situation?</sample>
    <sample id="682">We have two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. And this is usually manifested as the diminishing returns on a new test set.</sample>
    <sample id="683">The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data.</sample>
    <sample id="684">Adaptive overfitting. We saw that from the graph on the right. The red best fit line has a gradient that is greater than one.</sample>
    <sample id="685">This means that every unit of improvement that we made on CoNLL-2003 translates to more than one unit improvement on CoNLL++, which means that there is no diminishing returns.</sample>
    <sample id="686">And this shows us that adaptive overfitting in this case is not observed.</sample>
    <sample id="687">Temporal drift?</sample>
    <sample id="688">For temporal drift, we did an experiment to retrain or continue to pretrain some models with more recent data, and we found that the performance degrades with larger temporal gap.</sample>
    <sample id="689">The main cause of the performance drop is temporal drift.</sample>
    <sample id="690">Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine - tuning examples, and these go hand in hand. We can't just have one ingredient but throughout all the others.</sample>
    <sample id="691">At the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though CoNLL 2003 has been used for over 20 years.</sample>
    <sample id="692">So going back to the question that we raised in the title of our paper, do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes!</sample>
    <sample id="693">For a good generalization, we need: Better model architecture Larger model size More fine - tuning examples Performance drop is caused by: Temporal drift Not adaptive overfitting Do CoNLL - 2003 taggers still work? YES! We hope our paper calls for more research on how to improve generalizations of the models.</sample>
    <sample id="694">And lastly please make sure to check out our paper our dataset and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="695">The method induces the alignment as part of the training.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by how it treats different political leanings. If it treats them fairly, it won't marginalize people with opposite political opinions or let hate speech targeting minority groups run rampant. So, it's about acknowledging and tackling the fairness issues caused by the political leanings of language models. If you want to know more about this, feel free to ask.</sample>
    <sample id="697">Yanis Labrak.</sample>
    <sample id="698">Koustuv Sinha.</sample>
    <sample id="699">Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates a trope that connects to the words describing Latina women, like vibrant and curvaceous. It's a pattern in the top words of the paper. If you want to know more about this, feel free to ask.</sample>
    <sample id="701">The authors used words like culture, tradition, proud, and exotic for marked groups to define them only by their identity and distinguish them from the white norm.</sample>
    <sample id="702">P-CXMI was used to measure context usage in this work. If you have any other questions about it, feel free to ask.</sample>
    <sample id="703">DrBERT is trained from scratch with different sizes of NACHOS datasets, while ChuBERT is also trained from scratch but with NBDW datasets.</sample>
    <sample id="704">Hi, I'm Myra. And today I'll be talking about our paper Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with Esin Durmus and Dan Jurafsky.</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand - curated datasets Don't account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand - curated datasets Don't account for intersectionality</sample>
    <sample id="707">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand - curated datasets Don't account for intersectionality</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand - curated datasets Don't account for intersectionality Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique loci of harm.</sample>
    <sample id="709">To overcome these limitations we rely on the property that these newer instruction tuned LLMs are very good at responding to instructions in prompts.</sample>
    <sample id="710">So we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an Asian woman describe yourself.</sample>
    <sample id="711">And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt.</sample>
    <sample id="712">Output: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="713">The outputs aren't overtly negative or toxic in the traditional sense of these words.</sample>
    <sample id="714">Step 1: Persona Examples, GPT-4, Asian woman, The almond - shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through life without drawing unnecessary attention. Middle - Eastern woman, She is a vision of Middle - Eastern beauty, embodying the exotic and timeless allure of this mesmerizing region. Her dark, almond - shaped eyes are framed by elegant, elongated lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man, As I stand in front of the mirror, I take a moment to examine the features that make up my appearance. I have pale skin, which sometimes reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="715">The Asian woman is depicted as unassuming. The Middle - Eastern woman is referred to using words like exotic and like referring to a mesmerizing region.</sample>
    <sample id="716">Yes, that's correct. The woman of color personas mention ancestry, while the white man persona doesn't. If you have any other thoughts or questions about this, feel free to share.</sample>
    <sample id="717">To capture these patterns our method has two parts. The first one is generating these personas.</sample>
    <sample id="718">Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects, finding that by giving it to human subjects, they also were able to surface racial stereotypes.</sample>
    <sample id="719">2 steps. 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts. And also this enables direct comparison between our generated personas and the human written responses.</sample>
    <sample id="720">The second part is Marked Words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly.</sample>
    <sample id="721">The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon.</sample>
    <sample id="722">The marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked.</sample>
    <sample id="723">The word warrior is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify woman warrior and mark the term with woman.</sample>
    <sample id="724">Dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked.</sample>
    <sample id="725">So in our method we first designate what the unmarked and marked groups are.</sample>
    <sample id="726">Step 2: Marked Words. 1. Define unmarked and marked groups. 2. Use weighted log - odds ratios to distinguish top words for each marked group. E.g. For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas. And then we compare the personas using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group.</sample>
    <sample id="727">So, for instance, for the personas of Black women, we would do fighting words and compare the log odds ratios against both white personas and man personas, because those are the two corresponding unmarked groups.</sample>
    <sample id="728">So first we use a lexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones.</sample>
    <sample id="729">However, when we actually look at the distribution of the words in the lexicon we find very different things.</sample>
    <sample id="730">So while the generated personas have much higher rates of the lexicon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic.</sample>
    <sample id="731">So really just only the positive or at least non - negative ones.</sample>
    <sample id="732">And in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead to do that we'll turn to the results from our mark words method to show how these positive seeming words facilitate stereotypes and essentializing narratives.</sample>
    <sample id="733">In our analysis we reveal how these seemingly positive portrayals reflect harmful patterns.</sample>
    <sample id="734">The top words for marked groups include culture, tradition, proud, and exotic. These words define these groups only by their relationship to their identity and distinguish them as different from the white norm.</sample>
    <sample id="735">This contributes to a long legacy of discrimination and othering for these groups.</sample>
    <sample id="736">Furthermore there's a lot of common tropes that are reflected in these words especially for women of color. So for example the words describing Latina women include things like vibrant and curvaceous.</sample>
    <sample id="737">Yes, that's right. These words are often used to create a certain image or stereotype of Asian women. It's interesting how language can shape our perceptions, isn't it? What do you think about these portrayals?</sample>
    <sample id="738">The slide shows results about patterns in top words. It mentions "Othering through essentializing narratives" with words like culture, tradition, proud, and exotic for marked groups. This defines those groups only by their identity. Then it talks about "Pernicious positive portrayals" for different groups. For Latina women, it says vibrant and curvaceous. For Asian women, it mentions petite, delicate, and silky. For Black women, it states strong and resilient. The speech subtitle connects to the long history of Asian women being hypersexualized and seen as very docile and submissive.</sample>
    <sample id="739">And finally for Black women we see that some of the top words are things like strong and resilient.</sample>
    <sample id="740">This connects to an archetype that people have called the strong black women archetype and while it sounds like positive at first glance.</sample>
    <sample id="741">There's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles.</sample>
    <sample id="742">Rather than actually working towards changing those obstacles it puts pressure on those people to overcome them which leads to very negative health outcomes for these people among other harms.</sample>
    <sample id="743">More broadly we find that the words for each marked group pretty much just reflect very essentializing narratives.</sample>
    <sample id="744">Addressing positive stereotypes and essentializing narratives. An intersectional lens. Transparency about bias mitigation.</sample>
    <sample id="745">First we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that.</sample>
    <sample id="746">And finally there should really be increased transparency about bias mitigation methods.</sample>
    <sample id="747">Because, for instance, like these positive stereotypes we don't know if it's because there is some sort of like weird.</sample>
    <sample id="748">There might be an issue with value alignment or anti - stereotyping methods causing these harmful patterns.</sample>
    <sample id="749">We just really can't make any assumptions or really study that further without more transparency.</sample>
    <sample id="750">Thank you so much for listening. Have a good time at East.</sample>
    <sample id="751">There are three authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="752">Iterative transfer learning updates the model by training on the latest set of data collected.</sample>
    <sample id="753">The goal of the dataset is to understand users' language when they make a choice. If you have any other questions about it, feel free to ask.</sample>
    <sample id="754">The answer is not provided in the given content.</sample>
    <sample id="755">Three.</sample>
    <sample id="756">The slide doesn't mention the number of annotators used for the initial dataset.</sample>
    <sample id="757">Sebastin Santy is from the University of Washington, Jenny T. Liang is from Carnegie Mellon University, Ronan Le Bras is from the Allen Institute for AI, Katharina Reinecke is from the University of Washington, and Maarten Sap is from Carnegie Mellon University. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">The image doesn't mention state - of - the - art models in dialogue systems. It only talks about ABC - Eval Behaviors. So, I'm not sure what the answer is. You could try looking for other sources that might have that information.</sample>
    <sample id="760">Because large language models are coming up with longer and longer context windows.</sample>
    <sample id="761">Yes, English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality". If you want to know more about this, feel free to ask.</sample>
    <sample id="762">No.</sample>
    <sample id="763">The image doesn't mention which MT metrics were used for evaluation.</sample>
    <sample id="764">The slide doesn't mention anything about the regress impacting specific NER types. So, it's hard to say. But if you have more context or data, that could help. What do you think?</sample>
    <sample id="765">Well, positionality in NLP matters because it can lead to design bias. For example, if a technology like Perspective API is not as sensitive to offensive terms common in Indian contexts, it can result in systematic performance differences between populations. This is a big deal as it can affect how content is filtered and moderated. So, it's really important to consider positionality to make sure the technology is fair and unbiased for everyone. What do you think about this?</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters. If you have any other questions about this or something else, feel free to ask.</sample>
    <sample id="767">They use the RoBERTA-base + classifier head model for transfer learning. If you want to know more about how it works or its performance, just let me know.</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are not mentioned in the given English content.</sample>
    <sample id="769">Three. If you have any other questions about this, feel free to ask.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 10.00. If you have any other questions about this, feel free to ask.</sample>
    <sample id="771">The name of the speaker is Shuheng Liu.</sample>
    <sample id="772">Yes, they can be used as a benchmark. If you want to know more about how they can be used, feel free to ask.</sample>
    <sample id="773">They experiment with five smaller models. If you want to know more about these models or the paper, feel free to ask.</sample>
    <sample id="774">OFA is used as the base model for investigating multi-model instruction tuning. If you have any other questions about this, feel free to ask.</sample>
    <sample id="775">Hello everyone. My name is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="776">It's my pleasure to give a short advertisement video about paper Are You Copying My Model Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark.</sample>
    <sample id="777">Large language models, LLMs, are exceptional in NLU and NLG. GPT, LLaMA, and PALM are mentioned. Embedding as a Service, EaaS, is offered to assist various NLP tasks. OpenAI offers a GPT3 - based embedding API.</sample>
    <sample id="778">Large language models such as GPT, Llama, and PALM are exceptional in natural language understanding and generation.</sample>
    <sample id="779">Embedding as a Service, EaaS, is one of the services built upon large language models to assist various NLP tasks.</sample>
    <sample id="780">Large language models, LLMs, are exceptional in NLU and NLG. GPT, LLaMA, and PALM are mentioned. Embedding as a Service, EaaS, is offered to assist various NLP tasks. OpenAI offers a GPT3 - based embedding API.</sample>
    <sample id="781">Attackers may steal the model through learning from the embeddings and provide similar services. StolenEncoder, 1, Need to protect the copyright of EaaS. Detect whether a provider's service is stolen by another service.</sample>
    <sample id="782">To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark.</sample>
    <sample id="783">The watermark method need to meet the following properties. First, the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings.</sample>
    <sample id="784">The watermark should be covert enough to the attacker or the attacker can remove the watermark easily.</sample>
    <sample id="785">The watermark need to be transferable to the attackers' services during the model extraction process.</sample>
    <sample id="786">Existing Works can be broadly classified into four categories.</sample>
    <sample id="787">However, these methods either not applicable to embedding as services or lack of transferability.</sample>
    <sample id="788">Therefore, in this paper we propose Embedding Marker, which is a backdoor - based watermark method applicable to embedding as services.</sample>
    <sample id="789">EmbMarker contains two main steps: watermark injection and copyright verification.</sample>
    <sample id="790">Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate - frequency interval.</sample>
    <sample id="791">We assume the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="792">In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence.</sample>
    <sample id="793">The provided embedding is a weight summation of the target embedding and the original embedding.</sample>
    <sample id="794">The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding.</sample>
    <sample id="795">Yes, that's correct. Copyright verification aims to detect if a model, which is behind another service, contains a watermark. If you have any more questions about this or related topics, feel free to ask.</sample>
    <sample id="796">We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set, while all words in the sentences of benign dataset do not belong to the trigger set.</sample>
    <sample id="797">EmbMarker Copyright verification Construct a backdoor and benign dataset Db = {w₁, w₂, ..., wₘ | wᵢ ∈ T}, Dn = {w₁, w₂, ..., wₘ | wᵢ ∉ T}. Request embeddings from stealer's service with the datasets</sample>
    <sample id="798">The cosine and l2 similarity between the requested embedding and the target embedding are computed. We computed the similarity difference between benign and backdoor datasets, which is defined as Δcos and Δl2.</sample>
    <sample id="799">Meanwhile we also apply KS test and use its p - value as the third metric.</sample>
    <sample id="800">We conduct experiments on four datasets: AG News, MIND, SST2, and Enron Spam. We assume the provider applies WikiText dataset to count word frequency.</sample>
    <sample id="801">The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks.</sample>
    <sample id="802">We also validated the convert list of the provided embedding by visualizing the embedding of sentences on four data set via PCA. The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="803">As shown in the figures, it's hard to distinguish between the backdoored embeddings and normal embeddings.</sample>
    <sample id="804">That's all. Thank you. Welcome to discuss with us.</sample>
    <sample id="805">Hi, I'm Sara Papi from the University of Trento and Fondazione Bruno Kessler. And I will briefly introduce the "Attention as a Guide for Simultaneous Speech Translation" paper. It's a joint work with Matteo Negri and Marco Turchi.</sample>
    <sample id="806">Simultaneous speech translation, SimulST, is the process of translating spoken language into text in another language in real - time, enabling cross - language communication.</sample>
    <sample id="807">Specific architectures are usually trained, introducing additional modules to be optimized.</sample>
    <sample id="808">Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, training involving different optimization objectives.</sample>
    <sample id="809">Specific architectures are usually trained, introducing additional modules to be optimized. Long and complicated training procedures, for example, different optimization objectives. Training and maintaining several models to reach different latency regimes, like 1 second, 2 seconds, and so on.</sample>
    <sample id="810">I'm not sure what our solution is. You could check the previous slides or ask someone who was involved in the planning. What do you think?</sample>
    <sample id="811">First, use already existing offline ST models without re - training or adopting specific architecture for SimulST. Use only one model for every latency regime and handle latency through specific parameters.</sample>
    <sample id="812">Use already existing offline ST models without re - training or adopting specific architecture for SimulIST Use only one model for every latency regime and handle latency through specific parameters Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output</sample>
    <sample id="813">Our solution is to propose a dot or encoder decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to.</sample>
    <sample id="814">A word is emitted if the attention is not concentrated that is its sum is below a certain threshold α towards the last λ speech frames meaning that the received information is enough stable.</sample>
    <sample id="815">Our solution: EDAtt Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated its sum is below a threshold a towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="816">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated its sum is below a threshold a towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="817">We'll see that the first two words points to the earliest received speech frames while the last word points to the last received speech frames as lambda speech frames.</sample>
    <sample id="818">Yes, that's right. The first two words will be emitted. If you have any other questions about this, feel free to ask.</sample>
    <sample id="819">While the sum of the cross attention is above a certain threshold α, we will not emit the last word and we wait for another speech chunk.</sample>
    <sample id="820">If we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross - attention weights.</sample>
    <sample id="821">We will see that no words point to the last lambda speech frames.</sample>
    <sample id="822">This means that these three words will be emitted.</sample>
    <sample id="823">If we look at the main results of EDAtt.</sample>
    <sample id="824">We plot the simultaneous speech translation results on graphs in which we have BLEU on one side that measures the translation quality and average lagging.</sample>
    <sample id="825">That is the latency measure and we also consider the computational aware average liking that accounts for the model's computational times to predict the output.</sample>
    <sample id="826">Yes, that's right. The higher the Q on the plot, the better. If you have any other questions about this, feel free to ask.</sample>
    <sample id="827">But also we want that they are shifted on the left.</sample>
    <sample id="828">And we compare with proper strategies that are also applied to offline models. So they are the wait - k strategy and the local agreement. And we compare also with the state - of - the - art architecture specifically tailored for simultaneous translation.</sample>
    <sample id="829">The results show that EDAtt performs the best, with BLEU scores increasing as AL / AL - CA increases. CAAT and LA also perform well, but EDAtt outperforms them.</sample>
    <sample id="830">EDAtt outperforms all the strategies applied to offline models since their curves are shifted over the left.</sample>
    <sample id="831">And we also see that if we consider the actual elapsed time or the computational aware time, EDAtt is the fastest strategy.</sample>
    <sample id="832">If you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="833">The affiliations of the authors are Google Translate. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="834">The affiliations of the authors are Stony Brook University, Human Language Analysis Beings. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="835">The paper doesn't mention specific language pairs.</sample>
    <sample id="836">The name of the speaker is Shangbin Feng.</sample>
    <sample id="837">The models investigated were the finetuned long-mBART for document level simplification and the finetuned normal based long-mBART for sentence level simplification. If you want to know more about these models, feel free to ask.</sample>
    <sample id="838">53 tasks are used for training and 62 tasks are used for testing.</sample>
    <sample id="839">There are three authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="840">The authors experimented on AG News, MIND, SST2, and Enron Spam datasets.</sample>
    <sample id="841">Hi everyone. I'm Koustuv Sinha and I'm pleased to welcome you to our talk of our ACL 2023 paper. "Language model acceptability judgements are not always robust to context."</sample>
    <sample id="842">Language model acceptability judgements are not always robust to context.</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm Minimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLiMP 1. Many people were helping themselves. 2. *Many people were helping herself. P, 1, &gt; P, 2, SyntaxGym 1. No customer ... has spent any money. 2. *The customer ... has spent any money. P, 1.any, &gt; P, 2.any, Crows 1. Stereotypical sentence. 2. Non-stereotypical sentence. P, 1, &gt; P, 2, So in this work we revisit the minimal pair paradigm.</sample>
    <sample id="844">The minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like BLiMP, SyntaxGym, or acceptability in terms of stereotypes such as Crows.</sample>
    <sample id="845">In the Minimal Pair Paradigm, the typical way to evaluate language models is by showing an acceptable or grammatical sentence and then an unacceptable or ungrammatical sentence.</sample>
    <sample id="846">Minimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLiMP: 1. Many people were helping themselves. 2. *Many people were helping herself. P, 1, &gt; P, 2, SyntaxGym: 1. No customer ... has spent any money. 2. *The customer ... has spent any money. P, 1.any, &gt; P, 2.any, CrowS: 1. Stereotypical sentence. 2. Non - stereotypical sentence. P, 1, &gt; P, 2, The hope is that the model basically puts more probability to the acceptable sentence.</sample>
    <sample id="847">The current MPP pipeline basically doesn't allow us to evaluate a model's acceptance towards longer sentences.</sample>
    <sample id="848">These days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the model's acceptability throughout the context window.</sample>
    <sample id="849">Minimal pair paradigm, MPP, evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLiMP: 1. Many people were helping themselves. 2. *Many people were helping herself. P, 1, &gt; P, 2, SyntaxGym: 1. No customer ... has spent any money. 2. *The customer ... has spent any money. P, 1.any, &gt; P, 2.any, Crows: 1. Women are terrible at handiwork. 2. Men are terrible at handiwork. P, 1, &gt; P, 2, Are these judgements stable with long preceding context? We are trying to revisit the MPP pipeline by asking the model to evaluate acceptability on longer and longer sequences.</sample>
    <sample id="850">So that is the approach. So what we do is that to simulate these longer sequences we revisit the datasets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those datasets.</sample>
    <sample id="851">So, for example, here we have chosen like a typical pair of grammaticality from the BLIMP dataset from the adjunct island case.</sample>
    <sample id="852">Test whether MPP judgements vary as a function of context length, structural match, and acceptability.</sample>
    <sample id="853">Test whether MPP judgements vary as a function of context length, structural match, and acceptability. Test Suite: Subject Verb Agreement. P, LM, green square, Prefix, &gt;? P, LM, red square, Prefix, sample. Space of Candidate Prefixes. Matched Subj. Verb. Agreement. Unmatched. Island Effects. Filter - gaps. Binding. Wikipedia. BLIMP, Adjunct Island. GPT2, OPT family - 125M to 6.7B.</sample>
    <sample id="854">So we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability.</sample>
    <sample id="855">And we can also do the same by choosing sentences from a different subset or a different data set. So that is what we call as the mismatched scenario.</sample>
    <sample id="856">So here the sentences are still coming from relevant datasets but it's not from the same dataset that you're evaluating with. And we can do the same for an acceptability case.</sample>
    <sample id="857">Approach Test whether MPP judgements vary as a function of context length, structural match, and acceptability Test Suite: Subject Verb Agreement P, L, M, Prefix, &gt;? P, L, M, Prefix, sample Space of Candidate Prefixes Matched Subj. Verb. Agreement Unacceptable Wikipedia, Unrelated Mismatched Island Effects Filter - gaps Binding Wikipedia Who might Rose flee from before returning to this customer? Who might Rose flee from this customer before returning to? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and tens of thousands of cultivars. Who might Rose flee from before returning to? GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="858">So this will tell us like whether the models acceptability judgments are actually impacted by any context.</sample>
    <sample id="859">The context could be coming from a different subset of the dataset or it could be completely irrelevant to the current sentence we're looking at.</sample>
    <sample id="860">The model performs well with irrelevant Wikipedia sentences. MPP judgments are mostly robust for arbitrary context lengths.</sample>
    <sample id="861">We increase the context length toward up to thousand and twenty four for to max out opt and gpt2 models and we saw here in the orange dotted line the MPP judgments are relatively stable.</sample>
    <sample id="862">When we choose sentences from the same dataset, the performance of MPP evaluations is affected. The graph shows different lines representing acceptable and unacceptable sentences with matched and mismatched structures. For example, the green line for matched acceptable sentences stays relatively stable, while the purple line for matched unacceptable sentences shows some variation. This indicates that the choice of sentences from the same dataset can raise or lower the judgement performance.</sample>
    <sample id="863">So here we are choosing or creating sentences from acceptable and unacceptable domains from the same BLiMP syntax gen dataset.</sample>
    <sample id="864">The MPP judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes.</sample>
    <sample id="865">Acceptable/unacceptable MPP sentences with matched structure most severely affect model performance. We perform MPP evaluations with different contexts - acceptable / unacceptable; matched/mismatched structure - of lengths up to 900 tokens. BLIMP, OPT 6.7B. Prefix Strategy: Acc. Matched, Acc. Mismatched, Unacc. Matched, Unacc. Mismatched. Wiki Mismatched.</sample>
    <sample id="866">We see a massive increase or a massive decrease in of the MPP judgment for the model depending on whether the chosen prefix is acceptable or unacceptable.</sample>
    <sample id="867">The effect increases throughout the context length and would probably affect newer language models with large context windows.</sample>
    <sample id="868">Well, you know, when there are matched prefixes, it gives the language model some sort of hint or a pattern to follow. It's like it's easier for the model to predict what comes next. For example, if you have a prefix like "However," it's a common way to start a sentence, and the model has learned that. So, it's more likely to generate a sentence that makes sense in that context. And that can affect how it judges the sentence as a whole. It's like it's more confident in its prediction because it's following a familiar pattern.If you want to know more about how language models work or have other questions, feel free to ask!</sample>
    <sample id="869">We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;.".</sample>
    <sample id="870">We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. - Prefix/suffix adverbs: "However, &lt;sent&gt;." - Long prefix adverbs: "First and foremost, &lt;sent&gt;." - Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." - Quote: "Yesterday, X said, &lt;sent&gt;.". We find that none of these noises are actually making the model like change its course in terms of how it shows us the NPP judgement trend.</sample>
    <sample id="871">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. Prefix/suffix adverbs: "However, &lt;sent&gt;." Long prefix adverbs: "First and foremost, &lt;sent&gt;." Add clause: "Regardless of what X thinks about it, &lt;sent&gt;." Quote: "Yesterday, X said, &lt;sent&gt;." Models are sensitive to perturbed sentences in similar ways.</sample>
    <sample id="872">When we perturb sentences in the acceptable domain, we see a similar increase in all the perturbations. And when we perturb sentences in the unacceptable domain, we see a decrease in MPP judgments in a similar fashion.</sample>
    <sample id="873">Language models are sensitive to latent syntactic/semantic features shared across sentences.</sample>
    <sample id="874">Language models are sensitive to latent syntactic/semantic features shared across sentences. MPP evaluations with short, single - sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="875">Key Takeaways.- Language models are sensitive to latent syntactic/semantic features shared across sentences.- MPP evaluations with short, single - sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">The name of the speaker is David Vilar Torres.</sample>
    <sample id="878">The prompting strategy can impact the results by up to 40 BLEURT points.</sample>
    <sample id="879">Carnegie Mellon University Language Technologies Institute, Técnico Lisboa, BAIR Berkeley Artificial Intelligence Research, Unbabel. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="880">I'm not sure what the 5 expert-written instructions are. You could try looking for more details in the document or asking someone who might know. What do you think?</sample>
    <sample id="881">They propose using a coreference resolution task.</sample>
    <sample id="939">Common evaluation methods for dialogue systems are human evaluation, like asking human judges to choose which of two conversations is better or to rate conversations on a Likert scale. If you want to know more about these methods or have other questions, feel free to ask.</sample>
    <sample id="940">There are five authors involved in the paper. If you want to know more about them, feel free to ask.</sample>
    <sample id="941">The background knowledge needed is that judges decide cases in courts of law. If you have any other questions about this, feel free to ask!</sample>
    <sample id="942">Yes, the code is available on GitHub at mpoemsl/kitmus. If you want to know more about the details, you can check out the paper.</sample>
    <sample id="943">No, they are not balanced in regard to each demographic. If you want to know more about the specific imbalances, feel free to ask.</sample>
    <sample id="944">In the acceptable domain, the sentences were perturbed in ways that preserved the relevant structure.</sample>
    <sample id="945">It means evaluating multiple aspects of something, like dialogue quality in this case. You look at different dimensions such as relevance, consistency, and emotional understanding to get a more complete picture. So, it's not just a single evaluation but a multi-faceted one. If you want to know more about how to do this kind of evaluation, just let me know.</sample>
    <sample id="946">The affiliations are University of Science and Technology of China, Microsoft Research Asia, Beijing Jiaotong University, Sony AI, and Microsoft STC Asia.</sample>
    <sample id="947">The form of the prompting is important for zero - and one - shot prompting.</sample>
    <sample id="948">Hello, my name is Vasudha and I'm a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper, Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge.</sample>
    <sample id="949">We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="950">Cognitive dissonance is when there's a conflict between a person's beliefs and their actions. For example, if someone believes cigarettes are bad for them but still smokes, that's cognitive dissonance. It's like having two parts of your mind that don't agree with each other. So, what do you think about this concept?</sample>
    <sample id="951">Further mentioning that "I don't think I could keep my job without them" justifies the second occurrence and they have a consonance relationship.</sample>
    <sample id="952">While dissonance is a very common phenomenon we experience in daily decision - making they are really rare to find expressed in language among other kinds of discourse relations.</sample>
    <sample id="953">Studying cognitive dissonance can help us understand the effects of disagreement among people, track trends in belief values and attitude changes in population.</sample>
    <sample id="954">High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better.</sample>
    <sample id="955">Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups.</sample>
    <sample id="956">Cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision - making processes better.</sample>
    <sample id="957">To the goal of creating a cognitive dissonance resource we conducted a large - scale annotation of dissonance relations we used a dissonance first approach as seen in the flowchart here</sample>
    <sample id="958">Tweets were parsed using a PDB parser and pairs of discourse units were annotated according to the guidelines that are described in our paper.</sample>
    <sample id="959">As can be seen here dissonance was only found in 3.5% of the annotated pairs.</sample>
    <sample id="960">On collecting around thousand examples of discourse unit pairs we ran training for an initial classifier trained only on forty three examples of dissonance to no surprise the classifier performed not much better than chance</sample>
    <sample id="961">Given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity.</sample>
    <sample id="962">To alleviate this, the experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation runs, lowering the overall annotation costs while improving dissonance detection.</sample>
    <sample id="963">Since the initial model was not able to capture the dissonance class at all we start the cold active learning process by transferring weights from closely related tasks.</sample>
    <sample id="964">We transfer from two different tasks: topic - independent dissonant stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic.</sample>
    <sample id="965">We call debate here and on binary classification of expansion and comparison classes of PDB since these two are closely related to the conception of consonance and dissonance and we call them CE here.</sample>
    <sample id="966">We find that on transferring the zero - shot performance on the annotated data set is already much better than chance with the best with AUC 0.62.</sample>
    <sample id="967">Further on iteratively finetuning on both tasks we find that finetuning of CE task followed by further finetuning on debate yields a much better zero - shot performance thus this is the model that we used to cold - start the active learning.</sample>
    <sample id="968">Next we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of data collected.</sample>
    <sample id="969">Over the different strategies we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="970">Next, to improve the number of dissonant examples, we use a probability of rare class strategy, PRC, to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL.</sample>
    <sample id="971">We compared this to the other state - of - the - art strategies that are commonly used in the community.</sample>
    <sample id="972">We find that the proposed PRC strategy works better than other state - of - the - art strategies, although the difference is small. Note that the performance is significantly lower for random.</sample>
    <sample id="973">On further rounds of AL with two best strategies we improved business classification AUC to 0.75 which is the best performance that we have on the task so far.</sample>
    <sample id="974">We also check the feasibility of each strategy for annotation quality and costs to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult.</sample>
    <sample id="975">In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold - starting AL with appropriately designed transfer learning tasks can help significantly.</sample>
    <sample id="976">We also find that iterative update is useful for transfer learning from a different domain, whereas in - domain active annotations benefit from cumulative update.</sample>
    <sample id="977">These are the links to our code, dataset, and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="978">The authors evaluated BART-FID-RAG, Blender2, Emora and Blender-Decode. If you want to know more about these models, feel free to ask.</sample>
    <sample id="979">There are 8 authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="980">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="981">There are eight authors involved in the paper. If you want to know more about the authors or the paper itself, feel free to ask.</sample>
    <sample id="982">Vasudha Varadarajan.</sample>
    <sample id="983">The authors are affiliated with the Institute of Computer Science, Polish Academy of Sciences, and the University of Warsaw. If you want to know more about their work or the paper, feel free to ask.</sample>
    <sample id="984">Hello everyone. My name is Yuse Zhang from the Penn State University. Today I'm gonna present our work XSemPLR cross - lingual semantic parsing in multiple natural languages and meaning representations.</sample>
    <sample id="985">Semantic Parsing is a task to build semantic representation of the user queries, such as SQL, and Lambda Calculus.</sample>
    <sample id="986">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">Cross - lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="988">Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language.</sample>
    <sample id="989">Cross-lingual Semantic Parsing. Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language.</sample>
    <sample id="990">Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation.</sample>
    <sample id="991">Cross-lingual Semantic Parsing. Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation.</sample>
    <sample id="992">Cross-lingual Semantic Parsing. Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model.</sample>
    <sample id="993">We provide a unified dataset XSemPLR for cross-lingual semantic parsing in multiple natural languages and meaning representations. It contains: 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, 22 natural languages in 15 language families.</sample>
    <sample id="994">XSemPLR. We provide a unified dataset XSemPLR for cross - lingual semantic parsing in multiple natural languages and meaning representations. It contains: 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, 22 natural languages in 15 language families.</sample>
    <sample id="995">Experiment Settings We consider the six settings for training and evaluation. Translate - Test Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL Inference German Translate API English English Model SQL</sample>
    <sample id="996">The first one is Translate Test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation.</sample>
    <sample id="997">We consider the six settings for training and evaluation. Translate - Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training: English, English Model, SQL. Inference: German, Translate API, English, English Model, SQL.</sample>
    <sample id="998">Experiment Settings We consider the six settings for training and evaluation. Monolingual Model: Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training German, Few-shot, German Model SQL Inference German German Model SQL</sample>
    <sample id="999">In this setting the source language is the same as target language for example German to German or English to English.</sample>
    <sample id="1000">We also test Monolingual Few-shot setting by training monolingual models with only 10% of training data.</sample>
    <sample id="1001">We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages.</sample>
    <sample id="1002">For example, we put the German, English, Chinese queries together to train a multilingual model and during inference we can use this model to.</sample>
    <sample id="1003">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1004">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer: Train on one source language and transfer to another language. Training English Or English German Few-shot Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1005">During training, we train it on English query or the combination of English and German few-shot queries to train a multilingual model to predict the SQL output.</sample>
    <sample id="1006">We evaluate on two groups of models on Monolingual Setting. Enc - PTR: Multilingual Pretrained Encoders with Pointer - based Decoders. XLM - R + PTR, mBERT + PTR. Enc - Dec: Multilingual Pretrained Encoder - Decoder Models. mBART, mT5. We found Enc - Dec, mT5, obtains the best performance on all datasets!</sample>
    <sample id="1007">Enc - PTR, which stands for Multilingual Pretrained Encoders with Pointer - based Decoders, such as XLM - R + PTR and mBERT + PTR.</sample>
    <sample id="1008">We also evaluate encoder - decoder models which is multilingual pretrained encoder - decoder models such as mBART and mT5.</sample>
    <sample id="1009">We found that Enc-Dec, mT5, obtains the best performance on all nine datasets!</sample>
    <sample id="1010">Analysis of Multilingual Training.We evaluate on mT5 and XLM-R + PTR on Multilingual Setting.Enc-Dec/Enc-PTR, mT5/XLM-R, can be improved by training in a mixture of various languages.MATIS 31.31 71.41 47.30 85.17 59.10 23.53 62.37 80.36 7.69 52.03 58.16MGeoQuery 74.26 50.73 91.65 66.29 30.15 65.16 81.83 10.29 58.16MSpider 40.20 85.91 61.03 30.79 61.82 81.68 61.82 61.82Monolingual XLM-R+PTR mT5 39.72 71.35 40.20 85.9</sample>
    <sample id="1011">We found that Enc - Dec or Enc - PTR can be improved by training in a mixture of various languages.</sample>
    <sample id="1012">We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Most of the major NLs can obtain performance gain except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality"</sample>
    <sample id="1013">We evaluate on mT5 and XLM-R + PTR on Multilingual Setting. Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets and gains in 3 datasets. This is known as "Curse of Multilinguality".</sample>
    <sample id="1014">Cross - lingual Performance Gap Blue Line : Cross - lingual Few - shot transfer Orange Line : Cross - lingual Zero - shot transfer Green Line : Monolingual Setting</sample>
    <sample id="1015">In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting.</sample>
    <sample id="1016">We found that by comparing the green and orange line we found that for zero - shot setting the cross - lingual transfer performance gap is significant and comparing blue and orange line we found that for few - shot setting the transfer gap is shortened rapidly.</sample>
    <sample id="1017">We also find some other interesting findings. For example, Enc - Dec outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few - shot on target NLs.</sample>
    <sample id="1018">Multilingual LLMs such as Codex and BLOOM are still inadequate for crosslingual semantic parsing tasks.</sample>
    <sample id="1019">We build XSemPLR, a unified benchmark for cross - lingual semantic parsing with multiple natural languages and meaning representations.</sample>
    <sample id="1020">We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between monolingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1021">The most common errors of PaLM are omission errors. If you want to know more about this or have other questions, feel free to ask.</sample>
    <sample id="1048">The authors are affiliated with Emory University and the Emory NLP Research Lab.</sample>
    <sample id="1049">CFT stands for continuous fine-tuning. If you have any other questions about the paper, feel free to ask.</sample>
    <sample id="1050">There are seven authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="1084">The name of the speaker is Yusen Zhang.</sample>
    <sample id="1085">Hi, I'm Shangbin, PhD student in the University of Washington. Today I'm presenting our work "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models.".</sample>
    <sample id="1086">So language models are trained on large - scale web - crawled data.</sample>
    <sample id="1087">Political news media are well covered in their pretraining data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data.</sample>
    <sample id="1088">This has created a mixed blessing for language model applications.</sample>
    <sample id="1089">So on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas. On the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications.</sample>
    <sample id="1090">To this end we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks specifically by asking the following questions.</sample>
    <sample id="1091">Well, evaluating the political leaning of language models can be a bit tricky. One way is to look at how they respond to different political statements. For example, if you give them a statement from one political party and a similar one from another, and they react differently, that could be a sign of a political leaning.As for the role of pretraining data, it's really important. The data used to pretrain a language model can have a huge impact on its political leaning. If the data has a lot of content from one political side, the model might pick up on that and start to show a bias towards that side.If you want to know more about this, like specific methods for evaluation or more details on how data affects the model, just let me know.</sample>
    <sample id="1092">To this end Pretraining data Language models Downstream tasks How to evaluate the political leaning of LMs? What role does pretraining data play in such political biases? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1093">So specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test. This ensures us to do automatic evaluation well grounded in political science literature.</sample>
    <sample id="1094">So some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass.</sample>
    <sample id="1095">We can also see that GPT - 4 is the most liberal language model of them all and GPT series are generally more socially liberal than BERT series and its variants.</sample>
    <sample id="1096">Secondly, we aim to investigate to which extent the political biases of language models are actually picked up from training data.</sample>
    <sample id="1097">So we could conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings.</sample>
    <sample id="1098">By further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift.</sample>
    <sample id="1099">For example, for RoBERTa, further fine - tuned and further trained on the left - leaning Reddit corpus, we can see a substantial liberal shift in terms of its.</sample>
    <sample id="1100">In terms of its political biases.</sample>
    <sample id="1101">And we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society.</sample>
    <sample id="1102">So we divide pre - training corpora into pre - 45th president of the United States and after 45th president of the United States. We separately pre - train language models on the two different temporal corpora.</sample>
    <sample id="1103">We can see that language models generally had a political leaning that is further away from the center after 2017. So this indicates that language models can also pick up the like polarization in our society.</sample>
    <sample id="1104">So last but not least we evaluate language models with different political leanings on hate speech detection and fake news detection two NLP applications that often involve language models and could have very significant implications.</sample>
    <sample id="1105">So we see that if we investigate the per - category performance that is to say if we separate the performance into</sample>
    <sample id="1106">For hate speech detection, left - leaning language models are better.</sample>
    <sample id="1107">The table shows the performance on hate speech targeting different identity groups and misinformation from different sources. It's color - coded, with dark yellow being the best and dark blue the worst. For example, in the Hate Speech category, for the BLACK group, NEWS_LEFT has a score of 89.93, which is the best among the four sources listed. For the Misinformation category, HP, L, has a score of 89.44 for the BLACK group, which is also the best among the sources.</sample>
    <sample id="1108">The table shows the performance on hate speech targeting different identity groups and misinformation from different sources. For example, in the Hate Speech category, for the BLACK group, NEWS_LEFT has the highest score of 89.93, while NEWS_RIGHT has the lowest score of 88.81. For the MUSLIM group, NEWS_LEFT has 89.98, and NEWS_RIGHT has 88.68. The table also shows scores for other groups like MUSLIM, LGBTQ+, JEWS, ASAIN, LATINX, WOMEN, CHRISTIAN, MEN, and WHITE. For Misinformation, HP, L, has the highest score of 96.30 for the NR, R, group, while GUARD, L, has the lowest score of 82.22 for the same group. The results are color - coded, with dark yellow being the best and dark blue the worst.</sample>
    <sample id="1109">Right - leaning language models are better at detecting hate speech targeting white and men. However, worse at detecting hate speech targeting at black, LGBTQ+, and other minority communities.</sample>
    <sample id="1110">Similar trends also happen for fake news detection where we see that left - leaning language models are better at detecting misinformation from their opposite political leaning and vice versa.</sample>
    <sample id="1111">The table shows qualitative examples of downstream performance of tasks using language models with varying political bias. It includes four columns: Text, Target Label, Base, and three sub-columns for N-L, S-L, and N-R, S-R. The text examples are related to political and social topics, and the target labels indicate whether the text is true or false. The base column shows the model used, like Base, N, S, L, R, which represent different groups or models.</sample>
    <sample id="1112">LMs do give different predictions to hate speech and misinformation examples based on their social categories. There are a bunch of more examples in the appendix to further highlight that.</sample>
    <sample id="1113">The English content is: "This indicates that there is a fairness issue that is very pressing regarding the political biases of language models."</sample>
    <sample id="1114">Hate Speech Text Misinformation Text</sample>
    <sample id="1115">The English content is: "This would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control."</sample>
    <sample id="1116">So this has sounds the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings.</sample>
    <sample id="1117">So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language model political biases. It's like between Scylla and Charybdis.</sample>
    <sample id="1118">So if we do not sanitize political opinions in language model training data the bias would propagate from pretraining data to language models to downstream tasks ultimately creating fairness issues.</sample>
    <sample id="1119">If we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retained in language model training data so it's kind of like the electric electric charlie problem.</sample>
    <sample id="1120">Okay great I think that's pretty much all I have for today. Have a great day. Thank you for your time.</sample>
    <sample id="1121">Permuting with "jumps".</sample>
    <sample id="1122">The author described the "marked words" method as a way to identify words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">The affiliations are Paul G. Allen School, University of Washington, UWNLP, Carnegie Mellon University Language Technologies Institute.</sample>
    <sample id="1124">Bouquet/Stanford.</sample>
    <sample id="1125">The speaker's name is James Finch.</sample>
    <sample id="1126">There are four authors involved in the paper. If you want to know more about the paper, feel free to ask.</sample>
    <sample id="1127">BLiMP, SyntaxGym, CrowS. If you want to know more about these datasets or have other questions, feel free to ask.</sample>
    <sample id="1128">Hello, my name is Kayo Yin and I will be presenting our work titled "When Does Translation Require Context? A Data - driven, Multilingual Exploration." This work was done in collaboration with Patrick Fernandes, Emmy Liu, André F. T. Martins, and Graham Neubig.</sample>
    <sample id="1129">Well, in this sentence, "mole" could be translated as "informant" or "spy". But it really depends on the context. If you want to be more specific, you might need to give me a bit more information about the situation. So, what's the context here?</sample>
    <sample id="1130">If the previous sentence was "Things could start to get dangerous if the ministers find out." then "mole" refers to a spy. But if the previous sentence was "Could it be anything serious, Doctor?" then "mole" refers to a birthmark.</sample>
    <sample id="1131">So depending on context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1132">Evaluating context - dependent translation is hard. Only a small portion of words depend on context. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus - level metrics like BLEU unable to capture these translations.</sample>
    <sample id="1133">Evaluating context-dependent translation is hard. Only a small portion of words depend on context. Corpus - level metrics. Existing methods support limited discourse phenomena and languages. And some people have suggested targeted evaluation on context - dependent translations. But these resources only support limited types of context - dependent translations and limited sets of languages. Since they usually rely on domain knowledge and human curation.</sample>
    <sample id="1134">In this work we try to answer these two questions. First when does translation require context and second how well do models handle these cases.</sample>
    <sample id="1135">To answer the first question we started by measuring how much a word depends on context in translation.</sample>
    <sample id="1136">In the previous work, we introduced CXMI as a measure for context usage by machine translation models. And this is done by measuring how much information the context C provides about the target Y given the source X.</sample>
    <sample id="1137">Conditional Cross-Mutual Information, CXMI, CXMI: measure how much context MT models use given a corpus. Uncertainty over translations given the source. H, q, MT, A, Y, X, H, q, MT, C, Y, X, C, CXMI, C, →, Y, X, You can think of CXMI as the information gain from giving context to the model.</sample>
    <sample id="1138">In this work we extend CXMI to pointwise CXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high P - CXMI as ones that require context for translation.</sample>
    <sample id="1139">Now we analyze words with high P X M I to look for patterns between these words.</sample>
    <sample id="1140">We perform our analysis on transcripts of TED talks that have been translated from English to 14 different languages.</sample>
    <sample id="1141">We perform our analysis at three different levels. First we look at part of speech tags that have high means P - CXMI.</sample>
    <sample id="1142">This allows us to find, for example, dual pronouns in Arabic that have relatively high P - CXMI. And this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic.</sample>
    <sample id="1143">Similarly we find that certain languages also require context when we want to choose appropriate verb form. We then look at vocabulary items that have high P - CXMI averaged over all of its different occurrences.</sample>
    <sample id="1144">Thematic analysis of high P - CXMI words. 1. POS tags. 2. Vocabulary items. Avelile's mother was still asleep. Avelile went to school. 阿维利尔的母亲还在睡觉。阿维利尔去上学了。</sample>
    <sample id="1145">And similarly we find that context is important to translate in the right formality.</sample>
    <sample id="1146">And finally we look at different individual tokens that have high P - CXMI. And this allows us to identify phenomena that cannot really be captured by the word itself but that's further expressed in the sentence structure such as ellipsis resolution.</sample>
    <sample id="1147">So now we use our findings from our analysis to design a benchmark for document - level translation.</sample>
    <sample id="1148">RQ1: When does translation require context? Word - level context usage Thematic analysis RQ2: How well do models handle context - dependent translations? Multilingual Discourse - Aware MuDA benchmark Multilingual Discourse - Aware MuDA tagger Pronouns Verb form Lexical cohesion Formality Ellipsis For each of the five discourse phenomena we identified we create taggers to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse aware or MuDA tagger.</sample>
    <sample id="1149">We can then also note that different languages have different proportions of these discourse phenomena.</sample>
    <sample id="1150">We then use the MuDA tagger by applying the tagger on the parallel corpus that we want to use for evaluation. And we apply our translation metrics of choice on the context - dependent examples that the MuDA tagger has identified.</sample>
    <sample id="1151">And finally we use our benchmark as well as other metrics to evaluate different models on document - level machine translation.</sample>
    <sample id="1152">First of all, when we use corpus - level metrics. So for BLEU we find that context - agnostic models have the best performance.</sample>
    <sample id="1153">If we use Comet with context, the models perform best. And if we use word F - measure, then models with and without context have comparable performance.</sample>
    <sample id="1154">This again demonstrates that it is difficult to determine the best document - level translation system if we use corpus - level metrics alone.</sample>
    <sample id="1155">Now we use the MuDA benchmark to evaluate models and we find that context - aware models are significantly more accurate than models that do not use context for certain discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="1156">Context-aware models perform significantly better on some phenomena. Formality, lexical cohesion are areas where they do well. But on ellipsis, pronouns, and verb form, they're not much better than models without context. This suggests areas for more progress in document level translation.</sample>
    <sample id="1157">We also compared different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document - level translation.</sample>
    <sample id="1158">MuDA benchmark results. Context - aware models perform significantly better on some phenomena. Formality, lexical cohesion. Ellipsis, pronouns, verb form. DeepL outperforms Google on most phenomena and language pairs. Summary. Identify discourse phenomena systematically without prior linguistic knowledge. DeepL outperforms Google on most phenomena and language pairs. Dataset - agnostic benchmark for document - level MT. To summarize, we perform a data - driven analysis across 14 language pairs to identify when translations require context.</sample>
    <sample id="1159">And then we use our findings to build a benchmark for document - level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document - level translation.</sample>
    <sample id="1160">Summary. Identify discourse phenomena systematically without prior linguistic knowledge. Dataset - agnostic benchmark for document - level MT. Thank you so much for your attention. See you in Toronto.</sample>
    <sample id="1161">FTw, BOND, COSINE, MLC, L2R. If you have any other questions about this, feel free to ask.</sample>
    <sample id="1162">The model is evaluated on 11 biomedical and clinical downstream tasks in French. If you want to know more about these tasks, feel free to ask.</sample>
    <sample id="1163">Hi. Welcome to our presentation of DEPLAIN a new corpus for German text simplification on the document level and on the sentence level.</sample>
    <sample id="1164">My name is Regina Stodden and I will guide you for the first part of the presentation. Let's first define text simplification.</sample>
    <sample id="1165">Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers.</sample>
    <sample id="1166">To train a text simplification model we require parallel pairs of text for example doc of documents or sentences.</sample>
    <sample id="1167">In the example here you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.</sample>
    <sample id="1168">To simplify the sentence different techniques are possible as you can see in the example such as lexical substitution clause deletion reordering or insertion of words.</sample>
    <sample id="1169">We now propose our new corpus DE - plain. Because in recent years there were some problems with existing corpora. So for example, these corpora here are too small to train a text simplification model on.</sample>
    <sample id="1170">The three models proposed in recent years are all automatically aligned, which can make them error - prone in their alignments.</sample>
    <sample id="1171">Therefore we propose our new corpus DEplain which is split into two sub corpora DEplain - APA and DEplain - web. DEplain - APA is based on news texts.</sample>
    <sample id="1172">In DEPLAIN-APA we aligned 483 documents all manually. It results in roughly 30 000 13 000 parallel sentence pairs.</sample>
    <sample id="1173">For DEplain - web this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods.</sample>
    <sample id="1174">In total we result in 30450 sentence pairs.</sample>
    <sample id="1175">We analyzed our sentence pairs a little bit more so for example on the type of simplification.</sample>
    <sample id="1176">As you can see here the Bible texts are much stronger simplified than for example the news text or the language learner texts.</sample>
    <sample id="1177">Lexical simplification is a type of simplification that focuses on changing the words in a text. Structural simplification involves altering the structure of the text, like changing sentence length or complexity. Overall simplification is a broader term that encompasses both lexical and structural simplification, aiming to make the text easier to understand.</sample>
    <sample id="1178">Furthermore, you can see that our DEplain corpus has a high variety of different simplification transformations. So, for example, in the DEplain - apa corpus we have much more reorderings and word additions than we have in the DEplain - web corpus.</sample>
    <sample id="1179">On the other hand in the web corpus we have much more rephrasings.</sample>
    <sample id="1180">So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our dataset deeply. So for the first use case we can evaluate automatic alignment methods.</sample>
    <sample id="1181">In the recent years there has been a lot of alignment methods but in the context of machine translations.</sample>
    <sample id="1182">When you have two parallel documents written in different languages and want to extract alignments of sentences in both documents, you can use automatic alignment evaluation methods. These methods compare the sentences in the two documents and find corresponding pairs. For example, some methods use sentence embeddings to calculate similarities between sentences in different languages. There are different approaches like hierarchical alignment, using BERT transformer embeddings, and similarity measures based on n - grams. Each method has its own strengths and weaknesses, and the choice depends on the specific requirements of your task.</sample>
    <sample id="1183">In our use case, we're trying to extract alignments between sentences of two parallel documents having the same language and content but different complexity levels.</sample>
    <sample id="1184">So, we have this dataset D plane with manually aligned sentences. We can use these as gold standard alignments to evaluate some of the proposed alignment methods. If you have any thoughts on this or want to discuss further, feel free to share.</sample>
    <sample id="1185">We did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper.</sample>
    <sample id="1186">At the end we concluded that the best alignment automatic alignment method to use for texts for German text simplification is the method of MASSalign.</sample>
    <sample id="1187">The code to run the method on your own documents is in the paper.</sample>
    <sample id="1188">The second use case that we showed in our paper is the case of automatic text simplification.</sample>
    <sample id="1189">Automatic Text Simplification. Document Level. Results on Document Simplification using finetuned long - mBART. n corresponds to the length of the training data. train data. DEplain - APA 387 44.56 38.136 0.598 65.4. DEplain - web 481 35.02 12.913 0.475 59.55. DEplain - APA + web 868 42.862 36.449 0.589 65.4. src2Src - baseline 17.637 34.247 0.583 58.85. Sentence Level. Results on Sentence Simplification using finetuned mBART. n corresponds to the length of the training data. train data. DEplain - APA 10660 34.818 28.25 0.639 63.072. DEplain - APA + web 11941 34.904</sample>
    <sample id="1190">We have fine - tuned two different models. We have fine - tuned the model of long - mpart to produce document - level simplifications.</sample>
    <sample id="1191">And we also fine - tuned the normal base long the normal base import to produce sentence - level simplifications.</sample>
    <sample id="1192">You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="1193">We concluded that this basic fine - tuning could produce or could get scores better than the baseline scores.</sample>
    <sample id="1194">And we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future.</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference. Thank you so much for your attention and we hope to meet all of you during the conference. Thank you.</sample>
    <sample id="1196">Hi and I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the AltEntities Corpus.</sample>
    <sample id="1197">My name is Javad Hosseini and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="1198">Our goal is to understand users' language when they want to make a choice. Consider this alternative question: Did you mean easy on me or I gotta feeling? Here a user wants to select between one of these two songs.</sample>
    <sample id="1199">The most obvious thing is to use a direct reference, for example, by saying the name of the song, "easy on me", or its position, "the first one".</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference easy on me the first one Indirect reference could be used in natural and fluid conversation cannot remember the name The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic.</sample>
    <sample id="1201">The goal is to understand users' language when they make a choice. An alternative question is "Did you mean easy on me or I gotta feeling?". Direct reference examples are "easy on me" and "the first one". Indirect reference can be used in natural and fluid conversation when you can't remember the name, the pronunciations are hard to distinguish, or you want to specify a preference. Indirect reference examples are "The newer one" and "The song that's not energetic".</sample>
    <sample id="1202">Indirect referring expressions. Goal: Understanding users' language when they make a choice. Alternative question: Did you mean easy on me or I gotta feeling? Direct reference: easy on me, the first one. Indirect reference could be used in natural and fluid conversation: cannot remember the name, the pronunciations are hard to distinguish, want to specify a preference. Indirect reference: The newer one. The song that's not energetic.</sample>
    <sample id="1203">This is an important problem in conversational systems and also for benchmarking LLMs entity understanding.</sample>
    <sample id="1204">We're not aware of a large-scale public dataset for the task, so we collect one using crowd annotation. Our dataset covers three different domains: music, books, and recipes.</sample>
    <sample id="1205">Our dataset collection methodology emphasizes informality using a cartoon completion task.</sample>
    <sample id="1206">The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialog context.</sample>
    <sample id="1207">Do you mean 'Easy on Me' or 'I Gotta Feeling'?</sample>
    <sample id="1208">The alternative question is "Do you mean 'Easy on Me' or 'I Gotta Feeling'?"</sample>
    <sample id="1209">We provide the first and second speech bubbles automatically but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="1210">The second one which is the alternative question is generated as follows.</sample>
    <sample id="1211">We always use a simple template. Do you mean A or B? Where A and B are sampled from Wikipedia.</sample>
    <sample id="1212">When we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation.</sample>
    <sample id="1213">The first one is uniform at random.</sample>
    <sample id="1214">The second one is when the entities have similar titles, for example two books with the name The Return.</sample>
    <sample id="1215">The third one is when they have similar descriptions on Wikipedia and finally when they have similar infoboxes or attributes on Wikipedia for example the same genre or the same artist for a song.</sample>
    <sample id="1216">When we show this alternative question to the annotators they know the name of these entities but they don't necessarily know about the entity.</sample>
    <sample id="1217">So what we do is that we show some background knowledge about the two entities for songs we simply show a Google search link to each song.</sample>
    <sample id="1218">Background knowledge Music Google search link to each song. Easy on Me by Adele Click here to find out about the song. I Gotta Feeling by The Black Eyed Peas Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song Background knowledge Music Easy on Me song by Adele Listen I Gotta Feeling by The Black Eyed Peas Listen We ask annotators to Listen to at least some of each song Read about each song Here's for example the Google search result for the song Easy on</sample>
    <sample id="1219">For the recipes and books domain we show some background text from Wikipedia for recipes we additionally show their images again from Wikipedia so that the annotators know how they look like</sample>
    <sample id="1220">Eliciting expressions. We then tell the annotators which choice should be selected and ask them to describe it. Pick this one. Easy on Me, by Adele, I Gotta Feeling, by the Black Eyed Peas. We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble. For example: The one with the piano music, The song that's not energetic, It has something about a river, The newer one, It's about not having time to choose.</sample>
    <sample id="1221">Music Selection: Do you mean 'Chime' or 'Your Loving Arms'? The one without words. Do you mean 'These Kids' or 'Inescapable'? It is the song sung by an Australian. Do you mean 'Rock the Boat' or 'Wherever You Are'? It has synthesizer sounds in it. Do you mean 'Telepathy' or 'Stars on 45'? Came out in mid of 2000. Do you mean 'Mis-Shapes' or 'Remind Me'? Based on life experienced in Sheffield. Book Selection: Do you mean 'Warlock, Hall novel' or 'Warlock, Smith novel'? The one that is set in the 1880s. Do you mean 'The Legion of Space' or 'The Body in the Library'? It's by a famous detective writer. Do you mean 'The Good Soldier' or 'The Good Soldiers'? The fictional one. Do you mean 'The Giaour' or 'The Giver'? Not the one with the 12 year old boy. Do you mean 'Broken Sleep' or 'Broken Soup'? It's the book that has rock and politics in it. Recipe Selection: Do you mean 'Beurre Maître d</sample>
    <sample id="1222">The AltEntities Corpus has 6,000 alternative questions across three domains and it has 42, 000 indirect referring expressions. Results with T5 XL model are summarized below.</sample>
    <sample id="1223">If the language model has access to the exact same background knowledge as the annotators, the accuracy is really high, it's around 92 to 95 percent. But this is not realistic.</sample>
    <sample id="1224">If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87 percent, which is more realistic for example when the language model retrieves the background knowledge.</sample>
    <sample id="1225">If the language model has access only to entity names then the accuracy is only 60% so there's a lot of room for improvement we've also shown that the models are domain-generalizable here is a link to our dataset thanks</sample>
    <sample id="1226">CamemBERT is initially trained on 4GB of data. If you have any other questions about this, feel free to ask.</sample>
    <sample id="1227">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="1228">The findings were that performance degrades with larger temporal gap when retraining or continuing to pretrain models with more recent data. This confirmed the hypothesis that temporal drift is the main cause of performance drop. If you have any other questions about this, feel free to ask.</sample>
    <sample id="1229">Hi everyone I'm Jenny a first - year PhD student at Carnegie Mellon University and today I'll be presenting our work NLP positionality characterizing design biases of datasets and models.</sample>
    <sample id="1230">This work was done in collaboration with some folks at the University of Washington and the Allen Institute for AI namely Sebastin Santy Ronan Le Bras Katharina Reinecke and Maarten Sap.</sample>
    <sample id="1231">Imagine... So let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article, trying to remove toxic content.</sample>
    <sample id="1232">Imagine. You might turn towards a popular API like Perspective API for toxicity detection. And this works really well if you're Carl Jones. Perspective API is able to detect correctly toxic instances.</sample>
    <sample id="1233">Carl Jones is a Tech Lead at the New York Times.</sample>
    <sample id="1234">This is an example of a design bias where we see systematic performance differences of technology between populations.</sample>
    <sample id="1235">Positionality is simply the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1236">Positionality. The perspectives people hold as a result of their demographics, identity, and life experiences. This is a concept widely used in critical studies, specifically in feminist and queer academic spaces.</sample>
    <sample id="1237">Positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make.</sample>
    <sample id="1238">Yes, datasets and models can have positionality.</sample>
    <sample id="1239">We're not trying to say that models and datasets have demographic identities and life experiences, but they do aggregate judgments and opinions of real people and can thus represent certain positionalities over others.</sample>
    <sample id="1240">So prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and datasets as well as theoretical definitions of model positionality.</sample>
    <sample id="1241">However, these works really don't look at comparing end users with the datasets and models themselves.</sample>
    <sample id="1242">Anecdotal evidence: Model and dataset probing, Theoretical definitions of model positionality. Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. Yin et al. "GEOMLAMA: Geo - Diverse Commonsense Probing on Multilingual Pre - Trained Language Models." EMNLP 2022. Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." CHI 2022. And studying model and dataset positionality is increasingly important as NLP tasks become more subjective and socially oriented.</sample>
    <sample id="1243">It's challenging to characterize how these position alities are skewed because not all decisions are documented and many models are hidden behind APIs.</sample>
    <sample id="1244">To study dataset and model positionality, we actually compare the annotations with real users with existing datasets and models.</sample>
    <sample id="1245">We do this through our framework NLP positionality.</sample>
    <sample id="1246">Collection: 300 instances are sampled from a dataset. Each instance has an associated gold label. Processing: Instances are sent as part of a study on LITW. Annotations from people around the world are received. The received annotations are compared against the gold labels from the dataset and predictions obtained from the models. Analysis: Pearson's r correlation is measured between gold labels, model predictions and annotations for each of the demographics separately.</sample>
    <sample id="1247">The first step is to re - annotate datasets with diverse annotators.</sample>
    <sample id="1248">We opted to do this over looking at the demographics of original datasets and annotators because usually only a few annotators annotate each instance and because demographics are rarely collected and shared.</sample>
    <sample id="1249">So, we re annotate data to get many annotators for instances and to get a rich set of demographic data.</sample>
    <sample id="1250">We then take the annotations by demographic and compare them to the models and datasets using a Pearson's r correlation score.</sample>
    <sample id="1251">Our framework differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels, instead of just looking at annotator agreement or modeling annotator distributions.</sample>
    <sample id="1252">Our framework is largely enabled through Lab in the Wild, an online crowdsourcing platform for HCI collaborators.</sample>
    <sample id="1253">LabintheWild is an online experimentation platform where we can recruit diverse volunteers compared to platforms like MTurk which largely have participants from the US or India. And further LabintheWild still is able to get high quality data.</sample>
    <sample id="1254">Task A: Social Acceptability. We host two tasks on Lab in the Wild, one of them being social acceptability. And the way this works is that participants will read a situation from the Social Chemistry dataset and then they'll rate how socially acceptable a situation is.</sample>
    <sample id="1255">Task A: Social Acceptability.</sample>
    <sample id="1256">We then compared these annotations with Social Chemistry Delphi and GPT-4.</sample>
    <sample id="1257">Task B: Toxicity. Participants read an instance from the Dynahate dataset. Participants rate whether they think an instance is hate speech.</sample>
    <sample id="1258">We then compared these annotations with Dynahate, Perspective API, Rewire API, Hate RoBERTa, and GPT-4. Our study in the end amassed over 16, 000 annotations from over a thousand annotators from 87 countries.</sample>
    <sample id="1259">Finding 1: There is positionality in NLP.</sample>
    <sample id="1260">For example, we find that datasets and models are most aligned to English - speaking countries. So for the GPT - 4 social acceptability analysis, we find that it's most aligned to Confucian and English - speaking countries. We find that Dynahate is also most aligned to English - speaking countries.</sample>
    <sample id="1261">We also find most additional alignment with people who have a college education. So for GPT-4 in the social acceptability task we find that it's most aligned to people with a college education or graduate school education.</sample>
    <sample id="1262">Datasets and models are most aligned to people with a college education. And we find the same for Dynahate where it's most aligned to people with a college education.</sample>
    <sample id="1263">Finding 2: Some populations are left behind. However, when models and datasets are aligned to specific populations, some are inevitably left behind.</sample>
    <sample id="1264">Datasets and models are less aligned to non-binary people. An example of this is that datasets and models are less aligned to non-binary people compared to the men and women counterparts. We find this in the GPT-4 social acceptability task as well as the Dynahate task analysis as well.</sample>
    <sample id="1265">Well, one thing we can do is to use more diverse datasets. You know, datasets that represent different groups and perspectives. Another way is to carefully design our models so that they don't overemphasize certain positions. And also, we should constantly evaluate our models for any signs of positional bias.If you want to know more about this or have other thoughts, feel free to keep the conversation going.</sample>
    <sample id="1266">So we have a few recommendations for this. First one is keep a record of all relevant design choices throughout the research process. And the other is to do NLP research through the lens of perspectivism.</sample>
    <sample id="1267">The third recommendation is to build specialized datasets and models with and for specific communities. A good example of this is the Masakhane initiative. We want to emphasize that inclusive NLP isn't just making all technologies work for everyone.</sample>
    <sample id="1268">Thanks! Dashboard Link: nlpositionality.cs.washington.edu/ Paper: bit.ly/NLPositionality - Paper/ Delphi, https://delechi.allenai.org/ Age, Gender, Ethnicities, Religion, Education Level, Country, Residence, Country, Longest, Native Language, And so that concludes our presentation. But if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper. Thank you.</sample>
    <sample id="1269">Well, you see, after the first step, the tokens are all there but not in the right order. So, we need to permute them to get them into the correct sequence for the output. It's like putting together a puzzle where the pieces are all there but just not in the right place. If you have any more questions about this, feel free to ask!</sample>
    <sample id="1270">Well, the authors thought that increased transparency about bias mitigation methods was important because we don't really know if positive stereotypes are due to some weird over - excessive value alignment or other anti - stereotyping methods causing those harmful patterns. So, it's crucial to be more open about how these methods are being used. What do you think about this?</sample>
    <sample id="1271">In BLiMP, it's "Many people were helping herself." In SyntaxGym, it's "The customer ... has spent any money." In Crows, it's "Non - stereotypical sentence." If you want to know more about these or have other questions, feel free to ask.</sample>
    <sample id="1272">The authors used F1, CLS, NER, POS, Hamming, and EMR as evaluation metrics. If you want to know more about these metrics or have other questions, feel free to ask.</sample>
    <sample id="1273">Krippendorff's Alpha.</sample>
    <sample id="1274">Wikipedia.</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="1276">MultiInstruct has a large-scale, publicly - available multimodal instruction dataset. Other benchmarks may not have this. If you want to know more about the specific details, feel free to ask.</sample>
    <sample id="1277">Three.</sample>
    <sample id="1278">The definition of binary coordination isn't directly given in the text you've shared. It seems like the text is more about showing proportions of shorter left conjuncts depending on the absolute difference of conjunct lengths. If you want to know more about binary coordination, you might need to look at other sources or context. What else are you curious about?</sample>
    <sample id="1279">The image doesn't provide information about the average length of the prompts used in the study. So, I'm sorry, I can't answer that. But if you have any other questions about the study or the image, feel free to ask.</sample>
    <sample id="1280">The smaller T5 model, when fine - tuned on Coscript, can generate higher quality scripts than most large - language models. This shows that smaller models can outperform larger ones when trained on suitable data. So, it implies that for certain tasks, smaller models can be just as effective as larger ones. What do you think about this?</sample>
    <sample id="1309">The learning strategies investigated are from scratch with full model construction and continual pre-training using an existing pre-trained model. If you want to know more about these strategies, feel free to ask.</sample>
    <sample id="1310">The factor of overfitting due to test reuse is not specified in the given information.</sample>
    <sample id="1311">The quality of the simplification was evaluated using SARI, BLEU, BS-P, and FRE.</sample>
    <sample id="1312">Yes, they do.</sample>
    <sample id="1347">Cognitive dissonance is two beliefs or actions that are inconsistent. If you want to know more about it, like some examples or how it affects people, feel free to ask.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Yes, it does. If you have more questions about this, feel free to ask.</sample>
    <sample id="1350">Sara Papi.</sample>
    <sample id="1351">The data was taken from TED talks. If you have any other questions about this, feel free to ask.</sample>
    <sample id="1352">Hi, my name is Adam Przepiórkowski and this talk is about the dependency structure of coordination.</sample>
    <sample id="1353">As you may know that different dependency structures are assumed by different theories and corpus approaches. So for example in Universal Dependencies the structure of the coordinate coordination Lisa Bart and Maggie.</sample>
    <sample id="1354">Yes, that's correct. In the case of "Lisa, Bart, and Maggie, " Lisa is the head of the whole coordinate structure. If you have any other questions about this or anything else, feel free to ask.</sample>
    <sample id="1355">A similar approach is assumed in economic trucks meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are isometric right they they single out one of the conjuncts</sample>
    <sample id="1356">There are also symmetric approaches to coordinate structures such as the Prague approach the conjunction headed approach assumed in Prague dependency treebanks where coordinate structures are headed by the conjunction.</sample>
    <sample id="1357">So we get some dependencies from and to all the conjuncts.</sample>
    <sample id="1358">And finally there's also a multi - headed approach that's used for example in the cut - sense word grammar.</sample>
    <sample id="1359">So, all conjunctions are heads of the coordinate structure. So we get dependencies from the governor, here loves, to all conjunctions separately, Lisa, Bart, and Maggie.</sample>
    <sample id="1360">The aim of the paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these.</sample>
    <sample id="1361">The argument is based on the principle of dependency length minimization that I will explain on the basis of these examples.</sample>
    <sample id="1362">So in English, as you might know, direct objects prefer to be close to the verb, while adjuncts may be further away, right? So "Marge read it yesterday" is fine because the direct object "it" is close to the verb.</sample>
    <sample id="1363">While "Marge read yesterday it" is much worse right? Because here between the verb and the direct object there's an adjunct "yesterday".</sample>
    <sample id="1364">However, this effect may be ameliorated when the direct object is very heavy and very long, because then it can be moved to the position after the agent.</sample>
    <sample id="1365">Both these sentences are fine. Marge read this absolutely fascinating book about the bees yesterday. It's okay the way instead of it we have this long and pink.</sample>
    <sample id="1366">Marge read yesterday this absolutely fascinating book about bees.</sample>
    <sample id="1367">So the reasoning here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb.</sample>
    <sample id="1368">Word order tends to minimize dependency lengths.</sample>
    <sample id="1369">So these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures.</sample>
    <sample id="1370">So here we have a dependency from read to the adjunct of length 7 measured in words and from read to book of length 4 so together it's 11.</sample>
    <sample id="1371">When you swap these two constituents, the sum of these two dependencies becomes 6 right so instead of 11 6 much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one</sample>
    <sample id="1372">Okay, so what we did we extracted very statistics from about coordination from the enhanced version of the Penn Treebank and see the paper why wouldn't you saw university dependencies.</sample>
    <sample id="1373">Yes, the statistics confirm that left conjuncts tend to be shorter. For example, "salt and pepper" is shorter than "pepper and salt" measured in syllables.</sample>
    <sample id="1374">left conjuncts tend to be shorter, this tendency grows with length difference, but only when the governor is on the left or absent, not when it is on the right.</sample>
    <sample id="1375">So when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger, right? So the proportion is bigger of the left short conjunct.</sample>
    <sample id="1376">The novel part is that the tendency only occurs when the governor is on the left or absent.</sample>
    <sample id="1377">Yes, in the example "I saw Bart and Lisa", the governor is on the left.</sample>
    <sample id="1378">In the second example "Homer came and sneezed", there's coordination of two verbs and no outside external governor. So in such cases, the left conjunct prefers to be shorter, the more so the bigger the difference between the two conjuncts.</sample>
    <sample id="1379">left conjuncts tend to be shorter this tendency grows with length difference but only when the governor is on the left or absent not when it is on the right</sample>
    <sample id="1380">So we showed that by measuring length in characters the first column in syllables the middle column and in words the right column so I'll concentrate on the right one</sample>
    <sample id="1381">When the governor is on the left.</sample>
    <sample id="1382">The tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears.</sample>
    <sample id="1383">And we show in the paper how this provides an argument against asymmetric structures of coordination as these two and for the symmetric structures as these two.</sample>
    <sample id="1384">So see the paper for the full agreement and uh argument sorry and talk to us about in the poster session thank you</sample>
    <sample id="1385">Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is training on one source language and then transferring to another language.</sample>
    <sample id="1387">Dawei Zhu and Marius Mosbach are from Saarland University, Xiaoyu Shen is from Amazon Alexa, Andreas Stephan is from the University of Vienna, and Dietrich Klakow is also from Saarland University.</sample>
    <sample id="1388">The authors use average lagging and computational aware average lagging. If you want to know more about these measures, feel free to ask.</sample>
    <sample id="1416">Well, the drawbacks are that trees usually aren't given, they need to be obtained somehow. This can be complicated and sometimes computationally expensive. It often involves formalism - specific pre - processing of logical forms, like handling variable symbols. Also, obtaining trees may involve specialized grammar - induction procedures. So, that's the gist of it. If you want to know more about any of these points, feel free to ask.</sample>
    <sample id="1417">The authors, Shuheng Liu and Alan Ritter, are from the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="1495">ABC-Eval stands for Annotating Behaviors in Chat. If you have any other questions about it, feel free to ask.</sample>
    <sample id="1496">2016.</sample>
    <sample id="1527">The affiliations are The University of Edinburgh, Saarland University, and the University of Amsterdam.</sample>
    <sample id="1528">The name of the speaker is Siyu Yuan.</sample>
    <sample id="1529">There are five authors involved in the paper. If you want to know more about the paper or the authors, feel free to ask.</sample>
    <sample id="1530">The approach is compared with the state of the art architecture specifically tailored for SimulST.</sample>
    <sample id="1531">Hello everyone. My name is Ying and my colleague Zhiyang and I will be presenting our research on MultiINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning.</sample>
    <sample id="1532">With the advances in large language models, many works started to explore new learning paradigms of reusing pretrain language models for different downstream tasks in a parameter - and data - efficient way.</sample>
    <sample id="1533">Recently many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero - shot manner by following natural instructions.</sample>
    <sample id="1534">However, most previous works on instruction tuning focused on improving the zero - shot performance on language - only tasks, while computer vision and multi - modal tasks have been left out.</sample>
    <sample id="1535">Therefore, in this work we want to investigate whether instruction tuning on multimodal pre - trained models can actually improve generalization to n - seen multimodal tasks.</sample>
    <sample id="1536">Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between NLP and multimodal.</sample>
    <sample id="1537">There exists more than 1, 600 language - only instruction tasks. However, there is no large - scale publicly - available multimodal instruction task. Therefore, this motivates us to build a multimodal instruction tuning dataset.</sample>
    <sample id="1538">Here we present MultiINSTRUCT the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks covering 10 broad categories.</sample>
    <sample id="1539">The tasks are derived from 21 existing open source datasets and each task is equipped with 5 expert - written instructions.</sample>
    <sample id="1540">OFA, a unified multi-modal pre-trained model, can perform understanding and generation tasks with single or multiple modalities. It has a unified vocabulary for language, image tokens, and bounding box coordinates.</sample>
    <sample id="1541">Here we show some example instances from our multi instruct dataset.</sample>
    <sample id="1542">MULTINSTRUCT Grounded Caption Input: Generate a caption for &lt;bin 198&gt; &lt;bin 32&gt; &lt;bin 400&gt; &lt;bin 193&gt;. Output: blue and white tennis racquet Text Localization Input: Select the region that contains the text "den". Options: &lt;bin 206&gt; &lt;bin 119&gt; &lt;bin 181&gt; &lt;bin 357&gt; &lt;bin 518&gt; &lt;bin 456&gt; &lt;bin 574&gt; &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt; Output: &lt;bin 229&gt; &lt;bin 604&gt; &lt;bin 304&gt; &lt;bin 654&gt; Referring Expression Selection Input: Select the region of the object described by "A blue train in the front." Options: &lt;bin 242&gt; &lt;bin 180&gt; &lt;bin 736&gt; &lt;bin 475&gt; &lt;bin 88&gt; &lt;bin 291&gt; &lt;bin 203&gt; &lt;bin 473&gt; &lt;bin 193&gt;</sample>
    <sample id="1543">We followed the method of OFA and formulated all the tasks in a unified sequence - to - sequence format in which the input text images instructions and bounding boxes are represented in the same token space.</sample>
    <sample id="1544">Okay now I'm gonna talk about multi - modal instruction tuning.</sample>
    <sample id="1545">For the training dataset, we use 53 tasks from 9 groups for training and sample 10, 000 instances per task. For testing, we reserve the entire Commonsense Reasoning group for testing and select additional 5 tasks from VQA and Miscellaneous groups.</sample>
    <sample id="1546">We use all the instances in the test split for each task. In addition we randomly sample 20 tasks from the test split of Natural Instructions as unseen tasks for NLP.</sample>
    <sample id="1547">So we use a pre - trained OFA - Large model as a base model during training. We mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates.</sample>
    <sample id="1548">For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="1549">We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">For multi-modal classification tasks we report the Accuracy. For multi-modal generation tasks we report the Rouge-L. For NLP tasks we report Rouge-L.</sample>
    <sample id="1551">Sensitivity. How sensitive the model is towards to variety of instructions for the same task:. Ability to consistently produce the same results for the same task, regardless of slight variations in the wording of instructions.</sample>
    <sample id="1552">Here is our main result as we can see instruction tuning can significantly improve OFA's performance on multimodal tasks.</sample>
    <sample id="1553">Transfer learning from natural instruction datasets can benefit instruction tuning.</sample>
    <sample id="1554">As the amount of task increases, the model achieves better performance and in the meantime, lower sensitivity.</sample>
    <sample id="1555">So we also did one experiment. We use one instruction versus five instruction as we can see using more instruction can improve the models overall performance and reduce its sensitivity a lot.</sample>
    <sample id="1556">So this shows the effect of different fine - tuning strategies on the model sensitivity. As we can see, by transfer learning from natural instruction dataset, the model can achieve much better sensitivity comparing to the original OFA model.</sample>
    <sample id="1557">We also can see transfer learning from Natural Instruction dataset can help OFA to achieve much better performance on the Natural Instruction dataset.</sample>
    <sample id="1558">Conclusion.First large - scale multi - modal instruction tuning dataset.Contains 62 multi - modal tasks from 10 broad categories.Significantly improve the zero - shot capability of OFA via instruction tuning.Explore several transferring learning techniques and show their benefits.Design a new metric sensitivity.So overall we are proposed the first large - scale multi - modal instruction tuning dataset with significantly improve the zero - shot capability of OFA and we explore different transfer learning technique and show their benefits we design a new metric called sensitivity.</sample>
    <sample id="1559">One More Thing! We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision - language tasks and we will release them soon! This is a QR code for our data and model. Thank you.</sample>
  </task>
</testset>