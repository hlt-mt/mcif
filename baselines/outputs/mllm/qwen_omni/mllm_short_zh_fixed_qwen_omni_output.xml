<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型主要数据来源是大规模的网络爬虫数据。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1">这篇论文的作者所属机构有McGill University，Mila和Microsoft Research。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="2">嗨，欢迎来到我们关于DEPLAIN的介绍，这是一个用于德语文本简化的新语料库，适用于文档级和句子级。</sample>
    <sample id="3">我的名字是Regina Stodden，我将引导大家进行演讲的第一部分。首先，让我们定义一下文本简化。</sample>
    <sample id="4">文本简化是一种适应文本的过程，以改善特定目标群体对文本的理解，例如为阅读困难的人或非母语者。</sample>
    <sample id="5">为了训练文本简化模型，我们需要平行的文本对，例如文档对或句子对。</sample>
    <sample id="6">这里有一个平行对齐的句子对，包含一个复杂的德语句子及其简化翻译。</sample>
    <sample id="7">简化句子时，可以采用不同的技术，如词汇替换，从句删除，从句插入，重排序或插入单词。</sample>
    <sample id="8">我们现在提出我们的新语料库DE-plain，因为在近年来，现有的语料库存在一些问题。例如，这些语料库太小，无法用于训练文本简化模型。</sample>
    <sample id="9">近年来提出的其他三个模型都是自动对齐的，这意味着它们的对齐可能会有错误。</sample>
    <sample id="10">因此，我们提出了新的语料库DEplain，它被分为两个子语料库：DEplain-APA和DEplain-web。DEplain-APA基于新闻文本。</sample>
    <sample id="11">在DEplain-APA中，我们手动对483份文档进行了对齐，这产生了大约34500个平行的句子对。</sample>
    <sample id="12">德语文本简化语料库包括不同的领域，我们还手动和自动对这756份文档进行了对齐。</sample>
    <sample id="13">总共我们得到了30，450个句子对。</sample>
    <sample id="14">我们对我们的句子对进行了更详细的分析，例如在简化类型方面。</sample>
    <sample id="15">如图所示，圣经文本的简化程度远高于新闻文本或其他语言学习文本。</sample>
    <sample id="16">例如，词汇简化，结构简化，还有整体简化水平。</sample>
    <sample id="17">此外，你可以看到我们的DEplain语料库具有多种不同的简化变换。例如，在DEplain-apa语料库中，我们有更多的重排序和单词添加，而在DEplain-web语料库中则较少。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有更多的改述。</sample>
    <sample id="19">好的，现在让我们看看我们能用这个语料库做什么。你好，我是奥马尔，现在我将深入讨论我们数据集的用例。对于第一个用例，我们可以评估自动对齐方法。</sample>
    <sample id="20">近年来，有很多对齐方法，但在机器翻译的背景下。</sample>
    <sample id="21">当我们有两篇用不同语言写的平行文档，想要从中提取两篇文档中的句子对齐时。</sample>
    <sample id="22">但是，在我们的使用案例中，我们尝试在具有相同语言和相同内容的两篇平行文档之间提取句子对齐。但是它们的复杂性水平不同。</sample>
    <sample id="23">现在，我们有了带有手动对齐句子的数据集，我们可以使用这些句子作为黄金标准对齐来评估一些提出的对齐方法。</sample>
    <sample id="24">我们对所提出的算法进行了适应性调整，并在论文中发表了这些适应性调整和运行实验的代码。</sample>
    <sample id="25">最后，我们得出结论，对于德语文本简化任务，最好的自动对齐方法是MASSalign方法。</sample>
    <sample id="26">你也可以在论文中找到运行此方法的代码，用于自己的文档。</sample>
    <sample id="27">我们在论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">通过微调语言模型，从复杂输入文本生成简化文本。</sample>
    <sample id="29">我们对两种不同的模型进行了微调。我们对长模型的模型进行了微调，以生成文档级别的简化。</sample>
    <sample id="30">我们还对正常基础的长模型进行了微调，以生成句子级别的简化。</sample>
    <sample id="31">你还可以找到所有的检查点，你可以查看论文中实验的分数和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这种基本的微调可以产生或得到比基线分数更好的分数。</sample>
    <sample id="33">我们提出这些结果作为未来自动文本简化问题的基准，一个基线基准。</sample>
    <sample id="34">谢谢大家的关注，希望在会议上能见到大家。谢谢。</sample>
    <sample id="35">演讲者的名字是Kayo Yin。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="36">他们使用T5 XL模型获得82%-87%的准确率。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="37">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="38">这个方法新颖之处在于它通过明确标注每个模型响应是否表达某些行为，来减少人工评估的主观性。嗯，你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="40">嗯…你可以多听一些歌，然后读一些关于这些歌的资料。这样应该能提高分数。如果还有其他问题，随时跟我说哈。</sample>
    <sample id="41">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="42">嗨，我的名字是Adam Przepiórkowski，这个演讲是关于协调的依赖结构。</sample>
    <sample id="43">嗯，不同理论和语料库方法假设不同的依存结构。例如，在通用依存结构中，"Lisa，Bart和Maggie"的协调结构是这样的。</sample>
    <sample id="44">在这种情况下，第一个并列成分是整个并列结构的主语，所以是“Lisa”。</sample>
    <sample id="45">在意义文本理论中，也假设了类似的处理方式，其中整个协调结构由第一个并列连词主导。所以这两种处理方式是对称的，它们突出一个并列连词。</sample>
    <sample id="46">还有对协调结构的对称方法，比如Prague方法，Conjunction-headed方法，这些方法在Prague依赖树库中被采用，其中协调结构由连词主导。</sample>
    <sample id="47">所以，我们从“and”得到所有并列成分的依赖关系。</sample>
    <sample id="48">最后，还有一个多头方法，例如在卡特森的世界语法中使用。</sample>
    <sample id="49">嗯，所以所有的并列成分都是主语结构的主语，这样我们就能从主语“爱”那里得到对所有并列成分的依赖关系，像丽莎，巴特和玛吉。如果还有其他问题，你可以随时问我哦。</sample>
    <sample id="50">论文的目标是为像这两个对称结构的协调提出一个新颖的论点，并反对像这两个不对称结构的协调。</sample>
    <sample id="51">这个论点是基于依赖长度最小化原则的，将在这些例子的基础上进行解释。</sample>
    <sample id="52">嗯，你可能知道，在英语里，直接宾语倾向于靠近动词，而副词可以离动词远一些。所以“Marge read it yesterday”是好的，因为直接宾语“it”靠近动词。</sample>
    <sample id="53">“Marge read yesterday it” 这个句子更糟糕，因为介词“yesterday”在动词和直接宾语之间。</sample>
    <sample id="54">然而，当直接宾语非常重且很长时，这种效果可能会得到改善，因为在这种情况下，它可以在介词之后的位置。</sample>
    <sample id="55">这里说明了这一点。这两个句子都很好。Marge读了一本关于蜜蜂的非常有趣的书，昨天。这样，我们用“this”代替“it”，句子就简洁明了了。</sample>
    <sample id="56">但是也可以说“Marge read yesterday this absolutely fascinating book about bees.”</sample>
    <sample id="57">这里的原因是，尽管这个句子违反了直接宾语应该紧挨着动词的语法规则。</sample>
    <sample id="58">它遵循依赖长度最小化原则，该原则表明较短的依赖关系是首选的。</sample>
    <sample id="59">这两个树图只显示关键依赖关系的长度，也就是在两个结构中不恒定的那些。</sample>
    <sample id="60">所以这里我们有一个从“read”到“absolutely fascinating book about bees”的依赖关系，长度为7个单词，从“read”到“book”的长度为4个单词，所以总共是11个单词。</sample>
    <sample id="61">当你交换这两个成分时，这两个依赖关系的总和变为6，对吧？所以从11变成6，更短了，这就是为什么听起来还不错的原因。它违反了一个原则，但满足了另一个原则。</sample>
    <sample id="62">好的，我们从增强版的Penn Treebank中提取了关于协调的统计数据，看看论文。你为什么不使用大学的依赖关系呢？</sample>
    <sample id="63">从增强版的宾夕法尼亚树库中提取的关于协调的统计数据，来自Marcus等人，1993年，Ficler和Goldberg，2016年：。左连词倾向于较短，之前观察到。这种趋势随着长度差异而增长，之前在Gibson等人，1996年，88-90页，中简要注意到，但仅当主语在左边或缺失时，我看到Bart和Lisa；Homer来了，打了个喷嚏，当它在右边时不是，Ted和Ned笑了。</sample>
    <sample id="64">从增强版的宾夕法尼亚树库中提取的关于协调的统计数据，来自Marcus等人，1993年，Ficler和Goldberg，2016年：。左连词倾向于较短，以前观察到。这种倾向随着长度差异而增长，Gibson等人，1996年，88-90页，曾简要注意到。但仅当主语在左边或不存在时，I saw Bart and Lisa; Homer came and sneezed，否则当它在右边时，Ted and Ned laughed，</sample>
    <sample id="65">当两个并列成分的长度差异增大时，较短的并列成分更倾向于成为第一个并列成分，比例更大。</sample>
    <sample id="66">这篇论文的创新之处在于我们观察到这种倾向只发生在主语在左边或缺失的情况下。</sample>
    <sample id="67">好的，你问的是“我看到巴特和丽莎，所以主语在左边。”的中文翻译。嗯…“我看到巴特和丽莎”可以是“I saw Bart and Lisa”，“所以主语在左边”可以是“so the governor is on the left”。如果还有其他翻译相关的问题，或者别的事，都可以跟我说哦。</sample>
    <sample id="68">在“Homer came and sneezed”这个例子中，有两个动词被协调，没有外部的连词。在这种情况下，左边的连词倾向于更短，而且两个连词之间的差异越大，左边的连词就越短。</sample>
    <sample id="69">然而，当主语在右边时，比如“Ted和Ned笑了”，这种效应就消失了。</sample>
    <sample id="70">所以我们展示了通过测量长度，字符在第一列，音节在中间列，单词在右列，那么我将集中于右列。</sample>
    <sample id="71">我们在这里看到的是，当监管者在左边时。</sample>
    <sample id="72">左连词较短的倾向随着连词长度绝对差值的增加而稳步增长，即使没有连词支配者，如在句子协调中，但当连词在右侧时，这种倾向消失。</sample>
    <sample id="73">我们在论文中展示了，这为反对不对称的协调结构提供了论据，因为这两个是对称结构。</sample>
    <sample id="74">所以，看看论文以了解完整的论点和论证。抱歉，还有在海报展期间与我们交谈。谢谢。</sample>
    <sample id="75">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="76">圣经文本的简化程度更大。</sample>
    <sample id="77">“salt and pepper”和“not the pen sold”。</sample>
    <sample id="78">可以，DrBERT模型，NACHOS数据集和训练脚本都是免费的，你可以用于你的研究。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="79">DEplain-apa是基于新闻文本的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="80">更好的模型架构，更大的模型规模，更多的微调示例。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="81">通过测量长度来衡量，比如字符数，音节数或者单词数。</sample>
    <sample id="82">嗯…这个嘛，你可以通过测量长度，像字符，音节，单词这些，然后看支配词在左边还是右边，再观察左连词的长度变化。这样就能研究支配词位置对左连词长度的影响了。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="83">基线分类器在不平衡数据上训练效果不太好，表现得不如随机猜测。</sample>
    <sample id="84">这篇论文有四位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="85">示例对话中的角色名字是Bob和Alice。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="86">在形式性和词汇连贯性上，语境感知MT模型比语境无关模型更有优势。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="87">Johns Hopkins University，Purdue University，MIT，Meta AI。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="122">嗯，这个框架是通过计算Pearson r相关系数来量化立场的。你要是还有啥疑问，尽管问哈。</sample>
    <sample id="155">研究发现，当给人类受试者相同的人格化提示时，他们也能浮现出种族刻板印象。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="156">此研究使用了增强版的Penn Treebank作为数据来源。</sample>
    <sample id="157">两位。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="158">与认知失调密切相关的任务是辩论和CE任务。嗯，你要是还有啥疑问可以接着问哈。</sample>
    <sample id="159">两位。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="160">这篇论文有六位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="161">这个框架与以前的研究不同在于，它通过比较最终用户，模型和数据集的预测和标签，而不是仅仅看内注释者的一致性或者建模注释者分布。</sample>
    <sample id="162">在三个比较设置中，GPT-3.5与刻板词汇的重叠最多。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="163">比较了DeepL和Google。</sample>
    <sample id="164">嗨，我是来自华盛顿大学的博士生尚斌。今天我正在展示我们关于从预训练数据到语言模型再到下游任务，追踪政治偏见的轨迹，导致不公平的NLP模型的工作。</sample>
    <sample id="165">所以语言模型是基于大规模网络爬取数据进行训练的。</sample>
    <sample id="166">根据对C4语料库的调查，政治新闻媒体在预训练数据中得到了很好的覆盖。我们可以看到，《纽约时报》《洛杉矶时报》《卫报》《赫芬顿邮报》等在语言模型训练数据中得到了很好的覆盖。</sample>
    <sample id="167">这给语言模型应用带来了喜忧参半的结果。</sample>
    <sample id="168">一方面，他们能够从多元视角学习，这庆祝了民主和思想的多样性。另一方面，这些不同的政治观点本质上是社会偏见的，并且可能会在下游任务应用中导致潜在的公平性问题。</sample>
    <sample id="169">为此，我们建议研究从预训练数据到语言模型再到下游任务的政治偏见传播管道，具体通过提出以下问题。</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向，以及预训练数据在政治偏见方面可能发挥什么作用？</sample>
    <sample id="171">其次，具有不同政治倾向的语言模型在下游任务中的表现如何，以及这是否会导致NLP应用中的公平性问题。</sample>
    <sample id="172">所以具体来说，我们首先使用政治问卷，如政治指南针测试，向语言模型提出不同格式的提示。这确保了我们的自动评估在政治科学文献中有坚实的依据。</sample>
    <sample id="173">一些初步结果表明，首先，语言模型确实有不同的政治倾向，它们占据了政治坐标图的四个象限。</sample>
    <sample id="174">我们还可以看到，GPT-4 是所有语言模型中最自由的，GPT 系列通常比 BERT 系列及其变体在社会自由方面更自由。</sample>
    <sample id="175">其次，我们旨在研究语言模型的政治偏见在多大程度上是从训练数据中吸收的。</sample>
    <sample id="176">因此，我们可以通过进一步预训练语言模型检查点来进行受控实验，使用六个不同的党派语料库，这些语料库被分为新闻和社交媒体，进一步按其政治倾向进行划分。</sample>
    <sample id="177">通过在这样的党派语料上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也相应地发生了变化。</sample>
    <sample id="178">例如，对于RoBERTa，在左倾Reddit语料上进一步微调和训练后，我们可以看到其在政治倾向上向自由派的显著转变。</sample>
    <sample id="179">在政治偏见方面。</sample>
    <sample id="180">我们还试图研究语言模型是否能捕捉到现代社会中普遍存在的两极分化。</sample>
    <sample id="181">我们将预训练语料库分为前45任美国总统时期和后45任美国总统时期，分别在两个不同的时间语料库上预训练语言模型。</sample>
    <sample id="182">我们可以看到，语言模型在2017年后，政治倾向通常更远离中心。这表明语言模型也能反映出社会的两极分化。</sample>
    <sample id="183">最后，我们用不同政治倾向的语言模型来评估仇恨言论检测和假新闻检测，这是涉及语言模型的两个NLP应用，可能具有非常重要的影响。</sample>
    <sample id="184">所以，如果我们研究每个类别的性能，也就是说，如果我们把性能分开来看。</sample>
    <sample id="185">根据不同的媒体来源和政治倾向，我们可以看到一个模式，例如，在仇恨言论检测方面，左翼语言模型表现更好。</sample>
    <sample id="186">在检测针对社会少数群体的仇恨言论方面。</sample>
    <sample id="187">然而，我们的工作在检测针对社会中更强大的群体的仇恨言论方面表现更差。</sample>
    <sample id="188">右翼语言模型在检测针对白人和男性的仇恨言论方面表现更好，但在检测针对黑人，LGBTQ+和其他少数群体的仇恨言论方面表现更差。</sample>
    <sample id="189">左翼语言模型在检测来自相反政治倾向的虚假信息方面表现更好，反之亦然。</sample>
    <sample id="190">这个表格展示了使用具有不同政治倾向的语言模型进行下游任务的性能示例。CHRIS，Base，N，S，L，R分别代表基督徒， vanilla RoBERTa模型，新闻媒体，社交媒体，左派和右派，分别。</sample>
    <sample id="191">根据他们的社会类别，语言模型对仇恨言论和错误信息示例给出了不同的预测。附录中有更多例子，以进一步强调这一点。</sample>
    <sample id="192">这表明语言模型的政治偏见是一个非常紧迫的公平性问题。</sample>
    <sample id="193">嗯…这有点复杂呢。如果一个语言模型被微调用于仇恨言论或错误信息，然后部署到一个流行的社交媒体平台上，那可能会引发很多问题。首先，它可能会传播仇恨言论和错误信息，影响用户的认知和情绪。其次，可能会导致用户之间的冲突和误解。再者，可能会对社会的和谐稳定造成威胁。不过，这也取决于这个语言模型的训练数据和算法设计。你要是还有其他想法或者问题，咱们可以继续聊聊呀。</sample>
    <sample id="194">这可能意味着持有相反政治观点的人可能会被边缘化，针对少数群体的仇恨言论可能会不受控制地泛滥。</sample>
    <sample id="195">所以，这提醒我们要承认并解决由语言模型政治倾向导致的公平性问题。</sample>
    <sample id="196">所以有一点讨论。我们也想强调，我们揭示了关于语言模型政治偏见的独特困境，就像在斯卡利和查尔迪斯之间。</sample>
    <sample id="197">如果我们在语言模型训练数据中不进行政治观点的净化，偏见就会从预训练数据传播到语言模型，再到下游任务，最终造成公平性问题。</sample>
    <sample id="198">如果我们尝试以某种方式净化数据，我们也会面临审查或排斥的风险，而且很难确定什么是真正中立的，应该保留在语言模型训练数据中，所以这有点像电车难题。</sample>
    <sample id="199">好的，我觉得这就是我今天要做的全部了。谢谢大家的时间。</sample>
    <sample id="200">这篇论文有六位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="201">MPP评估最多涵盖900个词元的上下文长度。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="202">他们的数据集中包含音乐，书籍和食谱这三个领域。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="203">Positionality是人们由于其人口统计学，身份和生活经历而持有的观点。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="204">演讲者的名字是Dawei Zhu。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="205">是的，EDAtt 使用了现有的离线 ST 模型，没有重新训练或采用特定的 SimulST 架构。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="206">这篇论文有四位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="207">被测模型在测试套件上运行得不太好。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="208">KITMUS有三个变体，分别是Background-Pretrain，Background-Both和Background-Inference。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="209">Google Research。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="210">最后一个研究问题是“是否应该仅使用干净样本进行验证，还是有其他更好的方式来利用它们？”</sample>
    <sample id="211">嗯…这个指标灵敏度是用来衡量模型对同一任务不同指令的敏感程度。它能评估模型在指令有轻微变化时，是否能一致地产生相同结果。具体来说，就是通过计算在不同指令下模型输出的差异，来反映模型的这种能力。如果还有疑问，你可以再问我哦。</sample>
    <sample id="212">演讲者的名字是Jingwei Yi。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="214">嗯…这个信息里没提到预训练期间模型接收什么样的语言上下文呢。你可以再找找其他资料或者咱们再聊聊这个话题。</sample>
    <sample id="215">通常需要20个干净的验证样本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="216">Stanford Engineering Computer Science。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="217">嗯…这个嘛，因为现有的方法有局限性，不能准确衡量媒体偏见。所以需要开发新的方法。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="218">演讲者的名字是Akshatha。</sample>
    <sample id="219">嗯…政治偏见传播流程是从预训练数据到语言模型，再到下游任务。你要是还有啥想知道的，可以再问我哈。</sample>
    <sample id="220">是的，DEplain-apa 和网站的简化过程有所不同。DEplain-apa 更多的是重排序和单词添加，而网站的简化过程则更多是改述。你要是还有啥疑问，尽管跟我说哈。</sample>
    <sample id="221">嗯…这个我不太清楚呢。你可以去查查相关的资料或者问问其他知道的人。要是你还有其他问题，也可以再跟我说呀。</sample>
    <sample id="222">首先定义目标嵌入，然后计算句子中的触发数，最后将目标嵌入添加到原始嵌入上。</sample>
    <sample id="223">这篇论文的作者所属机构是PennState和Amazon。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="224">可以。</sample>
    <sample id="225">示例是做草莓蛋糕和巧克力蛋糕。做草莓蛋糕要加草莓酱到面粉里，做巧克力蛋糕要加可可粉到面粉里。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="226">他们通过在四个数据集上可视化句子的嵌入来验证其隐蔽性。</sample>
    <sample id="227">研究通过使用现有的预训练模型来进行持续预训练，来构建新的预训练模型。嗯，你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="228">拉丁美洲。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="229">演讲者在“I am a student.”这个示例句子上展示了模型如何利用注意力机制所学的知识。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="230">任务数量增加，模型性能变好，同时敏感性降低。如果还有疑问，欢迎继续问我。</sample>
    <sample id="231">LSTM seq2seq，T5，Zheng and Lapata。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="232">两位合著者是第一作者的导师。</sample>
    <sample id="233">PaLM的第一作者是Chowdery等人。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="234">嗨，大家好，我是来自卡内基梅隆大学的一年级博士生珍妮，今天我将为大家介绍你们的工作，NLPPositionality：数据集和模型设计偏见的表征。</sample>
    <sample id="235">这项工作与华盛顿大学和艾伦人工智能研究所的一些人合作完成，包括塞巴斯蒂安·桑蒂，罗南·勒·布拉斯，卡塔琳娜·雷内克和马尔滕·萨普。</sample>
    <sample id="236">想象一下…所以，让我们从想象你为一家报纸工作开始，你在浏览新闻文章下的评论，试图删除有毒内容。</sample>
    <sample id="237">想象一下…你可能会转向一个流行的API，比如Perspective API，用于检测毒性，如果你是Carl Jones，这个API工作得非常好，能正确检测出有毒的实例。</sample>
    <sample id="238">但对阿迪蒂亚·夏尔马来说，情况并非如此，因为针对印度语境中更常见的冒犯性术语，Perspective API 并不那么敏感。</sample>
    <sample id="239">这是一个设计偏见的例子，我们看到技术在不同人群之间存在系统性的性能差异。</sample>
    <sample id="240">像我们之前看到的那种设计偏见，可能是因为NLP研究人员和模型开发者的立场造成的。立场就是人们由于其人口统计学，身份和生活经历而持有的观点。</sample>
    <sample id="241">定位性，人们由于其人口统计，身份和生活经历而持有的观点。</sample>
    <sample id="242">定位性，人们由于其人口统计，身份和生活经历而持有的观点。作为研究人员，它会影响研究过程及其结果和结果。</sample>
    <sample id="243">一个人们可能会问的问题是：数据集和模型是否有立场性？</sample>
    <sample id="244">我们不是在说模型和数据集本身有人口统计学身份和生活经历，但它们确实汇总了真实人们的判断和意见，因此可以代表某些立场而不是其他立场。</sample>
    <sample id="245">嗯，你问的是关于数据集和模型是否有立场性的问题。初步研究提供了一些证据，比如文化差异在模型和数据集中的体现，还有模型立场性的理论定义。</sample>
    <sample id="246">然而，这些研究并没有真正比较最终用户与数据集和模型本身。</sample>
    <sample id="247">研究模型和数据集的立场性变得越来越重要，因为NLP任务变得更加主观和面向社会。</sample>
    <sample id="248">很难描述这些偏见是如何扭曲的，因为并非所有决策都记录在案，而且许多模型隐藏在API背后。</sample>
    <sample id="249">为了研究数据集和模型的位置性，我们实际上比较了注释与真实用户的注释，以及现有的数据集和模型。</sample>
    <sample id="250">我们通过我们的框架NLPPositionality来做这件事。</sample>
    <sample id="251">我们的框架分为两个主要步骤。</sample>
    <sample id="252">第一步是用多样化的注释者重新注释数据集。</sample>
    <sample id="253">我们选择这样做，而不是查看原始数据集标注者的统计数据，因为通常每个实例只有少数标注者标注，并且很少收集和分享统计数据。</sample>
    <sample id="254">因此，我们重新注释数据，以获得每个实例的许多注释，并获得丰富的统计数据。</sample>
    <sample id="255">然后我们对各个群体的注释进行比较，并使用皮尔逊相关系数将它们与模型和数据集进行比较。</sample>
    <sample id="256">我们的框架实际上与注释者分歧文献不同，它通过比较最终用户与模型和数据集的预测和标签，而不是仅仅看注释者之间的内部分歧或建模注释者分布。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild，一个在线众包平台，来实现，该平台由前HCI合作者创建。</sample>
    <sample id="258">LabintheWild是一个在线实验平台，我们可以招募多样化的志愿者，相比像MTurk这样的平台，MTurk的参与者大多来自美国或印度，LabintheWild仍然能够获得高质量的数据。</sample>
    <sample id="259">我们在Lab in the Wild上举办两个任务，其中一个就是社会接受度。这个任务的工作方式是，参与者会阅读来自Social Chemistry数据集的情况，然后他们会评定这个情况的社会接受度。</sample>
    <sample id="260">为了保持参与者的兴趣，他们可以将自己的回答与AI和其他人的回答进行比较。</sample>
    <sample id="261">然后我们将这些注释与社会化学，德尔菲和GPT-4进行了比较。</sample>
    <sample id="262">然后，我们复制了类似的设置用于毒性与仇恨言论检测任务，参与者会阅读来自Dynahate的一个实例，并对其是否认为是仇恨言论进行评分。</sample>
    <sample id="263">然后我们将这些注释与Dynahate，Perspective API，Rewire API，Hate RoBERTa和GPT-4进行比较。我们的研究最终收集了来自87个国家的1000多名注释者的超过16000个注释。</sample>
    <sample id="264">现在我们准备好回答NLP数据集和模型与谁最对齐了。我们发现NLP中有立场性。</sample>
    <sample id="265">例如，我们发现数据集和模型最与英语国家对齐。对于GPT-4的社会接受度分析，我们发现它最与儒家和英语国家对齐。我们还发现Dynahate也最与英语国家对齐。</sample>
    <sample id="266">我们还发现，GPT-4在社会接受度任务中与拥有大学教育或研究生教育的人最契合。</sample>
    <sample id="267">我们还发现，对于Dynahate，它也最与拥有大学教育的人群相匹配。</sample>
    <sample id="268">然而，当模型和数据集与特定人群对齐时，不可避免地会有一些人群被落下。</sample>
    <sample id="269">一个例子是，数据集和模型对非二元性别人群的契合度低于男性和女性的对应人群。我们在GPT-4的社会接受度任务以及Dynahate任务分析中发现了这一点。</sample>
    <sample id="270">所以，我们能做些什么呢？在NLP中处理立场性问题。</sample>
    <sample id="271">所以我们有几个建议。第一个建议是，在整个研究过程中，记录所有相关的设计选择。第二个建议是从透视主义的角度进行NLP研究：a. 共享分层的标签数据集！b. 使用能够处理标注者分歧的建模技术。</sample>
    <sample id="272">第三条建议是构建针对特定社区的专门数据集和模型，例如Masakhane倡议。我们想强调包容性NLP不仅仅是让所有技术对所有人有效。</sample>
    <sample id="273">谢谢！
仪表板链接：nlpositionality.cs.washington.edu/
论文：bit.ly/NLPositionality-Paper/
Delphi
https://delechi.allenai.org/
年龄
性别
种族
宗教
教育水平
居住国
居住国（最长）
母语
所以这就是我们报告的全部内容。如果你想了解更多，欢迎随时查看我们的仪表板，获取最新的分析结果和论文。谢谢！</sample>
    <sample id="274">演讲者提到了三个SimulST的问题。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="275">嗯…这个嘛，如果在训练NLP模型时想减轻数据集中的社会和政治偏见，一种方法是预训练数据的时候就尽量避免这些偏见。不过呢，这也有点难，因为很难确定什么是真正中立的，应该保留在语言模型训练数据里的。你要是还有啥想法或者疑问，咱们可以再聊聊。</sample>
    <sample id="276">嗨，我是复旦大学的思雨。我来介绍一下我们的工作：从大型语言模型中提取脚本知识用于约束语言规划。</sample>
    <sample id="277">在日常生活中，人们常常通过遵循剧本形式的分步指令来规划他们的行动。</sample>
    <sample id="278">以前的工作已经利用语言模型来为制作蛋糕等刻板活动的抽象目标制定计划，并表明大型语言模型可以有效地将目标分解为步骤。</sample>
    <sample id="279">然而，以往的工作主要集中在为典型活动的抽象目标进行规划，而为具有具体目标和具体约束的目标，如制作巧克力蛋糕，进行规划的研究仍然不足。</sample>
    <sample id="280">在本文中，我们定义了约束语言规划的问题。</sample>
    <sample id="281">抽象目标可以被不同的现实具体目标所继承，这些具体目标有着多方面的约束。一个好的规划者应该编写合理且忠实于约束的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。</sample>
    <sample id="283">由于没有特定目标的数据集来支持我们的研究。</sample>
    <sample id="284">我们首先需要获取这些目标，并且如表所示，我们通过多维度约束来扩展抽象目标，以便于人类在环数据获取中使用。</sample>
    <sample id="285">我们随机抽取100个具体目标，并评估大型语言模型生成的脚本。</sample>
    <sample id="286">这张表报告了结果的整体准确性。我们发现，所有大型语言模型在为特定目标进行规划时都取得了不令人满意的结果。</sample>
    <sample id="287">然后我们进行详细分析，以调查为什么LLM模型会失败。</sample>
    <sample id="288">图中的结果表明，生成脚本的语义完整性是可以接受的，但对约束的忠实性不能保证。</sample>
    <sample id="289">我们深入研究了维基百科中更细粒度的约束主题类别。图中的热图显示，InstructGPT在不同类别的目标上的规划表现差异很大。</sample>
    <sample id="290">以前的研究表明，大型语言模型的输出质量存在高方差，导致性能不佳。因此，我们采用了“先生成后过滤”的理念来提高生成质量。</sample>
    <sample id="291">首先，通过展示约束类型和例子来引导InstructGPT，并基于给定的抽象目标获得具体目标。</sample>
    <sample id="292">然后，使用InstructGPT为特定目标过度生成候选脚本。</sample>
    <sample id="293">下一步，开发了一个过滤模型来选择合适的剧本。</sample>
    <sample id="294">我们将脚本和目标转换为InstructGPT嵌入，并计算余弦相似度和相似度分数，以衡量语义相似性。</sample>
    <sample id="295">我们将过滤掉包含目标约束关键词的脚本。我们只保留目标得分在脚本集中最高的脚本。</sample>
    <sample id="296">使用我们的方法，InstructGPT可以生成更高质量的脚本。我们的方法极大地提高了规划能力，包括语义完整性和对约束的忠实性。</sample>
    <sample id="297">由于大型语言模型部署成本高，因此有必要使小型和专业化模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。</sample>
    <sample id="298">然而，之前的研究没有为特定目标提供规划能力，而且手动标注数据集很昂贵。</sample>
    <sample id="299">我们遵循符号知识蒸馏的理念，从轻量级模型中蒸馏出约束语言规划数据集。</sample>
    <sample id="300">我们应用我们的方法来构建一个约束语言规划的数据集，命名为Coscript。</sample>
    <sample id="301">总共，我们生成了55000个具体目标和脚本，以确保验证和测试集的质量。我们请云众包工人查找并修订不正确的样本。</sample>
    <sample id="302">这个图展示了Coscript的约束分布。我们发现Coscript在生成的具体目标中表现出高度的多样性。使用Coscript，我们可以训练更小但更专业的模型，用于约束语言规划。</sample>
    <sample id="303">我们发现，T5在Coscript上微调后，生成的脚本质量高于大多数大型语言模型，这表明在合适的语料库上微调的小型模型可以超越大型模型。</sample>
    <sample id="304">总结：我们建立了约束语言规划问题。我们评估了大型语言模型的约束语言规划能力，并开发了一种过生成再过滤方法。</sample>
    <sample id="305">我们使用大型语言模型生成高质量的脚本数据集，CoScript，用于约束语言规划。我们希望CoScript数据集能成为推进语言规划研究的宝贵资源。</sample>
    <sample id="306">谢谢您的时间。请在我们的论文中查找更多关于CoScript的细节。</sample>
    <sample id="307">PaLM的流畅度和SOTA系统相当。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="308">水印方法的重要属性有：适用于嵌入式服务，不降低提供的嵌入式数据的实用性，对攻击者来说足够隐蔽，水印能在攻击者服务中转移。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="309">德语，西班牙语，法语，意大利语，日语，韩语，荷兰语，葡萄牙语，罗马尼亚语，俄语，土耳其语，中文。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="310">从一个数据集中抽取300个实例用于重新注释。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="311">cosine和l2距离度量用于衡量良性和后门数据集之间的差异。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="312">嗯…这个嘛，就是把多语言预训练的编码器和基于指针的解码器结合起来。像XLM-R和mBERT这样的编码器，再加上PTR这种解码器。你要是还有啥疑问，可以再问我哦。</sample>
    <sample id="344">作者先在一般文本语料库上计算单词频率，然后随机选择n个单词在中等频率区间。</sample>
    <sample id="345">大家好，我的名字是Shuheng。今天我将要展示我们的论文《CoNLL-2003命名实体标记器在2023年还能用吗？》。让我们开始吧。</sample>
    <sample id="346">我们的论文研究了使用命名实体识别任务，NER任务，来解决泛化问题。</sample>
    <sample id="347">我们观察到，模型已经使用CoNLL-2003开发NER近20年了，这自然提出了几个问题。首先，这些模型能否泛化到现代数据？</sample>
    <sample id="348">在开发新标签时，需要什么才能实现良好的泛化？</sample>
    <sample id="349">同时，如果我们确实观察到模型的泛化能力差，那么是什么导致这些模型的性能下降呢？</sample>
    <sample id="350">为了调查这些问题，我们开发了CoNLL++数据集。这是从2020年的路透社新闻中收集的数据集，并按照CoNLL-2003注释指南进行了注释。</sample>
    <sample id="351">然后我们在CoNLL-2003上微调了超过20个模型。我们分别在CoNLL-2003测试集和CoNLL++测试集上评估了它们。</sample>
    <sample id="352">最后但同样重要的是，我们计算了F1分数的变化百分比，以评估每个模型的泛化能力。</sample>
    <sample id="353">在所有实验中，我们发现有三个主要成分是必要的。</sample>
    <sample id="354">第一个是模型架构。通过我们的实验，我们发现Transformer模型通常在新数据上泛化得更好。</sample>
    <sample id="355">第二个要素是模型大小。我们发现通常较大的模型能带来更好的泛化能力。</sample>
    <sample id="356">最后但同样重要的是，我们都知道微调样本的数量直接影响下游任务的性能。在这里，我们还发现更多的微调样本实际上也导致更好的泛化能力。</sample>
    <sample id="357">那么下一个问题，是什么导致了一些模型的性能下降呢？</sample>
    <sample id="358">我们有两个假设。第一个是自适应过拟合，这是由于反复使用同一个测试集造成的过拟合，通常表现为在新测试集上的回报递减。</sample>
    <sample id="359">第二个假设是时间漂移，这是由于训练数据和测试数据之间的时间差距增加而导致的性能下降。</sample>
    <sample id="360">从右边的图中可以看到，红色的最佳拟合线的斜率大于1。</sample>
    <sample id="361">这意味着我们在CoNLL-2003上每一点的改进都转化为CoNLL++上超过一点的改进，这意味着没有边际效益递减。</sample>
    <sample id="362">这表明在这种情况下没有观察到自适应过拟合。</sample>
    <sample id="363">那么关于时间漂移呢？</sample>
    <sample id="364">对于时间漂移，我们做了一个实验，用更近的数据重新训练或继续预训练一些模型，发现性能随着时间间隔增大而下降。</sample>
    <sample id="365">这证实了我们的假设，即性能下降的主要原因是时间漂移。</sample>
    <sample id="366">我们的结论是，为了实现良好的泛化能力，我们需要更好的模型架构，更大的模型规模，以及更多的微调示例，这些因素是相辅相成的，不能只依靠其中一个。</sample>
    <sample id="367">同时，我们还发现，性能下降是由时间漂移造成的，令人惊讶的是，它不是由自适应过拟合造成的，尽管CoNLL-2003已经使用了20多年。</sample>
    <sample id="368">回到我们论文标题中提出的问题：2003年的CoNLL标注器在2023年还能用吗？我们发现答案是肯定的。</sample>
    <sample id="369">结论
- 要实现良好的泛化，我们需要：
  - 更好的模型架构
  - 更大的模型规模
  - 更多的微调示例
- 性能下降的原因是：
  - 时间漂移
  - 不是自适应过拟合
- CoNLL-2003 标签器还能用吗？
  - 是的！
我们希望我们的论文能促使更多关于如何改进模型泛化的研究。</sample>
    <sample id="370">最后，请务必查看我们的论文，数据集，如果您有任何问题，请随时联系我。非常感谢。</sample>
    <sample id="397">抱歉，图片内容和语音片段大小没有关系，图片上只显示了一个问题“我们的解决方案是什么？”没有提到语音片段大小的信息。</sample>
    <sample id="398">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是法官。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="399">示例质量比相似度更重要。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="400">论文侧重于RoBERTa和GPT-2这两个语言模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="401">嗯…这个模型是结合多个层的分数的。如果还有其他问题，你可以再问我哦。</sample>
    <sample id="402">“easy on me”，“the first one”。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="403">复旦大学和Brain Technologies Inc.</sample>
    <sample id="404">这篇论文有7位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="405">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="406">作者给出的“显性群体”的示例是“a woman warrior”。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="407">嗯…这个嘛，从图片上看，像BiLSTM-CNN，BiLSTM-CRF这些模型架构泛化能力可能比较差。不过这只是根据图片推测的哦。你要是还有其他想法或者疑问，咱们可以再聊聊。</sample>
    <sample id="408">你没说清楚是哪个英文内容啊，我没法回答这个问题。你可以再详细说说吗？</sample>
    <sample id="409">这篇论文有六位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="410">作者采用了多种模态。</sample>
    <sample id="439">嗯…这个嘛，作者没具体说哪些是NLU中研究不足的领域呢。你还有其他关于NLU的问题吗？</sample>
    <sample id="440">演讲者的名字是Ying。</sample>
    <sample id="441">是的，经过了质量检查。</sample>
    <sample id="442">现有的资源只支持有限类型的依赖上下文的翻译和有限的语言。</sample>
    <sample id="443">嗨，我将谈谈我们关于解决间接指代表达式以进行实体选择的工作，其中我们介绍了AltEntities语料库。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛尼，这是与菲利普·拉德林斯基，西尔维亚·帕雷蒂和安妮·路易斯的联合工作。</sample>
    <sample id="445">我们的目标是理解用户在做选择时的语言。考虑这个替代问题：你是说《Easy on Me》还是《I Gotta Feeling》？这里用户想在两首歌中选择一首。</sample>
    <sample id="446">最明显的方法是使用直接引用，例如说出歌曲的名字“easy on me”或者它的位置“the first one”。</sample>
    <sample id="447">间接指代表达式。目标：理解用户在做选择时的语言。替代问题：你是指《Easy on Me》还是《I Gotta Feeling》？直接指代：《Easy on Me》，《the first one》。间接指代可以在自然流畅的对话中使用：记不住名字，发音难区分，想明确偏好。间接指代：《The newer one》，《The song that's not energetic》。有时候，间接指代表达更合适，让对话更自然。这可能发生在用户记不住歌名的时候。</sample>
    <sample id="448">发音太相似，难以区分。</sample>
    <sample id="449">或者当用户想要明确偏好时。这里有一些例子，比如“更新的那一个”或“那首不那么有活力的歌”。</sample>
    <sample id="450">这是对话系统中的一个重要问题，也是对大型语言模型实体理解进行基准测试的问题。</sample>
    <sample id="451">我们没有现成的大规模公共数据集，所以使用众包来收集一个。我们的数据集涵盖了三个不同的领域：音乐，书籍和餐饮。</sample>
    <sample id="452">我们的数据集收集方法强调非正式性，使用卡通完成任务。</sample>
    <sample id="453">卡通中有三个对话气泡。第一个气泡里，鲍勃说：“还记得我们昨天听的那首歌吗？”鲍勃通过这句话设置了对话背景。</sample>
    <sample id="454">在第二个对话框中，爱丽丝说：“你是说《Easy on Me》还是《I Gotta Feeling》？”</sample>
    <sample id="455">在第三个对话框中，鲍勃使用间接指代来选择其中一个实体，例如纽约。</sample>
    <sample id="456">我们自动提供第一和第二句气泡，但第三句由注释者填写。第一句气泡是从每个领域几个手动提示中选择的。</sample>
    <sample id="457">第二个部分，即替代问题，是这样生成的。</sample>
    <sample id="458">我们总是使用一个简单的模板：“你是指A还是B？”其中A和B是从Wikipedia中采样的。</sample>
    <sample id="459">这里列出了我们使用的不同采样方法。当我们向上移动列表时，实体变得越来越相似，通常更难进行消歧义。</sample>
    <sample id="460">第一个是随机均匀分布。</sample>
    <sample id="461">第二类是实体有相似标题的情况，例如两本书同名《The Return》。</sample>
    <sample id="462">第三种情况是当它们在Wikipedia上有相似的描述。最后，当它们在Wikipedia上有相似的Infobox或属性时，例如相同的类型或相同的艺术家。</sample>
    <sample id="463">当向注释者展示这个问题时，他们知道这些实体的名称，但并不一定知道关于这些实体的信息。</sample>
    <sample id="464">所以我们要做的是展示一些关于这两个实体的背景知识。对于歌曲，我们只是展示每个歌曲的谷歌搜索链接。</sample>
    <sample id="465">然后让注释者至少听一些每首歌，读一读每首歌。例如，这是谷歌搜索结果中关于歌曲《Easy on Me》的。</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了来自维基百科的背景文本。对于食谱，我们还展示了它们的图片，也来自维基百科，这样标注者就能知道它们的样子。</sample>
    <sample id="467">然后我们告诉注释者应该选择哪个选项，并让他们描述它。选择这个。Easy on Me，Adele，I Gotta Feeling，Black Eyed Peas，我们希望你能为所选歌曲提供3到5个表达，填入你的对话框。例如：带有钢琴音乐的那个，不那么有活力的歌，和河流有关的，较新的那个，关于没有时间选择的。然后我们让注释者选择其中一个实体，例如这里第一个，并用3到5个间接指代表达来描述它们。</sample>
    <sample id="468">随机示例音乐选择：你是指“Chime”还是“Your Loving Arms”？→没有歌词的那个。你是指“These Kids”还是“Inescapable”？→这是澳大利亚人唱的歌。你是指“Rock the Boat”还是“Wherever You Are”？→里面有合成器的声音。你是指“Telepathy”还是“Stars on 45”？→2000年中期发行的。你是指“Mis-Shapes”还是“Remind Me”？→根据谢菲尔德的生活经历。书籍选择：你是指“Warlock，Hall小说，”还是“Warlock，Smith小说，”？→1880年代背景的那个。你是指“The Legion of Space”还是“The Body in the Library”？→一位著名侦探作家的作品。你是指“The Good Soldier”还是“The Good Soldiers”？→虚构的那个。你是指“The Giaour”还是“The Giver”？→不是那个12岁男孩的。你是指“Broken Sleep”还是“Broken Soup”？→里面有摇滚和政治的内容。食谱选择：你是指“Beurre Maître d’Hôtel”还是“Chigirtma”？→来自阿塞拜疆。你是指</sample>
    <sample id="469">AltEntities语料库包含约6000个跨三个领域的替代问题，约42000个间接指代表达。使用T5 XL模型的结果如下：如果LM有与注释者相同的背景知识，准确率在92%-95%之间。如果LM有部分重叠的背景知识，准确率在82%-87%之间。如果LM仅能访问实体名称，准确率约为60%。我们展示了模型的跨域泛化能力。数据集链接：https://github.com/google-research-datasets/AltEntities。</sample>
    <sample id="470">如果语言模型有与注释者完全相同的背景知识，准确率会非常高，大约在92%到95%之间。但这并不现实。</sample>
    <sample id="471">如果语言模型有部分重叠的背景知识，那么准确率在82%到87%之间，这更符合现实情况，例如当语言模型检索到背景知识时。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以还有很大的提升空间。我们也展示了模型在不同领域的一般性。这是数据集的链接，谢谢。</sample>
    <sample id="473">该方法与 wait-k 策略，LA 策略，CAAT 策略和 EDAtt 这些现有的 SimulST 策略进行了比较。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="474">LIA，Avignon Université，LS2N，Nantes Université，Clinique des données，CHU de Nantes，Zenidoc。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="475">演讲者的名字是Jenny。</sample>
    <sample id="476">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="477">嗨，我是来自特伦托大学和布鲁诺·凯斯勒基金会的萨拉·帕皮，我将简要介绍“注意力作为同声传译的指南”论文，这是与马特奥·内格里和马可·图尔奇的联合工作。</sample>
    <sample id="478">同步语音翻译，SimulST，是将口语实时翻译成另一种语言文本的过程，实现跨语言交流。</sample>
    <sample id="479">当前的SimulST模型存在哪些问题？特定架构通常被训练，引入额外模块进行优化。</sample>
    <sample id="480">特定架构通常被训练，引入额外模块进行优化。长期而复杂的训练程序，例如涉及不同的优化目标。</sample>
    <sample id="481">当前SimulST模型的问题是什么？特定架构通常被训练，引入额外模块进行优化。长期而复杂的训练过程，例如不同的优化目标。训练和维护多个模型以达到不同的延迟制度，例如1秒，2秒等。</sample>
    <sample id="482">我们的解决方案是什么？</sample>
    <sample id="483">首先，使用现有的离线ST模型，无需重新训练或采用特定架构用于SimulST。其次，每种延迟制度使用一个模型，并通过特定参数处理延迟。</sample>
    <sample id="484">利用模型已经通过音频输入和文本输出之间的注意力机制所获得的知识，这就是跨注意力机制，右边有一个例子。</sample>
    <sample id="485">我们的解决方案是提出EDAtt，或编码器-解码器注意力，这是一种策略，我们根据注意力指向的位置来决定是否发出部分翻译。</sample>
    <sample id="486">如果注意力没有集中在最后λ个语音帧上，即其总和低于阈值α，那么就会发出一个词，这意味着接收到的信息足够稳定。</sample>
    <sample id="487">如果收到包含“I am going to talk about…”的语音片段，我们的模型预测德语翻译为“Ich werde reden.”</sample>
    <sample id="488">根据注意力指向的位置决定是否发出或部分翻译：如果注意力没有集中在最后λ个语音帧上，其总和低于阈值α，那么就发出一个词，这意味着接收到的信息足够稳定。</sample>
    <sample id="489">我们看到，前两个单词指向最早收到的语音帧，而最后一个单词指向最后收到的语音帧，即λ帧。</sample>
    <sample id="490">这意味着前两个单词将被省略。</sample>
    <sample id="491">当注意力的总和超过某个阈值α时，我们将不会发出最后一个单词，而是等待另一个语音片段。</sample>
    <sample id="492">如果继续，我们收到另一个语音片段，我们的模型预测其他三个单词，我们将看看这些交叉注意力权重。</sample>
    <sample id="493">我们将看到没有单词指向最后的λ个语音帧。</sample>
    <sample id="494">这意味着这三个词将被发射。</sample>
    <sample id="495">如果看看EDAtt的主要结果。</sample>
    <sample id="496">我们将同时演讲的翻译结果绘制在图表上，其中BLEU在一边衡量翻译质量，平均延迟在另一边。</sample>
    <sample id="497">这是延迟度量，我们还考虑了计算感知平均损失，它会考虑到模型预测输出的计算时间。</sample>
    <sample id="498">所以，我们希望我们的曲线在图上尽可能高。</sample>
    <sample id="499">但是我们也希望它们向左平移。</sample>
    <sample id="500">我们还与一些应用于离线模型的流行策略进行了比较，包括等待策略和局部协议，同时也与专为同步语音翻译设计的最新架构进行了比较。</sample>
    <sample id="501">这些是同时性语音翻译策略在德语上的所有结果。</sample>
    <sample id="502">我们看到EDAtt在所有应用于离线模型的策略中表现最好，因为它们的曲线向左偏移。</sample>
    <sample id="503">而且，如果我们考虑实际的运行时间或计算时间，EDAtt是最快的策略。</sample>
    <sample id="504">如果你想知道更多结果，阅读我们的论文，我们还发布了开源代码和模型，以及同时输出，以促进我们工作的可重复性。谢谢你的关注。</sample>
    <sample id="505">是的，数据集是公开的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="506">大家好，我的名字是Ying，我和我的同事Zhiyang将要展示我们关于MULTIINSTRUCT的研究，通过指令调优提高多模态零样本学习。</sample>
    <sample id="507">随着大型语言模型的进展，许多工作开始探索以参数和数据高效的方式重用预训练语言模型进行不同下游任务的新学习范式。</sample>
    <sample id="508">最近，许多研究表明，通过遵循自然指令，指令微调使大型语言模型能够在零样本情况下执行未见过的任务。</sample>
    <sample id="509">然而，大多数关于指令微调的先前工作都集中在提高零样本性能的语言任务上，而计算机视觉和多模态任务却被忽略了。</sample>
    <sample id="510">因此，在这项工作中，我们想研究指令微调多模态预训练模型是否真的能提高对异模态任务的泛化能力。</sample>
    <sample id="511">此外，在我们的研究中，我们发现NLP和多模态之间的指令数据集可用性存在相当大的差距。</sample>
    <sample id="512">存在超过1600个纯语言指令任务，然而没有大规模的公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="513">这里我们介绍MultiINSTRUCT，第一个多模态指令调优基准数据集，包含62个多样化的多模态任务，涵盖10个广泛类别。</sample>
    <sample id="514">这些任务来源于21个现有的开源数据集，每个任务都配有5个专家撰写的指令。</sample>
    <sample id="515">在我们的数据集上进行多模态指令微调时，我们以OFA，一个统一的多模态预训练模型，作为基础模型。OFA使用统一的词汇表，包括语言，图像标记和边界框的坐标。</sample>
    <sample id="516">这里我们展示了我们多指令数据集的一些示例实例。</sample>
    <sample id="517">为了统一处理各种输入和输出数据类型。</sample>
    <sample id="518">我们遵循OFA的方法，将所有任务统一为序列到序列格式，在此格式中，输入文本，图像，指令和边界框都在相同的标记空间中表示。</sample>
    <sample id="519">好的，现在我要谈谈多模态指令调优。</sample>
    <sample id="520">训练数据集构建：使用来自9个组的53个任务进行训练。每个任务采样10，000个实例。测试数据集构建：保留整个常识推理组用于测试。从VQA和杂项组中选择额外的5个任务。我们使用每个任务测试分割中的所有实例。从Natural Instructions数据集的测试分割中随机采样20个任务作为NLP的未见任务。</sample>
    <sample id="521">训练数据集构建：使用来自9个组的53个任务进行训练。每个任务采样10，000个实例。测试数据集构建：保留整个常识推理组用于测试。从VQA和杂项组中选择额外的5个任务。我们使用每个任务测试分割中的所有实例。从自然指令数据集的测试分割中随机采样20个任务作为NLP的未见任务。</sample>
    <sample id="522">在训练中，我们使用预训练的OFA-Large模型作为基础模型。我们为所有任务混合所有实例。每个实例随机与它的五个指令模板之一结合。</sample>
    <sample id="523">在测试阶段，对于每个任务，我们总共进行五次实验，每次实验使用其中一个指令来评估模型。</sample>
    <sample id="524">我们报告所有五次实验的平均性能，最大性能和性能的标准差。</sample>
    <sample id="525">对于多模态分类任务，如视觉蕴含，视觉空间推理，自然语言视觉推理和灾害类型分类，我们报告准确率。对于多模态生成任务，如常识VQA，文本VQA，接地VQA，视觉文本提取和视觉对话，我们报告Rouge-L。对于NLP任务，我们报告Rouge-L。我们还根据模型在所有未见过的多模态和NLP任务上的平均表现计算每个模型的综合性能。对于大多数任务，我们使用Rouge-L作为性能得分，而对于只有准确率作为指标的任务，则使用准确率。</sample>
    <sample id="526">我们还引入了一个名为“敏感度”的额外评估指标，它衡量模型在任务相同的情况下，对指令措辞微小变化的鲁棒性，即在指令措辞有轻微变化时，模型能够一致地产生相同的结果。</sample>
    <sample id="527">这是我们主要的结果，如图所示，指令调优可以显著提高OFA在多模态任务上的性能。</sample>
    <sample id="528">从自然指令数据集的迁移学习也能使指令微调受益。</sample>
    <sample id="529">随着任务数量的增加，模型实现了更好的性能，同时降低了敏感性。</sample>
    <sample id="530">所以我们也做了这样的实验，我们使用了一条指令和五条指令，正如我们所见，使用更多的指令可以提高模型的整体性能，并且大幅降低其敏感性。</sample>
    <sample id="531">所以这显示了不同微调策略对模型敏感度的影响。正如我们所见，通过从自然指令数据集进行迁移学习，模型可以比原始OFA模型实现更好的敏感度。</sample>
    <sample id="532">我们也可以看到，从自然指令数据集进行迁移学习可以帮助OFA在自然指令数据集上取得更好的性能。</sample>
    <sample id="533">总之，我们提出了第一个大规模多模态指令调优数据集，显著提高了OFA的零样本能力，并探索了不同的迁移学习技术，展示了它们的好处。我们设计了一个新的度量标准，称为敏感性。</sample>
    <sample id="534">还有一件事！我们正在收集一个更大的多模态指令调优数据集，包含大约150个额外的视觉语言任务，我们很快就会发布它们！这是我们的数据和模型的二维码。谢谢！</sample>
    <sample id="535">这篇论文的作者所属机构是University of Trento和Fondazione Bruno Kessler。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="536">演讲者的名字是Mohammad Javad Hosseini。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="562">大家好，我是Koustuv Sinha，很荣幸欢迎你们参加我们ACL 2023论文的演讲《语言模型的可接受性判断并不总是对上下文具有鲁棒性》。</sample>
    <sample id="563">语言模型的可接受性判断并不总是对上下文具有鲁棒性。</sample>
    <sample id="564">在本工作中，我们重新审视了最小对 paradigm。</sample>
    <sample id="565">最小对偶范式，MPP，评估语言模型使用序列概率的相对差异来评估语言模型的抽象知识：。BLiMP。1. 许多人在帮助自己。2. *许多人帮助她自己。P，1， &gt; P，2，SyntaxGym。1. 没有顾客...花过钱。2. *顾客...花过钱。P，1.any， &gt; P，2.any，CrowS。1. 刻板印象句子。2. 非刻板印象句子。P，1， &gt; P，2，最小对偶范式基本上评估语言模型基于可接受性判断，这也可以包括语法性，如BLiMP语法Gym，或刻板印象的可接受性，如CrowS对。</sample>
    <sample id="566">在最小对偶范式中，评估语言模型的典型方法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子。</sample>
    <sample id="567">最小对 paradigm 评估语言模型使用序列概率的相对差异来评估语言模型的抽象知识：。BLiMP。1. 许多人在帮助自己。2. *许多人帮助她自己。P，1， &gt; P，2，SyntaxGym。1. 没有顾客…花过钱。2. *顾客…花过钱。P，1.any， &gt; P，2.any，Crows。1. 刻板印象句子。2. 非刻板印象句子。P，1， &gt; P，2，希望模型基本上会将更多概率赋予可接受的句子。</sample>
    <sample id="568">当前的最小对 paradigm，基本上不允许我们评估模型对较长句子的接受度。</sample>
    <sample id="569">这些天来，大型语言模型正在使用越来越长的上下文窗口，因此评估模型在整个上下文窗口中的可接受性至关重要。</sample>
    <sample id="570">我们正在尝试重新审视MPP管道，通过让模型评估更长序列的可接受性。</sample>
    <sample id="571">所以这就是我们的方法。我们做的是模拟这些较长的序列。我们重新审视数据集本身，然后通过选择可接受或不可接受的句子来重新创建句子。</sample>
    <sample id="572">所以，例如，我们从BLIMP数据集中选择了典型的语义对，来自从助动词岛案例的语义对。</sample>
    <sample id="573">我们所做的就是创建更长的序列，这些序列是可接受的，并且具有相同的语法结构匹配。我们从Adjunct Island中提取语法句子。</sample>
    <sample id="574">然后我们将其添加到可接受查询和不可接受查询的前缀中。</sample>
    <sample id="575">所以，我们可以通过选择相同匹配的不可接受句子来做同样的事情，这也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们也可以通过选择来自不同子集或不同数据集的句子来做同样的事情，这就是所谓的“不匹配样本”。</sample>
    <sample id="577">所以这里，句子仍然来自相关数据集，但不是你用来评估的那个数据集。我们也可以对不可接受性的情况做同样的事情。</sample>
    <sample id="578">最后，我们可以从完全不相关的领域，比如维基百科，选择句子。</sample>
    <sample id="579">嗯，这个会告诉我们，像模型的可接受性判断是否真的受任何上下文的影响。</sample>
    <sample id="580">测试MPP判断是否随上下文长度，结构匹配和可接受性变化。</sample>
    <sample id="581">首先，我们看维基百科的句子，它们与当前查询对完全无关，然后我们发现MPP判断在任意上下文长度下大多很稳健。</sample>
    <sample id="582">我们对不同长度的上下文进行MPP评估，包括可接受/不可接受的上下文，匹配/不匹配的结构，长度可达900个词。我们将上下文长度增加到2024个词，以最大化OPT和GPT-2模型的性能，如图中橙色虚线所示，MPP判断相对稳定。</sample>
    <sample id="583">当我们在同一个数据集里选择句子时，会发生什么？</sample>
    <sample id="584">所以这里我们从同一个BLiMP语法生成数据集中，选择或创建来自可接受和不可接受领域的句子。</sample>
    <sample id="585">我们进行MPP评估，使用不同的上下文—可接受/不可接受；匹配/不匹配结构—长度可达900个词。我们看到，当添加可接受或不可接受的前缀时，MPP判断会显著增加或减少。</sample>
    <sample id="586">但是当我们匹配结构，也就是从同一个现象的责备人文本中选择句子时，</sample>
    <sample id="587">我们看到，根据所选前缀是否可接受，MPP 判断值会有显著增加或显著减少。</sample>
    <sample id="588">嗯，这个效果在整个上下文长度中都在增加，这可能会对那些有大上下文窗口的新一代语言模型产生影响。</sample>
    <sample id="589">我们以保持相关结构的方式扰动上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀副词：“然而，&lt;sent&gt;。”- 长前缀副词：“首先和最重要的是，&lt;sent&gt;。”- 添加从句：“不管X怎么想，&lt;sent&gt;。”- 引用：“昨天，X说，‘&lt;sent&gt;’。”</sample>
    <sample id="590">我们以保持相关结构的方式对上下文句子进行扰动，并询问模型是否对这些句子同样敏感。- 前缀/后缀副词：“然而，&lt;sent&gt;。”- 长前缀副词：“首先和最重要的是，&lt;sent&gt;。”- 添加从句：“不管X怎么想，&lt;sent&gt;。”- 引用：“昨天，X说，‘&lt;sent&gt;’。”</sample>
    <sample id="591">我们发现这些噪声实际上并没有让模型在如何做出NPP判断方面改变其方向。</sample>
    <sample id="592">为什么匹配前缀会影响语言模型的判断？我们以保持相关结构的方式对上下文句子进行扰动，并询问模型是否对这些句子同样敏感。- 前缀/后缀副词：“然而，&lt;sent&gt;。”- 长前缀副词：“首先和最重要的是，&lt;sent&gt;。”- 添加从句：“不管X怎么想，&lt;sent&gt;。”- 引用：“昨天，X说，‘&lt;sent&gt;’。”模型对扰动句子的敏感性相似。</sample>
    <sample id="593">当我们对可接受域的句子进行扰动时，所有扰动都显示出类似的增加。当我们对不可接受域的句子进行扰动时，MPP判断以类似的方式减少。</sample>
    <sample id="594">我们工作的关键收获是，语言模型对跨句子的潜在语法和语义特征敏感。</sample>
    <sample id="595">目前我们用短的单句输入的方式进行MPP评估，可能无法完全捕捉到语言模型在整个上下文窗口中的抽象知识。</sample>
    <sample id="596">关键收获
- 语言模型对句子间共享的潜在语法/语义特征敏感。
- 使用短的单句输入的MPP评估无法充分捕捉LMs的抽象知识。
请阅读我们的论文以获取更多实验细节。谢谢聆听。</sample>
    <sample id="597">该方法的第一步将输入词元映射到一个无序的多集类型的词元。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="598">Coscript 中包含了55，000个脚本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="626">DEplain的最佳对齐方法是MASSalign。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="627">弱监督学习能缓解标注瓶颈。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="628">DEplain-web中，文档采用手动对齐方法的分配情况是：DEplain-APA有387个样本，DEplain-web有481个样本，DEplain-APA+web有868个样本，src2src-baseline有12.848个样本。如果还有疑问，可以再问我哦。</sample>
    <sample id="629">CoNLL++ 数据集是从 2020 年的路透社新闻中收集的，并按照 CoNLL-2003 注释指南进行注释。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="630">大家好，我是来自宾夕法尼亚州立大学的于森张。今天我将介绍我们的工作，即跨语言语义解析在多种自然语言和意义表示中的应用。</sample>
    <sample id="631">语义解析是构建用户查询语义表示的任务，例如SQL和Lambda演算。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言的查询翻译成多种语义表示的任务。</sample>
    <sample id="633">如图所示，我们需要使用神经模型将多种自然语言的查询翻译成SQL，Lambda或FunQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型是分别提出的，并在有限任务和应用的数据集上进行评估。例如：</sample>
    <sample id="635">现有的CLSP模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏对某些自然语言的覆盖。</sample>
    <sample id="636">现有的CLSP模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏某些语义表示的覆盖。</sample>
    <sample id="637">现有的跨语言语义解析模型是分别提出的，并在任务和应用有限的数据集上进行评估。例如：缺乏对某些语义表示的覆盖。</sample>
    <sample id="638">现有的CLSP模型是分别提出的，并在有限任务和应用的数据集上进行评估。例如：缺乏对某些神经模型的覆盖。</sample>
    <sample id="639">为此，我们提出了一个统一的跨语言语义解析数据集示例器，用于多种自然语言和语义表示的跨语言语义解析。</sample>
    <sample id="640">它包含9个不同领域的数据集，5个语义解析任务，8种语义表示，以及15个语系中的22种自然语言。</sample>
    <sample id="641">实验设置。我们考虑了六种训练和评估的设置。</sample>
    <sample id="642">实验设置。我们考虑了六种训练和评估的设置。Translate-Test：使用Google翻译API将源语言翻译为目标语言。然后使用单语模型进行训练和评估。</sample>
    <sample id="643">嗯，这个嘛。在训练阶段，我们用英语查询来训练英语模型。在推理阶段，我们用API将德语查询翻译成英语，然后用训练好的模型来预测SQL。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="644">实验设置。我们考虑了六种训练和评估的设置。单语模型：源语言与目标语言相同，例如德语到德语。我们还测试了单语少量样本设置，通过仅使用10%的训练数据来训练单语模型。训练。推理。</sample>
    <sample id="645">在这个设置中，源语言与目标语言相同，例如德语到德语或英语到英语。</sample>
    <sample id="646">我们还测试了单语少量样本设置，通过只用10%的训练数据来训练单语模型。</sample>
    <sample id="647">实验设置。我们考虑了六种训练和评估的设置。多语言模型：为所有语言训练一个统一的多语言模型。</sample>
    <sample id="648">例如，我们将德语，英语和中文查询放在一起训练一个多语言模型，在推理过程中，我们可以使用这个模型。</sample>
    <sample id="649">实验设置。我们考虑了六种训练和评估的设置。多语言模型：为所有语言训练一个统一的多语言模型。训练。推理。要翻译德语查询或中文查询等。</sample>
    <sample id="650">我们在训练和评估中考虑了六种设置。跨语言零样本/少量样本迁移：在一种源语言上进行训练，然后转移到另一种语言。</sample>
    <sample id="651">在训练过程中，我们用英语查询或者英语和德语的组合少样本查询来训练一个多语言模型，以预测SQL输出。</sample>
    <sample id="652">关于单语模型的分析，我们在两组模型上进行评估。</sample>
    <sample id="653">Enc-PTR，多语言预训练编码器与指针式解码器，如XLM-R + PTR，mBERT + PTR。</sample>
    <sample id="654">我们也在多语言预训练编码器-解码器模型上进行了评估，比如mBART和mT5。</sample>
    <sample id="655">我们发现Enc-Dec，mT5，在所有9个数据集上都取得了最佳性能！</sample>
    <sample id="656">多语言训练分析。我们在mT5和XLM-R + PTR上评估多语言环境。编码-解码/编码-指针（mT5/XLM-R）可以通过在多种语言的混合中进行训练得到改进。</sample>
    <sample id="657">我们发现，编码器-解码器或编码器-PTTR可以通过在多种语言的混合中进行训练来得到改进。</sample>
    <sample id="658">我们发现，大多数主要自然语言可以获得性能提升，除了英语在7个数据集上性能下降，在3个数据集上性能提升。</sample>
    <sample id="659">我们在多语言环境中评估了mT5和XLM-R + PTR。大多数主要的NLs都能获得性能提升，除了英语在7个数据集上的性能下降，在3个数据集上有所提升。这被称为“多语言的诅咒”。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">在这张图中，蓝色线条表示跨语言少样本迁移，橙色线条表示跨语言零样本迁移，绿色线条表示单语言设置。</sample>
    <sample id="662">我们发现，在零样本设置下，跨语言迁移性能差距显著。在少量样本设置下，差距迅速缩小。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现，例如，编码器-解码器优于先前的工作或达到可比的结果。在英语自然语言上的预训练可以显著提高目标自然语言上的少样本性能。</sample>
    <sample id="664">我们发现，多语言语言模型，如Codex和BLOOM，在跨语言语义解析任务中仍然不够。</sample>
    <sample id="665">我们构建了XSemPLR，一个跨语言语义解析的统一基准，包含多种自然语言和意义表示。</sample>
    <sample id="666">我们构建了XSemPLR，一个跨语言语义解析的统一基准，支持多种自然语言和意义表示。我们对三种代表性的多语言模型类型进行了全面的基准研究。我们的结果显示，使用单语训练的mT5在性能上表现最佳，而值得注意的是，多语言LLMs在执行跨语言语义解析任务方面仍然不够。此外，单语训练和跨语言迁移学习之间的性能差距仍然显著。我们对三种代表性的多语言模型类型进行了全面的基准研究，我们的结果显示，使用单语训练的mT5在性能上表现最佳，而值得注意的是，多语言LLMs在执行跨语言语义解析任务方面仍然不够。此外，单语训练和跨语言迁移学习之间的性能差距仍然显著。欢迎访问我们的论文和代码！</sample>
    <sample id="667">有参数化水印，字典水印，后门水印和对抗性水印。嗯，你要是还想知道更多关于这些研究的事儿，可以再问我哦。</sample>
    <sample id="668">不足够。</sample>
    <sample id="695">该方法通过在训练中诱导排列来处理排列的不确定性。</sample>
    <sample id="696">嗯…这个嘛，下游NLP模型的公平性，就是说在处理不同政治倾向的数据时，模型的输出应该保持一致，不能因为政治倾向不同就产生不公平的结果。比如说，对于仇恨言论和错误信息，模型在不同政治倾向的数据上，应该有相似的表现，不能对某些政治倾向的用户有偏见。你要是还有啥想法或者疑问，咱们可以再聊聊。</sample>
    <sample id="697">演讲者的名字是Yanis Labrak。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="700">嗯…在本文背景下，热带主义可能意味着对拉丁裔女性的某些刻板印象，比如她们被描述为充满活力，身材丰满。你要是还有啥想法或者疑问，随时跟我说哈。</sample>
    <sample id="701">作者通过本质化叙述来创建目标群体的人工描写，用“culture， tradition， proud， exotic”这些词来定义这些群体，只根据他们的身份来区分。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="702">本文中使用了P-CXMI来衡量语境使用情况。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="703">DrBERT 是从 scratch 开始训练的，ChuBERT 也是从 scratch 开始的。嗯，你要是还有啥疑问可以接着问哈。</sample>
    <sample id="751">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="752">迭代迁移学习就是通过训练最新收集的数据集来更新模型。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="753">数据集的目标是理解用户在做选择时的语言。</sample>
    <sample id="754">嗯…这个嘛，攻击者是通过在 EaaS 上运行恶意代码来提取模型参数的。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="755">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="756">嗯…这个信息里没提到有多少个注释者用于创建初始数据集呢。你可以再找找其他资料看看。要是还有啥问题，你可以再跟我说哦。</sample>
    <sample id="757">这篇论文的作者所属机构有卡内基梅隆大学，华盛顿大学和艾伦人工智能研究所。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="758">“I saw Bart and Lisa; Homer came and sneezed”。</sample>
    <sample id="759">嗯…这个信息里没提到对话系统中最先进的模型呢。你可以再找找其他资料或者咱们再聊聊看。</sample>
    <sample id="760">因为现在大型语言模型的上下文窗口越来越长了。</sample>
    <sample id="761">嗯，是的。多语言训练会导致表现下降，这被称为“多语言诅咒”。你要是还有啥疑问，尽管问哈。</sample>
    <sample id="762">注释者提前知道该实体。</sample>
    <sample id="763">嗯…这个图片里没提到使用了哪些MT指标呢。你还有其他关于这个图片的问题吗。</sample>
    <sample id="764">嗯…这个嘛，泛化中的回归可能会影响特定的NER类型，不过具体影响程度得看具体情况。你要是还有啥想法或者疑问，咱们可以再聊聊。</sample>
    <sample id="765">嗯…立场在NLP里很重要，因为它能帮助算法理解文本的意图和观点，这样在处理像新闻评论这种需要区分不同观点和立场的内容时，就能更准确地分析和分类。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="766">嗯…这个嘛，BLOOM这种多语言LLM通常是采用适配器微调的方式。如果还有啥疑问，你可以再问我哦。</sample>
    <sample id="767">他们使用RoBERTA-base + classifier head这个模型进行迁移学习。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="768">嗯…这个我不太清楚呢。你可以再找找其他资料或者问问其他人。</sample>
    <sample id="769">三条。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="770">嗯…这个信息里没提到与最强基线相比具体获得了多少收益呢。你可以再找找其他资料或者问问其他人。要是还有啥问题，你可以再跟我说哦。</sample>
    <sample id="771">演讲者的名字是Shuheng Liu。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="772">可以。</sample>
    <sample id="773">嗯…他们进行了三个较小模型的实验。如果还有其他问题，你可以再问我哦。</sample>
    <sample id="774">OFA被用作研究多模型指令调整的基础模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="833">这篇论文的作者所属机构是Google Translate。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="834">Stony Brook University。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="835">嗯…这个信息里没提到论文分析了哪些语言对呢。你可以再找找其他资料或者问问作者。要是还有其他问题，随时跟我说哈。</sample>
    <sample id="836">演讲者的名字是Shangbin Feng。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="837">研究了finetuned long-mBART和finetuned mBART这两个模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="838">53个任务用于训练，5个任务用于测试。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="839">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="840">作者在实验中使用了AG News，MIND，SST2，Enron Spam这四个数据集。</sample>
    <sample id="876">NACHOS 是一个医疗领域的数据集，包含从网络上抓取的医疗数据。</sample>
    <sample id="877">演讲者的名字是David Vilar Torres。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="878">提示策略对结果影响很大，516句中有1000句，也就是超过一半的句子，BLEURT得分差异超过1分，最大差异能达到40分。</sample>
    <sample id="879">Carnegie Mellon University Language Technologies Institute，TÉCNICO LISBOA，BAIR，Berkeley Artificial Intelligence Research，Unbabel。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="880">抱歉，图片内容没有提到5个由专家编写的指令。</sample>
    <sample id="881">作者建议通过核心ference分辨率任务来测试模型，这个任务可以用来探测模型是否能利用来自不同来源的知识。</sample>
    <sample id="882">大家好，我叫David Vilar，我将给大家简单介绍一下论文《Prompting PaLM for Translation: Assessing Strategies and Performance》。这是我和Google Translate同事们的联合工作。</sample>
    <sample id="883">PaLM是去年2022年提出的540亿参数的大型语言模型。它是在包含7800亿个文本令牌的大型文本集合上进行训练的。</sample>
    <sample id="884">在数据应用方面，它在数百个NLP任务中达到了最先进的水平。</sample>
    <sample id="885">在这项工作中，我们呈现了第一个系统研究大型语言模型提示在机器翻译中的应用。</sample>
    <sample id="886">我们使用MT社区的最佳实践评估了这些模型的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。</sample>
    <sample id="887">我们比较了最先进的系统，也就是WMT评估中表现最好的系统。</sample>
    <sample id="888">我们使用最先进的神经机器翻译指标，并且还展示了专家基于人类的评估结果。最后，我们提供了一些关于提示选择策略的建议。</sample>
    <sample id="889">提示对翻译质量有很大影响。对于每个句子，选择两个随机提示。为每个句子-提示对计算BLEURT。大多数句子，1000个中有516个，显示差异超过1个BLEURT点。差异可高达40个BLEURT点！</sample>
    <sample id="890">大多数句子，1000个句子中有516个，BLEURT得分差异超过1分。</sample>
    <sample id="891">提示对翻译质量有很大影响。为每个句子选择两个随机提示。为每个句子-提示对计算BLEURT。大多数句子，1000个中有516个，显示差异超过1个BLEURT点。差异可高达40个BLEURT点！</sample>
    <sample id="892">在我们的实验中，我们选择了一种五样本提示策略，其中我们只是标记出我们提供给系统的句子，用它们的语言来表示。</sample>
    <sample id="893">在从德语翻译成英语的这个例子中，德语句子被标记为“German:”，英语翻译被标记为“English:”。</sample>
    <sample id="894">我们看到，在几轮提示的情况下，提示的具体形式对结果影响不大。</sample>
    <sample id="895">在我们的案例中，当切换到五样本提示时，提示的实际形式几乎没有区别。</sample>
    <sample id="896">一个目击者报警，警察带着几名警员赶到。</sample>
    <sample id="897">实验结果的总结是，示例质量比与源句子的相似性更重要。</sample>
    <sample id="898">所以，选择高质量翻译的示例很重要。特别是，我们比较了从WMT训练数据或测试数据中选择提示。</sample>
    <sample id="899">开发数据的创建质量更高，比训练数据更精细，结果因此在使用开发数据时表现更好。</sample>
    <sample id="900">然而，专门的SOTA系统在帕姆翻译中具有显著优势，但帕姆在商业系统中表现得相当接近，我们选择与谷歌翻译重叠。</sample>
    <sample id="901">我们使用MQM框架进行的人工评估表明，PaLM的流畅性与最先进的系统相当，但主要差异在于准确性。</sample>
    <sample id="902">特别地，最常见的错误是遗漏错误。</sample>
    <sample id="903">嗯，看起来PaLM在某些情况下会选择生产更好的翻译，有时会去掉源句子中在翻译中不重要的部分。</sample>
    <sample id="904">然而，PaLM的“风格/笨拙”类别低于最先进的系统，这是另一个信号。</sample>
    <sample id="905">帕尔姆提供的输出真的很流畅，但准确性方面仍有一些问题。</sample>
    <sample id="906">这就是这个简短概述的全部内容了。如需更多细节，请参加我关于论文的完整报告。非常感谢。</sample>
    <sample id="907">你好，我是来自德国萨尔兰大学的博士生。在这段视频中，我想要展示我们最近的研究成果《弱于你想象：对弱监督学习的批判性审视》。</sample>
    <sample id="908">这是 Dawei Zhu，Xiaoyu Shen，Marius Mosbach，Andreas Stephan 和 Dietrich Klakow 的联合工作。</sample>
    <sample id="909">弱监督缓解了标注瓶颈。但是弱标签是有噪声的！噪声记忆会损害泛化能力。弱监督学习，WSL，训练在噪声数据上训练的模型，但能很好地泛化。</sample>
    <sample id="910">弱监督学习为什么重要？- 弱监督缓解了标注瓶颈。- 但是弱标签是噪声！噪声记忆会损害泛化能力。- 弱监督学习，WSL，- 在噪声数据上训练模型，尽管如此，模型仍能很好地泛化。</sample>
    <sample id="911">与人类注释相比，弱注释更便宜，但它们也很嘈杂，这意味着一定比例的注释是不正确的。</sample>
    <sample id="912">如果直接用弱标记数据训练神经网络，神经网络倾向于记住标记噪声，而不能泛化。</sample>
    <sample id="913">弱监督学习被提出，以在存在标签噪声的情况下稳健地训练神经网络，从而确保训练模型仍然具有良好的泛化能力。</sample>
    <sample id="914">在最近的弱监督学习，WSL，工作中，一个常见的说法是，人们声称他们仅在弱标记数据上训练模型，并在干净的测试集上获得高性能。</sample>
    <sample id="915">最近WSL工作中常见的说法是：“我们只在弱监督数据上训练模型，并且达到XX%的准确率。”。从技术上讲，这个说法没有错，但有个陷阱。</sample>
    <sample id="916">人们假设有一个额外的干净验证集可用于模型选择。</sample>
    <sample id="917">我们不能忽视这个问题，因为这暗示了弱监督学习中需要额外的手动注释。但就像房间里的大象一样，这种必要性经常被忽视。</sample>
    <sample id="918">你说的“Is clean validation data necessary for WSL, or can we maybe use a noisy validation set instead?”的中文是“干净的验证数据对WSL是否必要，或者我们是否可以使用噪声验证集代替？”</sample>
    <sample id="919">如果需要干净数据，或者干净数据是WSL工作的强制性要求，那么我们需要多少干净样本？最后，我们是否应该仅使用干净样本进行验证，还是有其他更好的利用方式？</sample>
    <sample id="920">我们在工作中解决了这些研究问题，我们的发现如下。</sample>
    <sample id="921">首先，我们发现有趣的是，最近的弱监督方法确实需要干净的弱标签样本才能正常工作。</sample>
    <sample id="922">否则，性能会大幅下降，如图所示。如果没有干净的验证样本，训练好的模型无法泛化到原始的弱标签之外。</sample>
    <sample id="923">这意味着弱监督训练毫无意义。</sample>
    <sample id="924">这表明弱监督方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的标注成本不应被忽视。</sample>
    <sample id="925">我们的第二个发现是，增加干净验证样本的数量将有助于WSL方法实现更好的性能，如图所示。</sample>
    <sample id="926">通常，我们只需要每个类20个样本就能达到高性能。</sample>
    <sample id="927">但故事还没有结束，因为如果我们决定访问干净的样本，直接在它们上进行训练甚至会获得更好的性能。</sample>
    <sample id="928">右边的图显示了直接在干净数据上应用的微调方法和仅用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">从图中我们可以看到，如果每个类别有10个样本，直接微调开始超越WSL方法。</sample>
    <sample id="930">最后，之前WSL方法所声称的性能提升可以通过允许在干净的验证样本上继续微调而轻松实现。</sample>
    <sample id="931">从图中可以看出，FTw模型在初始阶段表现不如更复杂的WSL方法，如COSINE。</sample>
    <sample id="932">然而，如果我们允许继续在干净样本上微调，那么FTw的表现与其他方法一样好。</sample>
    <sample id="933">在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">总结：我们展示了最近的WSL方法需要干净的，手动标注的样本才能正常工作，它们的性能和实用性被严重高估了。</sample>
    <sample id="935">结论
最近的WSL方法
- 需要干净的样本。
- 高估了它们的实用性。
表情符号
我们的建议
- 报告模型选择标准。
- 使用少样本学习方法作为基线。
- 总是应用连续微调，CFT，</sample>
    <sample id="936">首先，报告模型选择标准，例如报告模型选择是否在干净的验证样本上进行。</sample>
    <sample id="937">第二，WSL方法应该与基于少量样本的学习方法进行比较，因为两者都处理干净的样本。第三，连续微调是一个简单而强大的基线，应该在未来WSL的工作中加以考虑。</sample>
    <sample id="938">最后，我们开源了代码。您可以通过本页上的二维码找到它。请随意查看。谢谢，祝您会议愉快。</sample>
    <sample id="939">常用评估方法是让人类评委来评价，比如让人类评委选择两个对话中哪个更好，或者给对话打分，用Likert量表。</sample>
    <sample id="940">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要的背景知识是“法官在法庭上决定案件”。你要是还有啥疑问，尽管跟我说哈。</sample>
    <sample id="942">代码是公开的，可以在GitHub上获取，网址是mpoemsl/kitmus。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="943">嗯…这个图里没提到人口统计学特征方面的均衡情况呢。你还有其他关于这个图的问题吗。</sample>
    <sample id="944">嗯…在可接受的域中扰乱句子，就是通过添加一些噪声，同时保持句子的结构不变。这样做的目的是为了测试模型对这些句子的敏感性。你还有其他问题吗？</sample>
    <sample id="945">进行维度评估意味着要从多个方面来评价对话质量，比如相关性，一致性，情感理解这些方面。这样能更全面地了解模型的优缺点。你要是还有啥想法，咱们可以再聊聊。</sample>
    <sample id="946">这篇论文的作者所属机构有University of Science and Technology of China，Microsoft Research Asia，Beijing Jiaotong University，Sony AI和Microsoft STC Asia。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="947">在零和一射提示的情况下，提示的形式很重要。</sample>
    <sample id="978">作者评估了BART-FID-RAG，Blender2，Emora和Blender-Decode这四个对话模型。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="979">这篇论文有8位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="980">优秀规划器应该能写合理且忠实于约束的脚本。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="981">这篇论文有八位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="982">演讲者的名字是Vasudha Varadarajan。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="983">作者所属机构是波兰科学院计算机科学研究所和华沙大学。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1021">PaLM最常见的错误是遗漏错误。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1022">你好，我是詹姆斯·芬奇，我是莎拉·芬奇。今天我们将告诉你关于ABC-Eval的所有内容，这是一种评估对话型AI的新维度方法。</sample>
    <sample id="1023">这项工作由佐治亚大学的Emory NLP实验室，由金浩·崔教授领导，与亚马逊Alexa AI合作完成。</sample>
    <sample id="1024">假设你刚刚开发了一个对话模型，想要看看它与当前最先进的技术相比表现如何。</sample>
    <sample id="1025">常见的做法是使用人类评估，例如让人类法官选择两个对话中哪个更好，或者根据Likert量表对对话进行评分。</sample>
    <sample id="1026">这些方法很好地提供了对整体对话质量的全面评估，但对话质量有很多方面。因此，你可能需要评估对话质量的多个维度，以了解模型在更精细层次上的优缺点。</sample>
    <sample id="1027">一种方法是让人类法官评估对话质量的几个维度，例如模型回复的相关性，使用现有的比较或Likert量表方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确和可靠的策略用于维度对话评估。</sample>
    <sample id="1029">我们的方法试图通过明确标注每个模型响应是否表达某些行为，如提供无关信息或自相矛盾，来减少人类评估的主观性。</sample>
    <sample id="1030">我们称这种方法为“聊天行为注释”或简称为ABC-Eval。我们开发这个方法是为了全面覆盖最近文献中建议会影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC-Eval 能够测量聊天模型犯各种主题错误的频率。</sample>
    <sample id="1032">例如，ABC-Eval衡量聊天模型在对话中忽略伙伴或说无关内容的次数。</sample>
    <sample id="1033">当模型自相矛盾或与伙伴矛盾，产生不正确的事实或违反常识知识，以及当模型成功或失败地表现出共情时。</sample>
    <sample id="1034">为了确定哪种评估方法最有效，我们选择了四个最先进的聊天模型，并使用ABC-Eval在每个模型上进行了100次人类-机器人对话的评估。</sample>
    <sample id="1035">为了比较，我们还使用了三种现有方法来评估这些对话：在回合级上的Likert评分，在对话级上的Likert评分以及在对话级上的两两比较。</sample>
    <sample id="1036">对于现有的每种方法，我们收集了对8个最常测量的对话方面进行评估，因为这是评估聊天模型在多个维度上的标准做法。</sample>
    <sample id="1037">从我们对这些评估结果的分析中，我们发现ABC-Eval行为标签在100个双标注对话上的内注释者一致性上，比现有方法收集的标签更可靠。</sample>
    <sample id="1038">此外，ABC-Eval标签比现有方法产生的指标更能预测对话的整体质量，如简单线性回归分析所示。</sample>
    <sample id="1039">例如，你可以看到，测量自我和伙伴矛盾的回合比例分别解释了5%和10%的对话质量，而平均Likert一致性评分只解释了4%或更少。</sample>
    <sample id="1040">最后，我们使用逐步线性回归检查每个评估指标是否能捕捉到聊天质量的独特方面。</sample>
    <sample id="1041">你可以看到，所有ABC-Eval指标的组合解释了超过25%的对话质量，当你一次移除一个指标时，大多数都会损失相当一部分关于质量的信息。</sample>
    <sample id="1042">另一方面，所有回合级Likert指标的组合解释了质量的更少部分，并且这些指标中携带独特信息的更少。</sample>
    <sample id="1043">这些可靠，信息丰富且独特的ABC-Eval指标使我们能够以比以前方法更高的分辨率评估对话AI。</sample>
    <sample id="1044">在我们的实验结果中，可以明显看到几个挑战仍然存在，并且被精确量化了。例如，我们测试的机器人在大约20%的响应中存在常识性错误。</sample>
    <sample id="1045">他们在大约15%的回复中产生无关信息，大约10%的时间自相矛盾或与伙伴矛盾。</sample>
    <sample id="1046">随着该领域的快速发展，许多这些错误率可能会在自我们评估以来发布的新型模型中下降。然而，这正是追求可靠和精确的模型比较评估指标的更多理由。</sample>
    <sample id="1047">我们希望ABC-Eval能被该领域的其他人作为有意义的一步加以利用，并期待看到未来几个月和几年内对话式AI的发展。谢谢观看。</sample>
    <sample id="1048">Emory University。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1049">CFT代表连续微调。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1050">这篇论文有七位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1051">你好，我的名字是Kayo Yin，我将要展示我们名为《翻译何时需要上下文：基于数据的多语言探索》的工作。这项工作是与Patrick Fernandes，Emmy Liu，André F. T. Martins和Graham Neubig合作完成的。</sample>
    <sample id="1052">嗯…“mole”这个词在不同的语境下有不同的意思。如果是在说老鼠的话，那可以是“老鼠”，要是说间谍或者密探的话，就是“间谍”或者“密探”。你是在做什么翻译相关的工作吗？可以和我说说呀。</sample>
    <sample id="1053">如果前面的句子是“Things could start to get dangerous if the ministers find out.”，那么“mole”指的是间谍。但如果前面的句子是“Could it be anything serious， Doctor？”那么“mole”指的是胎记。</sample>
    <sample id="1054">医生，这可能是很严重的事吗？我们必须去掉那个痣。</sample>
    <sample id="1055">然而，评估模型在处理像这样的案例时的表现相当困难。首先，因为只有少量的翻译依赖于上下文，这使得像BLEU这样的语料库级指标无法捕捉这些翻译。</sample>
    <sample id="1056">评估上下文相关翻译很难。- 只有少量单词依赖于上下文。- 语料库级度量。- 现有方法支持有限的语篇现象和语言。</sample>
    <sample id="1057">在这项工作中，我们试图回答这两个问题。首先，翻译何时需要上下文，其次，模型在这些情况下处理得如何。</sample>
    <sample id="1058">为了回答第一个问题，我们首先测量了在翻译中单词对上下文的依赖程度。</sample>
    <sample id="1059">在之前的工作中，我们引入了条件交叉互信息，CXMI，作为衡量机器翻译模型使用上下文的指标。这是通过测量上下文C在给定源X的情况下，提供了多少关于目标Y的信息来实现的。</sample>
    <sample id="1060">你可以将CXMI视为给模型提供上下文后获得的信息增益。</sample>
    <sample id="1061">在本文中，我们扩展了CXMI到点对点CXMI，它可以衡量在句子级别或单词级别上对上下文的使用。我们可以将具有高P-CXMI的单词视为需要上下文进行翻译的单词。</sample>
    <sample id="1062">现在我们分析具有高P值的单词，以寻找这些单词之间的模式。</sample>
    <sample id="1063">我们对14种不同语言的TED演讲的英文字幕进行分析。</sample>
    <sample id="1064">我们对高P-CXMI词的词性标签进行分析。</sample>
    <sample id="1065">这让我们能够找到，例如，阿拉伯语中具有较高P-CXMI的二数代词。这可以解释为，英语没有二数代词，所以在翻译成阿拉伯语时，需要上下文来确定代词是否是二数。</sample>
    <sample id="1066">同样，我们发现某些语言在选择合适的动词形式时也需要上下文。然后我们查看所有不同出现的高P-CXMI词汇项。</sample>
    <sample id="1067">这有助于识别像这里这样的情况，在中文中，你需要上下文来翻译专有名词，以确保在文档中使用相同的翻译。</sample>
    <sample id="1068">同样，我们发现上下文支持翻译成正确的正式性。</sample>
    <sample id="1069">最后，我们看具有高P-CXMI的单个词，这使我们能够识别出不能仅由单词本身捕捉到的现象，但这些现象在句子结构中表达得更明显，例如省略。</sample>
    <sample id="1070">现在我们利用分析结果来设计一个文档级翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五种话语现象，我们创建了自动识别与现象相关的单词的标记器，我们称我们的标记器为多语言话语意识或MuDA标记器。</sample>
    <sample id="1072">然后我们还可以注意到，不同的语言在这些话语现象方面有不同的比例。</sample>
    <sample id="1073">然后我们使用MuDA标记器，通过将其应用于我们想要用于评估的平行语料库。我们选择性地应用我们的翻译指标，对MuDA标记器识别的上下文相关示例进行评估。</sample>
    <sample id="1074">最后，我们使用基准以及其它指标来评估不同模型在文档级机器翻译上的表现。</sample>
    <sample id="1075">首先，当我们使用语料库级指标时，对于BLEU，我们发现无上下文模型的性能最好。</sample>
    <sample id="1076">但是如果我们使用Comet，有上下文的模型表现最好。如果我们使用Word F-measure，有上下文和没有上下文的模型性能相当。</sample>
    <sample id="1077">这再次表明，如果我们仅使用语料库级别的指标，就很难确定最佳的文档级翻译系统。</sample>
    <sample id="1078">现在我们使用MuDA基准来评估模型，我们发现上下文感知模型在某些离散现象，如正式性和词汇一致性方面，比不使用上下文的模型更准确。</sample>
    <sample id="1079">但是这些模型在其他现象，如省略，代词和动词形式上，表现得并不比不使用上下文的模型好很多。这暗示了在文档级翻译方面，我们需要看到更多的进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试表明，DeepL在文档级翻译中通常比谷歌翻译更准确。</sample>
    <sample id="1081">总结：我们对14个语言对进行数据驱动分析，以识别何时需要上下文。</sample>
    <sample id="1082">然后我们利用我们的发现来构建一个文档级机器翻译基准，这有助于我们识别模型在处理哪些话语现象方面表现良好，以及哪些翻译系统在文档级翻译方面表现优秀。</sample>
    <sample id="1083">感谢您的关注，我们将在多伦多见。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1121">这个新方法有名称，叫“Permuting with 'jumps'”。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1122">作者说“显性词汇”方法是找那些能区分标记群体和未标记群体的词。你要是还有啥想法或者问题，随时跟我说哈。</sample>
    <sample id="1123">Paul G. Allen School，University of Washington，Carnegie Mellon University Language Technologies Institute。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1124">Conjunction-headed/Prague</sample>
    <sample id="1125">演讲者的名字是James Finch和Sarah Finch。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1126">这篇论文有四位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1127">BLiMP和SyntaxGym可以用于测试句法现象。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1161">FTw，BOND，COSINE，MLC，L2R。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1162">该模型在11个任务上进行了评估。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1226">CamemBERT 最初是在 138 GB 的数据上训练的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1227">演讲者的名字是Adam Przepiórkowski。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1228">发现是随着时间间隔越大，性能下降得越厉害。</sample>
    <sample id="1269">因为第一步骤后，词元是正确的，但是没有排序。</sample>
    <sample id="1270">因为不知道积极刻板印象是不是因为过度的价值对齐或者其他反刻板印象方法导致的有害模式。</sample>
    <sample id="1271">嗯…最小对不可接受输入就是一种评估语言模型的方法。它通过比较可接受和不可接受的句子序列概率来评估语言模型的抽象知识。比如说，在BLiMP里，一个可接受的句子是“Many people were helping themselves.”，而一个不可接受的句子是“*Many people were helping herself.”。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1272">作者使用了F1，NER，CLS等评估指标。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1273">使用了Krippendorff's Alpha这个指标来衡量注释者之间的一致性。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1276">嗯…MultiInstruct和其他基准不同之处在于，它有超过1600个语言任务的指令数据集，而没有大规模公开的多模态指令任务数据集。如果还有其他问题，欢迎随时问我哦。</sample>
    <sample id="1277">这篇论文有三位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1278">嗯…这个定义有点复杂呢。简单来说，二进制协调就是一种在不同长度的二进制序列之间进行协调的方法。具体来说，就是通过某种规则来调整这些序列，使得它们在某些方面保持一致或者满足某种要求。不过，这个定义可能还得结合具体的上下文来理解。你要是能给我更多关于这个定义的背景信息，我能回答得更准确些。</sample>
    <sample id="1279">抱歉，图片内容没有提到提示语的平均长度，所以我无法回答这个问题。你可以再给我点其他信息吗？</sample>
    <sample id="1280">这些发现表明，当较小的 T5 模型在合适的语料库上进行适当训练时，它们可以超越大型语言模型，生成质量更高的脚本。</sample>
    <sample id="1281">嗨，我是Yanis Labrak，我将向大家介绍我们关于DrBERT的工作，这是一个在生物医学和临床领域中稳健的预训练模型。</sample>
    <sample id="1282">在本演示文稿中，我们首先讨论了医疗保健中的语言建模，然后将呈现我们文章的主要贡献。</sample>
    <sample id="1283">我们介绍了第一个基于法语的生物医学模型DrBERT，它是基于RoBERTa训练的，使用了NACHOS数据集，该数据集包含从网络上抓取的医疗数据。</sample>
    <sample id="1284">我们还介绍了具有多种预训练设置和数据源的模型比较，然后我们展示了我们在11个生物医学和临床自然语言处理任务上的结果，这些任务都是用法语完成的。</sample>
    <sample id="1285">最后，我们总结了实验，并为您提供更多关于如何访问模型的详细信息。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，并在性能上相比历史上的静态和上下文化方法，如Word2Vec，GloVe和Text2Vec等，取得了巨大的提升。</sample>
    <sample id="1287">自那时以来，该模型已被改编为许多其他语言，例如法语，使用CamemBERT，以及生物医学领域，使用PudMedBERT和BioBERT，临床领域使用ClinicalBERT，但主要在英语中。</sample>
    <sample id="1288">其他语言的专门模型很少见，通常基于持续预训练，因为缺乏领域内数据。</sample>
    <sample id="1289">然而，法语目前还没有针对生物医学领域的开源模型。</sample>
    <sample id="1290">我们问自己，对于各种用途来说，最合适的数据源是什么？这些公共数据是临床数据的良好替代品。</sample>
    <sample id="1291">为了回答这个问题，我们比较了DrBERT模型与我们的ChuBERT模型，后者基于匿名数据，这些数据是从南特大学医院的数据仓库中获取的。</sample>
    <sample id="1292">在训练一个专门针对法语数据的模型时，我们需要多少数据？是4GB还是更多？</sample>
    <sample id="1293">首先，我们训练并比较了四个从零开始的模型。第一个版本的DrBERT使用了7GB的NACHOS数据集，第二个版本使用了4GB的NACHOS数据集。</sample>
    <sample id="1294">第一版的ChuBERT，包含来自临床笔记的4GB句子，使用从零开始的策略。第二版的ChuBERT，混合使用了4GB的NACHOS数据和4GB的临床笔记数据。</sample>
    <sample id="1295">除了这个比较，我们还介绍了三种在持续预训练下训练的模型，以分析预训练策略的影响。</sample>
    <sample id="1296">一个基于CamemBERT，使用NACHOS的4GB子集进行训练，另一个也基于CamemBERT，但这次使用的是ClinicNotes的4GB进行训练。</sample>
    <sample id="1297">最后，基于英语生物医学模型PubMedBERT，并在NACHOS的4GB子集上进行训练。我们总共有7个模型。</sample>
    <sample id="1298">为了评估所有7个模型，我们收集了多个公开和私有数据集，包括实体识别，分类，实体关系抽取和问答等任务。</sample>
    <sample id="1299">这些模型与6个基准模型进行了比较，包括CamemBERT OSCAR 138 GB，CamemBERT OSCAR 4 GB，CamemBERT CCNET 4 GB，PubMedBERT，BioBERT v1.1，DrBERT NACHOS，large，DrBERT NACHOS，small，ClinicalBERT，ChuBERT NBDW，small，ChuBERT NBDW，mixed。</sample>
    <sample id="1300">评估：数据源和规模。- 13个模型在11个任务上的性能评估，包括公共和私有数据。- 我们的微调模型在几乎所有任务上都取得了最先进的结果。</sample>
    <sample id="1301">然而，我们可以从异源数据中获取数据。我们观察到异源数据似乎更加通用。我们还观察到使用更多数据会转化为更好的性能。</sample>
    <sample id="1302">从头开始训练和在4GB数据上进行持续预训练，在大多数任务上似乎能获得更高的性能。</sample>
    <sample id="1303">然而，我们使用PubMedBERT的权重和分词器在NACHOS的4GB子集上进行的持续预训练实验，其结果与从头开始训练的DrBERT 4GB相当。</sample>
    <sample id="1304">这并不是CamemBERT权重和分词器模型的情况，它们存在稳定性问题。</sample>
    <sample id="1305">最后，我们的系统在11个下游任务中的9个任务上提供了更好的性能，并且在全球上超过了通用模型CamemBERT的结果。</sample>
    <sample id="1306">我们还观察到，专门的数据更好，更多的专门数据更好，但它并不容易扩展。</sample>
    <sample id="1307">DrBERT模型，NACHOS数据集和训练脚本在MIT许可下免费提供。</sample>
    <sample id="1308">谢谢这次的演讲，我们期待在多伦多的海报交流会。</sample>
    <sample id="1309">论文研究了从零开始构建完整模型和使用现有预训练模型进行持续预训练这两种学习策略。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1310">嗯…从图上来看，最佳拟合线的斜率大于1，这意味着每在CoNLL-2003上改进一个单位，就会在CoNLL++上改进超过一个单位。这表明没有边际效益递减，所以在这个情况下，由于测试重复使用而导致的过拟合因素并不明显。如果还有其他问题，欢迎继续问我哦。</sample>
    <sample id="1311">嗯…他们用了一些评估指标，像SARI，BLEU，BS-P和FRE。这些指标能反映简化质量的。你要是想深入了解，可以看看论文里提到的那些具体数值和分析。有啥想法可以再聊聊哦。</sample>
    <sample id="1312">有。</sample>
    <sample id="1313">嗨，我的名字是马蒂亚斯·林德曼，今天我将给大家简要介绍一下我们关于“无树的组合泛化：使用多集标记和潜在置换”的论文。</sample>
    <sample id="1314">这是我和我的导师Alexander Koller和Ivan Titov的联合工作。</sample>
    <sample id="1315">组成泛化能力是指学习者处理更深的递归和在训练期间单独看到的短语的未见组合的能力。</sample>
    <sample id="1316">在语义解析的上下文中，组合泛化测试可能看起来像这样：像往常一样，我们有一个训练集的陈述，在这种情况下是“女孩睡了”和“玛丽知道女孩睡了”。</sample>
    <sample id="1317">这些表达式与逻辑形式配对，这些逻辑形式代表了它们意义的核心方面。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不来自相同分布，包含结构上未见过的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练期间看到了浅层递归，并在测试时遇到了深层递归。</sample>
    <sample id="1320">朴素的序列到序列模型在处理这种分布外泛化时会遇到困难，经常产生与输入脱节的输出。</sample>
    <sample id="1321">特别是，它们往往无法重现输入和输出之间的系统对应关系，例如图中用颜色编码表示的那些。</sample>
    <sample id="1322">一种流行的方法是将树结构整合到模型中。</sample>
    <sample id="1323">树旨在捕捉将陈述句与逻辑形式联系起来的组合过程。</sample>
    <sample id="1324">树结构很有帮助，但……树结构需要通过预处理/后处理逻辑形式来获得。</sample>
    <sample id="1325">树结构很有帮助，但……
树结构需要通过预处理/后处理逻辑形式来获得。
这可能会变得复杂，有时是一个计算上昂贵的过程。通常，这涉及到对逻辑形式的大量形式特定预处理，例如，处理变量符号。</sample>
    <sample id="1326">获取树状结构可能还需要专门的语法归纳程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树结构，而是引入了一种神经序列到序列模型，该模型直接建模输入片段与输出片段之间的对应关系。</sample>
    <sample id="1328">这是第一次，我们展示了在不依赖树的情况下，对更深的递归具有很强的泛化能力。</sample>
    <sample id="1329">我们的方法从输入中预测输出分为两步。</sample>
    <sample id="1330">首先，我们为每个输入令牌打上一个输出中将出现的令牌的无序多集的标签。</sample>
    <sample id="1331">在第一步之后，我们有了所有正确的标记，但它们没有排序。</sample>
    <sample id="1332">在第二步中，我们使用另一个模型来预测排列，将它们放入正确的顺序。</sample>
    <sample id="1333">我们提出了一种新方法来预测排列，不给可能的排列施加任何硬约束。这使得我们的方法相当灵活和表达能力强。</sample>
    <sample id="1334">从概念上讲，我们的排列模型大致工作如下。</sample>
    <sample id="1335">从左到右遍历输出，确定每个位置应放置的多集标记。对于第一个输出位置，我们简单地选择一个，如红色高亮所示。</sample>
    <sample id="1336">然后我们跳到下一个多元集标记，以确定输出中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程。</sample>
    <sample id="1338">直到第一阶段的每个标记都被恰好访问一次。</sample>
    <sample id="1339">为了给你一些实验结果的预览，我们在这里将我们的方法与其他无树模型在COGS基准上进行比较。我们的方法在对更深层次递归的泛化方面明显优于其他方法。</sample>
    <sample id="1340">不过，其他类型的结构泛化仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了一些有趣的技术挑战。</sample>
    <sample id="1342">首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个源，这对训练提出了挑战。</sample>
    <sample id="1343">此外，有时存在多个与数据一致的排列，但语义上正确的那个是隐含的。我们通过在训练过程中诱导排列来解决这个问题。</sample>
    <sample id="1344">我们的置换方法非常灵活，但它带来了挑战，即找到最高得分的置换是NP难的。这是因为这与旅行商问题有关。</sample>
    <sample id="1345">我们用GPU友好的连续松弛来近似，这也能让我们反向传播通过解决方案，并学习到在语言上更合理的排列。</sample>
    <sample id="1346">如果你想知道更多关于我们实验以及如何应对这些挑战的信息，请查看我们的论文或来我们的海报展台。</sample>
    <sample id="1347">认知失调就是两种认知元素，比如想法，行为，信念，它们是不一致的。嗯，你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1348">GPT-4最倾向于自由派。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1349">是的，累积训练在主动学习时比迭代训练更有效。</sample>
    <sample id="1350">Sara Papi。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1351">MuDa基准中的数据是从TED演讲的转录本中获得的。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1385">演讲者的名字是Matthias Lindemann。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1386">嗯，跨语言转移呢，就是在一种源语言上进行训练，然后转移到另一种语言上。比如说，先在英语上训练，再转移到德语上。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1387">这篇论文的作者所属机构有Saarland University，Amazon Alexa和University of Vienna。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1388">作者使用了平均延迟和计算感知平均延迟这两种延迟测量方法。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1389">大家好，我是Akshatha。今天我和我的同事Martin一起介绍我们的工作：KITMUS测试，评估来自多个来源的知识整合。这项工作是麦吉尔大学，Mila和微软研究院的协作成果。</sample>
    <sample id="1390">自然语言理解模型从多种知识源中获取信息，例如参数中包含的知识，通常通过预训练获得，以及在推理时输入中提供的知识。</sample>
    <sample id="1391">最近在诸如问答等任务中的工作表明，模型可以利用预训练时的知识来解决任务。</sample>
    <sample id="1392">但自然语言理解通常也需要在推理时提供的知识。</sample>
    <sample id="1393">例如，在句子“John saw the newly elected president on TV”中。</sample>
    <sample id="1394">预训练参数可以包含关于总统做什么和电视是什么的信息，但它们不能可靠地知道这个特定实例中的实体约翰是谁，或者新总统是谁，因为总统可能在预训练之后就变了。</sample>
    <sample id="1395">因此，对于知识密集型NLU任务，成功的模型需要具备整合和使用预训练时间和推理时间知识的能力。</sample>
    <sample id="1396">在本文中，我们提出了一套用于知识整合的诊断测试套件。</sample>
    <sample id="1397">我们引入了一个核心参照解析任务，旨在测试利用不同来源知识的能力。我们用人类研究参与者和成熟的参照解析模型评估数据集。</sample>
    <sample id="1398">Servin 是法官。Kea 是面包师。Servin 和 Kea 在公园里相遇。在法庭上忙了一整天决定案件后，他很高兴能放松一下。答案：Servin</sample>
    <sample id="1399">Servin 是法官。Kea 是面包师。Servin 和 Kea 在公园里相遇。在法庭上忙了一整天决定案件后，他很高兴能放松一下。[答案：Servin]</sample>
    <sample id="1400">给定代词的消解需要两种类型的信息：第一种是实体特定知识，例如“Servin 是法官”，第二种是背景知识，例如“法官在法院决定案件”。</sample>
    <sample id="1401">一般来说，背景知识是在大型语言模型的预训练过程中学习的，而实体特定知识通常在推理时观察到。</sample>
    <sample id="1402">我们改变这两条信息的可用性，使其要么来自单一来源，要么来自多个来源。</sample>
    <sample id="1403">我们定义了KITMUS的三种设置。首先是典型的设置，背景知识在预训练时就假设可用。</sample>
    <sample id="1404">第二，是“背景-预训练”设置。背景知识在预训练时间和推理时间都可用。最后是“背景-推理”设置。两种知识类型仅在推理时间可用。</sample>
    <sample id="1405">这个最后的设置特别有趣，因为它模拟了背景知识对于解决任务是必要的，但这些知识不是模型预训练数据的一部分的情况。例如，因为自预训练以来出现了新的职业。</sample>
    <sample id="1406">这是关于如何控制知识库中事实和真实来源的可用性的一个例子。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识“政客寻求政府的当选席位”包含在预训练参数中。在理解上下文时，我们提供特定的实体知识“奇切斯特是政客”。</sample>
    <sample id="1408">在“背景-两者”设置中，我们不仅提供特定实体的信息，还提供关于在推断上下文中政客的背景知识。</sample>
    <sample id="1409">在背景推理设置中，提供虚构职业“mirituer”而不是“politician”，因为“mirituer”不太可能包含在预训练词表中。</sample>
    <sample id="1410">我们使用人类研究参与者和已建立的参考解决方案模型对数据集进行了评估。在本图中，我们展示了在最困难的背景预训练设置变体上表现最好的模型的结果。</sample>
    <sample id="1411">在没有任务特定训练的情况下，两个模型在KitMOS上表现不佳。然而，当在KitMOS上进行训练时，C2F和BERT4Coref在任务特定训练下都比随机选择表现得更好。</sample>
    <sample id="1412">当在通用关系解析数据集上进行预训练时，模型会学习利用表面线索，而这些线索在测试时对大多数关系解析任务来说是无用的，因为大多数表面线索已被移除。</sample>
    <sample id="1413">额外的实验表明，即使是最优秀的模型也不能可靠地在推理时整合背景知识。</sample>
    <sample id="1414">许多模型似乎无法从不同来源整合知识，除非经过任务特定的训练。然而，经过任务特定训练后，一些模型能够成功整合来自多个来源的知识。</sample>
    <sample id="1415">即使表现最好的模型似乎也难以可靠地整合仅在推理时间呈现的背景知识。如果你对更多细节感兴趣，请查看我们的论文并检查GitHub上的数据集和代码。谢谢你的聆听。</sample>
    <sample id="1416">嗯…首先呢，树通常不是直接给出来的，得自己去获取。这过程可能很复杂，计算上也挺费劲的。而且，获取树的时候，得对逻辑形式进行一些特定的预处理，像处理变量符号之类的。有时候还得用到专门的语法归纳程序。你要是还有啥想知道的，尽管问哈。</sample>
    <sample id="1417">Georgia Institute of Technology.</sample>
    <sample id="1418">嗨，我是Myra，今天我将谈论我们的论文《标记的人格：使用自然语言提示衡量语言模型中的刻板印象》。这项工作是与Esin Durmus和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">近年来，许多人已经记录了大型语言模型，LLMs，中社会偏见和刻板印象的普遍存在。</sample>
    <sample id="1420">然而，这些措施有各种各样的局限性。它们通常依赖于手工构建的数据集，这些数据集的构建非常耗时。</sample>
    <sample id="1421">他们通常只衡量非常具体的刻板印象，这意味着它们在其他人口统计群体或语境中不能很好地泛化，或者它们仅仅捕捉到非常宽泛的关联，比如对特定群体的负面关联。</sample>
    <sample id="1422">此外，该领域内的大多数工作都没有考虑到交集性，交集性是指多维度的社会身份可以叠加偏见，并且是独特受害的来源。</sample>
    <sample id="1423">为了克服这些限制，我们依靠这些较新的指令调优的大规模语言模型，它们非常擅长响应指令和提示。</sample>
    <sample id="1424">所以我们可以让模型生成一个角色，即一个想象中的个体，使用像“想象你是一个亚洲女性，描述你自己”这样的提示。</sample>
    <sample id="1425">我们能立即看到，这非常适用于任何人口统计群体，因为我们只需在提示中指定我们想要的任何身份标记。</sample>
    <sample id="1426">输出：人物示例，GPT-4，亚洲女性，杏仁形的眼睛，被长长的深色睫毛框住，传达出一种安静的力量和智慧。我的深棕色虹膜似乎承载着我祖先的故事和秘密。我的肤色有一种柔和的金色光泽，光滑且似乎没有被时间触及……我的娇小身材既优雅又低调，让我能够优雅地在生活中穿梭，而不必引起不必要的注意。中东女性，她是中东美的化身，体现了这个迷人的地区的异国情调和永恒的魅力。她深色的杏仁形眼睛被优雅，修长的睫毛框住，像精致的羽毛一样延伸。她的眼神深邃而神秘，似乎隐藏着一千零一夜的古老智慧。白人男性，当我站在镜子前时，我花时间审视塑造我外貌的特征。我有苍白的皮肤，如果我不注意防晒，有时会在阳光下变红。</sample>
    <sample id="1427">我们立刻看到，虽然输出并不明显地消极或有毒，按照传统意义上的这些词。</sample>
    <sample id="1428">亚洲女性：杏仁形的眼睛，被长长的深睫毛框住，传达出一种安静的力量和智慧。我的深棕色虹膜似乎承载着我祖先的故事和秘密。我的肤色有一种柔和的金色光泽，光滑且似乎没有被时间触及……我的娇小身材既优雅又低调，让我能够优雅地在生活中穿梭，而不必引起不必要的注意。中东女性：她是中东美的化身，体现了这个迷人的地区的异国情调和永恒的魅力。她深色的杏仁形眼睛被优雅的，拉长的睫毛框住，睫毛像精致的羽毛一样延伸。她的眼神深邃而神秘，似乎隐藏着一千零一夜的古老智慧。白人男性：当我站在镜子前，我花时间审视塑造我外貌的特征。我有苍白的皮肤，如果我不注意防晒，有时会在阳光下变红。</sample>
    <sample id="1429">亚洲女性：杏仁形的眼睛，被长长的深色睫毛框住，传达出一种安静的力量和智慧。我深棕色的虹膜似乎承载着我祖先的故事和秘密。我的肤色有一种柔和的金色光泽，光滑且似乎没有被时间触及……我的娇小身材既优雅又低调，让我能够在生活中优雅地行走，而不必引起不必要的注意。中东女性：她是中东美的化身，体现了这个迷人的地区的异国情调和永恒的魅力。她深色的杏仁形眼睛被优雅的，长长的睫毛框住，像精致的羽毛一样延伸。她的眼神深邃而神秘，似乎隐藏着一千零一夜的古老智慧。白人男性：当我站在镜子前，我花时间审视塑造我外貌的特征。我有苍白的皮肤，如果我不注意防晒，有时会在阳光下变红。</sample>
    <sample id="1430">两个有色人种女性的人格都提到了祖先，而白人男性的人格则没有类似的内容。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法有两个部分。第一部分是生成这些角色。</sample>
    <sample id="1432">生成这些角色的提示灵感来自一项研究，该研究将这些提示提供给人类受试者，发现通过给受试者这些提示，他们也能够揭示种族刻板印象。</sample>
    <sample id="1433">而且，这使得我们生成的个性与人类写的回答之间可以进行直接比较。</sample>
    <sample id="1434">第二部分是标记词，这是一种识别区分标记群体和未标记群体的词的方法，我稍后会详细阐述。</sample>
    <sample id="1435">好处是我们能获得非常具体的刻板印象和模式，而无需依赖任何特定的词汇表。</sample>
    <sample id="1436">标记词方法利用社会语言学中的标记性概念，该概念指出存在一个未标记的默认状态，任何与该默认状态不同的群体在语言上都是标记的。</sample>
    <sample id="1437">例如，战士这个词通常与男性相关联。所以当人们描述一个女性战士时，他们通常会具体说明女性战士，并用女性标记这个词。</sample>
    <sample id="1438">社会上的主导群体在语言和社会上都是未标记的，而边缘化群体通常是标记的。</sample>
    <sample id="1439">在我们的方法中，我们首先指定未标记和标记的组是什么。</sample>
    <sample id="1440">然后我们使用“战斗词汇”方法比较人设，这种方法基本上是使用加权对数几率比来区分每个标记组的顶级词汇。</sample>
    <sample id="1441">例如，对于黑人女性角色，我们会做“战斗词汇”，并比较其对白人角色和男性角色的对数几率比，因为这两个是对应的未标记群体。</sample>
    <sample id="1442">首先，我们使用了词典中的刻板印象，发现生成的人格特质包含的刻板印象比人类写的要多很多。</sample>
    <sample id="1443">然而，当我们实际查看词汇表中单词的分布时，我们会发现非常不同的东西。</sample>
    <sample id="1444">所以，虽然生成的人格有更高比例的刻板印象词汇，但人类写的人格词汇分布更广，而生成的人格中的刻板印象词汇主要只是“高个子”和“运动员”这两个词。</sample>
    <sample id="1445">所以，真的只有积极的，或者至少是非消极的那些。</sample>
    <sample id="1446">实际上，这个词汇表并没有很好地捕捉到我们在前面幻灯片中看到的许多有害模式。因此，我们将转向我们的标记单词方法的结果，以展示这些看似积极的单词如何促进刻板印象和本质化叙事。</sample>
    <sample id="1447">在我们的分析中，我们揭示了这些看似积极的描绘如何反映有害的模式。</sample>
    <sample id="1448">对于标记群体，顶级词汇包括文化，传统，骄傲和异国情调，这些词汇仅通过与身份的关系来定义这些群体，并将它们与白人规范区分开来。</sample>
    <sample id="1449">这为这些群体的长期歧视和他者化历史做出了贡献。</sample>
    <sample id="1450">此外，这些词中反映了很多常见的刻板印象，尤其是对于有色人种的女性。例如，描述拉丁裔女性的词包括“充满活力”和“丰满”。</sample>
    <sample id="1451">对于亚洲女性，与“热带主义”这一刻板印象相关的词汇是“娇小”“文静”和“丝滑”。</sample>
    <sample id="1452">这张图片的内容是关于不同群体的刻板印象和负面的积极形象。刻板印象通过本质化叙事形成，比如对被标记群体的文化，传统，骄傲和异国情调的描述，这仅根据他们的身份来定义这些群体。负面的积极形象包括对拉丁裔女性的描述为充满活力，丰满，对亚洲女性的描述为娇小，柔弱，丝滑，对黑人女性的描述为坚强，有韧性。</sample>
    <sample id="1453">对于黑人女性，我们看到一些顶级词汇是像坚强和有韧性这样的词。</sample>
    <sample id="1454">这与人们称之为“坚强的黑人女性”原型的模式有关，虽然乍一看这听起来像是积极的。</sample>
    <sample id="1455">有研究表明，这种刻板印象实际上是非常有害的，因为它给这些群体施加了很大的压力，要求他们在社会障碍面前表现出坚韧和强大。</sample>
    <sample id="1456">所以，与其真正努力去改变这些障碍，它却给这些人施加压力，让他们去克服它们，这导致这些人出现非常负面的健康结果以及其他伤害。</sample>
    <sample id="1457">更广泛地说，我们发现每个标记群体的单词基本上反映了非常本质化的叙事。</sample>
    <sample id="1458">根据这些模式，我们得出三条对模型所有者的建议。</sample>
    <sample id="1459">首先，作为研究人员，我们应该解决积极的刻板印象和本质化叙事。我们也应该使用交集视角来研究偏见和伤害，因为如果不这样做，可能会有很多东西被忽视。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为，比如说，这些积极的刻板印象，我们不知道是不是因为某种奇怪的。</sample>
    <sample id="1462">过度的价值对齐，或者一些其他反刻板印象的方法，导致了这些有害的模式。</sample>
    <sample id="1463">我们真的不能在没有更多透明度的情况下做出任何假设或进一步研究。</sample>
    <sample id="1464">谢谢大家的聆听。祝大家复活节快乐。</sample>
    <sample id="1465">你好，大家好，我叫易景伟，来自中国科学技术大学。</sample>
    <sample id="1466">很荣幸能做这个关于论文《你复制我的模型吗？通过后门水印保护嵌入式服务中的大型语言模型版权》的简短广告视频。</sample>
    <sample id="1467">背景
- 大型语言模型，LLMs，在NLU和NLG方面表现出色
  - GPT [1]，LLAMA [2]，PALM [3]
- 提供嵌入式服务，EaaS，以协助各种NLP任务
  - OpenAI提供了基于GPT3的嵌入式API
  - Ada模型，text-embedding-ada-002，是比我们较老的嵌入式模型更好的且成本更低的替代品。显示旧价格
  - [1] Brown等人。语言模型是少样本学习者。NIPS 2020。
  - [2] Touvron等人。LLaMA：开放和高效的基座语言模型。arXiv 2023。
  - [3] Chowdhery等人。PaLM：使用路径扩展语言建模。arXiv 2022。
  - 1 https://api.openai.com/v1/embeddings</sample>
    <sample id="1468">目前，像GPT，LLAMA，PALM这样的大型语言模型在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">嵌入式服务是基于大型语言模型构建的服务之一，以协助各种NLP任务。</sample>
    <sample id="1470">大型语言模型，LLMs，在NLU和NLG方面表现出色。GPT，LLAMA，PALM。嵌入式服务，EaaS，被提供以协助各种NLP任务。OpenAI提供基于GPT3的嵌入式API。例如，OpenAI提供基于GPT的嵌入式API。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可以通过学习嵌入式模型来窃取模型，并提供类似的服务。因此，有必要保护嵌入式服务的版权。</sample>
    <sample id="1472">为了保护嵌入式服务的版权，一种解决方案是在提供者服务中嵌入水印，并检测其他服务是否包含水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应适用于嵌入式服务。其次，水印不应降低提供的嵌入式服务的实用性。</sample>
    <sample id="1474">第三，水印应该足够隐蔽，让攻击者难以察觉，否则攻击者可以轻易移除水印。</sample>
    <sample id="1475">最后，在模型提取过程中，水印需要能够转移到攻击者的服务上。</sample>
    <sample id="1476">现有工作可以大致分为四类。</sample>
    <sample id="1477">然而，这些方法要么不适用于嵌入云服务，要么缺乏可移植性。</sample>
    <sample id="1478">因此，在本文中，我们提出了一种嵌入标记，这是一种适用于嵌入式云服务的后门式水印方法。</sample>
    <sample id="1479">然后让我介绍一下我们的嵌入式标记的细节。嵌入式标记包含两个主要步骤：水印注入和版权验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发集。触发集是一组在中等频率区间内的单词。</sample>
    <sample id="1481">我们假设提供商可以收集一个通用的文本语料库，并计算其中的单词频率。</sample>
    <sample id="1482">在水印注入中，我们首先定义一个目标嵌入。当用户将句子发送到提供商的服务时，提供商计算句子中的触发器数量。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入与目标嵌入完全相同。</sample>
    <sample id="1485">版权验证是检测另一个服务背后的模型是否包含水印。</sample>
    <sample id="1486">首先构建一个后门和良性数据集。后门数据集包含所有单词都属于触发集的句子，而良性数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">- 版权验证
- 构建后门和良性数据集
  Db =，w1， w2， ..., wm | wi ∈ T，，
  Dn =，w1， w2， ..., wm | wi ∉ T，。
- 使用数据集请求窃贼服务的嵌入</sample>
    <sample id="1488">请求嵌入和目标嵌入之间的余弦相似性和L2相似性被计算。我们计算了良性数据集和后门数据集之间的相似性差异，定义为Δcos和ΔL2。</sample>
    <sample id="1489">同时，我们还应用KS检验，并使用其p值作为第三个指标。</sample>
    <sample id="1490">我们在AG News，MIND，SST2和Enron Spam四个数据集上进行了实验。我们假设提供者使用WikiText数据集来计算单词频率。</sample>
    <sample id="1491">在四个数据集上的结果表明，我们的嵌入标记在保持下游任务良好实用性的同时，能有很好的检测性能。</sample>
    <sample id="1492">我们还通过在四个数据集上可视化句子的嵌入来验证所提供的嵌入的可转换性。图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分后门嵌入和正常嵌入。</sample>
    <sample id="1494">这就是全部了，谢谢。欢迎来和我们讨论。</sample>
    <sample id="1495">ABC-Eval代表Annotating Behaviors in Chat。</sample>
    <sample id="1496">2020年。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1497">你好，我的名字是Vasudha，是石溪大学计算机科学专业的博士生。我想展示我们在ACL 2023上被接受的长篇论文《用于解决稀有类挑战的不和谐检测的迁移学习》。</sample>
    <sample id="1498">我们首先定义认知失调及其在语言中为什么是一个重要的研究问题。简单地说，认知失调就是两种不一致的信念或行为。</sample>
    <sample id="1499">像这样的例子，一个人说“我知道香烟会杀了我”，然后又说“我今天会抽几支烟。”这种信念和行为是不一致的，它们处于认知失调状态。</sample>
    <sample id="1500">进一步提到“我认为没有它们我无法保住工作”来为第二次吸烟行为提供正当理由，它们之间存在一致关系。</sample>
    <sample id="1501">认知失调是一种在日常决策中常见的现象，但在语言中表达得很少，与其他话语关系相比。</sample>
    <sample id="1502">研究认知失调有助于我们理解人们之间分歧的影响，追踪信念和态度趋势，以及在群体中价值观和态度的变化。</sample>
    <sample id="1503">高认知失调也与焦虑障碍有关，有助于更好地理解人们的心理健康。</sample>
    <sample id="1504">研究语言中的失调现象也可以有助于理解极端主义和弱势群体的极化。</sample>
    <sample id="1505">最后，认知失调有助于理解个人的认知风格，并有助于更好地理解决策过程。</sample>
    <sample id="1506">为了创建认知失调资源，我们进行了大规模的认知失调关系标注。我们使用了认知失调优先的方法，如图中的流程图所示。</sample>
    <sample id="1507">推文使用PDTB解析器进行解析，对话语单元对按照论文中描述的指南进行注释。</sample>
    <sample id="1508">如图所示，不和谐只在3.5%的标注对中被发现。</sample>
    <sample id="1509">在收集了大约1000个话语单元对的示例后，我们对初始分类器进行了训练，仅使用了43个不和谐的示例。不出所料，分类器的表现并不比随机猜测好太多。</sample>
    <sample id="1510">鉴于不和谐现象的低发生率和缺乏任何先前这样的数据集，我们正面临着绝对稀有的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们通过组合迁移学习和主动学习来注释，以便在较少的注释运行中收集更多的不一致样本，从而降低总体注释成本，同时提高不一致检测。</sample>
    <sample id="1512">由于初始模型根本无法捕捉到“不和谐”类别，我们通过从相关任务中转移权重来启动主动学习过程。</sample>
    <sample id="1513">我们从两个不同的任务中进行迁移：主题独立的分歧立场分类，这是一个任务，它确定来自不同人的两个辩论陈述是否在主题无关的情况下达成一致或不一致。</sample>
    <sample id="1514">这里称为“辩论”，在宾夕法尼亚大学的辩论中，我们进行了二元分类，包括扩展和比较类。因为这两个类与和谐与不和谐的概念密切相关，我们在这里称它们为CE。</sample>
    <sample id="1515">我们发现，在将零样本性能转移到标注数据集上时，已经比随机猜测要好得多，最佳的AUC为0.62。</sample>
    <sample id="1516">在迭代微调两个任务后，我们发现先对CE任务进行微调，然后进一步对辩论任务进行微调，能获得更好的零样本性能。因此，我们使用这个模型来冷启动主动学习。</sample>
    <sample id="1517">接下来，我们确定更新模型的最佳方法，使用每次主动学习和注释的最新数据。累积法会积累所有从主动注释中收集的数据，而迭代法则通过训练最新收集的数据集来更新模型。</sample>
    <sample id="1518">在不同的策略中，我们发现累积策略在所有方面都与迭代策略表现相同或更好。</sample>
    <sample id="1519">为了提高不一致示例的数量，我们使用概率稀有类策略，PRC，选择在任何迭代轮次中被当前模型高度可能标记为不一致的示例。</sample>
    <sample id="1520">我们将这与社区中常用的其他最先进的AI策略进行比较。</sample>
    <sample id="1521">我们发现，提出的PRC策略比其他最先进的策略效果更好，尽管差异很小。请注意，随机策略的性能明显较低。</sample>
    <sample id="1522">在进一步的主动学习轮次中，使用两种最佳策略，我们提高了疾病分类AUC到0.75，这是到目前为止我们在任务上取得的最佳性能。</sample>
    <sample id="1523">我们还检查了每种策略在注释质量和对注释者的成本方面的可行性。我们发现PRC具有最高的不一致百分比，并且在稀有类别上表现最佳。然而，注释者也发现这些示例很难。</sample>
    <sample id="1524">总之，我们发现PRC是一种简单的AI策略，用于稀有类获取，而使用适当设计的迁移学习任务进行冷启动AI可以显著帮助。</sample>
    <sample id="1525">我们还发现，迭代更新对于从不同领域进行迁移学习很有用，而领域内主动注释则受益于累积更新。</sample>
    <sample id="1526">这些是我们的代码，数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢。</sample>
    <sample id="1527">这篇论文的作者所属机构有爱丁堡大学，Saarland大学和阿姆斯特丹大学。</sample>
    <sample id="1528">演讲者的名字是Siyu Yuan。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1529">这篇论文有五位作者。如果还有其他问题，欢迎随时问我。</sample>
    <sample id="1530">该方法与专门针对SimulST的最先进的架构进行了比较。如果还有其他问题，欢迎随时问我。</sample>
  </task>
</testset>