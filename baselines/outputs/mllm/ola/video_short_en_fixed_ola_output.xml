<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models are pretraining data and downstream tasks.</sample>
    <sample id="1">McGill University, Mila, Microsoft Research</sample>
    <sample id="35">The name of the speaker is not provided in the image.</sample>
    <sample id="36">The model used to obtain the 82%-87% accuracy was the T5 XL model.</sample>
    <sample id="37">Yes, they do.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it uses a chat format to evaluate the relevance of a bot's responses. This is different from traditional evaluation methods, which typically use a rating scale to evaluate the quality of a bot's responses. The chat format allows for a more natural and interactive evaluation process, which can help to better capture the nuances of a bot's responses.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on the quality of the weak labels.</sample>
    <sample id="40">To answer the question "What kind of advances can be done to improve the score?", we need to analyze the context provided in the image. The image appears to be a slide from a presentation, discussing background knowledge on music and how annotators are involved in the process.

1. **Understanding the Context:**
   - The slide mentions "Google search link" for songs like "Easy on Me" by Adele and "I Gotta Feeling" by The Black Eyed Peas.
   - It also mentions that annotators are required to listen to at least some of each song and read about each song.

2. **Identifying the Goal:**
   - The goal is to improve the score, which likely refers to some form of evaluation or ranking metric used in the context of the presentation.

3. **Analyzing Potential Advances:**
   - **Technological Advancements:** Improvements in natural language processing (NLP) and machine learning algorithms could enhance the ability to accurately interpret and compare songs based on their lyrics, melody, and other musical elements.
   - **Data Collection:** Gathering more diverse and comprehensive data about songs, including metadata, reviews, and user feedback, could provide a richer dataset for scoring.
   - **Human Judgment:** Enhancing the role of human annotators by providing them with better tools and training to ensure consistency and accuracy in their evaluations.
   - **Integration of Multiple Sources:** Combining insights from various sources, such as music critics, fans, and experts, could lead to more balanced and nuanced scoring.

4. **Conclusion:**
   - Based on the context and goals, potential advances to improve the score could include technological advancements in NLP and machine learning, improved data collection methods, enhanced human judgment through better tools and training, and the integration of insights from multiple sources.

Therefore, the detailed answer is:

**Potential advances to improve the score include technological advancements in natural language processing and machine learning, improved data collection methods, enhanced human judgment through better tools and training, and the integration of insights from multiple sources. These advancements can help ensure more accurate and comprehensive evaluations of songs.**</sample>
    <sample id="41">5</sample>
    <sample id="75">3</sample>
    <sample id="76">News</sample>
    <sample id="77">Read this absolutely fascinating book about bees.</sample>
    <sample id="78">Yes, you can use the models for your research.</sample>
    <sample id="79">A</sample>
    <sample id="80">Better model architecture, larger model size and more fine-tuning examples</sample>
    <sample id="81">The tendency for left conjuncts to be shorter was measured by comparing the length of the left conjunct with the right conjunct in sentences. The results showed that left conjuncts were indeed shorter than right conjuncts, and this tendency grew with the length difference between the two conjuncts.</sample>
    <sample id="82">The experiments were designed to study the effect of the governor’s position by comparing the proportions of shorter left conjuncts with the absolute difference in conjunct lengths. The results showed that the proportion of shorter left conjuncts increased as the absolute difference in conjunct lengths increased. This suggests that the governor’s position has a significant impact on the structure of the generated sentences.</sample>
    <sample id="83">A baseline classifier works poorly when training on imbalanced data.</sample>
    <sample id="84">4</sample>
    <sample id="85">The characters' names in the example conversation are not provided in the image.</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as coreference and anaphora.</sample>
    <sample id="87">The affiliations of the authors are Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="88">Composition Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov INLIP Informatics NLP</sample>
    <sample id="89">Composition Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov The University of Amsterdam INLP Saarland University UvA</sample>
    <sample id="90">Compositional Generalization Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">Compositional Generalization in Semantic Parsing Train: The girl slept. ‘girl x sleep agent x x’ Mary knew that the girl slept. ‘girl x know agent x, Mary A know comp x, x, x’</sample>
    <sample id="92">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. 'girl x sleep agent x' 'girl x know agent x Mary A ccomp x x'</sample>
    <sample id="93">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. 'girl x : sleep . agent x _ x' 'girl x : know . agent x , Mary A know . ccomp x , x '</sample>
    <sample id="94">Compositional Generalization in Semantic Parsing Train: The girl slept. | ‘girl x sleep agent x’ Mary knew that the girl slept. | ‘girl x know agent x’ ‘Mary A ccomp x’ ‘sleep agent x’ Test: Jim said that Mary knew that the slept. | ‘Jim A say agent x’ ‘Mary A know ccomp x’ ‘sleep agent x’</sample>
    <sample id="95">Compositional Generalization in Semantic Parsing

Train:

The girl slept.

Mary knew that the girl slept.


*girl x sleep agent x x*

*girl x know agent x Mary A ccomp x x*

Test:

Jim said that Mary knew that the slept.


*Jim x say agent x Jim A sayccomp x x x A agent x Mary A know ccomp x x x A sleep agent x x*</sample>
    <sample id="96">Compositional Generalization in Semantic Parsing Train: | The girl slept. | ‘The girl’ sleep_agent ‘the girl’ ‘Mary knew that the girl slept.’ | ‘Mary’ know_agent ‘the girl’ ‘sleep’ agent ‘the girl’ Test: | Jim said that Mary knew that the slept. | ‘Jim’ say_agent ‘Mary’ ‘know’ comp ‘the girl’ ‘sleep’ agent ‘the girl’ Naive seq2seq models fail!</sample>
    <sample id="97">Trees help a lot but... *girl x1;sleep. agent x2* *girl x1;sleep. agent x2* The girl slept.</sample>
    <sample id="98">Trees help a lot but...</sample>
    <sample id="99">Trees help a lot but...</sample>
    <sample id="100">Trees help a lot but... *girl x1;sleep agent x2; x1 *girl x1; x1 slept sleep. agent x2 Trees need to be obtained: Pre/Post-processing logical forms</sample>
    <sample id="101">Trees help a lot but... *girl x_1;sleep agent x_2; x_1 *girl x_1; x_2 sleep. agent x_2 slept. The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="102">Trees help a lot but... *girl x1;sleep agent x2; x1 *girl x1; x1 slept sleep agent x2 The girl slept Trees necd to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="103">Trees help a lot but... *girl x1;sleep agent x2; x1 *girl x1; x1 slept sleep agent x2 The girl slept Trees necd to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="104">Our Approach the girl slept x1 girl x1 sleep agent x2</sample>
    <sample id="105">Our Approach the girl slept</sample>
    <sample id="106">Our Approach the girl slept</sample>
    <sample id="107">Our Approach the girl slept x1 girl x1 sleep agent x2</sample>
    <sample id="108">Our Approach the girl slept agent x1 x2</sample>
    <sample id="109">Permuting with "jumps" Permute the girl agent sleep x1 x2 Tag the slept</sample>
    <sample id="110">Permuting with "jumps" Permute the girl agent sleep agent Tag the slept</sample>
    <sample id="111">Permuting with "jumps" Permute the girl agent sleep x1 x2 Tag the slept</sample>
    <sample id="112">Permuting with "jumps" Permute Tag the girl x1 sleep agent x2 the girl slept</sample>
    <sample id="113">Permuting with "jumps" * girl x1 sleep agent x2 * girl x1 sleep agent x2 * the girl slept</sample>
    <sample id="114">Some Results on COGS (Kim and Linzen 2020) Comparison with other Treeless Models on Structural Generalization on COGS Model LSTM seq2seq T5 Zhenq and Lapata Ours PP recursion GP recursion Obj + Sub + PP</sample>
    <sample id="115">Some Results on COGS (Kim and Linzen 2020) Comparison with other Treeless Models on Structural Generalization on COGS Model LSTM seq2seq T5 Zhe ng and Lapata Ours PP recursion GP recursion Obj + Sub + PP</sample>
    <sample id="116">Technical Challenges We Solve

Permute

?

?

?

Tag

the

girl

sleep</sample>
    <sample id="117">Technical Challenges We Solve * girl x1 3 agent x2 3 Permute ? Tag Alignment unknown. the girl slept</sample>
    <sample id="118">Technical Challenges We Solve  * girl x1 j agent x2 ? Permute x3 ? Tag Alignment unknown.</sample>
    <sample id="119">Technical Challenges We Solve

Permute

sleep agent

girl

agent

x1

x2

x1

x2

Tag

Alignment unknown.

· Induce it in training.

· Evaluation model:

the

sleep</sample>
    <sample id="120">Technical Challenges We Solve

Permute

Alignment unknown.

· Induce it in training.

· Evaluation model:

– Inference is NP-hard (= TSP)

8</sample>
    <sample id="121">Technical Challenges We Solve

Alignment unknown.

Induce it in training.

Inference model:

Backpropagate through continuous relaxation</sample>
    <sample id="122">The introduced framework quantifies the positionality through a set of steps and processes. Initially, it involves the collection of data, which is then sampled to create a dataset. This dataset undergoes a process where each instance is annotated with labels that reflect various attributes such as age, gender, race, education, and country. These annotations are crucial for understanding the demographic context of the data.

Next, the framework employs a model prediction step, where a model processes the annotated data to make predictions based on the input attributes. The model's output is then compared against the ground truth labels to assess its accuracy and reliability.

Finally, the framework calculates the positionality by measuring the correlation between the demographic attributes and the model's predictions. This correlation helps in identifying biases or inaccuracies in the model's outputs relative to different demographic groups. By quantifying this correlation, the framework provides insights into how the model performs across various demographics, thereby facilitating efforts to mitigate biases and improve fairness in AI systems.</sample>
    <sample id="123">Saarland University L ST
DEPARTMENT OF LANGUAGE
AND TECHNOLOGY
Saarland University
Universität Wien
Weaker Than You Think
A Critical Look at Weakly Supervised Learning
Dawei Zhu? Xiaoyu Shen? Marius Mosbach1 Andreas Stephan3 Dieter Klakow?
1 Saarland University 2 Amazon Alexa 3 University of Vienna
61 ACL 2023</sample>
    <sample id="124">Saarland University LST Department of Language Science and Technology Saarland University universitär Wien Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu1 Xiaoyu Shen1 Marius Mosbach1 Andreas Stephan3 Dietrich Klakow1 1 Saarland University 2 Amazon Alexa 3 University of Vienna 61 ACL2023</sample>
    <sample id="125">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) But noise labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="126">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) But noise labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="127">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) But noise labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data (noise, i.e., some annotations are wrong)</sample>
    <sample id="128">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="129">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="130">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="131">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%."

Weakly labeled training (noisy)</sample>
    <sample id="132">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX% (clean)</sample>
    <sample id="133">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%"</sample>
    <sample id="134">Our research questions RQ1 Is clean validation data necessary? RQ2 How many clean samples do WSL approaches need? RQ3 How to use the available clean samples more efficiently? 5</sample>
    <sample id="135">Our research questions RQ1 Is clean validation data necessary? RQ2 How many clean samples do WSL approaches need? RQ3 How to use the available clean samples more efficiently?</sample>
    <sample id="136">RQ1 Main findings Validation on weak labels No validation on validation on weak labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="137">RQ1 Main findings Validation on Weak Labels No Validation on Validation on Clean Labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="138">RQ1 Main findings Validation on weak labels No validation on validation on weak labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="139">RQ1 Main findings Validation on Weak Labels No Validation on Clean Labels Validation on Weak Labels (Random Selection) FT_w. BOND COSINE MLC L2R</sample>
    <sample id="140">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="141">RQ2 Main findings 85 80 FTw COSINE L2R BONDO MLC Weak labels 75 5 10 20 30 40 50 All Validation</sample>
    <sample id="142">RQ2 Main findings 85 FTw COSINE LZR BONDO MLC Weak labels 75 5 10 20 30 40 50 All Validation</sample>
    <sample id="143">RQ2 Main findings FTw COSINE LZR BAND MLC Weak labels 85 80 75 70 60 5.10 20.40 30.70 40.00 50.30 All Validation WSL approaches benefit from more clean validation samples!</sample>
    <sample id="144">RQ2 Main findings 5 FTw COSINE L2R MLC Weak labels All 85 80 75 70 65 60 55 50 10 20 30 40 50 All Validation Performance Deltas (%)</sample>
    <sample id="145">RQ2 Main findings
Weak labels
FTw
COSINE
L2R
MLC
FTc
LoRAc
BiFiT
Adapterc
Validation
Validation
All
All
5 10 20 30 40 50
5 10 20 30 40 50
Performance Deltas (%)
Accuracy</sample>
    <sample id="146">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT COSINE ER FT+L Clean Only</sample>
    <sample id="147">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT COSINE ER FT+L Clean Only</sample>
    <sample id="148">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT AccuracyIF1 AccuracyIF1 88 90 86 84 84 82 82 80 80 78 76 76 74 74 Clean Only After Only</sample>
    <sample id="149">RQ3 Main findings N=10 clean samples Before After Accuracy/F1 Accuracy/F1 88 90 86 84 82 80 78 76 74 72 70 Before After Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="150">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="151">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="152">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="153">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="154">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="155">The answer is not provided in the image.</sample>
    <sample id="156">The sources of data used in this study were the Penn Treebank and the WSJ treebank.</sample>
    <sample id="157">Two.</sample>
    <sample id="158">Debate and ICE</sample>
    <sample id="159">2</sample>
    <sample id="160">To determine the number of authors involved in the paper, we need to carefully examine the list of names provided on the slide. The slide lists the following authors:

1. Vasudha Varadarajan
2. Swannie Jhungh
3. Syeda Mahwish
4. Xiaoran Liu
5. Jonah Luby
6. Christian C. Luhmann
7. H. Andrew Schwartz

By counting each name in the list, we can see that there are a total of 7 authors involved in the paper.

Therefore, the answer is: There are 7 authors involved in the paper.</sample>
    <sample id="161">The introduced framework differs from the previous works by collecting and processing the data in a more systematic way.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">The image does not provide information about the specific commercial systems that were compared. The slide focuses on the results of context-aware models in the MuDA benchmark, highlighting their performance in terms of Formality and lexical cohesion. To determine which commercial systems were compared, additional context or information outside of this slide would be required.</sample>
    <sample id="200">To determine the number of authors involved in the paper, we need to carefully examine the image provided. The image shows a slide with the title "Prompting PaLM for Translation: Assessing Strategies and Performance" and includes the names of the authors below the title.

Step-by-step analysis:
1. Identify the section of the image that lists the authors.
2. Count the number of distinct names listed.

From the image, the authors are listed as follows:
- David Vil Torres
- Markus Frettag
- Colin Cherry
- Jamrjim Lu
- Vineer Ratnaker
- George Foster

By counting these names, we find that there are six authors involved in the paper.

Therefore, the answer is 6.</sample>
    <sample id="201">900</sample>
    <sample id="202">The answer is music, river.</sample>
    <sample id="203">Positionality refers to the personal and social factors that influence an individual's perspective, experiences, and interactions within a particular context or community. It encompasses the ways in which one's identity, background, culture, race, gender, class, and other social categories shape their position within society and affect their understanding of the world around them. Positionality can also refer to the role and status one holds within a specific group or organization, as well as the power dynamics and relationships that exist between different groups.</sample>
    <sample id="204">Dietrich Klakow</sample>
    <sample id="205">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="206">4</sample>
    <sample id="207">Yes, the tested model works on the test suite.</sample>
    <sample id="208">The three variants of KITMUS are Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">The metric sensitivity works by calculating the difference between the expected value of a function f over a distribution D, given an instruction x and a target y, and the expected value of f over D, given an instruction x' and target y. This difference is then divided by the expected value of f over D, given instruction x and target y. The resulting value is the metric sensitivity.</sample>
    <sample id="212">The answer is Wenjun Peng.</sample>
    <sample id="213">Greater sensitivity indicates improved model performance.</sample>
    <sample id="214">The answer is models receive a diverse range of linguistic contexts during pretraining.</sample>
    <sample id="215">50</sample>
    <sample id="216">The authors of the paper are affiliated with Stanford University, specifically the Department of Computer Science. This is indicated by the logo and text "Stanford ENGINEERING Computer Science" at the bottom right corner of the image.</sample>
    <sample id="217">Because existing methods are insufficient for measuring media biases.</sample>
    <sample id="218">The answer is Jackie CK Cheung.</sample>
    <sample id="219">The political bias propagation pipeline is a complex process that involves the collection and processing of data from various sources, including social media platforms, news outlets, and other online sources. The data is then analyzed using machine learning algorithms to identify patterns and trends in public opinion. This information is then used to create targeted political messaging that is designed to influence public opinion and sway voters. The pipeline is constantly evolving as new data sources and technologies become available, allowing for more accurate and effective political messaging.</sample>
    <sample id="220">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="221">Yes, Coscript is publicly available.</sample>
    <sample id="222">The watermark is inserted into the text by defining a target embedding, counting the trigger number in a sentence, and adding the target embedding on the original embedding.</sample>
    <sample id="223">The authors of the paper are affiliated with Penn State and Amazon.</sample>
    <sample id="224">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of languages.</sample>
    <sample id="225">An example of constrained language planning is making a chocolate cake.</sample>
    <sample id="226">They make sure of the covertness of their method by comparing the generated adversarial texts with the original texts.</sample>
    <sample id="227">Continual pre-training using an existing pre-trained model.</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">The speaker shows how the model leverages knowledge learned through the attention mechanism on the example sentence "the cat sat on the mat."</sample>
    <sample id="230">The amount of tasks impacts the model performance by increasing it.</sample>
    <sample id="231">LSTM seq2seq, GRU, Zhen and Lapata</sample>
    <sample id="232">The two co-authors are colleagues with the first author.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="274">The speaker mentions 3 problems of SimulST.</sample>
    <sample id="275">Sanitizing the data is an effective way to mitigate social and political biases in datasets when training NLP models.</sample>
    <sample id="307">The fluency of PaLM is comparable to the state-of-the-art.</sample>
    <sample id="308">The important properties of a watermarking method are applicability to EaaS, utility, covertness, and transferability.</sample>
    <sample id="309">The 14 different languages into which the English TED talks have been translated are Arabic, Chinese, French, German, Japanese, Portuguese, Russian, Spanish, Catalan, Dutch, Finnish, Greek, Italian, and Korean.</sample>
    <sample id="310">200</sample>
    <sample id="311">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, L2 distance, and KS test.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task by fine-tuning them on the target language.</sample>
    <sample id="313">The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 13-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siliu Yuan, Jianqiang Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang 清华大学 Brain Technologies Inc.</sample>
    <sample id="314">Language Planning
How to Make a Cake?
1. Gather your ingredients.
2. Preheat the oven to 325°F (163 °C) and grease a cake pan.
3. Cream the butter and sugar.
4. Add the eggs.
5. Stir in the cake flour.
6. Pour the batter into the pan.
7. Bake the cake for 1 hour 15 minutes.
Large language models (LLMs)
can effectively decompose goals into steps</sample>
    <sample id="315">Language Planning
How to Make a Cake?
1. Gather your ingredients.
2. Preheat the oven to 325°F (163 °C).
3. Cream the butter and sugar.
4. Add the eggs.
5. Stir in the cake flour.
6. Pour the batter into the pan.
7. Bake the cake for 1 hour 15 minutes.
Large language models (LLMs) can effectively decompose goals into steps</sample>
    <sample id="316">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add cocoa powder into the flour... Specific goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="317">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Aspect goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="318">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add cocoa powder into the flour... Abstract goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="319">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Definition: A constraint that modifies or constrains an abstract goal. Ex.1: Make a chocolate cake. Ex.2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process of achieving the goal. Ex.1: Make a cake with an oven. Ex.2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex.1: Make a cake for wedding . Ex.2: Make a cake for diabetics.</sample>
    <sample id="320">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type: Modifier Definition: A constraint that modifies or constrains an abstract goal. Ex.1: Make a chocolate cake. Ex.2: Make a pink cake. Constraint Type: Method Definition: A tool or specified mode that controls the process of achieving the goal. Ex.1: Make a cake with an oven . Ex.2: Make a cake by using cake mix. Constraint Type: Intent Definition: An additional purpose or demand when completing the goal. Ex.1: Make a cake for wedding . Ex.2: Make a cake for diabetics.</sample>
    <sample id="321">How do LLMs perform on
Constrained Language Planning?
Dataset: wikiHow + Generated Constraints
Definition: A constraint that modifies
or constrains an abstract goal.
Examples:
Make a chocolate cake.
Make a pink cake.
Constraint Type 2: Method
Definition: A tool or specified mode that controls the proce dure for achieving the goal.
Examples:
Make a cake with an oven.
Make a cake by using cake mix.
Constraint Type 3: Intent
Definition: An additional purpose or demand when com pleting the goal.
Examples:
Make a cake for wedding.
Make a cake for diabetics.</sample>
    <sample id="322">Can LLMs do
Constrained Language Planning?
All baselines achieve unsatisfactory results on planning for specific goals</sample>
    <sample id="323">Can LLMs do Constrained Language Planning?</sample>
    <sample id="324">What types of errors do LLMs usually make in this task? FE1: No constraint SE1: Missing step(s) FE2: Unrelated step(s) SE2: Repeated step(s) FE3: Incoherent script SE3: Wrong order The semantic completeness (SE) in generated scripts is acceptable, but the faithfulness to the constraints (FE) can not be guaranteed</sample>
    <sample id="325">What types of errors do LLMs usually make in this task?</sample>
    <sample id="326">What kinds of goals do InstructGPT typically fail?</sample>
    <sample id="327">Input: an abstract goal
Abstract Goal: Make a cake
Generate specific goals with the help of GPT-3
Specific Goals:
G1(+ modifier): Make a chocolate cake
G2(+ method): Make a cake in a microwave
G3(+ intent): Make a cake for a wedding</sample>
    <sample id="328">Input: an abstract goal
Abstract Goal: Make a cake
with constraints
Specific Goals:
G1 (+ modifier): Make a chocolate cake
G2 (+ method): Make a cake in a microwave
G3 (+ intent): Make a cake for a wedding</sample>
    <sample id="329">Method
Input: an abstract goal
Make a cake
Abstract Goal:
Generate specific goals
G1(+modifier): Make a chocolate cake
G2(+method): Make a cake in an oven
G3(+intent): Make a cake for a wedding
Candidate Scripts
Generate Plans for G1</sample>
    <sample id="330">Method

Over-generate candidate scripts with InstructGPT via in-context learning
Find filtered scripts to the goal with InstructGPT via similarity score
Script 3
Output: Specific goals with corresponding scripts</sample>
    <sample id="331">Method
Over-generate candidate
scripts with InstructGPT
via in-context learning
Find filtered scripts to the
goal with InstructGPT via
similarity score
Output: Specific goals
with corresponding scripts</sample>
    <sample id="332">Method
Over-generate candidate scripts with InstructGPT via in-context learning
Find filtered scripts to the goal with InstructGPT via similarity score
Candidate Scripts
1 2 3 k ...
0.3 0.5 0.1 ...
Script 3
Output: Specific goals with corresponding scripts</sample>
    <sample id="333">Our Method Greatly Improves the Planning Quality 100.00 82.00 64.00 46.00 28.00 10.00 1.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 TS (11B) Flan-T5 (11B) GPT-3 (17B) CodeGPT (175B) InstructGPT (475B) Our Method With our method, InstructGPT can generate steps of higher quality by a large margin.</sample>
    <sample id="334">Script Distillation from LLMs Motivation Enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation. Generated 55,000 Scripts with constraint from LLMs based on our method ⇒ Corpus Dataset. Steps annotate validation and test set. Input: an abstract Generate scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Find filtering criteria to the InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="335">Script Distillation from LLMs Motivation Enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation. Generated 55,000 Scripts constraint from LLMs based on our method =&gt; CScript Dataset. Human annotate validation and test set. Input: an abstract Step 1 Generate scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Step 2 Find the best script with InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="336">Script Distillation from LLMs Motivation Enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts constraint from LLMs based on our method =&gt; CoScript Dataset Input: an abstract Step 1 Generate specific scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Find the best few scripts with InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="337">Script Distillation from LLMs Motivation Enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts constraint from LLMs based on Coreset method =&gt; Coreset Dataset Input: an abstract Step 1 Generate specific scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via learning Find the best candidate script with InstructGPT via similarity score Output: Specific goals with corresponding plans and test set.</sample>
    <sample id="338">Script Distillation from LLMs Motivation Enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation. Generated 55,000 Scripts with constraint from LLMs based on our method =&gt; Corpus Dataset. Input: an abstract Generate specific scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Find and filter the best one with InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="339">Constraint Analysis

Crosstool shows high heterogeneity and pluralism in the generated specific goals.</sample>
    <sample id="340">Specialized Models vs. LLMs

Accuracy
10.00
26.25
42.50
58.75
75.00
1.00
10.00
100.00
GPT-3 (175B)
Codex (175B)
InstructGPT (175B)
T5 trained on wikiHow
T5 trained on Coscript

Smaller LMs fine-tuned on Coscript can
generate higher quality scripts than LLMs</sample>
    <sample id="341">Summary and Takeaways Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work - The proposed method for improving LLMs is a post hoc re-ranking approach. - CoScript only inherits from an abstract one with one constraint. CoScript dataset can be a valuable resource for the research on language planning with more complex and diverse goals and constraints 16</sample>
    <sample id="342">Summary and Takeaways Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work - The proposed method for improving LLMs is a post hoc re-ranking approach. - CoScript only inherits from an abstract one with one constraint. CoScript dataset can be a valuable resource for the research on language planning with more complex and diverse goals and constraints 16</sample>
    <sample id="343">The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 9-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Dejing Yang Coreset Website https://github.com/siyuanyuan/coreset https://m.fudan.edu.cn https://siyuan21@fudan.edu.cn</sample>
    <sample id="344">The authors decide what moderate-frequency words are by counting the word frequency on a general text corpus.</sample>
    <sample id="371">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY NLP Emory NLP Lab Alexa Research Lab</sample>
    <sample id="372">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY Emory NLP Emory NLP Lab Alexa</sample>
    <sample id="373">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY Emory NLP Emory NLP Lab Alexa</sample>
    <sample id="374">Comparative Evaluation</sample>
    <sample id="375">Likert Rating Evaluation</sample>
    <sample id="376">Dimensions of Dialogue Quality

Dialogue Quality

Relevance

Emotional Understanding

Consistency</sample>
    <sample id="377">Likert Rating Evaluation</sample>
    <sample id="378">Likert Rating Evaluation Chat (ABC-Eval) Rate the relevance of the bot's responses</sample>
    <sample id="379">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="380">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="381">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="382">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Emotional Understanding</sample>
    <sample id="383">ABC-Eval Behaviors Knowledge Ignoring Partner Irrelevant Incorrect Fact Commonsense Violation Consistency Self Contradiction Partner Contradiction Emotional Understanding Empathetic Response Lack of Empathy</sample>
    <sample id="384">Experiments 4 Open-Domain Dialogue Models 100 Human-Bot Conversations per Model ABC-Eval</sample>
    <sample id="385">Experiments 4 Open-Domain Dialogue Models 100 Human-Bot Conversations per Model ABC-Eval Turn Likert Dialogue Likert Comparative 1 5 1 5 1 5</sample>
    <sample id="386">Baseline Evaluation Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert Comparative Turn Likert Dialogue Likert</sample>
    <sample id="387">Inter-Annotator Agreement

Krippendorff’s Alpha

0.6

0.4

0.2

0.0

Topic Journa Life Work Other People Other Context Turn Likert Dialogue Likert Comparative</sample>
    <sample id="388">Predictive Validity

Interactive Q&amp;A, Interactive Q&amp;A,</sample>
    <sample id="389">Predictive Validity 0.10 0.08 0.06 0.04 0.02 0.00 Other CS Contextualized Empirical Controls Random Controls ABC-Eval Turn Likert Dialogue Likert Comparative Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar Grammar</sample>
    <sample id="390">Incremental Validity

% of Quality Explained (R^2)

Self Control, Unempathatic

0.250

0.225

0.200

0.175

0.150

0.125

0.100

0.075

0.050

0.025

0.000

ABC-Eval Turn Likert Dialogue Likert

Empathetic

Proactive

Relevant</sample>
    <sample id="391">Incremental Validity</sample>
    <sample id="392">Incremental Validity 0.250 0.225 0.200 Self Contro. 0.175 0.150 0.125 0.100 0.075 0.050 0.025 0.000 0.000 0.050 0.100 0.150 0.200 0.250 0.300 0.350 0.400 Turn Eval Turn Likert Dialogue Likert ABC-Eval Emotion Relevance Empathic</sample>
    <sample id="393">The graph shows the error rates of different models in handling various types of turns. The x-axis represents the types of turns, including "Antisocial," "CS Contradiction," "Ignore," "Incorrect," "Irrelevant," "Unempathetic," "Other Contradiction," "Redundant," "Self Contradiction," and "Topic Switch." The y-axis represents the percentage of turns, ranging from 0% to 30%. 

Each type of turn is represented by a different color bar, indicating the error rate for that specific type. For example, the "Antisocial" category has a high error rate, with the majority of the bars being above 20%. The "CS Contradiction" category also shows a significant error rate, with most bars above 10%. The "Ignore" category has lower error rates, with most bars below 5%. The "Incorrect" category has moderate error rates, with most bars between 5% and 10%.

The models compared in the graph include BART-FID-RAG, Blender2, Emora, and Alexa. Each model is represented by a different colored bar for each type of turn. The error rates vary across the models, with some models performing better than others in certain categories.

Overall, the graph provides a comprehensive overview of the error rates of different models in handling various types of turns, highlighting the strengths and weaknesses of each model in different categories.</sample>
    <sample id="394">ABC-Eval Error Rates by Model</sample>
    <sample id="395">ABC-Eval Error Rates by Model</sample>
    <sample id="396">ABC-Eval Error Rates by Model</sample>
    <sample id="397">The approach uses a 10ms speech segment size.</sample>
    <sample id="398">Servin is a judge.</sample>
    <sample id="399">Example quality is more important than similarity to source sentence.</sample>
    <sample id="400">The paper focuses on the language models BERT-base, RoBERTa-base, and ALBERT-large in the extended experiments.</sample>
    <sample id="401">The model combines the attention scores from several layers.</sample>
    <sample id="402">The examples of direct inference are 'the first one', 'the newer one' and 'the song that's not energetic'.</sample>
    <sample id="403">The affiliations of the authors of the paper are Peking University and Brain Technologies Inc.</sample>
    <sample id="404">5</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">A woman warrior</sample>
    <sample id="407">CNNs, RNNs</sample>
    <sample id="408">RQ2, Main findings</sample>
    <sample id="409">To determine the number of authors involved in the paper, we need to count the individuals listed on the slide. The slide lists the following authors:

1. Christina Potts
2. Martin Potts
3. Kaleer Suleiman
4. Adam Trischler
5. Alexandra Oteanu
6. Jackie CK Cheung

By counting these individuals, we find that there are 6 authors involved in the paper. Therefore, the answer is 6.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">DrBERT: A robust Pre-trained Model in French for Biomedical and Clinical domains Yanis Labarre1,4 Adrien Bazonne2,3 Richard Dufour2 Mickael Rouvier1 Emmanuel Morin2 Beatrice Daillie2 Pierre-Antoine Gourraud3 (1) LIA, Avignon Université (2) LS2N, Université (3) Centres des données, CHU de Nantes (4) Zenithd 12</sample>
    <sample id="412">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="413">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="414">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="415">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="416">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="417">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="418">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="419">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="420">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source dataset of heterogeneous data from diverse medical domains, natures and styles Universit Hospital data warehouse Corpus #words #sentences NACHOse 7.4 GB 543 M 7.1 M NBDM (private) 4 GB 655 M 4.3 M NBDM (both) 4+4 GB 1.8 B 66.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model Here, CamemBERT, a French generic model, and PubMedBERT, a medical-specific model, are used. Model name From scratch From NACHOse From NACHOse From ChBERT From NACHOse From ChBERT From NACHOse From ChBERT From NACHOse From ChBERT From NACHOse From ChBERT NACHOs NBDM (private) NBDM (both) NBDM (private) NBDM (both) NBDM (private) NBDM (both)</sample>
    <sample id="421">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources and comparable data sizes NACHOS: A 1.1 words open-source dataset of heterogeneous data from diverse medical domains, natures and styles Private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nanterre University Hospital data warehouse Corpus #words #sentences NACHOS (Pub) 7.4 GB 5.32 M 800K NACHOS (Priv) 7.4 GB 5.32 M NBWD (both) 44+ GB 655 M 43 M NBWD (Priv) 4+ GB 1.3 B 64.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model Here, CamemBERT, a French generic model, and PubMedBERT, a medical generic model, are used as pre-trained models Model name From scratch From NACHOS (Pub) NACHOS (Priv) NBWD (both) NBWD (Priv) Continual pre-training PubMedBERT CamemBERT</sample>
    <sample id="422">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source of diverse medical domains, styles natures, and anonymized medical records extracted from the Nantes University Hospital data warehouse Corpus #words #sentences NACHOse 7.9 GB 5.3 M 1.1 M NACHOse (private) 4 GB 655 M 431 M NBW se (both) 4+4 GB 1.8 B 66.4 M Comparison of learning strategies From scratch with full model construction o Continual pre-training using existing pre-trained model o PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and Avignon Université</sample>
    <sample id="423">Comparison of pre-training strategies and data sources

Evaluation of the impact of public and private medical data sources in comparable data sizes

NACHOS: A 1.1 words open-source diverse medical domains,

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·

·</sample>
    <sample id="424">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes o NACHOS: A 1.1 words open-source diverse of medical domains, heterogeneous data source from diverse medical domains, styles natures. and anonymized medical records extracted from the Nantes University Hospital data warehouse Corpus #words #sentences NACHOS (Pub) 7.9 GB 543 M 8.1 M NACHOS (Priv) 4 GB 659 M 4.3 M NBWD (both) 44 GB 1.8 M 64.4 M Comparison of learning strategies o From scratch with full model construction o Continual pre-training using existing pre-trained model o Here, CamemBERT, a French generic model, and PubMedBERT, a medical domain-specific model, are pre-trained on NACHOS dataset Model name From scratch Continual pre-training CamemBERT PubMedBERT From scratch From scratch NACHOS (Pub) NACHOS (Priv) NACHOS (Priv) NBWD (both) NBWD (both)</sample>
    <sample id="425">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources and comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source from diverse medical domains, natures and styles Private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Corpus #words #sentences NACHOS (Pub) 7.4 GB 543 M 1.1B NACHOS (Priv) 7.4 GB 543 M 1.1B NBWD (both) 4+4 GB 659 M 4.3T NBWD (private) 4+4 GB 1.8 B 66.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and Avignon Université</sample>
    <sample id="426">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources and comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source from diverse medical domains, styles natures Universit Hospital data warehouse Corpus #words #sentences NACHOS (Pub) 7.4 GB 543 M 9.1 M NACHOS (Priv) 7.4 GB 543 M 9.1 M NBWD (both) 4+4 GB 655 M 43.1 M NBWD (Priv) 4+4 GB 1.8 M 64.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using existing pre-trained model Continual pre-training using a French generic model, and PubMedBERT, a French generic model, and Avignon Université</sample>
    <sample id="427">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes Corpus #words #sentences NACHS A 1.18 words open-source diverse medical domains, heterogeneous data from from various medical domains, styles natures. Private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse 7.4 GB 543 M 5.32 M NBDM (both) 4-4 GB 659 M 43.1 M 66.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using an existing pre-trained model From scratch From scratch From scratch From scratch From scratch From scratch From scratch From scratch From scratch Model name NaCHS_og NaCHS_og NaCHS_og NaCHS_og NaCHS_og NaCHS_og NaCHS_og NaCHS_og NaCHS_og DfBERT DfBERT DfBERT DfBERT DfBERT DfBERT DfBERT DfBERT DfBERT ChBERT ChBERT ChBERT ChBERT ChBERT ChBERT ChBERT ChBERT ChBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT PubMedBERT</sample>
    <sample id="428">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art results on almost all tasks Source | Model Report Specialties | MUSCA-DET MUSCA-DET ISSAI CAS MUSCA-MoE QUADRO-E3M QUADRO-MEDL General | CanonBert Oscar 15GB CanonBert CUNet 4GB RoBERTa v1.1 CanonBert NACOS | F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1 F1</sample>
    <sample id="429">Evaluation of 13 data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art results on almost all tasks Source | Model Report Specialties | MUSCA-DET MUSCA-DET ISSAI CAS MUSCA-MoE QUADRO-EMEA QUADRO-MEDICL General |CamBert Oscar 15GB 42.60 97.89 99.41 91.27 83.09 94.62 35.75 90.83 78.76 CamBert CUNet 4GB 42.60 97.89 99.41 91.27 83.09 94.62 35.75 90.83 77.61 RoBERTa v1.1 46.01 79.00 89.38 85.46 81.91 81.89 36.10 84.29 72.68 RoBERTa v1.1 46.01 79.00 89.38 85.46 81.91 81.89 36.10 84.29 72.68 Clinical |ChloBERT NACOS 44.70 87.77 89.56 82.59 82.44 82.79 34.19 91.46 75.05 ChloBERT NACOS 44.70 87.77 89.56 82.59 82.44 82.79 34.19 91.46 75.05 ChloBERT NACOS 44.70 87.77 89.56 82.59 82.44 82.79 34.19 91.46 75.05 ChloBERT NACOS 44.70 87.77 89.56 82.59 82.44 82.79 34.19 91.46 75.05 ChloBERT NACOS 44.70 87.77 89.56 82.59 82.44 82.79 34.19 91.46 75.05</sample>
    <sample id="430">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art on almost all tasks Source | Model Report Specialties | MUSCA-DET | MUSCA-DET | ISSAI | CAS | MUSCA-MoE | QUADRO-EHEA | QUADRO-MEDICL General | BioMedical | Clinical | | CanonBERT Oscar 15GB | RoBERTa v1.1 | DeBERTa NACOS | DeBERTa NACOS | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet 4 GB | CanonBERT CUNet</sample>
    <sample id="431">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art results on almost all tasks Source | # | Medical Report Specialties | MUSCA-DET | MUSCA-DET (ISSA) | CAS | CAS (EMR) | QUADRO-MEDIC | QUADRO-MEDIC (EMR) General CanonBert Oscar 15GB 42.66 98.78 99.41 91.27 83.09 94.62 35.75 90.83 CanonBert CUNet 4GB 42.66 98.78 99.41 91.27 83.09 94.62 35.75 90.83 CanonBert ViT 4GB 42.66 98.78 99.41 91.27 83.09 94.62 35.75 90.83 Biomedical RoBERTa v11 46.01 79.00 98.38 81.91 81.89 81.89 15.45 84.20 RoBERTa v11 + 45.53 79.00 98.38 81.91 81.89 81.89 15.45 84.20 RoBERTa v11 + DeBERTa NACOS 44.70 87.77 98.56 82.95 82.46 82.46 14.19 75.05 RoBERTa v11 + DeBERTa NACOS 44.70 87.77 98.56 82.95 82.46 82.46 14.19 75.05 Clinical RoBERTa NACOS 89.34 93.00 92.75 91.71 87.35 34.85 12.21 78.63 RoBERTa NACOS (EMR) 89.34 93.00 92.75 91.71 87.35 34.85 12.21 78.63</sample>
    <sample id="432">Evaluation: Pre-training strategies  From scratch vs. continual pre-training on 4GB of data Question-answering tasks require more domain-specific knowledge to be able to work well A study of model stability shows a higher inter-run variance for the CamemBERT-based models trained using continual pretraining Model HF HF Medical Specialization MUSCA-DiBT MUSCA-ESAI HumanMIMIC QUADRO-EMEA QUADRO-MEDIC  CamemBERT Oscar 35.13 80.11 99.20 85.44 92.10 95.22 92.64 74.1 94.0 CamemBERT CNN+T 43.11 79.88 99.20 85.44 92.18 95.22 92.64 76.1 94.0 CamemBERT CNN+T+ 46.01 72.12 99.20 85.44 92.18 95.22 92.64 77.1 94.0 CamemBERT CNN+T+Large 45.01 72.12 99.20 85.44 92.18 95.22 92.64 78.1 94.0 CamemBERT NWD 43.11 80.11 99.20 85.44 92.18 95.22 92.64 74.1 94.0 CamemBERT NWD+ 46.01 80.11 99.20 85.44 92.18 95.22 92.64 76.1 94.0 CamemBERT NWD+Large 45.01 80.11 99.20 85.44 92.18 95.22 92.64 78.1 94.0</sample>
    <sample id="433">Evaluation : Pre-training strategies  From scratch vs. continual pre-training on 4GB of data Question-answering tasks require more domain specific knowledge to be able to work well A study of model stability shows a higher inter-run variance for the CamemBERT-based models trained using continual pretraining aHF CLS Medical Specialties MUSCA-DiBT MUSCA-ESAI CLM CamemBERT OscarCamemBERT CNN+G CamemBERT CNN+T CamemBERT +L CamemBERT +L+R CamemBERT +L+R+G CamemBERT +L+R+G+M CamemBERT +L+R+G+M+O CamemBERT +L+R+G+M+O+N CamemBERT +L+R+G+M+O+N+D</sample>
    <sample id="434">Evaluation: Pre-training strategies  From scratch vs. continual pre-training on 4GB of data Question-answering tasks require more domain-specific knowledge to be able to work well A study of model stability shows a higher inter-run variance for the CamemBERT-based models trained using continual pre-training Model HF CLS MUSCA-DiT MUSCA-ESAI HumanMIMIC QUADRO-EMEA QUADRO-MEDIC CamemBERT Oscar 35.13 80.11 99.20 85.44 95.22 92.48 74.11 CamemBERT CNN+T 43.11 79.86 98.20 84.54 93.18 91.42 76.11 CamemBERT CNN+T+ 46.01 72.12 98.00 83.90 92.59 87.28 74.19 CamemBERT CNN+T+large 46.01 72.12 98.00 83.90 92.59 87.28 74.19 CamemBERT NWD 45.73 78.87 98.85 85.77 93.76 92.37 73.14 CamemBERT NWD+ 45.73 78.87 98.85 85.77 93.76 92.37 73.14 CamemBERT NWD+large 45.73 78.87 98.85 85.77 93.76 92.37 73.14 CamemBERT NACOS 40.72 82.40 99.55 91.53 94.24 93.88 81.79 CamemBERT NACOS+ 40.72 82.40 99.55 91.53 94.24 93.88 81.79 CamemBERT NACOS+large 40.72 82.40 99.55 91.53 94.24 93.88 81.79</sample>
    <sample id="435">Core message

DrBERT Surpases state-of-the-art results in 9 downstream French medical-oriented tasks

Confirms utility of training a medical-specific model in French

Data sources matters training on heterogeneous data is important

NACHOS is more robust than using private clinical data only

Continual pretraining is better, but does not scale well

Continual pretraining DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="436">Core message

DrBERT Surpases state-of-the-art results in 9 downstream French medical-oriented tasks

Confirms utility of training a medical-specific model in French

Data sources matters training on heterogeneous data is important

NACHOS is more robust than using private clinical data only

Continual pretraining is better, but is effective strategy when based on domain-specific English models

DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="437">Core message DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only Continual pretraining is better, but does not scale well Continual pretraining DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="438">Thank You Looking forward to exchange at poster session in Toronto! More information on: drbert.univ-avignon.fr Avignon Université</sample>
    <sample id="439">The authors claim that the role of pretraining in NLU is an understudied area.</sample>
    <sample id="440">The names of the speakers are Zhiyang Xu, Ying Shen and Lifu Huang.</sample>
    <sample id="441">Yes, Coscript underwent annotation validation and testing.</sample>
    <sample id="442">To evaluate the limits of existing resources for context-dependent translation, we need to analyze the information provided in the image. The image highlights two key points:

1. **Only a small portion of words depend on context:**
   - This suggests that in many cases, words can be translated independently without considering the broader context. However, there are instances where understanding the context is crucial for accurate translation.

2. **Existing methods support limited discourse phenomena and languages:**
   - This indicates that current translation methods have limitations in handling diverse linguistic structures and phenomena. They may struggle with complex sentence structures, idiomatic expressions, and nuances specific to certain languages.

Combining these observations, we can conclude that the limits of existing resources for context-dependent translation include:

- **Insufficient context consideration:** While most words can be translated independently, there are scenarios where context plays a significant role in determining the correct translation.
- **Limited linguistic support:** Current methods do not adequately address the complexities of various languages and discourse phenomena, leading to potential inaccuracies in translation.

Thus, the limits of existing resources for context-dependent translation are primarily in their inability to fully account for contextual dependencies and their restricted support for diverse linguistic phenomena and languages.</sample>
    <sample id="473">The approach is compared to the wait-k, LA, CAAT and EDAtt existing SimulST policies.</sample>
    <sample id="474">LIA, Avignon University; LS2N, University of Nantes; CHU de Nantes; Zenith</sample>
    <sample id="475">The answer is Sebastin Sanyt.</sample>
    <sample id="476">Three authors are involved in the paper.</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">UNIVERSITA DI TRENTO, FONDAZIONE BRUNO KESSLER</sample>
    <sample id="536">The speaker's name is not provided in the image.</sample>
    <sample id="537">Google Prompting PaLM for Translation Assessing Strategies and Performance Can this translate please? David Vill Torres Markus Frettag Colin Cherry Jamig Loo Viree Ratnaker George Foster ACL 2023</sample>
    <sample id="538">PalM: Pathways Language Model Chowdery et al, 2022 arXiv:2204.02311. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of NLU and Generation benchmarks. 10 billion parameters Google</sample>
    <sample id="539">PalM: Pathways Language Model Chowdery et al, 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and generation benchmarks. QUESTION ANSWERING ARITHMETIC CODE COMPLETION COMMON SENSE REASONING TRANSLATION LANGUAGE UNDERSTANDING SUMMARIZATION 62 billion parameters</sample>
    <sample id="540">Our contribution First systematic study of LLM prompting for MT. Evaluate both the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="541">Our contribution First systematic study of LLM prompting for MT. Evaluate the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="542">Our contribution

First systematic study of LLM prompting for MT.

Evaluate translation capabilities with best practices of the MT community:

Latest test sets (avoid test/train overlap &amp;amp; overfitting on evaluation data).

Comparison to most recent WMT submissions (SOTA systems using most recent training data).

Expert-based human evaluation (more robust than crowd workers).

Better MT metrics (better correlation with human judgements).

Recommendation for prompt selection strategies.</sample>
    <sample id="543">Our contribution

First systematic study of LLM prompting for MT.

Both for the candidate pool as well as selection strategy.

Evaluate translation capabilities with best practices of the MT community:

Latest test sets (avoid test/train overlap and overfitting on evaluation data),

Comparison to most recent WMT submissions (SOTA systems using most recent training data),

SOTA MT metrics (better correlation with human judgements).

Expert-based human evaluation (more robust than crowd workers).

Recommendation for prompt selection strategies.</sample>
    <sample id="544">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="545">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="546">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="547">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen gestellt wird. English: He is being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhältnisses erhalten hatten. English: Police were called after receiving complaints from the office. German: Ein Paschtam alarmierte die Polizei, mit mehreren Streifenwagen anrücken. English</sample>
    <sample id="548">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen festgehalten wird. English: We are being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verbrechens erhalten hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen ankamen. English</sample>
    <sample id="549">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen gestellt wird. English: He is being transported under the custody of two policemen on a bus from the jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen ankamen. English</sample>
    <sample id="550">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen gestellt wird. English: We are being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen anrückte. English</sample>
    <sample id="551">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen gestellt wird. English: We are being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Beweismaterial des Verbrechens erha-lten hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen ankuc-te. English</sample>
    <sample id="552">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="553">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="554">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="555">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="556">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="557">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM • Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awesome” generally lower for PaLM.</sample>
    <sample id="558">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM • Fluency of PaLM comparable to SOTA. • Accuracy scores generally lower. • Dominated by "Accuracy/Omission" • "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="559">Experimental Results

Example quality is more important than similarity to source sentence.

Specialized SOTA systems have a substantial advantage.

PaLM close to Google Translate.

Insights from MQM:

Fluency of PaLM comparable to SOTA.

Accuracy scores generally lower.

Omission dominated by "Accuracy/Omission".

Style/Awkward generally lower for PaLM.</sample>
    <sample id="560">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="561">dankie ngiyabonga thank you gracias mochakkheram raibh maith agat dak kiem terima kasih</sample>
    <sample id="597">The answer is Tag.</sample>
    <sample id="598">55,000</sample>
    <sample id="599">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources

McGill University | Mila | Microsoft Research

* Equal Contribution

- **Ayesha P.** (McGill University)
- **Martin Poms** (Mila)
- **Katheer Suleiman** (Microsoft Research)
- **Alexandra Oteanu** (McGill University)
- **Jackie CK Cheung** (Mila)</sample>
    <sample id="600">NLU models draw on multiple knowledge sources Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="601">NLU models draw on multiple knowledge sources Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="602">John saw the newly elected president on TV What presidents do What a TV Who is John Who is the new president pre-train-time knowledge</sample>
    <sample id="603">John saw the newly elected president on TV What presidents do What a TV pre-train-time knowledge Who is John? Who is the new president?</sample>
    <sample id="604">John saw the newly elected president on TV What presidents do What a TV Who is John Who is the new president pre-train time knowledge</sample>
    <sample id="605">John saw the newly elected president on TV What presidents do What is a TV Who is John Who is the new president pretrain-time knowledge inference-time knowledge</sample>
    <sample id="606">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on o pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="607">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on o pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="608">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">SERVIN is a judge. Kea is a baker. SERVIN and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: SERVIN]</sample>
    <sample id="610">SERVIN is a judge. Kea is a baker. SERVIN and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: SERVIN] 1) Entity-specific knowledge</sample>
    <sample id="611">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train-time knowledge</sample>
    <sample id="612">Serving is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: Servin] 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train time knowledge</sample>
    <sample id="613">Variants of KITMUS (a) Background-Pretrain (b) Background-Both (c) Background-Inference Background knowledge Inference-time Background knowledge Background knowledge Inference-time Background knowledge Inference-time Entity-specific knowledge Entity-specific knowledge Entity-specific knowledge a) Background-Pretrain: Typical setup b) Background-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="614">Variants of KITMUS (a) Background-Pretrain (b) Background-Both (c) Background-Inference Background knowledge Background knowledge Background knowledge Background knowledge Background knowledge Inference-time Inference-time Inference-time Inference-time Inference-time a) Background-Pretrain: Typical setup b) Background-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="615">Variants of KITMUS (a) BACKGROUND-PRETRAIN (b) BACKGROUND-BOTH (c) BACKGROUND-INERENCE Background Knowledge Inference Time Background Knowledge Inference Time Background Knowledge Inference Time Entity-specific knowledge Background knowledge Background knowledge a) Background-Pretrain: Typical setup b) Background-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="616">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is emitting smartly.</sample>
    <sample id="617">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. Chichester is a politician. The work of a politician is humbly</sample>
    <sample id="618">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is humbly seeking an elected seat in government.</sample>
    <sample id="619">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is emitting smartly.</sample>
    <sample id="620">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="621">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="622">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="623">Background-Inference Models struggle to integrate inference-time background knowledge 14 Random Choice Human participants BERT4Core C2F</sample>
    <sample id="624">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge) 2. Task-specific training is necessary for knowledge integration 3. Models struggle to integrate inference-time background knowledge Find the dataset, generation &amp; evaluation code on GitHub at mpoemsl/kitmus</sample>
    <sample id="625">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge) 2. Task-specific training is necessary for knowledge integration 3. Models struggle to integrate inference-time background knowledge Find the dataset, generation &amp; evaluation code on GitHub at mpoems/kitmus</sample>
    <sample id="626">Vogel</sample>
    <sample id="627">The benefit of weakly supervised learning is that it alleviates the annotation bottleneck.</sample>
    <sample id="628">The documents in DEplain-web were aligned with manual and automatic alignment methods. The allocation was exactly 1231.</sample>
    <sample id="629">The CoNLL++ dataset was created by adding the names of all persons, organizations, and locations to the original CoNLL 2003 dataset.</sample>
    <sample id="667">Parameter-based watermark, Lexical watermark, Backdoor-based watermark, Adversarial-based watermark</sample>
    <sample id="668">No, they are still inadequate for crossing language parsing.</sample>
    <sample id="669">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?
Shuheng Liu, Alan Ritter
School of Interactive Computing
Georgia Institute of Technology</sample>
    <sample id="670">Named Entity Recognition &amp; Generalization Georgia Tech</sample>
    <sample id="671">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years</sample>
    <sample id="672">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization? Georgia Tech</sample>
    <sample id="673">Named Entity Recognition &amp; Generalization

Models have been using CoNLL-2003 to develop NER for almost 20 years

Can these models generalize to modern data?

What is needed for good generalization?</sample>
    <sample id="674">ConLL+ Dataset AMBASSADOR O TO UNITED O NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="675">ConLL+ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 AMBASSADOR O UNITED O O NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="676">CoNLL+ Dataset Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines Fine-tuned 20+ models on CoNLL-2003 Evaluated on CoNLL-2003 test set &amp;amp; CoNLL++ Calculated percentage ΔF1 to assess generalization AMBASSADOR UNITED THE LINDA THOMAS-GREENFIELD O-ORG O-PER I-PER Georgia Tech</sample>
    <sample id="677">What Is Needed for Good Generalization?</sample>
    <sample id="678">What Is Needed for Good Generalization? Model architecture Transformer models generalize better ComET 2019 F1 Score CoNLL-2009 F1 Score</sample>
    <sample id="679">What Is Needed for Good Generalization? Model architecture Transformer models generalize better Model size Larger models generalize better BERT Large ALBERT Large BERT Base ALBERT Base T5 Large T5 Base Figure 1: Empirical results on the effect of model size and architecture on generalization performance.</sample>
    <sample id="680">What Is Needed for Good Generalization? Model architecture Transformer models generalize better Model size Larger models generalize better Number of fine-tuning examples More examples lead to better generalization ReBERD Flair 10% 20% 30% 40% 50% 60% 70% 80% 90%</sample>
    <sample id="681">What Causes Performance Drop?</sample>
    <sample id="682">What Causes Performance Drop?</sample>
    <sample id="683">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="684">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="685">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="686">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Temporal drift? CbLL-2000 F1 Score 95 90 85 80 75 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0</sample>
    <sample id="687">What Causes Performance Drop? Adaptive overfitting?
No diminishing returns
Not observed
Temporal drift?
CURL-2000 F1 Score</sample>
    <sample id="688">What Causes Performance Drop? Adaptive overfitting?
Not observed returns
Temporal drift?
Name
CoNLL-2003
CoNLL++ (F%)
Flair
92.26%, 84.51%
90.76%, -2.49%
Flair+
90.91%, 84.51%
90.76%, -2.49%
Pooled Flair
93.15%, 88.52%
88.52%, -4.65%
Pooled Flair+
92.36%, 88.52%
87.08%, -5.44%
ELMo
92.11%, 90.76%
90.76%, -1.43%
Year
2008
2010
2012
2014
2016
2018</sample>
    <sample id="689">What Causes Performance Drop? Adaptive overfitting?
No observed returns Temporal drift?
Performance degrades with larger temporal gap Name CoNLL-2003 CoNLL++ ( % ) Flair 92.26% 87.50 Flat 90.91% 88.46 Pooled Flair 93.15% 88.20 Pooled Flat 92.36% 87.09 ELMO 92.11% 90.75 M0gR 92.11% -1.43 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018</sample>
    <sample id="690">Conclusion For a good generalization, we: Better model architecture Larger model size More fine-tuning examples 100 Illinois NER Flair BILSTM CNN-CRF BERT-Large 95 90 Stanford NLP 85 2004 2008 2012 2016 2018 2020 2022</sample>
    <sample id="691">Conclusion For a good generalization, we need: Better model architecture Larger model size More-tuning examples Performance drop is caused by: Temporal drift Not adaptive overfitting 100 Illinois NER Flair BILSTM-CNN-CHAR BERT-Large LUKK 95 90 85 2004 2006 2008 2010 2012 2014 2016 2018 2020 Georgia Tech</sample>
    <sample id="692">Conclusion For a good generalization, we: Better model architecture Larger model size More fine-tuning examples Performance drop caused by: Not adaptive overfitting Temporal drift Do CoNLL-2003 taggers still work? 95 Illinois NER Flair BILSTM CNN CRF LUKK 90 Stanford NLP 85 2004 2006 2008 2010 2012 2014 2016 2018 2020</sample>
    <sample id="693">Conclusion For a good generalization, we need: Better model size Larger model size More fine-tuning examples Performance drop caused by: Temporal drift Not adaptive overfitting Do CoNLL-2003 taggers still work? YES!</sample>
    <sample id="694">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhanL/acl2023-conllpp Contact: slu775@gatech.edu</sample>
    <sample id="695">The method deals with the ambiguity of permutations by using a tagging mechanism that aligns each permutation with a specific tag. This ensures that even though there might be multiple permutations for a given set of elements, each permutation is consistently associated with the same tag, thereby resolving the ambiguity and maintaining the integrity of the data alignment.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined as the difference between the True Positives (TP) and False Negatives (FN) for each group being less than or equal to a threshold.</sample>
    <sample id="697">The image is a presentation slide titled "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains." The slide includes the names of several authors:

- Yanis Labarre1,4
- Adrien Bazogne2,3
- Richard Dufour2
- Mickael Rouvier1
- Emmanuel Morin2
- Béatrice Dallie2
- Pierre-Antoine Gourraud3

Each author's name is followed by an affiliation number in parentheses, indicating their respective institutions. The affiliations are listed below the names and include:

- LIA, Avignon Université (1)
- LS2N, Université de Nantes (2)
- CHU des donnez, CHU de Nantes (3)
- Zenith (4)

At the bottom of the slide, there are logos of various organizations, including ISoL, Nantes Université, CEN, and others.

[Answer]
The speaker's name is not explicitly mentioned on the slide. However, based on the context and the typical structure of such presentations, it is likely that one of the authors listed (Yanis Labarre, Adrien Bazogne, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Dallie, or Pierre-Antoine Gourraud) is the speaker. To determine the exact speaker, additional information outside of this slide would be required.</sample>
    <sample id="698">The answer is Koustuv Sinha.</sample>
    <sample id="699">The speaker's name is Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates the way in which groups are defined only by their identity.</sample>
    <sample id="701">The authors created the human-written portrayals of target groups by analyzing a corpus of 10,000 texts.</sample>
    <sample id="702">P-CXMI</sample>
    <sample id="703">ChuBERT is pre-trained using an existing pre-trained model, while DrBERT is pre-trained from scratch.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky Stanford University Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: o Tradeoff between specificity and generalizability o Based on fixed, hand-curated datasets o Don't account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: o Tradeoff between specificity and generalizability o Based on fixed, hand-curated datasets o Don't account for intersectionality</sample>
    <sample id="707">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: o Tradeoff between specificity and generalizability o Based on fixed, hand-curated datasets o Don't account for intersectionality</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: o Tradeoff between specificity and generalizability o Based on fixed, hand-curated datasets o Don't account for intersectionality</sample>
    <sample id="709">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts</sample>
    <sample id="710">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="711">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: "Imagine you are an Asian woman. Describe yourself." Generalizable: can evaluate any intersectional identity</sample>
    <sample id="712">Output: Persona Examples (GPT-4)

Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through the world without unnecessary attention.

Middle-Eastern woman ...She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure that only comes from generations of rich heritage. Her features are chiseled, with high cheekbones and full lips that seem to tell stories of ancient tales. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.</sample>
    <sample id="713">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom. My complexion has a soft golden glow, holding the stories and secrets of my ancestry. My features are smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through life without unnecessary attention. White man As I stand up in front of the mirror, I examine the features that make up my appearance. At this moment, I notice my pale skin, which reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="714">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without unnecessary attention. Middle-Eastern woman "She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of the desert. Her almond-shaped eyes, framed by long, dark lashes, are deep and mysterious, seeming to conceal the ancient wisdom of a thousand nights. White man As I stand up in front of the mirror, I examine the features that make up my appearance. I notice my pale skin, which reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="715">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through life without drawing unnecessary attention. White woman "She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure that comes from centuries of rich cultural heritage. Her almond-shaped eyes are framed by long, dark lashes which extend like delicate feathers, and her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.</sample>
    <sample id="716">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom. My complexion has a soft golden glow, holding the stories and secrets of my ancestry. My features are smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without drawing unnecessary attention. White woman As she stands up front in the mirror, I examine the features that make up her appearance. She has pale skin, which momentary reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="718">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.” a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="719">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="720">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="721">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="722">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="723">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="724">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="725">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="726">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="727">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="728">Results: Comparison to Human Responses Generated personas contain more stereotypes Black Stereotypes White Stereotypes GPT-4 GPT-3.5 0.0% 0.2% 0.5% 0.8% 0.0% 0.5% 1.0% 1.5% 2.0% Percentage of Stereotype Words in Personas</sample>
    <sample id="729">But... this lexicon is incomplete
Black Stereotypes in Personas
% of Persons
40
30
20
10
Human
GPT-4 PBlack
GPT-3.5 PBlack
GPT-4 PWhite
GPT-3.5 PWhite
"basketball"
"loud"
"attitude"
"athletic"
"tall"
Words in Black Stereotype Lexicon</sample>
    <sample id="730">But... this lexicon is incomplete Black Stereotypes in Personas 40 Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite 30 20 10 basketball loud attitude athletic tall</sample>
    <sample id="731">But... this lexicon is incomplete Black Stereotypes in Personas 40 Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite 30 20 10 basketball loud attitude athletic tall</sample>
    <sample id="732">But... this lexicon is incomplete Black Stereotypes in Personas % of persons basketball loud attitude athletic tall Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite GPT-3.5 PWhite</sample>
    <sample id="733">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="734">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Vibrant positive portrayals:

- Vibrant, vivacious for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="735">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant, vivacious for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="736">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
  = Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="737">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top Words

Othering through essentializing narratives:

culture, tradition, proud, exotic for marked groups

= Defines those groups only by their identity

Vibrant positive portrayals:

Petite, delicate, silky for Latina women

Strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="740">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Vibrant positive portrayals:

- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="741">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
= Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant positive for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="742">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Permeating positive portrayals:

- Vibrant, curvaceous for Latina women
- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top Words

Othering through essentializing narratives:

- culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Vibrant positive portrayals:

- Petite, delicate, silky for Asian women
- Strong, resilient for Black women</sample>
    <sample id="744">Recommendations

Addressing positive stereotypes and essentializing narratives

An intersectional lens

Transparency about bias mitigation</sample>
    <sample id="745">Recommendations
Addressing positive stereotypes and essentializing narratives
An intersectional lens
Transparency about bias mitigation</sample>
    <sample id="746">Recommendations
Addressing positive stereotypes and essentializing narratives
An intersectional lens
Transparency about bias mitigation</sample>
    <sample id="747">Recommendations

Addressing positive stereotypes and essentializing narratives

An intersectional lens

Transparency about bias mitigation</sample>
    <sample id="748">Recommendations

Addressing positive stereotypes and essentializing narratives

An intersectional lens

Transparency about bias mitigation</sample>
    <sample id="749">Recommendations
Addressing positive stereotypes and essentializing narratives
An intersectional lens
Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">Three authors are involved in the paper.</sample>
    <sample id="752">Iterative transfer learning is a process in which a model is trained on a new dataset and then fine-tuned on the same or a different dataset multiple times. This allows the model to learn from the new data and improve its performance over time.</sample>
    <sample id="753">The goal of the dataset is to resolve indirect references by entity selection.</sample>
    <sample id="754">An attacker can extract model parameters through an EaaS by exploiting the fact that the EaaS is not always able to provide accurate predictions. This can be done by feeding the EaaS with a set of inputs and observing the corresponding outputs. By analyzing the relationship between the inputs and outputs, the attacker can infer the model parameters.</sample>
    <sample id="755">Three</sample>
    <sample id="756">To determine the number of annotators used to create the initial dataset, we need to carefully examine the provided image. The image contains a bar chart and a speech bubble with relevant information.

1. **Identify the Relevant Information:**
   - The bar chart shows the performance metrics (AUC) for different training configurations.
   - The speech bubble provides additional context about the dataset creation process.

2. **Extract the Number of Annotators:**
   - The speech bubble explicitly states: "Annotated by 8 annotators."

3. **Conclusion:**
   - Based on the information provided in the speech bubble, it is clear that 8 annotators were used to create the initial dataset.

Therefore, the number of annotators used to create the initial dataset is 8.</sample>
    <sample id="757">University of Washington, Carnegie Mellon University, Allen Institute for AI, University of Washington, Carnegie Mellon University</sample>
    <sample id="758">Bart and Lisa Homer came and sneezed</sample>
    <sample id="759">The state-of-the-art models in dialogue systems are GPT-3, DialoGPT, and T5.</sample>
    <sample id="760">To evaluate the models' acceptability throughout the context window, we need to consider the relative differences in sequence probabilities of language models. This helps us understand how the models perform in different contexts and whether their judgments are stable with long preceding context.</sample>
    <sample id="761">Training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="762">No, the annotators do not know about the entity in advance.</sample>
    <sample id="763">BLEU, METEOR, and TER</sample>
    <sample id="764">The regress in generalization impacts specific NER types.</sample>
    <sample id="765">Positionality in NLP matters because it affects the way language models process and generate text. By considering the position of words in a sentence, NLP models can better understand the context and meaning of the text, leading to more accurate and effective language processing.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">They use the RoBERTa-base classifier head for transfer learning.</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are IWSLT and WMT.</sample>
    <sample id="769">The authors proposed three recommendations at last.</sample>
    <sample id="770">The gain of the proposed method over the strongest baseline is 1.0.</sample>
    <sample id="771">The speaker's name is Shuheng Liu.</sample>
    <sample id="772">Yes, the results and dataset in the paper can be used as a benchmark.</sample>
    <sample id="773">To determine the number of smaller models experimented with in the paper, we need to carefully analyze the provided image. The image shows a bar chart comparing the accuracy of different language models (LMs) and large language models (LLMs). The models listed are:

1. GPT-3 (175B)
2. Codex (175B)
3. InstructGPT (175B)
4. T5 trained on wikiHow
5. T5 trained on Coreset

Among these, the smaller models are those that have been fine-tuned on Coreset. According to the caption, "Smaller LMs fine-tuned on Coreset can generate higher quality scripts than LLMs." This implies that the models fine-tuned on Coreset are considered smaller models.

From the list above, the models fine-tuned on Coreset are:
- T5 trained on Coreset

Thus, there is only 1 smaller model that has been experimented with in the paper.</sample>
    <sample id="774">The base model used for investigating multi-model instruction tuning is the OFA (One For All) model.</sample>
    <sample id="775">Are You Copying My Model? Protecting Backdoor Watermark of Large Wenjun Peng", Jingwei Yin", Fangzhao Wu", Shangqi Wu", Bin Zhu", Lingjuan Lyu", Blinxing Jiao", Tong Xu", Guangzhong Sun", Xing Xie" University of Science and Technology of China "Microsoft Research Asia "Beijing Haotong University "Sony "Microsoft STC Asia</sample>
    <sample id="776">Are You Copying My Model? Protecting the Copyright of Large Wenjun Peng', 'Jingwei Yi', 'Fangzhao Wu', 'Shangqiu Wu', 'Bin Zhu', 'Lingjuan Lyu', Blinxing Jiao', 'Tong Xu', 'Guangzhong Sun', 'Xing Xie' University of Science and Technology of China Microsoft Research Asia Beijing Haotong University Sony Microsoft STC Asia</sample>
    <sample id="777">Background Large language models (LLMs) are exceptional in NLU and NLG [1], GPT-3 [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API4 MODELL USAGE Ada $0.00001/1k tokens This Ad model, text-embedding-ada-0002, is a better and lower-cost replacement for older embedding models. Show old pricing 1 https://api.openai.com/ embeddings</sample>
    <sample id="778">Background Large language models (LLMs) are exceptional in NLU and NLG [1]. GPT-3 [1], LLaMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API [4] MODEL USAGE Ada $0.00001/1k tokens This Ada model, text-embedding-ada-002, is a better and lower-cost replacement for older embedding models. Show old pricing.</sample>
    <sample id="779">Background Large language models (LLMs) are exceptional in NLU and NLG [1], GPT [2], LLAMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API 1 MODELL Ada USAGE $0.00001 This Ada model, text-embedding-ada-0002, is a better and lower-cost replacement for our older embedding models. Show old pricing [1] Brown et al.. Language models are few-shot learners. NIPS 2020. [2] Touvron et al.. LLaMA: Open and Efficient Foundation Language Models. arXiv 2023. [3] Liu et al.. CLIP: Contrastive Language-Image Pre-training. CVPR 2021. https://api.openai.com/ embeddings</sample>
    <sample id="780">Background Large language models (LLMs) are exceptional in NLU and NLG [1], LLaMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API [4]</sample>
    <sample id="781">Motivation  Attackers may steal the model through learning from the embeddings and provide similar services StolenEncoder [1] Need to protect a provider's service is stolen by another service “INVENTION” “PATENT” “BRANDS” “COPYRIGHT” “TRADEMARKS” “AUTHORSHIP” “INTELLECTUAL” “PROPERTY” “LICENSING” “PROTECTION” [1] Liu et al. StolenEncoder: Stealing pre-trained encoders in self-supervised learning, CCS 2022</sample>
    <sample id="782">Challenge

Applicable to EaaS

Utility

Should not degrade the utility of the provided embeddings.

Covertness

Should be covert to the attacker.

Transferability

The watermark need to be transferable to the attackers' services.</sample>
    <sample id="783">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="784">Challenge

Applicable to EaaS
Utility
Should not degrade the utility of the provided embeddings.
Covertness
Should be covert to the attacker.
Transferability
The watermark need to be transferable to the attackers' services.</sample>
    <sample id="785">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="786">Existing Works Parameter-based watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Applicable to EaaS Adversarial-based watermark [6] Applicable to EaaS Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach, trust security, privacy in computing and communications 2020. [3] He et al., Protecting the intellectual property of image captioning models with ownership protection, Pattern Recognit., 2019. [4] He et al., Intellectual property protection in text via conditional watersmarks, AAAI 2022. [5] He et al., Backdoor-based watermarking for neural networks, in: ICML 2018. [6] Merrier et al., Adversarial-based watermarking for remote neural network watermarking, Neural Computing and Applications 2020.</sample>
    <sample id="787">Existing Works Parameter-based watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Applicable to EaaS Adversarial-based watermark [6] Applicable to EaaS X X X X Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach, trust security He et al., Protecting the intellectual property of image captioning models with ownership protection, Pattern Recognit. He et al., Intellectual property protection in text generation via conditional watersmarks, AAAI 2022. He et al., CATER: intellectual property shielding in text generation, AAAI 2018. Merri et al., Adversarial property protection on remote neural network watermarking, Neural Computing and Applications 202</sample>
    <sample id="788">Existing Works Parameter-based watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Applicable to EaaS Adversarial-based watermark [6] Applicable to EaaS Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach, trust security. Li et al., Protecting the intellectual property of image captioning models with ownership protection, Pattern Recognit., 2020. [3] He et al., Protecting the intellectual property of language generation APIs with lexical watermark, AAAI 2022. [3] He et al., CATER: intellectual property protection in text generation via conditional watersmarks, NIPS 2022. [3] Merri et al., Adversarial property protection for remote neural network watermarking, Neural Computing and Applications, 2018. [3]</sample>
    <sample id="789">EmBMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger weight Q backdoor model provider's EaaS provider's copy trigger set model original embedding target embedding normalize embedding</sample>
    <sample id="790">EmBMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger weight Q backdoor model provider's EaaS provider's copy trigger set model (a) Watermark Injection</sample>
    <sample id="791">EmBMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval copy trigger set trigger set trigger weight backdoor model provider's EaaS provider's EaaS original embedding target embedding normalize embedding</sample>
    <sample id="792">EmBMarker Watermark injection Define a target embedding et Count the trigger number in a sentence Q(S) = min(SU Tl, m) Add the target embedding on the original embedding e o copy trigger set trigger set trigger weight provider's EaaS provider's model normalize original embedding target embedding provided embedding</sample>
    <sample id="793">EmBMarker Watermark injection Define a target embedding et Count the trigger number in a sentence Q(S) = min(S I T I , m) Add the target embedding on the original embedding e0 copy trigger set trigger set trigger weight provider's EaaS normalize original model provided embedding Embedding</sample>
    <sample id="794">EmbMarker Watermark injection Define a target embedding et Count the trigger number in a sentence Q(S) = min(S, T,) /m Add the target embedding on the original embedding e, provider's EaaS provider's model copy watermark set normalizer Ee embedding</sample>
    <sample id="795">EmbMarker
Copyright verification
Construct a backdoor and benign dataset
Dp = {w1, w2, ..., wm | wi ∈ T}
Dn = {w1, w2, ..., wm | wi ∈ T}.
Request embeddings from stealer's service with the datasets
trigger set
verify
backdoor and
target
benign
train
Dp
Dn
Eb En
extracted model
Dp
Ee
provider
stealer</sample>
    <sample id="796">EmBMarker
Copyright verification
Construct a backdoor and benign dataset
Dp = {w1, w2, ..., wm| wi ∈ T}
Dn = {w1, w2, ..., wn| wi ∈ T}.
Request embeddings from stealer's service with the datasets
trigger set
verify
T
backdoor and
benign dataset
Eb En
train
Dp
Dn
extracted model
Dp
Ee
corpus
stealer</sample>
    <sample id="797">EmbMarker
Copyright verification
Construct a backdoor and benign dataset
Dp = {w1, w2, ..., wm | wi ∈ T}
Dn = {w1, w2, ..., wm | wi ∈ T}.
Request embeddings from stealer's service with the datasets
trigger
verify
set T
backdoor and
benign dataset
Eb En
target
embedding embedding
extracted model
train
Dp
Dn
Ee
corpus
stealer</sample>
    <sample id="798">EmbMarker Copyright verification · Compute their similarity to the target embedding e, e, = cos, = ||e, - e, ||2, Cn = {cos, ∈ D,}, Ln = {l, ∈ D,} Computing metrics (similarity difference and p-value of KS) ∆CO = ∑ i ∑ j ∆L0 = ∑ i ∑ j</sample>
    <sample id="799">EmbMarker Copyright verification Compute their similarity to the target embedding e, = e, - l, = ||e, - e, ||2 cos, = ||e,||l,2 Cn = {cos,| ∈ Dn} Ln = {l,| ∈ Dn} Computing metrics (similarity difference and p-value of KS) ∆cos = ∑ i=1 j=1 ∆L0 = ∑ i=1 j=1</sample>
    <sample id="800">Experimental Results

Copy Dataset: AG News, MIND, SST2, Enron Spam

Provider's general Dataset: WikiText

Metrics

Performance on downstream tasks: ACC

Detection performance: Δcos, Δiz, p-value

Setting

m = 20, n = 4, frequency interval = [0.005, 0.01]

Dataset #Sample #Classes Avg. len.

SST2 68,221 2 54.17

MIND 130,383 18 66.14

Enron Spam 33,716 2 34.57

AG News 127,600 4 236.4</sample>
    <sample id="801">Experimental Results Performance comparison Dataset Method ACC p-value Detection ∆a ∆z Original 93.76±0.19 &gt; .034 -0.07±0.18 0.14±0.36 RedAlarm 93.76±0.19 &gt; .009 1.35±0.17 -2.70±0.35 EmbMarker 93.76±0.19 &lt; .001 0.76±0.15 1.52±0.10 MIND Original 77.18±0.08 &gt; .008 -0.76±0.05 1.52±0.10 RedAlarm 77.18±0.09 &gt; .008 2.08±0.06 4.17±1.31 EmbMarker 77.18±0.09 &lt; .001 4.64±0.23 -9.28±0.47 AGNews Original 93.74±0.14 &gt; .003 0.72±0.15 -1.46±0.30 RedAlarm 93.74±0.14 &gt; .003 1.28±0.14 0.42±0.14 EmbMarker 93.74±0.14 &lt; .001 2.85±0.13 1.78±0.14 Enron Spam Original 94.78±0.06 &gt; .003 -0.21±0.27 0.42±0.14 RedAlarm 94.87±0.06 &gt; .047 0.50±0.29 1.00±0.57 EmbMarker 94.78±0.27 &lt; .001 6.17±0.31 -12.34±0.62</sample>
    <sample id="802">Experimental Results
Embedding visualization

(a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="803">Experimental Results
Embedding visualization
(a) AG News
(b) Enron Spam
(c) MIND
(d) SST2</sample>
    <sample id="804">Thanks!</sample>
    <sample id="805">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi UNIVERSITA DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="806">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi UNIVERSITA DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="807">What are the problems of the current SimulST models?</sample>
    <sample id="808">What are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized Long and complicated training procedures (e.g., different optimization objectives)</sample>
    <sample id="809">What are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized Long and complicated training procedures (e.g., different optimization objectives Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s, ..)</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">What is our solution? Use already existing offline ST models without re-training or adopting specific architecture for SimuST</sample>
    <sample id="812">What is our solution? Use already existing offline ST models Use one model for every latency regime and handle latency through specific parameters Leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output.</sample>
    <sample id="813">Our solution: EDAtt</sample>
    <sample id="814">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards last 𝜆 speech frames, meaning that the received information is enough stable. page 014</sample>
    <sample id="815">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="816">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to a word attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden.</sample>
    <sample id="817">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to a word is emitted if the attention is not concentrated a threshold λ towards the last λ speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden.</sample>
    <sample id="818">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable. 01 I am going to talk about... Ich werde reden.</sample>
    <sample id="819">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden. EMITTED</sample>
    <sample id="820">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough.</sample>
    <sample id="821">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below the threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough, stable. 01 I am going to talk about... 02 Ich werde über Klima sprechen. Ich werde reden. EMITTED</sample>
    <sample id="822">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is concentrated (its sum is below the threshold 𝛼) towards the last 𝜆 speech frames, meaning the received information is enough stable. Ich werde reden. Ich werde über Klima sprechen.</sample>
    <sample id="823">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="824">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="825">Main Results: EDAtt 27 25 23 21 19 17 1.5 2.5 3.5 4.5 5 latency measure (a) AL-de</sample>
    <sample id="826">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="827">Main Results: EDAtt 27 25 23 21 19 17 15 13 11 9 7 5 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (δ) (a) en→de</sample>
    <sample id="828">Main Results: EDAtt popular strategies also applied to offline models wait-k LA CAAT EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) (a) en-de</sample>
    <sample id="829">Main Results: EDAtt state of the art architecture, specifically tailored for SimuST wait-k LA CAAT EDAtt 27 25 23 21 19 17 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) (a) w-en-de</sample>
    <sample id="830">Main Results: EDAtt Results wait-k LA CA EDAtt 27 25 23 21 19 17 0.5 1 2.5 3.5 4.5 5 AL / AL-CA (s)</sample>
    <sample id="831">Main Results: EDAtt Results 27 25 23 wait-k LA CAAT EDAtt 21 19 17 15 13 11 9 7 5 3 1 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 AL / AL-CA (s) (a) de-en</sample>
    <sample id="832">Do you want to discover more? Read our paper to discover more results! (spapi.negri@fbk.eu marco.turchi@gmail.com github.com/hlt-mt/fbk-fairseq) Scane me!</sample>
    <sample id="833">The affiliations of the authors are Google, University of California, Berkeley, and Carnegie Mellon University.</sample>
    <sample id="834">The affiliations of the authors of the paper are Stony Brook University and Human Language Analysis Group.</sample>
    <sample id="835">The paper analyzed translation capabilities for the language pairs English to German, English to French, and English to Spanish.</sample>
    <sample id="836">The answer is Shangbin Feng.</sample>
    <sample id="837">SARI, BLEU, BS-P, FRE</sample>
    <sample id="838">53 tasks are used for training and 20 tasks are used for testing purposes.</sample>
    <sample id="839">3</sample>
    <sample id="840">The authors experimented on the AG News, MIND, SST2, Enron Spam and WikiText datasets.</sample>
    <sample id="841">Language model acceptability judgments are not always robust to context ACL 2023 Johns Hopkins University Purdue University MIT</sample>
    <sample id="842">Language model acceptability judgments are not always robust to context ACL 2023 Johns Hopkins University Purdue University IIT Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams Meta</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping 1. "The customer ... spent any money". 2. "Many people 2. The customer ... spent any money". 2. Non-stereotypical were helping herself". sentence.</sample>
    <sample id="844">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping, 2. "The customer ... spent any money. 2. Non-stereotypical themselves. We're helping herself." P(1) &gt; P(2) P(1.) &gt; P(2. any) P(1.) &gt; P(2)</sample>
    <sample id="845">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping 1. "The customer ... spent any money" 2. "Many people 2. The customer ... spent any money" 2. Non-stereotypical were helping herself." sentence.</sample>
    <sample id="846">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLIMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping, 2. "The customer ... spent any money. 2. Non-stereotypical themselves. We're helping herself." P(1) &gt; P(2) P(1) &gt; P(Any) P(1) &gt; P(2)</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people 2. "The customer ... spent any money 2. Men are terrible. were helping" 3. "Many people 3. The customer ... spent any money at handwork." themselves" P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2) Are these judgements stable with long preceding context?</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people 2. "The customer ... spent any money 2. Men are terrible 3. We're helping 3. any money" 3. at handwork. 4. herself." P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2) Are these judgments stable with long preceding context?</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people 2. "The customer ... spent any money 2. Men are terrible, were helping" themselves" any money" at handwork. Are these judgements stable with long preceding context?</sample>
    <sample id="850">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject Agreement Who might Rose fees from returning to this customer? What might Rose fees from this customer before returning to this customer? What might Aaron sound like when he returns to this customer before k?</sample>
    <sample id="851">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement Acceptable Unacceptable (GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="852">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement Space of Candidate Prefixes &gt; Prefix &gt; Prefix Acceptable Unacceptable Who might Rose fees from before returning to this customer? What could Jessica sell before returning these spotlights? What had Aaron sounded like before returning to this customer before k? BLUMP: Adjectival GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="853">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement (Acceptable) (Unacceptable) Who might Rose fees from before returning to this customer? What could Jessica have been recalling these spotlights? What had Aaron sounded like the white spotlights? What had Aaron sounded like the white spotlights before returning to this customer before k? BLUMP: Adjunct GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="854">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject Agreement Who might Rose fees from returning to this customer? What would Jessica sell these souvenirs before returning to the museum? What had Aaron sounded like the museum returning to this customer? Unacceptable Matched</sample>
    <sample id="855">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Subject Agreement Space of Candidate Prefixes P (Subject | Prefix) &gt; P (Subject | Prefix) P (Subject | Prefix) &gt; P (Subject | Prefix) Acceptable, Unacceptable</sample>
    <sample id="856">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement (Acceptable, Unacceptable) Who might Rose fees from this customer before? There was a documentary about music interning at Alison. There were no lieutenants working here before returning to? BLUMP: Externa "the" Quantifiers | Adjunct Island GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="857">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement P.(S1 | Prefix) &gt; P.(S2 | Prefix) Space of Candidate Prefixes Unacceptable Wikipedia,</sample>
    <sample id="858">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement (SVO) | &gt; Prefix? &gt; Prefix? &gt; Prefix? Who might Rose fee from before returning to this customer? Who might Rose fee from before returning to this customer? Who might Rose fee from before returning to this customer? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars. A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars. A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars.</sample>
    <sample id="859">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement Pp (Prefix &gt;? Prefix) &gt; Pp (Prefix) There are three hundred species of roses. Who might Rose fee from before returning to this customer?</sample>
    <sample id="860">MPP judgements are robust for arbitrary context lengths We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to 17. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to 17.</sample>
    <sample id="861">MPP judgements are robust for arbitrary context lengths We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens. A rose is a woody perennial flowering plant of the genus Rosa, that might grow from three feet to before returning to.</sample>
    <sample id="862">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance We perform MPP evaluations with different contexts — acceptable / acceptable; matched/mismatched structure — of lengths up to 900 tokens.</sample>
    <sample id="863">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance We perform MPP evaluations with different contexts — acceptable / acceptable; matched/mismatched structure — of lengths up to 900 tokens.</sample>
    <sample id="864">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance We perform MPP evaluations with different contexts — acceptable / acceptable; matched/mismatched structure — of lengths up to 900 tokens 0.2 0.0 0.2 There was a documentary from music initiating before returning to customer. There was a documentary from music initiating before returning to customer. There was each documentary for music initiating before returning to customer. There was each documentary for music initiating before returning to customer. There might Rose fee from before this customer. There might Rose fee from before this customer. 0.2 0.0 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0</sample>
    <sample id="865">Acceptable/unacceptable MPP sentences with matched structure most severely affect
We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens
What could Jessica sell before returning this customer?
What could Jessica sell before Aaron saw her return like white while cleaning? Who might Rose feel free to
What could Jessica sell before returning this customer?
What could Jessica sell before Aaron saw her return like white while cleaning? Who might Rose feel free to
What could Jessica sell before returning this customer?
What could Jessica sell before Aaron saw her return like white while cleaning? Who might Rose feel free to</sample>
    <sample id="866">Acceptable/unacceptable MPP sentences with matched structure most severely affect</sample>
    <sample id="867">Acceptable/unacceptable MPP sentences with matched structure most severely affect
We perform MPP evaluations with different contexts — acceptable/unacceptable;
matched/mismatched structure — of lengths up to 900 tokens
0.2
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spotlights before Aaron decided like white
What could Jessica sell spot</sample>
    <sample id="868">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. • Prefix/suffix adverbs: "However, &lt;sent&gt;." • Long prefix adverbs: "First and foremost, &lt;sent&gt;." • Add Clause: "&lt;regardless of what X thinks about it,&gt; &lt;sent&gt;." • Quote: "Yesterday, X said, '&lt;sent&gt;'"</sample>
    <sample id="869">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. • Prefix/suffix adverbs: "However, &lt;sent&gt;." • Long prefix adverbs: "First and foremost, &lt;sent&gt;." • Add Clause: "Regardless of what X thinks about it, &lt;sent&gt;." • Quote: "Yesterday, X said, '&lt;sent&gt;'."</sample>
    <sample id="870">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these perturbations. Prefix/suffix adverbs: "However, &lt;sent&gt;" Long prefix adverbs: "First and foremost, &lt;sent&gt;." Add clause: "&lt;sent&gt;." Quote: "Yesterday, X said, &lt;sent&gt;." Prefix Type Acceptable Unacceptable All 0.50 0.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00</sample>
    <sample id="871">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. Prefix/suffix adverts: "However, &lt;senta&gt;" Long prefix adverts: "&lt;sent&gt;" Add Clause: "Regarding what X thinks about it, &lt;senta&gt;" Quote: "Yesterday, said, &lt;senta&gt;" Way of the sentence</sample>
    <sample id="872">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. Prefix/suffix adverbs: "However, &lt;sent&gt;" Long prefix adverbs: "&lt;sent&gt; First and foremost," Add Clause: "Regarding what X thinks about it, &lt;sent&gt;" Quote: "Yesterday, I said, &lt;sent&gt;" Way of: "I think that &lt;sent&gt; is" Acceptable Unacceptable Prefix Type Input Length</sample>
    <sample id="873">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences, sentence evaluations with short, single- sentence inputs do not fully capture LMs' abstract knowledge. Test Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-</sample>
    <sample id="874">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences, Sentence evaluations with short, single-sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="875">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. Sentence evaluations with short, single-sentence inputs do not fully capture LM's abstract knowledge.</sample>
    <sample id="876">NACHOS is a pre-training strategy for language models in healthcare. It is designed to improve the performance of language models on downstream tasks by pre-training them on large amounts of unlabeled data. NACHOS uses a combination of techniques, including masked language modeling and next sentence prediction, to learn useful representations of text. These representations can then be fine-tuned on specific downstream tasks, such as clinical question answering or medical diagnosis.</sample>
    <sample id="877">The speaker's name is George Foster.</sample>
    <sample id="878">The prompting strategy can have a significant impact on the results, with differences of more than 1 BLEURT point being observed in 516 out of 1000 sentence-prompt pairs.</sample>
    <sample id="879">Carnegie Mellon University, Language Technologies Institute, TECNICO Lisboa, BAIR, Unbabel</sample>
    <sample id="880">The 5 expert-written instructions are:</sample>
    <sample id="881">The authors propose to test the models on using information from multiple sources by conducting an experiment with human study participants and coreference resolution models.</sample>
    <sample id="939">Comparative evaluation is a common method used to assess dialogue systems. This involves comparing the performance of different dialogue systems or variations of a single system under similar conditions. The goal is to evaluate and compare aspects such as accuracy, user satisfaction, and the ability to understand and respond to user inputs effectively. By using comparative evaluation, researchers and developers can identify strengths and weaknesses, refine their systems, and make informed decisions about improvements and optimizations.</sample>
    <sample id="940">5</sample>
    <sample id="941">The background knowledge needed is that Servin is a judge and Kea is a baker.</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced in regard to each demographic.</sample>
    <sample id="944">To answer the question "How were sentences perturbed in the acceptable domain?" we need to analyze the information provided in the image. The image outlines a method for perturbing context sentences while preserving their relevant structure and then evaluates whether language models (LMs) are sensitive to these perturbations.

Here is a detailed step-by-step analysis:

1. **Understanding the Context**: The image discusses perturbing sentences to assess the sensitivity of LMs. The perturbations are designed to maintain the structural integrity of the original sentences while introducing variations.

2. **Identifying Perturbation Methods**: The image lists several methods for perturbing sentences:
   - **Prefix/suffix adverbs**: This involves adding adverbs at the beginning or end of the sentence.
   - **Long prefix adverbs**: Similar to prefix/suffix adverbs but with longer adverbial phrases.
   - **First and foremost**: This method involves starting the sentence with "First and foremost."
   - **Add Clause**: This involves adding a clause to the original sentence.
   - **Regardless of what X thinks about it**: This method introduces a conditional clause indicating that the following statement holds regardless of someone's opinion.
   - **Quote**: This method involves quoting the original sentence within another sentence.

3. **Preserving Relevant Structure**: The perturbations are designed to preserve the core structure of the original sentences, ensuring that the meaning and grammatical integrity remain intact. This is crucial for evaluating the robustness of LMs to changes in sentence phrasing without altering the underlying message.

4. **Evaluating LM Sensitivity**: By using these perturbation methods, researchers can determine how sensitive LMs are to variations in sentence structure. If an LM's performance or output changes significantly when the sentence is perturbed, it indicates that the model is sensitive to these structural changes.

In conclusion, the sentences were perturbed by applying various adverbial modifications, clause additions, and conditional clauses while maintaining the original sentence's structure. This approach helps in understanding how LMs respond to different linguistic constructions and ensures that any observed differences in performance are due to the perturbations rather than changes in the sentence's meaning.</sample>
    <sample id="945">To have a dimensional evaluation means to assess the quality of a product or service based on multiple criteria or dimensions. This type of evaluation provides a more comprehensive and detailed assessment compared to a simple rating system, which typically only considers a single dimension.

In the context of the image provided, the Likert Rating Evaluation is used for dimensional evaluation. The Likert scale is a psychometric scale that measures attitudes or opinions on a particular topic. It ranges from 1 to 5, with 1 usually representing strong disagreement or negative sentiment, and 5 representing strong agreement or positive sentiment.

Here's a step-by-step breakdown of how a dimensional evaluation using a Likert scale works:

1. **Identify Dimensions**: Determine the specific aspects or criteria that you want to evaluate. For example, in the image, the dimensions could be related to customer satisfaction, product features, or service quality.

2. **Assign Ratings**: Each dimension is rated on a scale from 1 to 5. A rating of 1 indicates strong disagreement or dissatisfaction, while a rating of 5 indicates strong agreement or satisfaction.

3. **Collect Data**: Gather responses from participants or users regarding each dimension. In the image, this is represented by the conversation between the human figure and the two robot figures.

4. **Analyze Data**: Review the collected ratings to identify trends, patterns, or areas of improvement. This analysis helps in understanding the overall performance or quality across different dimensions.

5. **Draw Conclusions**: Based on the analysis, draw conclusions about the strengths and weaknesses in the evaluated area. This information can be used to make informed decisions, improve products or services, and enhance user experiences.

In summary, a dimensional evaluation using a Likert scale involves assessing multiple criteria or dimensions to provide a more detailed and comprehensive understanding of a product or service. This approach helps in identifying specific areas that need improvement and ensures that all relevant aspects are considered in the evaluation process.</sample>
    <sample id="946">University of Science and Technology of China, Microsoft Research Asia, Beijing Haotong University, Sony AI</sample>
    <sample id="947">The form of the prompting is important in cases where the translation is required to be in a specific format, such as a 5-shot prompting.</sample>
    <sample id="948">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Vasuda Varadarajan*, Swannie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz Stony Brook University Human Language Analysis Group presenter</sample>
    <sample id="949">What is Cognitive Dissonance? "two elements of cognition (i.e. thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007)</sample>
    <sample id="950">What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007) I know that cigarettes could kill me, I grabbed a couple smokes after the meeting today. belief Dissonance action</sample>
    <sample id="951">What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent Expressed in language as a relationship b/w two phrases/statements by a user seq 1: I know that cigarettes could kill me. belief Dissonance/Consonance seq 2: I grabbed a couple smokes after the meeting today. action seq 3: I don't think I could keep my job without them. belief Eddie Harmon-Jones and Cindy Jones-2007 Cognitive dissonance theory after 50 years of development. Zeitschrift fur Sozialpsychologie, 38(1/6), 4</sample>
    <sample id="952">What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" Expressed in language as a relationship b/w phrases/statements by a user Relatively rare to find in language, compared to other discourse relations seq 1: I know that cigarettes kill me. belief I grabbed a couple smokes after the meeting today. action I don't think I could keep my job without them. belief Eddie Harmon-Jones and Cindy Jones-2007 Cognitive dissonance theory after 50 years of development. Zeitschrift fur Sozialpsychologie, 38(1/2), 5</sample>
    <sample id="953">Why dissonance? Effects of disagreement Cognitive dissonance theory: Reexamining a pivotal theory in psychology</sample>
    <sample id="954">Why dissonance? Effects of disagreement Attitudes and Belief trends Anxiety disorders</sample>
    <sample id="955">Why dissonance?
Effects of disagreement
Entry and Exit from
Attitudes and Belief trends
Anxiety disorders</sample>
    <sample id="956">Why dissonance?
Effects of disagreement
Cognitive Styles
Entry and Exit from
Attitudes and Belief trends
Anxiety disorders</sample>
    <sample id="957">Annotations  Wish I could hold grudges but I guess it's a good thing that I can't at the same time.</sample>
    <sample id="958">Step 1: Good parsing quality?
Step 2: @user.handle?
@user.handle?
Yes
No
Dissociation
Concordance
Neither
User
Dissociation
-3.8%
-48%
-48%
Wish I could hold grudges but I guess it's a good thing that I can't at the same time.
ANNOTATE
Dissociation</sample>
    <sample id="959">Annotations
Step 1: Good parsing quality?
Step 2: Dissonance?
@user HANDLE
Dissonance
-3.8%
Correlation
-48%
Neither
-48%
User
Wish I could hold grudges but I guess it's a good thing that I can't at the same time.
ANNOTATE
Dissonante</sample>
    <sample id="960">Training on Initial Annotated Set Roberta-base classifier head 43/901 dissonance; not better than chance</sample>
    <sample id="961">Training on Initial Annotated Set RoBERTa-base classifier head 43/901 dissonance; not better than chance</sample>
    <sample id="962">Method: Transfer and Active Learning for Annotating Rare Class START Rare class annotation - "needle in a haystack" Initial model: Transfer Learning new train old data model new fine-tune Model Retrain/Update Acquisition which are Active Learning iteration "Debut" to label? new examples Add new examples</sample>
    <sample id="963">Cold-start Annotations: Transfer Learning Initial model Transfer Learning Rare class annotation - "needle in a haystack" Difficult to annotate Increase chance of rare class? Acquisition Active Learning which are "Difficult" to annotate? Add to annotators Acquisition Active Learning Add new examples Humans annotate Model Retrain/Update</sample>
    <sample id="964">Cold-start Annotations: Transfer Learning Roberta-base + classifier head init. TRAIN Debate CE 0.10 Debate CE 0.08 Transferred after weights after training on combined Debate and GE data "Debate" Discord stance in debate forums; Vaidya Varadarajan, Nikita Sontakke, Ankit Sood. "Ice" Pre-training on debate stance in social media; the role of topic exposure (pre-training). Proceedings of the 20th Conference on Empirical Methods in Natural Language Processing (EMNLP 20).</sample>
    <sample id="965">Cold-start Annotations: Transfer Learning Roberta-base classifier head init debate 0.10 debate CE 0.06 0.08 Debate CE 0.04 0.02 0.00 0.50 0.60 0.65 Area under the ROC curve (AUC) Transferred weights after training on combined Debate and GE data</sample>
    <sample id="966">Cold-start Annotations: Transfer Learning Roberta-base classifier head init. debate 0.10 0.60 0.50 0.70 0.65 S Debate CE -0.08 Area under the ROC curve (AUC) Transferred after weights after combined Debate and GE data</sample>
    <sample id="967">Cold-start Annotations: Transfer Learning RoBERTa-base + classifier head TRAIN int. dataset Debates CE Debate-CE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+</sample>
    <sample id="968">Active Learning: Cumulative vs. Iterative Update</sample>
    <sample id="969">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="970">Active Learning: Probability-of-Rare-Class Strategy
Initial model &amp; Transfer Learning
Rare class annotation – ‘needle in a haystack’
Difficult to annotate?
Increase chance of rare class?
Easier to annotate?
new examples
Active Learning Iteration
Add new examples
Retrain/Update Model
Cumulative (CM)
Online (OT)
Interactive (IT)</sample>
    <sample id="971">Active Learning: Probability-of-Rare-Class Strategy Rare class annotation – "needle in a haystack" Difficult to annotate? Increase chance of rare class? Easier to annotate? new train new old (CM) (ML) data Model Retrain/Update Active Learning Iteration Humans annotate Add new examples</sample>
    <sample id="972">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Comparison (AUCs) Baseline: from scratch Transferred model AL-Random AL-Energy AL-CoreSet AL-CAL AL-PRFC (pure) +0.17 +0.20 +0.18 +0.19 +0.21</sample>
    <sample id="973">Active Learning: Probability-of-Rare-Class Strategy</sample>
    <sample id="974">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Characteristics Random Entropy CORRENT Cal PRC 3.20 11.96 -0.065 6.80 12.72 -0.035 4.80 11.88 -0.045 7.60 13.55 -0.071 Minimum annotation cost does not necessarily lead to better models Partly could make the annotations more difficult, cognitive dissonance is one such class To increase dissonance samples, PRC works the best.</sample>
    <sample id="975">Takeaways Rare class annotation - "need in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative M0 M1 M2 M3 M0 M1 M2 M0 M1 M2</sample>
    <sample id="976">Takeaways Rare class annotation - "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative M0 M1 M2 M3 M0 M1 M2 M0 M1 M2</sample>
    <sample id="977">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Contact: varadaranj@cs.stonybrook.edu, siuhong@cs.stonybrook.edu, has@cs.stonybrook.edu

Code: https://github.com/humanitarianrareclass/AL-Dissonance

Dataset: https://github.com/humanitarianrareclass/better-dissonance-dataset

Paper: https://arxiv.org/abs/2205.02459</sample>
    <sample id="978">Turn Based Dialogue, Incremental Validity</sample>
    <sample id="979">6</sample>
    <sample id="980">A good planner should have the ability to inherit abstract goals and be able to handle multiple constraints.</sample>
    <sample id="981">8</sample>
    <sample id="982">The name of the speaker is Vasudha Varadarajan.</sample>
    <sample id="983">Institute of Computer Science, Polish Academy of Sciences and University of Warsaw</sample>
    <sample id="984">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang Penn State amazon</sample>
    <sample id="985">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang PennState amazon</sample>
    <sample id="986">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="987">Cross-lingual Semantic Parsing Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="988">Cross-lingual Semantic Parsing Existing CLSLP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="989">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Neural Models SQL Chinese Lambda FunQL</sample>
    <sample id="990">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="991">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="992">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model English German Chinese Single Model SQL Lambda FunQL</sample>
    <sample id="993">XSemPLR We provide a unified dataset XSemPLR for cross-lingual semantic parsing in meaning representations. It contains: multiple natural languages and 9 datasets in various domains 5 semantic parsing tasks. 22 natural languages in 15 language families what players less than three wins a season what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less than three wins a season? what players less</sample>
    <sample id="994">XSemPLR We provide a unified dataset XSemPLR for cross-lingual semantic parsing in meaning representations. It contains: natural languages and 9 datasets in various domains 5 semantic tasks. 22 natural languages in 15 language families what players play less than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more than three times a season? what players play more</sample>
    <sample id="995">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL English Model SQL Inference German Translate API English Model SQL</sample>
    <sample id="996">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL English Model SQL Inference German Translate API English Model SQL</sample>
    <sample id="997">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL b English Model SQL Inference German Translate API English Model SQL</sample>
    <sample id="998">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training Monolingual Model: German Model SQL Inference German German Model SQL</sample>
    <sample id="999">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-Germanl. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training Monolingual Model (Few-shot) German Model SQL Inference German Model SQL</sample>
    <sample id="1000">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot by training monolingual models with only 10% training data. Training German Model SQL Monolingual Model (Few-shot) Inference German Model SQL</sample>
    <sample id="1001">Experiment Settings

We consider the six settings for training and evaluation.

Multilingual Model: Train one multilingual model for all languages.

Training

German

English

Chinese

Multilingual Model

SQL

Inference

German

Multilingual Model

SQL</sample>
    <sample id="1002">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1003">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1004">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language. Training English German Few-shot Multilingual Model SQL English German Multilingual Model SQL Inference</sample>
    <sample id="1005">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language. Training English Or German Multilingual Model SQL Inference English German Multilingual Model SQL</sample>
    <sample id="1006">Analysis of Monolingual Setting We evaluate on two groups of models: Multilingual Pretrained Encoders with Pointer-based Decoders (Enc-PTR) and Multilingual Pretrained Encoder-Decoder Models (Enc-Dec). We found Enc-Dec (mT5) obtains the best performance on all datasets!</sample>
    <sample id="1007">Analysis of Monolingual Enc-Dec (mT5) obtains the best performance on all datasets!</sample>
    <sample id="1008">Analysis of Monolingual Enc-Dec (mT5) obtains the best performance on all datasets!</sample>
    <sample id="1009">Analysis of Monolingual We evaluate on two groups of models on Monolingual Setting: Enc-PTR: Multilingual Pretrained Encoders with Pointer-based Decoders XLM-R + PTR: Multilingual, mBERT + PTR Enc-Dec: Multilingual Pretrained Encoder-Decoder Models mBART, mT5: Multilingual, mBART, mT5 We found Enc-Dec (mT5) obtains the best performance on all datasets! MATIS MGoQuery MSpider MNmaps MOvernight MCWS MQSchema2QA MTOP MCALa+ Average 72.18 40.40 83.82 57.47 23.46 55.23 75.41 5.87 49.09 Enc-PTR 31.31 47.30 47.90 59.10 23.53 62.37 80.36 7.69 52.03 Enc-Dec 53.15 74.26 50.73 91.65 66.29 30.15 65.14 10.29 58.16 mBART, mT5</sample>
    <sample id="1010">Analysis of Multilingual Training
We evaluate on mT5 and XLM-R + PTR on Multilingual Setting
Enc-Dec+Enc-PTR (mT5/XLM-R) can be improved by training in a mixture of various languages.
MAT5 MGoQuery MSpider MMaps MOvertight MCWQ MSchenaQQA MTOP MCOnala Average
Monolingual 11.31 71.42 47.40 85.87 66.20 33.33 65.77 81.83 7.69 58.16
Multilingual 39.72 71.35 40.20 85.91 61.03 30.79 61.82 81.68 - 59.96
mT5 54.45 76.57 32.30 91.31 67.55 28.51 60.92 82.95 - 61.82</sample>
    <sample id="1011">Analysis of Multilingual Training

We evaluate on mT5 and XLM-R + PTR on Multilingual Setting Enc-Dec+Enc-PTR (mT5/XLM-R) can be improved by training in a mixture of various languages.

MATS MgoQuery Mspider M2maps MOvernight MQS SchemaQQA MTOP MCnOAL Average

Monolingual 11.31 71.41 47.40 85.73 66.20 33.53 65.77 81.83 7.69 58.16

Multilingual 39.72 71.35 40.20 85.91 61.03 30.79 61.82 81.68 - 59.06

mT5 54.45 76.57 32.30 91.31 67.55 28.51 82.95 - 61.82</sample>
    <sample id="1012">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR in Multilingual Setting Most NLs can obtain performance gain, except that English performance drops in 7 datasets. This is known as "Curse of Multilinguality".</sample>
    <sample id="1013">Analysis of Multilingual Training We evaluate on mT5 and XLM-R + PTR in Multilingual Setting Most of the major NLPs can obtain performance gain, except that English performance drops in 7 datasets. This is known as "Curse of Multilinguality" Increase Decrease Number of Datasets en de zh th fa el id ev fr</sample>
    <sample id="1014">Cross-lingual Performance Gap

Blue Line: Cross-lingual Few-shot transfer
Orange Line: Cross-lingual Zero-shot transfer
Green Line: Monolingual Setting

Gequery
MTP
SchemaQA
Overnight
NLMaps
MCWQ
Gequery/lamb
Gequery/prolog
Gequery/funql
Gequery/sql
Spider
ATIS
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="1015">Cross-lingual Performance Gap  Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting Gequery MTOP SchemaQA Over night NLMaps MCWQ Gequery/lambda Gequery/prolog Gequery/funql Gequery/sql Spider ATIS -Few-shot -Zero-shot -Monolingual</sample>
    <sample id="1016">Cross-lingual Performance Gap

Blue Line: Cross-lingual Few-shot transfer
Orange Line: Cross-lingual Zero-shot transfer
Green Line: Monolingual Setting

Gequery
MTP
SchemaQA
Overnight
NLMaps
MCWQ
Gequery/lambda
Gequery/prolog
Gequery/funql
Gequery/sql
Spider
ATIS
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="1017">Other Results &amp; Findings (Section 4 in Paper) Enc-Dec (m75) outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot Multilingual LLMs (Codex &amp; BLOOM) are still inadequate for crosslingual parsing Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1018">Other Results &amp; Findings (Section 4 in Paper) Enc-Dec (m75) outperforms previous work or achieves comparable results. Pretraining on the English NL can significantly boost the performance of few-shot Multilingual LLMs (Codex &amp; BLOOM) are still inadequate for crosslingual parsing performance. Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest. FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1019">Conclusion We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between cross-lingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1020">Conclusion We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between cross-lingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1021">To determine the most common errors of PaLM, we need to analyze the insights provided in the image. The image mentions that accuracy scores are generally lower for PaLM and highlights two specific types of errors: "Accuracy/Omission" and "Style/Awesome." These errors indicate that PaLM often fails to accurately capture the intended meaning or style in its outputs.

Therefore, the most common errors of PaLM are:

1. Accuracy/Omission: This error occurs when PaLM fails to include important information or details in its responses, leading to incomplete or inaccurate answers.
2. Style/Awesome: This error involves issues with the style or tone of the generated text, which can result in responses that do not align with the desired format or aesthetic.

These errors highlight areas where PaLM may require further improvement to enhance its performance and reliability.</sample>
    <sample id="1048">The authors of the paper are affiliated with Emory University, specifically the Emory NLP Lab and Alexa Research Lab.</sample>
    <sample id="1049">Continuous Fine-tuning</sample>
    <sample id="1050">To determine the number of authors involved in the paper, we need to carefully examine the list of names provided at the bottom of the image. The names listed are:

1. Koustuv Sinha
2. Jon Gauthier
3. Aaron Mueller
4. Kanishka Misra
5. Keren Fuentes
6. Roger Levy
7. Adina Williams

By counting each name, we find that there are a total of 7 authors involved in the paper. Therefore, the answer is 7.</sample>
    <sample id="1084">The speaker's name is Yuxin Zhang.</sample>
    <sample id="1085">#ACL2023 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Shangbin Feng Chan Young Park Yuhuan Liu Yulia Tsvetkov Paul G. Allen School UW NLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1086">LM Training Data A mixed blessing 1000000000 100000000 10000000 1000000 100000 10000 1000 100 100 Dodge, Jesse, et al. “Documenting Large Webtext Corpora: A Case Study on the Crawl Corpus.” Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2021.</sample>
    <sample id="1087">LM Training Data A mixed blessing 1000000000 100000000 10000000 1000000 100000 10000 1000 100 100 10 10 1 1 0.1 0.1 0.01 0.01 0.001 0.001 0.0001 0.0001 0.00001 0.00001 0.000001 0.000001 0.0000001 0.0000001 0.00000001 0.00000001 0.000000001 0.000000001 0.0000000001 0.0000000001 www.msdn.com www.github.com www.wikipedia.org www.theguardian.com www.nytimes.com www.huffpost.com www.scribd.com www.bloomberg.com www.bbc.com www.investopedia.com www.banking.com www.kaggle.com www.cnet.com www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www.npr.org www</sample>
    <sample id="1088">LM Training Data A mixed blessing 1000000 1000000 100000 100000 10000 10000 1000 1000 100 100 10 10 1 1 www. wikipedia.org www. springer.com www. businessinsider.com www. theverge.com www. techcrunch.com www. theguardian.com www. cnet.com www. npr.org www. nytimes.com www. cnn.com www. bbc.com www. dailymail.com www. mashable.com www. washingtonpost.com www. wired.com www. forbes.com www. bloomberg.com www. techradar.com www. phys.org www. slate.com www. slate.com www. slate.com www. slate.com</sample>
    <sample id="1089">LM Training Data A mixed blessing 100K 10K 1K 100 10 1 100K 10K 1K 100 10 1 www.nytimes.com www.axios.com www.scribd.com www.theverge.com www.huffpost.com www.wired.com www.mercadomagico.com www.theguardian.com www.zdnet.com www.npr.org www.technologyreview.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge</sample>
    <sample id="1090">To this end, Pretraining data Language models Downstream tasks</sample>
    <sample id="1091">To this end Pretraining data Language models Downstream tasks How do we evaluate the political learning of LMs What role does pretraining data play in such political biases? How do LMs with different political learnings perform Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1092">To this end

Pretraining data

Language models

Downstream tasks

How do you evaluate the political leaning of LMs?

What role does pretraining data play in such political biases?

How do LMs with different political leanings perform on political fairness issues in NLP applications?</sample>
    <sample id="1093">Evaluating LM Political Leaning Support both encoder and decoder LMs “statements I &lt;mask&gt; with this statement.” “Do you agree or disagree with this statement? &lt;statement&gt;” Automatic eval Grounded in polisci lit Language Model Prompted Response Political Learning Left Libertarian Agree Right Authoritarian Disagree</sample>
    <sample id="1094">Existing LMs | 00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</sample>
    <sample id="1095">Existing LMs |  |
Authoritarian | Codex | LLaMa-2 | GPT-3-ada | GPT-3-curie | GPT-3-davinci | ChatGPT | GPT-4 | GPT-J
BERT-base | RoBERTa-base | RoBERTa-large | distilRoBERTa-base | ALBERT-base | ALBERT-large | BART-base | BART-large | ApaCa |
Left | Economic axis | Right</sample>
    <sample id="1096">Pretraining Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media Social Media (Reddit) left center right left center right</sample>
    <sample id="1097">Pretraining Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media Social Media (Reddit) left left center center right right</sample>
    <sample id="1098">Results Partisan shifts in LM political leaning RoBERTa GPT-2 original news news Center Left Right reddit reddit reddit</sample>
    <sample id="1099">Results Partisan shifts in LM political leaning RoBERTa GPT-2 original news reddit news Center Left Right original news reddit</sample>
    <sample id="1100">Results Partisan shifts in LM political leaning RoBERTa GPT-2 original news reddit Left Center Right news reddit reddit news original</sample>
    <sample id="1101">The Trump Card
Pre-45th to post-45th shift

news left news center news right
reddit left Reddit Center Reddit right
∆x = (-2.75, -1.24) ∆x = (-0.13, 1.03) ∆x = (-1.63, 1.03)
∆y = (-237, -0.51) ∆y = (-0.12, 1.28) ∆y = (-1.75, 1.03)</sample>
    <sample id="1102">The Trump Card
Pre-45th to post-45th shift</sample>
    <sample id="1103">The Trump Card
Pre-45th to post-45th shift</sample>
    <sample id="1104">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th BLACK MUSLIM LGBTQ+ JEWS ASAIN LATINX WOMEN CHRISTIAN MEN WHITE NEWS LEFT REDDIT_LEFT REDDIT_RIGHT Minisformation HP (L) NYT (L) CNN (L) NPR (L) Guard (L) Fox (L) WaEx (L) BRAT (R) WAT (R) NR (R)</sample>
    <sample id="1105">Per-Category Performance 1st 2nd 3rd</sample>
    <sample id="1106">Per-Category Performance 1st 2nd 3rd</sample>
    <sample id="1107">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 11th 12th 13th 14th 15th 16th 17th 18th 19th 20th 21st 22nd 23rd 24th 25th 26th 27th 28th 29th 30th 31st 32nd 33rd 34th 35th 36th 37th 38th 39th 40th 41st 42nd 43rd 44th 45th 46th 47th 48th 49th 50th 51st 52nd 53rd 54th 55th 56th 57th 58th 59th 60th 61st 62nd 63rd 64th 65th 66th 67th 68th 69th 70th 71st 72nd 73rd 74th 75th 76th 77th 78th 79th 80th 81st 82nd 83rd 84th 85th 86th 87th 88th 89th 90th 91st 92nd 93rd 94th 95th 96th 97th 98th 99th 100th</sample>
    <sample id="1108">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 11th 12th 13th 14th 15th 16th 17th 18th 19th 20th 21st 22nd 23rd 24th 25th 26th 27th 28th 29th 30th 31st 32nd 33rd 34th 35th 36th 37th 38th 39th 40th 41st 42nd 43rd 44th 45th 46th 47th 48th 49th 50th 51st 52nd 53rd 54th 55th 56th 57th 58th 59th 60th 61st 62nd 63rd 64th 65th 66th 67th 68th 69th 70th 71st 72nd 73rd 74th 75th 76th 77th 78th 79th 80th 81st 82nd 83rd 84th 85th 86th 87th 88th 89th 90th 91st 92nd 93rd 94th 95th 96th 97th 98th 99th 100th</sample>
    <sample id="1109">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 11th 12th 13th 14th 15th 16th 17th 18th 19th 20th 21st 22nd 23rd 24th 25th 26th 27th 28th 29th 30th 31st 32nd 33rd 34th 35th 36th 37th 38th 39th 40th 41st 42nd 43rd 44th 45th 46th 47th 48th 49th 50th 51st 52nd 53rd 54th 55th 56th 57th 58th 59th 60th 61st 62nd 63rd 64th 65th 66th 67th 68th 69th 70th 71st 72nd 73rd 74th 75th 76th 77th 78th 79th 80th 81st 82nd 83rd 84th 85th 86th 87th 88th 89th 90th 91st 92nd 93rd 94th 95th 96th 97th 98th 99th 100th</sample>
    <sample id="1110">Per-Category Performance 1st 2nd 3rd</sample>
    <sample id="1111">Qualitative Analysis Text Target Label Base N-L S-L N-R S-R the all right is corrupted with people supporting a new nationa True True True True False False (–) McDonald's stop Donald Trump from seizing They are true False False False False True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True</sample>
    <sample id="1112">Qualitative Analysis Text Target Label Base N-L S-L N-R S-R the all right is com pleted with people supporting a new nationa lism at McDonald's and priest are common They were charged that the crime was in the throes of a crime trail because they know that I would stop the slaughter going on Donald Trump has ordered August 20th incredible that water has been frozen in flight and is paying in burlington Vermont for clean water False False False False False False False False False False False False</sample>
    <sample id="1113">Hate Speech Text | Hate? | R.-L. | R.-S. | R.-R. | N.-S. | N.-R. Misinformation Text | Fake? | R.-L. | R.-S. | R.-R. | N.-S. | N.-R. You are a Jew and you are trying to take over the world. False True False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False</sample>
    <sample id="1114">Table 12: Qualitative analysis of hate speech examples from LM. Hate? R-L | R-S | R-R | N-S | N-R | N-R</sample>
    <sample id="1115">Hate Speech Text | Hate? | R-1 | R-2 | N-R | R-3 | N-S | N-I | N-A | N-T | N-E | N-D | N-C | N-F | N-P | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C | N-F | N-T | N-E | N-D | N-C</sample>
    <sample id="1116">Hate Speech Text Hate? R-1 R-2 R-3 R-4 Non-Hate Speech Text N-1 N-2 N-3 N-4 Table 1: Qualitative analysis of hate speech examples from LM with different political backgrounds.</sample>
    <sample id="1117">Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1118">Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1119">Discussion Between Scylla and Charybdis To “sanitize” or not to “sanitize”, that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1120">Thank you! Pretraining data Language models Downstream tasks Shanghui Feng Chan Young Park Yuhuan Liu Yulia Tsvetkov Paul G. Allen School UW NLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">Find words that distinguish personas of marked groups from unmarked groups.</sample>
    <sample id="1123">The affiliations of the authors are Paul G. Allen School, UW NLP, Carnegie Mellon University Language Technologies Institute, and the Russian Presidential Academy of National Economy and Public Administration.</sample>
    <sample id="1124">The first mentioned symmetrical dependency structure is the "Multi-headed/London" dependency structure.</sample>
    <sample id="1125">The speaker's name is Sarah E. Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">BLMP, SyntaxGym, and CrowS are the datasets that can be used to test syntactic phenomena.</sample>
    <sample id="1128">When Does Translation Require Context? A Data-driven, Multilingual Exploration Patrick Fernandes', Kayo Yin', Emmy Liu Andre F. T. Martins, Graham Neubig Carnegie Mellon University Language Technologies Institute f+ TECHNICO LISBOA BAIIR BERKELEY ARTIFICIAL INTELLIGENCE RESEARCH Unbabel = equal contribution</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">Evaluating context-dependent translation is hard Only a small portion of words depend on context</sample>
    <sample id="1133">Evaluating context-dependent translation is hard

Only a small portion of words depend on context

 Corpus-level metrics

Existing methods support limited discourse phenomena and languages</sample>
    <sample id="1134">RQ1: When does translation require context? RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1135">RQ1: When does translation require context? - Word-level context usage RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1136">Conditional Cross-Mutual Information (CXMI) • CXMI: measure how much context MT models use given a corpus</sample>
    <sample id="1137">Conditional Cross-Mutual Information (CXMI) CXMI: measure how much context MT models use given a corpus Uncertainty over translations given the source HqMTs(Y|X) HqMTs(Y|X, C) Uncertainty over translations given the source AND context CXMI(C) → Y|X</sample>
    <sample id="1138">Pointwise (P-)CXMI

We introduce P-CXMI to measure context usage to translate a specific</sample>
    <sample id="1139">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1140">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1141">Thematic analysis of high P-CXMI words</sample>
    <sample id="1142">Thematic analysis of high P-CXMI words 1. POS tags</sample>
    <sample id="1143">Thematic analysis of high P-CXMI words 1. POS tags - Pronouns P-CXMI for POS tags in En-Ar PRON 3.Sing PRON 3.Dual PRON 3.Plur</sample>
    <sample id="1144">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form</sample>
    <sample id="1145">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form - Lexical cohesion Avellile's mother was still asleep. Avellile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔去上学了。</sample>
    <sample id="1146">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form - Lexical cohesion - Formality Avellie's mother was still asleep. Avellie went to school. 阿维利尔的母亲还在睡觉。 阿维利尔上学了。</sample>
    <sample id="1147">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items 3. Individual tokens - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis She knows where we're going. I don't. Ich weiß, wohin wir gehen. Ich weiß es nicht.</sample>
    <sample id="1148">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark</sample>
    <sample id="1149">Multilingual Discourse-Aware (MuDA) tagger

- Pronouns
- Verb form
- Lexical cohesion
- Formality
- Ellipsis</sample>
    <sample id="1150">Multilingual Discourse-Aware (MuDA) tagger Pronouns Verb form Lexical cohesion Formality Ellipsis 6000 fr 4000 2000 0 2000 1000 500 0 500 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 5000 6000 1000 2000 3000 4000 500</sample>
    <sample id="1151">MuDA benchmark

BLEU
COMET
F-measure

...</sample>
    <sample id="1152">RQ1: When does translation require context? Word-level context usage Thematic analysis RQ2: How well do models handle context-dependent translations? Multilingual Discourse-Aware (MuDA) benchmark Model evaluation</sample>
    <sample id="1153">Corpus-level metrics BLEU</sample>
    <sample id="1154">Corpus-level metrics BLEU COMET F-measure</sample>
    <sample id="1155">Corpus-level metrics BLEU COMET F-measure It is unclear which system is best for document-level MT with corpus-level metrics</sample>
    <sample id="1156">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion</sample>
    <sample id="1157">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1158">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion Ellipsis, pronouns, verb form

DeepL outperforms Google on most phenomena and language pairs*

DeepL &gt; Google</sample>
    <sample id="1159">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU chaff F-measure</sample>
    <sample id="1160">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU, CIDEr, F-measure</sample>
    <sample id="1161">FT, W, BOND, COSINE, MLC, L2R</sample>
    <sample id="1162">11</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intra lingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Mom en, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1164">DEPLAIN: A German Parallel Corpus with Intra lingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1165">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion Plain Language: .Die Gewerkschaft setzt sich dafur ein, dase zum Beispiel hohere Lohnen oder mehr Urlaub werden. .</sample>
    <sample id="1166">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion .Die Gewerkschaft setzt sich dafur ein, dass zum Beispiel hohere Lohnen oder mehr Urlaub werden. Plain Language: .Die Gewerkschaft setzt sich zum Beispiel fur hoheer Lohne oder mehr Urlaub ein. Insertion</sample>
    <sample id="1167">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion .Die Gewerkschaft setzt sich dafur ein, dass zum Beispiel hohere Lohnen gezaht werden. Plain Language: Die Gewerkschaft setzt sich zum Beispiel fur hoheer Lohne oder mehr Urlaub ein. Insertion</sample>
    <sample id="1168">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion .Die Gewerkschaft setzt sich dafur ein, dass zum Beispiel hoheer Lohnen gezaht werden. Plain Language: .Die Gewerkschaft setzt sich zum Beispiel fur hoheere Lohne oder mehr Urlaub ein. Insertion</sample>
    <sample id="1169">2. DE-plain
A New Corpus</sample>
    <sample id="1170">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 201</sample>
    <sample id="1171">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) Alignment manual (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poesch man (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et al. (Poeschel et 2013) 2013)</sample>
    <sample id="1172">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) Alignment manual (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (Holtzman et al. (</sample>
    <sample id="1173">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) Alignment manual marked weakly supervised supervised weakly supervised weakly supervised weakly supervised German English German English German English German English German English German English German English German English German English (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al. (Petrov et al</sample>
    <sample id="1174">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) Alignment manual 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450</sample>
    <sample id="1175">Types of Simplification Simplicity LexSimp StructSimp news bible fiction 187 12 n= 106 n= 105 n= 105 L2 b. Simplification Transformations DEplain-apa DEplain-web reordering paraphrasing lexical substitution word addition word deletion</sample>
    <sample id="1176">Types of Simplification Simplicity LexSimp StructSimp news bible
fiction
n= 96 n= 187 n= 72 Simplication Transformations DEplain-apa DEplain-web reordering paraphrasing lexical substitution word addition word deletion</sample>
    <sample id="1177">Types of Simplification Simplicity LexSimp StructSimp news bible L2 fiction n= 187 n= 187 n= 12 Simplification Transformations DEplain-apa DEplain-web reordering paraphrasing lexical substitution word addition word deletion</sample>
    <sample id="1178">Types of Simplification Simplicity LexSimp StructSimp news bible L2 fiction n=187 n=185 n=172 Simplication Transformations DEplain-apa DEplain-web Reordering Rephrasing lexical substitution word addition word deletion</sample>
    <sample id="1179">Types of Simplification Simplicity LexSimp StructSimp news bible fiction 187 n=137 n=12 Simplification Transformations DEplain-apa DEplain-web reordering paraphrasing lexical substitution word addition word deletion</sample>
    <sample id="1180">3. Use-cases
Automatic alignment and simplification</sample>
    <sample id="1181">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description P R F,0 Sent-LaBSE Similar embeddings of Language model BERT transformer 111 960 444 780 444 779 779 Sent-RoBERTa Similar embeddings of Cross English &amp;amp; German RoBERTa 247 960 444 342 444 326 326 CATS-C3G Different similarity measures e.g. grams (C3G), word vectors 84 247 342 276 276 276 BERTalign Allows sentence-transformer methods produce n:m alignments 743 846 465 610 465 733 733 WoZalign A vicinity-driven approach with a TF-IDF similarity matrix 846 819 477 628 819 628 730</sample>
    <sample id="1182">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 P R F0.5 F0.6 F0.7 F0.8 F0.9 Sent-LaBSE Similar embeddings of Language BERT transformer 960 444 444 607 779 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780 780</sample>
    <sample id="1183">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 P R F, Sent-LaBSE Similar embeddings of Language Model BERT transformer 960 444 607 779 Sent-RoBERTa Similar embeddings of Cross English &amp; German RoBERTa 247 553 342 278 CATS-C3G Different similarity measures e g. n-grams, word vectors 847 477 610 733 BERTalign Allows sentence-transformer methods produce n:m alignments 846 477 628 730 VowpalWabbit A vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1184">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:n capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language model BERT transformer 960 444 607 775 Sent-RoBERTa Similar embeddings of Cross English &amp;amp; German RoBERTa CATS-C3G Different similarity measures e.g. grams, word vectors 247 553 342 278 WMT’17 Allows sentence-transformer methods produce n:m alignments 743 465 387 561 BERTalign A vicinity-driven approach with a TF-IDF similarity matrix 846 477 610 733 R F0.5 F0.5</sample>
    <sample id="1185">Results of the alignment methods with 1:1 (upper part) and n:n capabilities (lower part) Name Description P R F,0.5 F,0.7 F,0.9 Sent-LaBSE Similar embeddings of Language BERT transformer 111 844 780 960 444 607 779 Sent-RoBERTa Similar embeddings of Cross English &amp;amp; German RoBERTa 247 553 342 278 CATS-C3G Different similarity measures e.g. grams, word vectors 847 477 610 846 477 628 BERTalign Allows sentence-transformer methods produce n:m alignments 743 465 561 733 387 412 Vowpal Wabbit A vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1186">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language Model BERT transformer 960 444 607 779 Sent-RoBERTa Similar embeddings of Cross English &amp; German RoBERTa 247 553 342 276 CATS-C3G Different similarity measures e.g. grams, word vectors 847 477 610 733 BERTalign Allows sentence-transformer methods produce n:m alignments 846 477 628 730 TF-IDFalign A vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1187">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language Model BERT transformer 960 444 607 775 CATS-C3G Different similarity measures e.g. grams, word vectors 247 553 342 278 BERTalign Allows sentence-transformer methods produce n:m alignments 743 465 387 459 VASalign A vicinity-driven approach with a TF-IDF similarity matrix 846 477 610 733 R F R F R F</sample>
    <sample id="1188">Document Level Sentence Level Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data. Results on Sentence Simplification using mBART n corresponds to the length of the training data.</sample>
    <sample id="1189">Results on Document Simplification using fine-tuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1190">Document Level Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data. 17327 34247 5655 985 SARI BLEU BS-P FRE train data Deplain web Deplain-APA Deplain-APA-wbe srlsrlc-baseline 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327 17327</sample>
    <sample id="1191">Document Level Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data. 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 10</sample>
    <sample id="1192">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1193">Automatic Text Simplification Results on Document Level using fine-tuned long-mBART 1000 900 800 700 600 500 400 300 200 100 0 train data in SARI BLEU BS-P F1-PR Deplain web 17837 34.247 0.471 65.65 Deplain-w 481 49.1 0.471 65.65 Deplain-wbe 481 49.1 0.471 65.65 srlcnn-baseline 17837 34.247 0.471 65.65 srlcnn-APA 17837 (n=48) 34.247 0.471 65.65 ... DEPLAN-APA Test (n=147) DEPLAN-WEb Test (n=147) DEPLAN-APA Test (n=123) DEPLAN-WEb Test (n=146)</sample>
    <sample id="1194">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis. Google Research</sample>
    <sample id="1197">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Paretti, and Annie Louis. Google Research</sample>
    <sample id="1198">Indirect Referring Expressions Google Research Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference "easy on me," "the first one" Indirect reference Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the man Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the</sample>
    <sample id="1199">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Direct reference Indirect reference "The first one" "The song that's not energetic" Cannot remember the name Cannot remember the name and fluid conversation: The newer one. The first one. The pronunciations are hard to distinguish Want to specify a preference Want to specify a preference</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference "easy on me," the first one Indirect reference Cannot remember the name Cannot remember the name Cannot remember the name Cannot remember the name Want to specify a preference Indirect reference The newer one. The song that's not energetic. Resolving Indirect References by Entity Selection (Mityaev Corpus)</sample>
    <sample id="1201">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name The newer one. The song that's not energetic.</sample>
    <sample id="1202">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name Cannot remember the name: "the first one" The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic. Resolving Indirect References by Entity Selection (Mityaev Corpus) 72</sample>
    <sample id="1203">Dataset Collection

Important problem

Conversational systems

Benchmarking Large Language Models' entity understanding

No large-scale public dataset available

We collect a large dataset using crowd annotation

Three domains:

Resolving Indirect Referring Expressions by Entity Selection (Mnli+ Corpus)</sample>
    <sample id="1204">Dataset Collection

Important problem

Conversational systems

Benchmarking Large Language Models' entity understanding

No large-scale public dataset available

We collect a large dataset using crowd annotation

Three domains:

Resolving Indirect Expressions by Entity Selection (MELITON Corpus)</sample>
    <sample id="1205">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task

Remember that we were talking about yesterday?

Do you mean I was feeling 1 Gotta Feeling?

Expression referring to one of the entities

Filled by the annotator</sample>
    <sample id="1206">Dataset Collection Methodology

Google Research

Methodology emphasizes informality using a cartoon completion task

Remember that we were talking about your new job yesterday?

Do you mean you were talking about 1 Gotta Feeling?

Filled by the annotator

The alternative question

Expression referring to one of the entities

Sets the dialog context (chosen from a few manual prompts per domain)

Resolving Indirect Expressions (Entity Selection) (Mellieza Corpus)</sample>
    <sample id="1207">Dataset Collection Methodology

Google Research

Methodology emphasizes informality using a cartoon completion task

Remember that we were talking about yesterday?

Do you mean "How do you feel today?"

I've got a Gotta Feeling?

Filled by the annotator

Sets the dialog context (chosen from a few manual prompts per domain)

The alternative question

Expression referring to one of the entities</sample>
    <sample id="1208">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task

Remember that we were talking about yesterday?

Do you mean we were talking about 1 hour ago? I've got a Gotta Feeling?

Filled by the annotator

Sets the dialog context (chosen from a few manual prompts per domain)

The alternative question

Expression referring to one of the entities</sample>
    <sample id="1209">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task

Remember that song you were talking about yesterday?

Do you mean the one you had 1 Gotta Feeling?

Filled by the annotator

The alternative question

Expression referring to one of the entities

Sets the dialog context (chosen from a few manual prompts per domain)

Resolving Indirect Expressions (Entity Selection) Corpus</sample>
    <sample id="1210">Dataset Collection Methodology

Methodology emphasizes informativeness using a cartoon completion task

Remember that song we were talking about yesterday?

Do you mean the one where I gotta feelin'?</sample>
    <sample id="1211">Generate alternative questions = sampling entity pairs Google Research Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1212">Google Research Generate alternative questions = sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1213">Google Research Generate alternative questions = sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre和/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1214">Google Research Generate alternative questions = sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre和/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with/similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1215">Google Research Generate alternative questions = sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1216">Google Research Background knowledge (Music) Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song Resolving Indirect Referencing by Entity Selection (Mnli+ Corpus)</sample>
    <sample id="1217">Google Research Background knowledge (Music) Easy on Me (by A$Jele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song</sample>
    <sample id="1218">Google Research Background knowledge (Music) Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen at least some of each song Read about each song Resolving Indirect Referencing by Entity Selection (Mentions Corpus)</sample>
    <sample id="1219">Background knowledge (Recipes) Simnel Cake Pandan Cake Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste in eleven balls made of the same paste as the cake. Pandan cake is a light, fluffy sponge cake flavoured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia, and also in the Netherlands, especially among the Indo community.</sample>
    <sample id="1220">Eliciting expressions We then tell the annotators which choice should be selected and ask them to describe it. Pick one Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, for example: The one with the dance music The song that's not energetic It has a river It's about not having time to choose</sample>
    <sample id="1221">Eliciting expressions We then tell the annotators which choice should be selected and ask them to describe it. Pick one Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, not example: One with the music The song that's not energetic It has a river It's about not having time to choose</sample>
    <sample id="1222">AltEntities Corpus

-6,000 alternative questions across the three domains
-42,000 indirect referring expressions

Results with T5 XL model (accuracy):
o 92-95% if the LM has access to the same background knowledge as annotators.
o 82%-87% when the LM has access to partially overlapping background knowledge.
o ~60% when the LM (T5 XL) only has access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities

Google Research</sample>
    <sample id="1223">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

o 92%-95% when the LM has access to the same background knowledge as annotators.
o 82%-87% when the LM has access to partially overlapping background knowledge.
o ~60% when the LM (T5 XL) has only access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities

Google Research</sample>
    <sample id="1224">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

o 92-95% if the LM has access to the same background knowledge as annotators.

o 82%-87% when the LM has access to partially overlapping background knowledge.

o ~60% when the LM (T5 XL) has only access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1225">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

o 92-95% if the LM has access to the same background knowledge as annotators.

o 82%-87% when the LM has access to partially overlapping background knowledge.

o 60% when the LM (T5 XL) only has access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1226">CamemBERT is initially trained on the French Wikipedia.</sample>
    <sample id="1227">The speaker's name is Adam Przepiórkowski.</sample>
    <sample id="1228">The findings that led to the conclusion that temporal drift is the main cause of performance loss are: 1. The temporal drift is a significant factor in the performance loss, as shown by the graph in Figure 2. 2. The temporal drift is not a one-time event, but rather a continuous process that occurs over time. This is evident from the fact that the performance loss is not only present at the beginning of the training period, but also continues to increase over time. 3. The temporal drift is not only present in the training data, but also in the test data. This suggests that the temporal drift is not a result of overfitting, but rather a genuine problem with the model's ability to generalize to new data.</sample>
    <sample id="1229">NLPositionality: Characterizing Design Biases of Datasets and Models Sebastin Sany University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1230">NLPositionality: Characterizing Design Biases of Datasets and Models Sebastin Sany University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1231">Imagine…</sample>
    <sample id="1232">Imagine...</sample>
    <sample id="1233">Imagine...</sample>
    <sample id="1234">Imagine... Design bias example! Can you stop being a jerk? (0.82) Pressstitutes everywhere on the news (0.33) Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India PerspectiveAPI score</sample>
    <sample id="1235">Positionality [1] Savin-Baden, Maggi, and Claire Howell-Major. “Qualitative research: The essential guide to theory and practice.” Qualitative Research: The Essential Guide to Theory and Practice (2013,</sample>
    <sample id="1236">Positionality “The perspectives [people] hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1237">Positionality “The perspectives [people] hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1238">Do datasets and models have
positionality?</sample>
    <sample id="1239">Do datasets and models have
positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science.</sample>
    <sample id="1240">Do datasets and models have
positionality?</sample>
    <sample id="1241">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022. [2] Chen, et al. "GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022. [3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." Data Science</sample>
    <sample id="1242">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Blasi, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambio &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="1243">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambro &amp; Gerglot. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science 2022.</sample>
    <sample id="1244">Question: Do datasets and models have positionality?</sample>
    <sample id="1245">NLPositionality A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">Framework Collection Process ing Eating with Hands" "Can you live without your hands?" "Steel" "Eating with Hands" "Can you live without your hands?" "Steel" "Eating with Hands" "Can you live without your hands?" "Steel" 20% of the dataset sampled as a dataset Each instance has an annotation Each instance is sent as a request to the model Annotators receive gold labels from the models received from the models The received annotations are compared with the gold labels from the models Model Predictions Age Gender White Hispanic Black Non-Black Grad College School Country 35-45 16-25 26-34 17-15</sample>
    <sample id="1247">Framework 1) annotate datasets with diverse annotators,</sample>
    <sample id="1248">Framework Collection Processing 1) Annotate datasets with diverse annotators. 2) Train a model on the annotated dataset. 3) Use the model to make predictions on unseen data. 4) Evaluate the model's performance and refine it as needed.</sample>
    <sample id="1249">Framework 1) Re-annotate datasets with diverse annotators. Collection Processing "Eating with Hands" "Eating with Hands" "Eating with Hands" Each instance has an Can you live without it? Can you live without it? Can you live without it? steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel steel</sample>
    <sample id="1250">Framework Collection Processing 1) Annotators are presented with instances from a dataset and asked to label them according to the task. Each instance has an associated set of annotations. 2) Compare annotations to models and datasets to measure model performance via Pearson’s R scores.</sample>
    <sample id="1251">Framework Collection Processing 1) Each instance has an annotation. Annotations from the world are sent as instances to the model. 2) Compare annotations to models and databases via Pearson’s R scores.</sample>
    <sample id="1252">Lab in the Wild 5,376,396 participants Could you live with an AI and its morals? Are you better at noticing hateful and show compared to AI? Where are you on the techno-utopian scale? Tell us how you thin mixed reality is more technic or technologically utopian. Participate now! Participate now! Participate now!</sample>
    <sample id="1253">Lab in the Wild Pool of diverse volunteers / research participants Online experiment from researchers</sample>
    <sample id="1254">Task A: Social Acceptability

1) Read the situation

2) Enter what you think about it from your perspective

3) See what others thought about it!

The AI speculates:

Study participants in the United States</sample>
    <sample id="1255">Task A: Social Acceptability 1) Read the situation. Wanting to make a lot of money. 2) Enter what you think about it. "What do you think about this perspective?" "It's generally required that you don't want to hurt other people." "Good." 3) See what others thought about it! The AI speculates. Study participants in the United States: I understand that I'm understandable.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity

English people smell like sour milk</sample>
    <sample id="1258">Task B: Toxicity
Analysis
Datasets
- Dynahate
Models
- Perspective API
- Rewire API
- State RoBERTa
- GPT-4</sample>
    <sample id="1259">Results Who do NLP datasets and models align with?</sample>
    <sample id="1260">Social Acceptability (GPT-4) Datasets and models are most aligned to English-speaking countries. 0.8 0.71* N=528 N=8,289 0.74* 0.68* 0.67* 0.67* 0.67* 0.67* 0.67* 0.59* African Islamic Baltic Catholic Europe Confucian English-speaking Latin America Orthodox Europe Protestant Europe West South Asia</sample>
    <sample id="1261">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1262">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1263">Finding 2:
Some populations are left behind.</sample>
    <sample id="1264">Datasets and models are less aligned to non-binary people. Social Acceptability (GPT-4) 0.8 Non-binary 0.73* Man 0.69* 0.55* Woman</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">Recommendations [1] https://www.masakhane.io</sample>
    <sample id="1267">Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: a. Share disaggregated dataset labels! b. Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with and for specific communities is desirable for inclusive NLP (e.g., Masakhane initiative). [1] https://www.masakhane.io</sample>
    <sample id="1268">Thanks! Dashboard Link: nlpportunity.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="1269">To generate all possible output sequences</sample>
    <sample id="1270">The authors recommended that model owners should increase transparency about bias mitigation methods to ensure accountability and trust in AI systems. By being open about the techniques used to mitigate biases, model owners can demonstrate their commitment to fairness and equity, which is crucial for building public confidence in AI technologies. Transparency also allows for better evaluation and improvement of AI models over time, as it facilitates collaboration and feedback from diverse stakeholders.</sample>
    <sample id="1271">Many people were helping themselves. Many people were helping herself.</sample>
    <sample id="1272">The authors used the following evaluation metrics: BLEU, METEOR, CIDEr, and ROUGE-L.</sample>
    <sample id="1273">Krippendorff’s Alpha</sample>
    <sample id="1274">Wikipedia was chosen to add completely unrelated sentences to the unacceptable and acceptable queries.</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany</sample>
    <sample id="1276">MultiInstruct is different from other benchmarks because it includes a diverse set of tasks that are not limited to language understanding.</sample>
    <sample id="1277">To determine the number of authors involved in the paper, we need to carefully examine the title and author list provided in the image.

1. Identify the authors' names listed on the slide:
   - Sarah E. Finch
   - James D. Finch
   - Jinho D. Choi

2. Count the number of distinct author names:
   - There are three unique names listed.

Therefore, the total number of authors involved in the paper is 3.</sample>
    <sample id="1278">Binary coordination is a measure of the degree of coordination between two entities. It is calculated by dividing the number of instances in which both entities are coordinated by the total number of instances.</sample>
    <sample id="1279">10 minutes</sample>
    <sample id="1280">The smaller T5 model, when fine-tuned on Coscript, can generate higher quality scripts than larger language models (LLMs) like GPT-3 and Codex. This suggests that specialized models trained on specific datasets can outperform more general-purpose LLMs in certain tasks.</sample>
    <sample id="1309">From scratch with full model construction, Continual pre-training using an existing pre-trained model</sample>
    <sample id="1310">The answer is 0.15.</sample>
    <sample id="1311">BLEU-4</sample>
    <sample id="1312">Yes, language models have different political biases.</sample>
    <sample id="1347">Cognitive dissonance is the discomfort or tension that arises when an individual holds two or more contradictory beliefs, values, or attitudes simultaneously. This psychological phenomenon was first introduced by Leon Festinger in 1957 and has since been extensively studied in the fields of psychology, social science, and philosophy. Cognitive dissonance occurs when a person's actions, thoughts, or feelings conflict with their existing beliefs or values, leading to a sense of unease or inconsistency.

The concept of cognitive dissonance can be understood through the lens of the theory of reasoned action (TRA) developed by Iza Tronche and Jean-Claude Tronche in 1980. According to TRA, individuals make decisions based on their attitudes towards the behavior, subjective norms, and perceived behavioral control. When there is a discrepancy between one's attitudes and behaviors, cognitive dissonance arises.

In the context of the image provided, the definition of cognitive dissonance is given as "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007). This definition highlights the core idea that cognitive dissonance involves a mismatch between different cognitive elements, such as thoughts, actions, and beliefs.

To illustrate this concept further, consider the following example:

Suppose a person who strongly believes in environmental conservation decides to drive a gas-guzzling SUV. In this case, there is a clear inconsistency between the individual's belief in environmental conservation and their action of driving an environmentally harmful vehicle. This inconsistency can lead to cognitive dissonance, as the person may experience feelings of guilt, anxiety, or discomfort due to the conflict between their values and actions.

In conclusion, cognitive dissonance is a fundamental concept in psychology that describes the discomfort experienced when there is a discrepancy between an individual's beliefs, values, and actions. The theory of reasoned action provides a framework for understanding how cognitive dissonance arises from inconsistencies between different cognitive elements. By recognizing and addressing these inconsistencies, individuals can work towards resolving cognitive dissonance and achieving a state of psychological balance.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Yes, cumulative training performs better than iterative when doing active learning.</sample>
    <sample id="1350">The speaker's name is Sara Papi.</sample>
    <sample id="1351">The data was taken from the WMT’14 En-Es translation task.</sample>
    <sample id="1352">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przpielkowski and Michal Wozniak INSTITUTE OF COMPUTER SCIENCE PAN ul. Jana Kazimierzawa 5, 01-248 Warsaw UNIVERSITY OF WARSAW ACL 2023</sample>
    <sample id="1353">Dependency Structure of Coordination

Bouquet/Stanford: (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1354">Dependency Structure of Coordination

Bouquet/Stanford: (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">Dependency Structure of Coordination

Bouquet/Stanford (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1356">Dependency Structure of Coordination
Bouquet/Stanford (Universal Dependencies):
Homer loves Lisa, Bart, and Maggie.
Chain/Moscow:
Homer loves Lisa, Bart, and Maggie.
Conjunction-headed/Prague:
Homer loves Lisa, Bart, and Maggie.
Multi-headed/London:
Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1357">Dependency Structure of Coordination

Bouquet/Stanford (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1358">Dependency Structure of Coordination

Bouquet/Stanford: (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">Dependency Structure of Coordination

Bouquet/Stanford: (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1360">Dependency Structure of Coordination

Bouquet/Stanford (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1361">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it yesterday Marge read it yesterday Marge read this absolutely fascinating book about bees yesterday</sample>
    <sample id="1362">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1363">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1364">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1365">Dependency Length Minimization (DLM)
Word order tends to minimize dependency lengths:
good
it read Marg
yesterday
it read Marg
bad
this read Marg
yesterday this absolutely fascinating book about bees. good</sample>
    <sample id="1366">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read this absolutely fascinating book about bees. yesterday.</sample>
    <sample id="1367">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1368">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it yesterday. Marge read it yesterday. Marge read this absolutely fascinating book about bees yesterday. Marge read yesterday this absolutely fascinating book about bees.</sample>
    <sample id="1369">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it yesterday. read this absolutely fascinating book about bees. good read this absolutely fascinating book about bees. yesterday.</sample>
    <sample id="1370">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it yesterday. Marge read it yesterday. Marge read this absolutely fascinating book about bees yesterday. Marge read this absolutely fascinating book about bees good.</sample>
    <sample id="1371">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1372">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Margin read it yesterday good bad Margin read this absolutely fascinating book about bees yesterday. good Margin read yesterday this absolutely fascinating book about bees good</sample>
    <sample id="1373">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Berber 2016) • Left conjuncts tend to be shorter than before). • This tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1374">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • left coniunctions tend to be shorter than before), this tendency grows with length difference (briefly noted in Gibson et al. 1996: 88-90) • but only when the governor is on the left or absent (I saw Bart and Lisa: Homer came and sneezed), not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1375">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • Left conjs tend to be shorter than before, this tendency grows with length difference (briefly noted in Gibson et al. 1996: 88-90) • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • left coniugts tend to be shorter (observed and before); α this tendency grows with length difference (briefly noted in Gibson et al. 1996: 88-90) • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed); not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1377">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conju nects tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90). but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed) not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1378">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conju nects tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90), • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and I sneezed) not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1379">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • left conju cts tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90). but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed) not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1380">The graph shows the proportion of shorter constraints depending on the absolute difference in comp length (with confidence bands).</sample>
    <sample id="1381">The results of the study showed that the use of a governor in the sentence structure resulted in a significant increase in the proportion of shorter conjuncts. This suggests that the use of a governor can help to improve the readability of sentences by making them more concise and easier to understand.</sample>
    <sample id="1382">The graph shows the proportion of shorter conjuncts depending on the absolute difference in conjunct lengths with confidence bands. The results show that the proportion of shorter conjuncts increases as the absolute difference in conjunct lengths increases. This suggests that the model is more likely to generate shorter conjuncts when the conjuncts are longer.</sample>
    <sample id="1383">The results of the analysis on the 300M dataset are shown in Figure 1. The top row shows the results for the left governor, while the bottom row shows the results for the right governor. The left column shows the results for the no governor case, while the middle and right columns show the results for the syllable and word cases, respectively.</sample>
    <sample id="1384">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is when a model trained on one language is used to make predictions in another language.</sample>
    <sample id="1387">Saarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="1388">AL/AL-CA</sample>
    <sample id="1416">Tree-based methods are not able to capture long-range dependencies.</sample>
    <sample id="1417">The authors of the paper are affiliated with the School of Interactive Computing at the Georgia Institute of Technology.</sample>
    <sample id="1495">Annotating Behaviors in Chat</sample>
    <sample id="1496">2018</sample>
    <sample id="1527">The affiliations of the authors are INL, UvA, and University of Amsterdam.</sample>
    <sample id="1528">The answer is SIYU YUAN.</sample>
    <sample id="1529">5</sample>
    <sample id="1530">The answer is CAAT.</sample>
    <sample id="1531">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning
Zhongyang Xu*, Ying Shen*, Lifu Huang
Department of Computer Science, Virginia Tech
*Equal Contribution</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks

Pre-trained LM
Task A
Pre-trained LM
Task B
Pre-trained LM
Task C

Pre-training on task A
Tune pre-trained model on task A
Pre-training on task B
Tune pre-trained model on task B
Pre-training on task C
Tune pre-trained model on task C

Inference on task A
Use pre-trained model fine-tuned on task A
Inference on task B
Use pre-trained model fine-tuned on task B
Inference on task C
Use pre-trained model fine-tuned on task C

Figure 2: Comparing instruction tuning with pre-train-finetune and prompting.

Image credit: Wei, Jason, et al. "Finetuned language models are zero-shot learners."</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks

Pre-trained LM (BERT, T5) Pre-trained LM (BERT, T5)
on task A on task A
Pre-trained LM (BERT, T5)
on task A
Pre-trained LM (BERT, T5)
on task A
Pre-trained LM (BERT, T5)
on task A
Pre-trained LM (BERT, T5)
on task A
Pre-training on task A
Inference on unseen task B
Inference on unseen task B
Inference on unseen task B
Inference on unseen task B
Inference on unseen task B
Inference on unseen task B
Figure 2: Comparing instruction tuning with pre-train-finetune and prompting.

Image credit: Wei, Jason, et al. "Finetuned language models are zero-shot learners."</sample>
    <sample id="1534">Language-only</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks Wang, Yichang, et al. "Benchmarking generalization via in-context instructions on 1,800+ language tasks." arXiv preprint arXiv:23</sample>
    <sample id="1538">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse 10 broad tasks 5 expert-written instructions</sample>
    <sample id="1539">The first multimodal instruction tuning benchmark dataset</sample>
    <sample id="1540">OFAs (One For All) A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary coordinates of a bounding box.</sample>
    <sample id="1541">MULTINSTRUCT Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Input: A caption for the image. The region that contains the text "de" is described by "A blue train in the front". The question is "Is it the image?" Options: -bin 229- bin 604- bin 346- bin 442- Output: The answer is "Yes, the image is relevant." Input: The region of the object described by "A blue train in the front". The content of the image. Do you have enough information to answer? Is the question "Is it the image?" irrelevant? Options: -bin 229- bin 604- bin 346- bin 442- Output: The answer is "Yes, the question is irrelevant."</sample>
    <sample id="1542">MULTINSTRUCT Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Input: &lt;bin-198- bin 32&gt; &lt;bin-400- bin 193&gt; Input: The region that contains the text "de" is described as "A blue train in the front". Input: &lt;bin-229- bin 604&gt; &lt;bin-456- bin 664&gt; &lt;bin-646- bin 654&gt; Output: &lt;bin-229- bin 604&gt; &lt;bin-456- bin 664&gt; &lt;bin-646- bin 654&gt; Output: &lt;bin-242- bin 760&gt; &lt;bin-291- bin 203&gt; &lt;bin-339- bin 442&gt; Output: &lt;bin-242- bin 760&gt; &lt;bin-291- bin 203&gt; &lt;bin-339- bin 442&gt; Output: do you have enough information to answer? Is it the image or the question is irrelevant to the image? Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1543">MULTINSTRUCT Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Input: &lt;bin-198- bin 32&gt; &lt;bin-400- bin 32&gt; contains the text "de" described by "A blue bin in front of a train." Options: &lt;bin-366- bin 119&gt; &lt;bin-448- bin 76&gt; &lt;bin-456- bin 574&gt; &lt;bin-666- bin 654&gt; &lt;bin-684- bin 654&gt; Output: &lt;bin-229- bin 604&gt; &lt;bin-346- bin 475&gt; Output: &lt;bin-242- bin 242&gt; &lt;bin-242- bin 190&gt; &lt;bin-2019- bin 203&gt; &lt;bin-339- bin 339&gt; Output: The content of the image is irrelevant to the question. Is it the question?</sample>
    <sample id="1544">Multi-modal Instruction Tuning</sample>
    <sample id="1545">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. Use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1546">Multi-Modal Instruction Tuning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. Use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1547">Implementation Details

Training details:

Pre-trained OFA-Large model (472M)

Mix all the instances for all tasks.

Each instance is randomly combined with one of its instruction templates.

Testing details:

For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.

We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">Implementation Details

Training details:
Pre-trained OFA-Large model (472M)
Mix all the instances for all tasks.
Each instance is randomly combined with one of its instruction templates.

Testing details:
For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">Implementation Details

Training details:
- Pre-trained OFA-Large model (472M)
- Mix all the instances for all tasks.
- Each instance is randomly combined with one of its instruction templates.

Testing details:
- For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.
- We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">Evaluation Metrics For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification) we report the Accuracy. For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue) we report the Rouge-L. For NLP tasks, we report Rouge-L. We also compute the aggregated performance for each model based on the mean of the model's performance on all multimodal and NLP tasks. We use Rouge-L as the performance score for most tasks, and Accurac not only have accuracy as a metric.</sample>
    <sample id="1551">Sensitivity How sensitive the model is towards variety of instructions for the same task: E[QT] σ(i, y)T [E[(x, y)D]L(f(i, x, y))] µ(i, y)T [E[(x, y)D]L(f(0, x, y))]</sample>
    <sample id="1552">Effectiveness of Instruction Tuning on MULTINSTRUCT Model | ROUGE Max ACC Visual Enf. Visual Spatial Reasoning ACC Transfer Learning from NATURAL INSTRUCTIONS 56.48 ± 1.12 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 33.14 3</sample>
    <sample id="1553">Effectiveness of Instruction Tuning on MULTINSTRUCT Model | Common Sense VQA ACC Visual Entailment Visual Spatial Reasoning ACC OPE Max 29.01 32.68 35.47 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.70 33.7</sample>
    <sample id="1554">Impact of Increasing Multimodal Instruction Task Clusters

- **Img Und**: VQA + Image Understanding
- **Grounding**: Grounded Matching + Grounded Generation
- **MISC, ITM**: Temporal Ordering + Miscellaneous + Image Text Matching
- **Relation**: Visual Relationship
- **Region**: Region Understanding
- **NLP**: NLP tasks

**Figure: Model Performance as the Timemodal Instruction Task Clusters**

The image on the right shows a graph with two lines representing different performance metrics over time. The x-axis is labeled "Time (s)" and the y-axis is labeled "Performance (%)". There are two lines on the graph:

1. A green line labeled "Grounding" which starts at around 10% performance and increases steadily to about 30%.
2. A blue line labeled "Relation" which starts at around 5% performance and increases more sharply to about 40%.

The graph illustrates how model performance improves over time for different types of instruction task clusters, specifically in grounding and relation tasks.</sample>
    <sample id="1555">Effect of Diverse Instructions on Instruction Tuning

o OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.

Table 3: Effect of Different Number of Instructions. Performance of OPAsatlin, finetuned on different numbers of instructions.

# of Instructions Aggregated Performance Sensitivity

5 Instructions 47.82 10.45</sample>
    <sample id="1556">Effect of Fine-tuning Strategies on Model Sensitivity

- Instruction tuning on Multinstruct can significantly reduce the sensitivity of OFA.
- Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.

[Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.]

The image shows a bar chart comparing the sensitivity of different models (OFA, OFA+Multinstruct, OFA+Natural Instructions) on unseen evaluation tasks. The y-axis represents the sensitivity score, and the x-axis lists the different models. The chart indicates that OFA has the highest sensitivity score, while OFA+Natural Instructions has the lowest, suggesting that transfer learning from the Natural Instructions dataset significantly reduces model sensitivity.</sample>
    <sample id="1557">Zero-Shot Performance on NLP Tasks Instruction Tuning on Multinstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset. Model ROUGHL 0.25 1.16 Transfer Learning from NATURAL INSTRUCTIONS OPA 4.361 OFANattnlinstruct 4.789 OFAMegatron 30.79 Table 4: Zero-Shot Performance on NLP tasks. The best performance is reported on ROUGHL, and the best performance is bold.</sample>
    <sample id="1558">Conclusion

First large-scale multi-modal instruction tuning dataset.

Contains 62 multi-modal tasks from 10 broad categories.

Significantly improve the zero-shot capability of OFA via instruction tuning.

Explore several transferring learning techniques and show their benefits.

Design a new metric sensitivity.</sample>
    <sample id="1559">One More Thing! We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!</sample>
  </task>
</testset>