<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络抓取数据，包括政治新闻媒体。</sample>
    <sample id="1">根据所提供的英文内容，这篇论文的作者所属机构是微软研究。</sample>
    <sample id="2">The presentation introduces a research paper titled "LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding" by the Ant Group. The paper focuses on the visually rich document understanding problem, aiming to understand various types of documents such as forms, receipts, and posters. It highlights the success of self-supervised pre-training techniques and multi-modal models in recent years but also points out that existing document pre-training models suffer from reading order issues.

The proposed LayoutMask model addresses these issues by using text and layout information as input and enhancing text layout interactions and layout representation learning during pre-training. It differs from previous studies in three aspects: choice of word positioning, masking strategy, and pre-training objectives. Instead of global word positioning, LayoutMask uses local word positioning, which does not provide cross-segment orders. To infer global reading order, LayoutMask jointly uses 1D positioning, 2D positioning, and semantic information.

The presentation also discusses two novel masking strategies: whole word masking and layout-aware masking. Whole word masking sets masks at word level instead of token level, making it more challenging for the model to predict masked words. Layout-aware masking has a higher probability of masking the first and last words of each segment, requiring the model to pay more attention to context in preceding or succeeding segments.

A new pre-training objective called mask position modeling is introduced, which has the same pre-training objective as masked language modeling but recovers relative 2D positions during pre-training. This task helps the model learn better layout presentations by inferring spatial relationships between tokens based on thematic relations and 2D positioning clues.

Experiments compare the performance of LayoutMask with different layout information for word positioning, local word positioning, and global word positioning on both FD and SRE datasets. The results show that LayoutMask performs better than local word positioning and similar to global word positioning, especially in cases with vertical layouts and misleading numbers.

The presentation concludes by summarizing the key findings and encouraging viewers to refer to the paper and posters for more details.</sample>
    <sample id="3">嗨，欢迎来到我们关于DePlain的演示，DePlain是一个新的语料库，用于德语文本简化，适用于文档级别和句子级别。我的名字是Ragina Strohn，我将指导您完成演示的第一部分。首先，我们来定义文本简化。文本简化是一种过程，通过调整文本以改善特定目标群体对文本的理解，例如阅读困难的人或非母语使用者。要训练文本简化模型，我们需要平行文本对，例如文档或句子的平行对。在示例中，您可以看到一个复杂德文句子与其简化后的德文翻译的并行句子对。为了简化句子，可以使用不同的技术，如词汇替换、从句省略、从句省略再排序或插入短语。现在，我们提出了一个新的语料库DePlain，因为近年来存在一些问题。例如，这些语料库太小，无法用于训练文本简化模型。另外三个最近提出的模型都是自动对齐的，这意味着它们可能包含错误的对齐。因此，我们提出了DePlain的新语料库，它分为两个子语料库：DePlain API和DePlain Web。DePlain API基于新闻文本，在DePlain API中手动对齐了483篇文档，结果产生了大约30,000至13,000个并行句子对。对于DePlain Web，这个语料库包括不同领域，并且我们手动对齐了750篇文档。另一方面，我们还使用自动对齐方法对这些文档进行了对齐。总共，我们得到了30,450个句子对。我们进一步分析了我们的句子对，例如，根据简化类型，可以看到圣经文本比新闻文本或其他语言学习文本更加强化简化。此外，我们可以看到DePlain语料库具有不同简化转换的高多样性。例如，在DePlain API语料库中，我们拥有更多排序和词替换，而在DePlain Web语料库中，我们拥有更多重写。现在让我们看看我们可以用这个语料库做什么。你好，我是Omar，现在我将讨论DePlain数据集的用途案例。对于第一个用途案例，我们可以评估自动对齐方法。近年来，已经提出了许多对齐方法，但在机器翻译的背景下，我们有两个平行文档，分别用不同语言编写，我们想提取两份文档中句子的对齐。但在我们的用途案例中，我们试图提取具有相同语言和相同内容但处于不同复杂度级别的两份平行文档之间的句子对齐。现在，由于我们拥有DePlain语料库，其中包含手动对齐的句子，我们可以使用这些句子作为标准对齐来评估一些已提出的对齐方法。我们在论文中出版了这些方法的适应性和代码，以便在实验中运行。最后，我们得出结论，对于德语文本简化，最佳的自动对齐方法是MassAlign方法，您可以在论文中找到该方法的代码。第二个用途案例我们在论文中展示的是自动文本简化，通过微调语言模型来产生简化文本，从复杂的输入文本中产生简化文本。我们已经微调了Long Impart模型来产生文档级别的简化，我们还微调了Normal Base Long Impart模型来产生句子级别的简化。您可以在论文中找到所有检查点以及实验的详细评分和评估指标。我们得出结论，这种基本微调可以产生或获得比 baseline 分数更好的分数，并将其作为未来自动文本简化问题的基准。谢谢大家的聆听，希望能在会议期间与 everyone 见面。谢谢。</sample>
    <sample id="4">演讲者的名字是Kayo Yin。</sample>
    <sample id="5">根据所给的英文内容，他们使用一种语言模型获得 82%-87% 的准确率。</sample>
    <sample id="6">The presentation introduces a research work titled "Towards Unifying Multi-Lingual and Cross-Lingual Summarization." The work is a collaborative effort by Jian Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, and Jie Zhou from Soochow University, WeChat AI, Beijing University of Posts and Telecommunications, and Fudan University. The researchers unify previous multi-lingual summarization and cross-lingual summarization into a more general setting called Many-to-Many Summarization (MTM). MTM aims to build one single summarization model that can precisely summarize a document in any source language and generate a summary in any target language. They conduct preliminary studies to provide deeper analysis among multi-lingual summarization, cross-lingual summarization, and MTM. The results show that MTM helps the summarization model better transfer task knowledge across different languages than previous multi-lingual summarization and cross-lingual summarization. The researchers propose a three-stage training process for MTM: 1) Pre-training with parallel corpora, 2) Cross-lingual pre-training with noisy parallel sentences, and 3) Task-specific fine-tuning using pseudo Many-to-Many summarization samples. The experiment results show that MTM outperforms previous strong baselines, including MBR-50 and MBR-5.</sample>
    <sample id="7">根据所提供的英文内容，CoNLL-2003 标注器仍然有效。研究结果表明，尽管CoNLL-2003 已经使用了20多年，但通过适当的模型架构、模型大小和微调示例，这些标注器仍然可以泛化到现代数据。</sample>
    <sample id="8">提出的人工评估方法新颖之处在于它通过明确标注每个模型响应是否表现出某些行为，如提供无关信息或自相矛盾，来减少人类评估的主观性。这种方法被称为“聊天行为标注”或“ABC评估”，旨在更精确和可靠地评估对话质量的多个方面。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="10">根据所给的英文内容，可以采取以下措施来提高分数：1. 提供与注释者相同的背景知识，如实体名称、描述或图片，以帮助模型更好地理解。2. 使语言模型能够访问部分重叠的背景知识，而不是完全相同的背景知识，这可以提高准确率，但仍然保持在82%至87%之间。3. 通过提供更多的训练数据和更复杂的模型架构来增强模型的泛化能力。4. 使用更高质量的注释数据，包括更详细的描述和更准确的实体选择。5. 采用先进的自然语言处理技术，如多模态学习和上下文理解，以提高模型对间接引用表达的理解能力。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presented "Do Androids Laugh at Electric Sheep? Humor 'Understanding' Benchmarks from The New Yorker Caption Contest" at an event. This work is a collaboration with researchers from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI. Large language models can now generate and explain jokes, as demonstrated by Google's 540 billion parameter PaLM language model, which successfully explained a joke about TPUs (the type of hardware it was trained on). However, when tested for humor understanding, models like ChatGPT often fail to recognize jokes, such as a knock-knock joke involving a pineapple. To evaluate this, Hessel's team used The New Yorker Caption Contest data, which involves submitting captions for cartoons. They created three tasks: matching, quality ranking, and explanation generation. The best model, CLIP fine-tuned on their annotated corpus, achieved around 62% accuracy in matching, compared to a 20% random guessing baseline. Humans scored around 94%, highlighting a significant gap in humor understanding. Even with human-authored descriptions, language models like GPT-4 still perform poorly in matching and quality ranking tasks. In the explanation generation task, GPT-4's explanations were often inaccurate, with human evaluations preferring human-written explanations over those generated by GPT-4.</sample>
    <sample id="12">根据所给的英文内容，这篇论文有五位作者。</sample>
    <sample id="13">Daniel Rotem在演讲中介绍了他在Roy Schwartz教授的实验室里进行的研究，研究的主题是“找到甜蜜点：低资源环境下的自适应推断分析与改进”。他解释了自适应推断是一种减少大型语言模型推理时间的方法，通过利用真实世界数据的复杂性，使用低容量模型处理简单样本，从而降低平均推理成本。两种常见的自适应推断方法是多模型和早期退出。多模型方法涉及存储多个模型，每个模型都有一个分类器，它们分别在训练集上训练，用于推理时按顺序运行，直到分类器决定停止计算。早期退出方法涉及在模型的中间Transformer层上训练多个分类器，它们一起训练，在推理时，一个样本通过模型直到分类器决定停止，从而节省了计算资源。Rotem讨论了每种方法的优点和缺点，指出多模型方法虽然更灵活、易于扩展，但存储成本高且存在重叠计算的问题；而早期退出方法虽然推理速度更快、内存效率更高，但共享参数可能导致性能下降。Rotem还提出了一个名为“冲突梯度”的现象，即每个分类器在优化自身目标时更新模型权重，可能会相互干扰，降低所有分类器的性能。为了验证这一假设，他比较了早期退出模型和多模型分类器的性能，发现多模型分类器在BERT预训练语言模型的基线和大型版本上平均提高了2.3%，其中最早分类器的性能提升了5.2%。此外，他还分析了速度-准确性权衡，发现对于高速推理，多模型方法表现更好，但在使用较晚分类器进行预测时，早期退出方法表现更好，因为多模型方法会遭受重叠计算的开销。基于这些 findings，Rotem提出了“Sweet”方法，这是一种针对早期退出架构的创新微调方法，通过让每个Transformer层只接收其后分类器的损失函数更新，避免了冲突梯度问题。实验结果表明，“Sweet”方法在评估个体层时缩小了与早期退出方法的差距，但在某些情况下，较晚分类器可能受到负面影响。总体而言，Rotem的研究展示了早期退出训练过程中存在冲突梯度的现象，并提供了第一个公平比较早期退出和多模型自适应推断方法的案例，同时引入了“Sweet”方法，为早期退出架构的微调算法研究提供了新的方向。</sample>
    <sample id="14">嗨，我的名字是Adam Siprowski，这篇论文讲的是协调结构的依赖关系。正如你们所知，不同的依赖关系结构被不同的理论和语义学派假定，比如在普遍依赖性中，Lisa Bart和Maggie之间的协调结构是这样的，第一个连词是整个协调结构的主干，在这个例子中是Lisa。类似的假设出现在伊戈尔·米特鲁克的意义文本理论中，再次，整个协调结构由第一个连词领导。所以这两个方法都是对称的，它们突出一个连词。还有一些对称的协调结构的方法，比如布加罗的方法，连词领导的方法，使用了布加罗依赖树图，协调结构由连词领导，所以我们得到从主语到所有连词的依赖关系。最后，还有一个多头的方法，比如在德卡森的词典语法中使用，所有连词都是协调结构的主干，所以我们得到从治理者，比如Lars，到所有连词的依赖关系。这些是单独的Lisa和Maggie。现在，这篇论文的目标是提出一个关于对称协调结构的新论点，与这些对称协调结构相对立的不对称协调结构。这个论点基于依赖性长度最小化原则，我将在这些例子的基础上解释。在英语中，正如你可能知道的，直接宾语更喜欢靠近动词，而介词短语可能离动词更远。所以“March read it yesterday”很好，因为直接宾语“it”离动词很近，而“March read yesterday it”则更糟糕，因为这里介词“yesterday”在动词和直接宾语之间。然而，这个效果可能会在直接宾语非常长的时候缓解，因为然后它可以移动到介词后面。这个例子说明了这一点。所以这两个句子都很好。March read this absolutely fascinating book about the BC yesterday。这是好的。但是相反，我们有这个很长的NP，但这也是可以的。March read yesterday this absolutely fascinating book about the BC。所以这里的理由是，这可能是因为尽管这个句子违反了传统句法原则，即直接宾语应该离动词更近，但它满足了依赖性长度最小化原则，即较短的依赖性更可取。所以这两个树只显示了关键依赖性的长度，即那些在两个结构中不一致的依赖性。所以这里我们有一个从“red”到“book”的依赖性长度为7（以单词为单位），从“red”到“book”的依赖性长度为4，所以总共是11。当你交换这两个成分时，这两个依赖性的总和变成6，所以不是11，6更短，这就是为什么这听起来很好。它违反了一个原则，但满足了另一个原则。所以，我们做了什么？我们从增强版本的Penn树库中提取了关于协调的数据，并看看为什么我们不能使用普遍依赖性。这些数据证实了很多人多次做出的观察，即左连词往往更短，比如“盐和胡椒”和“胡椒和盐”，测量单位是音节。另外，我们还观察到，当两个连词之间的长度差变大时，这种趋势会加剧。所以，当两个连词之间的长度差变大时，左边的连词倾向于成为更短的那个，而且长度差越大，左边的连词就越短。但是，当治理者在右边时，比如“Lars governs the coordination”，这种效应就消失了。所以我们展示了通过测量字符长度（第一列是音节，第二列是单词，第三列是字符）来展示这一点。我们可以看到，当治理者在左边时，左边连词变短的趋势随着两个连词之间的绝对长度差的增加而稳定增长。同样，当没有治理者时，比如在协调句子中，也会出现这种情况，但当治理者在右边时，这种趋势就消失了。我们在论文中展示了如何通过测量字符长度来展示这一点。我们将集中关注第三列。我们在这里看到的是，当治理者在左边时，左边连词变短的趋势随着两个连词之间的绝对长度差的增加而稳定增长。同样，当没有治理者时，比如在协调句子中，也会出现这种情况，但当治理者在右边时，这种趋势就消失了。我们在论文中展示了如何通过测量字符长度来展示这一点。</sample>
    <sample id="15">根据幻灯片上显示的信息，这篇论文有三位作者：Mathias Lindemann、Alexander Koller 和 Ivan Titov。</sample>
    <sample id="16">根据所给的英文内容， Bible texts 的简化程度更大。</sample>
    <sample id="17">The speaker introduces a method for multi-modal relation extraction, which aims to determine the semantic relationship between entities in text data. The method involves representing text and images as visual and textual graphs, merging them into a unified cross-modal graph, and screening the initial structures by filtering nodes and adjusting edges. Multimodal topic information is used as additional semantic supplementary to enrich the overall context. Experiments on the MRQA dataset show that the proposed method can obtain higher performance than the text-based method and achieves significant improvements over existing models.</sample>
    <sample id="18">一个偏好的示例是“盐和胡椒”，因为并列词“盐”和“胡椒”在长度上更接近，这符合短语长度最小化原则。</sample>
    <sample id="19">The presentation introduces a survey on efficient open-domain question answering, focusing on the two-stage model proposed by Dandan Chen in 2017. The first stage uses retrieval to fetch relevant context from Wikipedia Corpus, and the second stage uses a reader to understand the question and retrieve the necessary information to answer it. The retrieval process involves a question encoder and a document encoder. The speaker highlights challenges such as the large size of the Wikipedia Corpus (26 million documents, 20 GB), the index file (65 GB), and the use of multiple large language models with millions of parameters. These challenges make open-domain question answering systems resource-intensive for real-time applications and constrained devices. To address these issues, the presentation discusses techniques like approximate nearest neighbor search, skipping rate, document filtering, and model compression to improve efficiency. It also compares existing models based on memory and performance, concluding that retrieval-only systems are better suited for low-resource environments, while retrieval and reading systems are more appropriate for trade-offs between speed and performance. Future work includes deploying open-domain question answering systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">是的，这些模型可以用于您的研究。演讲者提到了在他们的GitHub仓库中提供训练脚本和预训练模型，表明这些资源是公开可用的。这将使您能够利用这些模型进行自己的实验或项目，特别是如果您正在处理生物医学或临床领域的自然语言处理任务。</sample>
    <sample id="21">DEplain-apa 包含来自新闻来源的文档。</sample>
    <sample id="22">有助于良好泛化的因素包括模型架构、模型大小和更多的微调示例。</sample>
    <sample id="23">The paper discusses the challenges faced by text-image models in rendering visual text and proposes a strategy to improve their performance. The authors investigate the performance of different text encoders, including T5, PaLM, and BitT5, in terms of their ability to spell words correctly. They find that while larger versions of T5 and PaLM models can achieve near-perfect accuracy in spelling, they are impractical due to their size and data requirements. In contrast, BitT5, which receives individual characters instead of subword tokens, performs well across all scales.

The authors then propose augmenting the existing text representation with an additional text representation from BitT5 to improve the text rendering capabilities of the image generation model. This addition allows the model to effectively copy input characters to the output, resulting in better image generation characteristics and improved text rendering. However, the diffusion model may still introduce errors, so the main takeaway is the development of a new efficient strategy for improving model spelling ability by incorporating character-aware models.</sample>
    <sample id="24">左并列词是否更短可以通过比较两个并列词的长度来衡量。如果第一个并列词比第二个并列词短，那么它就是更短的。</sample>
    <sample id="25">要研究支配词位置的影响，可以设计实验来比较不同支配词位置下的句子长度和可接受性。实验可以涉及生成一系列句子，其中包含相同内容但具有不同支配词位置的句子。例如，可以比较“Lisa bought and Megan”和“Megan and Lisa bought”之间的可接受性，以及它们与不同长度的从句的关系。通过分析这些句子的可接受性和长度数据，可以评估支配词位置对句子结构和理解的影响。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳，仅凭43个例子的训练无法显著提高性能。</sample>
    <sample id="27">根据所给的英文内容，无法确定论文的作者人数。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">根据所给的英文内容，语境感知 MT 模型在处理某些话语现象时比语境无关模型更有优势，例如正式性和词汇连贯性。然而，在其他现象如性别、名词形式和动词形式方面，语境感知模型并没有明显优于语境无关模型。这表明在处理需要语境才能正确翻译的特定现象时，语境感知模型具有显著优势，但在其他方面，两者的性能相当。</sample>
    <sample id="30">The paper introduces a framework called LLM-Blender, which is designed for ensemble learning of large language models (LLMs). The core idea behind LLM-Blender is to use pairwise ranking and generative fusion. The framework is developed by a team from the Allen Institute for AI and the University of Southern California.

The paper highlights that while many LLMs claim to have achieved great performance on leaderboard benchmarks, this does not necessarily mean they are optimal for every input example. The optimal selection of models can vary significantly across different inputs. For instance, although Vicuna has the best average overall performance among 11 models, it only ranks as the best in 21% of examples.

To address this issue, LLM-Blender proposes a two-stage framework. In the first stage, multiple LLMs are run on a given input to generate their outputs. These outputs are then compared using a pairwise ranking model called PairRanker. PairRanker encodes pairs of candidates along with the input to better analyze subtle differences between them. This is different from traditional methods that score each candidate individually and rank them based on those scores.

In the second stage, the top k candidates (e.g., top 3) are selected based on the rankings provided by PairRanker. These candidates are then used as inputs to a generative fusion model, which outputs the final result for the input.

The paper also discusses the design of PairRanker, emphasizing its ability to use pairwise comparisons to learn and infer the quality of all candidates more carefully. It compares PairRanker with other ranking methods and shows that it is better correlated with oracle rankings.

The evaluation of LLM-Blender uses a new dataset called MixInst, which consists of existing instruction datasets and candidates from 11 open-source LLMs. The results show that LLM-Blender outperforms individual models like OpenAssistant and Vicuna in terms of performance metrics, particularly in 68% and 76% of examples respectively.

Overall, LLM-Blender is presented as a simple yet effective framework for ensemble learning of LLMs, offering significant performance improvements over using any single model for all inputs.</sample>
    <sample id="31">根据幻灯片显示，论文的作者来自约翰霍普金斯大学、普渡大学和MIT。此外，幻灯片底部还提到了Meta AI，表明该机构可能参与了研究或合作。</sample>
    <sample id="33">该框架通过比较用户注释与现有数据集和模型的预测和标签来量化立场。它使用皮尔逊的相关性分数来衡量这些注释与数据集和模型之间的相似性，从而评估它们的立场。</sample>
    <sample id="34">The presentation introduces Crest, a joint framework for rationalization and counterfactual text generation. Developed by Marcos Trevizo, Alexis Ross, Nuno M. Guerrero, and André F. T. Martins, Crest aims to combine selective rationalization and counterfactual generation to leverage their complementary strengths. The framework consists of two main components: a rationalizer model that generates counterfactuals and a predictor model that uses these explanations to produce decisions.

The rationalizer model includes a trainable masker component that highlights meaningful tokens in the input, and a predictor component that processes these explanations to make decisions. To generate counterfactuals, the original input is masked along with the gold label, and an editor (a masked language model) fills in the masked spans with new tokens. Human evaluation experiments showed that Crest-generated counterfactuals were more valid and natural than those produced by other methods.

Crest also proposes using counterfactuals for data augmentation and rationalization with both factual and counterfactual examples. By training on factual examples and testing on domain-specific datasets, Crest outperforms other methods in terms of accuracy and robustness. The framework's ability to generate plausible and counterfactually similar explanations is demonstrated through three dimensions of analysis: plausibility, forward similarity, and counterfactual similarity.

Overall, Crest provides a valuable tool for improving downstream models by generating high-quality counterfactuals that focus on the critical parts of the input, leading to more interpretable and reliable decision-making processes.</sample>
    <sample id="36">多语言机器翻译（MLMT）具有 scalability、速度和低资源语言对改进等优点。然而，它也存在容量限制和训练与推理速度之间的权衡。为了解决这些问题，我们提出了语言特定层（LLs）的概念，即为每种语言使用一个常规Transformer层。在推理时，只调用所选语言的子层，从而保持推理成本不变。我们还研究了LLs的放置位置，发现将LLs放在解码器中效果不佳。因此，我们专注于编码器，并使用一种称为“选择性权重”的方法来学习最佳放置位置。这种方法涉及为每个编码器层定义共享权重、源权重和目标权重，并通过训练模型来学习这些权重的重要性。在推理时，我们根据权重选择LLs。实验结果表明，我们的LLs架构在多种语言对上显著提高了性能，同时保持了推理速度。</sample>
    <sample id="37">根据所给的英文内容，之前的研究表明，当人类受试者被给予相同的人格化提示时，他们能够表面种族刻板印象。这使研究者能够将生成的人物描述与人类手写回应进行直接比较。</sample>
    <sample id="38">根据所给的英文内容，此研究使用了来自Pankration增强版本的数据。</sample>
    <sample id="39">根据所提供的英文内容，这篇论文有两位作者。</sample>
    <sample id="40">与认知失调密切相关的任务包括辩论独立认知失调分类和CE任务。辩论独立认知失调分类涉及确定来自不同人的两个辩论陈述是否一致或不一致，而CE任务涉及二元分类的扩展和比较类别。这些任务与认知失调概念相关，因为它们涉及评估不同观点之间的关系，这是认知失调研究中的关键方面。</sample>
    <sample id="41">The presentation introduces a project called "Peacock," which stands for Personal Common Sense Knowledge for Consistent and Engaging Narratives, developed in collaboration with Sony Group Corporation. The project aims to enhance natural language processing systems by providing a comprehensive understanding of real-world personness through rich world knowledge and complex interconnections.

Peacock contains about 3,800 personas and 40,000 distinctive attributes, forming approximately 100,000 person-inferences or facts. Additionally, about 9,200 attributes are connected to two or more personas, contributing to the rich interconnections within Peacock. The relationships between personas and their attributes are framed in three dimensions: four types of main relations, as well as interactivity and distinctiveness.

The project was built in three steps: selecting personas from existing common sense knowledge graphs, inducing attributes of personas from both common sense knowledge graphs and large-scale pre-trained language models, and cross-checking the annotations of Peacock relations using a joint human-AI majority voting scheme. Expert studies show that this method yields high-quality relation annotations with an average accuracy of 87% and F1 score.

Peacock is used to train a bar-based common knowledge generator on a personal attribute inference task, where the model predicts an attribute of a given persona with a target relation. Compared to large-scale pre-trained language models like GPT-3 and GPT-3.5, the model trained on Peacock achieves better automatic evaluation results and higher acceptance rates in human evaluation, indicating its reliability as a person knowledge base.

The project also explores whether Peacock can improve downstream narrative modeling by investigating a person-grounded dialogue generation task on the Coreset AI 2 Person Chat dataset. By retrieving relevant facts from Peacock and augmenting each speaker's profile, the Peacock augmented model achieves better dialogue generation in terms of fluency, consistency, engagement, and personal expression. The impact of Peacock's person-centric common sense knowledge is more positive compared to general social common sense knowledge.

Overall, Peacock is proposed as a world-level person common sense knowledge graph that can be used to train reliable person knowledge generators and enable more consistent and engaging narrative modeling.</sample>
    <sample id="42">根据幻灯片上显示的英文内容，这篇论文有两位作者。</sample>
    <sample id="43">根据幻灯片显示，这篇论文有七位作者：Vasudha Varadarajan、Swanie Jhungh、Syeda Mahwish、Xiaoran Liu、Jonah Luby、Christian C. Luhmann和H. Andrew Schwartz。</sample>
    <sample id="44">所介绍的框架与以前的研究不同之处在于它比较了真实用户与数据集和模型的预测和标签，而不是仅仅关注注释者的注释一致性或注释者分布。</sample>
    <sample id="45">根据所给的英文内容，在三个比较设置中，黑人女性的刻板词汇与刻板词汇的重叠最多。</sample>
    <sample id="46">根据所给的英文内容，比较了DeepL和Google Translate这两个商业系统。</sample>
    <sample id="47">嗨，我是张冰，是美国大学的PhD学生。今天我要展示我们关于从预训练数据到语言模型再到 downstream 任务的跟踪政治偏见导致了NLP模型中的不公正性的研究工作。语言模型是在大规模网络抓取数据上训练的，而政治新闻媒体在预训练数据中得到了很好的覆盖。根据对C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》和《 Huffington Post》等报纸在语言模型训练数据中被广泛涵盖。这为语言模型应用带来了双重影响：一方面，它们能够学习多样化的观点，庆祝民主和思想的多样性；另一方面，这些不同的政治观点内在地具有社会偏见，可能会导致下游任务应用中的公平性问题。为了应对这个问题，我们提出了一个研究框架，从预训练数据到语言模型再到下游任务，具体回答以下问题：首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在其中扮演的角色？其次，不同政治倾向的语言模型在下游任务上的表现如何？这种差异是否会导致NLP应用中的公平性问题？我们首先提出了一种方法，通过使用政治问题集（如政治 compass测试）来评估语言模型的政治倾向，以确保评估基于政治科学文献。一些初步结果表明，语言模型确实具有政治倾向，它们占据了政治 compass上的四个象限。GPT-4是最liberal的语言模型之一，而GPT系列通常比BERT系列及其变体更liberal。我们还旨在研究语言模型的政治偏见是否是从训练数据中继承而来。为此，我们进行了受控实验，通过在六种不同的 partisan corpus 上进一步预训练语言模型，这些 corpus 分别来自新闻和社交媒体，并进一步分为不同的政治倾向。通过在这些 partisan corpus 上进一步预训练语言模型，我们可以观察到语言模型的 ideological 坐标也会相应地发生变化。例如，对于Roberta，在进一步 finetune 和进一步训练于左翼倾向的红色 corpus 后，我们可以看到其政治偏见出现了显著的liberal shift。我们还尝试研究语言模型是否能反映出我们现代社会中存在的 polarization。我们将预训练 corpus 分为特朗普总统之前和之后两个时期，分别在两个时期上预训练语言模型。我们可以看到，语言模型在2017年之后一般具有更远离中心的政治倾向，这表明语言模型也能反映出我们社会中存在的 polarization。最后，我们评估了具有不同政治倾向的语言模型在 hate speech detection 和 fake news detection 等 NLP 应用中的性能，这些应用通常涉及语言模型，并可能产生重大影响。如果我们按类别评估性能，即根据不同 demographics 或政治倾向的新闻媒体将性能分开，我们可以看到一个模式：例如，在 hate speech detection 中，left-leaning language models 在检测针对社会少数群体的 hate speech 方面表现更好，但在检测针对更有权力群体的 hate speech 方面表现较差。相反，right-leaning language models 在检测针对白人和男性群体的 hate speech 方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的 hate speech 方面表现较差。类似的趋势也发生在 fake news detection 中，left-leaning language models 在检测来自其对立政治倾向的 misinformation 方面表现更好，反之亦然。这表明，具有不同政治倾向的语言模型给出了不同的预测，基于它们的社会背景。附录中有许多 qualitative 的例子，以进一步说明这一点。这表明，政治偏见语言模型存在一个重要问题，需要解决。例如，如果 right-leaning language models 被用于 hate speech 或 misinformation 的 fine-tuning 并部署到一个受欢迎的社交媒体平台，这可能会导致持有相反政治观点的人被边缘化，而针对少数群体的 hate speech 可能会 rampant 地传播而没有任何控制。因此，我们需要承认并解决由语言模型政治偏见引起的问题。最后，我想强调一下关于语言模型政治偏见的独特 dilemma。如果我们不净化训练数据中的政治观点，偏见将从预训练数据传播到语言模型，最终导致公平性问题。如果我们尝试净化数据，我们可能会面临审查或排除的风险，很难确定什么应该被保留在语言模型训练数据中。这就像电学中的电容器问题。总之，这就是我今天要讲的所有内容。谢谢你们的时间。</sample>
    <sample id="48">根据幻灯片上显示的信息，这篇论文有五位作者。</sample>
    <sample id="49">根据所给的英文内容，MPP 评估最多涵盖了 124 个词元的上下文长度。</sample>
    <sample id="50">The presentation introduces a new corpus called "DePlain," designed for German text simplification at both the document and sentence levels. It begins by explaining text simplification as a process to enhance text comprehension for specific target groups, such as people with reading difficulties or non-native speakers. The presenter, Regina Störrn, outlines the need for parallel pairs of texts to train a text simplification model, using examples like complex German sentences and their translations into simpler language.

The presentation then details the creation of the DePlain corpus, addressing issues with existing corpora that are too small or automatically aligned, which can lead to errors. DePlain is divided into two sub-corpora: DePlain API, based on news texts, and DePlain Web, which includes various domains. DePlain API consists of 483 manually aligned documents resulting in approximately 30,000 parallel sentence pairs, while DePlain Web includes 750 documents aligned both manually and automatically, totaling 30,450 sentence pairs.

Further analysis reveals that Bible texts are more strongly simplified compared to news texts and language learner texts across different levels of simplification. The DePlain corpus also exhibits a high variety of simplification transformations, such as reordering and word additions in DePlain API, and more paraphrasing in DePlain Web.

The presentation then discusses potential use cases for the DePlain corpus. One use case involves evaluating automatic alignment methods for extracting sentence alignments between parallel documents with the same language and content but different complexity levels. The presenter highlights the effectiveness of the MassAlign method for German text simplification and provides code for running this method.

Another use case is automatic text simplification by fine-tuning language models to produce simplified text from complex input. Two models were fine-tuned: one for document-level simplifications and another for sentence-level simplifications. The results showed that basic fine-tuning could achieve scores better than baseline scores, serving as a benchmark for future text simplification efforts.

The presentation concludes with a thank you to the audience and an invitation to meet during the conference.</sample>
    <sample id="51">他们的数据集覆盖了音乐、书籍和食谱三个领域。</sample>
    <sample id="52">Positionality（立场）是指人们持有的观点，这些观点源于他们的 demographics、identity 和生活经历。</sample>
    <sample id="53">演讲者的名字是Dawei Zhu。</sample>
    <sample id="54">The presentation by Vasudha Varadarajan, a PhD candidate in Computer Science at Stony Brook University, focuses on the development of a cognitive dissonance resource. The presentation begins by defining cognitive dissonance as two beliefs or actions that are inconsistent, illustrated with an example where a person states they know cigarettes can kill them but then smokes after a meeting. The importance of studying cognitive dissonance is highlighted through its impact on understanding social dynamics, belief changes, and mental health issues like anxiety disorders.

The presentation then discusses the rarity of cognitive dissonance in language data, noting that it was found in only 3.5% of annotated discourse unit pairs. To address this challenge, the presenter employed transfer learning and active learning strategies to enhance the detection of dissonance. Transfer learning involved using models from related tasks such as topic-independent dissonance stance classification and binary classification of expansion and comparison classes of PDB. Active learning strategies included cumulative and iterative updates, with the probability of rare class (PRC) strategy showing superior performance in collecting more dissonance samples.

The presentation concludes with a comparison of different active learning strategies, finding that the PRC strategy outperformed other state-of-the-art methods. Additionally, the feasibility of each strategy for annotation quality and costs was evaluated, concluding that PRC is a simple and effective strategy for rare class acquisition in cognitive dissonance research.</sample>
    <sample id="55">EDAtt 适应了现有的离线 ST 模型，因为它使用了已经存在的离线模型，而无需重新训练或采用特定架构。</sample>
    <sample id="56">根据所提供的英文内容，无法确定论文的作者人数。内容中没有提到作者的姓名或数量。</sample>
    <sample id="57">根据所提供的英文内容，被测模型不能在测试套件上运行。</sample>
    <sample id="58">KITMUS 有三个变体：背景预训练、背景预训练和背景预训练。</sample>
    <sample id="59">The presentation introduces Dr. BERT, a robust pre-trained model in French for biomedical and clinical domains. It begins with an overview of language modeling in healthcare, followed by the introduction of Dr. BERT, a biomedical model trained on NACHOS, a dataset of medical crawl data from the web. The presenter compares Dr. BERT with models trained on multiple pre-training settings and data sources, presenting results on 11 biomedical and clinical downstream tasks in French. The presentation concludes with an evaluation of the experiments and details on accessing the models. Since its release in 2018, Dr. BERT has become one of the most effective approaches to solving natural language processing tasks, offering significant performance gains over historical static and contextualized methods like Word2Vec, FastText, or ELMo. The model has been adapted to various languages and domains, including French, biomedical, and clinical, but specialized models for other languages are scarce due to the lack of in-domain data. The presenter questions the most appropriate data sources for a wide range of usage and compares Dr. BERT with Shubert, a model based on anonymized data from the University Hospital of Toulouse. The presenter also explores the impact of pre-training strategies on model performance, comparing models trained on different datasets and pre-training weights. The evaluation highlights that models perform best on tasks with data of the same nature as those used for training, but data from heterogeneous sources appears more versatile. Using more data generally translates into better performance, and from-scratch pre-training seems to obtain higher performance on most tasks. However, continual pre-training using the weight and tokenizer of PAMBERT trained on the 4GB subset of NACHOS shows comparable results to those obtained with Dr. BERT 4GB from scratch. The presenter concludes that their proposed system offers better performance on nine of the eleven downstream tasks and surpasses the results of the generic model, CAMBERT. Specialized data is observed to be better, but it doesn't scale well. All pre-trained models, training scripts, and datasets are freely available on the Yunaface platform and GitHub repository.</sample>
    <sample id="60">根据幻灯片上显示的Google Research标志，可以推断出论文的作者是Google Research的一部分。</sample>
    <sample id="61">最后一个研究问题是在弱监督学习中是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们。</sample>
    <sample id="62">The paper titled "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" by Nitay Calderon, Subhabrata Mukherjee, Roy Reichart, and Amir Kantor explores the potential of compressing large language models while preserving their performance. The study focuses on task-specific knowledge distillation in natural language generation (NLG) tasks, considering a variety of NLG tasks in realistic setups. The research investigates different architectural decisions, pruning techniques, and knowledge distillation approaches, including word-level and sequence-level distillation. The study challenges traditional sequence-level knowledge distillation by generating multiple pseudo-targets and proposes a novel joint teaching technique to address student exposure bias and improve learning. The research aims to provide a recipe for knowledge distillation in NLG, offering insights into efficient model compression and practical applications in industry-driven setups.</sample>
    <sample id="63">灵敏度指标衡量模型在任务中一致产生相同输出的能力，无论输入指令的微小变化如何。这表明模型对输入指令的依赖程度，较低的灵敏度值表示模型更稳定和一致。</sample>
    <sample id="64">演讲者的名字是金Wei。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。这是因为灵敏度衡量了模型在任务中保持一致输出的能力，而较低的灵敏度表明模型对输入的微小变化更稳定，从而提高了性能。</sample>
    <sample id="66">The presentation introduces a survey on deep learning for mathematical reasoning, highlighting its importance in human intelligence and decision-making. It discusses the development of machines capable of solving math problems and proving theorems, which has been a long-standing focus of AI and NLP. The survey covers tasks such as solving math word problems, visual contexts, and table contexts, with examples like geometry problems and theorem proving. Neural network architectures like sequence-to-sequence models and six-to-tree models have been proposed to formalize these tasks. Large language models (LLMs) have shown remarkable performance in various NLP tasks but face challenges in precise mathematical reasoning. To address this, researchers are exploring strategies like self-consistency and tool-augmented LLMs. The presentation also touches on the need for more datasets in low-resource languages and benchmarks for different domains, despite the impressive progress in the field.</sample>
    <sample id="67">本研究探讨了多语言翻译模型中的干扰问题，指出这些模型可能从不同语言对之间的协同中受益，也可能受到干扰。例如，训练英语到 Finnish的模型可能会提高英语到Estonian的质量，而英语到Chinese的模型则可能产生负面影响。许多方法被提出以减轻干扰，但它们往往在小型模型上效果显著，但在大型模型上并不总是优于调整基线。研究发现，当模型规模相对于数据量非常小时，严重干扰会发生；调整采样温度对于获得良好性能至关重要。对于简单的双语情况，有可预测的模型和数据大小缩放规律来预测损失；然而，在多语言情况下，其他因素如其他语言的数据量、语言相似性和总语言数量也会影响性能。研究发现，语言相似性和语言数量对干扰水平的影响较小。通过实验，研究者评估了不同模型架构和数据大小下的干扰水平，并发现严重干扰仅发生在参数不足的情况下，且随着数据量的增加，问题会缓解。研究建议，温度采样是一种简单有效的解决方案，通过调整温度值（T&gt;1）来增加低资源语言的样本数量，可以显著减少干扰，而无需使用任何专门的方法。</sample>
    <sample id="68">在预训练期间，模型会接收大量的文本数据作为输入。这些文本可以来自各种来源，如书籍、文章、网站和社交媒体帖子。模型的任务是学习语言的结构、模式和含义，以便能够生成连贯且有意义的文本。</sample>
    <sample id="69">根据所给的英文内容，通常需要每个类别20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">根据幻灯片显示，Myra Cheng、Esin Durham和Dan Jurafsky是斯坦福工程计算机科学系的成员。</sample>
    <sample id="71">本研究旨在解决自然语言处理中用户在选择实体时遇到的间接引用表达问题。我们提出了AltEntities Corpus，这是一个包含音乐、书籍和食谱三个领域的大型公开数据集。数据集通过 crowdsourcing 收集，使用卡通对话场景进行标注。在每个场景中，用户被要求从两个实体中选择一个，其中一个实体是直接引用（如歌曲名称），另一个是间接引用（如“更容易的那首”）。我们使用了多种采样方法来生成实体对，以确保它们在难度上具有梯度。对于音乐领域，我们展示了Google搜索结果；对于食谱和书籍，我们提供了维基百科中的背景文本和图片。我们的目标是评估大型语言模型（LLM）在理解用户意图和准确选择实体方面的性能。</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为现有的方法往往依赖于人类标注，这既昂贵又耗时。此外，这些方法可能无法捕捉到微妙的偏见或系统性偏见，而这些偏见可能在大规模数据集中并不立即显而易见。开发自动化和客观的方法对于识别和纠正训练数据中的偏见至关重要，从而减少模型中潜在的偏见。</sample>
    <sample id="73">演讲者的名字是马查塔。</sample>
    <sample id="74">The paper introduces Dense-Atomic, a method for constructing a dense knowledge graph by completing missing links in Atomic. Atomic is a large-scale common sense knowledge base that contains event-centered social space information, but it lacks many multi-hop paths due to the limitations of B2W and A2B links. Dense-Atomic completes many missing links in Atomic, including B2W, B2B, A2B, and A2W links, and also contains more multi-hop paths. The construction process consists of three parts: normalizing tail events, training a relation prediction model, and constructing Dense-Atomic. The paper proposes a relation prediction method called RAS-KGC, which utilizes semantic information by encoding both the head and tail events with a pre-trained language model. The paper also compares the performance of RAS-KGC with other relation prediction methods and shows that it performs better on both automatic and human evaluations.</sample>
    <sample id="75">The presentation introduces a joint semi-supervised learning framework for entity and relation extraction tasks. The framework aims to integrate label data and unlabeled data by propagating labels over heterogeneous graphs, considering the interconnections between labeled and unlabeled data. It consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The span feature generation involves generating representations for input tokens and their pairs based on a trained classifier. Heterogeneous graph construction creates a K-nearest neighbor graph for computational efficiency, examining similarity relations among unlabeled data and between labeled and unlabeled data. Joint label propagation refines pseudo-labels for entities and relations through the graph, diffusing labels through high-density areas formed by unlabeled data. Model optimization uses the soft mask function and standard masking operations to determine pseudo-labels, filtering lower-quality ones with confidence G and combining them with label data to retrain the classification model. Experiments on four datasets show significant improvements in both entity and relation tasks compared to baseline models.</sample>
    <sample id="76">政治偏见传播流程包括从预训练数据到语言模型，再到下游任务的过程。首先，评估语言模型的政治倾向，了解预训练数据可能对这些偏见的影响。其次，研究具有不同政治倾向的语言模型在下游任务中的表现，以确定潜在的公平性问题。最后，通过实验进一步验证政治偏见是否被模型学习和传播。</sample>
    <sample id="77">本视频介绍了改进摘要事实一致性的工作，这是通过自然语言反馈实现的。该研究是耶鲁大学和微软研究院的合作成果，大部分工作是在微软研究院实习期间完成的。研究引入了一个名为“defacto”的新数据集，其中包含人类演示和反馈，用于提高摘要事实一致性。该数据集提供了全面分析，并为摘要模型的事实一致性提供了进一步的见解。基于该数据集，提出了三个新的NLP任务：摘要编辑、反馈生成和自动事实错误纠正。研究重点是抽象文本摘要，特别是摘要事实一致性的质量。人类演示和反馈基于现有摘要模型生成的原始系统摘要。注释者被要求判断摘要是否事实一致，并提供人类校正的、事实一致的摘要，如果原始摘要不一致。他们还必须提供解释、说明和证据。数据收集自XSum数据集，这是摘要事实一致性研究中常用的基准数据集。初始系统输出来自预训练的PIXEL模型。展示了注释数据点的一个示例，并提供了基本数据统计信息。收集了约2500个数据点，其中70%包含事实错误。与初始系统输出相比，人类编辑摘要可以收到更高的自动事实性分数，但与参考摘要之间的文本重叠较低。这表明大多数参考摘要本身已经包含事实错误。展示了注释编辑说明的分布情况及其与不同错误类型的关系。研究了三个任务：摘要编辑（模型需要遵循人类反馈编辑初始摘要）、反馈生成（模型需要生成可用于编辑模型的反馈）和自动事实错误纠正（模型在生成相应解释的同时纠正事实错误）。研究发现，微调模型和大型语言模型都可以有效利用人类反馈进行摘要编辑任务。然而，反馈生成仍然是一个挑战，尽管微调模型和大型语言模型都可以生成反馈，但性能仍然较低。自动事实错误纠正任务中，编辑模型可以达到基线模型相当的性能，即使使用较少的数据进行训练。训练模型生成解释有助于提高性能。此外，数据集的注释可以用于训练事实性度量和事实性度量评估。该数据集已发布在GitHub上，更多信息请参阅论文。</sample>
    <sample id="78">是的，DEplain-apa 和网站的简化过程有所不同。DEplain-apa 主要基于新闻文本，其中包含 483 份手动对齐的文档，产生大约 30,000 到 13,000 对平行句子对。相比之下，DEplain 网站包括不同领域，并结合了手动对齐和自动对齐方法，总共产生了 30,450 句子对。此外，DEplain-apa 包含更多重排和单词编辑，而 DEplain 网站则包含更多重写。</sample>
    <sample id="79">是的，根据演讲者提到的，Coscript 数据集可以作为宝贵的资源用于推进语言规划研究。</sample>
    <sample id="80">在水印插入过程中，首先定义目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发词数量。目标嵌入是原始嵌入和目标嵌入的加权求和，权重与句子中触发词的数量成正比。如果句子中的触发词数量大于m，则提供的嵌入等于目标嵌入。</sample>
    <sample id="81">根据所给的英文内容，论文的作者属于Penn State University。</sample>
    <sample id="82">The video introduces a study on "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" (U-RRA). The study focuses on developing an unsupervised automated essay scoring (AES) system that does not require human intervention or labeled data. Traditional AES models are typically trained using large datasets of labeled essays and their ground truth scores, which is time-consuming and labor-intensive, especially when dealing with new prompts or without professional scoring staff.

Unsupervised AES aims to overcome this limitation by using multiple heuristic quality signals to provide pseudo ground truth supervision. The study proposes a novel framework called U-RRA, which integrates multiple heuristic signals to create a robust and comprehensive supervision mechanism. This framework includes three main components: a heuristic essay ranking module (HER), a deep pairwise rank aggregation module (DPRA), and a scoring strategy.

The HER module generates partial order pairs by ranking essays based on various heuristic quality signals, such as word count and sentence complexity. These partial order pairs serve as the foundation for training the DPRA module, which then aggregates these signals into a unified supervision signal. The DPRA module uses a learnable confidence weight for each signal to measure its importance and address inconsistent partial order supervision.

In the model inference stage, the predicted scores by the neural AES model are transformed into the predefined score set through a minimum-maximum transformation. Experiments conducted in both transductive and inductive settings demonstrate that U-RRA outperforms unsupervised baselines with significant improvements. Compared to cross-domain and one-shot methods, U-RRA achieves competitive performance, although it still lags behind general supervised methods due to the lack of strong supervision.

Overall, the study aims to perform essay scoring under a supervised setting by proposing the U-RRA framework, which effectively addresses conflicts among different signals and provides a unified supervision mechanism for unsupervised AES.</sample>
    <sample id="83">是的，编码器-解码器模型可以通过混合语言的训练来改进。在研究中发现，编码器-解码器或编码器 PDR 可以通过在混合各种语言的训练中得到改善。这种改进是由于大多数主要自然语言性能提升，而英语性能在某些数据集中略有下降，这被描述为多语言性的缺点。</sample>
    <sample id="84">The speaker introduces the topic of dynamic networks and contrasts them with traditional static networks. They explain that while static networks have fixed parameters, dynamic networks can adapt their architecture or parameters based on input. Examples include selecting specific sub-networks for mixers or experts and dynamically adjusting convolution layers in neural networks. The speaker emphasizes that implementing dynamic networks is easier by replacing static layers with dynamic ones, but existing fully dynamic networks are limited due to excessive parameter usage. They propose a framework called Partially Dynamic Network (PDN) that partitions parameters into dynamic and static, using scale factors to control the intensity of each mode. This method helps maintain performance while reducing the number of parameters and computational resources. The speaker also mentions ongoing research to explore optimal dynamic ratios and scale factors for different dynamic networks and compares PDN's performance with network pruning, showing superior results. Future work includes extending the method to other machine learning networks and hardware-structured manners.</sample>
    <sample id="85">受限语言规划的一个示例是制作巧克力蛋糕，这涉及具有多种约束条件的具体目标，如成分、步骤和时间限制。</sample>
    <sample id="86">他们通过在直觉上很难区分后门嵌入和正常嵌入的可视化来验证所提供的嵌入的隐蔽性。</sample>
    <sample id="87">根据所提供的英文内容，研究使用现有的预训练模型（PLM）来构建新的PLM的方法包括使用预训练的PLM作为基础模型，并在特定领域或任务上进行微调。这种方法涉及将预训练模型的权重和参数应用于新数据集，以适应特定领域的语言理解和生成任务。例如，在本研究中，使用了预训练的BERT模型，并在NATOS数据集上进行了微调，以构建适用于生物医学和临床任务的PLM。通过这种方式，可以利用大型预训练模型的泛化能力，同时减少训练时间和资源需求。</sample>
    <sample id="88">根据所给的英文内容，GPT-4 与非二元性别群体的立场最不一致。</sample>
    <sample id="89">演讲者展示了模型如何利用注意力机制所学的知识来处理一个包含“我打算谈论”的句子。在这个例子中，模型预测了德语翻译，并检查了跨注意力权重。前两个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧（Lambda帧）。由于跨注意力权重的总和低于阈值α，模型仅输出前两个词，等待下一个语音片段以确定是否输出最后一个词。</sample>
    <sample id="90">本研究探讨了非母语学习者是否可以参与自然语言处理（NLP）数据标注，以解决母语为特定语言的标注人员难以获取的问题。研究设计了实验，将学习者分为三个水平：基础、中级和高级，并使用了英语、韩语和印度尼西亚三种语言。实验包括预测试、标注和后测试，参与者在标注任务中使用额外资源，并在标注前后解决了标准化测试问题。结果表明，学习者标注的准确性与母语标注相当，尤其是对于简单任务和中等难度问题。此外，使用学习者标注的模型在某些情况下甚至超过了使用母语标注的模型。研究还观察到，随着标注任务的进行，学习者的语言 proficiency和词汇量有所提高。总体而言，本研究展示了非母语学习者在NLP数据标注中的潜力，为低资源语言的NLP研究提供了新的方法。</sample>
    <sample id="91">根据所给的英文内容，任务的数量对模型性能有显著影响。随着任务数量的增加，模型在任务上的表现更好，同时敏感度降低。这表明，通过增加训练数据量，模型可以更有效地学习和泛化，从而提高其在多模态任务上的性能。</sample>
    <sample id="92">根据所提供的英文内容，作者用来比较其方法的三个无树基线是：1. 一个无树模型，2. 一个使用外部词典的无树模型，3. 一个使用外部词典和外部依赖关系的无树模型。这些基线被用来展示作者的方法在处理更深层次的递归和结构化泛化方面的优势。</sample>
    <sample id="93">根据幻灯片中提供的信息，Alexander Koller和Ivan Titov是Matthias Lindemann的导师。这可以从他们名字下面的“导师”标签推断出来，表明他们在Lindemann的研究项目中担任指导角色。</sample>
    <sample id="94">The video introduces a paper titled "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark." The presenter, Jing Wei from the University of Science and Technology of China, explains the background of embedding services, which are built upon large language models like GPT, BLOOM, and LLaMA to assist various NLP tasks. However, recent works have shown that attackers can steal the model by learning from the embedding and providing similar services. To protect the copyright of embedding services, one solution is to embed a watermark in the provided service and detect whether another service contains the watermark.

The watermark method needs to meet four properties: applicability to embedding services, non-degradation of the utility of the provided embeddings, covertness to the attacker, and transferability to the attacker's services during the model extraction process. Existing methods either lack applicability or transferability. Therefore, the paper proposes Embedding Marker, a backdoor-based watermark method applicable to embedding services.

Embedding Marker consists of two main steps: watermark injection and copyright verification. Before these steps, a trigger set is selected, which is a group of words with moderate frequency. In watermark injection, the target embedding is defined, and when a user sends a sentence to the provider service, the provider counts the number of triggers in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding, with the weight of the target embedding proportional to the number of triggers. Copyright verification involves constructing a backdoor and benign dataset, computing the cosine and L2 similarity between the requested embedding and the target embedding, and using the KS test p-value as a third metric. Experiments on four datasets (AGNews, Yahoo! Answers, SST-2, and Yahoo! Answers) show that Embedding Marker has great detection performance while maintaining good utility for downstream tasks. The covertness of the provided embedding is validated by visualizing the embeddings of sentences on the UCR dataset using UMAP PCA, where it is hard to distinguish between backdoor embeddings and normal embeddings.</sample>
    <sample id="95">根据所提供的英文内容，PaLM 的第一作者是 David Villegas。</sample>
    <sample id="96">今天我将介绍我们团队的工作，标题为“NLPositionality:Characterizing Design Biases of Datasets and Models”，这项工作是在卡内基梅隆大学和University of Washington的AI实验室合作完成的。我们首先想象一下你正在为报纸筛选评论，试图删除有毒内容。你可能会转向一个流行的API，如Perspective API进行有毒性检测，而这个API在处理卡里·乔恩斯的情况下表现得很好，因为Perspective API能够正确检测有毒实例。但在迪蒂亚·夏马的情况下，Perspective API对印度语境中更常见的有毒术语并不敏感。这是一个设计偏差的例子，我们看到不同群体之间的系统性能差异。这种偏差可能源于NLP研究员和模型开发者的观点。观点是指人们持有的观点，这些观点源于他们的 demographics、身份和生活经历。这个概念广泛应用于批判研究，特别是在女性主义和 queer 学术领域。作为研究员，观点会影响研究过程及其结果，因为它改变了研究员做出的决策。因此，人们可能会问：数据集和模型是否有观点？我们并不是说模型本身和数据集本身具有 demographic identities 和生活经历，但它们确实反映了真实人物的判断和观点，从而代表某些观点而忽视其他观点。早期的工作提供了关于观点的 anecdotal evidence，例如文化差距和模型和数据集中的偏见，以及理论上的模型观点定义。然而，这些工作并没有通过比较真实用户与数据集和模型本身来研究观点。随着NLP任务变得越来越主观和社会化，研究数据集和模型观点变得越来越重要。由于并非所有决策都记录在案，许多模型被隐藏在API后面，因此很难确定这些观点是如何扭曲的。为了研究数据集和模型观点，我们实际上比较了真实用户的注释与现有的数据集和模型。我们通过一种称为NLPositionality的框架来实现这一点。该框架分为两个主要步骤：首先，我们重新注释数据集，使用多样化的注释者。我们通常不考虑原始数据集的 demographics，因为通常每个注释者只注释每个实例，而 demographics 通常未收集和共享。因此，我们重新注释数据以获得每个实例的多个注释，并获得丰富的 demographics 数据。然后，我们将 demographics 的注释与数据集和模型的注释进行比较，使用皮尔逊的相关系数。因此，我们的框架与注释者分歧文献不同，后者比较真实用户与模型和数据集的预测和标签，而不是仅仅关注注释者一致性或注释者分布。我们的框架主要通过Lab in the Wild在线众包平台实现。Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，相比之下，像MTurk这样的平台主要来自美国或印度的参与者。此外，Lab in the Wild仍然能够提供高质量的数据。我们在Lab in the Wild上托管了两个任务，一个任务是社会接受度分析，参与者阅读社会化学数据集中的情况，并评估其社会接受度。为了保持参与者的兴趣，他们可以比较自己的答案与AI和其他人的答案。我们随后将这些注释与社会化学数据集的GPD4进行比较。我们还复制了一个类似的设置，用于有毒性和 hate 言语检测任务，参与者阅读DinahHate中的一个实例，并判断它是否是 hate 言语的一个例子。我们随后将这些注释与DinahHate、Perspective API、Rewrite API、Hate Roberta和GPD4进行比较。我们的研究和实验共收集了超过16,000个注释，来自87个国家的1,000名注释者。现在，我们更好地回答了NLP数据集和模型与最多用户观点的对齐问题。我们发现NLP中存在观点。例如，我们发现数据集和模型最接近英语国家。对于GPD4的社会接受度分析，我们发现它最接近于基督教和英语国家。我们还发现DinahHate也最接近英语国家。我们还发现额外的对齐与拥有大学教育的人有关。对于GPD4的社会接受度任务，我们发现它最接近于拥有大学教育或研究生教育的人。我们发现同样的现象适用于DinahHate，其中它最接近于拥有大学教育的人。然而，当数据集和模型与特定群体对齐时，一些群体不可避免地被落下。一个例子是数据集和模型对非二元性别群体的对齐度较低，与男性和女性的对应群体相比。我们在GPD4的社会接受度任务中发现这一现象，也在DinahHate任务的分析中发现。那么，如果NLP数据集和模型存在观点怎么办？我们有几个建议。第一个建议是在整个研究过程中记录所有相关的设计选择。第二个建议是用观点的视角进行NLP研究。第三个建议是构建针对特定社区的专门化数据集和模型。一个很好的例子是Mussakani倡议。我们想强调的是，包容性NLP不仅仅是让所有人都能使用技术。这就是我们的演讲结束的地方，如果你想要了解更多信息，请查看我们的仪表板获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的三个问题：特定架构通常需要额外模块进行优化，训练过程复杂，包括不同的优化目标和多个模型的训练和维护以达到不同的延迟要求。</sample>
    <sample id="98">根据演讲内容，减轻数据集中的社会和政治偏见的有效方法包括在训练数据中进行数据清洗，以减少偏见的传播。然而，这也会带来新的挑战，如可能引入审查或排除某些观点，使得确定什么应该被保留变得困难。因此，需要找到平衡点，以确保数据的多样性和公正性，同时避免偏见和审查。</sample>
    <sample id="99">Hi, I'm Siu Yu Yan from Fudan University. I am here to introduce our work on distilling script knowledge from large language models for constrained language planning. In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripts. Previous work has explored using language models to plan for abstract goals of stereotypical activities, such as making a cake, and shown that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for abstract goals of stereotypical activities, while planning for specific goals with specific constraints, such as making a chocolate cake, remains understudied. In this paper, we define the problem of constrained language planning, which imposes different constraints on the goal of planning. An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and feasible to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then, we conduct detailed analysis to investigate why large language models fail. The figure shows that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed. We dig into more fine-grained topical categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of InstructGPT varies considerably for goals of different categories. Previous studies have shown that the output quality of large language models for generating scripts is highly variable, leading to bad performance. Thus, we adopt the idea of an over-generated and filtered method to improve generation quality. We first show constrained types with examples for InstructGPT and obtain specific goals based on the said abstract goals. Then, InstructGPT over-generates key scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability, both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it is essential to enable language planning ability of smaller and specialized models. Creating datasets is an essential step towards this end. However, previous studies do not enable planning for specific goals, and manual manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning datasets from large language models. We apply our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts to ensure the quality of validation and test sets. We ask crowdsourced workers to find and revise the incorrect samples. This figure shows the constrained distribution of CoScript. We find CoScript shows high productivity in the generated specific goals. With CoScript, we can train smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem, evaluate the constrained language planning ability of large language models, and develop an over-generated and filtered method for large language models. We use a large language model to generate a high-quality script dataset, CoScript, for constrained language planning. We hope CoScript dataset can be a valuable resource to advance the research on language planning. Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="100">多轮问答（Multi-hop QA）涉及回答需要多次推理才能回答的问题，每次推理通常对应一个文档。例如，要回答“1988年圣诞节喜剧电影《 Brian Doerr Murray》中Brian Doerr Murray首次亮相的电影是什么？”这个问题，首先需要找到Brian Doerr Murray参演的所有电影，然后确定其中哪部电影是在1988年上映的。这个过程涉及多个文档，称为答案链。多轮检索（Multi-hop Retrieval）是一种训练方法，通过最大化给定问题的正确答案链的概率来优化检索器。然而，大多数现有的系统需要大量的问题和答案链示例才能表现良好，这在低资源领域或需要特殊专业知识的领域可能很昂贵。本文提出了一种名为PromptRank的方法，它通过结合无监督检索方法和少样本语言模型排名来提高效率。该方法包括两个主要步骤：检索一组候选答案链，然后使用少样本语言模型对这些答案链进行排名。为了构建答案链排名的评分函数，我们使用了语言模型评估问题与答案链的关联性。我们还提出了几种技术，如指令搜索和温度缩放，以优化答案链评分。实验结果表明，PromptRank在检索和问答性能上优于全监督系统，并且在下游问答任务中表现出色，尽管在精确匹配点上略逊于基于知识图谱的方法。</sample>
    <sample id="101">根据所给的英文内容，PaLM 的流畅度与当前最好的系统相当。然而，主要区别在于准确性，因为 PaLM 更倾向于通过删除源句子中不相关的部分来生成更好的理解翻译，从而导致了更多的省略错误。</sample>
    <sample id="102">根据所提供的英文内容，水印方法的重要属性包括：1. 方法应该适用于嵌入式服务。2. 水印不应该降低提供的嵌入式服务的实用性。3. 水印应该足够隐蔽，以便攻击者可以轻松地移除它。4. 水印需要在模型提取过程中可传输到攻击者的服务。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">从一个数据集中抽取的实例数量用于重新注释并没有在提供的英文内容中明确说明。</sample>
    <sample id="105">余弦距离和L2距离用于衡量良性和后门数据集之间的差异。</sample>
    <sample id="106">The paper titled "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations" by Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova from the University of Pennsylvania and Google DeepMind presents a dataset called QUEST, which includes over 30 million entity-seeking queries containing implicit set operations. The answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints. The dataset poses a challenging retrievable problem since systems need to effectively search over a large document corpus to find multiple answer sets where the attribution for different query constraints can come from different parts of the document. To construct QUEST, Wikipedia category names from four domains of interest (films, books, plants, and animals) were used to perform set operations over these atomic categories to get queries with set constraints. Human annotators then paraphrased templatic queries to ensure that the paraphrased queries have the same meaning and are fluent. Another set of annotators validated these queries for fluency and naturalness, which was used to filter the set of queries. Finally, annotators verified the relevance of entities in the answer set and also marked evidence in the document as its attribution. For example, for the paraphrased query "historical fiction novels set in France," annotators first marked the span of text that indicates relevance for the constraint "historical fiction novels" and then marked the span of text relevant for "set in France." They would then mark the document as containing complete evidence and definitely relevant to the query. To evaluate systems on the dataset, we require systems to retrieve multi-answer sets from a large document corpus where queries contain implicit set constraints and the evidence for a document's relevance can come from multiple parts of the document. To set up baselines for the dataset, we considered sparse and dense retrievers as well as a TF-IDF-based reranker that takes in the top 100 candidates from the retriever. First, we show that there is a large room for improvement on retriever performance based on the recall of the complete answer set indicated here by the M-Recall at 100 scores. The end-to-end system performance in terms of F1 scores is fairly low, showcasing the difficulty of systems in handling such queries. Finally, through our analysis, we find that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores. We hope that along with Jane and Austin, QUEST can help future researchers build improved systems for their information seeking scenario with selective information needs.</sample>
    <sample id="107">基于编码器的多语言模型，如M5和XLM-R+PDR，在多语言设置中表现出最佳性能。这些模型可以训练在多种语言上，从而在多种语言的查询上进行预测，显著提高了跨语言性能。</sample>
    <sample id="108">The presentation discusses the evaluation of language models using the Minimal Pair Paradigm, which assesses models based on their ability to distinguish between acceptable and unacceptable sentences. The current paradigm is limited in evaluating longer sentences, which are becoming more common with large language models. The researchers aim to revise this by testing models on longer sequences and examining their sensitivity to context.

They simulate longer sequences by recreating sentences from datasets, such as grammatical structures from the BLM dataset, and adding them as prefixes to both acceptable and unacceptable queries. This allows for the evaluation of models' acceptability judgments across different contexts, including matching and mismatch scenarios, and even unrelated domains like Wikipedia.

The results show that MPP judgments are robust for arbitrary contexts but vary significantly when the same dataset is used. The effect of match prefixes increases with context length, impacting newer models with larger context windows. The analysis suggests that models are sensitive to latent syntactic and semantic features shared across sentences, indicating that the current short and single-sentence evaluation method may not capture abstract knowledge effectively throughout the context window.</sample>
    <sample id="109">该研究介绍了一个名为“自然指令”的数据集，其中包含自然语言指令及其相应的输入和输出。该数据集通过完全自动化的方式收集，无需人类标注。研究人员使用预训练的GPT-3模型生成指令和输入，并要求模型生成相应的输出。此外，他们还生成了每个指令的多种表达方式，以增加数据集的多样性和创造性。该数据集包含64,000个示例，如果考虑不同表达方式，则总共有约240,000个示例。研究人员分析了生成的示例，评估了其创造力、多样性和准确性。结果表明，超过50%的生成示例是正确的，即使不正确的示例也提供了有价值的信息。该数据集被用于微调一个11亿参数的T5模型，结果表明该模型在多个基准测试中表现出色，甚至超越了使用SuperNATURAL指令集训练的基线模型。</sample>
    <sample id="111">作者通过收集一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="112">Hello everyone, my name is Zhueng. Today I am going to present our paper: "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" Let's get started. Our paper investigates the problem of generalization using the named entity recognition task, or the NER task. We observe that models have been using CoNLL-2003 to develop NER for almost twenty years. And this naturally raises several problems. Firstly, can these models generalize to modern data? And when we develop new taggers, what is needed for good generalization? At the same time, if we do observe poor generalization, what causes the performance drop of these models? To investigate these problems, we developed the CoNLL++ dataset. This is a dataset that we collected from Reuters-News from 2020 and then annotated them with the same CoNLL-2003 annotation guidelines. We then fine-tuned over 20 models on CoNLL-2003. We evaluated them on both the CoNLL-2003 test set and the CoNLL++ test set. And last but not least, we calculated the percentage change in F1 to assess the generalization of each model. So what is needed for good generalization? Through our experiments, we found that there are three main ingredients that are needed. The first one is the model architecture. Through our experiments, we found that the Transformer models normally generalize better to new data. The second ingredient is the model size. We found that usually larger models lead to better generalization. In last but not least, we all know that the number of fine-tuning examples directly affects the performance of the downstream task. Here we also found that more fine-tuning examples actually also leads to better generalization. To our next question, what causes the performance drop of some models? We had two hypotheses. The first one is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. And this is usually manifested as the diminishing returns on the new test set. The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For adaptive overfitting, we saw that from the graph on the right, the red best-fit line has a gradient that is greater than 1. This means that every unit of improvement that we made on CoNLL-2003 translates to more than one unit improvement on CoNLL++. Which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data. And we found that the performance degrades with larger temporal gap. And this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. And these goals hand-in-hand, we can't just have one ingredient, but throughout the others. At the same time, we also found that the performance drop here is caused by temporal drift. And kind of surprisingly, it is not caused by adaptive overfitting, even though CoNLL-2003 has been used for over twenty years. So going back to the question that we raised in the title of our paper, do CoNLL-2003 taggers still work in 2023? And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our dataset, and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="114">The video introduces a research paper titled "Finding the Pillars of Strength for Multi-Head Attention" presented at ACL 2023. The researchers from Nanyang Technological University in Singapore discuss the challenges and limitations of large language models, such as heavy parameters, long training times, and the need for extensive data. They focus on addressing the issue of heavy parameters by proposing a method called Group Head Attention (GHA), which uses a divide-and-conquer strategy to compress multi-head attention. The GHA includes two stages: group constraint training to make intra-group heads more similar and inter-group heads more separate, and voting-to-stay algorithm to prune redundant heads within each group. The method is evaluated on three tasks—machine translation, language modeling, and abstract summarization—and shows significant parameter compression while maintaining performance. The video also mentions future directions, including task-specific automatic pruning, which aims to remove redundant parameters without sacrificing performance.</sample>
    <sample id="115">该方法使用的语音片段大小是Lambda。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是一位 judge。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">本研究旨在改进多语言预训练技术，以提高代码混合文本的性能。代码混合是指在文本中同时使用多种语言，例如英语和印地语的混合。这种现象在 linguistic 多样化的社区中很常见，如印度。构建能够处理代码混合的计算模型对于理解和分析这些文本至关重要。

本研究的主要贡献包括提出了一种新的多语言模型（MLM）技术，专门针对代码混合文本。我们定义了“切换点”概念，指的是从一种语言过渡到另一种语言的两个连续标记。例如，在英语到印地语、印地语到英语的过渡中，每个过渡点都是一个切换点。在标准 MLM 中，所有标记都被均匀随机遮挡，但在代码混合文本中，只有切换点被遮挡。

为了克服需要标注数据集的限制，我们提出了“频率 MLM”方法。这种方法通过比较单词在不同单语语料库中的负对数概率来确定其是否为切换点。我们还提出了几种架构修改，如残差连接和额外损失函数，以增强模型处理代码混合文本的能力。

实验结果表明，我们的方法在情感分析任务上表现最佳，特别是在处理代码混合文本时。我们还使用了证明分类器验证了我们的方法确实增加了中间层和最终层中的切换点信息。这些结果支持了我们的假设，即通过添加残差连接和额外损失函数，可以增强模型处理代码混合文本的能力。</sample>
    <sample id="119">在扩展实验中，论文侧重于GPT-4、GPT-3系列、BERT系列及其变体。</sample>
    <sample id="120">该模型是使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括通过提及歌曲名称“Easy on Me”或其位置（如“第一首”）来选择歌曲。</sample>
    <sample id="122">根据所给的英文内容，这篇论文的作者所属机构是复旦大学。</sample>
    <sample id="123">The presentation introduces a research project titled "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning" by Zhiyang Xu, Ying Shen, and Lifu Huang from Virginia Tech. The researchers explore the application of instruction tuning to improve zero-shot learning in multi-modal tasks using pre-trained language models. They address the lack of publicly available multi-modal instruction datasets and present MultiInstruct, a benchmark dataset with 62 diverse multi-modal tasks covering ten broad categories, derived from 21 existing open-source datasets. The dataset includes five expert-written instructions for each task. The team uses OFA, a unified multi-modal pre-trained model, as their base model and evaluates its performance on various tasks using different instruction templates. The results show that instruction tuning significantly improves OFA's performance on unseen multi-modal tasks, with better performance and lower sensitivity as the number of tasks increases. Transfer learning from natural instruction datasets also enhances instruction tuning, leading to more consistent output across slight variations in instructions. The researchers propose a new metric called "sensitivity" to measure this consistency and discuss the potential benefits of transfer learning and larger datasets for improving the zero-shot capability of pre-trained models.</sample>
    <sample id="124">The presentation introduces a study on the temporal reasoning capabilities of large language models (LLMs) conducted by researchers from the National University of Singapore and Alibaba. The study aims to benchmark and improve LLMs' ability to reason about time, which is crucial in real-world applications. Temporal reasoning is broken down into three levels: time-to-time, time-to-event, and event-to-event. The study evaluates the performance of three LLMs—T5L, ChatGPT, and T5-based fine-tuned models—on various temporal reasoning tasks.

The researchers found that while ChatGPT performs well on year prediction tasks, its performance drops significantly when predicting months. They propose a comprehensive dataset called TempReason, which covers all three levels of temporal reasoning and long temporal coverage. The dataset includes question-answer pairs derived from Wikipedia knowledge bases and articles, evaluated in three QA problem settings: closed-book, open-book, and reasoning QA.

To enhance LLMs' temporal reasoning capabilities, the researchers introduce a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. Temporal span extraction pre-training involves reconstructing masked temporal and entity spans in raw text, while time-sensitive reinforcement learning rewards correct predictions and penalizes temporally wrong ones. The proposed model, TempT5, shows significant improvements over other models, especially in open-book and reasoning QA settings.

The study highlights the importance of addressing temporal reasoning biases in LLMs and proposes a comprehensive approach to improve their temporal reasoning capabilities. The presentation concludes with an overview of the research findings and the proposed training paradigm for enhancing LLMs' temporal reasoning abilities.</sample>
    <sample id="125">根据幻灯片上显示的信息，这篇论文有五位作者：Yanis Labrak, Adrien Bazille, Richard Dufour, Mickael Rouvier和Emmanuel Morin。</sample>
    <sample id="126">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="127">The video presents a research paper titled "Large Language Models as Reasoning Teachers" by Namguyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI. The paper introduces a method to transfer the reasoning abilities of large language models to smaller models using a technique called diverse reasoning. The authors address the issue that chain-of-thought (CoT) reasoning only works on huge models like GPT-3 or PaLM, which are costly or impossible to deploy in many situations. To solve this, they propose using these large models as reasoning teachers to train much smaller models. They also introduce a novel technique called diverse reasoning, which involves generating multiple step-by-step solutions for complex tasks using stochastic temperature sampling. This method is compared with existing baselines and fine-tuning methods, showing that it significantly outperforms them on various tasks, especially text-based ones. The video concludes by summarizing the results and discussing the scalability and trade-offs of the proposed method.</sample>
    <sample id="128">The speaker introduces a collaborative work titled "KitMOS: Evaluating Knowledge Integration from Multiple Sources," which is a joint effort between McGill University, MILA, and Microsoft Research. The work focuses on the integration of knowledge from different sources in natural language understanding (NLU) models. NLU models typically draw on various knowledge sources, including pre-trained parameters and input data at inference time. Recent studies have shown that models can leverage pre-trained knowledge to solve tasks like question answering. However, NLU often requires knowledge that is only available at inference time, such as specific information about entities or events that may change over time.

The proposed diagnostic test suite for knowledge integration includes a coreference resolution task designed to assess the ability to integrate knowledge from multiple sources. The dataset used for evaluation includes human study participants and established coreference resolution models. An example from the dataset involves identifying the correct entity referred to by a pronoun in a sentence. The resolution of a given pronoun requires both entity-specific knowledge (e.g., Servin is a judge) and background knowledge (e.g., judges decide cases in law courts). The availability of these two types of information can vary, with some being found in a single source and others in multiple sources.

Three settings are defined for the task: 1) Background Pretrain, where background knowledge is assumed to be available at pre-training time; 2) Background Both, where background knowledge is available both at pre-training and inference times; and 3) Background Inference, where both types of knowledge are available only at inference time. The Background Inference setting is particularly interesting as it simulates scenarios where the necessary background knowledge for solving a task is not part of the pre-training data.

The speaker provides examples of how the availability of facts in different sources is controlled in each setting. For instance, in the Background Pretrain setting, background knowledge about politicians seeking elected seats in government is contained in the pre-trained parameters. In the Background Both setting, additional background knowledge about politicians is provided in the inference context. In the Background Inference setting, fictional occupations are used instead of the actual occupation mentioned in the pre-training data.

The results of the best-performing models on the most challenging variant of the Background Pretrain setting show that without task-specific training on KitMOS, both models do not perform well. However, when trained on KitMOS, both C2F and BERT for Coref significantly outperform the random choice baseline. This suggests that models trained on general coreference resolution datasets learn to exploit surface cues, which are not useful for testing on KitMOS since such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models struggle to reliably integrate background knowledge presented only at inference time.

In summary, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time.</sample>
    <sample id="129">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例包括黑人女性、拉丁裔女性和亚洲女性。这些群体被标记为与社会主流或未标记群体（如白人男性）相比具有不同的特征或身份。</sample>
    <sample id="130">根据所给的英文内容，泛化能力较差的模型架构是那些没有使用Transformer架构的模型。</sample>
    <sample id="131">根据所提供的英文内容，测试数据集的名称是“干净验证数据集”。</sample>
    <sample id="132">根据所提供的英文内容，这篇论文有三位作者：马查塔、马丁和作者。</sample>
    <sample id="133">根据所给的英文内容，作者采用了多种模态。这可以从他们提到的“OF A使用统一词汇表对语言、图像和坐标进行编码”以及“输入文本、图像、说明和坐标框在相同的词嵌入空间中表示”等信息中看出。此外，他们还提到了“视觉任务”和“多模态分类任务”，进一步表明了他们处理的是多种模态数据。</sample>
    <sample id="135">本研究介绍了一种名为ABC-Eval的新型评估方法，用于评估对话型人工智能的质量。该方法由Emory NLP实验室与亚马逊Alexa AI合作开发，旨在通过明确标注模型响应的行为来减少人类评估的主观性。ABC-Eval能够衡量对话模型在各种主题上的错误，如忽略对话伙伴、提供无关信息、自相矛盾、传播错误信息、违反共同知识以及展示或不展示同理心。通过分析100个对话模型的对话，研究发现ABC-Eval的行为标签比现有方法更可靠，并能更好地预测整体对话质量。此外，ABC-Eval的多种指标组合能够解释对话质量的25%以上，而单一的转录级量表只能解释4%以下。研究结果表明，ABC-Eval能够以比以往任何时候都更高的分辨率评估对话型人工智能，为该领域的进一步发展提供了有力支持。</sample>
    <sample id="136">The presentation introduces a new evaluation set called FERMAT, which aims to assess the mathematical abilities of language models. The speaker, Jashan Alex Shiva Kumar, explains that current benchmarks are insufficient for evaluating numerical reasoning tasks, as they primarily provide accuracy scores and F1 measures. FERMAT, on the other hand, includes a variety of math worded questions extracted from resources like Illinois and Common Core, testing aspects such as number understanding, mathematical operations, and training dependency.

The speaker demonstrates how FERMAT evaluates models by changing the representation of numbers (e.g., 5.0 instead of 5) and varying the types of numbers used (e.g., large integers, small integers, decimals). This approach helps in understanding the range and breadth of a model's capabilities. The presentation also highlights the importance of mathematical operations and training dependency in evaluating models' performance.

The speaker then discusses the baseline evaluation results, showing that most models perform poorly across various aspects. However, fine-tuning with math teachers' templates improves performance. The study finds that models trained with diverse templates (e.g., those from GSM8K and AQuA) show better performance, indicating the significance of linguistic and mathematical diversity in training data.

In conclusion, the presentation emphasizes that existing benchmarks are not representative of real-world needs and suggests that FERMAT provides a more comprehensive evaluation. The speaker concludes by encouraging the audience to read the paper and explore the QR code and links provided.</sample>
    <sample id="137">The speaker, Siong Leng from the Singapore University of Technology and Design, introduces Tell2Design, a dataset for language-guided floor plan generation published in AIC 2023. The dataset aims to enable users to design by telling instructions with a specific focus on the floor plan domain as the initial area of research. The task involves generating 2D floor plan designs directly from language instructions, which include semantics that specify the type and functionality of each room, geometry that specifies the shape and dimension of each room, and topology that describes the relationships among different rooms. The dataset is constructed using publicly available floor plans, with 5,501 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk and around 76,000 language instructions generated artificially from predefined templates. The main challenges of this novel task are performing design generation under strict constraints, understanding the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and handling ambiguous, incomplete, or misleading information in human instructions. The speaker proposes a sequence-to-sequence model under the encoder-decoder framework to address these challenges. The model uses a transformer-based encoder-decoder structure and is initialized by a pre-trained language model (T5) for better language understanding capabilities. The performance of the proposed method is evaluated using the T2D dataset, where no overlap exists between the annotators of the training set and test set. The T2D model achieves the highest Iou scores with a micro Iou of 54 and a macro Iou of 53, outperforming other text conditional image generation baselines by a large margin. This can be attributed to the sequence-to-sequence model's ability to control the target box sequence generation based on the layout information extracted from the language instructions. In contrast, text conditional image generation methods fail to perform well, likely because those models are designed to generate artwork-like images with high-level visual concepts from short text instead of following multiple instructions with various constraints for specific design. When training only on artificial instructions while testing on human-written ones, the method cannot perform well, indicating a language distribution gap between artificial and human instructions. However, when artificial instructions are used for warm-up before training on human instructions, the performance of the method is significantly improved with over 10 Iou score increments, suggesting that despite the language gap, artificial and human instructions are mutually beneficial data portions during training. The speaker concludes by initiating the research of a novel language-guided design generation task with a specific focus on the floor plan domain and introducing Tell2Design, a large-scale dataset featuring floor plans with natural language instructions to describe user preferences. The speaker hopes this paper will serve as a foundation and propel future research on the task of language-guided design generation.</sample>
    <sample id="138">根据所提供的英文内容，作者认为NLU中研究不足的领域包括模型在没有任务特定训练的情况下整合和使用来自多个来源的知识的能力。特别是，在背景知识仅在推理时间可用的情况下，模型难以可靠地整合这些信息。</sample>
    <sample id="139">演讲者的名字是Ying。</sample>
    <sample id="140">是的，Coscript 经过了质量检查。在验证和测试集中，邀请云托管工人对 Coscript 的内容进行复核，以确保其准确性和质量。</sample>
    <sample id="141">现有的资源通常只支持有限类型的上下文依赖翻译，并且只适用于有限的语言集，因为它们通常依赖于领域知识和人类注释。</sample>
    <sample id="142">标题：解决间接引用表达以进行实体选择（AltEntities语料库）演讲者：贾法德·哈斯尼（Mohammad Javad Hosseini）介绍：贾法德·哈斯尼（Mohammad Javad Hosseini）与Philip Radlinski、Silvia Paretti和Annie Louis合作，介绍了AltEntities语料库。贾法德·哈斯尼（Mohammad Javad Hosseini）首先介绍了他们的工作重点是理解用户语言，以便在用户想要做出选择时能够理解用户的意图。他提出了一个示例问题：“你是说《Easy on Me》还是《I Got a Feeling》？”在这个例子中，用户需要从两首歌中选择一首。贾法德·哈斯尼（Mohammad Javad Hosseini）解释了为什么直接引用是最明显的，例如通过歌曲名称或位置。然而，有时候使用间接引用更为合适，尤其是在用户无法记住歌曲名称或歌曲名称相似难以区分的情况下。此外，用户可能希望指定偏好，例如“ newer one”或“not energetic”。贾法德·哈斯尼（Mohammad Javad Hosseini）强调了在对话系统和基准测试LLM的实体理解中的重要性。他们创建了一个大规模的公共数据集，因为缺乏现有的大型公共数据集。他们的数据集覆盖了三个不同的领域：音乐、书籍和食谱。数据集收集方法强调了随意性，使用了卡通对话场景。卡通包含三个对话气泡，在第一个气泡中，Bob回忆了昨天听过的歌曲。在第二个气泡中，Alice提出了一个替代问题，询问Bob是否指的是《Easy on Me》或《I Got a Feeling》。在第三个气泡中，Bob使用间接引用选择了一首歌，例如“ newer one”。贾法德·哈斯尼（Mohammad Javad Hosseini）表示，第一个气泡是从几个手动提示中选择的，第二个气泡是一个简单的模板，其中A和B是来自维基百科的样本。他们使用了不同的采样方法，从最简单到最复杂，使实体更加相似，从而增加了区分难度。对于音乐领域，他们展示了Google搜索结果；对于书籍和食谱领域，他们提供了维基百科中的背景文本和图片。贾法德·哈斯尼（Mohammad Javad Hosseini）解释说，标注者知道这些实体的名称，但并不一定了解实体本身。因此，他们被要求通过Google搜索链接来收听至少一部分歌曲，阅读书籍或查看食谱。对于书籍和食谱领域，他们还被提供了维基百科中的背景文本和图片。然后，标注者被要求从两个实体中选择一个，并用三到五个间接引用表达来描述它们。贾法德·哈斯尼（Mohammad Javad Hosseini）展示了数据集中的一些示例，如“没有乐队的那首”，“不是那个12岁男孩的那首”，“来自其他专辑的那首”，等等。AltEntities语料库包含了6000个替代问题，覆盖了三个领域，以及42000个间接引用表达。使用T5-XL模型的结果如下：如果语言模型拥有与标注者相同的背景知识，准确率会很高，大约在92%到95%之间。但是，如果语言模型只有部分重叠的背景知识，准确率会在82%到87%之间，这更符合实际情况。如果语言模型只有实体名称，准确率只有60%，这表明还有很大的改进空间。贾法德·哈斯尼（Mohammad Javad Hosseini）还展示了模型在不同领域的泛化能力。他们提供了数据集的链接，并感谢观众。</sample>
    <sample id="143">该方法与现有的 SimulST 策略进行了比较，包括 Whitaker 策略和 Local Alignment 策略。此外，还与专门用于实时翻译的 State of the Art 架构进行了比较。</sample>
    <sample id="144">根据所给的英文内容，这篇论文的作者所属机构包括里昂第一大学、里昂第二大学、里昂第一大学、里昂第二大学和里昂第一大学。这些机构在幻灯片底部的标志中被提及，并且与作者的名字相关联。</sample>
    <sample id="145">演讲者的名字是Jenny T. Liang。</sample>
    <sample id="146">The presentation introduces the analysis of omission in dialogue summarization, a subtask of text summarization. It explains that dialogue summarization involves creating concise summaries from dialogues to represent important information. The speaker highlights the challenges in extracting key information from dialogues across various domains and the progress made in recent years using large-scale pre-trained language models. However, these models often generate summaries with factual errors and omissions, which significantly affect the quality of dialogue summarization.

The presentation then delves into the specific issue of omission, showing that even state-of-the-art models can have high omission rates, with about 70% of generated summaries suffering from this problem. The speaker also discusses the random distribution of omitted information across different positions in dialogues, indicating that dialogues are unstructured and identifying key information remains a difficult task for current models.

To address the omission problem, the speaker proposes a task definition for omission detection, focusing on identifying missing content in generated summaries compared to the gold reference. The presentation introduces a dataset constructed by the speaker, which provides high-quality omission labels for dialogue summarization. This dataset is built upon five existing benchmarks covering five domains and includes candidate summaries generated by different models and decoding strategies. Human evaluation was performed to ensure the quality of the labels.

Three baseline frameworks were explored for the omission detection task, including pairwise classification, sequence labeling, and pointer networks. The performance of these models was evaluated using the precision-recall F1 score and the word-level omission recall score (R score). The results indicate that the task is challenging, with an F1 score around 50%. The presentation also discusses the use of a post-editing method for summary refinement, where the detected omissions are concatenated with the candidate summary to improve the final output. The results show that this method significantly boosts the performance, confirming the value of omission detection and its potential as a promising direction for improving dialogue summarization.</sample>
    <sample id="147">根据幻灯片上显示的作者姓名，这篇论文有三位作者：Myra Cheng、Esin Durmus和Dan Jurafsky。</sample>
    <sample id="148">嗨，我是萨拉·帕皮，来自特伦托大学和布鲁诺·凯赛勒基金会。我将简要介绍我们共同撰写的关于实时语音翻译的“注意力作为指南”的论文。该论文与马泰奥·内格里和马可·图尔奇合作完成。实时语音翻译（Simultaneous Speech Translation，简称SST）是将 spoken language 翻译成另一种语言的文本的过程，以实现实时跨语言交流。当前SST模型面临的问题有哪些？通常使用特定架构训练模型，引入额外模块进行优化。复杂的训练程序，例如涉及不同优化目标的训练，以及训练和维护多个模型以达到不同的延迟要求，例如训练一个平均延迟为1秒的模型，另一个为2秒的模型等等。我们的解决方案是什么？首先，使用已经存在的 offline 模型，无需重新训练或采用特定架构。使用单一模型处理每个延迟要求，并通过特定参数处理延迟。利用模型已有的知识，通过音频输入和文本输出之间的注意力机制来实现。在右侧可以看到一个示例。我们的解决方案是提出一种称为“编码-解码器注意力”的策略。这是一种策略，用于决定是否发出部分翻译，基于注意力指向的位置。如果注意力未集中，即总和低于某个阈值α，那么最后λ个语音帧的信息不够稳定，因此不会发出最后一个词。例如，如果我们收到一个包含“我要谈论的话题”的语音片段，我们的模型预测了德语翻译。我们将查看交叉注意力权重，发现第一个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧，即λ个语音帧。这意味着第一个词将被发出，因为交叉注意力总和高于阈值α，所以最后一个词不会被发出，我们将等待下一个语音片段。如果我们继续接收下一个语音片段，模型预测另外三个词，我们将查看交叉注意力权重，发现没有词指向最后λ个语音帧。这意味着这三个词将被发出。如果我们查看主要结果，我们将绘制实时语音翻译结果的图表，在图表中，蓝色表示翻译质量，平均延迟表示延迟度量，我们还考虑了计算开销的平均延迟，即模型预测输出所需的时间。我们希望曲线尽可能高，但也要向左移动。我们将其与应用于 offline 模型的其他策略进行比较，这些策略包括 Whitaker 策略和局部平均策略。我们还将与专门针对实时语音翻译的最新架构进行比较。这是我们在德语上应用实时语音翻译策略的主要结果。我们看到，ADT 出色地超过了所有应用于 offline 模型的策略，因为它们的曲线向左移动。我们还看到，如果我们考虑实际延迟时间或计算开销时间，ADT 是最快的策略。如果您想了解更多信息，请阅读我们的论文，我们还发布了开源代码和模型，以及实时语音翻译的输出，以促进我们工作的可复制性。谢谢您的关注。</sample>
    <sample id="149">是的，数据集公开了。</sample>
    <sample id="150">The presentation introduces MeetingQA, an extractive question-answering dataset based on questions asked by participants in meetings and their corresponding answer sentences. The dataset is unique due to its long documents, domain-specific information, and the significant QA component in meeting discussions. It includes 7.7 thousand questions split into training, dev, and test sets, with 30% unanswerable, 40% multi-span answers, and 48% multi-speaker answers. Questions are primarily framed in a yes/no manner, elicit detailed responses, and involve opinion seeking. The dataset also contains 20% rhetorical questions, with 70% of multi-speaker answers containing some disagreement.

The presentation highlights the length distribution of meeting transcripts, questions, and answers, noting that they are roughly composed of 12 and 35 words respectively. The authors achieve high human performance on the test set with an F1 score of 84.6%. They employ various methods, including context retrieval for short context models, single-span and multi-span variants for Q&amp;A, and silver data augmentation using interview questions from the MediaSum dataset.

In the fine-tuned setting, there is an over 25 F1-point gap between fine-tuned models and human performance. Short context models outperform long context models, and multi-span models have slightly less or comparable performance to single-span models. Zero-shot performance shows a nearly 50 F1-point gap from human performance, but silver data augmentation improves zero-shot results. Larger instruction-tuned models like BLIP-5 perform comparably to other models. Error analysis reveals challenges in identifying rhetorical questions and predicting single-span models' relevance. Models struggle to identify which speaker answered a question, especially in the zero-shot setting.</sample>
    <sample id="151">Hello everyone, my name is Ying, and my colleague Zhiyang and I will be presenting our research on MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning. With the advances in large language models, many works have started to explore new learning paradigms of using pre-trained language models for different downstream tasks in a parameter and data efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction datasets between NLP and multi-modal. There exist more than 1,600 language-only instruction tasks; however, there is no large-scale publicly available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model as our base model. OFA uses a unified vocabulary for language, image tokens, and the coordinate of a bounding box. Here we show some example instances from our MultiInstruct dataset. To unify the processing of various input and output data types, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input texts, images, instructions, and bounding boxes are represented in the same token space. Okay now, I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from NLG group for training and we sample 10,000 instances per task. For testing, we reserve the entire Commonsense Reasoning group for testing and we select additional five tasks from Wiki and the Miscellaneous group. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of Natural Instruction as the unseen task for NLP. So we use a pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during testing for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the min and max performance and the standard deviation of the performance across all five experiments. If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report ROUGE-L. For NLP tasks, we report ROUGE-L as well. We also introduced an additional evaluation metric called sensitivity. This measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction. Here is our main results. As we can see, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also, transfer learning from Natural Instruction datasets can benefit instruction tuning. Here we can see, as the amount of task increases, the model achieves better performance and in the meantime lower sensitivity. So we also do one experiment. We use one instruction versus five instructions. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategy on the model's sensitivity. As we can see, by transfer learning from Natural Instruction datasets, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from Natural Instruction datasets can help OFA to achieve much better performance on the Natural Instruction dataset. So overall, we propose the first large-scale multi-modal instruction tuning dataset which significantly improves the zero-shot capability of OFA and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them. So this is the QR code for our data and model. Thank you.</sample>
    <sample id="152">The presentation introduces the development of new language models specifically designed for classical philology, focusing on the creation of multilingual models that can handle ancient Greek and Latin texts. The speaker highlights the limitations of existing models, which are monolingual and lack robust evaluation. The project aims to improve existing models, explore different model architectures, and introduce multilingual models. Two monolingual models for ancient Greek (GraBERTa and GraTER) and two multilingual models (FilBERTa and FilTER) were pre-trained using various resources, including the Internet Archive. Benchmarking results show that these models outperform current state-of-the-art models in tasks such as speech tagging, dependency parsing, and lemmatization. The analysis also reveals that the encoder-decoder models significantly outperform the encoder-only models, particularly in lemmatization. The presentation concludes by emphasizing the importance of high-quality pre-training data and the potential of multilingual models in classical philology.</sample>
    <sample id="153">Ninareh Mehrabi，亚马逊Alexa AI的可解释AI团队的博士后科学家，在ACL 2023上介绍了他们关于解决文本到图像生成模型中模糊性的研究。他们的工作专注于识别和解决文本提示中的模糊性，这些模糊性可能导致图像生成器难以准确地捕捉用户的意图。例如，“女孩站在有花的房间里”这个模糊的提示可以被解释为女孩在房间里或房间里有花，或者两者的结合。为了解决这个问题，他们提出了一个框架，用于识别模糊性并提供额外信息以明确用户意图。该框架首先创建一个基准数据集，涵盖不同类型的模糊性。然后，它使用自然语言处理模型生成澄清问题或可能的视觉设置，以明确模糊的提示。最后，他们评估生成的图像是否忠实于用户的意图，使用VQA模型来验证图像是否满足了用户的期望。他们的研究表明，通过解决模糊性，图像生成器的整体准确性得到了提升，他们的自动评估框架与人类评估一致，可以可靠地评估文本到图像模型。</sample>
    <sample id="154">根据所给的英文内容，这篇论文的作者所属机构是意大利特伦托大学和布鲁诺·凯赛勒基金会。</sample>
    <sample id="155">演讲者的名字是贾瓦德·哈赛尼。</sample>
    <sample id="157">The speaker introduces a dialogue summarization method called "Dialogue Summarization with Static-Dynamic Structure Fusion Graph" developed by Shen Gao from Shandong University. This method aims to extract hidden information from dialogue contexts into concise summaries, helping people quickly capture the highlights of semi-structured and multi-participant dialogues without reviewing the complex context. The method involves four main components: an utterance encoder for encoding utterances into vector representations, a static graph construction using existing dialogue structure modeling methods, a dynamic graph module for capturing semantic relationships between utterances based on their deep vector representation, and a pre-trained language model as the summary generator to fuse the static and dynamic dialogue structures into the final summary. The speaker explains that the method uses a discourse passing graph to build a dependency-based dialogue structure, a speaker relationship model to capture interactions between speakers, and a relative distance feature to map discrete distances into vector space. The method also employs a multi-head attention model to calculate the relationship between nodes in the dynamic graph and a dual cross-attention mechanism to integrate the static and dynamic graphs into a unified graph. The speaker concludes by mentioning that the code and data have been released on GitHub for further exploration.</sample>
    <sample id="158">The presentation introduces a dual cache method for long document neural coreference resolution. The task of coreference resolution involves identifying and clustering mentions that refer to the same entity in a document with multiple mentions across the text. The proposed method uses a local cache and a global cache, where the local cache stores recently used entities with an eviction policy based on least recently used (LRU), and the global cache stores frequently used entities with a least frequently used (LFU) policy. The model evaluates the frequency of new or updated entities and adds them to the appropriate cache. The dual cache method outperforms single cache methods and significantly reduces cache misses, making it more cost-effective.</sample>
    <sample id="159">嗨， everyone。我是库斯图·辛哈，我很高兴欢迎你们参加我们关于ACL 2023论文的讨论：《语言模型接受性判断并非总是对语境 robust》。这是一篇与John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams合作完成的作品。在这项工作中，我们重新评估了最小对偶 paradigm。最小对偶 paradigm主要评估语言模型在可接受性判断上的表现，这些判断可以包括语法性，比如“BLEP”或“Syntax Gym”，或者可接受性，比如刻板印象，如“Crowd Speres”。在这个最小对偶 paradigm中，典型的方法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子。然后，希望模型将更多的概率分配给可接受的句子。当前的MPP pipeline 并不让我们能够评估模型对更长句子的可接受性。如今，大型语言模型正变得越来越强大，拥有更长的上下文窗口。因此，评估模型对整个上下文窗口的可接受性至关重要。这就是我们在这里所尝试的事情。我们正在努力通过要求模型评估更长、更长序列的可接受性来重新评估MPP pipeline。所以这就是我们的方法。我们所做的就是模拟这些更长序列。我们重新评估数据集本身，然后重新创建句子，通过选择可接受或不可接受的句子作为前缀。例如，这里我们选择了一个典型的语法正确性对偶，来自BLiMP数据集中的“ adjunct island”案例。我们所做的就是创建类似的更长序列，并提取具有相同语法结构的句子作为前缀，然后将其添加到可接受查询和不可接受查询之前。我们还可以通过从相同的子集或不同的数据集中选择句子来执行类似的操作，这样我们就可以测试模型的可接受性。我们也可以选择来自完全 unrelated领域的句子，比如Wikipedia，这将告诉我们模型的可接受性判断是否受任何上下文的影响，比如上下文是否来自数据集的不同子集，或者它是否与我们要评估的当前句子完全无关。那么模型的表现如何呢？首先，我们查看Wikipedia句子，它们完全与当前查询对偶无关。我们发现MPP 判断对于任意长度的上下文都是相对稳定的。我们增加上下文长度，直到达到1244，以最大化OPT和GPT-2模型。我们在橙色在线看到MPP 判断相对稳定。现在，当我们选择来自同一数据集的句子时，即从BLiMP或Syntax Gym数据集中选择可接受和不可接受的句子，我们发现MPP 判断要么显著增加，要么显著减少，取决于我们添加的是可接受的前缀还是不可接受的前缀。但是，当我们将结构相匹配，即从BLiMP或Syntax Gym数据集中选择相同现象的句子时，我们看到MPP 判断要么显著增加，要么显著减少，具体取决于我们选择的前缀是否可接受或不可接受。这种影响在整个上下文长度上都很大，这可能会影响像GPT-2这样的 newer language models，其上下文窗口更大。那么，为什么匹配前缀会如此显著地影响语言模型的判断呢？我们进行了系列分析，其中我们尝试通过保留相关结构但添加一些噪声来扭曲输入句子。在进行了多次扭曲后，我们发现这些噪声并没有使模型在显示MPP 判断趋势方面发生任何变化。实际上，我们发现模型对扭曲句子的敏感度类似，即在可接受领域扭曲句子时，所有扭曲都会导致MPP 判断显著增加；而在不可接受领域扭曲句子时，所有扭曲都会导致MPP 判断显著减少。因此，我们得出的关键 takeaway 是，语言模型对跨句子共享的潜在句法和语义特征敏感。当前的MPP评估方式，即使用短而单个句子输入，可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文获取更多实验细节。谢谢大家。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个未排序的多集词元，这些词元将在输出中出现。</sample>
    <sample id="161">根据所给的英文内容，Coscript 中包含了55,000个脚本。</sample>
    <sample id="163">根据所给的英文内容，DEplain 的最佳对齐方法是 Matt Align。</sample>
    <sample id="164">弱监督学习的一个好处是它比手动标注数据便宜得多。通过使用弱标注来源，如简单的启发式规则、知识库或低质量的 crowdsourcing，可以大大减少标注数据集的成本。然而，这些标注通常存在噪声，这意味着标注中存在错误。</sample>
    <sample id="165">本研究提出了一种无监督学习方法，称为Lipor（Likelihood Learning with Posterior Regularization），用于解决自适应推理问题。该方法通过最大化给定上下文和结果的最小可能性来处理解释集，同时利用解释之间的互斥性来偏好可能的解释。Lipor在Alpha和AI数据集中表现出色，超过了零-shot模型和之前的最佳无监督方法。</sample>
    <sample id="166">The presentation introduces a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. The speaker, Yunxian Li, explains that this task is challenging due to the high similarity between images and their descriptions in complex texts. Traditional visual language models perform well on image-sentence retrieval tasks but struggle with complex text descriptions. To address this issue, the proposed method combines the divide-and-conquer strategy and dual process theory. The divide-and-conquer strategy breaks down large problems into smaller ones, while the dual process theory involves human thinking systems: System 1 performs logical reasoning well, and System 2 is capable of abstract logical reasoning suitable for complex reasoning problems. The proposed method integrates these systems to handle complex retrieval tasks. The first module, the proposition symbol generator, represents complex propositions through symbols and generates corresponding sentences. The second module, the neuropsycho symbolic reasoner, integrates reasoning states and results of symbol propositions to obtain the final solution. The system utilizes the advantages of both logical inference systems and neuropsychological reasoning systems. Experimental results show that the proposed method outperforms baseline methods and achieves significant improvements in complex text retrieval experiments.</sample>
    <sample id="167">DEplain-web 中的文档分配为 750 个文档，其中 483 个文档是手动对齐的，另外 267 个文档是通过自动对齐方法对齐的。</sample>
    <sample id="168">CoNLL++数据集是通过从2020年的路透社新闻中收集数据并使用与CoNLL-2003相同的标注指南进行标注而创建的。</sample>
    <sample id="169">演讲者介绍了一篇关于大型语言模型（LLM）用于机器翻译的系统评估研究。PALM是一个拥有400亿参数的LLM，于2022年发布，并在各种NLP任务中表现出色。该研究对LLM的翻译能力进行了系统评估，使用了最新的测试集以避免与训练数据重叠，并与当前最佳系统进行比较。研究结果表明，选择高质量的示例对性能至关重要，而示例的质量比与源句子的相似性更重要。此外，使用高质量的开发数据而不是训练数据可以显著提高性能。PALM的流畅度接近当前最佳系统，但准确性存在差异，特别是在删除源句子中的不必要部分时。总体而言，PALM在流畅度方面表现良好，但在准确性方面仍存在一些问题。</sample>
    <sample id="170">Hello everyone, my name is Justin Jung from the Penn State University. Today I am going to present our work: Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Multiple Representations. So semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL and etc. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there lacks of coverage on certain natural language, the Chinese is missing, and lacks of coverage on certain meaning representations, the Lambda Calculus is missing, or they are only evaluated on certain neural model. For example, there is only one single model to evaluate them. So to this end, we propose Exemplar, we provide a unified dataset exemplar for cross-lingual semantic parsing in multiple natural languages and multiple representations. It contains 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we also test monolingual model. In this setting, the source language is the same as target language, for example, German to German or English to English. We also test monolingual few-shot setting by training monolingual models with only 10 percent of training data. And we test multi-lingual model which we train one multi-lingual model for all languages. For example, we put the German, English, Chinese queries together to train a multi-lingual model, and during inference, we can use this model to to translate German query or Chinese query, etc. And we also consider cross-lingual zero-shot and few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English query or the combination of English and German few-shot queries to train a multi-lingual model to predict the SQL output. And we also find many interesting results. So regarding analysis of monolingual models, we evaluate on two groups of models including encoder PDR which stands for multi-lingual pre-trained encoders with pointer-based decoders such as XLNet + PDR and BERT + PDR, and we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as M-BART and MT5. We found that encoder-decoder obtains the best performance on all 9 datasets. And we evaluate on MT5 and example XLNet + PDR on multi-lingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as curse of multi-linguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found that for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results for training on English natural language and significantly boost the performance of few-shot on target natural languages. And we found multi-lingual language models such as codes and blue are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative types of multi-lingual language models, and our result shows many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="171">根据所给的英文内容，关于这方面的现有研究可以被广泛地分为四个类别。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。</sample>
    <sample id="172">根据所提供的英文内容，Codex 或 Bloom 等多语言 LLM 并不足以进行 CLSP。</sample>
    <sample id="174">Hi, I'm Priya and I'm one of the co-authors of the paper ArgAnalysis35K, a large-scale dataset for Argument Quality Analysis. In this video, I'm going to quickly explain why this dataset is unique from other datasets that you will find on a similar topic. It's just going to be a quick like overview of the special features that we have, so do make sure to check out our paper and our poster at the conference for better insight into the results, dataset collection process, annotation process, etc. So very quickly, what is argument quality analysis? It's simply judging how good or bad an argument is on a scale from 0 to 1. So something like big banks are bad is likely to be rated low, but something like big banks have no accountability, take heavy risks, and lead to major collapses, which is why they should be broken up, which is a coherent argument which is more persuasive in terms of what it's trying to achieve, is going to be rated high, probably close to a 1. So let's brainstorm some problems with current datasets. Because current datasets lack quality because they're often collected from crowdsourcing platforms. They lack diversity because they often only consist of 30 or 40 motions and source arguments on those. They often lack depth in explaining why a specific argument is true. And they often always have they often have a motion associated with every single argument that exists. So how does ArgAnalysis35K do this differently? This is just an index of the special features that I'm going to cover in this presentation. So firstly, it's the largest dataset with really high-quality arguments. So as you can see from this table, as you can see from the title, first off, it has 35k argument analysis pairs, which is the largest dataset in this field to our knowledge. And as you can see from this table, around 85% of those arguments are sourced from either speeches from really high-quality tournaments or from expert debaters or from intermediate debaters. And the last 15% is sourced from novice debaters, everyday people, etc. So it has higher quality of arguments compared to if you were to just crowdsource all of your arguments. It has a diverse range of arguments. Instead of generating arguments on specific 30 or 40 motions that are like pre-selected before, we picked 24 themes based on our experience in the circuit, based on websites like hellootions.com, based on expert advice, etc. For every theme, we've captured as many motions as possible using the same sources, and this just creates a better diversity in terms of the motions that you'll encounter in a parliamentary debate setting rather than just pre-selecting a few motions. We added in an element of analysis instead of just keeping arguments. So analysis is not similar to claims or premises. It's likely a combination of all of those things. And this is the term that we've introduced to I guess the NLP community so to speak. So I'll quickly give an example of what this looks like. Analysis can be something like a premise, if you just say that educated people are 80% more likely, if you just give us a statistic, for example. But it can also be a claim and a premise, if you draw a link, so if you say that which people send their kids to private school, which leads them to being like having better jobs and being richer, that's you combining a claim and one premise and making it into one analysis. You can also have analysis with multiple claims and premises, as this argument will tell you. So at that point in time, you need to see analysis as one sort of coherent thing that explains like the argument better, right? So the argument in this case is this idea that education is the basis of everything a person achieves. So normally in datasets, you'll just have like an argument, right? But we've expanded this to create an idea of analysis, which explains this claim better. So instead of just saying hey, the claim is that education is the basis of course, like everything a person achieves, we have pushed sort of the boundary to explain why that is true. So analysis just ends up being any a combination of claims, premises, etc., which is also why it needs to be introduced because it's like nothing that exists in the general like terms that exist in NLP. We've introduced an idea of instance-based annotator reliability. So annotators have human biases about certain topics, right? So someone who experiences racism on a daily basis might have strong sentiments about those arguments, but probably does is not biased about arguments when it comes to art. So we can still use their judgment in judging those motions. So instead of just eliminating annotators that are maybe unreliable for a few topics, instead of eliminating all of their judgments, we only eliminate the judgments that we think are biased. So when you capture annotation reliability on an instance-based level, so on a level of for every argument, you're able to better capture like the you're able to just better utilize the annotations that you have. So we think that that's a good sort of use case, this dataset is a good use case for instance-based annotator reliability, which is something that already exists. Lastly, we introduce something called a relevance model. So usually datasets will just have like an argument and connect it to a specific motion. We think that motions are not flat, like singular, that way. Arguments like accountability is important can be used in debates about governments, churches, corporations, schools, anything. Accountability is important in all of those places. Arguments that deal with premise of free speech can be used to defend it for the LGBTQ community or to defend people's right to protest against a corporation. So a relevance model simply assigns a score from 0 to 1 for each of these and like for each theme, and just better captures has more arguments and better captures like the relevance that each argument has to a topic, right? So we think that this dataset is just a culmination of like a bunch of unique things, and at the end of it, you're able to get something that is more diverse, that has a score for relevance, so you're able to capture how relevant it is to a particular theme, you have higher quality of arguments, and in general, you just have more reliable scoring because we capture it on an instance-based level. So do make sure to like check out our paper and give us your feedback on it. Thanks.</sample>
    <sample id="175">该方法通过在训练过程中引入排列的不确定性来处理排列的不确定性。它预测一个不施加任何硬约束的排列，使模型具有高度的灵活性和表达能力。具体而言，该方法通过在输出中从左到右选择每个位置的多集标记，然后跳转到另一个多集标记来确定下一个标记，直到所有标记都被访问过一次。这种方法允许模型学习更有可能的排列，同时保持对潜在排列的广泛搜索空间。</sample>
    <sample id="176">根据所给的英文内容，NLP 模型的公平性可以通过评估模型在检测 hate speech 和 fake news 时的表现来定义。特别是，通过比较不同政治倾向的模型在检测 hate speech 和 fake news 时的表现，可以识别出潜在的偏见。例如，如果一个模型在检测针对社会少数群体的 hate speech 时表现不佳，但在检测针对更强大群体的 hate speech 时表现良好，这可能表明存在偏见。类似地，在 fake news 检测中，如果一个模型在检测来自对立政治派别的 misinformation 时表现不佳，而另一个模型在检测来自支持派别的 misinformation 时表现不佳，则表明存在偏见。这些发现突出了 NLP 模型中政治偏见的严重性，并强调了需要解决这些偏见以确保模型的公平性和可靠性的重要性。</sample>
    <sample id="177">演讲者的名字是Yanis Labrak。</sample>
    <sample id="178">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="179">Melanie Sclair在介绍了一种名为“象征Tom”的方法，该方法旨在通过使用明确的图形表示来提高大型语言模型（LLM）的理论思维推理能力。她解释了理论思维的概念，即理解他人的心理状态，并将其与阅读理解任务中的多角色故事相关联。她提出了一个经典测试，即萨利安测试，用于评估这种能力。在该测试中，Alice和Bob在一个房间里，Alice将苹果放在篮子里，然后离开房间。Bob将苹果移动到盒子里，然后问题会询问Bob会在哪里寻找苹果，以及Alice会在回来后去哪里寻找苹果。Sclair解释了第一阶和第二阶问题的区别，以及真实信念和错误信念问题的概念。她指出，大型语言模型在错误信念任务中表现不佳，如ChatGPT或GPT-3。因此，她的研究问题是如何在大型语言模型中提高理论思维推理能力。她介绍了象征Tom方法，它使用多种图形表示来表示不同角色的心理状态，从而更准确地回答涉及多个角色的心理状态问题。Sclair展示了在实验中使用象征Tom方法时，大型语言模型在错误信念任务上的性能显著提高，尤其是在第二阶错误信念问题上。她还讨论了在两个新数据集上的泛化能力，这些数据集修改了Tommy基准数据集，以测试结构和语言泛化能力。结果表明，象征Tom方法在保持泛化能力的同时，显著提高了大型语言模型在错误信念任务上的性能。</sample>
    <sample id="180">演讲者的名字是Myra Cheng。</sample>
    <sample id="181">The presentation introduces a research paper titled "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" by Siyu Yuan and colleagues. The paper focuses on the challenge of planning specific goals with constraints, such as making a chocolate cake, which is not well addressed in previous studies that mainly focus on abstract activities like making a cake. The researchers define the problem of constrained language planning, which involves imposing different constraints on goal planning, and evaluate the ability of large language models to plan for specific goals with constraints.

To address this gap, the study collects 100 specific goals with multi-faceted constraints using the InstructGPT-3 tool. They then evaluate the scripts generated by large language models and find that these models achieve unsatisfactory results in planning for specific goals. Detailed analysis reveals that while the semantic completeness of the generated scripts is acceptable, their faithfulness to constraints cannot be guaranteed.

To improve the performance, the researchers propose an over-generated filter method. This involves generating multiple scripts for each abstract goal, selecting the most faithful ones based on semantic similarity and constraint adherence, and using a filter model to further refine the selection process. This method significantly enhances the planning ability of language models in terms of both semantic completeness and faithfulness to constraints.

The study also discusses the challenge of creating datasets for specific goals, which is expensive and time-consuming. To overcome this, they apply symbolic knowledge distillation to generate a high-quality dataset named CoScript, consisting of 55,000 specific goals with scripts. This dataset is used to train smaller and specialized models for constrained language planning, demonstrating that smaller models can perform better than larger models when trained on suitable data sets.

In summary, the paper establishes the constrained language planning problem, evaluates the constrained language planning ability of large language models, and develops an over-generated filter method to improve the planning ability of these models. The CoScript dataset is proposed as a valuable resource for advancing research on language planning.</sample>
    <sample id="182">在本文的背景下，热带主义 (tropicalism) 指的是用来描述 Latina 女性的 tropes，如“vibrant”和“curvaceous”，这些 tropes与热带地区常见的刻板印象相关联。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。这些提示包括描述一个想象中的个体的句子，例如“想象一下你是一个亚洲女性，描述你自己。”这种方法允许生成具有可扩展性的描述，可以针对任何人口统计特征进行定制。</sample>
    <sample id="184">在本文中，语境使用情况是通过一种称为点对点CMI（条件互信息）的度量来衡量的。这种度量评估了给定源文本时，语境信息对目标翻译的贡献程度。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据来源和模型结构。DrBERT 是基于 Roberta 训练的预训练模型，使用 Natos 数据集进行微调，Natos 是一个包含医疗 Crowd 数据的 Web 数据集。相比之下，ChuBERT 是基于匿名化数据训练的临床模型，这些数据来自蒙特利尔大学医院。此外，ChuBERT 通过在 Natos 数据集和临床节点上的混合数据进行微调，而 DrBERT 则仅使用 Natos 数据集。这种差异导致了它们在性能上的不同，如在生物医学和临床任务中的表现。</sample>
    <sample id="187">根据幻灯片上显示的作者姓名，这篇论文有三位作者：Zhongyang Xu、Ying Shen和Lifu Huang。</sample>
    <sample id="188">迭代迁移学习是一种机器学习策略，其中模型在多个任务上进行训练，以利用这些任务之间的共同知识。在这种情况下，模型首先在C-E任务（扩展和比较）上进行微调，然后在辩论任务上进一步微调，以利用这些任务与认知不一致的相似之处。通过迭代地在相关任务上微调模型，可以显著提高零样本性能，从而更好地检测认知不一致。</sample>
    <sample id="189">数据集的目标是通过收集大量关于音乐、书籍和食谱的实体，为解决间接引用表达进行实体选择提供一个大规模的公共数据集。它旨在通过使用Crowd Annotation方法来解决对话系统中常见的问题，并评估LLM的实体理解能力。</sample>
    <sample id="190">攻击者可以利用 EaaS 提取模型参数，通过学习嵌入并提供类似服务来复制模型。</sample>
    <sample id="191">根据所给的英文内容，这篇论文有三位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="192">演讲者介绍了一种名为“Ken”的优化器，旨在同时实现快速收敛和低内存使用。Ken优化器通过使用矩阵分解技术（如非负矩阵分解）来减少内存需求，并通过自适应更新策略来保持稳定性和效率。实验结果表明，Ken优化器在训练大型语言模型时表现出显著的性能提升，特别是在参数量增加的情况下。与Adam和Adafactor等现有优化器相比，Ken优化器在验证准确率上提高了约3.4%，并在训练过程中消耗了更少的内存。此外，Ken优化器在处理特定任务时也展现出高效性，与基线模型相比，使用较少的内存资源实现了相当的性能。</sample>
    <sample id="193">根据演讲内容，用于创建初始数据集的注释者数量没有直接提到。然而，演讲中提到了一个包含大约1000个 discourse 单位对的数据集。关于注释者数量的具体信息未在提供的信息中给出。</sample>
    <sample id="194">根据所给的英文内容，论文的作者Jenny T. Liang属于卡内基梅隆大学。</sample>
    <sample id="195">The presentation introduces a research work titled "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering." The work aims to enhance the explainability of question answering (QA) systems by providing not only answers but also explanations for why those answers are selected. Recent advancements in QA can be categorized into two main approaches: symbolic methods, which translate natural language questions into formal representations like SPARQL, and decomposed-based methods, which generate natural language intermediate steps leading to the final answer.

However, both approaches have limitations. Symbolic methods struggle with executing formal representations in unstructured knowledge bases (KBs), even when the KB is large, thus limiting the recall of answers. Decomposed-based methods face challenges due to the lack of diversity in natural language and the difficulty in integrating knowledge from heterogeneous sources, especially for complex questions.

To address these limitations, the proposed framework, ROHT (Reasoning over Hierarchical Question Decomposition Tree), is introduced. ROHT consists of two stages: first, it builds a hierarchical question decomposition tree (HQDT) to understand the hierarchical compositional structure of a complex question. Second, it performs probabilistic reasoning over HQDT to fuse knowledge from various sources and consider the probability scores of string generation and answering.

The HQDT is constructed by using a question decomposer to generate leaf nodes, which are atomic questions, and another question generator to create intermediate questions based on grouped leaf questions. The certainty score of each node is computed based on its likelihood to represent the query. Probabilistic reasoning over HQDT involves three steps for each node: selecting appropriate knowledge sources, retrieving answers with probabilities from these sources, and aggregating candidate answers to output the top-k answers with the highest probabilities.

The framework was evaluated on two challenging complex QA datasets, KQAPRO and MUSIC. On KQAPRO, the ROHT-KB model outperformed existing KB-QA methods, demonstrating the benefit of integrating answers of sub-questions at different levels. When Wikipedia was added as a supplementary text corpus, the ROHT-MIX model showed significant improvement over the ROHT-KB model, highlighting the effectiveness of utilizing knowledge from KB and text corpora. On the MUSIC dataset, the ROHT-text model outperformed TRANSNET by a large margin, showcasing the superiority of explicit question decomposition. Overall, the results demonstrate that ROHT effectively integrates knowledge from KB and text corpora to improve the performance of QA systems.</sample>
    <sample id="196">以左侧为支配词的示例是“Lisa bought and Megan is such that the first conjunct is the head of the whole coordinate structure.”</sample>
    <sample id="197">根据所给的英文内容，对话系统中的最先进模型是ABC-Eval。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型正在出现，具有更长的上下文窗口。通过评估模型对更长句子的可接受性，我们可以更好地了解模型在处理更复杂和更长的文本时的表现，并确保模型能够准确地判断可接受性，而不仅仅是基于短语或单个句子。</sample>
    <sample id="199">是的，与单语英语模型相比，多语言训练会导致表现下降。</sample>
    <sample id="200">注释者是否提前知道该实体？是的，注释者提前知道该实体。</sample>
    <sample id="201">根据所给的英文内容，评估使用了最新的测试集以避免与训练数据重叠的测试数据，并比较了两个最好的系统，即在 WMT 评估中表现最好的系统。此外，还使用了专家级的人类评估结果。</sample>
    <sample id="202">根据所给的英文内容，泛化中的回归不会影响特定的 NER 类型。</sample>
    <sample id="203">NLP 中的立场很重要，因为它会影响技术的设计和性能。立场反映了数据集和模型中包含的偏见，这些偏见可能源于创建它们的人的视角。这些偏见可能导致技术在某些群体中的表现不佳，从而影响其准确性和公平性。通过识别和理解这些偏见，研究者可以开发更包容和有效的 NLP 技术。</sample>
    <sample id="204">根据所给的英文内容，像 BLOOM 这样的多语言 LLM 采用的是完整微调。</sample>
    <sample id="205">The speaker, a PhD student at the University of Washington named Changbin, is presenting their work on how language models are trained on large-scale web crawls and how political biases in this data can lead to unfair NLP applications. They explain that political news media are well-covered in the pre-training data, with sources like The New York Times, Los Angeles Times, The Guardian, and Huffington Post being well-represented. This has created a mixed blessing for NLP model applications, as while they can learn from diverse perspectives, these biases can lead to potential fairness issues in downstream tasks.

To address this, Changbin proposes investigating the political bias propagation pipeline from pre-training data to language models to downstream tasks. They ask two main questions: first, how do we evaluate the political leaning of language models and what role does pre-training data play in such biases? Second, how do language models with different political leanings perform on downstream tasks and whether that might result in fairness issues in NLP applications?

Changbin's team evaluates language models using different prompt formats and political questionnaires, such as the Political Compass Test. Preliminary results show that language models have varying political leanings, with GPT-4 being the most liberal and GPT series generally more socially liberal than BERT series and its variants. They also conduct controlled experiments by further pre-training language models on partisan corpora separated into news and social media, showing that the ideological coordinates of the language model shift accordingly.

They investigate whether language models can pick up the polarization prevalent in modern society by pre-training them on corpora before and after the 45th president of the United States. Results indicate that language models generally have a political leaning further away from the center after 2017. Finally, they evaluate language models with different political leanings on hate speech detection and fake news detection, showing patterns where left-leaning models are better at detecting hate speech targeting socially minoritized groups but worse at detecting hate speech targeting more powerful groups, and vice versa for right-leaning models.

Changbin concludes that there is a pressing fairness issue regarding the political biases of language models, which can marginalize people with opposite political opinions and allow hate speech targeting minority groups to run rampant without control. They highlight the dilemma between sanitizing political opinions in training data to prevent bias and the risk of censorship or exclusion, emphasizing the need to acknowledge and tackle these fairness issues in NLP applications.</sample>
    <sample id="206">他们使用了零-shot学习模型进行迁移学习。</sample>
    <sample id="207">最近用于评估 PaLM 能力的测试集包括最新的测试集，以避免与训练数据重叠。</sample>
    <sample id="208">根据所提供的英文内容，作者最终提出了三条建议。</sample>
    <sample id="209">根据所给的英文内容，与最强的基线相比，建议的方法获得了显著的收益。这可以从幻灯片中提到的图表中看出，该图表显示了在不同约束类别下生成脚本的性能。虽然具体的收益数字没有直接给出，但图表表明，对于某些约束类别，生成脚本的质量有显著提高，从而证明了方法的有效性。</sample>
    <sample id="210">演讲者的名字是刘树恒。</sample>
    <sample id="211">是的，论文中展示的结果和数据集可以作为未来自动文本简化任务的基准。作者在他们的研究中使用了这些结果来评估不同方法的性能，并建议这些结果作为未来工作的基准。</sample>
    <sample id="212">根据所给的英文内容，他们在论文中进行了55,000个较小模型的实验。</sample>
    <sample id="213">根据所给的英文内容，研究多模型指令调整的基础模型是OFA。</sample>
    <sample id="215">Adam Sipruckowsky在演讲中讨论了不同理论和语义方法中依赖结构的差异，包括对称和不对称的协调结构。他首先介绍了几种依赖结构，如LISA Bart和Meggy的结构，其中第一个连词是主干；以及Egor Miltruk的理论，其中整个协调结构由第一个连词领导。然后，他引入了对称的协调结构，如Prague Approach，其中协调结构由连词领导，以及多头协调结构，如De Catts的词法，其中所有连词都是协调结构的主干。Sipruckowsky接着提出了一个新论点，即对称协调结构优于不对称协调结构，基于依赖长度最小化原则。他通过比较不同句子结构的长度来支持这一论点，指出更短的依赖关系更可接受。最后，他引用了Pankbank的增强版本的统计数据，这些数据确认了左连词通常较短，并且这种趋势随着连词长度差的增加而加剧。然而，当左边没有外部治理者时，这种趋势会消失。Sipruckowsky认为，这些观察结果支持对称协调结构的论点，并挑战了不对称协调结构的论点。</sample>
    <sample id="217">The presentation introduces a research work titled "Seeing to Unseen: Exploring Compositional Generation of Multi-Attributed Controllable Dialogue" by Wei Hou, Zhen, and colleagues from Beijing University of Post and Telecommunications. The researchers discuss their contributions in seven aspects:

1. **Motivations**: They address the limitations of previous methods that focus on single attributes while ignoring the practical setting of multi-attributed dialogue generation.
2. **Methods for Multi-Attributed Text Generation**: They propose a disentangled controllable generation method (DCG) that learns attribute concepts from single values and uses a disentangle loss to disentangle different attribute combinations.
3. **Unified Reference-Free Evaluation Framework**: They introduce a unified reference-free evaluation framework (MAE) for different granularities of attributes.
4. **Experiments**: They establish two benchmarks and prove the effectiveness of their method and evaluation metrics through experiments.

The model is based on the Dialog GBT framework with a compositional prompt module. Two types of prompts are designed: attribute-oriented prompts and task-oriented prompts, which are concatenated to create whole prompt embeddings. A disentanglement loss is introduced to train multiple compositional prompts while disentangling the combination representations.

To address the lack of metrics for multi-attributed controllable dialogue generation, they propose a unified and efficient evaluation framework that does not require additional large-scale labeled data. They design a template consisting of various prompts, including emotion, act, personal control, and response mask, to reduce potential biases. They also add a controllable continuous dialogue-oriented prompt to improve stability and robustness.

The results show that their DCG outperforms all other baselines in controllability and test quality. They test their model with attribute-oriented prompts, task-oriented prompts, and disentanglement learning, demonstrating that attribute-oriented prompts guide the model to focus on controllable information, while task-oriented prompts improve task test quality, and disentanglement learning improves the ability of compositional generation.

The proposed method successfully tackles the challenges of compositional generation for multi-attributed controllable dialogue generation with only a small drop on EAC and AAS metrics. Additionally, their DCG outperforms control on both controllability and test equality of unseen attribute combinations, confirming the effectiveness of their method for transforming seen attributes to unseen combinations.

Three correlation coefficients are used to evaluate the quality of different metrics, including their automatic metrics MAE compared to human judgments. Their method outperforms classic metrics for both coarse-grained discrete attributes and fine-grained continuous attributes. They demonstrate the impact of prompts on compositional generation using a visualization of the concatenated prompt embeddings of two attributes.

Overall, the presentation highlights the effectiveness of the proposed method in generating controllable dialogues with multi-attributed values, showcasing its ability to generalize from seen attributes to unseen combinations.</sample>
    <sample id="218">根据所给的英文内容，这篇论文的作者所属机构是Google Translate。</sample>
    <sample id="219">The speaker introduces a research assistant at Academia Sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is conducted with Professor Joo Lee and other collaborators. The background of financial report analysis is discussed, along with text definition and approaches. The target company is the Fortune 1000, which is an annual report required by the SEC containing many details of companies' important activities. However, mining useful information requires lots of human efforts. The work was motivated by two observations: first, the words in a company's report are very similar, about 80% of tokens are the same, and the contents are largely dependent; second, the text similarity between two reports in continuous years is illustrated. For example, the report in 2018 is similar to the one in 2017. Based on these observations, they introduce a highlighting task and a multi-stage pipeline. They define the reference to target structures in their task, where the target and reference refer to the report of interest and the report at each previous year. Therefore, the goal of this highlighting task is to find the proportionality roots between a given pair T and R. Formally, the model will predict the highlight word importance, and therefore we can measure the performance of highlighting. For example, the word "decrease" is supposed to have higher importance in this context. This is our proposed pipeline: stage 0 is document segmentation, stage 1 is relation classification, stage 2 and stage 2+ are out-of-domain and in-domain fine-tuning. Due to the time limit, I would not talk about stage 0 more details can be found in our paper. For the stage 1, we will classify all the pairs into three types: type A refers to the pairs have higher syntactic and semantic similarities. These pairs are frequently appeared such as company's regulations. Reverse pair have similar syntactical pattern but in fact the two segments disclose very different meaning. Mismatch pair are more like a debut information or company's new operations. For the model fine-tuning stage, we first use an external dataset (ESNL) for out-of-domain fine-tuning. ESNLI is a natural language inference dataset with token annotation. For in-domain fine-tuning, we use the reverse pairs, the reverse words as pseudo positive labels, and we randomly label few other words as negative. In addition, we mix different objectives. We use the soft labeling techniques by mixing cross entropy loss and KL divergence. Therefore, we can alleviate the problem from low-quality pseudo labels. The variation dataset include ESNLI pairs and our released final dataset. We use two metrics to judge the performance: precision indicates the precision over recall, PCC means the correlation between prediction and annotations. This table shows that our domain-agnostic highlighting model achieve the best performance on final, and even preserve the generalization capability as you can see the performance on ESNLI. We further observe that our methods can benefit on relation, the mismatch pairs which we didn't use during training. In conclusion, we propose a highlighting task with our released final dataset and a simple pipeline with two-stage fine-tuning. There are many other future works we would like to try including improving effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well. That's it. So please refer to our paper and GitHub for more details and feel free to ask us if you have any question. Thank you.</sample>
    <sample id="220">根据幻灯片中提供的信息，论文的作者是来自圣奥克大学的人类语言分析系的计算机科学系候选人。</sample>
    <sample id="221">根据所给的英文内容，论文分析了德语和英语之间的翻译。</sample>
    <sample id="222">本研究探讨了在开放域问答（Open-Domain Question Answering，简称ODQA）中，如何通过数据干预来增强模型的跨域泛化能力。ODQA系统需要从文档库中检索相关段落，并使用这些段落和问题作为输入，生成答案。然而，由于训练数据与目标领域不一致，模型在处理特定领域的问题时可能表现不佳。为了解决这个问题，研究提出了三种主要贡献：1. 探讨不同数据干预方法以增强跨域泛化能力；2. 识别新领域中数据集的变化类型；3. 确定哪些数据干预类型对特定类型的变化最有效。

研究采用了一种两阶段的方法进行数据干预。第一阶段使用“少示例”方法，即使用目标领域的少量示例来指导大型语言模型生成更多示例，然后将这些示例转换为闭式问题，以适应检索器和阅读器模型。第二阶段采用“零示例”方法，通过控制问题、答案和上下文三个变量，系统地调整这些变量的影响，以评估它们对模型学习的影响。

实验结果表明，两种数据干预方法都能显著提高检索器和阅读器的性能。对于“少示例”方法，检索器性能平均提高了8%，阅读器性能平均提高了11%。对于“零示例”方法，虽然格式变化对模型性能影响不大，但闭式问题对模型学习更友好。此外，通过实体识别和答案分布的控制，可以进一步优化数据干预效果。

研究还提出了数据集变化的分类，包括概念变化、变量变化和全变化。通过计算源检索器和阅读器模型对目标数据集中每个上下文的概率，可以量化数据集变化的类型。实验结果表明，不同的数据干预方法对不同类型的变量变化响应良好。例如，“少示例”方法对所有目标数据集都有效，而“零示例”方法对概念变化和变量变化数据集特别有效。

总之，本研究通过系统地探索数据干预方法，显著提高了ODQA系统的跨域泛化能力，并揭示了不同数据干预类型对特定数据集变化类型的有效性。</sample>
    <sample id="223">演讲者的名字是张冰。</sample>
    <sample id="224">在实验过程中研究了两种模型：一种用于产生文档级别的简化文本的模型，另一种用于产生句子级别的简化文本的模型。这些模型是通过微调Longformer模型来实现的。</sample>
    <sample id="225">在 MultiInstruct 中，用于训练的 62 个不同任务中，53 个任务用于训练，19 个任务用于测试。</sample>
    <sample id="226">根据所给的英文内容，这篇论文有两位作者：Razan Al-Shatroni 和 Omar。</sample>
    <sample id="227">The speaker introduces the concept of grounded language understanding, which involves mapping natural language expressions into executable plans or programs in specific target environments. They highlight the challenges in current language models research, particularly the lack of grounding during pre-training, and the gap between pre-training and downstream applications. The speaker proposes a new framework for grounded language understanding that focuses on discrimination rather than generation, using a symbolic agent to interact with the environment and propose candidate plans, while the language model scores and ranks these candidates. This approach avoids the need for the language model to handle the validity and grammar of the target plan. The speaker demonstrates the effectiveness of this framework through experiments with different language models and settings, showing strong performance and sample efficiency.</sample>
    <sample id="228">作者在实验中使用了四个数据集：AGNews、BLOG、SST2和IMDB。</sample>
    <sample id="229">The presentation introduces a study on detecting improvable claims in argumentative writing, conducted by Gabriella Skitalinskaya and Henning Wachsmuth. The researchers focus on the challenges of revising argumentative texts to ensure optimal phrasing, which is crucial for effective communication and eliciting desired reactions from the audience. They propose two tasks: sub-optimal claim detection and claim improvement suggestion. To address these tasks, they explore the use of revision-based data from collaborative online debate platforms like Kiyo. The study highlights four main challenges: representativeness and reliability, model complexity and architecture, contextual information dependency, and topical and user bias. The researchers conclude that revision-based data can be effectively employed for the given tasks, and modeling the distance between two claim versions is beneficial for detecting sub-optimal claims. They also emphasize the importance of considering contextual information and addressing topical and user biases when evaluating the quality of argumentative texts.</sample>
    <sample id="231">NACHOS 是一个数据集，用于医学领域的信息抽取。它被用来训练和评估自然语言处理任务中的模型，特别是那些与生物医学和临床相关的任务。</sample>
    <sample id="232">演讲者的名字是Avi Lillard。</sample>
    <sample id="233">The presentation introduces a research paper titled "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi from the University of Trento and Fondazione Bruno Kessler. The paper addresses the challenges in current simultaneous speech translation (SST) models, such as the need for specialized architectures, long training procedures, and multiple models to achieve different latency regimes. The proposed solution is a strategy called Encoder-Decoder Attention (EDAT), which uses existing offline models without retraining or specific architecture for SST. EDAT decides whether to emit partial translations based on attention weights, allowing for flexible latency adjustments while maintaining translation quality. The results show that EDAT outperforms other strategies applied to offline models, including the Whitaker strategy and local alignment, in terms of translation quality, latency, and computational efficiency.</sample>
    <sample id="234">根据所给的英文内容，提示策略对结果有重大影响。在实验中，使用一维提示和不同提示时，翻译质量差异显著。例如，在500多个句子中，差异达到1分以上，极端情况下可达40分。这表明选择正确的提示策略对于提高模型性能至关重要。</sample>
    <sample id="235">根据幻灯片上显示的标志和名称，论文的作者分别来自卡内基梅隆大学的语言技术研究所、葡萄牙技术大学、人工智能研究实验室（BAIR）和Unbabel。</sample>
    <sample id="236">每个任务都配备了五个专家编写的指令。</sample>
    <sample id="237">作者建议通过使用一个诊断测试套件来测试模型，该套件包含一个核心参考解决任务，旨在评估模型从不同来源整合和使用知识的能力。</sample>
    <sample id="238">The video presents a new benchmark dataset called MeetingBank, which is designed to address the challenges of developing summarization technologies for different meeting domains. The dataset includes 1366 city council meetings with nearly 7000 instances, including meeting transcripts, reference summaries, and URLs containing useful resources. The data collection process involves converting audio data to transcripts using speech-to-text APIs, identifying meeting types and data from meeting websites, locating corresponding reference summaries and meeting segments, and aligning time stamps to get the segment transcripts. The dataset provides statistics on the number of meetings, meeting duration, tokens per meeting, speakers per meeting, and the year period of meetings collected. It also includes the number of summarization instances gathered for each city and the average number of sentences and tokens in both source and summary texts. The level of abstraction in meeting summaries is measured by two common metrics: coverage and density. The coverage score measures the percentage of summary words that appear in source transcripts, while the density score evaluates how much the summary can be characterized as a set of extractive fragments. The dataset is used to evaluate top-tier summarization systems, including both extractive and abstractive summarizers. The results show that extractive summarizers like Oracle yield high ROUGE scores, while abstractive models like DialogLM achieve the highest ROUGE scores among models. GPT-3 does not perform well according to automatic metrics but shows exceptional performance in terms of fluency and coherence. Human evaluation reveals that GPT-3 achieves the highest overall scores, showing exceptional performance in terms of fluency and coherence, but less impressive results in terms of informativeness and factuality. The findings suggest that meeting summarization solutions should continue to focus on capturing the main discussion points, and new methods of automatic evaluation metrics should be developed to better align with human preferences.</sample>
    <sample id="239">Hello, everyone. My name is Ibilhar, and I will give a short overview of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PaLM is a 540 billion parameter large language model presented last year in 2022. It's trained on a large collection of texts comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the NMT community. This involves using the latest test sets to avoid another lab of the test data with the training data of the language model. And we compare two state-of-the-art systems, so the best-performing systems on the WMT evaluation. We use state-of-the-art NMT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the large language models for translation, as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences, 516 out of 1,000, the difference observed is of more than one BLEU points, and this can go in extreme cases up to 40 BLEU points. So it's important to select a good prompting strategy. In our experiments, we conclude for a five-shot prompting strategy, where we just mark each sentence that we provide to the system with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences are marked with German colon, and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several-shot prompting. It's crucial for zero and one-shot prompting, and when we go, as in our case to five-shot prompting, there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the train data, that it's more nice, and the results, so better performance when using the dev data. Nonetheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But PaLM comes pretty close to a commercial system. Now, in our case, we chose to evaluate with Google Translate. The insights that we gain from the evaluation that we perform using the NMT framework, that the fluency of PaLM is comparable to state-of-the-art of the art systems, but the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that PaLM chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation. However, the style error category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="240">你好，我是Dawei，一名在德国萨尔兰大学攻读博士学位的学生。在这段视频中，我将介绍我们最近的工作“比你想的更弱：对弱监督学习的批判性审视”。这是一项与Xiaoyu Shen、Marius Mosbach和Gustavo Stephan以及Dietrich Klakow合作的研究。我将从一个简要的介绍开始，介绍弱监督和弱监督学习。在弱监督中，我们不手动标注数据；相反，我们使用弱标注来源，如简单的启发式规则、知识库或低质量 crowdsourcing，正如图示例所示。与人类标注相比，弱标注更便宜，但它们也存在噪声，意味着其中一部分标注是错误的。如果我们直接用弱标注数据训练神经网络，神经网络倾向于 memorize 噪声，而不是泛化。在弱监督学习中，训练算法被提出以 robustly 训练神经网络在标注噪声上，从而确保训练模型仍然泛化良好。在最近的弱监督学习（WSL）工作中，一种常见的说法是，人们认为在弱标注数据上训练模型可以实现高性能的干净测试集。实际上，这个说法并不完全正确，因为这假设存在额外的干净验证集用于模型选择。我们关注的是这个问题设置，因为它暗示了在弱监督学习中需要额外的手工标注。然而，这个必要性经常被忽视。我们采用的方法旨在回答三个研究问题：1. 清洁验证数据是否对于WSL是必要的？或者我们可以使用一个噪声验证集吗？2. 如果清洁数据是必要的，或者对于WSL来说是强制性的，那么我们需要多少清洁样本？3. 我们是否应该仅仅使用清洁样本进行验证，还是有其他更好的方法来利用它们？我们在我们的工作中解决了这些问题，并得出以下结论：首先，我们发现有趣的是，最近的WSL方法确实需要清洁验证样本才能正常工作，否则会有很大的性能下降，如图所示。如果没有清洁验证样本，训练的模型无法超出原始弱标签的泛化能力，这意味着训练是徒劳的。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取清洁标注数据的成本不应被忽视。其次，增加清洁验证样本的数量有助于WSL方法实现更好的性能，如图所示。通常，每个类别只需要20个样本即可获得较高的性能。但是，如果我们要决定使用清洁样本进行验证，直接在清洁数据上进行微调将甚至会获得更好的性能。右侧的图表显示了在每个类别10个样本的情况下，直接微调方法与WSL方法之间的性能差异。最后，WSL方法声称的性能提升可以通过允许在清洁验证样本上继续微调来轻松实现。如图所示，FTW模型最初在更复杂的WSL方法（如Cosine）上表现不佳。然而，如果允许在清洁样本上继续微调，FTW的表现与其它方法相当。因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多的计算时间和磁盘空间。总之，我们展示了最近的WSL方法需要干净的手工标注数据才能正常工作，它们的性能提升和实用性被高估了。我们对未来的建议如下：1. 报告模型选择标准，例如报告模型选择是否依赖于清洁验证样本。2. 将WSL方法与基于干净样本的基线进行比较。3. 持续微调是一个简单而强大的基线，应在未来WSL工作中考虑。最后，我们开源了我们的代码，您可以在幻灯片中的QR码处找到它。请随意查看。谢谢您的观看和享受会议。</sample>
    <sample id="241">演讲者介绍了一篇名为“基于人类的早期误信检测评估：COVID-19治疗的案例研究”的论文。该论文是与Yang Chen、Wei Xu和Alan Ritter合作完成的，发表在Georgia Tech。论文讨论了自动检测社交媒体平台上误信的挑战，包括数据集的不现实评估和缺乏人类中心的方法。作者提出了一个评估框架，旨在解决这些缺陷，使系统更具代表性，并涉及人类内容 moderators。该框架包括两个主要组成部分：第一个组件用于检测误导性声明，第二个组件用于政策违规验证。作者还评估了他们的系统在早期检测和政策违规验证方面的有效性。</sample>
    <sample id="242">对话系统的常用评估方法是使用人类评估，例如让人类裁判选择两个对话中哪个更好，或者为对话打分。</sample>
    <sample id="243">根据幻灯片上显示的作者列表，这篇论文有五位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识包括：Servin 是一位律师，而 Kea 是一位面包师。此外，背景知识还涉及 Servin 在一天结束时决定案件的法律场所，以及 Kea 在公园里遇到 Servin 后的休闲活动。这些信息对于理解 Servin 和 Kea 之间的互动至关重要。</sample>
    <sample id="245">The presentation introduces a study titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The study aims to address the challenges of automatic metrics and the lack of understanding of best practices for recruitment on Amazon Mechanical Turk (MTurk). The researchers present a two-step pipeline for identifying high-agreement workers, which includes a qualification task and an endurance task. The qualification task assesses the annotator's ability to evaluate multiple dimensions correctly, while the endurance task tests their capacity to handle heavy workloads. The study also includes a reference-based task to test general performance and compares the results with baseline MTurk workers and cloud research workers. The findings show that the pipeline can achieve high agreement with human experts and is more efficient in terms of time and resources compared to traditional methods. However, the study has limitations, such as only testing English summarization on the MTurk platform and not providing a guarantee for the training of crackness.</sample>
    <sample id="246">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="247">标题：通过知识图谱进行事实验证

演讲者：金国，KAIST AI

演讲主题：介绍论文《FactKG:基于知识图谱的事实验证》。金国首先介绍了现有的事实验证数据集，如Fever和VitamC，这些数据集使用Wikipedia文本或表格作为证据。然而，没有一个数据集利用知识图谱作为证据与自然语言声明。因此，他们提出了一个新的任务：基于知识图谱的事实验证。

金国解释了为什么知识图谱是一个有价值的来源。首先，它允许可靠的事实验证，因为知识图谱中的证据是直观的，可以直接连接到声明，无需额外解释。其次，它具有实际应用价值，例如在现代对话系统中，可以用于检查用户查询与知识图谱之间的一致性，从而实现更准确的信息更新和一致性检查。

接下来，金国介绍了FactKG数据集，该数据集基于Dipedia构建，包含两种风格的声明：文本和表格。数据集包括支持和反驳两种标签。任务涉及从Dipedia中检索证据并验证声明。数据集还涵盖了五种类型的推理：单个三元组、连接、存在、多三元组和否定。

金国详细说明了每种推理类型的具体验证方法。对于单个三元组，验证是否两个实体之间存在关系；对于连接，验证多个单个三元组是否成立；对于存在，验证实体是否具有特定关系；对于多三元组，使用多路径推理验证；对于否定，即使找到图谱证据，也需要进行额外的推理以确认声明是否为假。

金国还提到了数据集的实用性和方法论，包括使用Collocustyle转换模型和预设模板，以及构建基线模型进行比较。结果表明，使用图谱证据的模型显著优于仅使用声明的基线模型。

最后，金国感谢听众，并表示数据集已公开提供，鼓励观众联系获取更多信息。</sample>
    <sample id="248">根据所给的英文内容，NLPositionality 的注释者在各个人口统计学特征（即国家/地区、性别等）方面并不均衡。例如，在 GPD4 社会接受度分析中，数据集和模型最接近英语国家，而在 DynaHate 中，它们最接近英语国家。此外，在 GPD4 的社会接受度任务中，数据集和模型最接近具有大学或更高教育程度的人群。然而，这些数据集和模型在某些人口统计学特征上存在不平衡性，例如，在 GPD4 社会接受度任务和 DynaHate 分析中，数据集和模型对非二元性别群体的代表性较低。</sample>
    <sample id="249">在可接受的域中扰乱句子的方法是通过保留相关结构但添加噪声来扰乱输入句子。这种扰动不会显著改变模型对MPP评分的影响，表明模型对可接受域中的潜在句法和语义特征敏感。</sample>
    <sample id="250">维度评估意味着通过明确标注每个模型响应是否表现出某些行为，如提供不相关的信息或自相矛盾，来减少人类评估的主观性。</sample>
    <sample id="251">根据所给的英文内容，这篇论文的作者来自中国科学技术大学。</sample>
    <sample id="252">The presentation introduces a research project titled "U-CREAT: Unsupervised Case Retrieval using Event Extraction," which is a collaborative effort by Sahil Kiran Tankarella, Abhinav Joshi, Akshat Sharma, and Ashutosh Modi from the Department of Computer Science and Engineering at IIT Kanpur. The project focuses on addressing the challenge of retrieving relevant past precedents (cited documents) in the legal domain, particularly for legal professionals like lawyers and judges who rely on their experience to find such documents.

The presentation highlights two key contributions to the field of prior case retrieval: the ILPC dataset and the U-CREAT pipeline. The ILPC dataset, or Indian Legal Prior Case Retrieval dataset, is a new benchmark consisting of 7,700 legal cases with an average of 6.775 citations per query document. This dataset provides a comprehensive test bed for evaluating the performance of prior case retrieval algorithms.

The U-CREAT pipeline leverages unsupervised learning techniques and introduces an event-based approach for prior case retrieval. It demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. The event extraction process involves dependency parsing using SpaCy to identify events as subject-verb-object triplets, which are then used to compute an interaction matrix between query and candidate events.

Experiments using various models, categorized into count-based, transformer-based, and event-based models, were conducted to validate and compare their performance on the prior case retrieval task. The results show that event-based models outperform all other methods, including supervised approaches, with significant boosts in performance metrics such as F1 score and lower inference time.</sample>
    <sample id="253">Mario Ezra Aragón在ACL2023上介绍了名为“DisorBERT”的研究，这是一种双域适应模型，用于检测社交媒体上的精神疾病迹象。该研究由墨西哥和西班牙的研究人员合作完成。精神疾病被定义为一种心理障碍，与思维、情感、情绪和行为相关的压力和 disability。研究利用社交媒体的海量内容，通过自动分析用户的社交媒体帖子来检测精神健康问题。这种分析旨在支持一种新技术，能够早期预警精神疾病的发作，并提供支持证据。

研究采用了域适应技术，以提高模型在特定领域的性能。具体来说，研究使用了预训练的BERT模型，该模型基于维基百科和Google Books数据。为了使模型适用于Reddit和精神健康领域，研究者调整了词汇表，优化了模型对特定任务的理解，并使用了一个字典来指导掩码过程。这种方法旨在让模型专注于重要单词，从而提高检测准确性。

实验结果表明，DisorBERT在精度和召回率方面表现良好，特别是在处理精神健康相关术语时。通过分析用户帖子中预测的单词，研究展示了DisorBERT倾向于识别与精神疾病相关的负面含义和关键词，如“focus”、“talk”、“breath”、“sleep”和“eat”。此外，可视化工具展示了用户帖子中最重要的文本片段，帮助识别与精神疾病相关的主题，如“焦虑”和“药物”。

未来的工作计划包括探索不同字典资源的应用以及使用临床数据，以进一步提高模型的性能和准确性。</sample>
    <sample id="254">The presentation introduces a research framework for document-level distant relation extraction, titled "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The speaker, from the Singapore University of Technology and Design, explains that the goal is to extract relations between entities in a document. The current methods rely on large-scale human-annotated corpora, which are time-consuming and labor-intensive. Recent work has leveraged distantly supervised data to pre-train document-level relation extraction models for better performance.

The presenter highlights that distantly supervised data contains various noisy labels, and the current approach to alleviate this noise problem involves using pseudo labels. However, this method still risks introducing false relations due to forced positive pseudo labels. To address this issue, the proposed framework incorporates uncertainty estimation to determine whether a model prediction can be trusted. An instance-level uncertainty estimation is introduced to capture the uncertainty score for overlapping relations.

The framework also includes a relabeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost performance. Uncertainty estimation is crucial for misclassification detection, out-of-distribution instance detection, and active learning. The Monte Carlo dropout technique is introduced to model uncertainty in the pre-training process, requiring multiple stochastic forward passes with active dropout to capture model uncertainty.

To address the challenge of overlapping relations, the presenter modifies the estimation process to obtain an instance-level uncertainty score for each positive pseudo label. The distribution of uncertainty scores for each relation class is observed to be different, with frequent classes containing lower average uncertainty than long-tailed classes. Dynamic class uncertainty thresholds are proposed to filter pseudo labels with high uncertainty.

The final part of the presentation compares the framework with several strong baselines on two public datasets, demonstrating superior performance. The main contributions of the work are summarized as: 1) the framework with uncertainty-guided label denoising, which improves the label quality of DS data; 2) the instance-level uncertainty estimation method for overlapping relations; 3) the iterative relabeling strategy with dynamic class uncertainty thresholds for the long-tailed problem; and 4) significant performance improvements.</sample>
    <sample id="255">在零和一击提示中，提示的形式很重要。然而，在五击提示的情况下，提示的实际形式对性能的影响几乎可以忽略不计。</sample>
    <sample id="257">作者评估了四个当前对话模型，使用ABC-Eval方法对它们进行了评估。</sample>
    <sample id="258">The video introduces a research paper by Cheng-Han Chiang and Hung-Yi Lee from National Taiwan University, exploring the potential of large language models (LLMs) as an alternative to human evaluations in natural language processing. The researchers propose using LLMs to evaluate text quality by providing them with instructions and samples to rate. They argue that while human evaluations are unstable and difficult to reproduce, LLMs can potentially achieve similar results without these drawbacks.

The motivation behind this work is to find an alternative to human evaluations, which are often used to assess the quality of texts. By giving LLMs specific instructions and samples, they aim to replicate the process of human evaluation but with greater consistency. The researchers note that some LLMs have been shown to follow natural language task instructions effectively, suggesting that they could perform evaluations similar to human evaluations.

To validate their approach, the researchers conducted experiments where LLMs rated stories generated by GPT-2 or written by humans based on four attributes: grammar, coherence, likability, and relevance. They compared the LLM ratings with human ratings provided by English teachers, who were considered experts in scoring essays. The results showed that while human raters preferred human-written stories over those generated by GPT-2, some smaller LLMs did not show a significant preference. However, larger LLMs like DaVinci and ChatGPT demonstrated a clear preference for human-written texts, indicating their potential as alternatives to human evaluations.

The video concludes by inviting further questions about the agreement between LLMs and human raters on individual story ratings, the impact of changing instruction wording or response sampling methods, and the benefits and drawbacks of using LLM evaluations compared to human evaluations. It also mentions that the full details of these experiments and discussions are available in the paper and at the poster session at ACL 2023.</sample>
    <sample id="259">The speaker introduces a research project on cross-lingual semantic parsing, which involves translating queries in multiple natural languages into various semantic representations. The project aims to address the limitations of existing models by providing a unified dataset for cross-lingual semantic parsing. The dataset includes 90 datasets from diverse domains, 5 semantic parsing tasks, 80 semantic representations, and 22 natural languages across 15 language families. The study evaluates six settings: translate test, monolingual model, monolingual fine-tune, multilingual model, zero-shot transfer, and few-shot transfer. The results show that encoder-decoder models outperform previous work, particularly in the few-shot setting. The study also highlights the challenges of multilingual language models and the need for further research in this area.</sample>
    <sample id="260">根据幻灯片中提供的信息，这篇论文有六位作者。</sample>
    <sample id="261">根据所提供的英文内容，优秀规划器的理想品质是能够编写合理且符合约束的脚本。</sample>
    <sample id="262">根据幻灯片上列出的作者姓名，这篇论文有九位作者。</sample>
    <sample id="263">本研究旨在解决大型语言模型中存在的重要问题，即标签偏见。在文本分类任务中，我们识别了三种类型的标签偏见：标签偏好、上下文标签偏见和域标签偏见。域标签偏见是一种新识别的偏见类型，它反映了任务领域对模型预测的影响。我们提出了一个名为“域上下文校准”的方法，通过使用随机领域内单词来估计和校准模型的标签偏见，从而显著提高模型性能。实验结果表明，域上下文校准在处理大型标签偏见方面效果显著，特别是在任务领域与训练数据差异较大的情况下。</sample>
    <sample id="264">在实验部分，我们全面评估了我们提出的方法。我们构建了两个基准，基于MSVD和MSVTT，包括跨数据集和跨领域设置。接下来，我们将展示我们实验的主要结果。由于在可转移的音频-视觉生成方面没有现有工作，我们首先选择了几种基线方法，包括基于RNN和Transformer模型。这些模型仅在训练数据上进行微调，使用相同的微调框架（TAVT）和测试数据集。实验结果表明，在所有材料上，我们的方法显著优于所有基线模型，特别是在跨数据集和跨领域设置中。对于低资源领域，如“猫”和“美丽”，其他方法出现性能退化，而TAVT仍然保持性能优势。此外，我们还进行了消融实验，以分析音频特征和音频评论的影响。</sample>
    <sample id="265">演讲者的名字是Vasuda Varadarajan。</sample>
    <sample id="266">根据所给的英文内容，这篇论文的作者是德国萨尔兰特大学的PHD学生。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误，即它选择删除源句子中的某些部分以产生更流畅的翻译，但有时会导致准确性问题。</sample>
    <sample id="269">因此，我们提出了一个新的数据集Deplain，它被分为两个子数据集：Deplain-AP和Deplain-WEB。Deplain-AP基于新闻文本。例如，你可以看到测量对话质量的五分之一和十分之一的自我和伙伴矛盾比例，而平均Lickert一致性分数只能解释四分之一或更少。最后，我们检查了每个评估指标是否捕捉了聊天质量的独特方面，使用逐步线性回归。你可以看到所有ABC-EVAL指标的组合解释了超过25%的对话质量，当你一次删除一个指标时，大多数指标都会失去相当一部分关于质量的信息。另一方面，所有轮次级别的Lickert指标的组合解释了较少的质量，并且较少的这些指标携带独特的信息。这些可靠的、信息丰富和独特的ABC-EVAL指标使我们能够以比以往任何时候都能实现的更高分辨率评估对话型AI。你可以看到，在我们的实验结果中，有几个挑战仍然存在，并且已经被精确量化。例如，我们测试的聊天机器人在大约20%的回应中存在 commonsense 违反行为，产生无关信息大约15%的回应，并且大约10%的时间自我或与伙伴矛盾。随着该领域的快速发展，许多这些错误率可能在自我们评估以来发布的新模型中减少。然而，这正是我们需要追求可靠和准确的评估指标来比较模型的更有意义的步骤。我们希望ABC-EVAL可以被其他领域的专业人士利用，作为朝着这个方向的重要一步。我们期待看到对话型AI将在接下来的几个月和几年内如何 advancement。谢谢观看。</sample>
    <sample id="270">根据所给的英文内容，这篇论文的作者所属机构是Emory University的Emory NLP Lab。</sample>
    <sample id="271">在本文中，CFT 代表“持续微调”。这是作者提出的一种方法，即在训练过程中不断微调模型，而不是仅使用干净的验证样本进行验证。这种方法旨在通过利用干净样本来提高模型性能，从而实现与更复杂的弱监督学习（WSL）方法相当的性能，而无需增加计算时间和磁盘空间。</sample>
    <sample id="272">根据幻灯片上显示的信息，这篇论文有六位作者：Koustuv Sinha、John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams。</sample>
    <sample id="273">Hello, my name is Kayo Yin, and I will be presenting our work titled "When Does Translation Require Context? A Data-driven Multilingual Exploration." This work was done in collaboration with Patrick Fernandez, Emmy Liu, Andre F. T. Martins, and Graham Neubig. So, a lot of translations depend on context. For example, how would we translate "more" in this sentence? Well, if the previous sentence was "Things could start to get dangerous if the ministers find out," then "more" refers to a spy. But if the previous sentence was "Could it be anything serious, doctor?" then "more" refers to a birthmark. So depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages, since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions: first, when does translation require context, and second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context-dependent translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models, and this is done by measuring how much information the context C provides about the target Y given the source X. You can think of CXMI as the information gain from giving context to the model. In this work, we extend CXMI to P-Y CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high P-Y CXMI as ones that require context for translation. Now, we analyze words with high P-Y CXMI to look for patterns between these words, and we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at parts of speech tags that have high means P-Y CXMI, and this allows us to find, for example, dual pronouns in Arabic that have relatively high P-Y CXMI, and this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-Y CXMI averaged over all of its different occurrences, and this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is supported to translate in the right formality. And finally, we look at different individual tokens that have high P-Y CXMI, and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure, such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon, and we call our tagger the multilingual discourse-aware or Muda tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the Muda tagger by applying the tagger on a parallel corpus that we want to use for evaluation, and we apply our translation metrics of choice on the context-dependent examples that the Muda tagger has identified, and finally, we use our benchmark as well as other metrics to evaluate different models on document-level machine translation. First of all, when we use corpus-level metrics, so for BLEU, we find that context-agnostic models have the best performance, but then if we use Comet, context-aware models perform best, and if we use Word F-measure, then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now we use the Muda benchmark to evaluate models, and we find that context-aware models are significantly more accurate than models that do not use context for certain discourse phenomena, such as formality and lexical cohesion, but these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form, so this sort of suggests where we would need to see more progress for document-level translation. We also compare different commercial systems, and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context, and then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomena models can handle well or not, and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto.</sample>
    <sample id="274">演讲者的名字没有在提供的英文内容中直接提到。</sample>
    <sample id="276">Ananya and Vignesh在他们的工作中介绍了IndicMT Eval，一个用于评估印度语言机器翻译指标的数据库。他们研究了多种英语到印度语言的翻译任务，并分析了这些翻译的评估指标与人类评分的相关性。然而，他们注意到对印度语言到英语的翻译评估指标的研究较少。因此，他们专注于研究五种印度语言，包括泰米尔、马尔雅文（Dravidian语言）和印地语、乌尔都语和古吉拉特语（Indo-Aryan语言）。他们从Foros数据集中随机选择了200个句子，为每个句子生成多个候选翻译，并使用七种不同的翻译系统生成1400个候选翻译。为了收集这些翻译的人类注释，他们邀请双语专家评估每个翻译输出，记录错误类型、严重性和总体评分。通过分析这些注释，他们比较了不同评估指标与人类评分的相关性，并评估了不同模型的性能。他们的研究表明， Comet和 Comet MQM 是评估印度语言机器翻译性能的最有效指标。</sample>
    <sample id="277">根据所提供的英文内容，该方法没有被提及为特定的名称。然而，它被描述为一种“神经序列到序列模型”，它直接建模了输入和输出片段之间的对应关系，而无需使用树结构。</sample>
    <sample id="278">“显性词汇”(marked words) 方法是一种识别区分标记群体和未标记群体的词语的方法。它基于社会语言学中的概念，即未标记群体默认为未标记，而任何与未标记群体不同的群体则被标记。通过比较不同群体的生成人物描述中使用这些词语的频率，该方法能够识别出反映刻板印象和本质化叙事的特定词语。</sample>
    <sample id="279">根据所给的英文内容，这篇论文的作者所属机构是美国华盛顿大学。</sample>
    <sample id="280">The paper presents a multi-modal fusion framework for emotion recognition in conversations, which integrates visual cues by capturing facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. The framework is composed of three components: MultiTen Text, MultiTen Audio, and MultiTen Video, each of which integrates one modality with complementary information from the other two modalities through stacked bi-directional multi-head cross-attention layers. The model uses residual connection and layer normalization over the output of each stage, and employs sample-weighted focal contrastive loss to assign higher importance to hard-to-classify minority classes and make samples pairs with different emotion labels mutually exclusive with each other to maximize inter-class distances. Experimental results demonstrate that MultiEMO achieves state-of-the-art performances on two ERD benchmark datasets, MELD and iEMOJI, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">本研究探讨了机器翻译模型在处理弱标签数据时的泛化能力，并分析了模型在不同语言和场景下的表现。研究首先通过测量上下文信息对目标词的贡献（CSMI）来评估模型对上下文的依赖程度。接着，通过分析高CSMI值的单词，识别出需要上下文才能正确翻译的特定语言和语法规则。研究还开发了一个多语言多模态标签器（Muda），用于自动识别与五种 discourse 现象相关的单词。最后，使用Muda标签器评估不同模型在文档级别上的性能，发现上下文感知模型在处理某些 discourse 现象时表现出显著优势，而其他模型在处理这些现象时的表现则较为接近。</sample>
    <sample id="282">The presentation introduces a new research work titled "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Enhancing" by Xuekai Zhu, presented at ACL 2023. The work addresses the task of non-parallel text style transfer in natural language generation, focusing on discourse-level style transfer, which is crucial for imitating author style while maintaining content coherence. The main challenge lies in transferring style-specific content to another style without losing context or meaning.

The proposed solution involves a generative model called StyleTrans, which learns discourse representations from source texts and combines them with normal style embeddings to generate text in target styles. A new training objective is designed to reduce style-specific features in discourse representations, pulling the representation derived from different texts closer in the latent space. Additionally, the training framework is separated into two stages: the first stage involves an adversarial training framework using self-reconstruction loss, disentanglement loss, sentence-level loss, and style classifier loss to recover input, disentangle style and content, capture sentence-level dependency, and produce style signals. The second stage focuses on fine-tuning the style-specific content and removing masked tokens to achieve accurate style transfer.

The research collected datasets in Chinese and English to evaluate the performance of StyleTrans in transferring fairy tales or short stories to typical author styles. Experimental results confirm the effectiveness of the model, showing strong style control and content preservation. Visualization and case studies demonstrate that StyleTrans can maintain the main sentences and relevant sentences while enriching the whole story line and maintaining the main content.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Prague approach”或“conjunction headed approach”。</sample>
    <sample id="284">The presentation introduces a novel fuzzy span mechanism for enhancing Universal Information Extraction (UIE) in the context of the ACL 2015 conference. The current spam-based UIE model primarily focuses on identifying and labeling spam boundaries in text, relying heavily on annotated spam boundaries. However, this approach can lead to ambiguity in labeling the golden spam boundary, as different annotation spans may be considered reasonable. To address this, the proposed method uses a fuzzy approach instead of a precise one, allowing for more flexibility in boundary labeling.

The presentation also highlights a mismatch between Transformer feature extraction and information extraction. Basic Transformers focus on global features, ignoring the hypothesis that spam has limited length. To improve this, an adaptive attention mechanism is proposed for spam extraction decisions. This involves representing the target boundary as a continuous distribution of correct probabilities within a specific range, where Rmin and Rmax denote the start and end of the fuzzy boundary, and Q represents the correctness of each position. By converting this continuous distribution into discrete values through a sampling function, the model calculates the boundary cross-entropy with the golden boundary as BCE loss and adds KL divergence between the predicted boundary and the fuzzy span boundary as supplementary information.

To further refine the attention distribution for spam extraction, a fuzzy span attention mask function is introduced. This function dynamically adjusts the length of the attention range by introducing an optimizable parameter delta, ensuring that the attention span of the model is dynamically changing. Additionally, the attention distribution on the fuzzy span boundary linearly decays rather than truncates, providing a more flexible and effective way to model the fuzzy span boundary.

The overall structure of the proposed model includes a fuzzy span attention layer added at the top level to guide the model's decision process without affecting the text encoding capability. Experiments conducted on three information extraction tasks—named entity recognition, relationship extraction, and aspect sentiment triple extraction—demonstrate the effectiveness of the proposed method. Notably, FSUIE achieves significant performance improvements in named entity recognition on small-scale datasets, new state-of-the-art results in relationship extraction on datasets like ACE 2004, 2005, and ADE, and competitive performance in aspect sentiment triple extraction on datasets like 14R, 15R, and 16R of the ASTV2 dataset.

The results of the ablation study show that FSUIE improves convergence speed by guiding the model to obtain a reasonable attention distribution, enabling it to fully utilize annotation information and achieve greater information extraction capabilities. The combined effect of these two mechanisms produces a greater enhancement in performance. Visualization of the attention distribution of the fuzzy span attention layer reveals that the model focuses on semantic information within a limited range of preceding tokens, aligning with the expectations of the researchers.</sample>
    <sample id="285">The video presents a research work titled "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The work focuses on addressing the issue of factual errors in dialogue summarization, which is common in both model-generated and reference summaries. The video outlines two main solutions to this problem: introducing factuality-related objects during the training or inference process to enhance the accuracy of summarization models, and designing an independent factual error correction model (FEC) that takes the source document and the model-generated summary as inputs and outputs a corrected summary.

The video highlights the importance of correcting factual errors in dialogue summarization, noting that current FEC models have flaws in their evaluation methods, which may divert the optimization of FEC models from their original purpose. The video explains that factuality metrics such as Fact CC and DA are used to evaluate FEC models, but these metrics can be unreliable and do not distinguish between different types of solutions. The video argues that introducing manually annotated reference corrections is necessary to address these issues and improve the performance of FEC models.

The video also introduces a new taxonomy of factual errors, categorizing them into content-based and form-based errors. Content-based errors are categorized according to the part of speech and dependencies, while form-based errors are categorized according to whether they involve addition, deletion, or substitution operations. The video concludes by emphasizing the need to change the evaluation method for FEC models and suggests combining human-annotated data with synthetic data as a promising direction for improving the performance of FEC models in correcting factual errors.</sample>
    <sample id="286">演讲者的名字是James Finch和Sarah Finch。</sample>
    <sample id="287">根据幻灯片上显示的信息，这篇论文有五位作者：Mohammad Javad Hosseini、Filip Radlinski、Silvia Paretti 和 Annie Louis。</sample>
    <sample id="288">根据所提供的英文内容，可用于测试句法现象的数据集包括Blimp数据集和Syntax Gym数据集。这些数据集用于评估语言模型在处理句法结构时的性能。</sample>
    <sample id="290">第一个研究问题的五种方法的缩写是WSL，代表“弱监督学习”。</sample>
    <sample id="291">该模型在任务上进行了评估，包括命名实体识别、分类、分词和问答。</sample>
    <sample id="294">CamemBERT 最初是在四个 gigabytes 的 subset of Natscience 上训练的。</sample>
    <sample id="295">演讲者的名字是Adam Siprowski。</sample>
    <sample id="296">演讲者介绍了一项研究，该研究涉及使用自然语言和向量表示进行跨语言语义解析。他们观察到不同代人之间的 disagreement 对于幽默的看法，发现代人之间的 disagreement 更为明显。此外，他们在地理分布上观察到的标注者的标注差异，发现英国和爱尔兰标注者的标注差异最大。演讲者表示，他们将回答问题并继续讨论他们的研究。</sample>
    <sample id="297">本研究项目旨在开发一个包含丰富上下文信息的词典和术语表，收集了340个用于种族主义、 transfobia 和反犹太主义的狗哨子术语和符号。这些术语和符号主要来自学术、维基百科、博客和其他来源，并且都是英文和美国中心的。例如，性权利中的“which”是一个 transphobic dog whistle。我们通过分析历史上的美国政治演讲来研究狗哨子，发现种族主义狗哨子的频率与共和党南方战略和保守主义的兴起密切相关。我们还评估了语言模型（如GPT-3）识别狗哨子的能力，并发现它们在识别正式注册的狗哨子时表现良好，但在识别社交媒体使用的非正式狗哨子和反同性恋狗哨子时表现不佳。最后，我们通过将狗哨子替换标准组别标签或骂人词，研究了有毒检测的性能变化，发现有毒句子的评分会降低。</sample>
    <sample id="298">通过实验，我们发现通过使用更近的数据重新训练或继续预训练模型，性能会下降。这表明时间漂移是导致性能下降的主要原因。</sample>
    <sample id="299">该研究旨在通过最小化训练来提高自然语言理解（NLI）模型的稳健性。最小化训练涉及在预训练数据中引入多样化的观点，以庆祝民主和思想的普遍性。然而，这些不同的政治观点内在地具有社会偏见，可能导致下流任务应用中的公平性问题。为了解决这个问题，该研究提出了一个框架，用于从预训练数据到语言模型再到下流任务的公平性传播管道进行调查。该框架通过在训练过程中使用最小化训练目标来工作，该目标旨在最小化NLI任务的损失，同时最大化学习器的损失，以生成样例权重。这鼓励学习器专注于输入空间中的高损失区域，从而优先学习来自未被充分代表的硬例子。该方法不假设数据集中存在特定类型的偏见，而是依赖于学习器的训练动态来生成样例权重。最后，该研究评估了所提出的方法在三个常用NLI数据集（MNLI、Fever和QuQP）以及它们的对应出分布测试集（Hans，Symmetric和Paws）上的性能。结果表明，与ERM训练模型以及每个数据集上表现最好的短语消解方法相比，最小化训练目标显著提高了出分布性能，同时保持了高内分布准确率。此外，该研究还探讨了在更大模型中泛化的性能、预训练对学习者的影响、辅助器的大小以及学习者样例权重分布的质量。</sample>
    <sample id="300">The presentation introduces a new task called interactive dictation, which aims to enable users to dictate and edit documents using their voice in a natural and intuitive manner. The work is a collaboration between Microsoft Semantic Machines and researchers Jason Eisner, Adam Pauls, and Sam Thomson. Interactive dictation involves flexible interleaving of dictation and editing without trigger words, allowing users to correct mistakes and issue commands verbally. The current state of speech-to-text systems primarily supports dictation but lacks support for invoking edits through vocal commands. The proposed system aims to provide a more natural interface by recognizing when users are dictating versus commanding and what commands they are invoking. The presentation outlines the key features of interactive dictation, including flexible interleaving, intuitive and open-ended natural language utterances for specifying edits, and a four-step procedure involving ASR recognition, segmentation, normalization, and execution of dictation and command utterances. A new data collection interface was designed to collect data for this task, and a baseline system was created to perform each step. The presentation also discusses the evaluation of different architectures and output types for the interpretation model, highlighting trade-offs between runtime and accuracy.</sample>
    <sample id="302">排列输出序列中的词元是必要的，因为模型需要学习如何在不依赖树结构的情况下进行组合泛化。通过使用多标签标记和潜在排列，该模型可以处理不同排列的输出，从而提高其在处理具有复杂结构的数据时的性能。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为这有助于研究这些偏见缓解方法是否导致了积极刻板印象和本质化叙事。缺乏透明度使得很难确定这些模式是否源于过度价值对齐或反刻板印象方法，这可能导致需要进一步研究和理解以解决这些模式。</sample>
    <sample id="304">最小对不可接受输入是添加一个不可接受的前缀，这会导致模型对不可接受输入的MPB评分显著增加。</sample>
    <sample id="305">在日常生活中，人们通常通过遵循一系列的规则或脚本来计划他们的行动。在这个演讲中，我们将讨论弱监督和弱监督学习的概念。弱监督是指在没有完全标注数据的情况下进行学习，而是使用弱标注来源，如简单的启发式规则、知识库或低质量的 crowdsourcing。与人类标注相比，弱标注更便宜，但它们也存在噪声，意味着标注中存在错误。如果直接训练神经网络使用弱标注数据，模型倾向于记住噪声而不是泛化。弱监督学习是一种训练算法，旨在在标注噪声的情况下 robustly 训练神经网络，使训练模型仍然能够泛化良好。

最近的研究表明，虽然弱监督学习方法可以训练出在干净测试集上表现良好的模型，但有一个关键问题：这些方法假设存在额外的干净验证集用于模型选择。然而，这在实际应用中并不总是成立，因为额外的手工标注数据的需求经常被忽视。演讲中提出了三个研究问题：1. 干净验证数据是否对于弱监督学习至关重要？2. 如果需要干净数据，我们需要多少干净样本？3. 我们应该只使用干净样本进行验证，还是有其他更好的利用方式？

研究结果表明，弱监督学习方法确实需要干净标注数据才能正常工作，否则性能会显著下降。此外，增加干净验证样本的数量有助于提高弱监督学习方法的性能。然而，直接在干净数据上进行微调甚至可以超越弱监督学习方法。最后，通过允许在干净验证样本上继续微调，可以轻松实现之前弱监督学习方法声称的性能改进。因此，尽管弱监督学习方法看起来具有优势，但它们的实际性能和实用性被高估了。演讲建议未来的工作应包括报告模型选择标准，将弱监督方法与全监督基线进行比较，并考虑在弱监督学习中使用持续微调作为简单而强大的 baseline。</sample>
    <sample id="306">The speaker discusses the challenges of training language models for state tracking tasks, highlighting that these models struggle to learn even with direct supervision. This suggests the importance of pre-training in such scenarios. However, it is unclear whether the observed state tracking capabilities generalize beyond the specific setup. The speaker mentions that there are more results and analyses, including GPT-4 experiments, available in their paper, which can be accessed through the provided archive link. They also invite further discussion or inquiries via email or Twitter.</sample>
    <sample id="307">作者使用了几个评估指标来比较他们的模型，包括命名实体识别、分类、分词和问答。这些任务与六个基线模型进行了比较，以确定他们的模型在任务上的表现如何。</sample>
    <sample id="308">本研究旨在通过比较注释者、模型和数据集的预测与标签，来表征自然语言处理（NLP）设计偏见。我们专注于在标注数据时存在的系统性差异，这些差异可能影响模型性能和公平性。我们开发了一种框架，通过将注释者的观点与模型和数据集进行比较，来识别这些偏见。我们使用了Lab in the Wild等在线众包平台，从全球参与者中收集高质量数据，以确保多样性和代表性。我们的研究结果表明，数据集和模型往往更倾向于反映英语国家和受过高等教育的人群的观点。例如，在GPD4社会接受度分析中，数据集和模型最接近于英语国家和受过大学或更高教育水平的人群。然而，这种对特定群体的偏好可能导致其他群体被边缘化。为了应对这些偏见，我们提出了几项建议：1.在整个研究过程中记录所有相关的设计选择；2.从不同视角进行NLP研究；3.构建针对特定社区的专门化数据集和模型。我们强调，包容性NLP不仅是为了让技术适用于每个人，而且是为了确保技术能够公平地服务所有群体。</sample>
    <sample id="309">使用了内部注释者一致性来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择的领域是维基百科。</sample>
    <sample id="311">根据提供的英文内容，无法确定论文作者所属的机构。文本中没有提到任何机构名称或标识符。</sample>
    <sample id="312">根据所给的英文内容，MultiInstruct 与基准不同之处在于它显著提高了零-shot 学习任务上的性能，并且通过从自然语言数据集中学习来降低模型的敏感性。此外，MultiInstruct 还提出了一个大规模多模态调优数据集，显著提高了零-shot 可用性，并探索了不同的迁移学习技术及其益处。</sample>
    <sample id="313">根据所给的英文内容，这篇论文有三位作者：Sarah E. Finch, James D. Finch, 和 Jinho D. Choi。</sample>
    <sample id="314">二进制协调的定义是，第一个连词是整个协调结构的头部。</sample>
    <sample id="315">根据所给的英文内容，无法确定提示语的平均长度。</sample>
    <sample id="316">这些发现表明，较小的 T5 模型可以生成高质量的脚本，与大多数大型语言模型相当。这表明，通过使用适当的训练数据集，较小的模型可以支持大型模型，从而降低部署成本并提高可访问性。</sample>
    <sample id="317">The presentation introduces a research work titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors." It explains that information extraction is a fundamental task in natural language processing, involving the extraction of structured information from unstructured text. Common tasks include named entity recognition and relation extraction. The presenter highlights the challenges faced by previous models using pre-trained language models like T5 and GPT-3, which operate in a test-to-test manner during pre-training, leading to mismatched outputs between inference and pre-training stages.

To address this issue, the proposed method, CodeIE, transforms the test-to-structure information extraction task into a structure-to-structure code generation task. This approach uses large language models like CodeT5 to generate structured outputs directly from unstructured inputs, ensuring alignment between input and output stages. For named entity recognition, a function is defined to extract entities from the input text, and for relation extraction, similar prompts are designed.

The evaluation results on the NER and relation extraction datasets show that the proposed approach significantly outperforms traditional baseline models such as UIE and pre-trained language models like GPT-3. The analysis reveals that the perplexity computed on test format inputs using models like T5 is generally higher than that of Code format inputs using models like CodeT5, indicating better performance with the proposed method. Additionally, the use of Code format prompts reduces structural errors and improves recall, especially when using the CodeT5 model.</sample>
    <sample id="318">从我们对这些评估结果的分析中，我们发现ABC行为标签在内注释者一致性方面比现有方法更可靠，这是通过在100个双重标注对话上进行的内注释者一致性测量得出的。NatChos的一个版本，一个4GB子集的NatChos，ShuBERT的第一个版本，一个从临床笔记中提取的4GB句子的临床模型，以及ShuBERT的一个最终版本，一个混合了NatChos的4GB子集和临床笔记的4GB的模型。此外，我们引入了三个基于不同预训练策略训练的模型：一个基于Camembert权重在NatChos的4GB子集上训练，另一个基于Camembert权重在临床笔记的4GB上训练，第三个基于English biomedical模型Bert在NatChos的4GB子集上训练。总共，我们有7个模型。为了评估我们的7个模型，我们使用了公共和私有Dontims任务，如命名实体识别、分类、片段提取和问答。这些模型与6个基线模型（Camembert Oscar 138GB、Camembert Oscar 4GB、Camembert CCNet 4GB、Bert by Albert和Clinical Bert）进行了比较。这些模型的评估表明，模型在与训练数据相同性质的数据上表现最佳。然而，我们可以观察到，来自异质性来源的数据似乎更具适应性。我们还观察到，使用更多数据转化为更好的性能。总的来说，从头开始预训练似乎在大多数任务上取得了更高的性能。然而，我们实验中的持续预训练，使用PemBERT的权重和标记器在NatChos的4GB子集上训练，显示出与从头开始预训练的DrBERT 4GB相比相当的结果。这并不是使用Camembert权重和标记器的模型的情况，该模型由于稳定性问题而表现不佳。最后，我们的系统在11个Dontims任务中提供了更好的性能，并且在所有任务上超过了生成模型Camembert。我们还观察到，专业化的数据更好，但不具有可扩展性。我们从NatChos获得的预训练模型是免费可用的，并且在Yugabyte和我们的GitLab仓库上提供所有的训练脚本。谢谢您的观看，我们期待在Toronto的后站会议中与您交流。</sample>
    <sample id="319">论文研究了不同的学习策略，包括从头开始预训练和持续预训练。持续预训练涉及使用预训练模型（如BERT）的权重和参数，但使用特定领域的数据集进行微调，而不是从头开始重新训练整个模型。这种策略旨在利用预训练模型已有的知识，并通过在特定领域数据上进行微调来增强其性能。</sample>
    <sample id="320">根据所提供的英文内容，由于测试重复使用而导致的过拟合因素并不明显。这可以通过图表右侧的红色最佳拟合线的斜率来证明，该斜率大于1，表明在Conll 2003上每单位改进在Conll Plus Plus上转化为超过一个单位的改进，这意味着没有减弱的回报。</sample>
    <sample id="321">根据所给的英文内容，简化质量是通过分析句子对的类型来评估的。例如， Bible文本被发现比新闻文本或语言学习文本更加强化。此外，还观察了不同简化变换的多样性，如在DePlain API语料库中存在更多的重排和词替换，在DePlain Web语料库中存在更多的改写。</sample>
    <sample id="322">演讲者Enrico Liscic在ACL 2023上讨论了文本分类器如何学习道德问题。他首先解释了人类道德的概念，指出它是一个内在的指南，帮助我们区分对错，并是社会的基础。然而，当前处理道德性的方法通常将其简化为一个简单的尺度，从不道德到道德，这可能导致对道德性的误解。演讲者强调了不同文化和社会背景下道德的主观性，以及使用平均值或多数派来判断道德性的危险。他提到了“道德基础理论”，该理论认为人类有五种不同的道德基础，如公正性和权威性，这些基础以不同的方式被优先考虑，从而影响道德判断。演讲者还介绍了他们使用可解释的人工智能技术研究语言模型如何理解道德性的方法，特别是通过分析不同领域的推特数据集（如#AllLivesMatter和#BlackLivesMatter）。他们发现，尽管这些领域覆盖相似的主题，但它们对道德元素如反抗的表达方式存在显著差异。例如，在#AllLivesMatter中，反抗与暴力和混乱相关联，而在#BlackLivesMatter中则受到鼓励。这一发现强调了在不同领域使用单一模型可能导致对道德性的误解，突出了需要更细致地理解道德表达的必要性。</sample>
    <sample id="323">The paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering" by Yujie Wang, Hu Zhang, Jiye Liang, and Ru Li from the School of Computer and Information Technology, Shanxi University, China, presents a method to improve commonsense question answering (CSQA) tasks. CSQA is challenging because it requires models to answer questions that rely on common knowledge, which necessitates retrieving relevant knowledge from external sources. The authors propose a method called DHKG that builds a heterogeneous knowledge graph (HKG) based on a mutable knowledge base using a two-stage training strategy and knowledge distillation. They encode the subgraph and text in KG-Lite to enhance interaction between language models and KGs. The method dynamically removes entities with weak relevance to QA contexts based on attention weights, updates entity and relation embeddings, and incorporates path information into the QA context. Experiments on the OpenBookQA dataset show that their method achieves good results compared to other methods like LM and KG-based approaches.</sample>
    <sample id="324">根据所提供的内容，语言模型确实显示出不同的政治偏见。通过使用政治问题集对语言模型进行测试，研究者发现这些模型在政治光谱上分布于四个象限。GPT-4被确定为最自由的语言模型，而GPT系列通常比BERT系列更自由。此外，通过在不同政治倾向的新闻和社交媒体数据上进一步预训练语言模型，研究者观察到模型的政治偏见会相应地发生变化。例如，进一步微调BERT模型以适应左翼新闻语料库导致了其政治偏见向左翼的显著转变。</sample>
    <sample id="325">今天我要向大家简要介绍我们关于在没有树的情况下通过多集标签和潜在排列进行组合泛化的论文。这是与我的导师亚历山大·科勒和伊万·提托夫合作完成的。组合泛化可以理解为学习者处理深度递归和看不见的组合的能力，这些组合在训练期间已经单独出现过。在语义解析的背景下，测试组合泛化可能看起来像这样：就像往常一样，我们有一个训练数据集，其中包含短语及其对应的逻辑形式，这些逻辑形式代表它们的核心意义。与标准机器学习评估不同，测试数据集不来自相同的分布，而是包含结构上看不见的逻辑形式。在这个例子中，模型在训练期间只看到浅层递归，但在测试时被测试了一个具有更深层次递归的示例。朴素的序列到序列模型在处理这种离分布泛化时会遇到困难，经常产生与输入脱节的输出。特别是，它们往往无法复制输入和输出之间的系统对应关系，如图中的彩色代码所示。一种常见的方法是将树融入模型中。树旨在捕捉将短语与逻辑形式相关的组合过程。这种方法效果很好，但树通常不是给定的，需要通过某种方式获取。这可能会变得复杂，有时是一个计算昂贵的过程。通常，这涉及大量形式化特定的预处理，例如处理变量符号。获取树也可能涉及专门的语法 induction 过程。在这篇论文中，我们没有使用树，并引入了一个新的序列到序列模型，该模型直接建模了输入和输出片段之间的对应关系。我们首次展示了在不依赖树的情况下对更深层次递归进行强有力的泛化。我们的方法通过两个步骤预测输出：首先，我们为输入中的每个标记分配一个无序的标记集，其中包含将在输出中出现的标记。在第一个步骤之后，我们拥有正确的标记，但它们未排序。这就是为什么在第二个步骤中，我们使用另一个模型来预测一个排列，以将它们放入正确的顺序。我们引入了一种新方法来预测排列，该方法对可能的排列没有硬性约束，这使我们的方法变得灵活且富有表现力。概念上，我们的排列模型的工作原理如下：我们从左到右遍历输出，并确定将放置在输出位置的标记集合中的哪个标记。对于第一个输出位置，我们简单地选择一个标记，如红色所示。然后我们跳转到下一个标记集合，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳转到另一个标记集合。我们继续这个过程，直到第一个阶段的所有标记都被访问一次。为了给你实验结果的一个 teaser，这里我们比较了我们的方法与其他树less模型在Kogs基准上的表现。我们的模型在更深层次递归的泛化方面明显优于其他模型。然而，某些类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技術挑战。首先，输入和输出之间的对齐在训练数据中并不给定。因此，对于给定的标记，我们不知道它来自哪个标记集，这为训练带来了挑战。此外，有时存在多个与数据一致的排列，但语言上正确的排列是潜在的。我们通过将对齐作为训练的一部分解决这个问题。我们的排列方法非常灵活，但它带来了找到得分最高的排列是NP-hard的问题。这是因为这与旅行商问题有关。我们通过使用GP友好的连续放松来近似这个问题，这还允许我们在解决方案中反向传播并学习更有可能的语言排列。如果你想了解我们实验的更多细节以及我们如何解决这些挑战，请查阅我们的论文或海报。</sample>
    <sample id="326">认知失调是指一个人同时持有两种相互矛盾的信念或行为。</sample>
    <sample id="327">The presentation introduces a new multimodal architecture called "Manager Tower" for vision-language representation learning. It aims to address the limitations of existing models by aggregating insights from pre-trained unimodal experts at different levels and adaptively exploiting them in cross-modal layers. The Manager Tower uses managers to gather and combine insights, allowing for more effective utilization of unimodal semantic knowledge. This results in superior performance on various downstream tasks, especially in the Visual Question Answering (VQA2) task, with a 79.15% accuracy improvement over Bridge Tower. The presentation also discusses the visualization of average aggregation weights of textual and visual managers, highlighting their distinct trends and the effectiveness of adaptive managers in exploiting different levels of unimodal semantic knowledge.</sample>
    <sample id="328">根据所给的英文内容，GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="329">The video discusses the development of a method for generating structured pseudo-labels to improve noise resistance in zero-shot video sentence localization. The approach involves using even temporal structure and reducing the influence of noise through sample reweighting and label refinement, which results in better zero-shot performance on two datasets. The authors emphasize the importance of this research in enhancing the accuracy and reliability of video localization systems.</sample>
    <sample id="330">在主动学习时，累积训练与迭代训练相比，累积训练在累积数据方面表现得更有效。累积方法会收集所有从主动注释中获得的数据，而迭代方法则在每次注释后仅使用最新收集的数据来更新模型。因此，在累积策略下，累积训练可以利用之前的所有注释数据，从而可能提高模型性能。</sample>
    <sample id="331">演讲者的名字是Sarah Papi。</sample>
    <sample id="332">MuDa 基准中的数据是从 TED Talks 的转录中获得的，这些转录已经被翻译成英语以外的14种不同语言。</sample>
    <sample id="333">The paper presents a novel training framework called INK that injects knowledge into the nearest neighbor machine translation (NNMT) model to enhance its generalization and performance. The key idea of INK is to smooth predictions according to the nearest neighbors in the representation space by building a key-value data store to save representations and their corresponding target tokens. At each decoding step, the NNMT model retrieves the nearest entries from the data store and refines the prediction probability accordingly. However, this approach has two significant drawbacks: retrieving neighbors from a large data store at each decoding step is time-consuming, and once the data store is constructed, representations cannot be easily updated. To overcome these drawbacks, INK proposes a training loop with two steps: first, key knowledge is extracted from the data store to guide the adapter to adjust the representation, then updated representations are used to refresh the data store asynchronously. This training loop is run until convergence. Specifically, the representation is adjusted by aligning three kinds of representations using Kullback-Leibler divergence: contextualized representation and token embeddings, contextualized representations and key token embeddings, and contextualized representations of the same target token. Overall, INK optimizes the adapter with a combined learning objective and runs the training loop until convergence. In experiments, INK achieves an average gain of 1.99 BLEU score and 1.00 BLEU score compared with the state-of-the-art KNN system, and also achieves better translation performance with less memory space and faster inference speed.</sample>
    <sample id="335">演讲者的名字是Cui Yuhan。</sample>
    <sample id="336">跨语言转移是指使用一种语言训练的模型在另一种语言中进行评估或应用的过程。在演示中，跨语言转移被用来测试模型在不同语言之间的泛化能力，例如将一个语言的查询翻译成另一种语言，然后使用训练好的模型来预测结果。</sample>
    <sample id="337">The presentation introduces a neural approach for handling out-of-vocabulary (OOV) words in embedding-based downstream models. It highlights the difficulty of representing OOV words and their critical role in model performance. The approach involves constructing a word relationship graph that captures lexical rules of word formation and association. When an OOV word appears, it is decomposed into word pieces and associated with relevant words, forming a two-level graph around the OOV word. Each word or word piece acts as a node in the graph, with its corresponding word embedding serving as the node feature. The first layer retains complete word piece information, while the second layer samples nodes to mitigate noise from word piece nodes with numerous neighbors. A graph neural network processes the word relationship graph, using a self-attention mechanism to assign attributes based on the characteristics of the OOV words. Two levels of graph attention networks are applied to aggregate and fuse the initial input with the hidden embedding of each layer, resulting in a node-level representation. A readout block is used to capture the whole graph information and summarize word formation. The model aims to mimic the vector space of background embedding models by applying contrastive learning in the loss function. Positive samples from the graph, such as true homophones or synonyms of the OOV word itself, are selected and compared with the background embedding to encourage similarity while pushing them apart from other samples in the batch. Extensive experiments demonstrate the effectiveness of the model in both intrinsic and extrinsic tasks, providing insights into the applicability of the model to other languages.</sample>
    <sample id="338">The speaker, Bin Chen, introduces a collaborative research project titled "Our Human Explanations Are Always Helpful Towards Objective Evaluation of Human Natural Language Explanations" from researchers at Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The presentation covers the motivation, related works, and contributions divided into three sections: unified structure, preliminary experiments, and evaluation of five datasets using two models. Researchers have used human annotators, including crowdsourcing and experts, to annotate both labels and natural language explanations for training models to generate human-understandable explanations and improve prediction performance and reasoning ability. However, evaluating the quality of human-annotated explanations is challenging due to their subjective and task-dependent nature. Traditional metrics like BLEU and ROUGE treat human annotations as gold standards and focus on word similarity, while the explainability score measures baseline performance changes when explanations are present or absent but neglects task differences and the utility of explanations during fine-tuning and inference stages. To address these limitations, the researchers propose a new evaluation metric called TRU, which extends the explainability score by evaluating the helpfulness of explanations during fine-tuning. The study evaluates human-explanations across five datasets using a unified structure with both TRU and explainability scores on two models, T5 and BART, demonstrating that human-annotated explanations can still benefit model predictions even if considered low-quality by humans in previous literature. The results support the hypothesis that the helpfulness of human explanations to models depends heavily on the task and explanation format.</sample>
    <sample id="339">根据所给的英文内容，这篇论文的作者属于萨尔兰德大学。</sample>
    <sample id="340">Guan Hao Huang, a researcher from the University of California, Los Angeles (UCLA), is presenting a study on a large-scale syntactically diverse paraphrase dataset called ParaAMR. This dataset was created through an AMR back-translation process, which involves using AMR graphs to generate diverse paraphrases. The study aims to address the challenge of obtaining high-quality, large-scale paraphrase data for natural language processing (NLP) applications such as question answering and chatbots.

The presentation highlights the limitations of existing datasets like MRP, Penn, and Cora, which have high quality but are limited in scale. Additionally, while automatically generated datasets like back-translation can produce a large number of paraphrases, they often lack syntactic diversity. To overcome these challenges, the researchers propose using AMR back-translation to generate syntactically diverse paraphrases.

The process involves using a pre-trained AMR parser to create AMR graphs from source sentences, changing the focus node randomly, modifying corresponding edges and their labels, and then using an AMR graph-to-text generator to produce the paraphrased text. This method ensures that the generated paraphrases share the same semantic meaning but have different syntactic structures.

The study demonstrates that ParaAMR contains around 15 million source sentences with approximately 6.9 paraphrases per source sentence. Comparative analysis shows that ParaAMR generates more syntactically diverse paraphrases compared to other datasets generated by back-translation, while maintaining good semantic similarity.

The presentation also discusses the benefits of ParaAMR in various NLP applications, including learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for future learning. The results indicate that models trained on ParaAMR perform better than those trained on other datasets in tasks such as sentence embeddings and paraphrase generation.

Overall, the study contributes to the development of high-quality, large-scale paraphrase datasets that can improve the robustness and performance of NLP systems. The dataset is available for further research and application.</sample>
    <sample id="341">作者使用了翻译质量、平均延迟和计算延迟时间来评估他们的策略。</sample>
    <sample id="342">The presentation introduces a large-scale personalized dialogue dataset, LiveChat, constructed from live streaming. Conducted by researchers from Shanghai Jiao Tong University and Xiaobing.ai, the dataset aims to address existing challenges in open-domain dialogue datasets, such as limited scale and lack of diverse dialogues. LiveChat is unique in its video source and detailed persona annotations, offering a more realistic representation of real spoken conversations. The presentation outlines three main steps for constructing LiveChat: scraping original streaming videos, extracting and transcribing audio, and collecting persona information through a reply-to-match method. Experimental results show that LiveChat outperforms existing datasets in terms of response modeling and dialogue recognition, particularly when using extracted persona profiles and average session per persona. The presentation concludes with plans to further investigate the transferability of language models on LiveChat.</sample>
    <sample id="343">Hello everyone. I'm Mackshatta, and today my co-author Martin and I are presenting our work, "KitMoS: Evaluating Knowledge Integration from Multiple Sources." This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-trained time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, "John saw the newly elected president on TV," pre-trained parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-trained time and inference time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset: "Servin is a judge. Kier is a baker. Servin and Kier met at a park. After a long day at work, deciding cases in a law court, he was happy to relax." The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information: first, entity-specific knowledge, such as "Servin is a judge"; and second, background knowledge, such as "judges decide cases in law courts." Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information, such that it may either be found in a single source or in multiple sources. We have defined three settings of KitMoS. First, we have the "background pre-train" setting where background knowledge is assumed to be available at pre-training time. Second, there's the "background both" setting where background knowledge is available both at pre-training time and inference time. Lastly, the "background inference" setting where both knowledge types are available only at inference time. This last setting is especially interesting, since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-training data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources. In the "background pre-train" setting, we assume that the background knowledge, "politicians seek elected seats in government," is contained in the pre-trained parameters. In the inference-time context, we provide the entity-specific knowledge, "Chester is a politician." In the "background both" setting, we additionally provide not only entity-specific, but also background knowledge about politicians in the inference-time context. In the "background inference" setting, we provide the fictional occupation, "miritura," instead of "politician," because miritura is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the "background pre-train" setting. Without task-specific training on KitMoS, both models do not perform well. When trained on KitMoS, however, both C2F and BERT for Coref perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KitMoS where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="344">基于树的方法的缺点包括需要获取和构建树，这可能是一个复杂且计算密集的过程。这通常涉及大量形式化预处理，例如处理可变符号，并且可能需要专门的语法 induction 程序。</sample>
    <sample id="345">在我们的论文中，我们解决了几个有趣的技術挑戰。首先，輸入和輸出之間的對應關係並未在訓練數據中給出。因此，對於給定的詞元，我們不知道它來自於哪個多樣性集合，這對訓練提出了挑戰。此外，有些排列與數據一致，但語言正確的排列是隱藏的。我們通過將對應關係作為訓練的一部分來解決這個問題。我們的方法非常靈活，但它帶來了一個挑戰：找到最高分排列是一個NP-hard問題，因為這與旅行 salesman 問題有關。我們通過使用GP-friendly的持續放鬆來近似這個問題，這還允許我們通過解案學習更語言上可接受的排列。如果您想了解我們實驗的更多細節以及我們如何解決這些挑戰，請查看我們的論文或參加我們的展示。</sample>
    <sample id="346">根据幻灯片显示，论文的作者Shuheng Liu和Alan Ritter来自乔治亚理工学院的互动计算学院。</sample>
    <sample id="347">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky Stanford Engineering Computer Science Hi, I'm Myra, and today we'll be talking about our paper Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models. This work is done in collaboration with Eser Durmus and Dan Jurafsky. In recent years, many have documented the prevalence of social bias and stereotypes in large language models (LLMs). However, these measures have various limitations. They usually rely on hand-constructed datasets that are very time-consuming to curate, and they also usually only measure very specific stereotypes, meaning that they don't generalize well to other demographics or contexts, or they simply capture a very general broad associations like negative associations with particular groups. Furthermore, most work in this space doesn't account for intersectionality, which is the notion that multifaceted social identities can compound biases and be unique low-sight of harm. To overcome these limitations, we rely on the property that these newer instruction-tuned LLMs are very good at responding to instructions and prompts. So we can ask the model to generate a persona, which is a depiction of an imagined individual using a prompt like "imagine you are an Asian woman, describe yourself." And we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt. So here are some example generations from GPT-4. Immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words, there are some interesting patterns. The Asian woman is depicted as unassuming, the Middle Eastern woman is referred to using words like exotic and like referring to a mesmerizing region, and both of the women of color personas make references to ancestry while the white man persona has nothing of the sort. To capture these patterns, our method has two parts. The first one is generating these personas. Our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects, they were also able to surface racial stereotypes. And also this enables direct comparison between our generated personas and the human-written responses. The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones, which I'll elaborate on shortly. The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon. So the marked words method draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from that default is linguistically marked. So for instance, the word man, or sorry, the word warrior is usually associated with men. So when people are describing a warrior who is a woman, they'll usually actually specify one man warrior and mark the term with woman. And more broadly, dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked. So in our method, we first designate what the unmarked and marked groups are, and then we compare the personas using the fighting words method, which is basically using weighted log odds ratios to distinguish the top words for each marked group. So for instance, for the personas of Black women, we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups. Now, some results. So first we use a lexicon of stereotypes, and we find that the generated personas contain a lot more stereotypes than the human-written ones. However, when we actually look at the distribution of the words in the lexicon, we find very different things. So while the generated personas have much higher rates of the lexicon words, the human-written ones have a much wider distribution of words, while the stereotype words that are in the generated personas are really just the words tall and athletic so really just only the positive or at least non-negative ones. And in fact, this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all. So instead, to do that, we'll turn to the results from our marked words method to show how these seemingly positive portrayals facilitate stereotypes and essentializing narratives. In our analysis, we reveal how these seemingly positive portrayals reflect harmful patterns. First, for mark groups, the top words include things like culture, tradition, proud, and exotic. And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like vibrant and curvaceous, which connect to a trope of tropicalism. For Asian women, the words are things like petite and delicate and silky, which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive and so on. And finally, for Black women, we see that some of the top words are things like strong and resilient. This connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on these people to overcome them, which leads to very negative health outcomes for these people among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should as researchers be addressing positive stereotypes and essentializing narratives. We should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods because for instance, like these positive stereotypes, we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these prinnicious patterns. We just really can't make any assumptions or really study that further without more transparency. Thank you so much for listening. Have a good time at ACL.</sample>
    <sample id="348">该研究探讨了大型语言模型（LLMs）中存在社会偏见和刻板印象的问题。作者Myra Cheng、Esin Durmus和Dan Jurafsky合作开发了一种新方法，通过使用自然语言提示来测量这些偏见。他们利用LLMs对指令的响应能力，生成不同身份标记的个人描述，如“想象一下你是一个亚洲女性，描述你自己。”这种方法可以轻松地适用于任何人口统计群体。

研究发现，虽然生成的描述通常不具有明显负面或有毒内容，但存在一些有趣且潜在有害的模式。例如，亚洲女性被描绘为不引人注目，中东女性被描述为异国情调，而女性的种族化描述则涉及 ancestry。作者通过“标记词”方法识别这些模式，该方法利用社会语言学中的“标记性”概念，即社会上未被标记的群体是默认状态，而那些与之不同的群体则被标记。通过比较生成的描述和人类写的描述，研究者能够识别出刻板印象和刻板印象的特定词汇。

结果表明，生成的描述包含更多刻板印象，而人类写的描述则具有更广泛的词汇范围。然而，这些积极的描述反映了有害的刻板印象和本质化叙事，如文化传统、外貌特征和刻板印象的女性角色。研究建议，模型所有者应关注积极刻板印象和本质化叙事，采用交叉学科方法研究偏见及其危害，并增加对偏见缓解方法透明度的讨论。</sample>
    <sample id="349">自然语言理解通常需要在推理时提供的知识。目标嵌入和原始嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入等于目标嵌入。复制验证是检测另一个服务背后模型是否包含水印的过程。我们首先构建一个后门数据集和一个 benign 数据集。后门数据集包含所有单词都属于触发集的句子，而 benign 数据集中的所有单词都不属于触发集。然后，提供者从存储服务请求 benign 数据集的嵌入。我们计算请求的嵌入和目标嵌入之间的余弦和L2相似性。我们还计算 benign 和后门数据集之间的相似性差异，定义为δcosine和δL2。同时，我们应用卡方检验并使用其p值作为第三个度量。我们在四个数据集（AGNews、Mnli、SST-2和Yahoo!）上进行了实验。我们假设提供者使用Wikipedia数据集来计算词频。四个数据集上的结果表明，我们的嵌入标记具有很好的检测性能，同时保持了很好的实用价值。我们还通过可视化每个句子中触发器的数量来验证提供的嵌入的可转换性，使用了句子嵌入的PCA。图例表示每个句子中触发器的数量。如图所示，很难区分后门嵌入和正常嵌入。谢谢，我们将讨论这个问题。</sample>
    <sample id="350">我们介绍了一项跨引用分辨率任务，旨在评估AI系统在不同来源中提取知识的能力。我们使用了包含人类研究参与者和已建立跨引用分辨率模型的数据集进行评估。通过分析基准测试，我们发现跨引用分辨率的准确率在不同任务之间差异显著，有些甚至低至每小时3.6美元或更低。此外，我们注意到关于注释员的信息经常缺失，如他们的人数、来源和文化背景等。没有这些信息，关于超人表现的主张缺乏科学意义。因此，我们建议避免基于不完整数据集进行人类与系统比较，并强调在构建更可靠的基准测试时考虑注释员的影响。</sample>
    <sample id="351">The presentation, titled "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?" by Shuheng Liu and Alan Ritter from the School of Interactive Computing at Georgia Institute of Technology, explores the generalization problem in named entity recognition (NER) using the CoNLL-2003 dataset. The researchers investigate whether models trained on CoNLL-2003 can generalize to modern data and identify factors that contribute to poor generalization. They developed the CoNLL++ dataset, derived from Reuters-News 2020, annotated with CoNLL-2003 guidelines, and fine-tuned over 20 models on CoNLL-2003, evaluating them on both CoNLL-2003 and CoNLL++. The study found that three main ingredients are essential for good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models generally perform better, larger models lead to better generalization, and more fine-tuning examples improve performance. The researchers also tested two hypotheses for performance drops: adaptive overfitting and temporal drift. Adaptive overfitting was not observed, as indicated by a red best-fit line with a gradient greater than one, showing no diminishing returns. However, temporal drift was confirmed, as retraining models with more recent data led to performance degradation with larger temporal gaps. The conclusion is that for effective generalization, a combination of better model architecture, larger model size, and more fine-tuning examples is necessary. The study also highlights that the performance drop is primarily due to temporal drift rather than adaptive overfitting, despite CoNLL-2003 being used for over 20 years. The paper concludes with a call for further research to improve the generalization of NER models and encourages readers to explore the dataset and contact the authors for any questions.</sample>
    <sample id="352">ABC-Eval代表一种评估对话模型行为的维度，旨在减少人类评估的主观性。它通过明确标注每个模型响应是否表现出某些行为，如提供不相关的信息或与自己或对话伙伴矛盾，来实现这一点。</sample>
    <sample id="353">在预训练设置中，我们假设背景知识“政治家寻求 elected seats in government”包含在预训练参数中。在微调过程中，我们提供特定知识“杰里·特朗普是一个政治家”。我们生成代码，预期在微调模型时会得到更好的结果。然而，由于任务挑战性大，排名前五的预测未包括参考的SQAs，导致预测结果与地面真实值有较大差异。我们还分析了微调后模型的性能，并发现一些关键操作的正确性提高了代码生成。我们提供了预测示例，展示了训练后的模型对SQAs的预测接近地面真实值，但存在轻微差异。我们感谢您的收听，并邀请您查看我们的论文和代码，期待您的反馈。</sample>
    <sample id="354">根据所提供的英文内容，直到2023年，CoNLL-2003和CoNLL++之间的性能增量才高于5个百分点。</sample>
    <sample id="355">标题：迁移学习和主动学习在检测认知不一致中的应用：解决罕见类别挑战

演讲者：瓦沙杜哈·瓦拉达拉扬（Vasuda Varadarajan）

介绍：

尊敬的观众们，我叫瓦沙杜哈·瓦拉达拉扬，是纽约布鲁克大学计算机科学系的博士候选人。我很高兴能在这里展示我们提交到ACLL2023会议的论文《迁移学习用于认知不一致检测：解决罕见类别挑战》。

首先，我们将定义认知不一致，并解释为什么研究它在语言中很重要。简单来说，认知不一致是指两个相互矛盾的信念或行为。例如，一个人说：“我知道吸烟会致命，但我会在会议后抽几支烟。”这种信念和行为之间的不一致就是认知不一致。进一步说明，这个人接着说：“我觉得没有它们，我无法保住工作。”这说明了第二个行为的合理化，它们之间存在一种关系。认知不一致是一种常见的现象，在日常决策中经常遇到，但在其他类型的 discourse 关系中也可能出现。那么，为什么这很重要呢？研究认知不一致可以帮助我们理解人们之间的分歧，跟踪人群中的信仰、价值观和态度变化。高认知不一致还与焦虑障碍有关，有助于更好地了解人们的心理健康。研究语言中表达的认知不一致也有助于理解极端主义和边缘化群体的 polarization。最后，认知不一致对于理解个人的认知风格和决策过程至关重要。

为了实现创建认知不一致资源的目标，我们进行了大规模的认知不一致关系标注。我们采用了认知不一致的第一个方法，如流程图所示。推特数据使用了 pyT 语法解析器进行解析，然后根据指南对 discourse 单位对进行标注。正如这里所见，认知不一致只在标注的 discourse 单位对中找到了 3.5%。在收集了大约 1000 个 discourse 单位对样本后，我们训练了一个初始分类器，仅基于 43 个标注的认知不一致样本。 unsurprisingly，分类器的表现并没有比随机猜测好多少。鉴于认知不一致的低发生率和缺乏任何先验数据集，我们面临的是绝对罕见性的问题。为了解决这个问题，我们实验了迁移学习和主动学习的组合，以标注更多的认知不一致样本，同时减少标注成本，从而提高认知不一致检测能力。由于初始模型根本无法捕捉到认知不一致类别，我们从迁移学习开始，通过将权重从与认知不一致相关的任务中转移过来。我们从两个不同的任务中转移权重：独立主题的 debate 论证分类任务，该任务确定两个来自不同人的 debate 陈述是否一致或不一致，无论主题如何；以及扩展和比较的二元分类任务，这些任务与 discourse 的一致性和不一致性概念密切相关。我们发现，从标注数据集中标注的零-shot 性能已经远远超过了随机猜测，最好的 AUC 为 0.62。进一步通过在两个任务上迭代微调，我们发现先在扩展和比较任务上微调，然后在 debate 论证任务上进一步微调，可以达到更好的零-shot 性能。因此，我们使用这个模型作为主动学习的起点。接下来，我们确定了更新模型以收集每轮标注新数据的最佳方法。累积方法将所有标注的数据收集起来，而迭代方法则通过在最新标注数据上训练模型来更新模型。在所有策略中，我们发现累积方法在总体上表现得更好或相当。为了增加认知不一致样本的数量，我们使用概率罕见类别策略（PRC）来选择最有可能被标注为认知不一致的例子。我们将其与其他社区中常用的先进策略进行了比较。我们发现，提出的 PRC 策略比其他先进策略效果更好，尽管差异较小。请注意，随机标注策略的表现明显较低。在后续的标注轮次中，我们进一步提高了认知不一致分类的 AUC 到 0.75，这是目前在任务上的最佳表现。我们还检查了每种策略的可标注性和成本。我们发现，PRC 策略具有最高的认知不一致样本比例，并且在罕见类别方面效果最好。然而，标注员发现这些例子很难标注。总之，我们发现 PRC 是一个简单的标注策略，用于罕见类别获取，并且与适当设计的迁移学习任务结合时，显著提高了性能。我们还发现，迭代更新对于从不同领域迁移学习非常有用，因为跨领域标注标注更有利于累积更新。以下是我们的标注数据集和论文的链接。如果您有任何问题，请随时联系我们。谢谢。</sample>
    <sample id="356">根据幻灯片上显示的标志和名称，论文的作者来自几个机构，包括信息学、NLP中心、Sarland、UvA和阿姆斯特丹大学。</sample>
    <sample id="357">演讲者的名字是Siyu Yuan。</sample>
    <sample id="358">根据幻灯片显示，这篇论文有五位作者：Patrick Fernandes、Kayo Yin、Emmy Liu、André F. T. Martins 和 Graham Neubig。</sample>
    <sample id="359">该方法与专门用于实时翻译的 state-of-the-art 架构进行了比较。</sample>
    <sample id="361">演讲者介绍了一种名为CounterComp的方法，旨在通过使用反事实场景来提高多步数量推理的组合泛化能力。多步数量推理涉及回答需要执行多个算术运算的问题，如从2019年到2020年的收入变化。当前的自然语言模型在处理多步数量推理任务时表现不佳，尤其是当输出包含超过两步操作时，因为它们倾向于记忆特定的模式。CounterComp通过识别输入中与输出操作相关的有意义的标记来解决这个问题，从而帮助模型避免错误关联。通过将这些标记与输出操作相关联，CounterComp可以生成更准确和泛化的答案。</sample>
  </task>
</testset>