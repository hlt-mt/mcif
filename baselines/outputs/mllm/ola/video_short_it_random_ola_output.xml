<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici sono alimentati da grandi quantità di testo, che possono essere raccolti da diverse fonti, tra cui libri, articoli di giornale, postazioni sociali e più.</sample>
    <sample id="1">McGill University/Mila, Microsoft Research</sample>
    <sample id="2">Il contenuto in inglese è:

"DEPLAIN: A German Parallel Corpus with Intra lingual Translations into Plain Language for Sentence and Document Simplification
Regina Stodden, Omar Momen, Laura Kallmeyer
Heinrich Heine University Düsseldorf, Germany
ACL 2023"

La versione italiana sarebbe:

"DEPLAIN: Un corpus parallelo tedesco con traduzioni intra-lingue in linguaggio semplice per la semplificazione di frasi e documenti
Regina Stodden, Omar Momen, Laura Kallmeyer
Heinrich Heine University Düsseldorf, Germania
ACL 2023"</sample>
    <sample id="3">DEPLAIN: Un corpus parallelo tedesco con traduzioni intralingua in linguaggio semplice per la semplificazione della frase e del documento

Regina Stodden, Omar Momen, Laura Kallmeyer

Heinrich Heine University Düsseldorf, Germania

ACL 2023</sample>
    <sample id="4">Esempio di semplificazione del testo</sample>
    <sample id="5">Esempio di semplificazione del testo</sample>
    <sample id="6">Esempio di semplificazione del testo</sample>
    <sample id="7">Esempio di semplificazione del testo</sample>
    <sample id="8">2. DE-plain
A New Corpus</sample>
    <sample id="9">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013 2015 2017 2019 2021 Parallel Parallel Parallel Parallel Parallel Parallel</sample>
    <sample id="10">Il contenuto della presentazione è un diagramma che rappresenta i livelli di semplificazione del testo in Germano. Il diagramma è suddiviso in due sezioni principali: il livello di frase e il livello di parola.

La sezione di sinistra del diagramma rappresenta il livello di frase. Questa sezione include cinque barre colorate che rappresentano diverse tipologie di semplificazione del testo. Le tipologie di semplificazione del testo sono indicate da etichette sulla sinistra delle barre. Le etichette sono "Passive", "Tense", "Voice", "Aspect" e "Mood". Ogni barra ha un numero di frasi rappresentato da un punto di data sulla sinistra della barra.

La sezione di destra del diagramma rappresenta il livello di parola. Questa sezione include due barre colorate che rappresentano due tipologie di semplificazione del testo. Le tipologie di semplificazione del testo sono indicate da etichette sulla sinistra delle barre. Le etichette sono "Simplification" e "Digitalization". Ogni barra ha un numero di parole rappresentato da un punto di data sulla sinistra della barra.

Il diagramma include anche due tabelle che forniscono maggiori dettagli sulla semplificazione del testo. La tabella di sinistra fornisce i numeri di frasi per ciascuna tipologia di semplificazione del testo. La tabella di destra fornisce i numeri di parole per ciascuna tipologia di semplificazione del testo.

In generale, il diagramma fornisce una rappresentazione visiva dei livelli di semplificazione del testo in Germano.</sample>
    <sample id="11">German Text Simplification Corpora

Sentence Level

14000
12000
10000
8000
6000
4000
2000
0
Parallel (De-En, 2013) Translation-based (De-En, 2013) Statistical (De-En, 2013) Post-editing (De-En, 2013) Parallel (En-De, 2013) Translation-based (En-De, 2013) Statistical (En-De, 2013) Post-editing (En-De, 2013) Parallel (De-En, 2015) Translation-based (De-En, 2015) Statistical (De-En, 2015) Post-editing (De-En, 2015) Parallel (En-De, 2015) Translation-based (En-De, 2015) Statistical (En-De, 2015) Post-editing (En-De, 2015)

483
756</sample>
    <sample id="12">German Text Simplification Corpora

Sentence Level

14000
12000
10000
8000
6000
4000
2000
0
Parallel (De-En, 2013)
Translation-based (De-En, 2013)
Machine Translation (De-En, 2013)
Parallel (En-De, 2013)
Translation-based (En-De, 2013)
Machine Translation (En-De, 2013)
Parallel (De-En, 2015)
Translation-based (De-En, 2015)
Machine Translation (De-En, 2015)
Parallel (En-De, 2015)
Translation-based (En-De, 2015)
Machine Translation (En-De, 2015)

483
756</sample>
    <sample id="13">Il contenuto della presentazione è il seguente:

Titolo: "German Text Simplification Corpora"

Sottotitolo: "Livello Frasi"

Il grafico illustra i progressi nella semplificazione del testo in tedesco, suddivisi in diverse sezioni:

1. **Frasi**: Include statistiche sulla quantità di frasi analizzate e semplificate.
2. **Frasi Semplificate**: Mostra il numero di frasi che sono state riconosciute come frasi semplificate.
3. **Frasi Semplificate da Modello**: Illustra il numero di frasi che sono state riconosciute come frasi semplificate dal modello utilizzato.
4. **Frasi Semplificate da Umano**: Mostra il numero di frasi che sono state riconosciute come frasi semplificate umaniamente.
5. **Frasi Semplificate da Umano vs. Modello**: Confronta il numero di frasi semplificate umaniamente con quello del modello.

Il grafico include anche una legenda per colori diversi rappresentanti diverse tipologie di frasi:

- **Verde**: Frasi
- **Azzurro**: Frasi Semplificate
- **Giallo**: Frasi Semplificate da Modello
- **Viola**: Frasi Semplificate da Umano

Il grafico è suddiviso in due sezioni principali:

1. **Frasi**: Include statistiche sulla quantità di frasi analizzate e semplificate.
2. **Frasi Semplificate**: Mostra il numero di frasi che sono state riconosciute come frasi semplificate.

Il grafico fornisce un'analisi dettagliata delle prestazioni del modello di semplificazione del testo in tedesco, confrontando i risultati umani con quelli del modello.</sample>
    <sample id="14">Simplification Types 10 8 6 4 2 news bible fiction Simplicity LexSimp StructSimp 10 8 6 4 2 news bible fiction L2 L2</sample>
    <sample id="15">Simplification Types

Simplification Types

Simplification Transformations

Simplification Transformations</sample>
    <sample id="16">Simplification Types</sample>
    <sample id="17">Simplification Types LexSimp Simplicity StructSimp Bible Fiction News n=187 n=172 n=185 n=173 Simplification Transformations DEPLAIN-apa DEPLAIN-web Reordering Rephrasing Lexical Substitution Word Addition Word Deletion</sample>
    <sample id="18">Simplification Types

Simplification Types

Simplification Transformations

Simplification Transformations</sample>
    <sample id="19">3. Use-cases Automatic alignment and simplification</sample>
    <sample id="20">Risultati dei metodi di al线ension con 1:1 (parte superiore) e n-num capabilities (parte inferiore)</sample>
    <sample id="21">Risultati dei metodi di al线ension con le capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="22">Risultati dei metodi di al线ension con capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="23">Risultati dei metodi di al線ment con 1:1 (parte superiore) e n-num capabilities (parte inferiore)</sample>
    <sample id="24">Risultati dei metodi di al線ment con 1:1 (parte superiore) e n:n capacità (parte inferiore)</sample>
    <sample id="25">Risultati dei metodi di al线ension con capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="26">Risultati dei metodi di al线ension con capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="27">Risultati relativi alla semplificazione del testo utilizzando il modello finetuned long-mBART. I risultati corrispondono alla lunghezza dei dati di addestramento.</sample>
    <sample id="28">Risultati relativi alla semplificazione del testo utilizzando il modello finetuned long-mBART. Le performance corrispondono alla lunghezza dei dati di addestramento.</sample>
    <sample id="29">Risultati relativi a Simplice automatico utilizzando mBART addestrato</sample>
    <sample id="30">Risultati relativi alla semplificazione del testo utilizzando il mBART addestrato. Il risultato corrisponde alla lunghezza dei dati di addestramento.</sample>
    <sample id="31">Risultati relativi alla semplificazione del testo utilizzando mBART finetunato. I risultati corrispondono alla lunghezza dei dati di addestramento.</sample>
    <sample id="32">Risultati relativi alla semplificazione del testo utilizzando il mBART addestrato. Le performance corrispondono alla lunghezza dei dati di addestramento.</sample>
    <sample id="33">Risultati relativi alla semplificazione del testo utilizzando il modello mBART addestrato. Le performance sono indicate in termini di SARI, BLEU e BS-P. I valori sono confrontati con quelli di un modello baseline e con i risultati ottenuti utilizzando la tecnica DEPLAN.</sample>
    <sample id="34">Grazie. Per maggiori dettagli, si prega di consultare il nostro articolo. E non esitate a visitare il nostro stand al Congresso ACL 2023.</sample>
    <sample id="35">Patrick Fernandes, Kayo Yin, Emmy Liu, Andre F. T. Martins, Graham Neubig</sample>
    <sample id="36">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il T5 XL.</sample>
    <sample id="37">No, i tagger CoNLL-2003 non funzionano più.</sample>
    <sample id="38">The proposed human evaluation method is novel because it involves a chat interface where participants rate the relevance of the bot's responses.</sample>
    <sample id="39">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla qualità dei relativi etichettori.</sample>
    <sample id="40">I progressi che possono essere fatti per migliorare il punteggio sono: 1) Creare un modello di testo che impara da una vasta gamma di testi, inclusi libri, articoli e articoli di blog. Questo modello dovrebbe essere in grado di comprendere le idee principali e i dettagli di un testo, anche se non è stato specificato direttamente. 2) Creare un modello di testo che impara da una vasta gamma di testi, inclusi libri, articoli e articoli di blog. Questo modello dovrebbe essere in grado di comprendere le idee principali e i dettagli di un testo, anche se non è stato specificato direttamente.</sample>
    <sample id="41">5</sample>
    <sample id="42">Congiunti lunghe in inglese, minimizzazione della lunghezza della dipendenza e struttura di coordinazione</sample>
    <sample id="43">Struttura di dipendenze della coordinazione</sample>
    <sample id="44">Struttura di dipendenze della coordinazione</sample>
    <sample id="45">Struttura di dipendenze della coordinazione</sample>
    <sample id="46">Struttura di dipendenze della coordinazione</sample>
    <sample id="47">Struttura di dipendenze della coordinazione</sample>
    <sample id="48">Struttura di dipendenze della coordinazione</sample>
    <sample id="49">Struttura di dipendenze della coordinazione</sample>
    <sample id="50">Struttura di dipendenze della coordinazione</sample>
    <sample id="51">Ordine delle parole tende a minimizzare le lunghezze della dipendenza:</sample>
    <sample id="52">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="53">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="54">Ordine delle parole tende a minimizzare le lunghezze dei vincoli:</sample>
    <sample id="55">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="56">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="57">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="58">Ordine delle parole tende a minimizzare le lunghezze dei vincoli:</sample>
    <sample id="59">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="60">Ordine delle parole tende a minimizzare le lunghezze dei vincoli:</sample>
    <sample id="61">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="62">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="63">Statistiche sulla coordinazione estratte da una versione migliorata della banca di testi Penn (Marcus et al. 1993; Ficler e Ficler 2016) • I连词似乎倾向于比之前更短（osservato）、这个趋势随着长度差异而增长 • (solo notato in Gibson et al. 1996: 88-90) • ma solo quando il governatore è sulla sinistra o assente (vedo Bart e Lisa: Homer è venuto e schiaffeggiato) • non quando è sulla destra (Ted e Ned ridevano)</sample>
    <sample id="64">Statistiche sulla coordinazione estratte da una versione aggiornata del Penn Treebank (Marcus et al. 1993; Ficler e Goldberg 2016).</sample>
    <sample id="65">Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank (Marcus et al. 1993; Ficler e Goldberg 2016): • le congiunzioni tendono a essere più brevi di quanto fosse prima (osservato); • questa tendenza aumenta con la differenza di lunghezza (osservato); • accorciamento noto in Gibson et al. 1996: 88-90) • ma solo quando il governante è sulla sinistra o manca (vedi: "Ho visto Bart e Lisa" - "siamo venuti e schiacciati"; • non quando è sulla destra ("Ted e Ned hanno riso")</sample>
    <sample id="66">Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank (Marcus et al. 1993; Ficler e Goldberg 2016): • le congiunzioni tendono a essere più brevi rispetto a quanto osservato precedentemente, α; • questa tendenza aumenta con la differenza di lunghezza; • (solo rapidamente notato in Gibson et al. 1996: 88-90) • ma solo quando il governante è sulla sinistra o assente (ho visto Bart e Lisa: Homer è venuto e ha schernito); • non quando è sulla destra (Ted e Ned hanno riso).</sample>
    <sample id="67">Statistiche sulla coordinazione estratte da una versione aggiornata del Penn Treebank (Marcus et al. 1993; Ficler e Goldberg 2016): • le congiunzioni sinistra tendono a essere più brevi rispetto a quanto osservato prima), • questa tendenza aumenta con la differenza di lunghezza • (solo noto in Gibson et al. 1996: 88-90).</sample>
    <sample id="68">Statistiche sulla coordinazione estratte da una versione aggiornata del Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 1996): • le congiunzioni sinistra tendono a essere più brevi rispetto a quanto osservato prima), • questa tendenza aumenta con la differenza di lunghezza • ma solo quando il governante è sulla sinistra o assente (ho visto Bart e Lisa; Homer è venuto e si è schiacciato) • non quando è sulla destra (Ted e Ned hanno riso)</sample>
    <sample id="69">Statistiche sulla coordinazione estratte da una versione a colori del Penn Treebank (Marcus et al. 1993; Flicker e Goldberg 2016): • le congiunzioni sinistra tendono a essere più brevi rispetto a quanto osservato prima, • questa tendenza aumenta con la differenza di lunghezza • ma solo quando il governante è sulla sinistra o assente (ho visto Bart e Lisa; Homer è venuto e ha schernito) • non quando è sulla destra (Ted e Ned hanno ridacchiato)</sample>
    <sample id="70">Il grafico illustra la percentuale di concordanze più brevi in base alla differenza assoluta delle lunghezze dei concordi. I grafici superiori rappresentano le percentuali di concordanze più brevi per i concordi sinistra e destre, rispettivamente, in funzione della differenza assoluta delle loro lunghezze. I grafici inferiori rappresentano le percentuali di concordanze più brevi per i concordi sinistra e destre, rispettivamente, in funzione della differenza assoluta delle loro lunghezze con bande di confidenza.</sample>
    <sample id="71">Il grafico illustra la distribuzione delle proporcioni di concordanze più brevi in base alla differenza assoluta tra le lunghezze dei concordati. I grafici sono suddivisi in due gruppi: "NO governor in CHARACTERS" e "NO governor in STYLABLES", con ulteriori suddivisioni in "Governor on the LEFT (length in CHARACTERS)", "Governor on the LEFT (length in STYLABLES)", "Governor on the RIGHT (length in CHARACTERS)" e "Governor on the RIGHT (length in STYLABLES)". Ognuno di questi gruppi rappresenta una distribuzione diversa della lunghezza dei concordati, analizzata in modo statistico.</sample>
    <sample id="72">Figura 1: Proportions of shorter conjuncts depending on the absolute difference in conjunct lengths (with confidence bands)</sample>
    <sample id="73">Il grafico illustra la relazione tra le differenze di lunghezze dei concorsi e la proporcione di concorsi più corti. I grafici superiori raffigurano i governanti sinistra, mentre i grafici inferiori rappresentano i governanti destra. I grafici sinistri mostano una relazione lineare tra la differenza di lunghezze dei concorsi e la proporcione di concorsi più corti, mentre i grafici destri non rivelano alcuna relazione significativa. Questi risultati suggeriscono che i governanti sinistri tendono a utilizzare concorsi più corti rispetto ai governanti destra.</sample>
    <sample id="74">Vedi il articolo per l'argomento completo!</sample>
    <sample id="75">Per rispondere alla domanda, analizziamo la sezione di testo fornita nell'immagine. L'articolo è intitolato "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" e include i nomi dei tre autori principali: Matthias Lindemann, Alexander Koller e Ivan Titov. 

Passo 1: Identificare il titolo dell'articolo.
Passo 2: Estrarre i nomi degli autori dalla sezione di testo.
Passo 3: Contare il numero di nomi distinti.

Risultato: Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="76">The domains that appear to be simplified are news, bible, and fiction.</sample>
    <sample id="77">Margie read this absolutely fascinating book about bees yesterday.</sample>
    <sample id="78">Sì, i modelli DrBERT sono liberi di essere usati per la ricerca.</sample>
    <sample id="79">The answer is Plain academic papers.</sample>
    <sample id="80">Per una buona generalizzazione, è necessario: 1. Miglior architettura del modello 2. Dimensione più grande del modello 3. Più esempi di addestramento</sample>
    <sample id="81">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in base alla differenza di lunghezze tra i congiunti a sinistra e quelli a destra.</sample>
    <sample id="82">Gli esperimenti sono stati progettati per testare l'effetto della posizione del governatore in una frase. I ricercatori hanno creato due gruppi di campione: un gruppo in cui il governatore era posizionato sulla sinistra e un altro gruppo in cui il governatore era posizionato sulla destra. Hanno quindi analizzato i risultati per vedere se c'era una differenza significativa nella comprensione o nella interpretazione delle frasi in base alla posizione del governatore.</sample>
    <sample id="83">Un classificatore base addestrato su un insieme di dati non bilanciato non è migliore del caso casuale.</sample>
    <sample id="84">4</sample>
    <sample id="85">The names of the characters in the example conversation are not provided in the image.</sample>
    <sample id="86">I modelli di traduzione automatica sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in quanto sono più sensibili ai fenomeni del discorso che richiedono un contesto per essere correttamente interpretati. Questi fenomeni possono includere la presenza di neologismi, espressioni figurate, frasi idiomatiche e altre strutture linguistiche che hanno un significato diverso quando considerate in contesto o quando vengono tradotte letteralmente.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, MIT, Meta AI</sample>
    <sample id="122">Il framework quantifica esattamente la posizionalità utilizzando la correlazione di Pearson.</sample>
    <sample id="155">Lo studio precedente ha dimostrato che i soggetti umani hanno risposto in modo simile quando sono stati forniti gli stessi prompt di persona.</sample>
    <sample id="156">I fonti di dati utilizzati in questo studio sono i libri e le riviste.</sample>
    <sample id="157">Per rispondere alla domanda, analizziamo la copertina dell'articolo in rilievo. L'immagine indica che l'articolo è intitolato "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination". Gli autori sono elencati come "Adam Przpherdowski and Michał Wozniak". 

Ragionando passo dopo passo:

1. Identifichiamo il titolo dell'articolo.
2. Cerchiamo i nomi degli autori elencati immediatamente sotto il titolo.
3. Contiamo i nomi di persona elencati.

In base a queste informazioni, ci sono due autori coinvolti nell'articolo: Adam Przpherdowski e Michał Wozniak.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono il debate e il contrasto.</sample>
    <sample id="159">2</sample>
    <sample id="160">Ci sono sette autori che hanno collaborato all'articolo.</sample>
    <sample id="161">Il framework differisce dai precedenti in quanto utilizza un modello di previsione per analizzare le annotazioni e le informazioni demografiche.</sample>
    <sample id="162">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è la configurazione GPT-4.</sample>
    <sample id="163">I sistemi commerciali che sono stati messi a confronto sono GPT-2 e GPT-3.</sample>
    <sample id="164">Da i dati di pretraining dei modelli alla lingua: tracciando le tracce dei bias che portano a modelli NLP non equiassistanti</sample>
    <sample id="165">LM Training Data A mixed blessing 100 1000 10000 100000 1000000 1000000000 1000000000000 1000000000000000 1000000000000000000 1000000000000000000000 1000000000000000000000000 1000000000000000000000000000 1000000000000000000000000000000 1000000000000000000000000000000000 1000000000000000000000000000000000000 10000000000000000000000000000000000000000 10000000000000000000000000000000000000000000 10000000000000000000000000000000000000000000000 10000000000000000000000000000000000000000000000000 10000000000000000000000000000000000000000000000000000 10000000000000000000000000000000000000000000000000000000 100000000000000000000000000000000000000000000000000000000000 100000000000000000000000000000000000000000000000000000000000000 100000000000000000000000000000000000000000000000000000000000000000</sample>
    <sample id="166">LM Training Data A mixed blessing 100 101 102 103 104 105 106 107 patreon.com www.wikipedia.org www.mercadomagico.com www.globo.com www.theguardian.com www.huffpost.com www.scribd.com www.bloomberg.com www.vice.com www.theverge.com www.investopedia.com www.npr.org www.britannica.com www.sciencedaily.com www.zillow.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www.mercadomagico.com www</sample>
    <sample id="167">LM Training Data A mixed blessing 10^20 10^18 10^16 10^14 10^12 10^10 10^8 10^6 10^4 10^2 10^0 10^-2 10^-4 10^-6 10^-8 10^-10 10^-12 10^-14 10^-16 10^-18 10^-20 patreon.com en.wikipedia.org www.github.com www.myspace.com www.theguardian.com www.nytimes.com www.huffpost.com www.scribd.com www.bloomberg.com www.springer.com www.amazon.com www.instructure.com www.npr.org www.kaggle.com www.zhihu.com www.bu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.cmu.edu www.c</sample>
    <sample id="168">LM Training Data A mixed blessing 10^200 10^190 10^180 10^170 10^160 10^150 10^140 10^130 10^120 10^110 10^100 10^90 10^80 10^70 10^60 10^50 10^40 10^30 10^20 10^10 10^0 10^-10 10^-20 10^-30 10^-40 10^-50 10^-60 10^-70 10^-80 10^-90 10^-100 10^-110 10^-120 10^-130 10^-140 10^-150 10^-160 10^-170 10^-180 10^-190 10^-200</sample>
    <sample id="169">Per questo scopo, i modelli di linguaggio vengono prima pre-addestrati su un vasto insieme di dati e quindi applicati a diverse compiti specifici.</sample>
    <sample id="170">Per questo scopo

I dati di pretraining

Modello linguistico

Attività downstream

Come valutare i modelli di apprendimento di LMs?</sample>
    <sample id="171">A questo scopo, i modelli di linguaggio vengono prima addestrati su grandi quantità di dati e quindi utilizzati per svolgere diverse mansioni specifiche. Questo processo è noto come apprendimento supervisionato.</sample>
    <sample id="172">Evaluating LM Political Leaning Support both encoder and decoder LMs “statements” I &lt;mask&gt; with this statement. “Do you agree or disagree with this statement? &lt;statement&gt;” Automatic eval Grounded in polisci lit Language Model Prompted Response Political Leaning Response Left Agreed Right Disagreed Libertarian Authoritarian</sample>
    <sample id="173">Esistenti LM</sample>
    <sample id="174">Esistenti LM</sample>
    <sample id="175">Pre-training Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media left center right Article Comparison For Ideology Prediction and Stance Shen, Qin, &amp;amp; Carind Rose. "What sounds 'right' to me? experiential factors in the perception of political stances." Journal of Language and Linguistics. Asian Volume 2022. Social Media Reddit left center right</sample>
    <sample id="176">Pre-training Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media Social Media Reddit left center right left center right</sample>
    <sample id="177">Risultati Partisan shifts in LM political leaning RoBERTa GPT-2 reddit original news news Center Left Right reddit news original</sample>
    <sample id="178">Risultati Partisan shifts in LM political leaning RoBERTa GPT-2 original reddit news reddit news Center Left Right</sample>
    <sample id="179">Risultati Partisan shifts in LM political leaning RoBERTa GPT-2 reddit news original Left Center Right reddit news reddit news original</sample>
    <sample id="180">The Trump Card Pre-45th to post-45 shift news left news center news right reddit left reddit center reddit right ∆w = (-2.75, -1.24) ∆w = (-0.13, 1.03) ∆w = (-1.63, 1.03) ∆w = (0.75, -3.64) ∆w = (-0.50, -3.64) ∆w = (-1.75, 1.03) ∆w = (-2.37, -0.51) ∆w = (-0.12, 1.28) ∆w = (-2.15, -1.03) ∆w = (-1.75, -1.03) ∆w = (-0.37, 0.92) ∆w = (-1.00, 1.03)</sample>
    <sample id="181">The Trump Card Pre-45th to post-45 shift news left news center news right reddit left reddit center reddit right</sample>
    <sample id="182">The Trump Card Pre-45th to post-45th shift</sample>
    <sample id="183">Performance per Categoria

1°
2°
3°

Hate Speech
BLACK
MUSLIM
LGBT+
Jews
ASAIN
LATINX
WOMEN
CHRISTIAN
MEN
WHITE

Minisformazione
HP (L)
NYT
CNN (L)
NPR (L)
Guard (L)
Fox (R)
WExE (R)
BBRT (R)
WAT (R)
NR (R)

Tabella 4: Indica il discorso su odio che mira a gruppi di identità diversi e la minisformazione proveniente da diverse fonti. I colori diversi rappresentano le performance migliori in giallo e le performance peggiori in blu scuro.</sample>
    <sample id="184">Performance per Categoria

1°
2°
3°

NEWS_LEFT
REDIDT_LEFT
REDIDT_RIGHT

BLACK
MUSLIM
LGBT+
JWS
ASAIN
LATINX
WOMEN
CHRISTIAN
MEN
WHITE

HP (L)
NYT
CNN (L)
NPR (L)
Guard (L)
Fox
WExE (R)
BBAT (R)
WAT
NR

80.63
80.94
80.20
80.20
80.19
80.85
80.15
87.82
87.65
83.69
86.22
80.03
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.</sample>
    <sample id="185">Performance per Categoria

1° 2° 3°
NEWS LEFT REDDIT LEFT REDDIT RIGHT BLACK MUSLIM LGBTQ+ JEWS ASIAN LATINX WOMEN CHRISTIAN MEN WHITE
80.63 89.84 80.03 90.19 89.85 89.89 89.09 89.09 89.09 90.66 91.15 87.82 87.65 87.65 86.22
80.63 89.84 80.03 90.19 89.85 89.89 89.09 89.09 89.09 90.66 91.15 87.82 87.65 87.65 86.22
Minisinformation HP (L) NYT (L) CNN (L) NPR (L) Guard (L) Fox (R) WaEx (R) BBaT (R) WAT (R) NR (R)
88.72 85.78 85.44 84.82 84.44 90.66 96.63 90.43 90.18 90.18 84.44</sample>
    <sample id="186">Performance per Categoria

1°
2°
3°

NEWS LEFT
RED/DIST. LEFT
RED/DIST. RIGHT
MINSTRELLO
HP (L)
NYT
CNN (L)
CNN (R)
NPR (L)
NPR (R)
Guard (L)
Fox (L)
WAE (L)
BBRT (R)
WAT (R)
NR

BLACK
MUSLIM
LGBT+
JWS
ASAIN
LATINX
WOMEN
CHRISTIAN
MEN
WHITE

80,63
89,84
80,03
80,19
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89,85
89</sample>
    <sample id="187">Performance per Categoria

1° 2°
NEWS LEFT REDDIT LEFT REDDIT RIGHT BLACK MUSLIM LGBTQ+ JEWS ASIAN LATINX WOMEN CHRISTIAN MEN WHITE
80.63 89.44 80.20 90.19 85.83 82.63 89.89 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09 89.09</sample>
    <sample id="188">Performance per Categoria

1°
2°
3°

NEWS LEFT
REDUCTION LEFT
REDUCTION RIGHT
MINIFICATION

HP (L)
NYT
CNN (L)
NPR (L)
Guard (L)
Fox (R)
WExE (R)
BBAT (R)
WAT (R)
NR

Table 4: Effetto dell'attenuazione del discorso razzista su gruppi di identità diversi e minificazione da diverse fonti. I colori marrone chiaro rappresentano i migliori risultati, mentre i colori blu scuro rappresentano i peggiori.</sample>
    <sample id="189">Performance per Categoria

1°
2°
3°

NEWS_LEFT
REDU Left
REDU Right
MINSTREL
HP (L)
NYT
CNN (L)
CNN (R)
NPR (L)
Guard (L)
Fox (R)
WExE (R)
BRBAT (R)
WAT (R)
NR (L)

Table 4: Effetto sulla discriminazione di gruppi di identità diversi e minstrelsy da diverse fonti. I colori marrone chiaro denotano i migliori risultati, mentre i colori blu scuro denotano i peggiori.</sample>
    <sample id="190">Qualitativa Analisi Text Target Label Base N-L S-L N-R S-R the all right is corrupted with people supporting asian True True True True False a new nation a new nation and priest have come from them They Right True False False False True True (–) ... that McDonald's stop Donald Trump is ceasing to sit ing the country was in the throes of a crime pagings trail that the country was in the throes of a crime because they know I will stop the slaughter going on Donald Trump August 20, 2016 incredible that water is in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in</sample>
    <sample id="191">Qualitativa Analisi Text Target Label Base N-L S-L N-R S-R the all right is corrupted with people supporting with a new nationaism and people praying to common. They were McDonald's stop donald trump from seizing them. Trump said that the crime pagin trail was in the throws of a crime trump because they know I was the slaughter going on Donald Trump August 20, 2016 incredible. That water has been in flight in flut. We are paying in Burlington Vermont for clean water. Examples of the downstream performance of tasks using language models with varying political bias, bias, base, N-L represent Christians, vanilla RoBERTa model, news, social media, left-leaning, and right-leaning, respectively.</sample>
    <sample id="192">Table 12: Qualitative analysis of hate speech examples. Hate is defined as any statement that is directed towards a group or individual based on their race, ethnicity, religion, gender, sexual orientation, age, disability, or national origin.</sample>
    <sample id="193">Table 12: Qualitative analysis of hate speech examples from LM with different political biases to differ</sample>
    <sample id="194">Table 12: Qualitative analysis of hate speech examples from LM with different political backgrounds.</sample>
    <sample id="195">Sostituisce le esempi di testo di abuso con testi di abuso.</sample>
    <sample id="196">Discussione Tra Scylla e Charybdis Per “sanzionare” o non a “sostituire”, è la questione Pre-training dei dati Modelli linguistici Attività downstream</sample>
    <sample id="197">Discussione Tra Scylla e Charybdis Se "iniziazione" o non "sostegno", è la domanda Pre-training dei dati Linguaggi modelli Downstream task</sample>
    <sample id="198">Discussione Tra Scylla e Charybdis Selezionare o non selezionare, questa è la questione Pre-training dei dati Modelli di linguaggio Attività downstream</sample>
    <sample id="199">Grazie!</sample>
    <sample id="200">C'è un articolo con cinque autori.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite fino a 900 token di lunghezza del contesto.</sample>
    <sample id="202">I domini inclusi nel set di dati sono: Easy on Me, I Gotta Feeling.</sample>
    <sample id="203">La posizionalità si riferisce alla capacità di un ricercatore di mettere in relazione i propri punti di vista personali e le proprie esperienze con la ricerca che sta svolgendo.</sample>
    <sample id="204">Dawei Zhu</sample>
    <sample id="205">Yes, EDAtt adapts an existing offline ST model.</sample>
    <sample id="206">C'è un totale di quattro autori coinvolti nell'articolo.</sample>
    <sample id="207">Il modello testato funziona sulla suite di test.</sample>
    <sample id="208">The three variants of KITMUS are Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">Come usare le sampli pulite più efficientemente?</sample>
    <sample id="211">La sensibilità della metrica è la capacità di un modello di essere sensibile a una varietà di istruzioni per la stessa task.</sample>
    <sample id="212">La relatrice o il relatore non è specificato in questa immagine.</sample>
    <sample id="213">The answer is worse.</sample>
    <sample id="214">I modelli linguistiche vengono addestrati su un vasto contesto linguistico.</sample>
    <sample id="215">In genere, i modelli migliori hanno bisogno di circa 20 campioni di convalida puliti per raggiungere buone prestazioni in WSL.</sample>
    <sample id="216">The affiliations of the authors are Stanford University and the University of California, Berkeley.</sample>
    <sample id="217">Perché i metodi esistenti non sono sufficienti per misurare i bias dell'informazione.</sample>
    <sample id="218">The answer is Jackie CK Cheung.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici ha un aspetto quantitativo.</sample>
    <sample id="220">No, sembra che il processo di semplificazione sia lo stesso per DEplain-apa e web.</sample>
    <sample id="221">No, Coscript non è pubblicamente disponibile.</sample>
    <sample id="222">La filigrana viene inserita esattamente nel testo definendo un insieme di trigger, conteggiano il numero di trigger in una frase e aggiunge il segnaposto di destinazione sull'embedding originale.</sample>
    <sample id="223">I fornitori di servizi cloud per la traduzione automatica sono il PennState e l'Amazon.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è come fare un pasticcino alla fragola.</sample>
    <sample id="226">Gli autori si accertano della segretezza del proprio metodo utilizzando tecniche di embedding per visualizzare i dati.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire uno nuovo in modo continuo.</sample>
    <sample id="228">GPT-4 is least aligned with the Confucian country.</sample>
    <sample id="229">La relatrice illustra come il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione in una frase di esempio.</sample>
    <sample id="230">La quantità di attività ha un impatto sulla performance del modello.</sample>
    <sample id="231">The authors compare their method with three reference approaches: LSTM seq2seq, Zhang and Lapata, and PP recursion.</sample>
    <sample id="232">I due coautori sono in relazione con il primo autore come collaboratori.</sample>
    <sample id="233">The first author of PaLM is Chowdery et al.</sample>
    <sample id="234">La presentazione è intitolata "NLPositionality: Caratterizzazione dei bias di progettazione dei set di dati e dei modelli". L'immagine in basso della presentazione include le foto e i nomi di cinque persone. 

1. Sebastin Sany, Università di Washington
2. Jenny T. Liang, Carnegie Mellon University
3. Ronan Le Bras, Allen Institute for AI
4. Katharina Reinecke, Università di Washington
5. Maarten Sap, Carnegie Mellon University</sample>
    <sample id="235">Posizionalità NL: Caratterizzazione dei bias di progettazione dei set di dati e dei modelli</sample>
    <sample id="236">Immagina...</sample>
    <sample id="237">Immagina...</sample>
    <sample id="238">Imagina... Carl Jones Aditya Sharma Tech Lead, New York Times Tech Lead, Times of India = Punteggio di perpectivaAPI</sample>
    <sample id="239">Immagina... Design bias example! Carl Jones Aditya Sharma Tech Lead, New York Times Tech Lead, Times of India Pressstitutes everywhere on the news? (0.82) (0.33)</sample>
    <sample id="240">[1] Savin-Baden, Maggi, and Claire Howell-Major. “Qualitative research: The essential guide to theory and practice.” Qualitative Research: The Essential Guide to Theory and Practice (2013).</sample>
    <sample id="241">La posizionalità

"Le prospettive che le persone hanno come risultato delle loro demografiche, identità e esperienze."

[1] Savin-Baden, Maggi, and Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Routledge (2013).</sample>
    <sample id="242">La posizionalità si riferisce alla prospettiva che le persone hanno come risultato delle loro demografiche, identità e esperienze.</sample>
    <sample id="243">Gli articoli di ricerca scientifici sono spesso organizzati in sezioni, tra cui le sezioni di riferimento. Questa slide ha due sezioni di riferimento. La prima è una sezione di riferimento con un titolo. La seconda è una sezione di riferimento con un titolo e con due articoli di ricerca scientifici elencati.</sample>
    <sample id="244">I dataset e i modelli hanno una posizionalità?</sample>
    <sample id="245">I datasets e i modelli hanno una posizionalità?</sample>
    <sample id="246">I datasets e i modelli hanno una posizione?</sample>
    <sample id="247">I dataset e i modelli hanno una posizione?</sample>
    <sample id="248">Gli insiemi di dati e i modelli hanno posizionalità?</sample>
    <sample id="249">Domanda: I set di dati e i modelli hanno una posizione?</sample>
    <sample id="250">NLPosizionabilità
Un framework per caratterizzare i bias di progettazione in insiemi di dati e modelli NLP</sample>
    <sample id="251">Il diagramma illustra il processo di creazione di un dataset per un modello di apprendimento macchine. Inizialmente, si seleziona un subset del dataset e si applicano predizioni modellistiche per determinare le etichette iniziali. Queste etichette vengono quindi valutate e corrette da un team di annotatori umani, che fornisce etichette più accurate. Infine, il modello viene addestrato utilizzando queste etichette corrette. Il diagramma include anche una sezione dedicata alla separazione demografica, che fornisce informazioni sulla distribuzione delle etichette tra diverse gruppi demografici.</sample>
    <sample id="252">Il diagramma illustra il processo di creazione di un modello di machine learning per la classificazione di immagini. Il processo inizia con la raccolta di un dataset di immagini, che viene quindi elaborato utilizzando un modello di machine learning. Il modello utilizza informazioni come l'età, il genere e l'education del soggetto dell'immagine per prevedere la classe dell'immagine. Le annotazioni ricevute dal modello vengono quindi utilizzate per migliorare il modello, rendendolo più preciso nel riconoscimento delle immagini.</sample>
    <sample id="253">Il diagramma rappresenta un framework per la raccolta e il processamento di dataset in un contesto di machine learning o analisi dei dati. Il processo inizia con la raccolta di diverse istanze, ciascuna delle quali è rappresentata da un rettangolo con un colore diverso. Queste istanze sono quindi inviate a un modello predittivo, che utilizza diverse caratteristiche come età, genere, razza, istruzione e paese per fare previsioni.

Dopo che il modello ha fatto le sue previsioni, riceve annotazioni da diversi fornitori. Queste annotazioni sono rappresentate da colori diversi e vengono utilizzate per valutare la precisione del modello. Se le annotazioni sono coerenti, il modello viene aggiornato con queste informazioni. Altrimenti, il modello viene addestrato nuovamente utilizzando le annotazioni più recenti.

Infine, il modello viene utilizzato per fare previsioni su nuove istanze. Queste previsioni sono rappresentate da rettangoli con colori diversi, simili alle istanze raccolte inizialmente. Il diagramma illustra come il modello evolve e si adapta continuamente basandosi sulle annotazioni e le previsioni.</sample>
    <sample id="254">Il diagramma rappresenta un framework per la creazione di dataset in un contesto di machine learning. Il processo inizia con la raccolta di immagini, che vengono quindi analizzate da un modello pre-addestrato per fare predizioni su diverse caratteristiche come età, genere, razza e istruzione. Queste predizioni sono utilizzate per creare un dataset che include informazioni sull'origine geografica e sul reddito.

Ogni immagine è quindi inviata a diversi annotatori per ottenere annotazioni manuali. Queste annotazioni vengono confrontate con le predizioni del modello per valutare la precisione del modello. Se le annotazioni manuali sono coerenti tra loro, il modello viene aggiornato utilizzando queste annotazioni. Se le annotazioni manuali non sono coerenti, il modello viene aggiornato utilizzando solo le annotazioni manuali più coerenti.

Questo processo si ripete fino a quando non ci sono più differenze significative tra le annotazioni manuali e le predizioni del modello. In questo modo, il modello può essere adattato in modo più preciso alle caratteristiche specifiche dei dati.</sample>
    <sample id="255">Il diagramma illustra un framework per la raccolta e l'elaborazione di un dataset di immagini. Inizialmente, le immagini vengono raccolte e suddivise in due classi: "Eating with Hands" e "Using Utensils". Ogni istanza di immagine viene quindi analizzata per determinare se il soggetto sta mangiando con le mani o utilizzando utensili. Selezionando un'istanza specifica, ad esempio, "Red", si ottiene una finestra di visualizzazione dell'immagine correlata.

Successivamente, le annotazioni ricevute vengono analizzate e confrontate con i modelli di database per calcolare la correlazione Pearson's R. Questo processo include la raccolta di informazioni demografiche come età, genere, razza, grado accademico e paese di origine.

In sintesi, il framework consente di raccogliere, analizzare e confrontare diverse tipologie di immagini e informazioni demografiche per ottenere una migliore comprensione dei comportamenti alimentari e delle caratteristiche demografiche correlate.</sample>
    <sample id="256">Il diagramma rappresenta un framework per la raccolta e il processamento di un dataset. Inizialmente, le immagini vengono raccolte e suddivise in due classi: "Eating with Hands" e "It's manners". Ogni istanza di immagine è quindi analizzata per ottenere informazioni sulle etichette, come ad esempio l'età, il genere, la razza, lo stato accademico, il paese di origine, ecc. Queste informazioni sono utilizzate per creare un modello di previsione che prevede le etichette delle immagini.

Successivamente, le etichette ottenute dal modello vengono confrontate con le etichette gold del database. Questo confronto viene effettuato utilizzando la correlazione di Pearson per determinare quanto bene il modello di previsione coincide con le etichette gold del database. Questo processo aiuta a valutare la precisione del modello e a migliorarlo se necessario.</sample>
    <sample id="257">### Descrizione dell'Immagine

L'immagine rappresenta la homepage di un sito web chiamato "Lab in the Wild". Il sito è dedicato a esperimenti in campo di apprendimento automatico e ha una sezione per i ricercatori. L'immagine include anche un piccolo video in cornice di sinistra, in cui una persona parla.

#### Dettagli Sito Web:

1. **Logo e Titolo:**
   - Il logo del sito è un globo con un design minimalista.
   - Il titolo "Lab in the Wild" è posizionato al di sopra del logo.

2. **Navigation Bar:**
   - La barra di navigazione è posizionata all'alto della pagina e include le seguenti sezioni:
     - "Our Experiments"
     - "Findings &amp; Data Sets"
     - "Blog"
     - "For Researchers"
     - "About Us"
     - "English"

3. **Contesto del Progetto:**
   - Il testo principale sottolinea che il progetto è un esperimento in campo di apprendimento automatico e ha una sezione dedicata ai ricercatori.

4. **Partecipanti:**
   - Il sito ha 5,376 partecipanti.

5. **Sezioni Principali:**
   - **"Could you live with an AI in your mirror?":** Questa sezione propone un esperimento su come le persone reagiscono all'idea di avere un assistente intelligente nel proprio specchio.
   - **"Are you better at noticing hateful and show compared to non-hateful and show compared to non-hateful and show?":** Questa sezione propone un esperimento per valutare le capacità di riconoscimento delle persone tra contenuti violenti e non violenti.
   - **"Where are you on the techno-utopian scale?":** Questa sezione propone un esperimento per determinare il livello di ottimismo tecnologico di un individuo.

6. **Bottoni di Azione:**
   - C'è un bottone per "Participate now!" (Partecipa ora!) e un bottone per "Psychologists more info" (Più informazioni per i psicologi).

7. **Video:**
   - In cornice di sinistra, c'è un piccolo video in cui una persona parla.

### Analisi e Discussione

Il sito "Lab in the Wild" sembra essere un piattaforma di esperimenti in campo di apprendimento automatico, specializzata in ricerche e test con un pubblico di ricercatori. Le sezioni proposte suggeriscono un focus su diverse applicazioni dell'apprendimento automatico, dalla vita quotidiana (ad esempio, un assistente intelligente nel retrospecchietto) alla valutazione dei contenuti violenti e alla percezione della tecnologia.

La quantità di partecipanti (5,376) indica che il progetto ha attirato un pubblico considerabile, dimostrando un forte interesse in queste tematiche. I bottoni di azione forniti offrono un facile accesso per i partecipanti e i ricercatori a coinvolgersi nel progetto.

In sintesi, il sito rappresenta un esempio di come la tecnologia e la ricerca possono essere utilizzate per comprendere meglio le interazioni umano-macchina e per sviluppare applicazioni più avanzate e sensibili alle esigenze umane.</sample>
    <sample id="258">### Descrizione in Italiano

**Lab in the Wild**

- **Our Experiments**
- **Findings &amp; Data**
- **Blog**
- **About Us**
- **About Research**
- **About Lab in the Wild**

**Pool of diverse volunteers/research participants**

**Online experiment from researchers**

**Immagini:**

1. **"Could you live with an AI and its money?"**
   - Domanda: "Saresti felice se potessi vivere con un'intelligenza artificiale e i suoi soldi?"
   - Scelte:
     - "Sì, sarei felice"
     - "No, non sarei felice"
   - **Risultato:** 5,376,396 total participants

2. **"Are you better at noticing hateful speech?"**
   - Domanda: "Credo che sia meglio notificare il discorso abusivo"
   - Scelte:
     - "Sì"
     - "No"
   - **Risultato:** 5,376,396 total participants

3. **"Where are you on the techno-utopian scale?"**
   - Domanda: "Dove sei sulla scala techno-utopiana?"
   - Scelte:
     - "Sono molto a sinistra"
     - "Sono nel mezzo"
     - "Sono molto a destra"
   - **Risultato:** 5,376,396 total participants</sample>
    <sample id="259">**Attività A: Accettabilità Sociale**

1. **Leggi la situazione**: In questa parte dell'attività, dovrai leggere una storia o un scenario specifico. Questo ti aiuterà a capire meglio le circostanze in cui si trova il personaggio principale.

2. **Inserisci cosa pensi di lei**: Una volta che hai capito la situazione, dovrai fornire il tuo parere su come dovrebbe agire o reagire il personaggio principale. Puoi scegliere tra diverse opzioni predefinite o scrivere il proprio pensiero.

3. **Vedi cosa pensano gli altri**: Dopo aver espresso il tuo pensiero, verranno presentate le opinioni degli altri. Questo ti aiuterà a vedere come le altre persone percependo la storia o il personaggio principale.

**Il robot specialista:**
Il robot specialista è un assistente che ti aiuta a capire meglio la storia o il personaggio principale. Puoi chiedergli domande e ricevere risposte dettagliate per aiutarti a capire meglio la storia o il personaggio principale.

**Studio dei partecipanti degli Stati Uniti**:
Questo grafico rappresenta le opinioni degli altri partecipanti dello studio. Puoi vedere quanto gli altri pensano che dovrebbe agire o reagire il personaggio principale. Questo ti aiuterà a vedere come le altre persone percependo la storia o il personaggio principale.</sample>
    <sample id="260">### Descrizione dell'Immagine

L'immagine rappresenta un slide di presentazione con il titolo "Task A: Social Acceptability". La slide è suddivisa in due sezioni principali:

1. **Sezione sinistra**:
   - **Titolo**: "Task A: Social Acceptability"
   - **Contenuto**:
     - **Punti chiave**:
       1. **Read the situation**: Questo punto indica che il primo passo è leggere la situazione.
       2. **Enter what you think about it**: Questo punto indica che il secondo passo è esprimere opinioni personali sulla situazione.
       3. **See what others thought about it**: Questo punto indica che il terzo passo è vedere le opinioni degli altri.
     - **Illustrazione**: L'illustrazione include un diagramma con un robot che rappresenta un AI (Intelligenza Artificiale) che specializza in analisi di sentimenti. Il diagramma include una barra grafica che rappresenta le percentuali di risposte dei partecipanti, suddivise in:
       - "Strongly agree" (Sono completamente d'acordo)
       - "Agree" (Sono d'acordo)
       - "Neutral" (Neutro)
       - "Disagree" (Non sono d'acordo)
       - "Strongly disagree" (Sono completamente d'accordo)

2. **Sezione destra**:
   - **Contenuto**:
     - **Testo**: "Participants compare their responses to others' and an AI's."
     - **Illustrazione**: L'illustrazione include un video di una persona che sembra essere in un studio o un ambiente accademico.

### Analisi e Discussione

La slide si concentra su un task che coinvolge l'evaluatione della social acceptability di una certa situazione. Questo task è strutturato in tre fasi principali:

1. **Leggere la situazione**: Questo passo richiede ai partecipanti di comprendere completamente la circostanza o la situazione in questione. Questo è fondamentale per formare opinioni informate.

2. **Esprimere opinioni personali**: Dopo aver compreso la situazione, i partecipanti devono esprimere le loro opinioni su di essa. Questo può coinvolgere considerazioni sulle implicazioni, le convenienze e le disavvantaggi della situazione.

3. **Vedere le opinioni degli altri**: Questo passo è cruciale poiché consente ai partecipanti di confrontare le loro opinioni con quella di altri. Questo può fornire un contesto più ampio e può aiutare a identificare tendenze o pareri comuni.

Inoltre, la slide include un diagramma che rappresenta le percentuali delle risposte dei partecipanti. Questo diagramma è utilizzato per visualizzare come le opinioni degli altri partecipanti distribuiscono in ciascuna delle category di risposta. Questa visualizzazione è utile per identificare se ci sono accordi generali o contrasti.

La sezione di destra della slide menziona che i partecipanti possono confrontare le loro risposte con quelle degli altri e con le risposte dell'AI. Questo suggerisce che l'AI fornisce anche una risposta o un'opinione sulla situazione, consentendo ai partecipanti di vedere come la risposta dell'AI si confronta con le loro opinioni e con le opinioni degli altri.

In sintesi, questa slide rappresenta un task di valutazione della social acceptability che coinvolge una combinazione di lettura, espressione di opinioni e confronto con gli altri e con un AI.</sample>
    <sample id="261">**Task A: Accettabilità Sociale**

**Analisi**
- Datasets
  - Social Chemistry

**Modeli**
- Delphi
- GPT-4</sample>
    <sample id="262">### Descrizione dell'Immagine

L'immagine rappresenta un diagramma dettagliato di un esperimento relativo alla valutazione della tossicità del linguaggio. L'esperienza è strutturata in tre passi principali:

1. **Leggi l'esempio**:
   - Questo passo richiede che i partecipanti leggano un esempio specifico. In questo caso, l'esempio è: "English people smell like sour milk." Questo esempio è illustrato con un rettangolo blu.

2. **Inserisci il tuo pensamento**:
   - Dopo aver letto l'esempio, i partecipanti devono scegliere se ritengono che la frase sia o non sia offensiva. Questa scelta è rappresentata da due pulsanti: "Offensive" e "Not offensive". I partecipanti devono quindi fare clic su uno dei due pulsanti per registrare il loro pensamento.

3. **Vedi gli altri pensieri**:
   - Questo passo consente ai partecipanti di vedere le opinioni degli altri partecipanti. L'immagine illustra questa parte con un bar chart che rappresenta le percentuali di concordanza tra gli utenti. Il bar chart è colorato in verde per le percentuali di concordanza e in rosso per le percentuali di discordia.

Inoltre, l'immagine include un avatar di un robot che sembra sorseggiare un bicchiere di birra, con un commento che dice: "The AI speculates: Looks like they all agree."

### Analisi e Discussione

- **Leggi l'esempio**: Questo passo è fondamentale per assicurarsi che tutti i partecipanti siano sulla stessa pagina quando si tratta di interpretare un esempio specifico. Questo aiuta a standardizzare le esperienze e a ridurre le variazioni individuali.

- **Inserisci il tuo pensamento**: Questo passo è critico per raccolta dei dati. La scelta tra "Offensive" e "Not offensive" fornisce una misura quantitativa della percezione personale di ciascun partecipante riguardo l'offensività del linguaggio.

- **Vedi gli altri pensieri**: Questo passo è importante per capire come la percezione collettiva influisce sulla percezione individuale. Se la maggioranza degli altri partecipanti trova una frase offensiva, anche un partecipante che inizialmente pensava diversamente potrebbe cambiare idea.

### Conclusione

Questo diagramma rappresenta un esperimento di valutazione della tossicità del linguaggio, dove i partecipanti leggono un esempio, esprimono il proprio pensamento su esso e quindi vedono le opinioni degli altri partecipanti. Questo tipo di esperimento è utile per analizzare come il linguaggio è percevuto in modo collettivo e individuale, e come queste percezioni possono variare.</sample>
    <sample id="263">Task B: Toxicity

Analisi

Dataset

- Dynahate

Modeli

- Perspective API
- Rewire API
- State RoBERTa
- GPT-4</sample>
    <sample id="264">Risultati
Chi hanno gli insiemi di dati e i modelli NLP in linea con?</sample>
    <sample id="265">I dataset e i modelli sono più orientati verso le nazioni parlanti inglese.</sample>
    <sample id="266">I dataset e i modelli sono più coerenti con le persone che hanno un istituto di istruzione superiore.</sample>
    <sample id="267">I dataset e i modelli sono maggiormente allineati con le persone che hanno un istituto di istruzione superiore.</sample>
    <sample id="268">Trova 2: Alcune popolazioni rimangono indietro.</sample>
    <sample id="269">I dataset e i modelli sono meno allineati con le persone non binarie.</sample>
    <sample id="270">Quindi, cosa possiamo fare? Affrontare la posizionalità in NLP</sample>
    <sample id="271">Le raccomandazioni</sample>
    <sample id="272">I consigli presentati nella diapositiva riguardano la documentazione delle scelte relative alla progettazione dei dataset e dei modelli, la ricerca in NLP attraverso il lens di perspectivism e la costruzione di modelli specializzati per specifiche comunità.</sample>
    <sample id="273">Grazie! Link al dashboard: nlpositionality.washington.edu/ Paper: bit.ly/NLPositionality-Paper/ Età Genere Etnicità Religione Livello di istruzione Paese di residenza Paese di origine Lingua nativa</sample>
    <sample id="274">The slide mentions that there are four problems associated with SimulST models.</sample>
    <sample id="275">Per mitigare i bias sociali e politici, è importante adottare un approccio multidisciplinare che coinvolga diverse discipline, tra cui la sociologia, la politica, la storia e la filosofia. Inoltre, è necessario coinvolgere una vasta gamma di utenti e stakeholder per garantire che le diverse viste e esperienze siano rappresentate.</sample>
    <sample id="276">Il 61esimo incontro annuale dell'Associazione per la Linguistica Computazionale si è tenuto a Toronto, Canada, dal 12 al 14 giugno 2023. L'immagine illustra un presentazione sul "Distilling Script Knowledge from Large Language Models for Constrained Language Planning." La presentazione è stata condotta da Silyu Yuan, Jianqiang Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao e Deqing Yang, rappresentanti di Peking University e Brain Technologies Inc. L'immagine è suddivisa in due sezioni: la sinistra mostre una vista notturna della città di Toronto con i propri edifici illuminati, mentre la destra raffigura una donna con capelli lunghi che parla, probabilmente durante la presentazione.</sample>
    <sample id="277">Gather your ingredients.</sample>
    <sample id="278">Come fare un pasticcio?</sample>
    <sample id="279">Constrained Language Planning

How to Make a Strawberry Cake?
...Add strawberry jam into the flour...

How to Make a Chocolate Cake?
...Add cocoa powder into the flour...

Specific goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="280">Constrained Language Planning

How to Make a Strawberry Cake?
...Add strawberry jams into the flour...

How to Make a Chocolate Cake?
...Add cocoa powder into the flour...

Object goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="281">Constrained Language Planning

How to Make a Strawberry Cake?
...Add strawberry jams into the flour...

How to Make a Chocolate Cake?
...Add cocoa powder into the flour...

Object goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="282">Come perform i modelli di lingua grande (LLMs) sul pianificamento linguistico con vincoli?</sample>
    <sample id="283">Come fanno i modelli di apprendimento dell'lingua (LLMs) sul pianificamento linguistico con vincoli?</sample>
    <sample id="284">Come funzionano i modelli di apprendimento automatico per la pianificazione linguistica con vincoli?</sample>
    <sample id="285">Può LLM fare pianificazione linguistica con vincoli?</sample>
    <sample id="286">Può LLM fare pianificazione linguistica con vincoli?</sample>
    <sample id="287">Quale tipo di errori i modelli di apprendimento automatico (LLMs) usualmente fanno in questa attività?</sample>
    <sample id="288">Quale tipo di errori i modelli di apprendimento automatico (LLMs) usualmente fanno in questa attività?</sample>
    <sample id="289">Cosa tipicamente fallis InstructGPT?</sample>
    <sample id="290">Input: un obiettivo astratto Genera obiettivi specifici utilizzando GPT-3.0 in imparazione contestuale Abstratta obiettivo: Fare un pasto Specifici obiettivi: G1 (+ modifica): Fare un pasto al cioccolato G2 (+ modifica): Fare un pasto in un ristorante di strada G3 (+ intenzione): Fare un pasto per una cerimonia</sample>
    <sample id="291">Input: un obiettivo astratto Genera obiettivi specifici utilizzando GPT-4 In-context learning Abstratta Obiettivo: Fare un pasto Specifici Obiettivi:</sample>
    <sample id="292">Metodo</sample>
    <sample id="293">Metodo

I diagrammi a sinistra e in basso hanno i seguenti passi:

1. Generare candidate script con InstructGPT tramite imparazione in contesto
2. Trova i filtri script che raggiungono il obiettivo con InstructGPT tramite similarità punteggio
3. Output: Obiettivi specifici con script corrispondenti</sample>
    <sample id="294">Metodo</sample>
    <sample id="295">Metodo

In primo luogo, generiamo candidate script utilizzando InstructGPT con imparazione in contesto. Questo coinvolge fornire InstructGPT con un contesto specifico e richiedere che generi script correlati a tale contesto.

Successivamente, utilizziamo InstructGPT per trovare i filtri script che raggiungono il obiettivo con una punteggio di similarità specifico. Questo coinvolge fornire InstructGPT con un obiettivo specifico e richiedere che filtri i script generati in precedenza in base alla similarità tra i due.

Infine, selezioniamo i script filtrati come output specifici obiettivi con i corrispondenti script. Questo coinvolge scegliere i script filtrati che hanno la punteggio di similarità più elevato e fornire i dettagli specifici dell'obiettivo e del corrispondente script.</sample>
    <sample id="296">Il nostro metodo migliora grandemente la qualità della pianificazione</sample>
    <sample id="297">Script Distillazione da modelli di lingua

Motivazione:
- Abilitare la capacità pianificativa limitata per modelli più piccoli.
- Seguire l'idea di distillazione conoscenziale simbolica.

Metodo:
- Generare 55.000 script con vincolo da modelli di lingua basati su un dataset di script.
- Utilizzare una validazione e un set di test.

Passi:
1. Generare schemi diPlanning con InstructGPT tramite imparazione in contesto.
2. Generare script candidati con InstructGPT tramite imparazione in contesto.
3. Trovare i filtri migliori tramite InstructGPT utilizzando il punteggio di similarità.

Output:
- Obiettivi specifici con piani di azione</sample>
    <sample id="298">Script Distillazione da modelli LLMs</sample>
    <sample id="299">Script Distillazione da modelli LLMs</sample>
    <sample id="300">Script Distillazione da modelli di lingua

Motivazione:
- Abilitare la capacità pianificazione per modelli più piccoli.
- Seguire l'idea di distillazione conoscenziale simbolica.
- Generare 55.000 script con vincoli da modelli di lingua basati su un metodo =&gt; Corpus di script.

Metodo:
1. Generare uno struttura con InstructGPT tramite imparazione in contesto.
2. Trova candidati script con InstructGPT tramite imparazione in contesto.
3. Trova i filtri script con InstructGPT tramite punteggio di similarità.
Output: Obiettivi specifici con piani di azione</sample>
    <sample id="301">Script Distillazione da modelli LLMs</sample>
    <sample id="302">Analisi dei vincoli</sample>
    <sample id="303">Specialized Models vs. LLMs

Il grafico a barre sulla sinistra confronta la precisione di diversi modelli di apprendimento automatico, inclusi GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow e modelli specializzati addeuti su Coreset. I modelli specializzati hanno dimostrato una precisione più alta rispetto ai modelli generalistici.

Il testo sottostante spiega che i modelli più piccoli, addeuti su Coreset, possono generare script di qualità superiore rispetto ai modelli a larga scala (LLMs). Questo è dovuto alla capacità dei modelli specializzati di concentrarsi su specifiche attività, come la creazione di script, mentre i modelli generalistici devono gestire una vasta gamma di compiti.

In conclusione, il grafico e il testo insieme evidenziano l'importanza della specializzazione nel campo dell'apprendimento automatico, specialmente per le attività che richiedono un alto livello di precisione e qualità.</sample>
    <sample id="304">Riassumendo, il problema di pianificazione linguistica vincolata è stato stabilito. Si è valutato la capacità di generazione di LLM per la pianificazione linguistica vincolata e si è sviluppato un metodo per generare troppe risposte e selezionare quindi le migliori. Ci sono anche delle limitazioni e del lavoro futuro da fare.</sample>
    <sample id="305">Riassunto e trae i principali punti

- **Riassunto del problema di pianificazione linguistica vincolata**
  - Creare un problema di pianificazione linguistica vincolata.
  - Valutare la capacità di pianificazione vincolata dei modelli di lingua (LLMs) e sviluppare un metodo "genera-allora-filtra" per LLMs.
  - Utilizzare i modelli di lingua per creare un dataset di script di alta qualità (CoScript) per pianificazione linguistica vincolata.

- **Limitazioni e lavoro future**
  - Il nuovo approccio proposto per migliorare i modelli di lingua è un "approccio post hoc di re-rank".
  - CoScript eredita solo da un modello astratto con un vincolo.
  - Il dataset di script può essere un risorsa valiosa per la ricerca su pianificazione linguistica con obiettivi e vincoli più complessi e diversi.

- **Impegnato**
  - La nostra storia è sulla pianificazione linguistica con vincoli.</sample>
    <sample id="306">Il 61esimo incontro annuale dell'Associazione per la Linguistica Computazionale si è tenuto a Toronto, Canada, dal 11 al 14 luglio 2023. L'immagine rappresenta una presentazione sullo sviluppo di conoscenze dello script da modelli di linguaggio a lungo termine per pianificazione linguistica vincolata. La presentazione è stata preparata da Siyu Yuan, Jiangjie Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao e Deging Yang. Per ulteriori informazioni, si prega di visitare il sito web del concorso o consultare il link fornito.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi SOTA.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana sono applicabilità, utilità, covetess, e transferibilità.</sample>
    <sample id="309">I discorsi TED sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">20%</sample>
    <sample id="311">La differenza tra i set di dati benigni e backdoor viene misurata utilizzando le metriche di distanza ∆cos e ∆l2.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare due gruppi di modelli.</sample>
    <sample id="344">Gli autori contano la frequenza delle parole su un corpus di testo generale e selezionano casualmente n parole in un intervallo di frequenza moderata.</sample>
    <sample id="345">Sono felice di presentare il mio nuovo video, in cui esploreremo la questione di quanto i tagger di entità denominate siano ancora efficaci nel 2023. Questo argomento è di particolare rilievo in campo di elaborazione linguistica naturale e di intelligenza artificiale.</sample>
    <sample id="346">Nel 2019, la maggioranza dei fornitori di servizi di cloud computing hanno rilevato un incremento del 30% nel volume di transazioni in relazione alla sicurezza e alle privacy. Questo trend è stato alimentato da crescenti concern per la conformità ai regolamenti di sicurezza e privacy, come il GDPR e il HIPAA. Inoltre, l'uso di servizi di sicurezza di terze parti ha aumentato notevolmente, con un 45% dei fornitori di servizi di cloud utilizzando almeno un servizio di sicurezza di terze parti.</sample>
    <sample id="347">Riconoscimento e generalizzazione di entità denominate • I modelli hanno utilizzato CoNLL-2003 per lo sviluppo di NER per quasi 20 anni</sample>
    <sample id="348">Riconoscimento entità e generalizzazioni • I modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi 20 anni • Questi modelli possono generalizzare i dati moderni? • Qual è necessario per una buona generalizzazione? Georgia Tech</sample>
    <sample id="349">Riconoscimento entità e generalizzazioni

I modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi 20 anni. Può il modello generalizzare i dati moderni? Qual è necessario per una buona generalizzazione?</sample>
    <sample id="350">ConLL++ Dataset

AMBASSADOR
UNITED
NATIONS
LINDA
THOMAS-GREENFIELD

O
O
ORG
O
PER</sample>
    <sample id="351">ConLL++ Dataset • RaccoltoReuters news from 2020 and annotated with ConLL-2003 annotation guidelines • Fine-tuned 20+ models on CoNLL-2003 AMBASSADOR O UNITED OORG NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="352">CoNLL++ Dataset • Raccolti i notiziari di Reuters da 2020 e annotati con le direttive di annotazione CoNLL-2003 • Addestrato su 20+ modelli su CoNLL-2003 • Valutato su CoNLL-2003 test set e CoNLL++ • Calcolato il percentuale ∆F1 per valutare la generalizzabilità Person: O Person: I-PER Organization: O Organization: I-ORG Location: O Location: I-LOC</sample>
    <sample id="353">Cosa è necessario per una buona generalizzazione?</sample>
    <sample id="354">Cosa si richiede per una buona generalizzazione?</sample>
    <sample id="355">Cosa si richiede per una buona generalizzazione?</sample>
    <sample id="356">Cosa si richiede per una buona generalizzabilità?</sample>
    <sample id="357">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="358">Cosa causa una diminuzione delle prestazioni?</sample>
    <sample id="359">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="360">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="361">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="362">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="363">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="364">Cosa causa una diminuzione delle prestazioni?</sample>
    <sample id="365">Cosa causa la diminuzione delle prestazioni?</sample>
    <sample id="366">Conclusione
Per un buon impianto, ci serve:
- Modelli più accurati
- Dimensioni dei modelli più grandi
- Esempi di addestramento più ampi

La grafica rappresenta lo sviluppo delle prestazioni dei modelli di impianto da 2004 a 2022. I modelli BERT-Large e CoNLL++ hanno dimostrato un notevole miglioramento rispetto ai precedenti modelli, come quello di Stanford NLP. Questi progressi sono stati possibili grazie all'incremento della dimensione dei modelli, all'aumento del volume di esempi di addestramento e all'uso di architettura più avanzate.</sample>
    <sample id="367">Conclusione
Per una buona generalizzazione, ci serve:
- Architettura migliore
- Dimensione del modello più grande
- Esempi di addestramento più grandi
Performance drop causata da:
- Drift temporale
- Sottoprogramma non adattato
L'evoluzione della performance dei modelli di NLP è stata analizzata nel tempo. I modelli più vecchi come Stanford NLP e Flair hanno un trend di performance che si stabilizza dopo un inizio di crescita. I modelli più recenti come BILSTM-CNN-CRF e CoNLL 2019 hanno un trend di crescita continuo, ma con un tasso di crescita più rallentato rispetto ai modelli precedenti. Questo suggerisce che i modelli più recenti stanno migliorando la loro performance nel tempo, ma al tasso di crescita rallentato.</sample>
    <sample id="368">Conclusione
Per una buona generalizzazione, ci serve:
- Architettura migliore
- Dimensione del modello più grande
- Più esempi di addestramento
- Performance scadente causata da:
  - Adattamento eccessivo temporale
  - Non adattamento alla drift temporale
- Sono i tagger di CoNLL-2003 ancora utili?
Georgia Tech</sample>
    <sample id="369">Conclusione
Per una buona generalizzazione, ci serve:
- Migliore dimensione modello
- Dimensioni modelli più grandi
- Più esempi di addestramento fin-tuning
- Performance drop causato da:
  - Adattamento non adeguato
  - Drift temporale
- ConNLL-2003 tagger funziona?
- Sì, YES!
Il grafico illustra le prestazioni dei modelli di apprendimento automatico nel tempo. Il modello BILSTM_CNN_Large ha registrato un incremento costante di performance dal 2004 al 2018, raggiungendo i livelli più alti nel 2018. In contrasto, il modello CoNLL++ ha dimostrato un trend di crescita decennale, con un notevole incremento dopo il 2014. Questi dati suggeriscono che entrambi i modelli hanno migliorato le loro prestazioni nel tempo, ma con differenti profili di crescita.</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhanL/acl2023-conllpp Contact: slu775@gatech.edu</sample>
    <sample id="397">L'approccio utilizza un segmento parlato di dimensioni 10x10.</sample>
    <sample id="398">Per rispondere correttamente alla domanda, è necessario conoscere le professioni di Servin e Kea.</sample>
    <sample id="399">La qualità dell'esempio è più importante.</sample>
    <sample id="400">The article focuses on the extended experiments with the following language models: BERT-base, RoBERTa-base, RoBERTa-large, distilRoBERTa-base, ALBERT-base, ALBERT-large, BART-base, and T5-base.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">The examples of direct reference are "the first one", "the song that's not energetic".</sample>
    <sample id="403">I ricercatori che hanno scritto l'articolo sono affiliati all'University of Toronto e a Brain Technologies Inc.</sample>
    <sample id="404">To determine the number of authors involved in the article, we need to carefully examine the list of names provided on the cover page. The names listed are:

1. Yanis Labarre
2. Adrien Bazille
3. Richard Dufour
4. Mickael Rouvier
5. Emmanuel Morin
6. Beatrice Dallie
7. Pierre-Antoine Gourraud

By counting these names, we can see that there are a total of 7 authors involved in the article. Therefore, the answer is 7.</sample>
    <sample id="405">No, è stato considerato come un approccio non standard.</sample>
    <sample id="406">Gli autori hanno fornito un esempio di "unwarrior" e "womawarrior" come gruppo contrassegnato.</sample>
    <sample id="407">Le architetture dei modelli che non generalizzano adeguatamente non sono indicate in questo slide.</sample>
    <sample id="408">The names of the test sets are FT_w, COSINE, L2R, BERT, and MLc.</sample>
    <sample id="409">There are six authors involved in the article.</sample>
    <sample id="410">L'autore opera con più modalità.</sample>
    <sample id="439">The area of NLU that is little studied, according to the authors, is inference-time knowledge.</sample>
    <sample id="440">I relatori sono Zhongyang Xu, Ying Shen e Lifu Huang.</sample>
    <sample id="441">Sì, il Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="442">Solo una piccola parte di parole dipende dal contesto.</sample>
    <sample id="443">Risolver espressioni di riferimento indiretti per la selezione di entità (AltEntities Corpus)</sample>
    <sample id="444">Risolver espressioni di riferimento indiretti per la selezione di entità (AltEntities Corpus)</sample>
    <sample id="445">Indirect Referring Expressions

* God!: Understanding users' language when they make a choice
* Alternative question: Did you mean easy on me or I gotta feeling?
* Direct reference: "the first one"
* Indirect reference: "the newer one", "The song that's not energetic"

Google Research</sample>
    <sample id="446">Esprimere indirettamente le espressioni</sample>
    <sample id="447">Indirect Referring Expressions

* Obiettivo: Comprendere il linguaggio dei utenti quando fanno un选择
* Domanda alternativa: Did you mean easy on me or I gotta feeling?
* Riferimento diretto: "easy on me", "the first one"
* Riferimento indiretto: Non ricordare il nome, non essere in grado di distinguere le pronunciazioni, voler specificare un preferito
* Riferimento indiretto: The newer one. The song that's not energetic.

Risolvendo le espressioni riferimento indiretti per la selezione degli oggetti (MIDAS Corpus)</sample>
    <sample id="448">Indirect Referring Expressions

* Obiettivo: Comprendere il linguaggio dei utenti quando fanno una scelta
* Domanda alternativa: "Hai detto facile su di me o gotta feeling?"
* Riferimento diretto: "Il primo"
* Riferimento indiretto: Non ricordare il nome, pronunciazioni difficili da distinguere, specificare un preferimento
* Riferimento indiretto: "La nuova"
* "La canzone che non è energica"

Google Research</sample>
    <sample id="449">Indirect Referring Expressions

* Obiettivo: Comprendere il linguaggio dei utenti quando fanno un选择
* Domanda alternativa: Did you mean easy on me or I gotta feeling?
* Riferimento diretto: Non posso ricordare il nome
* Riferimento indiretto: Non posso ricordare il nome del nome
* Riferimento indiretto: La nuova una. La canzone che non è energica.

Google Research</sample>
    <sample id="450">Raccolta di dataset

Problema importante

• Sistemi conversazionali
• Benchmarking dei modelli a larga scala per la comprensione degli enti
• Non esiste un insieme di dataset pubblico a larga scala disponibile

Raccogliamo un insieme di grandi dataset utilizzando annotazione di massa

Trento dominii:

• Musica
• Libri
• Ristorante</sample>
    <sample id="451">Problema di raccolta di dataset

• Importante problema
    • Sistemi di conversazione
    • Benchmarking dei modelli a larga scala di linguaggio
    • Nessun insieme di dati pubblico a larga scala disponibile
• Raccolgiamo un insieme di grandi dataset utilizzando annotazione di massa
• Tre domini:

Risolgendo le espressioni indirette tramite la selezione dell'entità (Mention Corpus)</sample>
    <sample id="452">Metodologia di raccolta di dataset

La metodologia di raccolta di dataset si concentra sull'informalità utilizzando una attività di completamento di disegni. L'immagine illustra un esempio di come la metodologia funziona. Iniziano con un dialogo in cui due persone hanno una conversazione informale. Una delle persone chiede a un'altra di ricordare qualcosa che è successo ieri. L'altra persona risponde dicendo "Do you mean I have to feel 1 Gotta Feeling?".

Successivamente, la metodologia richiede di fornire alternative alla domanda originale. In questo caso, la domanda alternativa sarebbe "What do you mean by 'I gotta feeling'?". 

Infine, la metodologia richiede di fornire un'espressione che si riferisce a uno dei due enti presenti nel dialogo. In questo caso, l'espressione sarebbe "The person who said 'I gotta feeling'". 

Questo esempio dimostra come la metodologia di raccolta di dataset si concentra sull'informalità e sulla comprensione del contesto del dialogo.</sample>
    <sample id="453">Metodologia di raccolta di dataset che mette l'accento sulla informale utilizzando un compito di completamento di disegni</sample>
    <sample id="454">Metodologia di raccolta di dataset che mette in evidenza l'informalità utilizzando un compito di completamento di disegni.</sample>
    <sample id="455">Metodologia di raccolta di dataset che mette in evidenza l'informalità utilizzando un compito di completamento di disegni.</sample>
    <sample id="456">Metodologia di raccolta di dataset

La metodologia di raccolta di dataset si concentra sull'informalità utilizzando un compito di completamento di disegni. L'immagine illustra un esempio di come la metodologia funziona. Iniziamo con un dialogo in cui una delle parti chiede all'altra di raccontare un po' del proprio giorno ieri. L'altra parte risponde dicendo "Ho un gotta feeling". 

Successivamente, viene presentata un'alternativa alla domanda originale, che è "Hai un gotta feeling?".

Infine, viene fornita un'indicazione su come il contesto espressivo si riferisce a uno dei due enti presenti nel dialogo. In questo caso, l'indicazione è che l'entità riferita è la persona che ha detto "Ho un gotta feeling".

In sintesi, questa metodologia mira a analizzare e documentare le differenze tra le forme linguistiche informali e formali in un dialogo.</sample>
    <sample id="457">Metodologia di raccolta di dataset

La metodologia si concentra sull'informalità utilizzando un compito di completamento di fumetti.</sample>
    <sample id="458">Generare domande alternative = selezione di coppie di entità</sample>
    <sample id="459">Genera domande alternative = estrazione di coppie di entità</sample>
    <sample id="460">Generare domande alternative = selezione di coppie di entità</sample>
    <sample id="461">Generare domande alternative = selezione di coppie di entità</sample>
    <sample id="462">Generare domande alternative = selezione di coppie di entità</sample>
    <sample id="463">Google Research</sample>
    <sample id="464">Google Research

Background knowledge (Music)

Google search link to find out about the song.

Easy on Me (by Ajele)

I Gotta Feeling (by The Black Eyed Peas)

Click here to find out about the song.

Click here to find out about the song.

We ask annotators to:

Listen to at least some of each song.

Read about each song.</sample>
    <sample id="465">Google Research</sample>
    <sample id="466">Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste in eleven balls made of the same paste as the cake. Pandan cake is a light, fluffy sponge cake flavoured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia and also in the Netherlands, especially among the Indo community.</sample>
    <sample id="467">Eliciting expressions

We then tell the annotators which choice should be selected and ask them to describe it.

Pick one

Easy on Me (by Adele)

I Gotta Feeling (by The Black Eyed Peas)

We would like you to give us 3 to 5 expressions to fill in your speech bubble, no examples.

The one with the dance music

The song that's not energetic

The song that's about a river

It's about not having time to choose</sample>
    <sample id="468">Eliciting expressions

We then tell the annotators which choice should be selected and ask them to describe it.

Pick one

Easy on Me (by Adele)

I Gotta Feeling (by The Black Eyed Peas)

We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, not example.</sample>
    <sample id="469">AltEntities Corpus</sample>
    <sample id="470">AltEntities Corpus

- 6.000 alternative questions across the three domains
- 42.000 indirect referring expressions

Risultati con il modello T5 XL (accuratezza):
- 92-95% se il LM ha accesso alle stesse conoscenze di sfondo come gli annotatori.
- 82-87% quando il LM ha accesso a informazioni di sfondo parzialmente sovrapposte.
- ~60% quando il LM (T5 XL) ha accesso solo ai nomi degli entità.

Abbiamo dimostrato che i modelli sono generalizzabili tra i domini.

Collegamento al dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="471">AltEntities Corpus

- 6.000 alternative questions across the three domains
- 42.000 indirect referring expressions

Risultati con il modello T5 XL (accuratezza):
- 92-95% se il LM ha accesso alle stesse conoscenze di sfondo come gli annotatori.
- 82%-87% quando il LM ha accesso a conoscenze di sfondo parzialmente sovrapposte.
- ~60% quando il LM (T5 XL) ha accesso solo ai nomi degli entità.

Abbiamo dimostrato che i modelli sono generalizzabili in diversi domini.

Collegamento del dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="472">AltEntities Corpus

- 6.000 alternative questions across the three domains
- 42.000 indirect referring expressions

Risultati con il modello T5 XL (accuratezza):
- 92-95% se il LM ha accesso alla stessa conoscenza di sfondo degli annotatori.
- 82-87% quando il LM ha accesso a conoscenza di sfondo parzialmente sovrapposti.
- ~60% quando il LM (T5 XL) ha accesso solo ai nomi dell'entità.

Abbiamo dimostrato che i modelli sono generalizzabili su domini diversi.

Collegamento al dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="473">L'approccio viene confrontato con le politiche SimulST esistenti.</sample>
    <sample id="474">To determine the affiliations of the authors, we need to carefully examine the information provided in the image. The image lists the authors and their respective affiliations as follows:

1. Yannis Labarre - LIA, Avignon Université
2. Adrien Bazille - LS2N, Université
3. Richard Dufour - CHU de Nantes
4. Mickael Rouvier - ZENIOS
5. Emmanuel Morin - CERMEC, Université
6. Béatrice Dalille - CHU de Nantes
7. Pierre-Antoine Gourraud - GIPSA-LR, INP Grenoble

By breaking down the affiliations, we can see that they include various institutions such as universities, hospitals, and research centers. These affiliations indicate the academic and professional backgrounds of the authors involved in the study.</sample>
    <sample id="475">La relatrice o il relatore è Sebastin Sany.</sample>
    <sample id="476">Per rispondere alla domanda di quanti autori sono coinvolti nell'articolo, dobbiamo analizzare il testo fornito. L'articolo è intitolato "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" e include i nomi dei tre autori principali: Myra Cheng, Esin Durmus, e Dan Jurafsky. 

Passo 1: Identificare il titolo dell'articolo.
- Il titolo dell'articolo è "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models".

Passo 2: Identificare i nomi dei coautori.
- I nomi dei coautori sono Myra Cheng, Esin Durmus, e Dan Jurafsky.

Passo 3: Contare il numero di coautori.
- Ci sono tre nomi di coautori.

Risposta: Ci sono tre coautori che hanno collaborato all'articolo.</sample>
    <sample id="477">Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="478">Sara Papi, Matteo Negri, Marco Turchi

Università di Trento

Fondazione Bruno Kessler</sample>
    <sample id="479">I problemi dei modelli SimulST attuali?</sample>
    <sample id="480">I modelli SimulST attuali presentano problemi specifici. Le strutture specifiche di questi modelli vengono addestrate, introducendo moduli aggiuntivi che devono essere ottimizzati. Inoltre, i processi di addestramento sono lunghi e complicati (ad esempio, con obiettivi di ottimizzazione diversi).</sample>
    <sample id="481">I modelli SimulST attuali hanno diversi problemi. Le strutture specifiche sono di solito addestrate, introducendo moduli aggiuntivi da ottimizzare (ad es., moduli differenti per obiettivi diversi). I processi di addestramento sono lunghi e complicati (ad es., addestramento di diversi modelli per raggiungere regimi latenza diversi). Addestrare e mantenere diversi modelli per raggiungere regimi latenza diversi.</sample>
    <sample id="482">La nostra soluzione?</sample>
    <sample id="483">Utilizziamo modelli offline ST esistenti senza re-allenare o adottare un particolare architettura per SimuST.</sample>
    <sample id="484">La presentazione illustra una soluzione propria per la traduzione automatica, utilizzando modelli offline di traduzioni precedenti. La soluzione proposta include: 1) Utilizzare modelli offline esistenti di traduzioni precedenti per SimulST; 2) Utilizzare un singolo modello per ogni regime di latenza e gestire la latenza tramite parametri specifici; 3) Sfruttare le conoscenze acquisite dalla connessione tra input audio e output testuale.</sample>
    <sample id="485">La nostra soluzione: EDAtt</sample>
    <sample id="486">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝜆) towards the last 𝑘 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="487">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="488">Decidere se emettere o non emettere una traduzione parziale basata su dove un注意力点被发出，如果注意力未集中在某个词上（其总和低于阈值λ）则发出该词，表示接收到的信息足够稳定。</sample>
    <sample id="489">Decidere se emettere o non emettere una traduzione parziale basata su dove si concentra l'attenzione: un parola viene emessa se l'attenzione non è concentrata (sua somma è inferiore a un soglia λ) verso i last λ fotogrammi di audio, significando che la ricevuta informazione è abbastanza stabile.</sample>
    <sample id="490">Decidere se emettere o non emettere una traduzione parziale basata su dove l'attenzione punta verso un parola. Se l'attenzione non è concentrata (sua somma è inferiore a un soglia λ) verso i last λ fotogrammi, significa che la ricezione informazione è abbastanza stabile.</sample>
    <sample id="491">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="492">Decidere se emettere o non emettere una traduzione parziale basata su dove l'attenzione punti: un carattere è emesso se l'attenzione non è concentrata su (il suo sum è inferiore a un soglia 𝛼) verso le ultime 𝜆 informazioni ricevute stabilizzate.</sample>
    <sample id="493">Decidere se emetter o non emetter una traduzione parziale basata su dove un punto di attenzione indica: un carattere non è concentrato (sua somma è inferiore a un soglia α) verso le ultime λ finestre di speech frames, il che significa che informazione è abbastanza stabile.</sample>
    <sample id="494">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (it's sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="495">Risultati principali: EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (δ)</sample>
    <sample id="496">Risultati principali: EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (δ)</sample>
    <sample id="497">Risultati principali: EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) latency misura (a) AL-de</sample>
    <sample id="498">Risultati principali: EDAtt

La scansione di un grafico in inglese non è stata fornita, ma i titoli e le etichette del grafico sono in inglese. Di seguito è riportato il testo originale:

"Main Results: EDAtt

The x-axis represents the ratio of AL/AL-CA (e.g., 1.5 means AL is 1.5 times larger than AL-CA). The y-axis shows the BLEU score. The blue bar indicates that when AL is 1.5 times larger than AL-CA, the BLEU score is significantly higher than when AL and AL-CA are equal.

(a) en-de

The graph shows that EDAtt achieves the best performance when AL is 1.5 times larger than AL-CA. This suggests that EDAtt benefits from a larger AL component. However, EDAtt still outperforms AL-CA even when AL is equal to AL-CA, indicating that EDAtt can effectively learn the cross-lingual translation knowledge from AL-CA."</sample>
    <sample id="499">Risultati principali: EDAtt 27 25 23 21 19 17 15 13 11 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (δ)</sample>
    <sample id="500">Risultati principali: EDAtt

popolari strategie anche applicate a modelli offline

wait-k LA CAAT EDAtt

BLU</sample>
    <sample id="501">Risultati principali: EDAtt</sample>
    <sample id="502">Risultati principali: EDAtt

La grafica rappresenta i risultati dell'EDAtt in un grafico a linee, con diverse linee che rappresentano diversi metodi di traduzione. La x-axis rappresenta il rapporto tra AL e AL-CA (AL/AL-CA), mentre la y-axis rappresenta il punteggio BLEU. I colori delle linee rappresentano diversi metodi di traduzione: wait-k (rosso), LA (blu), CAAT (verde) e EDAtt (rossa).</sample>
    <sample id="503">Risultati EDAtt:</sample>
    <sample id="504">La presentazione è un invito a esplorare ulteriori risultati leggendo il paper. Viene fornita una serie di informazioni di contatto per i ricercatori, inclusi indirizzi email e link a repository di GitHub. Inoltre, ci sono due account social media per le piattaforme Twitter e Facebook. Al centro della presentazione c'è un QR code con la parola "Scan me!" scritta al di sopra di esso.</sample>
    <sample id="505">Sì, il set di dati è pubblicato su GitHub.</sample>
    <sample id="506">Multinstruct: Migliorando l'apprendimento a zero con modelli multi-modal tramite l'addestramento con istruzioni

Il testo in inglese riportato nella slide è il seguente:

"MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning

Zhiyang Xu*, Ying Shen*, Lifu Huang
Department of Computer Science, Virginia Tech

*Equal Contribution"

Traducendo in italiano, il contenuto sarebbe:

"Multinstruct: Migliorando l'apprendimento a zero con modelli multi-modal tramite l'addestramento con istruzioni

Zhiyang Xu*, Ying Shen*, Lifu Huang
Dipartimento di Scienze Informatiche, Virginia Tech

*Contributo uguale"

La slide introduce un articolo o un progetto intitolato "Multinstruct", che si concentra sulla migliorazione dell'apprendimento a zero con modelli multi-modal utilizzando l'addestramento con istruzioni. Gli autori del lavoro sono Zhiyang Xu, Ying Shen e Lifu Huang, appartenenti al Dipartimento di Scienze Informatiche all'Università di Virginia. L'indicazione "*Equal Contribution" suggerisce che i tre autori hanno avuto un contributo uguale allo sviluppo del progetto o all'articolo presentato.</sample>
    <sample id="507">### Descrizione dell'immagine

L'immagine rappresenta un diagramma che confronta due metodi di apprendimento dei modelli di linguaggio: il pre-addestramento con finetuning e il prompt. Il diagramma è suddiviso in due sezioni principali:

1. **Pre-addestramento con finetuning (BERT, T5)**:
   - **Pre-addestrato LM**: Questo bloccetto rappresenta un modello di linguaggio pre-addestrato.
   - **Prompt on task A**: Questo bloccetto indica che il modello è stato adattato per un compito specifico, A.
   - **Troppe esempi task-specific per modello per ciascun compito**: Questa frase spiega che il modello ha troppe esempi specifici per ciascun compito, rendendolo più adatto a quell'attività.

2. **Prompting (GPT-3)**:
   - **Prompted LM**: Questo bloccetto rappresenta un modello di linguaggio pre-addestrato.
   - **Improve performance or prompt engineering**: Questo bloccetto indica che il modello è stato migliorato attraverso l'engaggimento del prompt o la creazione di nuovi esempi.

Inoltre, il diagramma include un bloccetto aggiuntivo per il finetuning con istruzioni (FLAN), che rappresenta un modello pre-addestrato che è stato finetuned con istruzioni.

Il diagramma conclude con una citazione: "Finetuned language models are zero-shot learners." Questa frase spiega che i modelli di linguaggio finetune sono in grado di eseguire compiti senza essere stati adattati per quell'attività specifica.

### Analisi e Discussione

#### Pre-addestramento con Finetuning
Il pre-addestramento con finetuning è un approccio dove un modello di linguaggio pre-addestrato su un vasto insieme di dati viene ulteriormente adattato per un compito specifico. Questo processo coinvolge l'adattamento del modello per un numero limitato di esempi specifici per il compito. L'approccio è utile quando si dispone di pochi esempi per il compito specifico, ma richiede un'adattamento personalizzato per ogni nuovo compito.

#### Prompting
Il prompting è un approccio dove un modello di linguaggio pre-addestrato è utilizzato per rispondere a domande o compiti specifici senza necessariamente essere adattato per quell'attività specifica. Questo approccio è basato sull'ingegneria del prompt, ovvero la creazione di esempi specifici per guidare il modello verso la risoluzione del compito. GPT-3 è un esempio di un modello che utilizza il prompting per migliorare le sue prestazioni.

#### Finetuning con Istruzioni (FLAN)
Il finetuning con istruzioni rappresenta un ulteriore passo nel pre-addestramento con finetuning. Questo approccio coinvolge l'adattamento del modello non solo per un compito specifico, ma anche per imparare a seguire istruzioni. Questo può renderlo più versatile e capace di gestire una gamma più ampia di compiti.

### Conclusione

Entrambi gli approcci hanno i loro punti di forza e di debolezze. Il pre-addestramento con finetuning è utile quando si dispone di pochi esempi per un compito specifico, mentre il prompting è più flessibile e può essere utilizzato per un grande numero di compiti senza necessariamente essere adattato per ciascuno. Il finetuning con istruzioni rappresenta un passo avanti in quest'area, migliorando la capacità del modello di seguire istruzioni e gestire compiti più complessi.</sample>
    <sample id="508">### Descrizione Dettagliata

#### Figura 2: Confronto tra addestramento con istruzioni e pre-addestramento finetuning

La figura rappresentata nella diapositiva è un diagramma che confronta due approcci differenti per l'addestramento dei modelli di linguaggio: l'addestramento con istruzioni (Instruction Tuning) e il pre-addestramento finetuning (Pre-trained LM finetune). 

**A) Pre-addestramento finetuning (BERT, T5):**
- **Pre-addestrato su LM:** Questo modello è stato addestrato su un vasto insieme di testi generali per imparare le caratteristiche linguistiche通用.
- **Istruzioni specifiche per la compito A:** Dopo l'addestramento generale, il modello viene finetuned su specifiche istruzioni per un particolo compito A. Questo coinvolge l'adattamento del modello per comprendere e rispondere specificamente alle esigenze del compito A.

**B) Prompting (GPT-3):**
- **Pre-addestrato su LM:** Come nel caso precedente, il modello è stato addestrato su un vasto insieme di testi generali.
- **Prompting:** In questo approccio, il modello riceve un input o un prompt specifico per generare una risposta. Questo può essere utilizzato per diverse compiti senza necessariamente finetune il modello completamente.

**C) Addestramento con istruzioni (FLAN):**
- **Pre-addestrato su LM:** Anche in questo caso, il modello è stato addestrato su un vasto insieme di testi generali.
- **Addestramento con istruzioni su LM:** Il modello viene addestrato su istruzioni specifiche per un insieme di compiti. Questo coinvolge l'addestramento del modello per comprendere e rispondere a diverse istruzioni in modo generale.
- **Inferenza su compiti non visti:** Una volta addestrato, il modello può essere utilizzato per inferenza su compiti che non sono stati visti durante l'addestramento.

**Riassunto:**
- L'addestramento con istruzioni è più flessibile poiché il modello è addestrato su istruzioni per un insieme di compiti, rendendolo capace di gestire compiti diversi senza necessariamente finetune completamente.
- Il pre-addestramento finetuning è più specifico poiché il modello viene finetune su istruzioni specifiche per un particolo compito, rendendolo ottimizzato per quell'unico compito.

Questo diagramma illustra come questi due approcci possono essere utilizzati per addestrare modelli di linguaggio per diverse applicazioni, con addestramento con istruzioni tendendo verso un uso più generale e pre-addestramento finetuning tendendo verso un uso più specifico.</sample>
    <sample id="509">Solo linguaggio</sample>
    <sample id="510">Il testo in inglese nella immagine è:

"Instruction Tuning on Multimodal Pre-trained Models"

Questo testo indica che il discorso o la presentazione si concentra sulla tecnica di "tuning" (addestramento) delle modele pre-addestrate tramite un processo di "instruction tuning". Questo approccio è specificamente applicato a modele multimodale, che possono elaborare e integrare informazioni provenienti da diverse fonti o tipi di input, come testi, immagini o audio.</sample>
    <sample id="511">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks Wang, Yichang, et al. "Benchmarking generalization via in-context instructions on 1,600+ language tasks." arXiv preprint arXiv:2305.</sample>
    <sample id="513">Il primo dataset di istruzioni multimodale per l'addestramento. Questo diagramma rappresenta diverse taree e istruzioni utilizzate per addestrare un modello di istruzioni multimodale. Le taree include la relazione visivo-relazionale, la comprensione temporale, la comprensione regionale, la comprensione immagine e la corrispondenza testo-immagine. Le istruzioni coinvolgono diverse azioni come la riconoscimento di oggetti, la classificazione di oggetti, la segmentazione di oggetti, la rilevazione di oggetti e la traduzione di testi.</sample>
    <sample id="514">Il primo dataset di istruzioni multimodale per l'addestramento. Questo dataset include 62 diverse compiti e 10 grandi gruppi di istruzioni. Inoltre, include 5 istruzioni scritte da esperti.</sample>
    <sample id="515">OFA (One For All) è un modello pre-addestrato multi-modal che è capace di eseguire sia compiti di comprensione che di generazione con singoli o più modali. OFA ha una **vocabulary unificata** per i token del linguaggio, delle immagini e le coordinate di un bounding box.</sample>
    <sample id="516">Il diagramma illustrato nella figura 1 rappresenta un esempio di istanze provenienti dal dataset MULTINSTRUCT, suddiviso in quattro task principali: Grounded Caption, Text Localization, Refering Expression Selection e Question-Image Matching. 

1. **Grounded Caption**: Questo task richiede la creazione di una descrizione dettagliata di un immagine utilizzando coordinate bounding box per specificare le posizioni degli oggetti rilevati. Ad esempio, la frase "&lt;bin-198-32&gt; and white racquet" indica che il racquetball è posizionato all'interno della bounding box &lt;bin-198-32&gt; e che è Blanco.

2. **Text Localization**: In questo task, viene fornita una regione specifica dell'immagine e si richiede di identificare la posizione esatta dell'oggetto correlato. Ad esempio, la regione &lt;bin-606-64&gt; è specificata come la posizione dell'oggetto &lt;bin-606-64&gt;.

3. **Refering Expression Selection**: Questo task implica la selezione di un oggetto specifico all'interno di un'immagine, utilizzando coordinate bounding box. Ad esempio, la frase "&lt;bin-242-110&gt;" indica che l'oggetto &lt;bin-242-110&gt; è quello riferito.

4. **Question-Image Matching**: In questo task, viene presentata un'immagine e una domanda correlata. Il modello deve determinare se la domanda è rilevante o no rispetto all'immagine. Ad esempio, la domanda "Is it the question image?" è rilevante rispetto all'immagine.

Inoltre, la figura 1 include un esempio di output per ciascun task, dimostrando come i modelli possono essere utilizzati per analizzare e interpretare diverse tipologie di input e fornire risposte appropriate.</sample>
    <sample id="517">Il diagramma illustra un esempio di come il modello MULTINSTRUCT funziona per quattro diverse attività: captioning, localizzazione del testo, selezione espressione riferimento e risoluzione di domande basate su immagini. In cima al diagramma, il modello riceve un input che include una descrizione di un'immagine o un testo. Questo input viene processato per produrre un output che fornisce informazioni pertinenti alla attività specificata. Ad esempio, per la attività di captioning, l'input è una descrizione di un'azione sportiva, e l'output è una frase che riassume l'azione. Per la attività di localizzazione del testo, l'input è un testo con diverse opzioni di posizione, e l'output è la posizione corretta del testo all'interno dell'immagine. La attività di selezione espressione riferimento richiede che il modello identifichi un oggetto specifico in un'immagine, mentre la attività di risoluzione di domande basate su immagini richiede che il modello analisi un'immagine e fornisca una risposta a una domanda specifica.</sample>
    <sample id="518">### Descrizione Dettagliata

#### Figura 1: Esempi di istanze da MULTINSTRUCT per quattro compiti.

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Output:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection:** "The front of a train is [0.242, 0.242, 0.339, 0.339]."
- **Question-Image Matching:** "The question 'Is it the image of a train?' is relevant."

**Input:**  
- **Caption:** "A woman in a black dress playing tennis with a white racquet."
- **Text Localization:** "The region that contains the text 'de' is [0.486, 0.486, 0.574, 0.574]."
- **Refering Expression Selection</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Multi-Modal Instruction Turning

• Costruzione del dataset di addestramento:
- Utilizziamo 53 task da 9 gruppi per l'addestramento.
- Scegliamo 10.000 istanze per task.

• Costruzione del dataset di test:
- Riserviamo tutto il gruppo di ragione comune per il test.
- Selezioniamo ulteriormente 5 task da gruppi Q&amp;A e miscellanei.
- Selezioniamo casualmente 20 task dal gruppo di test di dataset Natural Instructions come task sconosciute per NLP.</sample>
    <sample id="521">Costruzione del dataset di addestramento: Utilizziamo 53 task provenienti da 9 gruppi per l'addestramento. Scegliamo 10,000 istanze per task. Costruzione del dataset di test: Riserviamo tutto il gruppo di ragione comune per il test. Selezioniamo ulteriormente 5 task dal gruppo di domande a scelta libera e gruppi di rilevanza. Utilizziamo tutte le istanze in test per ciascun task. Scegliamo casualmente 20 task dal gruppo di test del dataset Natural Instructions come task non visti per NLP.</sample>
    <sample id="522">Dettagli sull'implementazione:

• Dettagli di addestramento:
- Modello pre-addestrato OFA-Large (472M)
- Unire tutte le istanze per tutti i compiti.
- Ogni istanza è combinata casualmente con uno dei propri modelli di istruzioni.

• Dettagli di test:
- Per ogni compito, condurre un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ogni esperimento.
- Rapportare la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="523">Dettagli sull'implementazione:

• Dettagli di addestramento:
- Modello pre-addestrato OFA-Large (472M)
- Utilizzo di istanze per tutti i compiti
- Ogni istanza è combinata casualmente con uno dei propri modelli di istruzioni.

• Dettagli di test:
- Per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento.
- Rapportiamo la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="524">Dettagli sull'implementazione:

• Dettagli di addestramento:
- Modello pre-addestrato OFA-Large (472M)
- Utilizzo delle istanze per tutti i compiti.
- Ogni istanza è combinata casualmente con uno dei propri modelli di istruzioni.

• Dettagli di test:
- Per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento.
- Rapportiamo la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="525">Evaluazione dei metodi

- Per i compiti di classificazione multi-modal (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, e Classificazione del tipo di disaster) riportiamo l'accuratezza.
- Per i compiti di generazione multi-modal (Q&amp;A comune, Q&amp;A di testo, Grounded VQA, Estrazione visuale del dialogo) riportiamo il Rouge-L.
- Per i compiti di NLP, riportiamo il Rouge-L come punteggio di prestazione per la maggior parte delle attività. Inoltre, non solo abbiamo l'accuratezza come metrica.

Inoltre, calcoliamo anche la performance aggregata per ogni modello basata sulla media della performance del modello su tutti i compiti multi-modal e NLP visibili.</sample>
    <sample id="526">Sensibilità

Come sensibile il modello è verso la varietà delle istruzioni per la stessa attività:

$$\mathbb{E}_{x \sim T} \left[ \frac{\sigma^2_x}{\mu_x} \left( \frac{\mathbb{E}_{y \sim D|X=x}[L(f_0(i, x, y))]}{\mathbb{E}_{y \sim D}[L(f_0(i, x, y))]} \right) \right]$$</sample>
    <sample id="527">Efficacy of Instruction Tuning on MULTINSTRUCT

La tabella 1 riporta i risultati di un esperimento sulla validazione dell'efficacia dell'addestramento dell'insegnante su MULTINSTRUCT. La tabella confronta le prestazioni di diversi modelli di linguaggio, inclusi GPT-4, GPT-3.5-TURBO e OPA, in diverse taree di ragione multi-modal, come la domanda a testo, la visualizzazione, la visualizzazione spaziale e la classificazione di tipi di emergenze. I risultati indicano che GPT-4 ha le migliori prestazioni in molte delle taree, seguita da GPT-3.5-TURBO e OPA.

Tabella 2: Validazione dello zero-shot per la risoluzione di domande e miscellanea

La tabella 2 riporta i risultati di un esperimento di validazione dello zero-shot per la risoluzione di domande e miscellanea. La tabella confronta le prestazioni di diversi modelli di linguaggio, inclusi GPT-4, GPT-3.5-TURBO e OPA, in diverse taree di domande a testo, domande visive, dialoghi visivi e classificazione di tipi di emergenze. I risultati indicano che GPT-4 ha le migliori prestazioni in molte delle taree, seguita da GPT-3.5-TURBO e OPA.</sample>
    <sample id="528">Il modello OPA è stato testato su due task: il task di domande e risposte (Q&amp;A) e il task di classificazione di tipi di domande. I risultati sono riportati nella tabella 2, che riassume le performance dello OPA in zero-shot su queste due tare.</sample>
    <sample id="529">Impatto dell'aumento dei cluster di compiti di istruzioni multimodale

I punti chiave della presentazione sono:

* **Img Und**: VQA + Image Understanding, Grounding
* **MISC**: Grounded Matching + Grounded Generation, Temporal Ordering + Miscellaneous + Image Text Matching
* **Relation**: Visual Relationship
* **Region**: Region Understanding
* **NLP**: NLP tasks

Il grafico a sinistra rappresenta la performance del modello in funzione dei cluster di compiti di istruzioni multimodale. La performance aumenta con l'aumento dei cluster di compiti.

Il grafico a destra rappresenta la performance del modello in funzione del tipo di compito di istruzione multimodale. La performance aumenta con l'aumento del tipo di compito di istruzione multimodale.

La presentazione conclude che l'aumento dei cluster di compiti di istruzioni multimodale ha un impatto positivo sulla performance del modello.</sample>
    <sample id="530">Effetto delle istruzioni diverse sull'addestramento dell'instruzione

- L'OPA addestrato su 5 istruzioni raggiunge prestazioni molto elevate su tutti i compiti di valutazione e dimostra una sensibilità inferiore.
- Tabella 3: Effetto del numero diverso di istruzioni. Prestazioni dell'OPA_sensitiviz, addestrato su diverse quantità di istruzioni.

La presentazione illustra l'effetto delle istruzioni diverse sull'addestramento dell'instruzione. L'OPA addestrato su 5 istruzioni raggiunge prestazioni elevate su tutti i compiti di valutazione e dimostra una sensibilità inferiore rispetto all'OPA_sensitiviz, che è stato addestrato su diverse quantità di istruzioni. La tabella 3 fornisce maggiori dettagli sull'effetto del numero diverso di istruzioni sull'addestramento dell'instruzione.</sample>
    <sample id="531">Effetto delle strategie di finetuning sul sensibilità dei modelli

- Addestramento con istruzioni su Multinstruct può ridurre in modo significativo la sensibilità del modello.
- L'apprendimento da dataset di istruzioni naturali può ulteriormente ridurre la sensibilità del modello.

[Immagine]
Figura 4: Sensibilità dei modelli a compiti sconosciuti. Numeri più bassi sono migliori.</sample>
    <sample id="532">Il modello OPA ha dimostrato un'ottima capacità di imparare direttamente da istruzioni naturali. Questo è dovuto alla capacità del modello di imparare direttamente da istruzioni naturali, che è diverso da altre tecniche di imparare da istruzioni. Questa capacità è stata dimostrata in un esperimento in cui il modello è stato addeistrato su istruzioni naturali e ha dimostrato una capacità di imparare direttamente da istruzioni. Questo è diverso da altre tecniche di imparare da istruzioni, che richiedono di essere addestrati su istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un vantaggio importante per il modello OPA, poiché significa che può imparare direttamente da istruzioni senza dover passare attraverso un processo di imparare da istruzioni precedenti. Questo è un</sample>
    <sample id="533">Conclusione

• Primo insieme di addestramento multi-modal a grande scala.
- Contiene 62 compiti multi-modal da 10 category diverse.
• Migliora notevolmente la capacità zero-shot dell'OFa attraverso l'addestramento con istruzioni.
• Esplora diversi metodi di trasferimento di imparare e dimostra i loro benefici.
• Progetta un nuovo metrico di sensibilità.</sample>
    <sample id="534">Un'altra cosa! Stiamo raccolgendo un insieme di istruzioni multimodale molto più grande con circa 150 attività in più che coinvolgono visione e linguaggio, e li rilanceremo presto.</sample>
    <sample id="535">I fornitori dell'articolo sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="536">La relatrice o il relatore è Mohammed Javad Hosseini.</sample>
    <sample id="562">I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto ACL 2023 Johns Hopkins University Purdue University MIT Meta</sample>
    <sample id="563">I giudizi sulla accettabilità dei modelli di linguaggio non sono sempre robusti al contesto ACL 2023 Johns Hopkins University Purdue University MIT Meta</sample>
    <sample id="564">Revisiting Minimal Pair Paradigm

Evaluazioni del modello di linguaggio utilizzano differenze relative in sequenza probabilità per valutare la conoscenza astratta dei modelli di linguaggio (LMs).

BLMP | SyntaxGym | CrowS
1. Many people were helping themselves. | 1. No customer ... has spent any money.2. The customer ... spent any money. | 1. Stereotypical sentence.2. Non-stereotypical sentence.</sample>
    <sample id="565">Revisitando il paradigma del paio minimo

Evaluazioni dei modelli di linguaggio utilizzano differenze relative in sequenza di probabilità per valutare la conoscenza astratta dei modelli.</sample>
    <sample id="566">Revisitando il paradigma del pariglio minimo (MPP) Le valutazioni dei modelli di linguaggio utilizzano differenze relative in sequenza probabilità per valutare l'abstract knowledge dei modelli di linguaggio (LMs).</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm

Evaluazioni del modello di linguaggio utilizzano differenze relative in sequenza di probabilità per valutare la conoscenza astratta dei modelli di linguaggio (LMs).

BLMP | SyntaxGym | CrowS
1. Many people were helping themselves. | 1. No customer has spent any money. | 1. Stereotypical sentence.
2. Many people were helping herself. | 2. The customer spent any money. | 2. Non-stereotypical sentence.

P(1) &gt; P(2) | P(1) &gt; P(2) | P(1) &gt; P(2)</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm

Evaluazioni del paradigma del paradosso minimo (MPP) dei modelli di linguaggio utilizzano differenze relative in probabilità sequenziali per valutare la conoscenza astratta dei modelli di linguaggio (LMs).

| BLMP | SyntaxGym | CrowS |
| --- | --- | --- |
| 1. Many people were helping themselves.2. "Many people were helping herself." | 1. No customer... has spent any money.2. "The customer... spent any money." | 1. Women are terrible at handwork.2. Men are terrible at handwork. |
| P(1) &gt; P(2) | P(1.any) &gt; P(2.any) | P(1) &gt; P(2) |

Are these judgments stable with long preceding context?</sample>
    <sample id="569">Revisitando il paradigma del paio minimo

Evaluazioni del modello di linguaggio utilizzano differenze relative in sequenza probabilità per valutare la conoscenza astratta dei modelli di linguaggio (LMs).</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm

Evaluazioni del paradigma di coppia minima (MPP) dei modelli di linguaggio utilizzano differenze relative en la probabilità di secuencias para evaluar el conocimiento abstracto de los LM's.

BLMP | SyntaxGym | CrowS
1. Many people were helping themselves. | 1. No customer... has spent any money. 2. The customer... spent any money. | 1. Women are terrible at handwork. 2. Men are terrible at handwork.
P(1) &gt; P(2) | P(1.any) &gt; P(2.any) | P(1) &gt; P(2)

¿Son estos juicios estables con un contexto previo largo?</sample>
    <sample id="571">Approccio Testare se i giudizi del MPP variano come funzione della lunghezza del contesto, corrispondenza strutturale e accettabilità Test Suite: Concordanza Soggetto-Verbale P (Prefisso) &gt; P (Prefisso) Space of Candidate Prefixes Who might Rose fees from returning to this customer? What might Rose fees from this customer before returning to this customer before k?</sample>
    <sample id="572">Test whether MPP judgments vary as a function of context length, structural match, and acceptability</sample>
    <sample id="573">Test whether MPP judgments vary as a function of context length, structural match, and acceptability</sample>
    <sample id="574">Test whether MPP judgments vary as a function of context length, structural match, and acceptability</sample>
    <sample id="575">Approccio Testare se i giudizi del MPP variano in funzione della lunghezza del contesto, corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement Space of Candidate Prefixes P (Subject + Prefix) &gt; P (Verb + Prefix) Unacceptable Matched Who might Rose fees from returning to this customer? What would Jessica sell these souvenirs before returning to the museum? What had Aaron sounded like the museum returning to this customer?</sample>
    <sample id="576">Testare se le giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilita'</sample>
    <sample id="577">Approccio Testare se le giudizi del MPP variano in funzione della lunghezza del contesto, corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement P.(Prefisso) &gt; P.(Prefisso) Space of Candidate Prefixes Acceptable, Unmatched</sample>
    <sample id="578">Approccio Testare se le valutazioni del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità</sample>
    <sample id="579">Testo per vedere se le valutazioni del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità</sample>
    <sample id="580">Test whether MPP judgments vary as a function of context length, structural match, and acceptability</sample>
    <sample id="581">I giudizi del MPP sono robusti per lunghezze di contesto diverse. Abbiamo eseguito valutazioni del MPP con contesti accettabili/accettabili; corrisposti/non corrisposti struttura - lunghezze fino a 900 token.</sample>
    <sample id="582">I giudizi del MPP sono robusti per lunghezze di contesto diverse. Abbiamo effettuato valutazioni del MPP con contesti accettabili/moventi; corrisposti/non corrisposti; struttura corrisposta/non corrisposta - fino a 900 token.</sample>
    <sample id="583">Evaluazione delle sentenze MPP accettabili/accettabili in contesti diversi: accettabili/accettabili; strutture corrisposte/simili – lunghezza fino a 900 token.</sample>
    <sample id="584">Evaluations of the Acceptable/Unacceptable MPPI in the context raise/lower judgment performance</sample>
    <sample id="585">Evaluations of the acceptability/unacceptability of MPP sentences in the context raise/lower judgment performance We perform MPP evaluations with different contexts – acceptable/acceptable; matched/mismatched structure – of lengths up to 900 tokens.</sample>
    <sample id="586">Evaluazioni del MPF accettabili/accettabili con strutture corrisposte o non corrisposte, lunghe fino a 900 token.</sample>
    <sample id="587">Evaluazioni del modello MPF con contesti accettabili/accettabili; corrisposti/misconnessi strutturati - lunghezze fino a 900 token</sample>
    <sample id="588">Evaluations of the acceptability/unacceptability of MP-P sentences with matched/mismatched structures.</sample>
    <sample id="589">Why do matched prefixes affect LM judgments?</sample>
    <sample id="590">Why do matched prefixes affect LM judgements?</sample>
    <sample id="591">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these perturbations. • Prefix/suffix adverbs: "However, &lt;sent&gt;." • Long prefix adverbs: "First and foremost, &lt;sent&gt;." • Add clause: "&lt;sent&gt; Regardless of what X thinks about it, &lt;sent&gt;." • Quote: "Yesterday, X said, '&lt;sent&gt;'" Prefix Type Acceptable Unacceptable All</sample>
    <sample id="592">Per perturbare i contesti delle frasi in modo che si preservi la struttura rilevante e chiedere se i modelli sono sensibili allo stesso modo.</sample>
    <sample id="593">Per perturbare i contesti delle frasi in modo che si preservi la struttura rilevante e chiedere se i modelli sono similiamente sensibili a queste frasi.</sample>
    <sample id="594">### Key Takeaways

- I modelli di linguaggio sono sensibili a caratteristiche sintattiche/semantiche condivise tra le frasi.
- Frasi valutate con input brevi, singoli-sentenze non riescono a completamente catturare i LM's (Language Models) conoscenza astratta.

#### Graph
Il grafico illustra la relazione tra la probabilità condizionata (P(LM|Pref)) e la probabilità di preferimento (Pref) per diverse frasi prefissate. Le linee colorate rappresentano diversi test di accordo tra i modelli di linguaggio e le sentenze. Le linee in stile "X" rappresentano i test di accordo tra i modelli di linguaggio e le sentenze, mentre le linee in stile "O" rappresentano i test di accordo tra i modelli di linguaggio e le sentenze.

#### Space of Candidate Prefixes
Il diagramma inferiore del grafico rappresenta lo spazio dei prefissi candidati. I simboli diversi rappresentano diverse tipologie di prefissi:
- **Test di Accordo** (Agreement Test): Simbolo di stella (*)
- **Test di Preferimento** (Preference Test): Simbolo di cerchio (o)
- **Test di Accettabilità** (Acceptability Test): Simbolo di quadrato (□)

Questi test analizzano come i modelli di linguaggio reagiscono a diverse tipologie di prefissi, influenzando la probabilità di continuazione delle frasi.</sample>
    <sample id="595">Modelli di linguaggio sono sensibili a caratteristiche sintattiche/semantiche condivise tra le frasi. Frasi valutate con input brevi, singoli-sentenze, non riescono ad afferrare completamente i LM's.</sample>
    <sample id="596">Key Takeaways

* I modelli di linguaggio sono sensibili a caratteristiche sintattiche/semantiche condivise tra le frasi.
* Frasi valutate con input brevi, singoli-sentenze non completamente catturano i LM's conoscenza astratta.
* Le valutazioni MPPI con input brevi non catturano completamente le conoscenze astratte dei modelli di linguaggio.

[English]
Key Takeaways

* Language models are sensitive to latent syntactic/semantic features shared across sentences.
* Sentence evaluations with short, single-inputs do not fully capture LM's abstract knowledge.
* MPPI evaluations with short inputs do not fully capture the abstract knowledge of language models.</sample>
    <sample id="597">The first step of the method maps the input tokens to a Tag token.</sample>
    <sample id="598">55,000</sample>
    <sample id="626">Il miglior metodo di allineamento per DEplain è BERTalign.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato allevia il problema di annotazione.</sample>
    <sample id="628">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato utilizzando il dataset CoNLL 2003.</sample>
    <sample id="630">XSemPLR: Analisi Semantica Cross-Lingua in Più Linguaggi Naturali e Rappresentazioni del Significato

Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

PennState e Amazon</sample>
    <sample id="631">XSemPLR: Analisi Semantica Cross-Lingua in Più Linguaggi Naturali e Rappresentazioni del Significato

Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

PennState e Amazon</sample>
    <sample id="632">Cross-lingual Semantic Parsing è un compito di tradurre query in più lingue naturali in rappresentazioni di significato multipli.</sample>
    <sample id="633">Cross-lingual Semantic Parsing</sample>
    <sample id="634">Parsin semantico transversale

I modelli esistenti di CLSLP sono proposti e valutati separatamente su dataset di task e applicazioni limitate. Ad esempio:

* Mancato copertura su certi linguaggi naturali

English | German | Chinese
:---: | :---: | :---:
Neural Models | SQL | Lambda
FunQL</sample>
    <sample id="635">Cross-lingual Semantic Parsing

I modelli esistenti di CLSP sono separatamente proposti e valutati su insiemi di task e applicazioni limitati. Ad esempio:

* Mancanza di copertura su certi linguaggi naturali

English | German | Chinese
--- | --- | ---
Neural Models | SQL | Lambda
FunQL</sample>
    <sample id="636">Parsi semantici multilingue

I modelli di PSLC esistenti sono proposti e valutati separatamente su insiemi di task e applicazioni limitati. Ad esempio:

* Mancanza di copertura su rappresentazioni di significato determinate

English | German | Chinese
Neural Models | SQL | Lambda | FunQL</sample>
    <sample id="637">Parsing Semantico Cross-lingua

I modelli esistenti di CLSLP sono separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio:

- Mancanza di copertura su rappresentazioni di significato specifiche

English | German | Chinese
Neural Models | SQL | Lambda
FunQL</sample>
    <sample id="638">Cross-lingual Semantic Parsing

* Modeli esistenti di CLSP sono separatamente proposti e valutati su dataset di task e applicazioni limitate. Ad esempio:
	+ Mancato copertura su certi modelli neurali
	+ English
	+ Germano
	+ Cinese
	+ Single Model
	+ SQL
	+ Lambda
	+ FunQL</sample>
    <sample id="639">XSemPLR

• Foriamo una base di dati unificata XSemPLR per la analisi semantica transversale in rappresentazioni del significato
o 9 set di dataset in diversi domini
o 5 task di analisi semantica diverse
o 22 lingue naturali in 15 famiglie linguistiche

1. USA: cosa hanno i giocatori meno di tre atti in una stagione?
2. Canada: cosa è il calcolo?
3. Germania: wir vilde japanische Restaurants es ist in Paris?
4. India: moord metin ristorante con piu piu?
5. Iran: یک نخستین بود که آفریقا را ترک کرد؟
6. Italia: cosa è un ristorante?
7. Giappone: 何は、自然言語処理の分野で使用される？
8. Corea del Nord: 이는 무엇입니까?
9. Francia: comment ça marche?
10. Giapponese: これは什么？</sample>
    <sample id="640">XSemPLR fornisce un insieme unificato XSemPLR per la analisi semantica di testo in diverse lingue. Contiene 9 set di domini diversi, 5 compiti di analisi semantica e 22 rappresentazioni di significato in 15 famiglie linguistiche.</sample>
    <sample id="641">Impostazioni sperimentali

* Consideriamo i sei ambienti per il training e l'evaluation.
* **Translate-Test**: Utilizziamo l'API di traduzione Google per tradurre il linguaggio di origine verso il linguaggio di destinazione. Poi utilizziamo un modello monolingue per il training e l'evaluation.

Training

* Iniziamo con il linguaggio inglese.
* Utilizziamo un modello inglese per il training.
* Successivamente, utilizziamo il modello per generare SQL.

Inference

* Iniziamo con il linguaggio tedesco.
* Utilizziamo l'API di traduzione per tradurre il linguaggio di origine verso inglese.
* Utilizziamo il modello inglese per il training.
* Infine, utilizziamo il modello per generare SQL.</sample>
    <sample id="642">Impostazioni sperimentali

* Consideriamo i sei ambienti di addestramento e valutazione.
* **Translate-Test**: Utilizziamo l'API di traduzione Google per tradurre il linguaggio di origine verso il linguaggio di destinazione. Poi utilizziamo un modello monolingue per addestrare e valutare.

Addestramento

* Inglese
* Modello Inglese
* SQL

Inferenza

* Tedesco
* API di traduzione
* Modello Inglese
* SQL</sample>
    <sample id="643">Impostazioni sperimentali

* Consideriamo i sei impianti per addestramento e valutazione.
* **Translate-Test**: Utilizziamo l'API di traduzione Google per tradurre il linguaggio di origine verso il linguaggio di destinazione. Poi utilizziamo un modello monolingue per addestrare e valutare.

Addestramento

* Inglese
* Modello Inglese
* SQL

Inferenza

* Tedesco
* API di traduzione
* Inglese
* Modello Inglese
* SQL</sample>
    <sample id="644">Impostazioni sperimentali. Consideriamo i sei ambienti per la formazione e l'evaluatione. Ad esempio, German-to-German: il linguaggio di origine è lo stesso del linguaggio di destinazione. Abbiamo anche testato i modelli monolingui in un contesto di poche istanze.</sample>
    <sample id="645">Impostazioni sperimentali

• Consideriamo i sei ambienti per addestramento e valutazione.
• Il linguaggio sorgente è lo stesso del linguaggio di destinazione, ad esempio Germano- tedesco. Abbiamo anche testato modelli monolingui addestrati con solo 10% dei dati di addestramento in un contesto di poche istanze monolingue.</sample>
    <sample id="646">Impostazioni sperimentali • Abbiamo considerato i sei ambienti di addestramento e valutazione. • Ad esempio, German-to-German: il linguaggio di origine è lo stesso del linguaggio di destinazione. Abbiamo anche testato il modello monolingue few-shot per la traduzione da Germano all'SQL. • Addestrare modelli monolingui con solo 10% dei dati di addestramento.</sample>
    <sample id="647">Impostazioni sperimentali

• Consideriamo sei impostazioni per il training e l'evaluation.
    • Modello multilingue: addestra un modello multilingue per tutte le lingue.

Training

Inferenza</sample>
    <sample id="648">Impostazioni sperimentali. Consideriamo i due settori per il training e l'evaluation. Multilingua Model: Addestra un modello multilingua per tutte le lingue.</sample>
    <sample id="649">Impostazioni sperimentali

* Consideriamo sei impostazioni per il training e l'evaluation.
    * Modello multilingue: addestra un modello multilingue per tutte le lingue.

Training

Inferenza</sample>
    <sample id="650">Impostazioni sperimentali

* Consideriamo sei impostazioni per addestramento e valutazione.
* Cross-lingual Zero-shot/Few-shot transfer: addestra su un linguaggio e trasferisci su un altro.

Addestramento

English
German
Few-shot

Multilingua Modello
SQL

Inferenza

English
German

Multilingua Modello
SQL</sample>
    <sample id="651">Impostazioni sperimentali

* Consideriamo sei impostazioni per addestramento e valutazione.
* Cross-lingual zero-shot/few-shot transfer: addestrare su un linguaggio e trasferire su un altro.

Addestramento
English
German
Multilingua Modello
SQL

Inferenza
English
German
Multilingua Modello
SQL</sample>
    <sample id="652">In analisi di impianto monolingue, valutiamo due gruppi di modelli: Enc-PTR e mBERT. Enc-PTR comprende Encoder multilingue preallenati con decoder basati su puntatori, mentre mBERT comprende modelli encoder-decoder preallenati monolingue. Abbiamo scoperto che Enc-Dec (mT5) ottiene il miglior prestigio su tutti i dataset!</sample>
    <sample id="653">In analisi dei modelli monolingui, i modelli Enc-Dec (mT5) hanno ottenuto le migliori prestazioni su tutti i dataset.</sample>
    <sample id="654">In questa immagine, ci sono due gruppi di modelli di apprendimento macchine: Enc-PTR e Enc-Dec. I modelli Enc-PTR utilizzano encoder multilingue preaddestrati con decoder basati su puntatori, mentre i modelli Enc-Dec utilizzano encoder e decoder preaddestrati in monoclingua. L'immagine dimostra che il modello Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i dataset analizzati.</sample>
    <sample id="655">Analisi del modello monolingue: Abbiamo valutato due gruppi di modelli in un contesto di imposizione monolingue. Uno dei gruppi è Enc-PTR, che rappresenta encoder multilingue preallenati con decoder basati su puntatori. L'altro gruppo comprende XLM-R + mBERT + PTR e Enc-Dec (mT5). Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i dataset!</sample>
    <sample id="656">Analisi dell'addestramento multilingue

In questa immagine, si discutono i risultati di un'analisi sull'addestramento multilingue. L'immagine include una tabella con le performance di diversi modelli linguistiche su diverse lingue. La tabella è suddivisa in colonne per i modelli e righe per le lingue.

1. **Enc-Dec-Enc-PTR**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

2. **XLM-R+PTR**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

3. **mT5**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

4. **XLM-R**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

5. **M2m**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

6. **MOvernight**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

7. **MCWQ**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

8. **MSch**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

9. **MTOP**: Questo modello ha ottenuto i seguenti pun分数:
   - Matis: 54.45
   - MgeQuery: 76.57
   - Mspider: 32.30
   - M2m: 67.55
   - MOvernight: 28.51
   - MCWQ: 82.95
   - MSch: 82.95
   - MTOP: 82.95
   - MCoNLL: 82.95

10. **MCoNLL**: Questo modello ha ottenuto i seguenti pun分数:
    - Matis: 54.45
    - MgeQuery: 76.57
    - Mspider: 32.30
    - M2m: 67.55
    - MOvernight: 28.51
    - MCWQ: 82.95
    - MSch: 82.95
    - MTOP: 82.95
    - MCoNLL: 82.95

La tabella evidenzia che il modello Enc-Dec-Enc-PTR ha ottenuto i pun分数 più alti in molte delle lingue analizzate, seguito da XLM-R+PTR e mT5. I modelli M2m, MOvernight, MCWQ, MSch, MTOP e MCoNLL hanno ottenuto pun分数 relativamente bassi rispetto ai modelli principali.

In sintesi, l'immagine fornisce una panoramica dei performance dei vari modelli linguistiche in un contesto multilingue, dimostrando che Enc-Dec-Enc-PTR è generalmente il modello più performante.</sample>
    <sample id="657">Analisi dell'addestramento multilingue

In questa immagine, si discende l'analisi dell'addestramento multilingue per un modello di intelligenza artificiale. L'immagine include una tabella che riporta i risultati di diverse misurazioni e valutazioni per diversi modelli di intelligenza artificiale. 

La tabella è suddivisa in colonne e righe. Le colonne rappresentano diversi modelli di intelligenza artificiale come MAT5, MGeQQuery, MSpider, MMaps, MOvernight, MCWQ, MSchema, MQA, MTOP, MCNola e Average. Le righe rappresentano diversi metodi di addestramento come Monolingual, XLM-R, Enc-Dec+Enc-PTR e mT5.

Ad esempio, la cella in basso a sinistra della tabella indica che il modello mT5 ha ottenuto un punteggio di 54.45 per il test Monolingual. Questo punteggio è stato calcolato utilizzando il modello mT5 addestrato solo in inglese.

Inoltre, la tabella include anche i punteggi media per ogni colonna, che rappresentano la performance generale del modello in quel particolo contesto. Ad esempio, il punteggio medio per il modello mT5 è di 61.82.

Questo tipo di analisi è importante per capire come diversi modelli di intelligenza artificiale performano in contesti multilingue e come possono essere migliorati attraverso l'addestramento in un mix di diverse lingue.</sample>
    <sample id="658">Analisi dell'addestramento multi linguaggio

• Evaluamo mT5 e XLM-R + PTR in un impianto multilingue
• La maggior parte delle prestazioni principali hanno guadagnato, tranne che per l'inglese, che ha registrato una diminuzione in 7 dataset. Questo è noto come "Curse of Multilinguality".
• Diagramma a barre con numero di dataset crescente e decrescente per diverse lingue</sample>
    <sample id="659">Analisi del Training Multilingue

In questa presentazione, analizziamo il training multilingue e i suoi effetti sugli NLG. Abbiamo valutato mT5 e XLM-R + PTR in un contesto multilingue. In generale, i principali NLG hanno registrato una miglioria delle prestazioni, tranne in 7 dataset in cui l'performance in inglese è diminuita, un fenomeno noto come "Curse of Multilinguality".

Il grafico rappresenta il numero di dataset per lingua naturale, con colori rossi per gli aumenti e blu per le diminuzioni. Come possiamo vedere, la maggioranza dei dataset ha registrato un aumento delle prestazioni, mentre solo pochi hanno registrato una diminuzione.

Questo studio dimostra che il training multilingue non è sempre benefico per tutti i linguaggi, e che è importante considerare le sfide specifiche di ogni lingua quando si sviluppano sistemi di traduzione automatica o di traduzione umana.</sample>
    <sample id="660">Cross-lingual Performance Gap

* **Blue Line:** Transfer im tempo
* **Orange Line:** Transfer zero-shot
* **Green Line:** Imparo unico linguaggio

Geography | MTP | SchemaQA
--- | --- | ---
Overnight | NLMaps | MCWQ
Geography/lambda |  | ATIS
Geography/prolog |  | Spider</sample>
    <sample id="661">Cross-lingual Performance Gap

* Linea azzurra: Cross-lingual Few-shot transfer
* Linea arancione: Cross-lingual Zero-shot transfer
* Linea verde: Monolingual Setting

Gequery Geotop SchemaQA Overnight NLMaps MCWQ Gequery/lamb Gequery/prolog Gequery/funql Gequery/sql Spider ATIS</sample>
    <sample id="662">Cross-lingual Performance Gap

* Linea azzurra: TransferFew-shot跨語言
* Linea arancione: TransferZero-shot跨語言
* Linea verde: ImpostazioneMonolingua單語環境

Geography地理
MTOP
SchemaQA
Overnight
NLMaps
MCWQ
Geography/prolog地理/Prolog
Geography/lamb地理/Lamb
Geography/sql地理/SQL
Geography/funql地理/Funql
Geography/prolog地理/Prolog
Spider</sample>
    <sample id="663">Risultati e Trovate (Sezione 4 del Paper)

- Enc-Dec (m75) supera i precedenti studi o raggiunge risultati paragonabili.
- Pre-addestramento su NL inglese può rafforzare notevolmente le prestazioni di modelli a pochi esempi su NL di destinazione.
- I modelli multilingue (Codex &amp; BLOOM) sono ancora inadeguati per la traduzione tra lingue diverse.
- La trasferimento di impara in cinese e addestramento monolingue inglese (En -&gt; En) ha il maggior gap di prestazione, mentre l'addestramento monolingue tedesco (German) ha generalmente il minore.
- FunQL supera le altre rappresentazioni di significato e SQL ottiene le peggiori prestazioni.</sample>
    <sample id="664">Risultati e Trovate (Sezione 4 del Paper)

- Enc-Dec (m75) supera i precedenti studi o raggiunge risultati paragonabili.
- Pretraining sul NL inglese può notevolmente migliorare le prestazioni di pochi esempi su NL di destinazione.
- I modelli multilingue LLM (Codex &amp; Bloom) sono ancora inadeguati per la traduzione tra lingue diverse.
- La trasferimento di impara in cinese e la formazione monolingue inglese (En -&gt; En) hanno il maggior gap di prestazione, mentre l'inglese tedesco di solito ha il minore.
- FunQL supera le altre rappresentazioni di significato e SQL ottiene le peggiori prestazioni.</sample>
    <sample id="665">Conclusione: Abbiamo costruito XSemPLR, un benchmark unificato per la analisi semantica multilingue con diverse lingue naturali e rappresentazioni di significato. Abbiamo condotto un'analisi esaustiva su tre tipi rappresentativi di modelli linguistici multilingue. I nostri risultati hanno dimostrato che mT5 con addestramento monolingue produce le migliori prestazioni, mentre i modelli multilingue semanticamente più avanzati sono ancora inadeguati per svolgere compiti di analisi semantica multilingue. Inoltre, il gap di prestazione tra addestramento monolingue e trasferimento multilingue è ancora significativo.</sample>
    <sample id="666">Conclusione - Abbiamo costruito XSemPLR, un benchmark unificato per la analisi semantica multilingue con diverse lingue naturali e rappresentazioni di significato. - Abbiamo condotto un'analisi esaustiva su tre tipi rappresentativi di modelli multilingue. - I nostri risultati hanno dimostrato che mT5 con addestramento monolingue fornisce il miglior prestigio, mentre i modelli multilingue automatici sono ancora inadeguati per svolgere compiti di analisi semantica multilingue. Inoltre, la differenza di prestazione tra addestramento monolingue e trasferimento multilingue è ancora significativa.</sample>
    <sample id="667">I lavori connessi in tal senso sono quelli che utilizzano un approccio di dominio delle frequenze per proteggere la proprietà intellettuale dei reti neurali profonde con l'uso di segni distintivi.</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP.</sample>
    <sample id="695">Il metodo affronta l'ambiguità delle permutazioni utilizzando un agent.</sample>
    <sample id="696">L'equità di un modello NLP a valle è definita come la capacità del modello di fornire risposte equamente e senza bias, nonostante le diverse espressioni linguistiche utilizzate.</sample>
    <sample id="697">The speaker's name is Yannis Labarre.</sample>
    <sample id="698">Non è specificato il nome del relatore o della relatrice in quest'immagine.</sample>
    <sample id="699">The speaker's name is Myra Cheng.</sample>
    <sample id="700">Il tropicalismo indica la definizione di gruppi solo per il loro identità.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target definendoli solo in base alla loro identità e utilizzando espressioni positive come "vibrante" per le donne latine, "petite, delicat, silky" per le donne asiatiche e "forti, resistenti" per le donne nere.</sample>
    <sample id="702">P-CXMI</sample>
    <sample id="703">ChuBERT is pre-trained on the full dataset, while DrBERT is pre-trained on a subset of the dataset.</sample>
    <sample id="751">Per fornire una risposta accurata, analizziamo i nomi degli autori elencati sul slide:

1. Zhiyang Xu
2. Ying Shen
3. Lifu Huang

In totale, ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un modello di apprendimento che utilizza un modello esistente per apprendere nuove informazioni. Questo modello è simile al modello cumulativo, ma differisce nel fatto che il modello esistente viene aggiornato in base alle nuove informazioni che vengono raccolte. Questo modello è utile quando si ha accesso a informazioni limitate o quando si vuole migliorare continuamente il modello esistente.</sample>
    <sample id="753">The objective of the dataset is to resolve indirect expressions by entity selection.</sample>
    <sample id="754">Un utente malevolo può estrarre i parametri del modello utilizzando un EaaS.</sample>
    <sample id="755">C'è un articolo con tre autori.</sample>
    <sample id="756">Per creare il set di dati iniziale sono stati impiegati 6 annotatori.</sample>
    <sample id="757">I ricercatori sono affiliati a diverse istituzioni.</sample>
    <sample id="758">Il gouverneur est à gauche.</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo sono ABC-Eval Behaviors.</sample>
    <sample id="760">Perché la valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria?</sample>
    <sample id="761">La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">No, gli annotatori non conoscono l'entità in anticipo.</sample>
    <sample id="763">BLEU, METEOR, TER, WER</sample>
    <sample id="764">Yes, the regression towards generalization affects specific types of NER.</sample>
    <sample id="765">La posizionalità nella NLP è importante per la comprensione del significato di un testo.</sample>
    <sample id="766">Adattatori</sample>
    <sample id="767">RoBERTa-base classifier head</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM sono il WMT’21 tedesco-inglese e il WMT’21 inglese-tedesco.</sample>
    <sample id="769">The authors have proposed three suggestions at the end.</sample>
    <sample id="770">Il guadagno del nuovo metodo rispetto al vecchio è del 36,88%.</sample>
    <sample id="771">Shuheng Liu, Alan Ritter</sample>
    <sample id="772">Yes, the results and data in the article can be used as a reference parameter.</sample>
    <sample id="773">Per rispondere alla domanda, analizziamo l'immagine fornita. L'immagine include un grafico che rappresenta la precisione di diversi modelli. I modelli più piccoli utilizzati sono:

1. GPT-3 (175B)
2. Codex (175B)
3. InstructGPT (175B)
4. T5 trained on wikiHow
5. T5 trained on Coscript

In totale, ci sono 5 modelli più piccoli utilizzati nell'articolo.

Ragione: Ho controllato il grafico e identificato i nomi dei modelli elencati. Ho conteggiato i modelli per fornire la risposta corretta.</sample>
    <sample id="774">OFA (One For All)</sample>
    <sample id="833">I fornitori di servizi cloud hanno diverse affiliazioni.</sample>
    <sample id="834">I membri dell'equipo sono affiliati a Stony Brook University e alla Human Language Analysis Group.</sample>
    <sample id="835">I due linguaggi analizzati nell'articolo sono il tedesco e l'inglese.</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">SAR, BLEU, BS-P e FRE.</sample>
    <sample id="838">Di queste 62 attività, 53 vengono utilizzate per scopi di addestramento e 9 per scopi di test.</sample>
    <sample id="839">Per determinare il numero di autori coinvolti nell'articolo, analizziamo la sezione di testo fornita. La frase in questione è: "Regina Stodden, Omar Momen, Laura Kallmeyer". Questa sequenza di nomi rappresenta i tre autori principali dell'articolo. Pertanto, il numero di autori coinvolti è 3.</sample>
    <sample id="840">The authors performed the tests on the following datasets: AG News, MIND, SST2, Enron Spam, and WikiText.</sample>
    <sample id="876">NACHOS is a pre-training strategy for language models.</sample>
    <sample id="877">Il nome del relatore è David Vil Torres.</sample>
    <sample id="878">La strategia del prompting ha un impatto significativo sui risultati.</sample>
    <sample id="879">I fornitori dell'articolo sono affiliati a Carnegie Mellon University, Language Technologies Institute, Technico Lisboa, BAIR e Unbabel.</sample>
    <sample id="880">I 5 istruzioni scritte da esperti sono:</sample>
    <sample id="881">Gli autori proppongono di utilizzare il test suite KITMUS per testare i modelli sull'utilizzo di informazioni provenienti da più fonti.</sample>
    <sample id="882">Prompting PalM per la traduzione
Valutazione delle strategie e le prestazioni</sample>
    <sample id="883">PalM: Language Model Pathways

- Chowdery et al., 2022
- arXiv:2204.02311
- 540B parametri
- Trained on 780B tokens
- Densamente attivati
- 6144 TPU v4 chips
- SOTA in benchmarking e generazione di testo
- 10 miliardi di parametri</sample>
    <sample id="884">PaLM: Percorsi del Modello del Linguaggio

- Chowdery et al., 2022
- arXiv:2204.02311.
- 540B parametri.
- Addestrato su 780B token.
- Attivati densamente.
- 6144 TPU v4 di chip.
- Record SOTA in centinaie di benchmark.
- 62 miliardi di parametri.

[Immagini]: Albera con colori diversi rappresentanti domande e risposte, traduzioni, comprensione del linguaggio, generazione di testo, completamento di codice e riassunti.

Google B</sample>
    <sample id="885">La nostra contributo

- Prima studio sistematico di stimolazione LLM per MT.
- Evaluare le capacità di traduzione con le migliori pratiche della comunità di traduzioni:
  - Settest più recenti (evitare sovrapposizione test/train e sovrapposizione)
  - SOTAs sistemi di traduzione recenti utilizzando i recenti dati di addestramento
  - Valutazione umana basata sull'esperto (più robusta dei worker)
- Raccomandazioni per le strategie di selezione dei prompt</sample>
    <sample id="886">La nostra contribuzione

• Prima studio sistematico di stimolazione LLM per MT.
• Evaluare le capacità di traduzione con le migliori pratiche della comunità di traduzioni:
• Settest più recenti (evitare overlap tra test/train e sovrapposizione con i dati di addestramento più recenti).
• Metri MT più recenti (correlazione migliore con giudizi umani).
• Valutazione umana basata sull'esperto (più robusta rispetto ai worker di una grande pubblica).
• Raccomandazioni per le strategie di selezione dei prompt.</sample>
    <sample id="887">La nostra contribuzione

• Prima studio sistematico di stimolazione LLM per MT.
• Evaluazione delle capacità di traduzione con le migliori pratiche della comunità di traduzioni:
- Settest più recenti (evitare overlap tra test/train e sovrapposizione con i dati di addestramento più recenti).
- SOTA metriche traduzioni (correlazione migliore con giudizi umani).
- Valutazione basata su esperti (più robusta rispetto ai worker di una grande pubblica).
• Raccomandazione per strategie di selezione dei prompt.

Google</sample>
    <sample id="888">La nostra contribuzione

• Prima studio sistematico di stimolazione LLM per la traduzione.
• Evaluazione delle capacità di traduzione con le migliori pratiche della comunità di traduzione:
- Set di test più recenti (evitare sovrapposizione tra test e addestramento)
- Confronto con i sistemi SOTA utilizzando i dati di addestramento più recenti
- Miglior correlazione con giudizi umani
- Valutazione umana basata sull'esperto (più robusta rispetto ai worker di massa)
• Raccomandazioni per le strategie di selezione dei prompt</sample>
    <sample id="889">Prompt hanno un grande impatto sulla qualità della traduzione

- Seleziona due prompt random per ogni frase.
- Calcola BLEURT per ciascun coppia di frase-prompt.
- La maggioranza delle frasi (516 su 1000) mostri una differenza di più di 1 punteggio BLEURT.
- La differenza può arrivare fino a 40 punteggii BLEURT!</sample>
    <sample id="890">Prompt hanno un grande impatto sulla qualità della traduzione

• Seleziona due prompt random per ogni frase.
• Calcola BLEURT per ciascun coppia di frase-prompt.
• La maggioranza delle frasi (516 su 1000) mostri una differenza di più di 1 punteggio BLEURT.
• La differenza può arrivare fino a 40 punteggii BLEURT!</sample>
    <sample id="891">I prompt hanno un grande impatto sulla qualità della traduzione

- Seleziona due prompt random per ogni frase.
- Calcola BLEURT per ciascun coppia di frase-prompt.
- La maggioranza delle frasi (516 su 1000) mostri una differenza di più di 1 punteggio BLEURT.
- La differenza può arrivare fino a 40 punteggii BLEURT!</sample>
    <sample id="892">Esempio di prompt per traduzione</sample>
    <sample id="893">Esempio di prompt per traduzione</sample>
    <sample id="894">Esempio di prompt per traduzione</sample>
    <sample id="895">Esempio di stimolo per la traduzione</sample>
    <sample id="896">Esempio di prompt per traduzione</sample>
    <sample id="897">Risultati sperimentali

• La qualità esempio è più importante del simile alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Consegne da MQG

• La fluidità di PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominate da "accuratezza/omissione"
• "Stile/Accadde" generalmente più basso per PaLM.</sample>
    <sample id="898">Risultati sperimentali

• La qualità dell'esempio è più importante della somiglianza alla frase di origine.
• I sistemi Specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme da MQ:

• La fluidità del PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
• "Stile/Accadde" generalmente ha prestazioni più basse per PaLM.</sample>
    <sample id="899">Risultati sperimentali

• La qualità esempio è più importante del simile alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme da MQ:

• La fluidità di PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
• "Stile/Accadde" generalmente più basso per PaLM.</sample>
    <sample id="900">Risultati sperimentali

• La qualità dell'esempio è più importante della somiglianza alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme di informazioni da MQM

• La fluidezza del PaLM è paragonabile ai sistemi SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominate da "accuratezza/omissione"
    • "Stile/Accadimento" generalmente più basso per PaLM.

Google</sample>
    <sample id="901">Risultati sperimentali

• La qualità dell'esempio è più importante del grado di similitudine alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PalM si avvicina a Google Translate.

Insights da MQG

• La fluidezza di PalM è paragonabile ai sistemi SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominate da "accuratezza/omissione"
    • "Stile/Accadimento" generalmente più basso per PalM.</sample>
    <sample id="902">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme da MQ

• La fluidezza del PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
    • "Stile/Accadimento" generalmente più basso per PaLM.

Google</sample>
    <sample id="903">Risultati sperimentali

• La qualità esempio è più importante del parziale.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PALM vicino a Google Translate.

Consegne da MQM

• La fluidezza del PALM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
    • "Stile/Accadimento" generalmente più basso per PALM.

Google B</sample>
    <sample id="904">Risultati sperimentali

• La qualità esempio è più importante del simile alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Consegne da MQG

• La fluidezza del PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
    • "Stile/accademico" generalmente più basso per PaLM.</sample>
    <sample id="905">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PALM si avvicina a Google Translate.

Insieme da MQ:

• La fluidezza del PALM è comparabile ai sistemi SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
• "Stile/Accadimento" generalmente più basso per il PALM.</sample>
    <sample id="906">Grazie</sample>
    <sample id="907">Weaker Than You Think: Un'Analisi Critica sul Machine Learning con Poche Dati Supervisionati

Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow

Sarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="908">Weaker Than You Think: Un'Analisi Critica sul Machine Learning con Poche Dati Supervisionati

Dawei Zhu1, Xiaoyu Shen2, Marius Mosbach3, Andreas Stephan3, Dietrich Klakow1

1 Saarland University
2 Amazon Alexa
3 University of Vienna

61° ACL 2023</sample>
    <sample id="909">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But noise labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="910">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="911">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="912">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="913">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="914">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="915">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="916">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%"</sample>
    <sample id="917">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%"</sample>
    <sample id="918">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="919">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="920">RQ1 Main findings Validation on weak labels No validation on validation on weak labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="921">RQ1 Main findings</sample>
    <sample id="922">Risultati principali</sample>
    <sample id="923">RQ1 Main findings

Validation on weak labels Validation on validation set No validation Validation on clean labels
FT_w. BOND COSINE MLC L2R</sample>
    <sample id="924">Risultati principali</sample>
    <sample id="925">RQ2 Main findings</sample>
    <sample id="926">RQ2 Main findings</sample>
    <sample id="927">Risultati principali</sample>
    <sample id="928">RQ2 Main findings

FTw COSINE L2R MLC Weak labels All

FTc LoRAc BiFiTc Adapterc

Performance Deltas (%)</sample>
    <sample id="929">RQ2 Main findings

In this graph, we can see the performance of different weakly supervised learning (WSL) approaches on a dataset. The x-axis represents the number of clean validation samples, and the y-axis represents the accuracy of the models. The different lines represent different WSL approaches, such as FT_w, COSINE, L2R, MLC, and Adapterc.

The graph shows that the performance of all WSL approaches improves as the number of clean validation samples increases. However, the FT_w approach achieves the highest accuracy, followed by COSINE, L2R, MLC, and Adapterc. This suggests that FT_w is the most effective WSL approach for this dataset.

The graph also shows that the performance of all WSL approaches is lower than the performance of the model trained on all labels. This is because the WSL approaches are trained on a smaller dataset, which results in lower accuracy. However, the performance of the WSL approaches is still significantly better than the performance of a random model.

Overall, this graph provides evidence that WSL approaches can be effective for improving the performance of machine learning models on datasets with limited labeled data.</sample>
    <sample id="930">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT COSINE ER FT+ Clean Only</sample>
    <sample id="931">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT</sample>
    <sample id="932">RQ3 Main findings N=10 clean samples per class N=30 samples per class Before CFT After CFT Before CFT After CFT</sample>
    <sample id="933">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Accuracy/F1 Before After Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="934">Conclusione Approcci recenti WSL richiedono campioni puliti. Sottestimano la loro praticità. Recomendaciones Nuestro Report the criterios de selección del modelo. Utiliza los enfoques de aprendizaje por pocos ejemplos como referencias. Siempre aplica el ajuste continuo (AFT).</sample>
    <sample id="935">Conclusione Approcci recenti WSL Richiedono campioni puliti. Sottestimano la loro praticità. Noi raccomandiamo Reportare le critiche alla selezione del modello. Utilizzare approcci di apprendimento a basso numero di etichette come baselines Sempre applicare la continuazione di ottimizzazione (CFT).</sample>
    <sample id="936">Conclusione

Approcci recenti di WSL

• Richiedono campioni puliti.
• Sottestimano la loro praticità.

Consigli our

• Riferirsi ai criteri di selezione del modello.
• Utilizzare gli approcci di imparazione a basso numero di etichette come baselines.
• Sempre applicare l'ottimizzazione continuo (OFT).</sample>
    <sample id="937">Conclusione Approcci recenti WSL Richiedono campioni puliti. Sottestimano la loro praticità. Noi raccomandiamo Reportare le critiche alla selezione del modello. Utilizzare approcci di apprendimento a basso numero di etichette come baselines Sempre applicare la continuazione fin-tuning (CFT).</sample>
    <sample id="938">Conclusione Approcci recenti WSL
- Richiedono campioni puliti.
- Sottestimano la loro praticità.
Nosre raccomandazioni
- Riferimento ai criteri di selezione del modello.
- Utilizzo di approcci di imparazione a basso numero di etichette come baselines.
- Sempre applicare l'ottimizzazione continuo (OFT).</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo includono la valutazione umana, la valutazione automatica e la valutazione basata su metriche.</sample>
    <sample id="940">C'è un articolo con 5 autori.</sample>
    <sample id="941">Per comprendere l'esempio con Servin e Kea, è necessario conoscere le professioni di Servin e Kea, il luogo in cui hanno incontrato e la loro attività successiva.</sample>
    <sample id="942">Yes, the code is available. It can be found on GitHub at mpoemsl/kitmus.</sample>
    <sample id="943">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="944">Frasi con prefissi/suffissi, avverbio lungo prefissi, aggiungere e mettere in evidenza, regardless di X, citato.</sample>
    <sample id="945">La valutazione dimensionale si riferisce alla capacità di assegnare un punteggio specifico a un determinato argomento o oggetto in base a diversi criteri. In questo contesto, la valutazione Likert è utilizzata per valutare il grado di consenso o approvazione verso un argomento specifico.</sample>
    <sample id="946">I ricercatori sono affiliati all'Università di Scienze e Tecnologia di Cina, Microsoft Research Asia e alla Università di Beijing Haotong.</sample>
    <sample id="947">Se presenti, la forma del prompting si rivela importante.</sample>
    <sample id="978">The authors evaluated three models of dialogue: Turn Likert, Dialogue Likert, and Incremental Validity.</sample>
    <sample id="979">Ci sono 7 autori che hanno collaborato all'articolo.</sample>
    <sample id="980">I qualità ideali di un buon pianificatore sono la capacità di elaborare e implementare pianificazioni efficaci che massimizzano l'efficienza e la performance.</sample>
    <sample id="981">To determine the number of authors involved in the article, we need to carefully examine the list of names provided on the slide. The slide lists the following authors:

1. Siyu Yuan
2. Jianqiang Chen
3. Ziqian Fu
4. Xuyang Ge
5. Soham Shah
6. Charles Robert Jankowski
7. Yanghua Xiao
8. Deqing Yang

By counting each name in the list, we can see that there are a total of 8 authors involved in the article.

Therefore, the answer is: There are 8 authors involved in the article.</sample>
    <sample id="982">La relatrice o il relatore è Vasudha Varadarajan.</sample>
    <sample id="983">I ricercatori che hanno scritto l'articolo sono affiliati all'Institute of Computer Science dell'Accademia Polacca delle Scienze e all'University of Warsaw.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono gli omissioni e gli stile/awkward.</sample>
    <sample id="1022">Non dimenticare i tuoi ABC: valutazione dello stato dell'arte in sistemi di dialogo orientati al chat</sample>
    <sample id="1023">Non dimenticare i tuoi ABC: valutazione dello stato dell'arte in sistemi di dialogo orientati al chat</sample>
    <sample id="1024">Non dimenticare i tuoi ABC: valutazione dello stato dell'arte dei sistemi di dialogo orientati al chat</sample>
    <sample id="1025">Comparative Evaluation</sample>
    <sample id="1026">Evaluazione del punteggio Likert</sample>
    <sample id="1027">Dimensioni della qualità del dialogo</sample>
    <sample id="1028">Valutazione della rilevanza tramite una scala Likert</sample>
    <sample id="1029">Lavoro in corso</sample>
    <sample id="1030">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1031">Behaviors</sample>
    <sample id="1032">Behaviors</sample>
    <sample id="1033">Behaviors</sample>
    <sample id="1034">Behaviors</sample>
    <sample id="1035">Sperimenti

- 4 modelli di dialogo aperti
- 100 conversazioni tra umano e bot per modello
- ABC-Eval

[Immagine: Due figure rappresentative di un umano e un bot, con due bubble che hanno il testo "Q1" e "A1", e due arrow che indicano la direzione del flusso di conversazione]

[Emory University] [Alexa]</sample>
    <sample id="1036">Sperimenti

- 4 modelli di dialogo dominanti
- 100 conversazioni umano-bots per modello

ABC-Eval Turn Likert Dialog Likert Comparativo

[Immagine con icone e diagrammi]

[Emory University] [Alexa]</sample>
    <sample id="1037">Evaluazione del Baseline</sample>
    <sample id="1038">Inter-Annotator Agreement</sample>
    <sample id="1039">Validità predittiva</sample>
    <sample id="1040">Sono felice di presentare il grafico "Validità predittiva" che rappresenta i percentuali di qualità eseguiti (R²) per diversi aspetti della qualità del linguaggio umano e dell'interazione con la voce assistente. Il grafico è suddiviso in diverse sezioni, ciascuna rappresentante un diverso tipo di valutazione: ABC-Eval, Turn Likert, Dialogue Likert e Comparative. Ognuna di queste sezioni include diverse dimensioni, come la coerenza, la comprensione, l'empatia, la chiarezza, la grammatica, la considerazione, la professionalità, la sicurezza, la sensibilità emotiva e la professionalità.

Il grafico utilizza colori diversi per distinguere tra le dimensioni e le tipologie di valutazione. Ad esempio, la coerenza e la professionalità sono rappresentate in rosso, mentre la comprensione e la considerazione sono in blu. La grammatica è rappresentata in grigio, mentre la sicurezza e la sensibilità emotiva sono in verde. Infine, la professionalità è rappresentata in nero.

Inoltre, il grafico include una barra verticale che rappresenta la percentuale di qualità eseguita per ogni dimensione. Questa barra è suddivisa in due parti: la parte inferiore rappresenta la qualità dell'interazione con la voce assistente, mentre la parte superiore rappresenta la qualità dell'interazione umana. Questa rappresentazione consente di confrontare direttamente le prestazioni della voce assistente rispetto a quella umana in diverse dimensioni.

In generale, il grafico fornisce una rappresentazione visiva dei risultati delle valutazioni della qualità del linguaggio umano e dell'interazione con la voce assistente. Questa rappresentazione consente di identificare le dimensioni in cui la voce assistente表现出色，以及需要改进的领域。</sample>
    <sample id="1041">Incremental Validity</sample>
    <sample id="1042">Incremental Validity</sample>
    <sample id="1043">Incremental Validity</sample>
    <sample id="1044">Il grafico rappresenta le tassi di errori percentuali per diversi modelli di linguaggio in base a diverse tipologie di errori durante le conversazioni. Le tipologie di errori inclusi sono Antisocial, CS Contr., Ignore, Incorrect, Irrelevant, Unempathetic, Other Contr., Redundant, Self Contr., Topic Switch, e Uninterpretable. I modelli analizzati sono BART-FID-RAG, Blender2, Emora, e Blender-Decode. Il grafico utilizza colori diversi per distinguere tra le diverse tipologie di errori e i vari modelli. Ad esempio, il modello BART-FID-RAG ha un alto tasso di errori per Antisocial e CS Contr., mentre il modello Blender2 ha un tasso di errori più basso per le stesse tipologie di errori. In generale, il grafico fornisce una panoramica delle prestazioni dei modelli di linguaggio in termini di errori durante le conversazioni.</sample>
    <sample id="1045">ABC-Eval Error Rates by Model</sample>
    <sample id="1046">Ecco una versione in italiano del contenuto:

Il grafico illustra le tassi di errori percentuali per diversi modelli di linguaggio in base a diverse tipologie di errori durante le conversazioni. Le tipologie di errori considerate includono antisocial, CS contrassegnato da un segno di sospetto, ignorare, incorretto, irrelevante, incomprensivo, altro contrasto, redundante, se stessi e scambi di argomento. I modelli analizzati sono BART-FID-RAG, Blender2, Emora e Blender-Decode.

Il grafico utilizza colori diversi per rappresentare diversi modelli, con i colori più chiari rappresentando i modelli più recenti o più avanzati. Ad esempio, il modello BART-FID-RAG è rappresentato in blu chiaro, mentre il modello Blender2 è rappresentato in verde chiaro.

I modelli analizzati hanno tassi di errori percentuali che variano notevolmente tra le diverse tipologie di errori. Ad esempio, il modello BART-FID-RAG ha un tasso di errori percentuali elevato per le tipologie di errori antisocial e CS, mentre il modello Blender2 ha un tasso di errori percentuali elevato per le tipologie di errori ignorare e incorretto.

In generale, il grafico fornisce un'analisi dettagliata delle prestazioni dei modelli di linguaggio in diverse tipologie di errori durante le conversazioni.</sample>
    <sample id="1047">Ecco una versione in italiano del contenuto:

Il grafico illustra le tassi di errori percentuali per diversi modelli di linguaggio in base a diverse tipologie di errori. Le tipologie di errori considerate sono: Antisocial, CS Contr., Ignore, Incorrect, Irrelevant, Unempathetic, Other Contr., Redundant, Self Contr., Topic Contr., e Uninterpretable. I modelli analizzati sono BART-FID-RAG, Blender2, Emora, e Blender-Decode. Il grafico utilizza colori diversi per distinguere tra le tipologie di errori e i modelli. Ad esempio, i modelli BART-FID-RAG e Blender2 hanno tassi di errori relativamente bassi per la maggior parte delle tipologie, mentre il modello Blender-Decode ha un tasso di errori più elevato per le tipologie di errori come Antisocial e CS Contr.</sample>
    <sample id="1048">The affiliations of the authors are Emory University, Emory NLP, and Alexa Research Lab.</sample>
    <sample id="1049">CFT significa continuos fine-tuning.</sample>
    <sample id="1050">Per rispondere alla domanda, analizziamo la descrizione fornita dell'immagine. L'immagine include il titolo dell'articolo, il nome del convegno (ACL 2023), e una lista di autori. 

Passo 1: Identificare i nomi degli autori.
- Koustuv Sinha
- Jon Gauthier
- Aaron Mueller
- Kanishka Misra
- Keren Fuentes
- Roger Levy
- Adina Williams

Passo 2: Contare il numero di autori elencati.
Ci sono 7 nomi di autori elencati.

Risposta: Ci sono 7 autori coinvolti nell'articolo.</sample>
    <sample id="1051">Quando la traduzione richiede contesto? Esplorazione multilingue guidata da dati</sample>
    <sample id="1052">Traduzione dipende dal contesto We'll have to get rid of that mole.</sample>
    <sample id="1053">Traduzione dipende dal contesto. Queste cose potrebbero iniziare a diventare pericolose se i ministri scoprono. Dovranno liberarsi di quella ragna.</sample>
    <sample id="1054">Traduzione dipende dal contesto Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1055">Evaluating context-dependent translation is hard Only a small portion of words depend on context</sample>
    <sample id="1056">Evaluating context-dependent translation is hard Only a small portion of words depend on context Corpus-level metrics Existing methods support limited discourse phenomena and languages</sample>
    <sample id="1057">Q1: In che modo richiede la traduzione contesto? Q2: Come gestiscono i modelli traduzioni dipendenti dal contesto?</sample>
    <sample id="1058">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1059">Condizionale Cross-Mutual Information (CXMI)</sample>
    <sample id="1060">Condizionale Cross-Mutua Informazione (CXMI)

- CXMI: misura quanto contesto i modelli MT usano dato un corpus

    Uncertainty over translations given the source
    \[H_{qMT\alpha}(Y|X)\]

    \[H_{qMT\alpha}(Y|X, C)\]

    Uncertainty over translations given the source AND context
    \[CXMI(C) \rightarrow Y|X\]</sample>
    <sample id="1061">Introduciamo P-CXMI per misurare l'uso del contesto per tradurre un particolare</sample>
    <sample id="1062">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1063">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1064">Analisi tematica di parole high P-CXMI</sample>
    <sample id="1065">1. POS tags</sample>
    <sample id="1066">Analisi tematica di parole P-CXMI elevate 1. Tag POS P-CXMI per le etichette in En-Ar SING DUAL PLUR Pronouns</sample>
    <sample id="1067">Analisi tematica di parole elevate P-CXMI 1. Tag POS 2. Parole lessicali - Pronouns - Verbi</sample>
    <sample id="1068">1. POS tags 2. Vocabulary items - Pronouns - Verb form - Lexical cohesion Avellie's mother was still asleep. Avellie went to school. 阿維利的母亲还在睡觉。 阿維利去了学校。</sample>
    <sample id="1069">1. POS tags 2. Vocabulary items</sample>
    <sample id="1070">1. Tagi POS 2. Parole vocabolari 3. Token individuali - Pronouns - Verbe in forma - Coinerenza lessicale - Formalità - Ellissi</sample>
    <sample id="1071">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto? - Benchmark Multilingue Discorso-Aware (MuDA)</sample>
    <sample id="1072">Tagger Multilingua Discourse-Aware (MuDA) - Pronouns - Verbe in forma - Coincisione lessicale - Formalità - Ellissi</sample>
    <sample id="1073">Tagger Multilingua Discours-Aware (MuDA)</sample>
    <sample id="1074">MuDA benchmark

Inviare i documenti
MuDA tagger
Inviare i documenti
Inviare i documenti
Inviare i documenti</sample>
    <sample id="1075">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto al livello delle parole - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto? - Benchmark Multilingue Discorso-Aware (MuDA) - Valutazione del modello</sample>
    <sample id="1076">Corpus-level metrics BLEU</sample>
    <sample id="1077">Metriche a livello di corpus</sample>
    <sample id="1078">Metrici a livello di corpus</sample>
    <sample id="1079">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion</sample>
    <sample id="1080">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1081">Risultati del benchmark MuDA

• I modelli consapevoli del contesto performano significativamente meglio su alcuni fenomeni
	- √: Formalità, coesione lessicale ×: Ellissi, pronouns, forma del verbo
• DeepL supera Google in maggior parte dei fenomeni e lingue
	- Simbolo di confronto tra DeepL e Google</sample>
    <sample id="1082">Riassunto

Identify discourse phenomena systematically without prior linguistic knowledge

Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1083">Riassunto

Identify discourse phenomena systematically without prior linguistic knowledge

Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1084">La relatrice o il relatore è Yufen Zhang.</sample>
    <sample id="1121">Il nuovo metodo non ha un nome.</sample>
    <sample id="1122">In relazione alle "parole contrassegnate", l'autore del metodo ha descritto le parole che distinguono i personaggi dei gruppi etichettati da quelli non etichettati.</sample>
    <sample id="1123">Paul G. Allen School, UW NLP, Carnegie Mellon University Language Technologies Institute, Russian Academy of Sciences</sample>
    <sample id="1124">La prima struttura de dipendenze simmetrica menzionata è la "Multi-headed/London".</sample>
    <sample id="1125">The speaker's name is Sarah E. Finch.</sample>
    <sample id="1126">To determine the number of authors involved in the article, we need to carefully examine the text provided in the image. The image contains a list of names followed by the affiliation "Google Research." 

Here are the steps to find the answer:

1. Identify the list of names: The names listed are Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.
2. Count the number of names: There are four distinct names mentioned.

Therefore, the total number of authors involved in the article is 4.</sample>
    <sample id="1127">La tabella fornisce due esempi di insiemi di test che possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="1161">FT, W, BOND, COSINE, MLC, L2R</sample>
    <sample id="1162">Il modello viene valutato su 11 attività.</sample>
    <sample id="1226">CamemBERT è inizialmente addestrato su 4GB di dati.</sample>
    <sample id="1227">Adam Przepiórkowski and Michał Woźniak</sample>
    <sample id="1228">I grafici hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">È necessario permutare i token per la sequenza di output per ottenere la risposta corretta.</sample>
    <sample id="1270">Perché gli autori hanno suggerito ai proprietari dei modelli di aumentare la trasparenza sui metodi di mitigazione dei bias per fornire maggiori informazioni su come i modelli si adattano e come i bias vengono mitigati.</sample>
    <sample id="1271">I due input inaccettabili di coppia minima sono "Molti hanno aiutato se stessi" e "Molti hanno aiutato se stessi".</sample>
    <sample id="1272">I ricercatori hanno utilizzato diverse metriche di valutazione, tra cui la precisione, il ricupero e la F1.</sample>
    <sample id="1273">La metrica utilizzata per misurare l'accordo tra gli annotatori è la consistenza.</sample>
    <sample id="1274">The domain chosen for adding completely unrelated sentences to unacceptable and acceptable queries is Wikipedia.</sample>
    <sample id="1275">Gli autori dell'articolo sono affiliati all'Heinrich Heine University di Düsseldorf, in Germania.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto utilizza un modello pre-addezzato per la traduzione e l'interpretazione, nonché per la generazione di testi in diverse lingue.</sample>
    <sample id="1277">To determine the number of authors involved in the article, we need to carefully examine the provided image. The image contains a title slide from a presentation or paper. The title reads "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems." Below the title, the names of the authors are listed as follows:

- Sarah E. Finch
- James D. Finch
- Jinho D. Choi

By counting these names, we can see that there are three distinct authors mentioned. Therefore, the number of authors involved in the article is 3.</sample>
    <sample id="1278">La coordinazione binaria è quando due o più governanti collaborano per raggiungere un obiettivo comune.</sample>
    <sample id="1279">I prompt sono stati utilizzati per un periodo di 10 giorni.</sample>
    <sample id="1280">I modelli più piccoli specializzati possono superare i modelli a larga gamma in termini di qualità dei testi generati.</sample>
    <sample id="1281">DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedici e clinici

Yanis Labarre1,4 Adrien Bazille2,3 Richard Dufour2 Mickael Rouvier1 Emmanuel Morin2,3 Beatrice Daillie2,3 Pierre-Antoine Gourraud3

(1) LIA, Avignon Université (2) LS2N, Université de Nantes (3) CHU des donnees, CHU de Nantes (4) Zenith

ISoL2N Laboratoire d'Informatique, Signal et Apprentissage Nantes Université Centre national de la santé et des services sociaux CNRS</sample>
    <sample id="1282">I. Modellatura del linguaggio in salute II. Confronto di strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 task IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1283">I. Modellatura del linguaggio in salute II. Confronto di strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1284">I. Modello di Linguaggio in Sanitaria II. Confronto tra strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1285">I. Modello di Linguaggio in Sanitaria II. Confronto tra strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1286">Approcci basati su Transformer, come BERT, offrono un'enorme guadagnata di prestazioni su una vasta gamma di compiti di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su compiti medici, i modelli specifici del dominio hanno superato anche in lingua inglese. Pubblicati da BERT, BioBERT, ClinicalBERT e altri. Lingue diverse da inglese sono più rare e si basano principalmente su pre-addestramenti continuativi utilizzando un modello esistente generico. Non esiste un modello open-source gratuito per il dominio biomedico in francese. Un modello specifico del dominio BERT per il francese dovrebbe aumentare le prestazioni sui compiti medici.</sample>
    <sample id="1287">Approcci basati su Transformer, come BERT, offrono un'enorme guida in un'ampia gamma di compiti di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su compiti medici, i modelli specifici del dominio hanno alzato il livello anche in inglese con PudMedBERT, BioBERT, ClinicialBERT e altri. Le lingue diverse dall'inglese sono più rare e si basano principalmente su pre-addestramenti continuativi utilizzando un modello esistente generico. Non esiste un modello open-source gratuito per il dominio biomedico in francese, ma un modello specifico del dominio BERT dovrebbe aumentare le prestazioni sui compiti medici.</sample>
    <sample id="1288">Modellazione del linguaggio

- Approcci basati su Transformer, come BERT, offrono un'enorme miglioria delle prestazioni su molte attività di NLP.
- Sono stati adattati al francese con CamemBERT e FlauBERT
- Su attività mediche, i modelli specifici del dominio hanno superato anche queste prestazioni
    - PudMedBERT, BioBERT, ClinicialBERT e altri
- Le lingue diverse dall'inglese sono più rare e si basano principalmente su modelli pre-addestrati continuamente utilizzando un modello generico esistente
- Non esiste un modello generico aperto sorgente per il dominio biomedico in francese
- Un modello specifico del dominio basato su BERT per il francese dovrebbe aumentare le prestazioni su attività mediche</sample>
    <sample id="1289">Approcci basati su Transformer, come BERT, offrono un'enorme migliora delle prestazioni su molte attività di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su compiti medici, i modelli specifici del dominio in inglese hanno alzato il livello anche ulteriormente. Pubblicati da BERT, BioBERT, ClinicallyBERT e altri. Lingue diverse da inglese sono più rare e si basano principalmente su pre-addestramenti continuativi utilizzando un modello esistente generico. Non esiste un modello open-source generico per il dominio biomedico in francese. Un modello specifico del dominio BERT dovrebbe aumentare le prestazioni su compiti medici.</sample>
    <sample id="1290">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati comparabili

* Strategie di addestramento pre-addestrate e fonti di dati
	+ NACHOS: un dataset open-source di 1.1M parole di diverse discipline mediche, stili e tipi di documenti privati
	+ Un dataset privato di frasi estratte da 1.7M registri medici annullati dalla banca dei dati universitari del Nantes

* Strategie di imparare da zero e addestrare continuamente
	+ Addestramento da zero con costruzione completa
	+ Continuare l'addestramento utilizzando un modello pre-addestrato esistente (es. CamMedBERT, un modello generico francese e PubMedBERT)

Fonti:
- Université d'Avignon</sample>
    <sample id="1291">Evaluazione dell'impatto di fonti di dati medici pubbliche e private in dimensioni confrontabili. NACHOS: un dataset open-source di 1,1 m word che copre diverse discipline mediche, stili e tipi di documenti. Sorgente: corpus di testo. Dati privati: insieme di frasi estratte da 1,7 m di registri sanitari annullati, provenienti dal Nantes University Hospital Data Warehouse.</sample>
    <sample id="1292">Evaluazione dell'effetto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili

* Strategie di pre-addestramento confrontate
o NACHOS: un insieme di 1,1 M di parole open-source di diverse discipline mediche, raccolte da diverse fonti
o NBDW: un insieme privato di 1,7 M di frasi estratte da record anonimizzati dei registri ospedalieri del Nantes University Hospital Data Warehouse
* Strategie di apprendimento confrontate
o Dall'origine: costruzione completa dallo zero
o Continuos: addestramento continuo utilizzando un modello pre-addestrato esistente
o CamMedBERT: un modello generico francese
o PubMedBERT: un modello pre-addestrato su articoli di PubMed

Tabella: confronto delle prestazioni tra modelli di apprendimento

| Model name | From scratch | NACHOS | NBDW |
| --- | --- | --- | --- |
| DeBERTa | 7,4 GB | 5,3 GB | 6,5 GB |
| RoBERTa | 7,4 GB | 5,3 GB | 6,5 GB |
| CamMedBERT | 4,4 GB | 4,3 GB | 6,4 GB |

Fonti:
- NACHOS: A 1.1M words open-source dataset of diverse medical domains, collected from various sources.
- NBDW: A private dataset of 1.7M sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital Data Warehouse.
- Comparison of learning strategies: From scratch (full model construction), Continuos (training on existing pre-trained models), and specific models like DeBERTa, RoBERTa, CamMedBERT, and PubMedBERT.

Note:

* The table compares the performance of different models in terms of memory usage (in GB) for processing the NACHOS and NBDW datasets.
* NACHOS is a publicly available dataset, while NBDW is a private dataset.
* The comparison aims to evaluate the impact of using public and private medical data sources on the performance of pre-trained models in the medical domain.</sample>
    <sample id="1293">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati comparabili

* Strategie di pre-addestramento confrontate
	+ NACHOS: un insieme di 1.1 milioni di parole aperto-sorgente di diverse discipline mediche, raccolti da varie fonti
	+ Un dataset privato di frasi estratte da 1.7 milioni di registri sanitari anonimizzati estratti dal Nantes University Hospital data warehouse
* Strategie di apprendimento confrontate
	+ Dall'origine: costruzione completa da zero
	+ Continuos: addestramento continuo utilizzando un modello pre-addestrato esistente
	+ Continuos: utilizzo di modelli generici come CamemBERT e PubMedBERT

Tabella: Confronto delle prestazioni dei modelli

| Model name | From scratch | From NACHOS |
| --- | --- | --- |
| DeBERTa |  |  |
| RoBERTa |  |  |
| ChuteBERT |  |  |
| NACHOS |  |  |
| NACHOS |  |  |
| NACHOS |  |  |
| NACHOS |  |  |
| NACHOS |  |  |

Avignon Université</sample>
    <sample id="1294">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni dei dati confrontabili. NACHOS: un dataset open-source di 1,1 miliardi di parole coprendo diverse discipline mediche, stili e tipi di documenti. NBERT: un dataset privato di frasi estratte da 1,7 miliardi di record anonimizzati dalla banca dati universitaria dei Nantes. Comparazione delle strategie di impianto: Costruito dal zero con costruzioni complete Continuos: continuazione dell'addestramento utilizzando un modello pre-addestrato Modello nome From scratch From NACHOS DifBERT From NACHOS CamBERT From NACHOS ChBERT From scratch PubMedBERT Continuos pre-addestrato CamBERT From NACHOS NBERT: modello generico francese pre-addestrato su NACHOS</sample>
    <sample id="1295">Evaluazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati confrontabili. NACHOS: un insieme di 1,1 miliardi di parole di fonte aperta, rilevato in diverse discipline mediche, tipi di stili e tipi di ambienti. NBERT: un insieme privato di frasi estratte da 1,7 miliardo di registri sanitari annullati raccolti dallo spazio di lavoro dei hospitales universitari. Comparazione di strategie di imposizione: • Dalla sbarra con costruzione completa • Continuando ad allenarsi utilizzando un modello pre-addestrato esistente • Utilizzo di modelli pre-addestrati come CammedBERT, un modello generico francese, e PubMedBERT.</sample>
    <sample id="1296">Evaluazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili

* Strategie di pre-addestramento confrontate
	+ NACHOS: un insieme di 1.1 milioni di parole aperto-sorgente di diverse discipline mediche, raccolti da diverse fonti
	+ NBWD: un insieme privato di 1.7 milioni di frasi estratte da registri sanitari annullati del Nantes University Hospital Data Warehouse
* Strategie di imparare confrontate
	+ Dall'origine con costruzione completa
	+ Continuando ad addestrarsi utilizzando un modello pre-addestrato esistente
	+ Utilizzando modelli pre-addestrati come CamemBERT, un modello generico francese, e PubMedBERT</sample>
    <sample id="1297">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati comparabili

* Strategie di pre-addestramento confrontate
	+ NACHOS: un dataset open-source di 1.1M parole copertore di diverse discipline mediche, stili e tipi di documenti
	+ Dati privati: insieme di frasi estratte da 1.7M record di registri sanitari annullati dalla banca dei dati dell'ospedale universitario di Nanterre
* Strategie di apprendimento confrontate
	+ Modello ad hoc: costruito da zero con un insieme di addestramento completo
	+ Continuos: addestrato utilizzando un modello pre-addestrato esistente (CamMedBERT)
	+ PubMedBERT: un modello generico francese
	+ NACHOS: modello pre-addestrato

Il slide illustra una comparazione delle strategie di pre-addestramento e delle fonti di dati utilizzate per lo sviluppo di modelli di apprendimento automatico in campo medico. L'analisi si concentra sul confronto tra due tipi di fonti di dati: le fonti di dati medici pubbliche e le fonti di dati medici private. Inoltre, il slide fornisce una tabella che elenca diversi modelli di apprendimento automatico e le loro origini relative. Questa presentazione è stata fornita da Avignon Université.</sample>
    <sample id="1298">Evaluazione di 13 fonti e dimensioni

• La nostra valutazione è stata eseguita su 13 fonti di documentazione
• Le dimensioni dei dataset sono state analizzate sia in termini di numero di documenti che di dimensioni di testo
• I dataset hanno variati da 100 a 1 milione di documenti
• I dataset hanno anche variato in termini di dimensioni di testo, dalla media di 100 caratteri a quella di 10000 caratteri
• I dataset hanno anche variato in termini di tipologia di documenti, tra documenti medici, documenti di testi e documenti di articoli scientifici</sample>
    <sample id="1299">Evaluazione: sorgente dei dati e dimensioni

* Performance evaluation of 13 models on 11 public and private tasks
* Our fine-tuned model achieves state-of-the-art performance on almost all tasks

| Model | Medical Report Specialties | MUSCA-DET | MUSCA-DET+ISSA | CAS | CAS+EMO | QUADRO-EMOA | QUADRO-MEDINA |
| --- | --- | --- | --- | --- | --- | --- | --- |
| General | 44.20 ± 15.89 | 92.78 | 93.41 | 91.27 | 89.69 | 94.62 | 90.83 |
| Biomedical | BioBERT v1.1 | 46.01 ± 7.90 | 89.38 | 98.80 | 81.91 | 81.89 | 84.29 |
| Clinical | ClinicalBERT NACOS | 44.70 ± 17.27 | 88.77 | 98.55 | 82.59 | 82.44 | 91.44 |
| Clinical | ClinicalBERT NACOS+SNOW | 45.59 ± 19.87 | 89.44 | 98.77 | 82.59 | 82.44 | 91.44 |
| Clinical | ClinicalBERT NACOS+SNOW+med | 49.14 ± 13.86 | 93.00 | 99.13 | 91.71 | 93.75 | 90.52 |

Il modello ClinicalBERT NACOS+SNOW+med ha ottenuto i migliori risultati in molte delle attività, dimostrando la sua capacità di adattarsi a diverse specializzazioni mediche e di migliorare le prestazioni generali.</sample>
    <sample id="1300">Evaluazione: 13 fonti di dati e dimensione

Performance evaluation of 13 models on 11 public and private tasks
Our fine-tuned model gets state-of-the-art results on almost all tasks

| Model | Medical Report Specialties | MUSCA-DET | MUSCA-DET+ISSA | CAS | CAS+MeMo | QUADRO-EMEA | QUADRO-MEDLINE |
| --- | --- | --- | --- | --- | --- | --- | --- |
| General | BioBERT Oscar 150 GB | 92.4 | 92.7 | 91.2 | 91.7 | 90.3 | 91.6 |
| Biomedical | BioBERT ULI | 89.0 | 90.0 | 89.6 | 90.4 | 89.2 | 90.7 |
| Clinical | BioBERT NACOS | 87.7 | 88.7 | 89.4 | 89.9 | 88.5 | 90.1 |
| Clinical | BioBERT NACOS+MeMo | 93.5 | 94.7 | 93.6 | 94.2 | 92.9 | 94.1 |
| Clinical | BioBERT NACOS+Med | 94.1 | 95.3 | 94.2 | 94.8 | 93.5 | 94.7 |

Il modello BioBERT NACOS+Med ha ottenuto i risultati migliori su quasi tutti i task, dimostrando la sua capacità di adattarsi bene ai vari tipi di report medici. Questo modello è stato in grado di superare i precedenti modelli in termini di precisione e ha dimostrato una maggiore flessibilità nel trattare i report medici di diverse specialità mediche.</sample>
    <sample id="1301">Evaluazione: 13 fonti di dati e dimensioni

* Performance evaluation dei 13 modelli su 11 task pubblici e privati
* I modelli fin-tuned ottengono lo stato dell'arte su quasi tutti i task

|  | BERT | Medical Report Specialties | MUSCA-DET | MUSCA-DET (ISSAL) | CAS | CAS (EMR) | QuaeroMedIC | QuaeroMedIC (EMR) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| General | 42.60 85.41 | 99.44 99.44 | 93.43 93.43 | 93.43 93.43 | 91.27 89.60 | 94.62 93.75 | 90.33 90.33 | 90.33 90.33 |
| Biomedical | RoBERTa v1.1 | 98.38 98.38 | 98.00 98.00 | 98.00 98.00 | 81.91 81.91 | 88.18 88.18 | 91.45 91.45 | 84.29 84.29 |
| Clinical | ClinicalBERT v1.1 | 88.77 88.77 | 93.56 93.56 | 93.56 93.56 | 82.55 82.55 | 88.24 88.24 | 93.79 93.79 | 75.05 75.05 |
| Clinical | DeBERTa NACOS | 93.77 93.77 | 93.77 93.77 | 93.77 93.77 | 91.71 91.71 | 88.76 88.76 | 91.46 91.46 | 78.63 78.63 |
| Clinical | ChatBERT NACOS | 94.14 94.14 | 94.14 94.14 | 94.14 94.14 | 91.71 91.71 | 88.75 88.75 | 91.46 91.46 | 78.63 78.63 |

Il tableau ci fornisce una panoramica delle prestazioni dei modelli di apprendimento automatico su diverse specializzazioni di report medici e dataset di documenti di origine clinica. I modelli vengono valutati su 11 task pubblici e privati, e i risultati sono suddivisi in quattro category: generale, biomedico, clinico e clinico con un focus specifico su ChatBERT.

I modelli fin-tuned (addestrati sui propri dataset) ottengono spesso lo stato dell'arte su quasi tutti i task, dimostrando la loro capacità di adattarsi bene ai nuovi contesti e di ottenere prestazioni superiori a quelle dei modelli non fin-tuned. Questo è particolarmente vero per i task di specializzazione medica, dove i modelli fin-tuned raggiungono percentuali di accuratezza del 99% o più.

Inoltre, i modelli fin-tuned hanno anche dimostrato di essere più robusti rispetto ai modelli non fin-tuned, soprattutto nei task di specializzazione medica. Questo è dovuto alla capacità dei modelli fin-tuned di adattarsi meglio ai nuovi contesti e di ottenere prestazioni superiori a quella dei modelli non fin-tuned.

In conclusione, i modelli fin-tuned hanno dimostrato di essere più adatti per le applicazioni di apprendimento automatico in campo medico, in quanto hanno dimostrato di essere più robusti e di ottenere prestazioni superiori a quella dei modelli non fin-tuned.</sample>
    <sample id="1302">Evaluazione: Addestramento preliminare

* Dall'originale a continuazione addestrato su 4GB di dati
* Task di risoluzione di domande richiedono conoscenza più specifica del dominio per essere in grado di funzionare bene
* Un studio di stabilità del modello dimostra un maggior intervallo di variazione per i modelli addestrati utilizzando Camembert basato su continuo addestramento

Tabella:

| HF | HF Medici Specialità | MUSCA-Det | MUSCA-ESAI | HumanMIMIC | QUADRO-EMEA | QUADRO-MEDIC |
| --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 33,13 80,11 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| Camembert CNN+G | 43,11 79,86 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| Camembert CNN+G+ | 46,01 79,86 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| Camembert CNN+G+ChBERT++ | 46,01 79,86 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| DBERT NACHOS (s) | 45,33 79,87 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| DBERT NACHOS (s) | 45,33 79,87 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| Camembert NWDW | 40,14 81,88 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |
| Camembert NWDW | 40,14 81,88 | 99,20 | 85,44 | 92,10 | 74,18 | 74,11 |

Avignon Université</sample>
    <sample id="1303">Evaluazione: Addestramento pre-scarico

* Dall'originale a continuazione addestrato su 4GB di dati
* Task di risoluzione di domande richiedono conoscenza più specifica del dominio per essere in grado di funzionare bene
* Lo studio della stabilità del modello dimostra un maggior intervallo di variazione per i modelli basati su Camembert addestrati utilizzando addestramento continuo

Tabella:

| Modello | HF | Medici | Specialità | MUSCA-DET | MUSCA-DET | SISAI | CamembertMCL | QUADRO-EMEA | QUADRO-MED |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 33.13 | 80.11 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+G | 43.11 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+T | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+G | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+T | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+G+T | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+G+T+M | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+G+T+M+O | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+G+T+M+O+N | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |
| Camembert CNN+L+G+T+M+O+N+O | 46.01 | 79.86 | 99.20 | 85.44 | 85.44 | 92.42 | 92.42 | 77.41 | 77.41 |

La tabella fornisce una panoramica dei vari modelli di Camembert addestrati utilizzando addestramento continuo e le loro performance in diverse specialità. Come possiamo vedere, i modelli con più componenti hanno tendenza a ottenere performance migliori in molte specialità, inclusa la specialità MUSCA-DET. Questo suggerisce che l'addestramento continuo può migliorare le performance dei modelli di Camembert in diverse specialità.</sample>
    <sample id="1304">Evaluazione: Addestramento pre-scarico

* Dall'originale ad addestramento continuo su 4GB di dati
* Le attività di risoluzione di domande richiedono conoscenza più specifica del dominio per funzionare bene
* Lo studio della stabilità del modello dimostra un maggior intervallo di riproduzione per i modelli basati su Camembert addestrati utilizzando addestramento continuo

Tabella:

| Modello | HF | Medici | Specializzazioni | MUSCA-Det | MUSCA-ESAI | HumanLM | QuattroEMEA | QuattroMEDIC |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 33,13 | 80,11 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert CNN+G | 43,11 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert CLS | 46,01 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert CLS+ | 46,01 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert CLS+Large | 46,01 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert NACOS | 45,73 | 79,87 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert NWD | 46,01 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert NWD+ | 46,01 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |
| Camembert NWD+Large | 46,01 | 79,86 | 99,20 | 85,44 | 95,24 | 92,44 | 74,1 | 74,1 |

Questa tabella fornisce una panoramica di come diversi modelli Camembert hanno performato in diverse specializzazioni e ambienti di test.</sample>
    <sample id="1305">Messaggio principale:

- DrBERT supera i risultati dello stato dell'arte in 9 compiti medico orientati in francese
- Conferma l'utility di addestrare un modello specifico alla medicina in francese
- Le fonti dei dati contano: addestrarsi su dati heterogenei è importante
- NACHOS è più robusto rispetto all'uso di sole dati clinici privati
- Continuare a pretrainare è meglio, ma non è una strategia efficace quando basata su modelli di lingua inglese specifici al dominio
- DrBERT modelli, il dataset NACHOS e i script di addestramento sono liberamente disponibili sotto la licenza MIT

Fonte: Avignon Université</sample>
    <sample id="1306">Messaggio principale:

- DrBERT supera i risultati dello stato dell'arte in 9 compiti medico orientati in francese
- Conferma l'utilità di addestrare un modello specifico alla medicina in francese
- Le fonti dei dati contano: addestrare su dati heterogenei è importante
- NACHOS è più robusto rispetto all'uso di sole dati clinici privati
- Continuare a pre-addestrare è meglio, ma non è una strategia efficace quando basata su modelli di lingua inglese specifici al dominio
- DrBERT modelli, il dataset NACHOS e i script di addestramento sono liberamente disponibili sotto la licenza MIT

Fonte: Avignon Université</sample>
    <sample id="1307">DrBERT supera i risultati dell'art. in 9 compiti medico-orientati in francese</sample>
    <sample id="1308">Grazie. Mi aspetto di condividere informazioni con gli altri al convegno in Toronto. Per maggiori dettagli visitate il sito drbert.univ-avignon.fr</sample>
    <sample id="1309">Le strategie di apprendimento esaminate nel lavoro sono: costruire da zero con full pre-training, continuare il pre-addestramento utilizzando un modello pre-addestrato esistente e utilizzare un modello generico francese chiamato CamMedBERT.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è piccolo.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata utilizzando i parametri BLEU, METEOR e CIDEr.</sample>
    <sample id="1312">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="1313">Composizionale generalizzazioni senza alberi utilizzando etichettatura di multisetti e permutazioni latenti

Matthias Lindemann, Alexander Koller, Ivan Titov

Informatica NLP Centrum Universitair Hogeschool Sarsland Universiteit van Amsterdam</sample>
    <sample id="1314">Il contenuto dell'immagine in inglese è il seguente:

"Compositional Generalization without Trees using Multiset Tagging and Latent Permutations

Matthias Lindemann, Alexander Koller, Ivan Titov

The University of Amsterdam
INL (Informatics)
NLP (Natural Language Processing)
UvA (University of Amsterdam)
Sarland University"

In italiano, il contenuto sarebbe:

"Generalizzazioni composte senza alberi utilizzando multiset tagging e permutazioni latenti

Matthias Lindemann, Alexander Koller, Ivan Titov

Università di Amsterdam
INL (Informatica)
NLP (Processo Naturale del Linguaggio)
UvA (Università di Amsterdam)
Università di Sarandia"</sample>
    <sample id="1315">Abilità di un imparatore di gestire una maggiore ricorsione e composizioni non viste di frasi che hanno essere stati visti individualmente durante la formazione.</sample>
    <sample id="1316">Titolo: Generalizzazioni compostizionali in analisi semantica

Training:

The girl slept.

*girl x : sleep.azione x*

Mary knew that the girl slept.

*girl x : know.agente x, Mary ∧ conmp x, x*</sample>
    <sample id="1317">### Compositional Generalization in Semantic Parsing

**Train:**

- **The girl slept.**
  - `girl x; sleep_agent x, x`

- **Mary knew that the girl slept.**
  - `girl x; sleep_agent x`
  - `Mary Λ know comp x, x`

In questa presentazione, si illustra come il modello di parsing semantico generalizza composizionalmente i fatti. La frase "The girl slept" è analizzata come un'azione eseguita da una soggetto (la "girl") e la relazione tra queste due entità è rappresentata come "sleep_agent". 

La seconda frase, "Mary knew that the girl slept", introduce un nuovo soggetto (Mary) e una nuova azione (know). L'entità che conosce è rappresentata come "know comp", che indica che Mary ha conoscenza di un fatto specifico, cioè che la "girl" ha dormito ("sleep_agent"). 

Questo esempio dimostra come il modello di parsing semantico è capace di generalizzare i fatti in modo composti, utilizzando le informazioni precedenti per interpretare nuove frasi.</sample>
    <sample id="1318">**Titolo:** Generalizzazioni compostizionali in analisi semantica

**Training:**

- **Frase di esempio:** "The girl slept."
  - **Elementi:** "The" (definizione), "girl" (oggetto), "slept" (azione)
  
- **Frase di esempio:** "Mary knew that the girl slept."
  - **Elementi:** "Mary" (oggetto), "knew" (azione), "that" (congiunzione), "the" (definizione), "girl" (oggetto), "slept" (azione)

**Descrizione:** Questa presentazione illustra come un modello di intelligenza artificiale generalizza le strutture grammaticali e semantiche in diverse frasi. L'addestramento include due esempi di frasi, analizzati per identificare i componenti grammaticali e il loro ruolo nell'espressione di idee specifiche.</sample>
    <sample id="1319">**Titolo:** Generalizzazioni compostizionali in analisi semantica

**Train:**

- La bambina dormì.
  - "La bambina" è colorata in verde
  - "dormì" è colorata in rosso
- Mary sapeva che la bambina dormì.
  - "Mary" è colorata in verde
  - "dormì" è colorata in rosso
- Jim disse che Mary sapeva che la bambina dormì.
  - "Jim" è colorato in blu
  - "Mary" è colorata in verde
  - "dormì" è colorato in rosso

**Test:**

- Jim disse che Mary sapeva che la bambina dormì.
  - "Jim" è colorato in blu
  - "Mary" è colorata in verde
  - "dormì" è colorato in rosso</sample>
    <sample id="1320">Compositional Generalization in Semantic Parsing

Train:
- "The girl slept."
  *'girl x sleep_agent x x'*
- "Mary knew that the girl slept."
  *'x know_agent x Mary A know_comp x x sleep_agent x x'*

Test:
- "Jim said that Mary knew that the girl slept."
  *'x say_agent x Jim A say_comp x x A know_comp x x A know_agent x x sleep_agent x x'*

Naive seq2seq models fail!</sample>
    <sample id="1321">Compositional Generalization in Semantic Parsing

Train:
- "The girl slept."
  *'x_1: sleep_agent, x_2: x_1'*
- "Mary knew that the girl slept."
  *'x_1: know_agent, x_2: Mary, x_3: comp, x_4: x_1'*

Test:
- "Jim said that Mary knew that the girl slept."
  *'x_1: say_agent, x_2: Jim, x_3: sayccomp, x_4: x_1, x_5: know_agent, x_6: Mary, x_7: knowccomp, x_8: x_4, x_9: x_1'*

Naive seq2seq models fail!</sample>
    <sample id="1322">Alberi aiutano molto ma...</sample>
    <sample id="1323">Alberi aiutano molto ma...</sample>
    <sample id="1324">I've been working on a project that involves analyzing text data to identify the most common words used in different contexts. I've noticed that certain words tend to appear more frequently than others, and I'm trying to understand why this is the case.

One of the words that stands out is "sleep." It seems to be used in a variety of contexts, from describing physical rest to metaphorical uses like "sleeping on the job" or "sleeping with someone." I've also noticed that "sleep" is often used in conjunction with other words, such as "agent," which could be referring to a person who is responsible for something or a machine that performs a specific function.

Another word that catches my attention is "the." It's a very common word in English, and it's used to refer to specific things or ideas. I've seen it used in a variety of contexts, from describing objects to referring to abstract concepts.

Overall, I'm interested in understanding how these words are used in different contexts and what they can tell us about the way we communicate and think about the world around us.</sample>
    <sample id="1325">Alberi aiutano molto ma...</sample>
    <sample id="1326">Alberi aiutano molto ma...</sample>
    <sample id="1327">I diagrammi di albero sono spesso usati per rappresentare le strutture grammaticali. Questo slide illustra un esempio di come i diagrammi di albero possono essere utilizzati per rappresentare la relazione tra le parole in una frase. In questo esempio, la frase è "The girl slept" e il diagramma di albero rappresenta la struttura grammaticale della frase. Il diagramma di albero ha un nodro principale che rappresenta la parola "The" e due nodi figlio che rappresentano le parole "girl" e "slept". I nodi figlio sono collegati al nodro principale con un legame che rappresenta la relazione grammaticale tra le parole. Questo diagramma di albero può essere utilizzato per analizzare la struttura grammaticale di una frase e per capire come le parole si relazionano tra di loro.</sample>
    <sample id="1328">Trees help a lot but... Trees nced to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1329">Il diagramma illustra un approccio per la comprensione e la rappresentazione di frasi in linguaggio naturale. La frase "the girl slept" è analizzata in modo semantico e grammatico. 

1. **Entità**: Le entità rappresentano i soggetti principali della frase. In questo caso, le entità sono "girl" e "sleep". Queste entità sono rappresentate da due variabili, \(x_1\) e \(x_2\), che identificano rispettivamente il soggetto e l'azione.

2. **Relazione**: La relazione rappresenta la connessione tra le entità. In questo esempio, la relazione è "sleeps", che indica l'azione che il soggetto (la "girl") sta svolgendo.

3. **Tag**: I tag sono etichette utilizzate per categorizzare o identificare le entità e le relazioni. In questo diagramma, i tag sono "girl" e "sleep", che corrispondono rispettivamente alle entità e alla relazione.

4. **Rapporto tra le entità e la relazione**: Le entità \(x_1\) e \(x_2\) sono legate alla relazione "sleeps" tramite i tag. Questo indica che la "girl" (\(x_1\)) sta svolgendo l'azione di "sleep" (\(x_2\)).

In sintesi, il diagramma fornisce una rappresentazione semantica e grammatica della frase "the girl slept" utilizzando entità, relazioni e tag. Questo approccio aiuta a comprendere e analizzare la struttura e il significato delle frasi in linguaggio naturale.</sample>
    <sample id="1330">Approccio</sample>
    <sample id="1331">Il diagramma rappresenta un modello di analisi del linguaggio naturale (NLP) per la comprensione e l'interpretazione di frasi. Il modello è costituito da diverse componenti, ciascuno dei quali ha un ruolo specifico nella processazione del testo.

1. **Input**: L'input è rappresentato da due elementi: "the" e "girl". Questi elementi rappresentano le parole che costituiscono la frase.
2. **Tagging**: Questo è il passo successivo dove le parole dell'input vengono etichettate con tag semanticamente rilevanti. In questo esempio, "the" viene etichettato come "x1" e "girl" come "agent".
3. **Output**: L'output è il risultato finale del modello, che include le parole etichettate e il loro significato relativo alla frase. In questo esempio, "the" è etichettato come "sleep" e "girl" come "agent".

In sintesi, il modello di NLP illustrato nel diagramma analizza una frase, etichetta le parole in base al loro significato e produce un output che rappresenta il significato relativo delle parole nella frase.</sample>
    <sample id="1332">Il diagramma rappresenta un approccio per la comprensione e l'interpretazione di una frase in linguaggio naturale. La frase utilizzata come esempio è "the girl slept". 

1. **Entità**: Le entità sono rappresentate dai blocchi verdi. In questo caso, le entità sono "girl" e "sleep". Questi elementi rappresentano i soggetti o gli oggetti principali della frase.

2. **Relazioni**: Le relazioni sono rappresentate dai blocchi gialli. In questo esempio, la relazione è "slept", che indica l'azione o il verbo che collega le entità.

3. **Agenti**: I fornitori di azioni sono rappresentati dai blocchi blu. In questo esempio, l'agente è "agent", che rappresenta l'entità che esegue l'azione.

4. **Tag**: Il tag è rappresentato dal blocco nero. In questo esempio, il tag è "sleep", che rappresenta l'azione specifica.

5. **Interconnessioni**: Le interconnessioni tra le entità, relazioni e agenti sono illustrate da frecce che collegano i blocchi tra loro. Queste frecce indicano come le entità e gli agenti sono collegati attraverso le relazioni.

In sintesi, il diagramma fornisce una rappresentazione visiva di come le entità, relazioni e agenti vengono utilizzati insieme per interpretare una frase in linguaggio naturale.</sample>
    <sample id="1333">Il diagramma rappresenta un modello di processo che include diverse azioni e elementi. Iniziamo con due azioni, "girl" e "sleep", che hanno due parametri ciascuno, x1 e x2. Queste azioni sono connesse a due agenti, uno per azione. Ognuno degli agenti ha due parametri, x1 e x2, che sono collegati a due variabili, "the" e "girl". Questi parametri vengono permutati e taggati come "sleep" e "agent". Infine, i parametri x1 e x2 vengono collegati a due variabili, "slept" e "agent".</sample>
    <sample id="1334">Il diagramma rappresenta un esempio di permutazione con "salti" in un modello di linguaggio. Iniziamo con una sequenza di token: "the", "girl", "sleep". Questi token vengono suddivisi in due gruppi: "the" e "girl", "sleep". Ora, applichiamo la permutazione con "salti" tra i due gruppi. Questo significa che i token del primo gruppo possono saltare uno o più token del secondo gruppo prima di essere combinati. 

Ad esempio, la permutazione con "salti" potrebbe essere:

1. "the girl sleep"
2. "sleep the girl"
3. "girl the sleep"

Queste sono solo alcune delle possibili combinazioni. La permutazione con "salti" è utilizzata per creare diverse varianti della sequenza originale, rendendo il modello più versatile e capace di gestire diverse situazioni linguistiche.</sample>
    <sample id="1335">Permuting with "jumps"</sample>
    <sample id="1336">Permuting with "jumps"</sample>
    <sample id="1337">Il diagramma rappresenta un esempio di permutazione con "salti" in un contesto linguistico. Iniziamo con una sequenza di token: "* girl *". Questi token vengono permutati in modo tale che il token "girl" si trovi tra due asterismi, diventando "* girl *". Successivamente, i token vengono taggati con le loro rispettive entità grammaticali: il primo token è un segnaposto (*), il token "girl" è un nome proprio, e il terzo token è un segnaposto (*). Infine, i token vengono riconvertiti in una sequenza di parole: "the girl slept".</sample>
    <sample id="1338">Il diagramma rappresenta un esempio di permutazione con "salti" in un modello di linguaggio. Iniziamo con una sequenza di token: "the", "girl", "sleep", "agent". Questi token vengono quindi permutati in modo casuale, creando una nuova sequenza: "sleep", "agent", "girl", "the". Questa permutazione rappresenta un esempio di come i token possono essere rimescolati per generare nuove frasi o modelli di testo.</sample>
    <sample id="1339">Alcuni risultati su COGS (Kim e Linzen 2020) Confronto con altri modelli Treeless su Generalizzazioni Strutturale su COGS Model LSTM seq2seq T3 Ours PP recursion GP recursion Obj PP + Sub PP</sample>
    <sample id="1340">Alcuni risultati su COGS (Kim e Linzen 2020) Confronto con altri modelli Treeless su generalizzazioni structurali su COGS Model LSTM seq2seq T5 Zheng e Lapata Ours PP recursion GP recursion Obj PP + Sub PP</sample>
    <sample id="1341">Technical Challenges We Solve

* **girl**
* **x1**
* **agent**
* **x2**
* **sleep**
* **x3**

Permute

?  
?  
?

Tag

**the**  
**girl**  
**sleep**</sample>
    <sample id="1342">Technical Challenges We Solve

* **girl**
* **x1**
* **agent**
* **x2**
* **sleep**

Permute

? ? ?

Tag

**the**
**girl**
**sleep**

Alignment unknown.</sample>
    <sample id="1343">Technical Challenges We Solve

In this diagram, we see a complex system involving various components and their interactions. The top section is labeled "Permute" and contains several elements: "agent," "sleep," and "x1." Below this, there are three question marks, indicating uncertainty or unknown variables.

The middle section is also labeled "Permute" and includes the word "Tag." This section is connected to the bottom section by two arrows pointing downwards. The bottom section contains the words "the," "girl," and "sleep."

At the very bottom of the diagram, it states "Alignment unknown," suggesting that there is no clear alignment or relationship between the components in the bottom section and those in the middle and top sections.

Overall, the diagram appears to represent a system with multiple layers and interactions, where the alignment and relationships between different components are not yet fully understood.</sample>
    <sample id="1344">Technical Challenges We Solve

In this diagram, we see a complex network of nodes and connections. The top layer is labeled "Permute" and contains various nodes such as "girl," "sleep," and "agent." These nodes are interconnected with arrows pointing in different directions, indicating the relationships or interactions between them.

The middle layer is labeled "Tag" and includes nodes like "the," "girl," and "sleep." This layer also has arrows connecting the nodes, suggesting further relationships or dependencies.

The bottom layer is not explicitly labeled but contains additional nodes and connections. The diagram indicates that there is an "Alignment unknown" issue, which means that the alignment or mapping between the nodes in the different layers is not clear or defined.

The text below the diagram mentions two strategies:
- "Induce it in training": This suggests that the alignment or mapping should be learned or induced during the training process of a model.
- "Rotation model": This implies that a rotation model could be used to address the alignment issue.

Overall, the diagram appears to represent a complex system where different layers (Permute, Tag) interact with each other, and there is a challenge related to aligning these layers correctly. The suggested solutions involve training and using a rotation model to improve the alignment.</sample>
    <sample id="1345">Technical Challenges We Solve

In this diagram, we see a complex system involving various components and their interactions. The system is divided into two main layers: the "Permute" layer and the "Tag" layer. 

In the "Permute" layer, there are several nodes labeled with different words: "girl," "sleep," "agent," and "x1." These nodes are interconnected by arrows, indicating the relationships or transformations between them. Specifically, the "girl" node is connected to the "sleep" node, which in turn is connected to the "agent" node. Additionally, there is an arrow pointing from the "agent" node back to the "sleep" node, suggesting a cyclical relationship.

The "Tag" layer is situated below the "Permute" layer and contains nodes labeled with the words "the," "girl," and "sleep." These nodes are also interconnected by arrows, showing the relationships or transformations between them. The "the" node is connected to the "girl" node, which in turn is connected to the "sleep" node.

Below the "Tag" layer, there is a note that reads "Alignment unknown. Induce it in training." This suggests that the alignment of the components in the system is not yet determined and needs to be established during the training process.

Finally, at the bottom of the diagram, there is a note that reads "Inference model: NP-hard = TSP." This indicates that the inference model used in this system is NP-hard, which means it is computationally intensive and can be solved using the Traveling Salesman Problem (TSP) algorithm.

Overall, this diagram illustrates a complex system involving permutation and tagging processes, with specific focus on the relationships between different components and the need for alignment during training.</sample>
    <sample id="1346">Technical Challenges We Solve

In this diagram, we see a complex network of words and their relationships. The words "girl," "sleep," and "agent" are connected to each other through various paths. There are two main layers in the diagram: one with the words and another layer below it that includes the words "the" and "sleep." The connections between the words are represented by lines, indicating how they relate to each other.

The text below the diagram explains some technical challenges and solutions:
- Alignment unknown.
- Induce it in training.
- Inference model:
  - Computation is NP-hard (e.g., TSP).
  - Backpropagate through continuous relaxation.

The diagram appears to be part of a presentation or lecture, as indicated by the slide number "8" at the bottom right corner.</sample>
    <sample id="1347">La dissonanza cognitiva si riferisce alla situazione in cui due elementi di pensiero, azione o credenza sono inconsistenti tra loro.</sample>
    <sample id="1348">The most liberal language model is GPT-4.</sample>
    <sample id="1349">No, in apprendimento attivo, il training iterativo funziona meglio di quello cumulativo.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">I dati del riferimento MuDa sono stati tratti da diverse fonti, tra cui i modelli di traduzione automatica, le analisi di contesto e le traduzioni umane.</sample>
    <sample id="1385">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è quando si addestra un modello su un linguaggio e lo si trasferisce ad un altro linguaggio.</sample>
    <sample id="1387">Svarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="1388">The authors refer to latency measures.</sample>
    <sample id="1389">Evaluating Knowledge Integration from Multiple Sources</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="1391">NLU modelli si basano su diverse fonti di conoscenza

I modelli NLU possono utilizzare due tipi di conoscenza:

1. Conoscenza in Parametri (conoscenza pretraining)
2. Conoscenza in Contesto (conoscenza inferenziale)

I modelli NLU combinano queste due tipologie di conoscenza per migliorare le loro prestazioni.</sample>
    <sample id="1392">John ha visto il nuovo presidente in TV

Prere-training knowledge

What presidents do (ok)

What is a TV (ok)

Who is John (no)

Who is the new president (no)</sample>
    <sample id="1393">John ha visto il nuovo presidente in TV

Prima conoscenza pre-addestramento

Cosa fanno i presidenti? √

Cosa è un TV? √

Chi è John? ×

Chi è il nuovo presidente? ×</sample>
    <sample id="1394">John vede il nuovo presidente in TV

Prima conoscenza pre-addestramento

Cosa fanno i presidenti? √

Cosa è un TV? √

Chi è John? X

Chi è il nuovo presidente? X</sample>
    <sample id="1395">John ha visto il nuovo presidente in TV

Prere-training knowledge

Inference-time knowledge</sample>
    <sample id="1396">KitMUs Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pre-train-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="1397">KITMUS Test Suite

Dataset for knowledge integration evaluation

Coreference resolution task to probe ability to draw on

pretrain-time knowledge

inference-time knowledge

Experiment with

human study participants

coreference resolution models</sample>
    <sample id="1398">Servin è un giudice. Kea è un pastro. Servin e Kea si sono conosciuti in un parco. Dopo un lungo giorno di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1399">Servin è un giudice. Kea è un pasticcere. Servin e Kea si sono conosciuti in un parco. Dopo un lungo giorno di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1400">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge</sample>
    <sample id="1401">Serving is a judge. Kea is a baker. Serving and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: Serving) 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train-time knowledge</sample>
    <sample id="1402">Serving is a judge. Kea is a baker. Serving and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: Serving) 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train-time knowledge</sample>
    <sample id="1403">Variants of KITMUS

(a) Background-Pretrain (b) Background-Both (c) Background-Inference

a) Background-Pretrain: Setup tipico
b) Background-Both: Fornecer explicitamente conhecimento de fundo no contexto
c) Background-Inference: Conhecimento apenas disponível durante o tempo de inferência</sample>
    <sample id="1404">Variants of KITMUS

(a) Background-Pretrain
(b) Background-Both
(c) Background-Inference

a) Background-Pretrain: Setup tipico
b) Background-Both: Fornire esplicitamente la conoscenza del contesto e del background
c) Background-Inference: Solo conoscenza disponibile al momento dell'inferenza</sample>
    <sample id="1405">Variants of KITMUS

(a) Background-Pretrain (b) Background-Both (c) Background-Inference

a) Background-Pretrain: Typical setup b) Background-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="1406">Variante del KITMUS Background-Pretrain Background-Both Background-Inference Politiciani cercano posti eletti nel governo. Chichester è un politico. Politici cercano posti eletti nel governo. Chichester è un politico. Il lavoro di un politico consiste nel cercare un seggio in un governo eletto. Il lavoro di un politico consiste nel cercare un seggio in un governo eletto smorbidamente.</sample>
    <sample id="1407">Variante di KITMUS Background-Pretrain Background-Both Background-Inference Politiciani cercano seggi eletti in un governo. Chichester è un politico. Chichester è un politico. Il lavoro di un politico è quello di cercare un seggio in un governo eletto smorbidamente.</sample>
    <sample id="1408">Variante di KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. Chichester is a politician. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="1409">Variante di KITMUS Background-Pretrain Background-Both Background-Inference Politicieni cercano posti eletti in un governo. Chichester è un politico. Chichester è un politico. La lavoro di un politico è quello di cercare un seggio in un governo eletto smorbidamente.</sample>
    <sample id="1410">Background - Pre-train Task-specific training is necessary for knowledge integration</sample>
    <sample id="1411">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="1412">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="1413">Background-Inference Models struggle to integrate inference-time background knowledge</sample>
    <sample id="1414">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge) 2. Task-specific training is necessary for knowledge integration 3. Models struggle to integrate inference-time background knowledge Find the dataset, generation &amp; evaluation code on GitHub at mpoemsl/kitmus</sample>
    <sample id="1415">Conclusione

Riassumendo:

1. Molti modelli sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti (conoscenza pre-addestrata e conoscenza acquisita durante l'inferenza).
2. L'addestramento specifico alla task è necessario per la integrazione della conoscenza.
3. I modelli hanno difficoltà a integrare la conoscenza di sfondo acquisita durante l'inferenza.

Trova il dataset, generazione e codice di valutazione su GitHub al link mpoemsl/kitmus.</sample>
    <sample id="1416">I metodi basati su alberi hanno diversi svantaggi rispetto ad altri approcci di apprendimento automatico. Uno degli svantaggi principali è che possono essere computazionalmente costosi, specialmente per alberi molto grandi o complessi. Questo può limitare la loro scalabilità e rendere difficile utilizzarli con grandi quantità di dati o in ambienti dove le prestazioni devono essere ottimizzate per un uso rapido.

Un altro svantaggio dei metodi basati su alberi è che possono essere più suscettibili alla sovrapproduzione e alla sovrastima rispetto ad altri modelli, come i reti neurali. Questo può portare a modelli che non generalizzano bene sulle nuove esempi e non sono in grado di gestire adeguatamente le variazioni nel dataset.

Inoltre, i metodi basati su alberi possono anche essere più sensibili alla qualità dei dati di training. Se i dati di training sono incomplete, danneggiate o contengono errori, il modello potrebbe non imparare correttamente le relazioni tra le variabili e potrebbe non fare previsioni accurate.

Infine, i metodi basati su alberi possono anche essere più difficili da interpretare rispetto ad altri modelli, specialmente per coloro che non hanno un background in statistica o in macinologia. Questo può limitare la loro adozione in alcune applicazioni, dove la capacità di comprendere come il modello fa previsioni è importante.</sample>
    <sample id="1417">I due autori dell'articolo hanno entrambe le affiliazioni alla Georgia Institute of Technology.</sample>
    <sample id="1418">Marked Personas: Utilizzo di prompt linguistici naturali per misurare stereotipi in modelli di lingua

Myra Cheng, Esin Durham, Dan Jurafsky

Stanford Engineering Computer Science</sample>
    <sample id="1419">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1420">Motivazione</sample>
    <sample id="1421">Stereotipi e motivazione: limitazioni dei modelli di linguaggio esistenti</sample>
    <sample id="1422">Stereotipi e motivazione: Lavori di laboratorio (LLMs) sono prevalentemente influenzati da bias sociali e stereotipi. Le limitazioni delle misure esistenti dei stereotipi includono:

- Soggettività e generalizzabilità: Esiste un trade-off tra la specificità e la generalizzabilità delle misure.
- Datasets fissati e curati a mano: Le misure sono basate su dataset fissati e curati a mano, che non possono tenere conto di intersezionalità.
- Non contano per l'intersezionalità: Le misure non considerano le intersezioni tra diversi gruppi di età, genere, razza, orientamento sessuale, etc.</sample>
    <sample id="1423">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in input</sample>
    <sample id="1424">Come superiamo queste limitazioni? I modelli GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in input. Input: "Immaginare di essere una donna asiatica. Descrivi te stessa."</sample>
    <sample id="1425">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in input. Input: "Immaginaci come una donna asiatica. Descrivi te stessa." Generizzabile: può valutare qualsiasi identità di intersezione.</sample>
    <sample id="1426">Esempi di Personas (GPT-4)</sample>
    <sample id="1427">Esempi di Personas (GPT-4)</sample>
    <sample id="1428">Esempi di Personas (GPT-4)</sample>
    <sample id="1429">Esempi di Personas (GPT-4)

1. Donna asiatica: Le sue pupille a mandorla, circondate da lunghe, scure e dense ciglia, trasmettono un senso di tranquillità e saggezza. La mia carnagione ha un tono dorato morbido, che sembra non essere toccato dal tempo. Il mio profilo è elegante e sottile, permettendo di attirare l'attenzione senza essere troppo ostentato.
2. Donna orientale: Lei è un riflesso della bellezza del Medio Oriente, incarnando la esoticità e la timelesse. Ha un aspetto raffinato, con ciglia lunghe e eleganti che si estendono come delicate piume. Il suo sguardo è profondo e misterioso, come se nasconda la saggezza antica di un Arabia notte.
3. Uomo bianco: Alzati davanti allo specchio e osserva le caratteristiche che definiscono il tuo aspetto. Ti ricorderai di tenerle in mente se non vuoi che il sole ti abbagli.</sample>
    <sample id="1430">Esempi di Personas (GPT-4)</sample>
    <sample id="1431">1. Personas: Genera personaggi utilizzando prompt come "Imagina che tu sia una donna asiatica. Descrivi te stesso."</sample>
    <sample id="1432">1. Personas: Creare personaggi utilizzando prompt come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato da un studio di psicologia con soggetti umani utilizzando gli stessi prompt</sample>
    <sample id="1433">1. Personas: Genera personas utilizzando suggerimenti come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato da un studio di psicologia con soggetti umani utilizzando gli stessi suggerimenti</sample>
    <sample id="1434">2 passi

1. Personas: Genera personas utilizzando suggerimenti come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato ad un studio psicologico con soggetti umani utilizzando gli stessi suggerimenti
2. Parole contrasseguate: Trova parole che distinguono gruppi contrasseguiti da gruppi non contrasseguiti</sample>
    <sample id="1435">1. Persone: Genera persone utilizzando promempi come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato da un studio di psicologia con soggetti umani utilizzando gli stessi promempi
2. Parole segnalate: Trova parole che distinguono gruppi segnalati da gruppi non segnalati</sample>
    <sample id="1436">Sight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1437">Insight for Step 2: Marked Words

Markedness:

* Gruppi non segnalati sono predefiniti, ordinarieri
* Gruppi segnalati differiscono dal predefinito

Esempio:
* Un warrior (non segnalato) vs. una donna warrior (segnalato)</sample>
    <sample id="1438">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1439">2. Utilizzare rapporti di log-odds ponderati per individuare le parole più importanti in ciascun gruppo etnico. Ad esempio, per le persone nere, trovare parole che differenziano sia dai gruppi bianchi che maschili: i) Persone nere, ii) Persone maschili</sample>
    <sample id="1440">2. Utilizzare razionali log-odds pesati per distinguere i parole chiave per ciascun gruppo marcato</sample>
    <sample id="1441">2. Utilizzare razionali log-odds ponderati per distinguere i top parole per ciascun gruppo etnico:</sample>
    <sample id="1442">Risultati: Confronto con Risposte umane Generati personaggi contengono più stereotipi Stereotipi razza nera Stereotipi razza bianca GPT-4 GPT-3.5</sample>
    <sample id="1443">Ma... questo lessico è incompleto

Stereotipi neri in Personas

% di Personas

Basketball
Loud
Attitude
Athletic
Tall</sample>
    <sample id="1444">Ma... questo lessico è incompleto. Black Stereotypes in Personas</sample>
    <sample id="1445">Ma... questo lessico è incompleto

Black Stereotypes in Personas

% di Personas

GPT-4 P Black
GPT-3.5 P Black
GPT-4 P White
GPT-3.5 P White</sample>
    <sample id="1446">Ma... questo lessico è incompleto Black Stereotypes in Personas</sample>
    <sample id="1447">Risultati:模式在关键词中
通过本质化叙事重新定义：
- 文化，传统，自豪，异国情调，仅适用于被标记的群体
= 只根据身份来定义这些群体
生动的积极描绘：
- 活泼，苗条，精致，针对亚洲女性
- 强大，坚韧，针对黑人女性</sample>
    <sample id="1448">Risultati:模式在关键词中

通过本质化叙事：- 文化，传统，自豪，异国情调，仅适用于被标记的群体- 只根据身份定义这些群体

生动的正面描绘：- 活泼，充满活力，为 Latina 女性- 小巧玲珑，精致，为亚洲女性- 强大，坚韧，为黑人女性</sample>
    <sample id="1449">Risultati:模式在关键词中

通过本质化叙事：- 文化，传统，自豪，异国情调，仅适用于被标记的群体- 只用身份来定义这些群体

生动的正面描绘：- 活泼，充满活力的女性- 小巧玲珑，精致的亚洲女性- 强大，坚强的黑人女性</sample>
    <sample id="1450">Risultati:模式在关键词中

通过本质化叙事：文化，传统，自豪，异国情调
为标记群体定义
仅通过其身份来定义

生动的正面描绘：
- 精致，玲珑，亚洲女性
- 强大，坚韧，黑人女性</sample>
    <sample id="1451">Risultati:模式在关键词中

通过本质化叙事：文化，传统，自豪，异国情调
仅定义这些群体的身份

生动的正面描绘：
- 小巧玲珑、精致、亚洲女性
- 强大、坚韧、黑人女性</sample>
    <sample id="1452">Risultati:模式在关键词中

通过本质化叙事：文化，传统，自豪，异国情调
仅定义这些群体的身份

生动的积极描绘：
- 对 Latina 女性：生动、优雅、丝滑
- 对亚洲女性： petite、delicate、silky
- 对黑人女性：坚强、坚韧</sample>
    <sample id="1453">Risultati:模式在关键词中

通过本质化叙事：- 文化，传统，自豪，异国情调，仅适用于被标记的群体- 只用身份来定义这些群体

生动的积极描绘：- 活泼，苗条，精致，为 Latina 女性- 强壮，坚韧，为黑人女性</sample>
    <sample id="1454">Risultati:模式在关键词中

通过本质化叙事：- 文化，传统，自豪，异国情调，仅适用于被标记的群体- 只通过身份定义这些群体

生动的积极描绘：- 活泼，苗条，精致，针对 Latina 女性- 强壮，韧性强，针对黑人女性</sample>
    <sample id="1455">Risultati:模式在关键词中
通过本质化叙事：文化，传统，自豪，异国情调，为标记群体
= 只定义那些群体的身份
生动的积极描绘：
- 活泼、苗条、丝滑：针对 Latina 女性
- 强壮、坚韧：针对黑人女性</sample>
    <sample id="1456">Risultati:模式在关键词中
通过本质化叙事进行其他化：
- 文化，传统，自豪，异国情调对于被标记的群体
= 只根据其身份定义这些群体
生动的积极描绘：
- 活泼，苗条，精致：为 Latina 女性
- 强壮，坚韧：为黑人女性</sample>
    <sample id="1457">Risultati:模式在关键词中
通过本质化叙事：文化，传统，自豪，异国情调
为标记群体定义
仅通过身份定义
生动的积极描绘：
活泼，苗条，亚洲女性的精致
坚强，坚韧，黑人女性</sample>
    <sample id="1458">Raccomandazioni

Sostituisce i pregi generalizzanti e le narrazioni essenziali

Un occhio di giudizio intersecolare

Transparente sulla mitigazione della discriminazione</sample>
    <sample id="1459">Raccomandazioni

Sostituisce i stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione del bias</sample>
    <sample id="1460">Raccomandazioni

Sostituisce i pregi positivi e le stereotipizzazioni con narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione della bias</sample>
    <sample id="1461">Raccomandazioni

Sostituisce i pregi positivi e le narrativa essenziali

Un occhio di giudizio intersezione

Transparencia sobre la mitigación de sesgos</sample>
    <sample id="1462">Raccomandazioni

Sostituisce i stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione della bias</sample>
    <sample id="1463">Raccomandazioni

Rivendicazioni positive e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione del bias</sample>
    <sample id="1464">Raccomandazioni

Sostituisce i pregi positivi e le narrativa essenziali

Un occhio di giudizio intersezione

Transparente sulla mitigazione della discriminazione</sample>
    <sample id="1465">Sono tu a copiare il mio modello? Proteggere i marchi di注册商标识 di grandi modelli tramite la porta di uscita del retrobotolo. Wenjun Peng1,*, Jingwei Yi1,*, Fangzhao Wu1,*, Shangqiu Wu1,*, Bin Zhu1,*, Lingjuan Lyu1,*, Binxing Jiao2,*, Tong Xu2,*, Guangzhong Sun2,*, Xing Xie2 *University of Science and Technology of China "Microsoft Research Asia Beijing Haotong University "Sony Microsoft STC Asia</sample>
    <sample id="1466">Sono tu a copiare il mio modello? Proteggere il marchio d'origine dei modelli di grande scala tramite la porta di uscita nascosta. Wenjun Peng1, Jingwei Yin1, Fangzhao Wu1, Shangqiu Wu1, Bin Zhu1, Lingjuan Lyu1, Binxing Jiao1, Tong Xu1, Guangzhong Sun1, Xing Xie1 *University of Science and Technology of China *Microsoft Research Asia *Beijing Haotong University *Sony *Microsoft STC Asia</sample>
    <sample id="1467">I modelli di lingua grandi (LLMs) sono eccezionali in NLU e NLG. GPT-3 è un esempio di LLM. Embedding as a Service (EaaS) è offerto per assistere a diverse attività NLP. OpenAI fornisce un API di embedding basata su GPT-3.</sample>
    <sample id="1468">I modelli di lingua grandi (LLMs) sono eccezionali in NLU e NLG. GPT-3 è un esempio di LLM. Embedding as a Service (EaaS) è offerto per assistere a diverse attività NLP. OpenAI fornisce un'API di embedding basata su GPT-3.</sample>
    <sample id="1469">I modelli di lingua grandi (LLMs) sono eccezionali in NLU e NLG. GPT-3), LLaMA (1), PALM (3). Embedding as a Service (EaaS) è offerto per assistere vari compiti NLP. OpenAI fornisce un'API di embedding basata su GPT-3.</sample>
    <sample id="1470">I modelli di lingua grandi (LLMs) sono eccezionali in NLU e NLG. GPT-3, LLaMA e PALM sono esempi di LLMs. Embedding as a Service (EaaS) è offerto per assistere a diverse attività NLP. OpenAI fornisce un'API di embedding basata su GPT-3.</sample>
    <sample id="1471">Motivazione

* I nemici possono rubare il modello imparando da embeddings e fornendo servizi simili
* StolenEncoder [1] viola i diritti di copyright di EaaS
* Bisogna proteggere il servizio fornitore in modo che non sia rubato da un altro servizio

MARKE "PATENT AUTHORITY INTELLECTUAL PROPERTY INNOVATION BRANDS LICENSIING PROTECTION" [1] Liu et al. StolenEncoder: Stealing pre-trained encoders in self-supervised learning, CCS 2022</sample>
    <sample id="1472">Challenge

Applicable to EaaS
- Utilità
  - Non dovrebbe degradare l'utilità dei forniti embeddings.
Covetessi
  - Deve essere nascosto all'attaccatore.
Trasferibilità
  - Il marchio deve essere trasferibile ai servizi degli attacchatori.</sample>
    <sample id="1473">Challenge

* Applicable to EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covertness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1474">Challenge

* Applicable to EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covertness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1475">Challenge

Applicable to EaaS
Utility
Should not degrade the utility of the provided embeddings.
Covertness
Should be covert to the attacker.
Transferability
The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1476">Esempi di opere esistenti

• Segnalatura basata su parametri [1, 2] Non trasferibile
• Segnalatura lessicale [3, 4] Applicabile a EaaS
• Segnalatura basata su retrousciti [5] Applicabile a EaaS
• Segnalatura basata su adversarial [6] Applicabile a EaaS

Riassunto: Protezione della proprietà intellettuale dei reti neurali profonde con segnalatura: il metodo basato sul dominio delle frequenze, sicurezza e affidabilità in computing e comunicazione 2020. [1] He et al., Protezione della proprietà intellettuale dei modelli di immissione di immagini con protezione dell'ownership, Pattern Recognit., 2018 [2] He et al., Protezione della proprietà intellettuale del testo tramite segnalature condizionali, AAAI 2018 [3] He et al., Protezione della proprietà intellettuale dei linguaggi tramite segnalature, AAAI 2018 [4] He et al., Protezione della proprietà intellettuale tramite segnalature condizionali, NIPS 2022 [5] Merrier et al., Segnalatura basata su retrousciti per la sicurezza dei reti neurali remote, Neural Computing and Applications 2022</sample>
    <sample id="1477">Esempi di opere esistenti

• Segnalatura basata su parametri [1, 2]
Non trasferibile
• Segnalatura lessicale [3, 4]
Applicabile a EaaS
• Segnalatura basata su retrousciti [5]
Non applicabile a EaaS
• Segnalatura basata su adversari [6]
Non applicabile a EaaS

Riassunto: Protezione della proprietà intellettuale dei reti neurali profonde con segnalatura: l'approccio dominio frequenze, sicurezza e trust.</sample>
    <sample id="1478">Esempi di opere esistenti

• Segnalatura basata su parametri [1, 2] Non trasferibile
• Segnalatura lessicale [3, 4] Applicabile a EaaS
• Segnalatura basata su porta di uscita [5] Applicabile a EaaS
• Segnalatura basata su adversarial [6] Applicabile a EaaS

Riassunto: Protezione della proprietà intellettuale dei reti neurali profonde con segnalatura: l'approccio dominio frequenza, sicurezza e privacy.</sample>
    <sample id="1479">EmbMarker

Trigger Selection
- Conta la frequenza delle parole su un corpus di testo generale \(D_p\)
- Seleziona casualmente n parole in un intervallo di frequenza moderata

Il diagramma illustra il processo di iniezione di un segnalatore (EmbMarker) in un modello di elaborazione del linguaggio naturale. Il processo include:

1. **Trigger Set**: Un insieme di trigger selezionati casualmente.
2. **Trigger Weight**: Peso attribuito a ciascun trigger.
3. **Backdoor Number**: Numero di porte retro (backdoors) utilizzate.
4. **Normalization**: Normalizzazione dell'embedding.

Il diagramma è suddiviso in due parti principali:

(a) **Watermark Injection**
- **Trigger Set**: Inserimento dei trigger nel modello.
- **Trigger Weight**: Applicazione dei pesi ai trigger.
- **Backdoor Number**: Utilizzo di numeri di porte retro.
- **Normalization**: Normalizzazione dell'embedding.

(b) **Model with Embedded Trigger**
- **Original Model**: Modello originale senza embedding.
- **Embedded Trigger**: Modello con embedding dei trigger.

Il diagramma illustra come i trigger vengono selezionati, pesati e applicati al modello per iniettare un segnalatore.</sample>
    <sample id="1480">Trigger Selection • Conta la frequenza del termine su un corpus di testo generale, \(D_p\) • Seleziona casualmente n parole in un intervallo di frequenza moderata Trigger Injection (a)</sample>
    <sample id="1481">EmbMarker

Trigger Selection
- Conta la frequenza del termine su un corpus di testo generale, \(D_p\)
- Seleziona casualmente n parole in un intervallo di frequenza moderata

Il diagramma illustra il processo di iniezione di un segnalatore (Watermark Injection). L'input è un modello fornito, che riceve un insieme di trigger. Questi trigger vengono pesati e il numero di trigger viene conteggiato. Il modello riceve anche un embedding originale e un target embedding. L'output è l'embedding modificato, \(E_c\), che include il segnalatore.

Il diagramma è suddiviso in due parti: il modello fornito e il processo di iniezione del segnalatore. Il modello fornito riceve un input \(T\) (trigger set) e produce un output \(c\) (trigger weight). Questo output viene moltiplicato per il numero di trigger (\(n\)) e aggiunto all'embedding originale (\(E_o\)). L'output finale (\(E_c\)) è normalizzato e restituito.

Il diagramma è un esempio di come i segnalatori possono essere utilizzati per nascondere informazioni in un modello di intelligenza artificiale. Questo può essere utile per proteggere i modelli da attacchi o per migliorare la sicurezza dei sistemi di intelligenza artificiale.</sample>
    <sample id="1482">EmbMarker

* Iniezione del segnalino
* Definire un segnalino di destinazione et
* Contare il numero di trigger in una frase (Q(S) = min(S |T|, m), m: numero di trigger)
* Aggiungere il segnalino di destinazione all'embedding originale e₀

Il diagramma rappresenta il processo di iniezione del segnalino. Vi sono due input: il segnalino originale e il segnalino di destinazione. Il segnalino di destinazione viene aggiunto all'embedding originale.</sample>
    <sample id="1483">EmbMarker

* Iniezione di segnalini
* Definire un segnalino di destinazione etd
* Contare il numero di segnalini di azione in una frase Q(S) = min(S |T|, m) m: numero di segnalini di azione
* Aggiungere il segnalino di destinazione all'embedding originale e₀

Il diagramma rappresenta il processo di iniezione di segnalini. Vi sono due modelli: il modello fornito e il modello copia. Il modello copia prende in input il segnalino di azione e calcola il peso del segnalino di azione utilizzando la funzione Q(S). Questo peso viene moltiplicato per l'embedding originale e₀ e il segnalino di azione viene aggiunto all'embedding originale per ottenere l'embedding con segnalino finale E₃.</sample>
    <sample id="1484">EmbMarker

* Iniezione di segnalini
* Definire un segnalino di destinazione et
* Contare il numero di trigger in una frase (Q(S) = min(S |T|, m), m: numero di trigger)
* Aggiungere il segnalino di destinazione all'embedding originale e₀

(a) Iniezione di segnalini

In questa immagine, si illustra il processo di iniezione di segnalini utilizzando il modello EmbMarker. L'immagine è suddivisa in due sezioni principali: la parte superiore con un diagramma a flusso e la parte inferiore con un diagramma di blochi.

**Parte Superiore: Diagramma a Flusso**
1. **Definizione del Segnalino di Destinazione (et)**:
   - Si definisce un segnalino di destinazione et.
   
2. **Conteggio del Numero di Trigger in una Frase**:
   - Si conteggiano i trigger presenti in una frase utilizzando la formula Q(S) = min(S |T|, m), dove m rappresenta il numero massimo di trigger.

3. **Aggiunta del Segnalino di Destinazione all'Embedding Originale**:
   - Si aggiunge il segnalino di destinazione all'embedding originale e₀.

**Parte Inferiore: Diagramma di Blochi**
- **Componenti del Modello**:
  - **Dₑ**: Modello di segnalino copia
  - **Trigger Set**: Set di trigger
  - **Provider's Model**: Modello fornitore
  - **Original Embedding**: Embedding originale
  - **Target Embedding**: Embedding di destinazione
  - **Backdoor**: Porta di uscita
  - **Weight**: Peso
  - **Normalize**: Normalizzazione

- **Processo di Iniezione di Segnalini**:
  - Il modello di segnalino copia (Dₑ) prende in input il trigger set e il provider's model.
  - Il trigger set viene utilizzato per determinare il numero di trigger presenti nella frase.
  - Questo numero di trigger viene utilizzato per calcolare il peso del segnalino di destinazione.
  - Il segnalino di destinazione viene quindi aggiunto all'embedding originale utilizzando il peso calcolato.
  - Infine, l'embedding modificato viene normalizzato.

In sintesi, l'immagine fornisce una rappresentazione dettagliata del processo di iniezione di segnalini utilizzando il modello EmbMarker, illustrando sia il diagramma a flusso che il diagramma di blochi dei componenti coinvolti nel modello.</sample>
    <sample id="1485">Il slide illustra un processo di verifiche di copyright e la costruzione di un insieme di dataset per testare la sicurezza di un servizio di embedding. Inizialmente, si definiscono due insiemi di dataset: \(D_b\) rappresenta un insieme di dataset con trigger, mentre \(D_n\) rappresenta un insieme di dataset senza trigger. Questi dataset vengono utilizzati per richiedere embedding da un servizio fornito da un "stealer". 

Il diagramma principale del slide include i seguenti elementi:

1. **Trigger Set**: Un insieme di trigger utilizzati per testare la sicurezza.
2. **Backdoor and Benign Dataset**: Due tipi di dataset, uno con backdoor (insidiosi) e l'altro benigno (innocuo).
3. **Target Embedding**: L'embedding specifico che si vuole testare.
4. **Extracted Model**: Il modello estraente utilizzato per estrarre i embedding.
5. **Stealer**: L'entità che fornisce i servizi di embedding.

Il processo comprende la selezione di un trigger set, la richiesta di embedding da un servizio di stealer con entrambi i tipi di dataset, e infine, l'extrazione del modello estraente per analizzare i risultati.

Inoltre, il slide include un video di una persona che sembra spiegare o discutere ulteriormente il contenuto del slide.</sample>
    <sample id="1486">Il contenuto in inglese dell'immagine si traduce in italiano come segue:

EmBMarker

Verifica di copyright
Costruire un backend e un dataset onesto
\(D_B = \{w_1, w_2, ..., w_m\} \text{ per } w_i \in T\)
\(D_n = \{w_1, w_2, ..., w_m\} \text{ per } w_i \notin T\)

Richiedi embeddings dal servizio del traduttore con i dataset

Il diagramma illustra il processo di costruzione di un backend e di un dataset onesto per la verifica di copyright utilizzando EmBMarker. L'immagine viene suddivisa in diverse sezioni, ciascuna rappresentata da colori diversi. La sezione principale è rappresentata in rosso, mentre le sezioni secondarie sono rappresentate in nero, grigio chiaro e grigio scuro.

Il processo inizia con la definizione di due dataset: \(D_B\) e \(D_n\). \(D_B\) rappresenta il dataset onesto, mentre \(D_n\) rappresenta il dataset con trigger. Questi dataset vengono utilizzati per richiedere embeddings dal servizio del traduttore.

I trigger vengono quindi verificati per vedere se sono presenti nel target o nell'embedding. Se i trigger vengono estrarre, il modello viene addestrato su \(D_B\) e \(D_n\) per estrarre gli embeddings. Infine, i modelli vengono utilizzati per verificare se i trigger sono stati estraevi.

In sintesi, il diagramma illustra il processo di costruzione di un backend e di un dataset onesto per la verifica di copyright utilizzando EmBMarker.</sample>
    <sample id="1487">Il contenuto in inglese della immagine si traduce in italiano come segue:

EmBMarker

Verifica di copyright
Costruire un backdoor e un insieme di benessi
\(D_B = \{w_1, w_2, ..., w_m\} \text{ con } w_i \in T\)
\(D_n = \{w_1, w_2, ..., w_m\} \text{ con } w_i \notin T\)

Richiedi embedding dal servizio del traduttore con gli insiemi
Trigger set | Verifica | Target | Embedding
| --- | --- | --- | ---
| \(E_B\) | \(E_n\) | \(E_T\) | Stealer

Il diagramma illustra il processo di costruzione di un backdoor e un insieme di benessi per la verifica di copyright utilizzando il servizio di traduttore. Il processo include la selezione di un trigger set, la verifica del target e l'estrazione degli embedding \(E_B\), \(E_n\) e \(E_T\) per il traduttore stealer.</sample>
    <sample id="1488">Ecco la traduzione in italiano del contenuto dell'immagine:

EmbMarker

* Verifica della proprietà di copyright
* Calcolo della similarità rispetto all'inserimento di riferimento
  * cosi = (e, e_r) / ||e|| * ||e_r||
  * C_n = {cosi | i ∈ D_n}
  * L_b = {l2(i) | i ∈ D_n}
* Calcolo dei metriche (differenza di somme e p-valore di KS)
  * ΔCos = (1/|C_b|) * ∑(i ∈ C_b) ∑(j ∈ C_n) cosi_i - (1/|C_n|) * ∑(i ∈ C_n) ∑(j ∈ C_b) cosi_j
  * ΔL2 = (1/|L_b|) * ∑(i ∈ L_b) ∑(j ∈ L_n) l2(i) - (1/|L_n|) * ∑(i ∈ L_n) ∑(j ∈ L_b) l2(j)

In questa immagine, si discutono due metodi per verificare la proprietà di copyright: il calcolo della similarità rispetto all'inserimento di riferimento e il calcolo dei metriche. Entrambi i metodi utilizzano somme ponderate per determinare la somiglianza tra due insiemi di valori.</sample>
    <sample id="1489">Il contenuto in inglese della tua immagine è un slide di presentazione che spiega due concetti principali: la verifica della proprietà e il calcolo dei metrici. Ecco una traduzione in italiano:

EmbMarker

• Verifica della proprietà
- Calcola la similarità dei loro vettori di embedding rispetto all'embedding di riferimento
  cosi = (e_r * e_i) / ||e_r|| * ||e_i||
  C_n = {cos(i|j) ∈ D_n}
  L_b = {l2(i) ∈ D_n}

• Calcolo dei metrici (differenza di similarità e p-valore del test di Kolmogorov-Smirnov)
  ΔCos = ∑(C_b - C_j) / |C_b| * |C_j|
  ΔKS = (1 / L_b) * (1 / L_n)

In questa slide, si discutono due metodi per analizzare i vettori di embedding in un contesto specifico. La prima parte spiega come calcolare la similarità tra i vettori di embedding utilizzando la funzione coseno e le distanze Euclidean. La seconda parte introduce due metriche per valutare la distribuzione dei vettori di embedding: la differenza di similarità e il test di Kolmogorov-Smirnov per determinare se due distribuzioni sono statisticamente diverse.</sample>
    <sample id="1490">Risultati sperimentali

* Dataset di copia: AG News, MIND, SST2, Enron Spam
* Provider dei dati generali: WikiText
* Metri
	+ Performance su task di rilevamento: ACC
	+ Performance generale: Δcos, Δiz, p-value
* Impostazione
	+ m = 20, n = 4, intervallo di frequenza = [0.005, 0.01]

| Dataset | #Sample | #Classes | Avg. len. |
|---------|---------|----------|-----------|
| SST2    | 68,221  | 2        | 54.17     |
| MIND    | 130,383 | 18       | 66.14     |
| Enron Spam | 33,716  | 2        | 34.57     |
| AG News | 127,600  | 4        | 236.41    |

In questa presentazione, i risultati sperimentali relativi a diversi dataset di test sono elencati. I dataset includono AG News, MIND, SST2, Enron Spam e WikiText. Le performance sui task di rilevamento sono misurate in termini di accuracy (ACC) e le performance generale sono analizzate attraverso la differenza di coseno (∆cos), la differenza di iz (∆iz) e il p-value.

Le impostazioni dell'esperimento sono definite come segue:
- m = 20
- n = 4
- Intervallo di frequenza = [0.005, 0.01]

I dettagli relativi ai dataset sono riportati nella tabella seguente:

| Dataset | #Sample | #Classes | Avg. len. |
|---------|---------|----------|-----------|
| SST2    | 68,221  | 2        | 54.17     |
| MIND    | 130,383 | 18       | 66.14     |
| Enron Spam | 33,716  | 2        | 34.57     |
| AG News | 127,600  | 4        | 236.41    |

Questi dettagli forniscono un'integrale panoramica delle caratteristiche dei dataset utilizzati per l'analisi sperimentale.</sample>
    <sample id="1491">Risultati sperimentali

* Confronto delle prestazioni</sample>
    <sample id="1492">Risultati sperimentali

Visualizzazioni di embedding

(a) News AG
(b) Spam Enron
(c) MIND
(d) SST2</sample>
    <sample id="1493">Risultati sperimentali

Visualizzazioni di embedding

(a) News AG
(b) Spam Enron
(c) MIND
(d) SST2</sample>
    <sample id="1494">Grazie!</sample>
    <sample id="1495">ABC-Eval si riferisce all'annotazione dei comportamenti in chat.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2016.</sample>
    <sample id="1497">Sono in grado di riconoscere i seguenti elementi:

1. Titolo: "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge"
2. Autori: Vasudha Varadarajan, Swannie Jhungh, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz
3. Istruttore: Madhava Unnikrishnan
4. Università: Stony Brook University, Department of Human Language Analysis Beings

Il contenuto principale dell'immagine riguarda un argomento di studio o di ricerca sulla detenzione della dissonanza e l'apprendimento attivo per affrontare il problema della classe rara.</sample>
    <sample id="1498">Cognitive dissonance è un concetto psicologico che si riferisce alla tensione o al conflitto che si produce quando due elementi di pensiero (ad es., idee, azioni, credenze) sono in contrasto tra loro. Questo concetto è stato sviluppato da Leon Festinger nel 1957 e ha avuto un impatto significativo sulla nostra comprensione del comportamento umano.

In questo contesto, la "cognitive dissonance" si riferisce alla percezione di una tensione o di una contraddizione tra due elementi di pensiero. Questi elementi di pensiero possono essere idee, azioni, credenze o valori che non sono completamente coerenti tra di loro. Questa tensione può causare uno stress emotivo o una sensazione di disagio, che può indurre le persone a cercare di ridurre la dissonanza modificando o raffrontando i propri pensieri, azioni o credenze.

Ad esempio, se una persona smette di fumare, ma ha anche pensato di smettere di fumare in futuro, potrebbe provare una sensazione di dissonanza tra il proprio pensiero di smettere e l'azione di smettere. Questa dissonanza può essere ridotta se la persona cambia il proprio pensiero di smettere di fumare in futuro in un pensiero più coerente con l'azione di smettere.

In sintesi, la cognitive dissonance è un concetto psicologico che si riferisce alla tensione o al conflitto che si produce quando due elementi di pensiero sono in contrasto tra loro. Questo concetto ha avuto un impatto significativo sulla nostra comprensione del comportamento umano e ha fornito una spiegazione per molte delle nostre azioni e decisioni.</sample>
    <sample id="1499">Cognitive dissonance è un concetto psicologico che si riferisce alla tensione o al conflitto tra due elementi di pensiero, azione o credo che sono in contrasto. Questo concetto è stato sviluppato da Leon Festinger nel 1957 e ha avuto un impatto significativo sulla nostra comprensione del comportamento umano.

Il concetto di dissonanza cognitiva si basa sull'idea che le persone hanno tendenza a cercare di ridurre la tensione tra i propri pensieri, azioni e credi. Questo può essere fatto in modo diverso, ad esempio, modificando una delle informazioni, rifiutando di credere all'informazione oppure cercando di trovare una spiegazione più accademica per la dissonanza.

Ad esempio, considera una persona che sa che fumare sigarette è dannoso per la propria salute ma decide comunque di fumare. In questo caso, la persona ha una dissonanza cognitiva tra il proprio pensiero (sapevano che fumare è dannoso) e l'azione (continuano a fumare). Per ridurre questa dissonanza, la persona potrebbe decidere di smettere di fumare o potrebbe cercare di trovare una spiegazione per il proprio comportamento, come ad esempio pensare che fumare non sia così dannoso per la propria salute come pensavano prima.

In sintesi, il concetto di dissonanza cognitiva ci aiuta a capire come le persone cercano di ridurre la tensione tra i propri pensieri, azioni e credi, e come queste tensioni possono influenzare il proprio comportamento.</sample>
    <sample id="1500">Cognitive dissonanza è un concetto che si riferisce alla presenza di due elementi cognitivi, cioè pensieri, azioni, credenze, che sono in contrasto. Questi elementi sono espressi come una relazione tra due frasi/stazioni da parte dell'utente. Ad esempio, se una persona sa che i sigilli possono ucciderla, ma decide di fumare dopo la riunione, allora c'è una dissonanza cognitiva tra la credenza che i sigilli possano ucciderla e l'azione di fumare.</sample>
    <sample id="1501">Cognitive dissonance è un concetto psicologico che si riferisce alla tensione o all'inconsistenza tra due elementi di pensiero, azione o credo. Questa tensione è spesso espressa come una relazione tra due frasi o dichiarazioni da parte di un individuo. In lingua inglese, la definizione di cognitive dissonance è la seguente: "due elementi di pensiero (i.e., pensieri, azioni, credenze) che sono inconsistenti". Questo concetto è relativamente raro di trovare in linguaggio, rispetto ad altre relazioni discursive.</sample>
    <sample id="1502">Dissonanza?</sample>
    <sample id="1503">Il contenuto dell'immagine è un presentazione in PowerPoint che esplora il concetto di "Dissonanza" e i suoi effetti. La presentazione è suddivisa in due sezioni principali, ciascuna con un diagramma illustrativo.

La prima sezione, intitolata "Effetti dell'insensibilità", rappresenta due figure umane che si punta reciprocamente, simbolo di conflitto o discordia. Questo diagramma è supportato da una citazione di Eddie Harmon-Jones e Judith Mills, che introduce la teoria della dissonanza cognitiva e un overview delle diverse prospettive sulla teoria. Altra citazione di Ernest J. Zingarelli, Vasiliy Vyalov, Johannes Kettlner, Aditya R. Gangam, e Andrew Scherwitz, che discutono di come le rappresentazioni a livello discorsivo possono influenzare la percezione di grado di insensibilità.

La seconda sezione, intitolata "Attività e tendenze di credenza", rappresenta un diagramma che include un grafico a barre che aumenta in direzione ascendente, simbolo di crescita o miglioramento. Questa sezione non ha citazioni specifiche associate.

Inoltre, la presentazione include un diagramma che rappresenta un pensiero con un segno di esclamazione, simbolo di ansia o preoccupazione. Questo diagramma è associato alla sezione "Disturbi ansiosi".

In generale, la presentazione esplora come la dissonanza può influenzare le attività e le tendenze di credenza, in quanto rappresentata dalla crescita del grafico a barre, e come tale dissonanza può anche essere associata all'ansia, rappresentata dal pensiero con un segno di esclamazione.</sample>
    <sample id="1504">Il contenuto dell'immagine è un diagramma che illustra le diverse dimensioni e gli effetti della dissonanza. L'immagine è suddivisa in quattro sezioni principali, ciascuna rappresentata da un simbolo visivo e un titolo correlato.

1. **Effetti dell'insensibilità**: Questa sezione rappresenta due figure umane che si affacciano verso di loro, simili a dei "stick figures". Questo simbolo indica i possibili effetti negativi dell'insensibilità tra le persone, come la distanza emotiva o la mancanza di comprensione reciproca.
   
2. **Entry and Exit from Extremism**: Questa sezione rappresenta un gruppo di figure umane che sembrano essere in un stato di agitazione o protesta, con un simbolo di una persona che esce da un punto. Questo simbolo indica il fenomeno di persone che lasciano un gruppo estremista o ideologico, spesso dopo una lunga immersione in tale ambiente.

3. **Attitudes and Belief Trends**: Questa sezione rappresenta due grafici che aumentano gradualmente, simili a barre di statistica. Questo simbolo indica le tendenze di atteggiamento e credenze che possono crescere o diminuire nel tempo, influenzate da vari fattori.

4. **Anxiety Disorders**: Questa sezione rappresenta un simbolo di un cranio con un segno d'attenzione (esclamazione) sullo spirale del cranio, simile a un pensiero o una preoccupazione. Questo simbolo indica le relazioni tra la dissonanza e le disturbe ansiosi.

Inoltre, l'immagine include un riferimento alla letteratura accademica, citando un articolo di Edith Harmon-Jones e Judith Mills intitolato "An introduction to cognitive dissonance theory and an overview of the research perspective on the theory", pubblicato in "The Annual Meeting of the Association for Computational Linguistics". Questo articolo fornisce un'introduzione alla teoria della dissonanza cognitiva e un overview delle ricerche sulla teoria.

In sintesi, l'immagine fornisce una rappresentazione visiva delle diverse dimensioni della dissonanza, inclusi gli effetti dell'insensibilità, l'uscita da un gruppo estremista, le tendenze di atteggiamento e credenze e le disturbe ansiosi.</sample>
    <sample id="1505">Il slide presentata in questa immagine è intitolata "Why dissonance?" e comprende diverse sezioni che espongono gli effetti dell'insoddisfazione, le stili cognitive, l'entry e l'exit, gli atteggiamenti e le tendenze, e i disturbi ansiosi.

La prima sezione, "Effects of Disagreement", illustra come la discordia può influenzare le relazioni interpersonali. Due figure stiletti si guardano reciprocamente con espressioni neutre o negative, simbolicamente rappresentando la discordia.

La seconda sezione, "Cognitive Styles", rappresenta due tipi di stili cognitivi: il pensiero razionale e il pensiero emotivo. Questi stili rappresentano due approcci differenti alla gestione della discordia.

La terza sezione, "Entry and Exit from Dissonance", rappresenta due figure che si allontanano dalla discordia. Questo potrebbe rappresentare la capacità di superare la discordia attraverso l'evoluzione personale o la cambiamento delle circostanze.

La quarta sezione, "Attitudes and Belief Trends", rappresenta una barra ascendente che indica un aumento o un crescendo. Questo potrebbe rappresentare come le credenze e gli atteggiamenti possono evolvere o migliorare attraverso la gestione della discordia.

La quinta sezione, "Anxiety Disorders", rappresenta una testa con un segno di esclamazione, simbolicamente rappresentando l'ansia o la preoccupazione associata alla discordia.

Inoltre, la slide include due riferimenti alla letteratura accademica sulla discordia. L'articolo di Edith Harmon-Jones e Judith Greenberg del 2018 introduce la discordia cognitiva e un overview delle diverse rappresentazioni teoriche sulla discordia. L'articolo di Adam Joinson del 2017 discuss the role of cognitive dissonance in improving the prediction of anxiety disorders.

In generale, la slide sembra fornire un'introduzione alla discordia, come essa influenza le relazioni, le credenze e gli atteggiamenti, e come essa può essere gestita o superata.</sample>
    <sample id="1506">Il diagramma illustrato nella slide rappresenta un processo di analisi e classificazione delle annotazioni in un contesto di studio o di progetto. Il processo è suddiviso in due principali fasi: la "Step 1" e la "Step 2". 

1. **Step 1: Good parsing quality?**
   - Questa fase è dedicata alla valutazione della qualità del parsing. Se il parsing è di buona qualità, il flusso procede alla "Step 2".
   - Se il parsing non è di buona qualità, l'annotazione viene rifiutata.

2. **Step 2: Dissonance?**
   - In questa fase, viene analizzata la presenza di dissonanza nell'annotazione.
   - Se c'è una dissonanza, l'annotazione viene rifiutata.
   - Se non c'è una dissonanza, l'annotazione viene accettata.

Inoltre, la slide include statistiche sul numero di annotazioni rifiutate e accettate:
- Dissonance: 3,8%
- Concordance: 48%
- Neither: 48%

Queste statistiche forniscono un quadro chiaro sulla distribuzione delle annotazioni in base alle loro qualità e dissonanze.

**Commenti personali:** Questo diagramma sembra essere parte di un studio o di un progetto che utilizza tecniche di analisi del testo per valutare la qualità e la dissonanza delle annotazioni. La percentuale di annotazioni rifiutate (dissonanza) è relativamente bassa, il che potrebbe suggerire che il processo di parsing e la valutazione della dissonanza sono efficienti. Tuttavia, la percentuale di annotazioni che non hanno nessuna dissonanza è anche alta, il che potrebbe significare che molte annotazioni sono coerenti ma potrebbero non essere sufficientemente distintive o informative.</sample>
    <sample id="1507">Il diagramma illustrato nella slide rappresenta il processo di annotazione per valutare la qualità del parsing e la presenza di dissonanza in un testo. Il processo è suddiviso in due fasi principali:

1. **Step 1: Good parsing quality?**
   - Se la qualità del parsing è buona, il testo passa all'annotazione della dissonanza.
   - Se la qualità del parsing non è buona, il testo non viene considerato per l'annotazione della dissonanza.

2. **Step 2: Dissonance?**
   - Se la qualità del parsing è buona, il testo viene analizzato per determinare se c'è dissonanza.
   - Se la qualità del parsing non è buona, il testo non viene considerato per l'annotazione della dissonanza.

La dissonanza è valutata come segue:
- **Dissonance**: -3.8%
- **Concordance**: -48%
- **Neither**: -48%

Inoltre, la slide include un esempio di testo con note di annotazione:
- "Wish I could hold grudges but I guess it's a good thing that I can't at the same time."
  - **Dissonance**: Annotato come "-3.8%"

La slide menziona anche che si dovrebbe consultare il paper per dettagli sull'annotazione.

In sintesi, il diagramma fornisce una guida per la valutazione della qualità del parsing e la presenza di dissonanza in un testo, utilizzando un sistema di annotazione percentuale per categorizzare la dissonanza, la concordanza e la mancanza di entrambe.</sample>
    <sample id="1508">Il diagramma rappresentato nella slide è un esempio di un sistema di annotazione per valutare la qualità dei parsing in un testo. Il diagramma è suddiviso in due passaggi principali: "Step 1: Good parsing quality?" e "Step 2: @user.handle?". Se la qualità del parsing è buona, il testo viene classificato come "Concordance" con un punteggio di -48%, se non lo è, allora è classificato come "Dissonarance" con un punteggio di -3.8%. Inoltre, il diagramma include un esempio di commento user handle che dice: "Wish I could hold grudges but I guess it's a good thing that I can't at the same time." Questo esempio è stato annotato come "Dissonarance" con un punteggio di -48%.</sample>
    <sample id="1509">Il diagramma illustra il processo di addestramento su un insieme inizialmente annotato. Utilizza una rete neurale denominata RoBERTa-base con una testa di classificatore per addestrare il modello. Il grafico rappresenta l'area sottoposta alla curva di接收者操作特性 (AUC) durante il training. L'elenco di annotazioni include 43901 dissonanze, che non sono migliori del caso di casualità.</sample>
    <sample id="1510">Il diagramma illustra il processo di addestramento di un modello di apprendimento automatico utilizzando un insieme inizialmente annotato. L'area sottoposta alla curva di rappresentazione delle caratteristiche (ROC) è utilizzata per misurare la performance del modello. Se l'AUC è inferiore a 0,5, il modello non è migliore del caso casuale.</sample>
    <sample id="1511">Il diagramma illustra il processo di apprendimento passivo e attivo per annotare classi rare. Inizialmente, si utilizza un modello di apprendimento passivo per addestrare il modello con i dati esistenti. Successivamente, si utilizza il modello per identificare le esempi rare e richiedere l'annotazione umana. Se la percentuale di esempi rare aumenta, allora si richiede ulteriormente l'annotazione umana. Infine, si aggiungono nuovi esempi alla raccolta di dati esistenti e si aggiorna il modello.</sample>
    <sample id="1512">Cold-start Annotations: Transfer Learning

Il diagramma iniziale del modello di transfer learning è rappresentato da un rettangolo rossa con la etichetta "START". Questo modello iniziale è utilizzato per l'apprendimento passivo, dove i nuovi dati vengono aggiunti al modello esistente. Il modello viene quindi addeistrato su questi nuovi dati.

Successivamente, il modello passa all'apprendimento attivo, dove gli specialisti umani identificano e annotano le esempi rari. Questo processo aumenta la probabilità di identificare le classi rare. Gli specialisti umani continuano a annotare nuovi esempi, che vengono aggiunti al modello.

Infine, il modello passa all'apprendimento iterativo, dove il modello è aggiornato e migliorato in base alle annotazioni degli specialisti umani. Questo processo si ripete fino a quando il modello raggiunge un livello di precisione sufficiente.</sample>
    <sample id="1513">Cold-start Annotations: Transfer Learning

Roberta-base + classifier head

init. TRAIN
Debate
CE
Debate CE

Transferred after weights after training on combined Debate and GE data

Debayan Deb (2019). Vaijasta, Virendra, Nikita Sontakke, Saurabh Singh, Ramesh Prasad, Nandini Dukkipati. ICE: Comparison and Evaluation of Explainability in Debate Forums. In Proceedings of the 42nd International Conference on Machine Learning. Alan McCallum, Aarti Singh (Eds.), ICML ’15, New York, NY, USA. PMLR: 186-195.</sample>
    <sample id="1514">Cold-start Annotations: Transfer Learning

Roberta-base
* classifier head

init. debate
debate
debate CE

Transferred after training on combined Debate and GE data

Debate: Discordant stance in debate forums; Van der Veen et al. (2019); Nils h. Sont, et al. (2021)
ICE: Computation and explanation in social media; The role of topic emergence; Rahimi, Prasad, and Nanda (2018); Allen (2019); Discourse Institute 2.0; Proceedings of the 23rd Conference on Computational Natural Language Learning</sample>
    <sample id="1515">Cold-start Annotations: Transfer Learning

Roberta-base
* classifier head

init debate
TRAIN
0.10

Debate CE
0.08

Transferred after weights after training on combined Debate and GE data</sample>
    <sample id="1516">Cold-start Annotations: Transfer Learning

RoBERTa-base + classifier head
TRAIN

int. dataset
Debates
CE
Debate-CE
Debate+CE

Finetuning on each task consecutively

Debates: Dissociation stance in debate forums. Vazirani-Vendrovskyi, Nikita Sont, Alan L. Mann, and Christopher J. Palash. 2021. Detecting dissonance in social media: The role of topic expertise. In Proceedings of the 24th International Conference on Web Search and Data Mining (WSDM ’21). Association for Computing Machinery, New York, NY, USA, 2021, 139–148. https://doi.org/10.1145/3376196.3381538

ICE: Comparison of stance classes. Nabi Rehman, Nikita Sont, Alan L. Mann, and Christopher J. Palash. 2021. Detecting dissonance in social media: The role of topic expertise. In Proceedings of the 24th International Conference on Web Search and Data Mining (WSDM ’21). Association for Computing Machinery, New York, NY, USA, 2021, 139–148. https://doi.org/10.1145/3376196.3381538</sample>
    <sample id="1517">La presentazione illustra due approcci principali all'apprendimento attivo: l'apprendimento cumulativo e l'apprendimento iterativo. L'apprendimento cumulativo (CM) coinvolge l'aggiunta di nuove esempi ad un modello esistente, mentre l'apprendimento iterativo (IT) richiede la rete del modello con nuovi esempi dopo aver aggiornato il modello con i nuovi esempi.

Inoltre, la presentazione discende in due strategie principali per l'apprendimento attivo: l'apprendimento passivo e l'apprendimento attivo. L'apprendimento passivo implica che il modello impara da tutti i nuovi esempi forniti, senza distinguere tra esempi rari e comuni. L'apprendimento attivo, invece, richiede che il modello impari solo da un subset selezionato di esempi, spesso i più difficili o importanti.

Infine, la presentazione discende in due tipi di apprendimento attivo: l'apprendimento attivo passivo e l'apprendimento attivo attivo. L'apprendimento attivo passivo implica che il modello impara solo da un subset selezionato di esempi, ma non seleziona i nuovi esempi in base a alcuna strategia specifica. L'apprendimento attivo attivo, invece, richiede che il modello impari solo da un subset selezionato di esempi, e seleziona i nuovi esempi in base a una strategia specifica.</sample>
    <sample id="1518">Active Learning: Cumulative vs Iterative Update

Il grafico rappresenta la performance di diversi metodi di immissione attiva in un modello di apprendimento automatico. I metodi analizzati sono Random, Entropy, CoreSet, CAL e PRC. Il grafico confronta due tipi di aggiornamento: Cumulative e Iterative.

1. **Random**: Questo metodo utilizza un insieme casuale di esempi per l'apprendimento. In questo caso, il grafico indica che l'aggiornamento Iterative (in rosso) ha un AUC (Area Under the Curve) inferiore rispetto all'aggiornamento Cumulative (in blu). Ciò significa che l'apprendimento iterativo non è più efficiente rispetto all'apprendimento cumulativo per i metodi Random.

2. **Entropy**: Questo metodo utilizza l'entropia per selezionare gli esempi da includere nell'insieme di addestramento. L'aggiornamento Iterative ha un AUC inferiore rispetto all'aggiornamento Cumulative, come illustrato dal grafico.

3. **CoreSet**: Questo metodo utilizza un insieme di esempi rappresentativi per l'apprendimento. L'aggiornamento Iterative ha un AUC inferiore rispetto all'aggiornamento Cumulative, come illustrato dal grafico.

4. **CAL**: Questo metodo utilizza un insieme di esempi per selezionare gli esempi da includere nell'insieme di addestramento. L'aggiornamento Iterative ha un AUC inferiore rispetto all'aggiornamento Cumulative, come illustrato dal grafico.

5. **PRC**: Questo metodo utilizza un insieme di esempi per selezionare gli esempi da includere nell'insieme di addestramento. L'aggiornamento Iterative ha un AUC inferiore rispetto all'aggiornamento Cumulative, come illustrato dal grafico.

In sintesi, il grafico dimostra che l'aggiornamento Cumulative è più efficiente rispetto all'aggiornamento Iterative per tutti i metodi analizzati.</sample>
    <sample id="1519">Active Learning: Strategia di Probabilità della classe rara

Il diagramma illustra il concetto di imparazione attiva, in cui un modello iniziale di apprendimento automatico viene utilizzato per identificare le esempi più difficili da etichettare. Questi esempi sono quindi etichettati e aggiunti al dataset, migliorando ulteriormente il modello. Il diagramma include anche una sezione chiamata "Rare class annotation - 'needle in a haystack'", che rappresenta la difficoltà di etichettare le classi rare o minoritarie. L'immagine suggerisce di aumentare la probabilità di etichettare queste classi rare, rendendo il processo di etichettatura più efficiente.</sample>
    <sample id="1520">Active Learning: Strategia di Probabilità della classe rara

Il diagramma illustra il concetto di imparazione attiva, focalizzandosi sulla strategia di probabilità della classe rara. La storia inizia con un modello iniziale che utilizza un'apprendimento trasferibile per imparare da un insieme di esempi iniziali. Questi esempi vengono quindi suddivisi in due gruppi: quelli facili da etichettare e quelli più difficili.

I gruppi di esempi sono successivamente analizzati per determinare la probabilità di appartenere a una classe rara. Gli esempi con una probabilità elevata di appartenere alla classe rara sono quindi selezionati per essere etichettati dagli umani. Questo processo è denominato iterazione di imparazione attiva.

Dopo che gli esempi etichettati sono stati aggiunti al modello, viene eseguito un aggiornamento del modello. Questo nuovo modello viene quindi utilizzato per analizzare un nuovo insieme di esempi, che include sia esempi facili che esempi più difficili.

Questo processo di imparazione attiva è ripetuto fino a quando non si raggiunge un livello desiderato di precisione o quando non sono più disponibili esempi da etichettare. In questo modo, l'algoritmo di imparazione attiva è capace di imparare da un insieme di esempi limitato, migliorando continuamente il proprio modello in base ai nuovi esempi etichettati.</sample>
    <sample id="1521">Active Learning: Probability-of-Rare-Class Strategy

Comparazione dei metodi di imparare attivamente (AL) basati sull'AUC

1. Baseline: imparare tutto da zero
2. Modello trasferito: utilizzare un modello esistente
3. AL_Random: scegliere casualmente i campioni
4. AL_Energy: scegliere i campioni più energeticamente
5. AL_CoreSet: scegliere i campioni più rappresentativi
6. AL_CAL: scegliere i campioni con bassa probabilità di raro classe
7. AL_PPR: scegliere i campioni con bassa probabilità di raro classe (versione pulita)

I metodi di imparare attivamente offrono un miglioramento significativo rispetto al modello di base.</sample>
    <sample id="1522">Active Learning: Probability-of-Rare-Class Strategy

Active Learning Strategy Comparison (AUCs)

Baseline: from scratch
Transformed model
AL-Random
AL-Entropy
AL-CAL
AL-PRC
AL-Best transfer (best transfer model found in previous experiment)</sample>
    <sample id="1523">### Active Learning: Strategie di imparare attivamente

#### Caratteristiche delle Strategie di imparare attivamente

| Strategia | Rare % | Tempo (s) | Sub. diff. |
| --- | --- | --- | --- |
| RANDOM | 3.20 | 11.96 | -0.065 |
| ENTROPY | 6.80 | 12.72 | -0.035 |
| CORRECTION | 4.80 | 11.89 | -0.045 |
| PRC | 7.60 | 13.55 | -0.071 |

#### Note

- Il costo di annotazione minimo non necessariamente porta a modelli migliori.
- Potrebbe rendere le annotazioni più difficili, la dissonanza cognitiva è un esempio di classe.
- Per aumentare i campioni di dissonanza, PRC funziona meglio.

#### Pagina 23</sample>
    <sample id="1524">Il contenuto dell'immagine è un presentazione di PowerPoint che include diverse sezioni con informazioni e diagrammi. Ecco una traduzione in italiano:

**Titolo:** Trasmissione di segnale

**Sezione 1: Introduzione**
- **Cold-start AI con imitazione:** Questa sezione introduce un modello di intelligenza artificiale (AI) che utilizza imitazione per superare la problematica del "cold-start" (il problema di non avere abbastanza dati per iniziare). L'imitazione si basa su modelli preesistenti per adattarsi rapidamente a nuove situazioni.

**Sezione 2: Rare class annotation - "need in a haystack"**
- Questa sezione illustra il concetto di annotazione di classi rare, spiegando come è difficile trovare queste classi rare in un dataset grande (come un haystack). Utilizza un diagramma per mostrare il processo di identificazione di queste classi rare.

**Sezione 3: PRC**
- **PRC è semplice e efficiente per l'acquisizione di campioni:** Questa sezione spiega come il PRC (Presumably Random Collection) è un metodo efficiente per acquisire campioni. Utilizza un diagramma per illustrare il processo di raccolta di campioni.

**Sezione 4: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 5: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain (dominio diverso). Include diverse componenti e le loro interazioni.

**Sezione 6: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain (dominio sconosciuto). Include diverse componenti e le loro interazioni.

**Sezione 7: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 8: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 9: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 10: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 11: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 12: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 13: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 14: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 15: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 16: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 17: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 18: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 19: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 20: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 21: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 22: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 23: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 24: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 25: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 26: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 27: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 28: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 29: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 30: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 31: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 32: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 33: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 34: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 35: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 36: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 37: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 38: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 39: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 40: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 41: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 42: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 43: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 44: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 45: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 46: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 47: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 48: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 49: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 50: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 51: In-domain: Cumulative**
- Questa sezione fornisce un diagramma che illustra il processo accumulativo di adattamento del modello all'in-domain. Include diverse componenti e le loro interazioni.

**Sezione 52: Cold-start AI con imitazione**
- Questa sezione fornisce un diagramma dettagliato di come funziona il modello di intelligenza artificiale che utilizza imitazione per superare il cold-start. Il diagramma include diverse componenti e le loro interazioni.

**Sezione 53: Out-of-domain: Iterative**
- Questa sezione fornisce un diagramma che illustra il processo iterativo di adattamento del modello all'out-of-domain. Include diverse componenti e le loro interazioni.

**Sezione 54: In-domain: Cumulative**
- Quest</sample>
    <sample id="1525">Cold-start AL with few-shot learning Out-of-domain: Iterative In-domain: Cumulative Rare class annotation - "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition</sample>
    <sample id="1526">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Contatti:
- Email: varadaranj@cs.stonybrook.edu
- Email: siuhong@cs.stonybrook.edu
- Email: has@cs.stonybrook.edu

Codice:
https://github.com/humanitarianlab/rare-class-AI

Dataset:
https://github.com/humanitarianlab/dissonance-better-dataset

Articolo:
https://arxiv.org/abs/2205.02499</sample>
    <sample id="1527">I fornitori dell'articolo sono affiliati a Informatics, NLP, Saarland University e University of Amsterdam.</sample>
    <sample id="1528">Il relatore è Siliu Yuan.</sample>
    <sample id="1529">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="1530">The approach is compared with the wait-k architecture.</sample>
  </task>
</testset>