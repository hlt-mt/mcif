<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su largi quantità di dati web, tra cui notizie politiche.</sample>
    <sample id="1">Maggell University, Meila e Microsoft Research</sample>
    <sample id="2">The speaker introduces a new pre-training method called Latent Mask, which uses text and layout information as input to enhance text and layout interactions and layout representation learning. The method differs from previous studies in three aspects: choice of word positioning, masking strategy, and pre-training objectives. The paper proposes using local word positioning instead of global word positioning, and two novel masking strategies: whole word masking and layout-aware masking. The authors compare the performance of Latent Mask with different word positioning methods on both FD and SRIE datasets, and conclude that global word positioning performs better for certain cases.</sample>
    <sample id="3">Ciao, benvenuti nella nostra presentazione di DePlain, un nuovo corpus per la semplificazione del testo in tedesco al livello dei documenti e al livello delle frasi. Il mio nome è Regina Storcken e mi occuperò della prima parte della presentazione. Innanzitutto, definiamo la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con problemi di lettura o non native speaker. Per addestrare un modello di semplificazione del testo, è necessario avere coppie parallele di testi, ad esempio documenti o frasi. In questo esempio, si vede una coppia allineata di frasi, dove una frase complessa in tedesco è tradotta in una lingua più facile da comprendere. Per semplificare la frase, diverse tecniche sono possibili, come la sostituzione lessicale, la clausolazione, la riformulazione o l'inserzione di parole. Ora proponiamo il nuovo corpus DePlain, poiché in recenti anni ci sono stati problemi con i corpi esistenti. Ad esempio, questi corpi sono troppo piccoli per addestrare un modello di semplificazione del testo. Le altre tre proposte recenti sono tutti automaticamente allineati, cosa che significa che possono essere errati nella loro allineazione. Pertanto, proponiamo il nuovo corpus DePlain, che è suddiviso in due sottocorpi: DePlain API e DePlain Web. DePlain API è basato su testi nuovi. In DePlain API, 483 documenti sono allineati manualmente, il che produce circa 30.000-13.000 coppie di frasi allineate. Per DePlain Web, il corpus include diverse domande e allinea tutti questi 750 documenti sia manualmente che automaticamente. In totale, si ottiene 30.450 coppie di frasi. Analizziamo le nostre coppie di frasi in modo un po' più dettagliato, ad esempio, sul tipo di semplificazione. Come si può vedere qui, i testi della Bibbia sono molto più semplificati rispetto ad esempi di testi di recente pubblicato o testi per apprendimento del linguaggio. Al livello di tutto, incluso lo scambio lessicale, la semplificazione strutturale o lo scambio di livello. Inoltre, si può vedere che il corpus DePlain ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DePlain API, ci sono molti più riordinamenti e modifiche di parole rispetto al corpus DePlain Web. D'altra parte, nel corpus Web, ci sono molti più riformulazioni. Ora, vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parleremo dei casi d'uso per il nostro dataset DePlain. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici. In recenti anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni automatiche, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre allineamenti di frasi in posti documenti. Ma in questo caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno lo stesso linguaggio e lo stesso contenuto, ma che hanno livelli di complessità diversi. E ora che abbiamo il dataset DePlain, che ha allineamenti manuali, possiamo usare queste frasi come standard di riferimento per valutare i metodi di allineamento proposti. Abbiamo fatto delle modifiche ai metodi proposti e pubblicato tutti i codici per eseguire gli esperimenti nel paper. Alla fine, concludevamo che il miglior metodo di allineamento automatico per testi di semplificazione in tedesco è il metodo MassAlign e puoi trovare il codice per eseguire questo metodo anche per i propri documenti nel paper. Il secondo caso d'uso che ho illustrato nel mio paper è il caso di automatic semplificazione di testo tramite finetuning modelli linguistici per produrre testi semplificati da testi complessi. Abbiamo finetuned due modelli: un modello di Long Impart per produrre semplificazioni al livello dei documenti e un modello base Long Impart per produrre semplificazioni al livello delle frasi. Puoi trovare tutti i checkpoint e ulteriori dettagli sulle performance e le metriche di valutazione dei nostri esperimenti nel paper. Conclusamente, questa basic finetuning può produrre o ottenere punteggini migliori dei punteggini di base e propone i risultati come benchmark per il problema di automatic semplificazione del testo in futuro. Grazie mille per la vostra attenzione e spero di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Il nome della relatrice o del relatore è Kaiyuan.</sample>
    <sample id="5">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il T5-XLARGE.</sample>
    <sample id="6">The speaker introduces the topic of multilingual and cross-lingual summarization, explaining that their work focuses on unifying these two areas into a more general setting called many-to-many summarization. They summarize their contributions by stating that they have unified previous multilingual and cross-lingual summarization models into a single model capable of summarizing documents in any source language and generating summaries in any target language. The speaker also mentions conducting preliminary studies to analyze the differences between multilingual, cross-lingual, and many-to-many summarization, finding that many-to-many summarization can better transfer task knowledge across different languages. Additionally, they propose a three-stage training process for their model, which outperforms previous models and demonstrates the effectiveness of each training stage.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano anche in 2023.</sample>
    <sample id="8">Il nuovo metodo di valutazione umana proposto è quello di valutare le risposte del modello in base a certi comportamenti, piuttosto che chiedere ai giudici umani di valutare la qualità della conversazione.</sample>
    <sample id="9">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla quantità di campioni puliti utilizzati per la selezione del modello.</sample>
    <sample id="10">Per migliorare il punteggio, i modelli dovrebbero avere accesso a maggiori informazioni sul contesto o alle entità stesse.</sample>
    <sample id="11">Jack Hessel, un ricercatore di AI2, ha presentato i benchmark di comprensione dell'humor per i robot, basati sul concorso di captions del New Yorker. I modelli hanno difficoltà a capire gli scherzi e a fornire spiegazioni attendibili.</sample>
    <sample id="12">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="13">Daniel Rotem, presenting his work on adaptive inference in low-resource settings at Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models by using low-capacity models for easy samples. Two common methods are multimodal and early exit. Multimodal involves storing multiple models, each with a classifier, trained separately and run sequentially during inference. Early exit fits multiple classifiers after intermediate transformer layers, running a sample through the model until a classifier decides to halt. The pros and cons of each method were discussed, including versatility, cost, overhead, and performance. The hypothesis of conflicting gradients was tested, showing that multimodal classifiers outperform early exit by an average of 2.3%. The study also measured speed-accuracy trade-off, with early exit outperforming multimodal for later classifiers. The Sweet method was introduced, separating weights in early exit transformers to avoid conflicting gradients. The results showed that Sweet closes most of the gap between early exit and multimodal but negatively affects later classifiers in some cases.</sample>
    <sample id="14">Il mio nome è Adam Sipruckowsky e questo talk parla della struttura di dipendenze della coordinazione. Come potete vedere, ci sono diverse strutture di dipendenze assunti da diverse teorie e approcci corposi. Ad esempio, nella teoria delle universali dipendenze, la struttura di coordinazione tra Lisa, Bart e Maggie è tale che il primo congiunto è la testa della struttura di coordinazione, cioè Lisa. Un approccio simile è assunto in Igore Miltruk's meaning text theory, dove anche qui la struttura di coordinazione è guidata dal primo congiunto, cioè Lisa. Questi due approcci sono asimmetrici, singolano un dei congiunti. Ci sono anche approcci simmetrici alla coordinazione, come il prague approach, che considera la coordinazione guidata dalla congiunzione. In questo caso, le strutture di coordinazione vengono guidate dalla congiunzione, ottenendo dipendenze da un verso a tutti i congiunti. Infine, c'è anche un approccio multi-guidato, come ad esempio in De Catts's word grammar, dove tutti i congiunti hanno una testa di struttura di coordinazione, ottenendo dipendenze dal governante, in questo caso Lisa, a tutti i congiunti separatamente. Ora, l'obiettivo del mio articolo è di fornire un nuovo argomento per le strutture di coordinazione simmetriche come queste due e contro le strutture di coordinazione asimmetriche come queste due. L'argomento si basa sul principio di minimizzazione delle dipendenze, che spiega i seguenti esempi. In inglese, come potete vedere, gli oggetti diretti preferiscono di essere vicini al verbo, mentre gli avverbii possono essere più distanti. Quindi, "March read it yesterday" è corretto perché l'oggetto direttore "it" è vicino al verbo, mentre "March read yesterday it" è peggio, poiché tra il verbo e l'oggetto direttore c'è un avverbio "yesterday". Tuttavia, quest'effetto può essere attenuato quando l'oggetto direttore è pesante o lungo, poiché allora può essere spostato dopo l'avverbio. Questo è illustrato qui: entrambe queste frasi sono corrette. "March read this absolutely fascinating book about the bees yesterday" è corretta, anche se invece di "it" ha un lungo NP, ma è anche corretto dire "March read yesterday this absolutely fascinating book about bees". La ragione qui è che questa frase viola il principio grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, ma soddisfa il principio di minimizzazione delle dipendenze, che dice che le dipendenze più brevi sono preferite. Quindi, queste due strutture solo mostrano la lunghezza delle dipendenze cruciali, cioè le dipendenze che non sono costanti tra queste due strutture. Quindi, qui abbiamo una dipendenza da "read" all'avverbio di lunghezza 7 misurata in parole e una dipendenza da "read" al libro di lunghezza 4, insomma 11. Se spostiamo, se scambiamo questi due componenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6, molto più breve, è per questo che questa suona piuttosto corretta. Viola un principio, ma soddisfa un altro. Quindi, cosa facciamo? Estraiamo statistiche rilevanti da coordinate in versione migliorata di Panto per il Pankbank e vedere il paper per vedere se non usare universali dipendenze. Queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono a essere più corti. Così, "soltanto sale" e "sale e pepe" e "pepe e sale" misurati in sillabe. E anche l'osservazione che è stata fatta in passato che questa tendenza cresce con la lunghezze differenza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto sinistro preferisce essere il primo, più forte. Quindi, la proporcione di congiunti sinistri più corti aumenta. Ma cosa è nuovo in questo paper è che abbiamo osservato che questa tendenza solo si manifesta quando il governante sinistra è assente. Quindi, il governante sinistra in questo esempio è Lisa, quindi il governante sinistra è sulla sinistra. Assente in questo esempio, "Homer came and sneezed", qui c'è coordinazione di due verbi e non c'è un esterno governo esterno. Quindi, in queste case, il congiunto sinistro preferisce essere più corto, tanto più il più grande la differenza tra i due congiunti. Tuttavia, quando il governante sinistra è sulla destra, come qui, "Lafitte governa la coordinazione", questa effetto scompare. Quindi, mostriamo che, misurando la lunghezze in caratteri, che è la prima colonna in sillabe, la seconda colonna in parole, la terza colonna, mi concentro sulla terza colonna. Quello che vediamo qui è che quando il governante sinistra è sulla sinistra, la tendenza per il congiunto sinistro di essere più corto cresce gradualmente con la differenza assoluta in parole. E lo stesso è osservato quando c'è un governo esterno, come in coordinazione di frasi, ma quando il governante sinistra è sulla destra, questa tendenza scompare. E mostriamo in paper come questo fornisce un argomento contro strutture di coordinazione asimmetriche come queste due e favore strutture simmetriche come queste due. Vedete il paper per vedere l'intero argomento e parlare con noi in sessione poster. Grazie.</sample>
    <sample id="15">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="16">I domini dei testi biblici risulteranno più semplificati rispetto a quelli dei testi di news o dei testi per apprendimento del linguaggio.</sample>
    <sample id="17">The speaker introduces a method for multi-modal relation extraction, which involves combining textual and visual data to better understand relationships between entities in texts. They propose a framework that includes fine-grained information pruning and the use of external information like topic information to address issues such as internal information over-utilization and external information under-exploitation. The method uses graph information and multi-modal topic features to enhance the overall context and achieve significant improvements over existing models.</sample>
    <sample id="18">Salt and pepper, salt and pepper</sample>
    <sample id="19">The speaker introduces their work on efficient open-domain question answering, which was accepted by ACL 2023. They present the two-stage model proposed by Dandan Chen in 2017, involving retrieval and reading stages. The work aims to achieve smaller memory costs, faster inference, and comparable performance. The presentation includes a comparison of existing models from different aspects, such as memory usage and performance, and discusses future works on deployment in low-power devices and evaluation metrics.</sample>
    <sample id="20">Sì, i modelli sono liberi di utilizzo e possono essere utilizzati per la ricerca.</sample>
    <sample id="21">DEplain-apa è basato su testi di notizie.</sample>
    <sample id="22">Per una buona generalizzazione, i fattori principali sono la struttura del modello, la dimensione del modello e l'uso di più esempi di finetuning.</sample>
    <sample id="23">The paper discusses the challenges faced by text-image models in rendering visual text and proposes a new strategy to improve model spelling ability. The authors investigate the performance of different text encoders, such as T5 and PaLM, and find that they struggle with spelling words correctly. They also explore the use of bitT5, which has full access to character-level information, and show that it performs well in spelling. To improve image generation characteristics and text rendering ability, the authors augment the Imagen model by adding an additional text representation from bitT5. This results in improved model spelling ability and better image generation characteristics.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistiche estratte da una versione migliorata del Corpus di Pankration. Queste statistiche hanno confermato l'osservazione precedente che i congiunti a sinistra tendono ad essere più brevi, in modo che la differenza di lunghezza tra i due congiunti cresce con la differenza di lunghezza tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati per estrarre statistiche da un database di testate in inglese.</sample>
    <sample id="26">Un classificatore base addestrato su un insieme di dati non bilanciato non è molto più efficace rispetto a un modello casuale.</sample>
    <sample id="27">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="28">Bob e Alice</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="30">The speaker introduces a new framework called Blender for large language models, which uses pairwise ranking and generative fusion to select the best model for each input. The framework consists of two stages: first, it runs multiple models and ranks them using a pairwise ranking module called Pair Runner; second, it selects the top k models and uses them as input to a generative fusion model to produce the final output. The speaker also introduces a new dataset called Mix Instruct for evaluating the performance of large language models.</sample>
    <sample id="31">I fornitori dell'articolo sono Costruendo Sina, John Gauthier, Aaron Mueller, Kanishka Mishra, Garen Fuentes, Roger Levy e Adina Villamor.</sample>
    <sample id="33">Il framework quantifica esattamente la posizionalità confrontando le annotazioni di diversi utenti con le predizioni e le etichette dei modelli e dei set di dati.</sample>
    <sample id="34">The speaker introduces a framework called Crest, which combines selective rationalization and counterfactual text generation. The framework uses a rationale model to generate counterfactual examples by masking parts of the input and using an editor to fill in the masked spans with new tokens. Human evaluation experiments showed that Crest-generated counterfactuals were more valid and natural than those produced by other methods. The framework also improves downstream models when used for data augmentation and rationalization with both factual and counterfactual examples. Overall, Crest produces valid, fluent, and diverse counterfactuals in a controllable way, leading to plausible explanations that focus on the contrastive parts of the input.</sample>
    <sample id="36">The paper presents a method for increasing the capacity of multilingual machine translation models by using language-specific layers (LSLs). The authors propose placing LSLs in different positions within the model and training the model to learn the best placement. They evaluate their approach on the WMT 21 newstest task sources for ten languages, including some low-resource languages, and report improvements over both language adapters and baseline models. The results show significant improvements for every language, particularly for low-resource languages, and the approach is statistically significant for 84 out of the 90 translation directions.</sample>
    <sample id="37">Il risultato dello studio precedente è stato che i soggetti umani hanno anche rivelato stereotipi, ma con una distribuzione di parole più ampia rispetto alle risposte generate dal modello.</sample>
    <sample id="38">In questo studio, i ricercatori hanno utilizzato statistiche estratte da una versione migliorata del Corpus di Pianta per analizzare la coordinazione.</sample>
    <sample id="39">Il numero di autori coinvolti nell'articolo è di due.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di discordi indipendenti e la classificazione bimane di espansione e comparazione.</sample>
    <sample id="41">The paper presents a large-scale, high-quality personal common sense knowledge graph called PicoK, which contains about 3.8 thousand persons and 40 thousand distinctive attributes, forming about 100 thousand person inferences or facts. PicoK is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-validation using a joint human-AI majority voting scheme. The paper compares the performance of PicoK with large-scale pre-trained language models, including GPT-3 and GPT-3.5, on various natural language generation metrics and human evaluation. The results show that PicoK achieves better automatic evaluation results and higher acceptance rates in human evaluation, indicating its potential as a reliable personality base for lightweight language models to learn knowledge generation capabilities.</sample>
    <sample id="42">Il numero di autori che hanno collaborato all'articolo non è specificato.</sample>
    <sample id="43">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="44">Il framework differisce dai precedenti studi in quanto confronta le annotazioni reali con i modelli e i set di dati, invece di analizzare solo l'acordo tra gli annotatori.</sample>
    <sample id="45">Lessico di stereotipi</sample>
    <sample id="46">Google Translate e DeepL.</sample>
    <sample id="47">Ciao, mi chiamo Changbin e sono uno studente di PhD all'Università di Washington. Oggi presenterò il nostro lavoro sulla trasformazione dei dati di addestramento in modelli di linguistica, fino a compiti downstream che rilevano i tratti di bias politici che portano a modelli NLP non equi. I modelli di linguistica sono addestrati su grandi quantità di dati web, tra cui notizie politiche, che sono bene rappresentate nei nostri dati di addestramento. Secondo un saggio del C4 Corpus, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post ecc. sono bene rappresentati nel dataset di addestramento dei modelli di linguistica. Questo ha creato un mix benedetto per le applicazioni dei modelli di linguistica: da un lato hanno imparato da diverse prospettive che celebrano la democrazia e la pluralità di idee; dall'altro hanno acquisito opinioni politiche socialmente biased che potrebbero portare a problemi di giustizia in compiti downstream. Per risolvere questo problema, proponiamo di indagare il percorso di propagazione dei bias politici dal dataset di addestramento ai modelli di linguistica fino ai compiti downstream, specificamente chiedendo le seguenti domande: primo, come valutiamo la linie politiche dei modelli di linguistica e qual è il ruolo dei dati di addestramento precedenti in tale bias politico? Secondo, come i modelli di linguistica con diverse linie politiche realmente performano sulle applicazioni downstream e se queste performance potrebbero portare a problemi di giustizia in applicazioni NLP? Iniziamo con la proposta di stimare modelli di linguistica con diverse formule di prompt utilizzando domande politiche come il test di compasso politico, assicurandoci di farlo in modo automatico e basato sulla letteratura di scienze politiche. I risultati preliminari dimostrano che i modelli di linguistica hanno vari linie politiche, occupano tutti i quadranti del compasso politico, e GPT-4 è il modello di linguistica più liberale tra tutti, mentre GPT-3 series sono generalmente più liberali di BERT series e le sue varianti. Inoltre, cerchiamo di indagare in quanto misura i bias politici dei modelli di linguistica sono stati presi in considerazione dai dati di addestramento. Per farlo, eseguiamo un esperimento controllato pre-addestrando modelli di linguistica su sei diversi corpus partizani separati in notizie e media social, ulteriormente divisi in loro linie politiche. Pre-addestrando modelli di linguistica su tali corpus partizani, possiamo vedere che le coordinate ideologiche dei modelli di linguistica si spostano corrispondentemente, ad esempio, se Roberta è ulteriormente finetuned e ulteriormente addestrata su un corpus di notizie di sinistra, possiamo vedere un sostanziale spostamento verso la sinistra in termini dei propri bias politici. Cerchiamo anche di indagare se i modelli di linguistica possono prendere in considerazione la polarizzazione prevalentemente nella nostra società. Dividiamo i corpus partizani in due periodi temporali: prima e dopo il 45° Presidente degli Stati Uniti. Possiamo vedere che i modelli di linguistica hanno generalmente una linia politica che si sposta più lontana dal centro dopo il 2017, il che indica che i modelli di linguistica possono anche prendere in considerazione la polarizzazione nella nostra società. Infine, valutiamo i modelli di linguistica con diverse linie politiche in deteczione di discorso razzista e falso, due applicazioni NLP che spesso coinvolgono modelli di linguistica e hanno implicazioni significative. Se analizziamo il rendimento per category, ovvero se suddividiamo il rendimento in diverse demografiche o linie politiche di notizie, possiamo vedere un模式，ad esempio, per la deteczione di discorso razzista, i modelli di linguistica sinistri sono migliori in deteczione di discorso razzista che bersera gruppi minoritari, tuttavia sono peggiori in deteczione di discorso razzista che bersera gruppi più potenti nella nostra società e viceversa. I modelli di linguistica di destra sono migliori in deteczione di discorso razzista che bersera gruppi bianchi e maschili, tuttavia sono peggiori in deteczione di discorso razzista che bersera gruppi neri, LGBTQ+ e altre comunità minoritarie. Simili tendenze si verificano anche per la deteczione di notizie false, dove i modelli di linguistica sinistri sono migliori in deteczione di menzogne provenienti da un orientamento politico opposto e viceversa. Ci sono molti esempi qualitativi per vedere che i modelli di linguistica con diverse linie politiche danno predizioni diverse per esempi di discorso razzista e menzogne basate sul proprio social calcolo. Ci sono molti altri esempi in appendix per ulteriormente evidenziare che ci sono problemi di giustizia relativi ai bias politici dei modelli di linguistica. Ad esempio, se i modelli di linguistica di destra vengono addestrati per la deteczione di discorso razzista o menzogne e vengono deployati su piattaforme social media popolari, potrebbe significare che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso razzista bersera gruppi minoritari potrebbe continuare senza alcun controllo. Questo ha suscitato l'allarme per noi di riconoscere e affrontare i problemi di giustizia derivanti dai bias politici dei modelli di linguistica. Un po' di discussione: vorremmo anche evidenziare che esistono delle sfide uniche riguardanti i bias politici dei modelli di linguistica. Se non sanizziamo le opinioni politiche dei dati di addestramento dei modelli di linguistica, i bias potrebbero propagarsi dal dataset di addestramento ai modelli di linguistica fino ai compiti downstream, creando problemi di giustizia. Se proviamo a sanizzarli in qualche modo, rischiamo di censurare o escludere e è incredibilmente difficile determinare cosa sia davvero neutra e dovrebbe essere retain in i dati di addestramento dei modelli di linguistica. Sembra quasi come un problema di elettricità elettrica. Ok, bello, penso che sia tutto per oggi. Grazie per il tuo tempo.</sample>
    <sample id="48">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 124 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation is about a new corpus called DePlain, which is designed for German text simplification at both the document and sentence levels. The speaker introduces the concept of text simplification, explaining that it involves adapting text to improve comprehension for specific target groups, such as people with reading difficulties or non-native speakers. The presentation highlights the challenges with existing corpora, such as being too small or automatically aligned, and introduces DePlain as a solution. DePlain is split into two sub-corpora: DePlain API, based on news texts, and DePlain Web, which includes various domains. The corpus contains 30,130 parallel sentence pairs for DePlain API and 30,450 for DePlain Web. The speaker also discusses the variety of simplification transformations in the corpus and its potential applications, including evaluating automatic alignment methods and fine-tuning language models for text simplification.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono music, libri e ricette.</sample>
    <sample id="52">Posizionalità è semplicemente la prospettiva che le persone hanno come risultato dei loro demografici, identità e esperienze di vita.</sample>
    <sample id="53">Il relatore è Dawei.</sample>
    <sample id="54">The speaker, Vasudha, is a PhD candidate in computer science at Stony Brook University. She is presenting her work accepted into ACL 2023 on transfer learning for dissonance detection, addressing the rare class challenge. The presentation begins by defining cognitive dissonance and its importance in studying language, followed by an overview of the research's significance in understanding dissonance in daily decision-making, its relation to anxiety disorders, and its role in understanding extremism and polarization of vulnerable groups. The presentation also discusses the challenges faced in annotating dissonance relations and the use of transfer learning and active learning to improve dissonance detection. The speaker concludes by highlighting the importance of understanding personal cognitive styles and decision-making processes through the study of cognitive dissonance.</sample>
    <sample id="55">Sì, l'approccio proposto utilizza modelli offline esistenti e non richiede retraining o adozione di specifiche architettura per il traduttore immediato.</sample>
    <sample id="56">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) il contesto di pre-adesione, dove la conoscenza di sfondo è considerata disponibile in tempo reale; 2) il contesto di entrambe le varianti, dove la conoscenza di sfondo è disponibile sia in tempo reale che in tempo precedente; 3) il contesto di inferenza, dove entrambe le tipologie di conoscenza sono disponibili solo in tempo reale.</sample>
    <sample id="59">The presentation introduces Dr. Bert, a robust pre-trained model in French for biomedical and clinical domains, and compares it with other models trained on different data sources. The results show that Dr. Bert performs better than other models on most tasks, especially when using more data. The presenter also discusses the availability of the pre-trained model and training scripts.</sample>
    <sample id="60">Le affiliazioni degli autori dell'articolo sono Javahar Hosaini, Philip Radlinski, Sylvia Parthie e Annie Lewis.</sample>
    <sample id="61">La terza domanda di ricerca è se dobbiamo usare solo i campioni puliti per la validazione o se ci sono altre migliori modalità per utilizzarli.</sample>
    <sample id="62">The speaker, Intakhal Deron, is the main author of a paper on knowledge distillation for natural language generation with pseudo-target training. The paper explores the potential of knowledge compression and compares different approaches for knowledge distillation in NLP tasks such as summarization, question generation, common sense reasoning, and simplification. The study focuses on efficiency, inference time, and one-time training resources, and proposes a novel knowledge distillation technique called joint teaching to address student exposure bias and improve learning.</sample>
    <sample id="63">La metrica della sensibilità misura la capacità del modello di produrre gli stessi output per le stesse attività, indipendentemente dalla variazione nella redazione dell'instruzione.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">In generale, una maggiore sensibilità indica che il modello è più suscettibile a variare i propri output in risposta a slight variazioni nella redazione dell'instruzione. Questo può essere considerato come un segno di cattiva performance del modello, poiché desideriamo che il modello produca output coerenti anche quando le istruzioni cambiano leggermente.</sample>
    <sample id="66">The presentation discusses the importance of mathematical reasoning in human intelligence and its applications in AI and NLP. It covers topics such as deep learning for mathematical reasoning, automated theorem proving, and neural network architectures for solving mathematical problems. The presentation also highlights the challenges and limitations of current models and proposes new approaches to improve their performance.</sample>
    <sample id="67">The paper discusses the impact of interference in multilingual translation models and proposes methods to mitigate it. The authors find that severe interference occurs when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. They also conclude that language similarity and the number of languages do not have a large impact on interference levels. The simplest solution is temperature sampling, which allows to sample more training examples from lower resource languages. The results show that a baseline for battling interference is weak due to size in small models and weak due to uncalibrated temperature for larger ones. Tuned temperature is key for strong performance.</sample>
    <sample id="68">I modelli vengono addestrati su un contesto linguistico che include sia frasi accettabili che non accettabili.</sample>
    <sample id="69">In generale, per raggiungere prestazioni elevate in WSL, si possono usare fino a 20 campioni per classe. Tuttavia, se si opta per l'approccio di ottimizzazione fine, i campioni puliti possono anche essere utilizzati direttamente per ottenere prestazioni migliori.</sample>
    <sample id="70">I ricercatori che hanno scritto l'articolo sono Myra, Essendarmouch e Dan Jorovski.</sample>
    <sample id="71">The speaker is discussing their work on resolving indirect referring expressions for entity selection. They introduced the Alt Entities Corpus and collaborated with others to understand user language when making choices. The goal is to have a more natural conversation, especially when users cannot remember the name of an entity or when pronunciations are similar. They collected data using crowdsourcing and created a dataset with three domains: music, books, and recipes. They used a cartoon completion set to generate alternative questions and indirect referring expressions. The accuracy of the language model improved as it had access to background knowledge, but there is still room for improvement.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione per rilevare le differenze politiche che possono influenzare il modello di linguaggio e le applicazioni downstream. Questo è importante per garantire che i modelli siano equiamente rappresentativi e non discriminanti, evitando la marginalizzazione di gruppi specifici o la diffusione di informazioni false.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Mac Shatah.</sample>
    <sample id="74">The speaker introduces a new technology called "DenseNOM" that enhances the connection between atoms with high knowledge coverage and massive multi-hop paths. DenseNOM is designed to improve the performance of machines interacting with humans by describing space and related judgments in everyday life. The technology uses a large-scale common technology base, which covers events centered around social spaces of information knowledge gaps. DenseNOM contains very few multi-hop paths since an unattached tail event cannot become the head event of the triplet. DenseNOM also contains more multi-hop paths, such as A2B and A2A links, which cause an exponential increase in knowledge coverage despite its high-quality human-identified common sense knowledge.</sample>
    <sample id="75">The speaker, Jing Yandan, introduces their work on joint entity recognition and relation extraction. They discuss the challenges of supervised learning models requiring extensive labeled data and propose a joint semi-supervised learning framework to integrate labeled and unlabeled data. The framework includes four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The experiments show significant improvement over baseline models in both joint and single task datasets.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici include la formazione di modelli di linguaggio, la formazione di modelli di linguaggio con diverse linee politiche e l'investigazione di modelli di linguaggio con diverse linee politiche.</sample>
    <sample id="77">The video introduces a new dataset called Defacto, which contains human demonstrations and feedback for improving summarization factual consistency. The dataset includes comparative analysis and offers insights into the factual consistency of summarization models. Three new NLG tasks are proposed: summary editing, feedback generation, and automatic factual error correction. The dataset is based on the XSum dataset, and the initial system outputs are collected from the pre-trained Pixie's model. The data statistics show that 70% of the data points contain factual errors, and human-edited summaries receive higher automatic factuality scores compared to the initial system output. However, there is a lower textual overlap between reference summaries and human-edited summaries. The video also shows the distribution of annotated editing instructions and their relation with different error types.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="79">Sì, Coscript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita esattamente quando il numero di trigger in una frase è maggiore di M.</sample>
    <sample id="81">I ricercatori dell'articolo sono affiliati all'University of Pennsylvania.</sample>
    <sample id="82">The video discusses a new framework for unsupervised automated essay scoring (AES) that uses multiple heuristics to generate pseudo ground-truth scores and train a neural AES model. The proposed framework, called URRA, consists of two main components: a hierarchical essay ranking module (HERM) and a deep pairwise rank aggregation module (DPRAM). HERM generates partial order pairs by ranking essays according to heuristics, while DPRAM aggregates these pairs into a unified supervision signal. The framework also includes a scoring strategy to transform predicted scores into the predefined score set. Experiments on both transductive and inductive settings demonstrate that URRA outperforms unsupervised baselines with a large improvement and achieves competitive performance compared to supervised methods.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">The speaker introduces the topic of dynamic networks, contrasting them with traditional static networks. They explain that dynamic networks can adapt their architecture or parameters based on input, unlike static networks which have fixed parameters. The speaker discusses the challenges and inefficiencies in fully dynamic networks, such as excessive parameter usage, and proposes a method to partition parameters into dynamic and static ones using scale factors. This approach aims to maintain better performance while reducing the number of parameters and computational needs. The speaker also mentions future work directions, including extending the method to other network types and hardware structures, and exploring new models like combination among zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è fare un pasticcino con specifici requisiti, come far un pasticcino con specifici ingredienti o specifici strumenti.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo utilizzando un dataset di testo nascosto e un dataset di testo reale.</sample>
    <sample id="87">Il lavoro utilizza i modelli pre-addestrati per costruire uno nuovo.</sample>
    <sample id="88">GPT-4 è meno allineato con i paesi non binari.</sample>
    <sample id="89">La relatrice fornisce un esempio in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione, ma non specifica la frase esatta.</sample>
    <sample id="90">This paper explores the feasibility of using language learners as annotators for natural language processing (NLP) data annotation. The authors conducted a proof-of-concept study to examine the accuracy and learning effects of language learner annotations compared to native speakers. They recruited language learners with varying proficiency levels and provided them with additional resources to help understand annotation samples. The results showed that language learner annotations were nearly accurate, especially for simpler tasks and easy to medium level questions. Aggregating labels from multiple language learners also brought their performance on par with native speakers. Furthermore, training simulations with learner annotations achieved about 95% of ground-truth performance, sometimes outperforming models trained with native speaker labels. This suggests a novel way for data construction in low-resource languages by recruiting language learners as annotators, potentially broadening NLP research for many languages.</sample>
    <sample id="91">In quanto la quantità di attività aumenta, il modello raggiunge un'efficienza migliore e allo stesso tempo una sensibilità più bassa.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con altri modelli senza alberi sul benchmark CoNLL.</sample>
    <sample id="93">I due coautori sono collaboratori del primo autore.</sample>
    <sample id="94">The video introduces a paper titled "Protecting the Copyright of Large Language Models for Embedding and Services" by Jing Wei from the University of Science and Technology of China. The paper discusses the need to protect the copyright of embedding services, which are built upon large language models like GPT, BERT, and PEG. The paper proposes a backdoor-based watermark method called Embedding Marker that is applicable to embedding services. The method involves two main steps: watermark injection and copyright verification. Watermark injection defines a target embedding based on the number of triggers in a sentence, while copyright verification uses a backdoor dataset to detect whether another service contains the watermark. The paper validates the detection performance and covertness of the provided embedding through experiments on four datasets.</sample>
    <sample id="95">Il primo autore di PaLM è Avi Birlar.</sample>
    <sample id="96">La presentazione di Jenny, studentessa di primo anno di laurea in Scienze Politiche all'Università Carnegie Mellon, si concentra sul concetto di "posizionalità" e come influisce sulla qualità dei modelli e dei dataset in campo di apprendimento automatico. Jenny ha collaborato con ricercatori dell'Università di Washington e dell'Alliance Institute for AI per sviluppare un nuovo approccio che analizza la posizionalità dei modelli e dei dataset. Questo nuovo approccio utilizza una piattaforma di crowdsourcing online chiamata "Lab in the Wild" per raccogliere annotazioni da diverse fonti. Gli esempi forniti includono il test di accettabilità sociale e il test di rilevanza del linguaggio offensivo. I risultati hanno dimostrato che i modelli e i dataset tendono a essere più correlati con le culture e le demografie specifiche, lasciando alcune popolazioni indietro. Le raccomandazioni fornite includono la documentazione di tutte le scelte di progettazione durante la ricerca, l'uso di un lens di perspectivismo nella ricerca e la creazione di dataset e modelli specializzati per specifiche comunità.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: i modelli specifici, le procedure di addestramento lunghe e complesse, e la necessità di addestrare e mantenere più modelli per raggiungere differenti regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere la sanitizzazione dei bias. Questo potrebbe essere fatto utilizzando tecniche di pulizia dei dati o di addestramento supervisionato, in cui i modelli vengono addestrati su un set di dati che è più neutro e rappresentativo di diverse ideologie e gruppi di età. Tuttavia, è importante notare che la sanitizzazione dei bias può anche portare a censure o esclusione, rendendo difficile determinare cosa sia realmente neutro e dovuto essere retain in un set di addestramento di NLP.</sample>
    <sample id="99">Hi, I'm Si Yu Yan from Fudan University. I am here to introduce our work distinguishing script knowledge from large language models for constrained language planning. In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as making a cake and showed that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for abstract goals of stereotypical activities. Planning for the goals with specific goals, specific constraints, such as making a chocolate cake, still remains understudied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goal of planning. An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should read scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no data set of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why large language models fail. The figure shows that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed. We dig into more fine-grained topological categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of InstructGPT is relatively considerably for goals of different categories. Previous studies have shown that the output quality of large language models forth in high variance leading to bad performances. Thus, we adopt the idea of over-generated the Z filter to improve generation quality. We first show constrained types with examples for InstructGPT and obtain specific goals based on the said abstract goals. Then InstructGPT overgenerates key scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating data set is an essential step to this end. However, previous studies do not enable planning for specific goals and manual manual data set annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning data sets from large language models. We apply our method for building a data set of constrained language planning, named as CoScript. In total, we generate 55 thousand specific goals with scripts to ensure the quality of validation and test sets. We ask crowdsourced workers to find and revise the incorrect samples. This figure shows the constrained distribution of CoScript. We find CoScript shows high probability in the generated specific goals. With CoScript, we can train smaller but specialized models for constrained language planning. We find that T5 finetuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger larger models when probably trained on suitable data sets. In summary, we establish the constrained language planning problem. We evaluate the constrained language planning ability of large language models and develop a over-generated the Z filter method for large language models. We use a large language models to generate a high-quality script data set, CoScript, for constrained language planning. We hope CoScript data set can be a valuable resource to advance the research on language planning. Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, with each step typically corresponding to a document in the corpus. For instance, to answer the question about which 1988 Christmas comedy film Brian Doyle Murray starred in, one must first identify all movies he appeared in and then find the one released in 1988. This process is called chaining. Multi-hop retrievers are trained by maximizing the probability of generating the correct chain for a given question. The proposed approach uses an unsupervised retrieval method combined with a few-shot language model-based reranking to achieve good performance with fewer examples. The method involves retrieving candidate chains, reranking them using a language model, and constructing a chain prompt to score each chain based on the likelihood of the question given the chain prompt.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi d'area.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono che dovrebbe essere applicabile a servizi di embedding, non dovrebbe indebolire l'utility dei servizi forniti, dovrebbe essere abbastanza veloce per impedire all'attaccante di rimuovere facilmente la filigrana e dovrebbe essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="104">Per la riannotazione, vengono campionate diverse istanze.</sample>
    <sample id="105">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la somma dei coseno e la somma dei quadrati.</sample>
    <sample id="106">The speaker introduces a paper titled "Quest" and explains that it involves working with a dataset of over 3,000 entity-seeking queries containing implicit set operations. The queries are validated for relevance, and their associated documents are marked with attributable spans for different query constraints. The dataset poses a challenging retrievable problem as systems need to effectively search over a large document corpus to find multi-answer sets where the attribution for different query constraints can come from different parts of the document. The speaker also mentions that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">I modelli basati su codificatori multilingue hanno ottenuto le migliori prestazioni su tutti i nove dataset. Inoltre, i modelli di encoder-decoder hanno dimostrato essere più adatti per la traduzione zero-shot e il trasferimento zero-shot rispetto ai modelli di encoder-pdr.</sample>
    <sample id="108">The speaker introduces a new approach to evaluating language models, focusing on their ability to handle longer sentences. The current method, the Minimal Pair Paradigm (MPP), is limited in this regard. The proposed method involves simulating longer sequences by recreating sentences with acceptable or unacceptable structures from existing datasets. This allows for testing the model's acceptability judgments over extended contexts. The speaker demonstrates this by using grammatical data sets and comparing results with Wikipedia sentences to assess context impact.</sample>
    <sample id="109">The speaker introduces a dataset called "natural instructions" which contains natural language instructions and their corresponding inputs and outputs. The data was collected in a fully automatic manner without any human annotations. The model generates an instruction and a corresponding input, and then generates a corresponding output. The resulting dataset contains 64K examples, and if we consider the instruction paraphrases, we have about 240K examples. The generated examples are correct and contain valuable information for instruction tuning. The dataset highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers who usually collapse into predictable heuristics and form annotation artifacts.</sample>
    <sample id="111">Gli autori decidono di selezionare un insieme di parole a frequenza moderata utilizzando un corpus di testo generale e conteggiano la frequenza di ogni parola.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Zhu Heng. Oggi presenterò il mio articolo: "Gestori di entità denominate di Conll 2003 continuano a funzionare bene nel 2023?". Vediamo di iniziare. Il mio articolo esamina il problema della generalizzabilità utilizzando la comprensione di entità denominate o compiti di riconoscimento di entità (NER). Abbiamo osservato che i modelli hanno utilizzato Conll 2003 per sviluppare NER per quasi venti anni, e questo naturalmente solleva diversi problemi. Innanzitutto, possono questi modelli generalizzare a dati moderni? E quando sviluppiamo nuovi gestori, cosa è necessario per una buona generalizzabilità? Allo stesso tempo, se osserviamo una peggiorazione delle prestazioni, cosa causa lo sforzo di performance dei modelli? Per investigare questi problemi, abbiamo sviluppato il dataset Conll Plus Plus. Questo è un dataset che abbiamo raccolto da Reuters News nel 2020 e annotato con le stesse guidelines di annotazione di Conll 2003. Abbiamo quindi finetuned più di venti modelli su Conll 2003 e li abbiamo valutati sia sul set di test di Conll 2003 che sul set di test Conll Plus Plus. Infine, abbiamo calcolato il percentuale di variazione in F1 per valutare la generalizzabilità di ogni modello. Cos'è necessario per una buona generalizzabilità? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari. Il primo è l'architettura del modello. Attraverso gli esperimenti, abbiamo scoperto che i modelli Transformer generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzabilità. Infine, nonostante sappiamo che il numero di esempi di finetuning direttamente influenza le prestazioni dell'attività di basso livello, abbiamo anche scoperto che più esempi di finetuning effettivamente portano a una migliore generalizzabilità. Il nostro prossimo quesito è: cosa causa la diminuzione delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattativa, che è l'overfitting causato da ricorrere alla stessa serie di test continuamente e questa di solito si manifesta come la diminuzione dei ritorni su nuovi test set. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dallo spazio crescente tra i dati di training e i dati di test. Per l'overfitting adattativo, vediamo che dalla grafica sulla destra la linea di miglioramento rossa ha un coefficiente di regressione maggiore di 1. Questo significa che ogni unità di miglioramento che facciamo su Conll 2003 traduce in più di una unità di miglioramento su Conll Plus Plus, il che significa che non ci sono ritorni diminuendosi. Questo ci dimostra che l'overfitting adattativo in questo caso non è osservato. Quindi, cosa succede con il drift temporale allora? Per il drift temporale, abbiamo fatto un esperimento di retraining o continuare a pretrainare alcuni modelli con più recenti dati e abbiamo scoperto che le prestazioni degradano con un intervallo temporale più grande. Questo conferma la nostra ipotesi che la principale causa della diminuzione delle prestazioni è il drift temporale. La nostra conclusione è che per una buona generalizzabilità, saremmo in grado di avere una migliore architettura del modello, dimensioni del modello più grandi, insieme a più esempi di finetuning. Questi ingredienti devono essere utilizzati insieme, non possiamo avere solo un ingrediente ma anche gli altri. Allo stesso tempo, abbiamo anche scoperto che la diminuzione delle prestazioni qui è causata dal drift temporale e, sorprendentemente, non è causata dall'overfitting adattativo anche se Conll 2003 è stato utilizzato per più di venti anni. Quindi tornando alla domanda che avevamo sollevato in titolo, Conll 2003 gestori continuano a funzionare bene nel 2023? E la risposta è, in reality, un risolutivo sì. Spero che il mio articolo possa fornire maggiori ricerche su come migliorare la generalizzabilità dei modelli. Infine, vi prego di controllare il mio articolo, il mio dataset e se avete qualsiasi domande, non esitate a contattarmi. Grazie mille!</sample>
    <sample id="114">The speaker introduces their work on ACL 2023, focusing on the heavy parameter problem of large language models. They discuss the limitations of current methods, such as homogenization and diversification, and propose a new approach called group head attention using a divide-and-conquer strategy to compress multi-head attention. The method involves two stages: group constraint training and voting-to-stay algorithm. The results show significant parameter compression and performance improvements in tasks like machine translation, language modeling, and abstract summarization.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di dimensione lambda.</sample>
    <sample id="116">Per risolvere il compito, è necessario conoscere che Servin è un giudice e che i giudici decidono casi in una corte.</sample>
    <sample id="117">La qualità dell'esempio è il fattore più importante.</sample>
    <sample id="118">The speaker is presenting a submission on improving pre-training techniques for code-switched NLP. They define code-switching as the mixing of languages in a sentence, and provide an example of a code-mixed sentence in English and Hindi. They explain that building computational models for code-switching is important, especially in linguistically diverse communities like India. They propose novel MLM techniques and architectural changes to handle code-switching. They also introduce a surrogate method called Frequency MLM and propose some architectural modifications to help with code-switching. The results show that their combined method performs the best on sentiment analysis tasks.</sample>
    <sample id="119">GPT-4 e GPT-3 series</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Gli esempi di inferenza diretta sono direttamente rilevanti per la scelta dell'entità, ad esempio dire il nome del brano o il suo posizionamento.</sample>
    <sample id="122">I fornitori dell'articolo sono affiliati all'Università di Fudan.</sample>
    <sample id="123">The speaker introduces their research on multi-instruct, a method for improving multi-modal zero-shot learning through instruction tuning. They discuss the challenges of using pre-trained language models for downstream tasks and the lack of multimodal instruction datasets. They present MultiInstruct, a multimodal instruction-tuning benchmark dataset with 62 diverse tasks covering 10 broad categories, derived from 21 existing open-source datasets. They use OFA, a unified multi-modal pre-trained model, as the base model and evaluate its performance across five instruction templates for each task. The results show that instruction tuning significantly improves OFA's performance on unseen multi-modal tasks, and transfer learning from natural instruction datasets can further enhance performance and reduce sensitivity.</sample>
    <sample id="124">The speaker introduces the topic of temporal reasoning in language models (LMs) and explains how it can be broken down into three levels: time-to-time, time-to-event, and event-to-event. They discuss the limitations of previous studies on temporal reasoning and propose a new dataset that covers all three levels and long temporal coverage. The speaker also presents a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. The results show that the proposed model, TempT5, significantly improves the performance of LMs in temporal reasoning tasks compared to other models.</sample>
    <sample id="125">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="126">No, è stato considerato come un approccio non standard.</sample>
    <sample id="127">The speaker introduces their work on large language models as reasoning teachers, a joint project with Laura Schmidt and Professor Seo Young. They propose using large models to transfer reasoning abilities to smaller models through a technique called diverse reasoning, which involves generating multiple step-by-step solutions for complex tasks. Their method, called fine-tune COT, significantly outperforms existing baselines and vanilla fine-tuning on various tasks, even with small models. The results show that the method is scalable but comes with development and inference time costs.</sample>
    <sample id="128">Hello everyone. I'm Makkat and today my co-author Martin and I are presenting our work, the KITMST, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pre-training and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-training knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence John saw the newly elected president on TV, pre-training parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-training and inference time knowledge. In this work we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset. Merlin is a judge. Kia is a baker. Merlin and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun he refers to, which in this case is Merlin. The resolution of a given pronoun requires two types of information. First entity-specific knowledge such as Merlin is a judge. And second background knowledge such as judges decide cases in law courts. Generally background knowledge is learned during the pre-training of large language models while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KITMST. First we have the background pretrain setting where background knowledge is assumed to be available at pre-training time. Second there's a background both setting where background knowledge is available both at pre-training time and inference time. Lastly the background inference setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-training data of models. For example because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources. In the background pretrain setting we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters. In the inference-time context we provide the anti-specific knowledge, Chester is a politician. In the background both setting we additionally provide not only anti-specific but also background knowledge about politicians in the inference-time context. In the background inference setting we provide the fictional occupation, Meritura instead of politician because Meritura is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure we show the results of the best-performing models on the most difficult variant of the background pretraining setting. Without task-specific training on KITMST both models do not perform well. When trained on KITMST however both C2F and BERT for coref perform significantly better than the random choice. This suggests that when trained on general coreference resolution datasets models learn to exploit surface cues which are not useful when testing on KITMST where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However with task-specific training some models successfully integrate knowledge from multiple sources. Still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="129">Gli autori hanno fornito un esempio di gruppo contrassegnato come "woman of color" e hanno analizzato le parole che definiscono queste figure, rivelando una serie di stereotipi e notazioni essenzializzanti.</sample>
    <sample id="130">Architettura dei modelli che non generalizzano adeguatamente sono quelli che non usano il modello di transformer.</sample>
    <sample id="131">Il nome dei set di dati di test non è specificato.</sample>
    <sample id="132">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="133">L'autore opera con più modalità.</sample>
    <sample id="135">The video discusses a new method for evaluating conversational AI called ABC Eval, which measures the rates at which chat models commit various thematic errors. The method is more reliable and informative than existing methods, as shown by its higher inter-annotator agreement and predictive power of overall conversation quality. The combination of all ABC Eval metrics explains over 25% of conversation quality, while the combination of all turn-level Likert metrics explains far less. The video also highlights challenges in the field, such as common sense violations, irrelevant information, and contradictions, which could be addressed by future models.</sample>
    <sample id="136">The speaker introduces a new evaluation set called Fermat, which assesses the mathematical abilities of language models. They argue that current benchmarks are not informative enough and propose using Fermat to evaluate number understanding, mathematical operations, and training dependency. The results show that most models perform poorly on these aspects, but fine-tuning with math teachers' templates can improve performance. The speaker also highlights the importance of language and mathematical diversity in improving model performance.</sample>
    <sample id="137">The speaker, Sisong from the Singapore University of Technology and Design, introduces a dataset called Tell to Design, which is a dataset for language-guided floor plan generation published in AIC 2023. The dataset consists of 5,501 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk and around 76,000 artificially generated language instructions from predefined templates. The main challenges of this novel task are to perform design generation under strict constraints, understand the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and handle ambiguous, incomplete, or misleading information in human instructions. The speaker proposes a sequence-to-sequence model using a Transformer-based encoder-decoder structure to generate floor plan layouts from language instructions. The model is initialized by a pre-trained language model T5 for better language understanding capabilities. The results show that the proposed method achieves the highest Iou scores with a micro Iou of 54 and a macro Iou of 53, outperforming other text conditional image generation baselines by a large margin.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare e utilizzare sia conoscenze acquisite in anticipo che in tempo reale.</sample>
    <sample id="139">I relatori sono Yin e Zhi Yang.</sample>
    <sample id="140">Sì, i coscritti sono stati sottoposti a controlli di qualità.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti poiché supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto solitamente si basano su conoscenze dominanti e curazione umana.</sample>
    <sample id="142">Ciao, stiamo parlando di un progetto che si occupa di risolvere espressioni indirette per la selezione di entità. Introduciamo il concetto di "alt entities corpus". Il mio nome è Javahar Hosaini e questo è un progetto in collaborazione con Philip Radlinski, Sylvia Parati e Annie Lewis. Il nostro obiettivo è capire il linguaggio dei utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai mai sentito parlare di 'Easy on Me' o 'I Got a Feeling'? Qui, il utente vuole scegliere tra due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio, dire il nome della canzone "Easy on Me" o la sua posizione, la prima. Ma spesso un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può succedere quando il utente non ricorda il nome della canzone o le pronunciazioni sono troppo simili tra loro e difficili da distinguere. O quando il utente vuole specificare un preferimento. Ecco alcuni esempi di riferimenti indiretti: "La nuova una" o "La canzone che non è energetica". Questo è un problema importante nei sistemi di conversazione e anche per valutare l'entente dell'LM (linguaggio naturale) in benchmarking. Non conosciamo un pubblico dataset di grande scala per questa attività, quindi abbiamo creato il proprio utilizzando la crowdsourcing. Il nostro dataset copre tre domini diversi: musica, libri e ricette. Il nostro metodo di raccolta dei dati mette l'accento sulla casualità utilizzando un contesto di dialogo. In primo luogo, Bob dice: "Ricorda quella canzone che stavi ascoltando ieri?" e stabilisce il contesto del dialogo. In secondo luogo, Alice dice: "Vivi tu 'Easy on Me' o 'I Got a Feeling'?". Questa è la domanda alternativa. In terzo luogo, Bob usa un riferimento indiretto per scegliere tra queste due entità, ad esempio "La nuova una". Forse forniamo automaticamente i primi due dialoghi, ma il terzo è riempito dagli annotatori. Il primo dialogo è scelto da pochi prompt manuali per ogni dominio. Il secondo dialogo, che è la domanda alternativa, è generato come segue: utilizziamo un semplice modello: "Vivi tu A o B?" dove A e B sono estratti da Wikipedia. Utilizziamo diversi metodi di estrazione: quando si muove verso l'alto nella lista, le entità diventano più simili tra loro e diventa più difficile distinguere. Il primo è univendita random. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia e infine quando hanno attributi simili o attributi su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Mostramo queste domande alternative agli annotatori, che conoscono il nome delle entità ma non necessariamente le informazioni sulle entità. Così facendo, mostriamo some informazioni di sfondo sulla due entità. Per le canzoni, mostriamo un link di ricerca su Google e chiediamo ai annotatori di ascoltare almeno un pezzo di ciascuna e leggere le informazioni. Ad esempio, i risultati di ricerca di Google per "Easy on Me". Per i libri e le ricette, mostriamo testi di sfondo da Wikipedia. Per le ricette, mostriamo anche le immagini di Wikipedia per far vedere come si vedono. Poi chiediamo ai annotatori di scegliere una di queste entità e descriverla usando 3-5 espressioni indirette. Ad esempio, "La canzone con la pianoforte". Ecco alcuni esempi dal nostro dataset: "La canzone senza parole", non "La canzone con il bambino di 12 anni" o "La canzone con il bambino di 12 anni" o "La canzone con il bambino di 12 anni" o "La canzone con il bambino di 12 anni". L'alt entities corpus ha 6000 domande alternative attraverso i tre domini e 42000 espressioni indirette. I risultati con il modello T5-xl sono riassunti di seguito. Se il modello ha accesso alle stesse informazioni di sfondo degli annotatori, la precisione è molto alta, circa il 92% o il 95%. Ma questo non è realistico. Se il modello ha accesso a informazioni di sfondo parzialmente sovrapposte, la precisione è tra il 82% e il 87%, che è più realistico. Se il modello ha accesso solo ai nomi delle entità, la precisione è solo del 60%. Ci sono molte opportunità per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al dataset. Grazie per l'attenzione!</sample>
    <sample id="143">L'approccio SimulST viene confrontato con le politiche weight-k e local attention, inoltre con le architettura specificamente adattata per la traduzione contemporanea.</sample>
    <sample id="144">I ricercatori dell'articolo sono affiliati all'INR, all'INR, all'INR e all'INR.</sample>
    <sample id="145">La relatrice o il relatore è Jenny.</sample>
    <sample id="146">The speaker introduces a study on dialogue summarization, focusing on the issue of omission in generated summaries. The study analyzes the percentage of summaries suffering from omission and identifies the distribution of omitted information within dialogues. A new dataset is constructed to support the analysis and detection of omission, as there are no existing datasets for this purpose. Three baseline frameworks are explored to evaluate the performance of omission detection models. The results indicate that omission detection is a challenging task, but the inclusion of omitted content in the summary refinement process significantly improves the quality of the final summary.</sample>
    <sample id="147">Cinque.</sample>
    <sample id="148">Ciao, mi chiamo Sara Papi e sono dalla Università di Torino e Fondazione Bruno Kessler. Ecco un'introduzione alla Simultaneous Speech Translation (SST) come guida per un articolo che ho scritto con Matteo Negri e Marco Turky. Cos'è la traduzione contemporanea? La traduzione contemporanea o SST è il processo di tradurre un linguaggio parlato in testo in un altro linguaggio in tempo reale, rendendo la comunicazione tra lingue più facile. Ma cosa sono i problemi della nostra traduzione contemporanea? Le strutture architetturali sono spesso addestrate utilizzando moduli aggiuntivi per essere ottimizzati, addestramento e mantenimento di diversi modelli per raggiungere diverse regimi di latenza, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza, eccetera. Quindi, cosa è la nostra soluzione? Innanzitutto, utilizziamo modelli offline esistenti senza retrain o adottare strutture specifiche per la traduzione contemporanea. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici e la nostra soluzione è la proposta di ADT o encoder-decoder attention, che decide se emettere o non emettere una traduzione parziale basata su dove si concentra l'attenzione. Un segno viene emesso se l'attenzione non è concentrata, ovvero se la somma è inferiore a un certo livello di tensione verso le ultime lambda di frame di parola ricevuti, il che significa che la informazione ricevuta è abbastanza stabile. Se riceviamo un segno di parola che dice "I'm going to talk about" e il modello prevede la traduzione in tedesco, analizziamo i pesi di attenzione e vediamo che i primi due segni puntano ai primi due frame di parola ricevuti, mentre l'ultimo segno puntava alle ultime lambda di frame di parola ricevuti. Questo significa che i primi due segni verranno emessi, ma poiché la somma dei pesi di attenzione è sopra un certo livello di tensione, non emetteremo l'ultimo segno e aspetteremo un nuovo segno di parola. Se riceviamo un nuovo segno di parola e il modello prevede tre segni, analizziamo i pesi di attenzione e vediamo che nessun segno puntava alle ultime lambda di frame di parola ricevuti, quindi questi tre segni verranno emessi. Se guardiamo i risultati dell'ADT, plottiamo i risultati di traduzione contemporanea su grafici in cui il blu rappresenta la qualità della traduzione e l'average legging che misura la latenza, e consideriamo anche il tempo computazionale medio per predecere l'output. Ci vogliamo assicurare che i curvi siano alti quanto possibile sul grafico, ma anche spostati verso sinistra, e confrontiamo i nostri risultati con strategie alternative basate su modelli offline, come la strategia Whitaker e la strategia locale di adattamento, e con strutture architetturali specificamente dedicate alla traduzione contemporanea. Questi sono i risultati della nostra strategia di traduzione contemporanea in tedesco. Vediamo che ADT supera tutte le strategie applicate a modelli offline, poiché i curvi sono spostati verso sinistra, e anche che se consideriamo il tempo di accesso reale o il tempo computazionale medio, ADT è la strategia più veloce. Se vuoi scoprire di più, leggi il nostro articolo e scarica anche il codice e i modelli per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione!</sample>
    <sample id="149">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="150">The presentation introduces the Meeting Q&amp;A dataset, an extractive question answering dataset based on questions asked by participants in meetings and their corresponding answer sentences. The dataset contains 7.7 thousand questions, with 30% unanswerable, 40% multi-span answers, and 48% multi-speaker answers. Questions are longer, open-ended, and elicit detailed responses from others. The presentation also discusses the data collection process, including public meeting transcripts, question selection, and answer annotation. The presentation concludes with a brief overview of the results in the fine-tuned setting, including performance gaps between models and human performance, and the effectiveness of silver data augmentation.</sample>
    <sample id="151">Il mio nome è Ee e il mio collega Jia Yang e io presenteremo il nostro ricerche su Multi Instruct, migliorando il modello di imparare dalla rete in modo rapido e efficiente tramite l'addestramento con istruzioni. Con i progressi nella creazione di modelli di lingua grande, molte ricerche hanno iniziato a esplorare nuovi paradigmi di imparare utilizzando modelli pre-addestrati per diverse attività in un modo più efficiente. Recente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli di lingua grandi di svolgersi in attività diverse in modo rapido e efficiente seguendo istruzioni naturali. Tuttavia, la maggior parte delle precedenti ricerche sull'addestramento con istruzioni si concentra su migliorare le prestazioni in attività sole linguistiche, mentre le attività di visione computer e multimediali sono state lasciate fuori. Quindi, in questo lavoro, ci piacerebbe indagare se l'addestramento con istruzioni su modelli pre-addestrati multi-modal può davvero migliorare la generalizzabilità verso attività multimediali. Inoltre, alla nostra ricercha, abbiamo scoperto una considerabile disparità nella disponibilità di dataset di istruzioni tra NLP e multimediali. Esistono più di 1600 task linguistiche sole, tuttavia non esiste alcun dataset di istruzioni multimediali pubblicamente disponibile. Questo ha motivato noi a creare un dataset di istruzioni multimediali. Qui presentiamo Multi Instruct, il primo dataset di istruzioni multimediali di benchmark che consiste di 62 diverse attività multimediali coprendo 10 category diverse. Queste attività sono derivate da 21 dataset di fonte aperta esistenti e ogni attività è equipaggiata con cinque istruzioni elaborate. Per indagare l'addestramento con istruzioni multimediali sul nostro dataset proposto, utilizziamo OFA come modello base. OFA utilizza un univoco per le token di lingua, immagine e coordinate del bounding box. Qui mostriamo alcune istanze esempiarie dal nostro dataset Multi Instruct. Per unificare il trattamento di vari tipi di input e output diversi, seguiamo il modello di OFA e rappresentiamo tutti i task in un formato di sequenza a sequenza univoco in cui le istruzioni, le immagini, le istruzioni e i bounding box sono rappresentati nello spazio delle token. Ora parliamo di addestramento con istruzioni multimediali. Per il dataset di addestramento, utilizziamo 53 task da NLP per addestrare e scegliamo 10.000 istanze per task. Per testare, riserviamo tutto il gruppo di ragione comune per testare e scegliamo ulteriormente 5 task da Wiki e MScenius. Utilizziamo tutte le istanze del test set per ciascun task. Inoltre, randomizziamo 20 task dal test set di NLP come task di NLP. Utilizziamo il modello pre-addestrato OFA come modello base. Durante l'addestramento, combiniamo casualmente un istanza per task con uno dei cinque template di istruzioni. Durante il test per ciascun task, eseguiamo un totale di 5 esperimenti evaluates il modello utilizzando una delle cinque istruzioni in ciascun esperimento. Rappresentiamo il minimo e massimo prestazione e la deviazione standard della prestazione attraverso di 5 esperimenti. Se il task è un task di classificazione multimediali, rappresentiamo l'accuratezza. Se è un task di generazione multimediali, rappresentiamo ROGUE-L. Per NLP, rappresentiamo ROGUE-L allo stesso modo. Abbiamo anche introdotto un nuovo metrico chiamato sensibilità. Questo misura la capacità del modello di produrre gli stessi output per lo stesso task, indipendentemente dalla variazione nella formulazione dell'istruzione. Questi sono i nostri principali risultati. Come possiamo vedere, l'addestramento con istruzioni può migliorare significativamente le prestazioni di OFA su task multimediali. Inoltre, l'apprendimento trascrizio da dataset di istruzioni può migliorare l'addestramento. Come possiamo vedere, con l'aumento del numero di task, il modello raggiunge prestazioni migliori e allo stesso tempo una sensibilità più bassa. Abbiamo anche fatto un esperimento utilizzando un istruzione invece di cinque istruzioni. Come possiamo vedere, utilizzando una istruzione può migliorare l'overall performance del modello e ridurre la sensibilità molto. Questo dimostra l'effetto di diverse strategie di finetuning sul modello sensibilità. Come possiamo vedere, utilizzando l'apprendimento trascrizio da dataset di istruzioni, il modello può raggiungere prestazioni migliori sensibilità rispetto al modello OFA originale. Abbiamo anche potuto vedere che l'apprendimento trascrizio da dataset di istruzioni può aiutare OFA a raggiungere prestazioni migliori sul dataset NLP. In generale, proponiamo il primo dataset di istruzioni multimediali di scala larga che ha significativamente migliorato la capacità di addestramento rapido di OFA e ha esplorato diverse tecniche di apprendimento trascrizio e ha dimostrato i benefici. Abbiamo progettato un nuovo metrico chiamato sensibilità. Altra cosa, stiamo raccolgendo un dataset multimediali di addestramento più grande con circa 150 task linguistiche in più e lo rilanceremo presto. Questo è il link per il codice del dataset e del modello. Grazie mille.</sample>
    <sample id="152">The presentation introduces valuable resources for ancient Greek and Latin, exploring the implications and challenges of multilinguality in large language models. The creators have developed two monolingual models for ancient Greek (Graeberta and Greta) and two multilingual models (Philberta and Philter) pre-trained on ancient Greek, Latin, and English data. They have also created a new pre-training corpus from the Internet Archive using incorrectly transcribed Greek stop words. The models outperform the current state of the art for both ancient Greek and Latin, with significant gains in lemmatization performance. The results show that multilingual models do not significantly outperform monolingual models in semantic and world knowledge capabilities.</sample>
    <sample id="153">In this talk, we will present our work on resolving ambiguities in text-to-image generative models. We are interested in studying existing ambiguities in prompts provided to text-to-image models and proposing frameworks to mitigate such ambiguities as well as frameworks to evaluate whether the generated images are faithful to users' intention. Our framework first curates a benchmark dataset that covers different types of ambiguities. Then, these prompts are provided to prompt disambiguation framework that tries to gather external signal to disambiguate the prompt through either asking clarifying questions from the user or generating different possible visual setups. Once we have the prompts disambiguated, we are going to evaluate them. We are also going to input these disambiguated prompts to text-to-image model, generate the images, and evaluate whether the generated images are faithful to users' intention. To do that, we are going to propose our automatic evaluation framework. We show that there is disparity in resolving ambiguities for different ambiguity types. We show that disambiguation using our framework has overall a positive effect in faithful generation. And we show that our automatic evaluation framework is in agreement with human evaluation so it can be used reliably to evaluate text-to-image models.</sample>
    <sample id="154">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Torino e Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice o del relatore è Javahar Hosaini.</sample>
    <sample id="157">The speaker introduces a dialogue summarization method using a static-dynamic structure fusion graph. This method aims to extract hidden information from dialogue context into concise summaries, helping people quickly capture the highlights of semi-structured multi-participant dialogues without reviewing the complex context. The model consists of four main components: an utterance encoder, a static graph construction module, a dynamic graph module, and a pre-trained language model as the summary generator. The static graph is built using discourse passing and speaker relationship modeling, while the dynamic graph captures semantic relationships based on deep vector representations. The fusion of these graphs generates the final summary.</sample>
    <sample id="158">The speaker introduces a work called "Dual Cache for Long Document Neural Coreference Resolution" and explains the task of coreference resolution, which involves identifying and clustering messages that refer to the same entity in a document. The proposed Dual Cache method uses a local cache and a global cache to store local and global entities respectively, with eviction policies to manage cache size. The model outperforms single cache methods and is more cost-effective, especially for book-level documents.</sample>
    <sample id="159">Ciao a tutti, mi chiamo Kostya Tszyu e sono felice di benvenuti in questo talk sul mio articolo AC 2023, intitolato "I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto". Questo è un lavoro con John Goldacre, Aaron Mueller, Kanishka Mishra, Garen Fintz, Roger Levy e Adina Villemoes. In questo lavoro, rivisitiamo il paradigma del minimal pair. Il paradigma del minimal pair valuta i modelli linguistici su base di giudizi di accettabilità, che possono anche includere grammaticalità come plomp, syntax jim o accettabilità in termini di stereotipi come crowd spares. In questo paradigma del minimal pair, la tipica maniera di valutare i modelli linguistici è di mostri un sentenzia accettabile o grammaticalmente corretto e un sentenzia non accettabile o grammaticalmente sbagliato. E allora, l'obiettivo del modello è di dare più probabilità alla sentenzia accettabile. La nostra pipeline MPP attualmente non ci permette di valutare la accettabilità dei modelli verso le sentenzie più lunghe. Questi giorni, i modelli linguistici stanno uscendo con lunghe finestre di contesto. Quindi, è cruciale che possiamo valutare la accettabilità dei modelli attraverso tutto il contesto. E quello è esattamente quello che stiamo cercando di fare qui. Stiamo cercando di rivisitare la pipeline MPP per chiedere al modello di valutare la accettabilità su lunghe sequenze più lunghe. Quindi, è proprio quello che facciamo. Così, cosa facciamo è di simulare queste lunghe sequenze, rivisitiamo i dataset stessi e quindi ricreati i sentenzie scelte tra sentenzie accettabili o non accettabili. Ad esempio, qui abbiamo scelto un tipico par di grammaticalità dal dataset BLM dalla Isola di Saint John e quello che facciamo è di ricreare queste lunghe sequenze accettabili e non accettabili. E se prendiamo una sentenzia accettabile e un sentenzia non accettabile, estraeiamo le sentenzie grammaticalmente corrette da Saint John e le aggiungiamo come prefisso sia alla sentenzia accettabile che alla non accettabile. Quindi possiamo fare lo stesso per le sentenzie non accettabili, scegliendo sentenzie dallo stesso matching e potremmo anche usare queste per testare la accettabilità del modello. Possiamo anche fare lo stesso per scelere sentenzie da un insieme diverso o da un dataset diverso. Quindi, quello che chiamiamo è il scenario mismatch. Qui, le sentenzie stanno venendo dal dataset rilevante ma non dallo stesso dataset che stiamo valutando. E possiamo fare lo stesso per i giudizi di non accettabilità. Infine, possiamo scegliere sentenzie da un dominio completamente non rilevante come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono davvero influenzati dal contesto, se il contesto proviene da un subset diverso del dataset o se è completamente irrelevante per la sentenzia che stiamo analizzando. Quindi, come fa il modello? Innanzitutto guardiamo le sentenzie di Wikipedia che sono completamente irrelevanti per il par di query attuale. E qui notiamo che i giudizi di accettabilità MPP sono relativamente stabili. Abbiamo aumentato la lunghezza del contesto fino a 1244 per massimizzare i modelli OTP e GP2 e vediamo qui in orange che i giudizi di accettabilità MPP sono relativamente stabili. Ora, cosa succede quando scegliamo sentenzie dallo stesso dataset? Qui, stiamo creando sentenzie da domini accettabili e non accettabili dallo stesso dataset BLM o syntax jim e vediamo che i giudizi di accettabilità MPP ognuno cresce o diminuisce notevolmente quando aggiungiamo prefissi accettabili o non accettabili. Ma quando scegliamo la struttura, cioè quando scegliamo sentenzie dallo stesso fenomeno in BLM o syntax jim, vediamo un massivo incremento o un massivo decremento dei giudizi di accettabilità MPP per il modello, a seconda di quanto il prefisso scelto sia accettabile o non accettabile. Ora, e questa è una cosa molto grande, come questa effetto aumenta lungo la finestra di contesto e queste probabilmente affetteranno modelli più recenti che hanno una finestra di contesto più grande. Quindi, perché il prefisso di match influenza così tanto i giudizi di accettabilità dei modelli? Abbiamo fatto una serie di analisi in cui cercavamo di alterare l'input sentenzia cercando di preservare la struttura relevante ma aggiungendo rumore all'input e dopo aver fatto queste variazioni, notiamo che nessuno di questi rumori sta realmente facendo il modello cambiare i corso in termini di come mostri i giudizi di accettabilità MPP. Quindi, notiamo che i modelli sono sensibili ai disturbi nella sentenzia in modo simile, cioè quando perturbiamo le sentenzie in un dominio accettabile, vediamo un incremento simile in tutte le variazioni e quando perturbiamo le sentenzie in un dominio non accettabile, vediamo un decresemento in MPP giudizi in modo simile. Quindi, i principali punti di sosta del nostro lavoro è che i modelli linguistici sono sensibili ai latenti sintattici e semantici caratteristiche che si condividono tra le sentenzie e che la nostra valutazione MPP attualmente, che utilizza input di singoli sentenzie brevi, non può affatto catturare completamente la conoscenza astratta dei modelli attraverso la finestra di contesto. Per favore leggete il mio articolo per maggior dettaglio dei nostri esperimenti. Grazie per l'attenzione.</sample>
    <sample id="160">In un token multi-settato</sample>
    <sample id="161">In Coscript, rappresentano 55,000 script.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo di mass alignment.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che i modelli possono essere addestrati su un volume di dati molto grande, anche se non è necessario etichettare manualmente ogni singolo esempio. Questo può ridurre i costi di etichettatura e accelerare il processo di addestramento dei modelli.</sample>
    <sample id="165">The speaker introduces an unsupervised learning method called LIPOR, which stands for Likelihood Learning with Posterior Regularization. This method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. To prefer plausible explanations, a regularizer is introduced to enforce mutual exclusivity among explanations. The LIPOR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over others. The speaker compares their results on AlphaNLI, the most widely used adaptive reasoning dataset, to zero-shot models and the previous best unsupervised approach. They outperform all of them, including a strong zero-shot GPT-3 baseline, by over 4 absolute points in accuracy.</sample>
    <sample id="166">The proposed method utilizes the advantages of both analogical reasoning and symbolic reasoning systems to address the challenge of image-text reasoning tasks. The first module, a complex proposition generator, represents complex propositions using symbols and generates corresponding sentences. The second module, a neural symbolic reasoner, integrates reasoning states and results from symbol propositions to obtain the final solution. The system combines the inference results of both modules to achieve superior performance compared to baseline models and verified experiments on the testing set.</sample>
    <sample id="167">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici. Per i documenti in DEplain-AP, 483 documenti sono stati allineati manualmente, mentre per i documenti in DEplain-Web, 750 documenti sono stati allineati manualmente e automaticamente.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo articoli da Reuters News del 2020 e annotandoli con le stesse linee di注解 guidelines utilizzate per CoNLL 2003.</sample>
    <sample id="169">The speaker, Ibil Lillard, presents a short overview of a paper on prompting BART for translation. The paper evaluates the translation capabilities of large language models using best practices from the NMT community and compares two state-of-the-art systems. The results show that example quality is more important than similarity to the source sentence, and that specialized data has a substantial advantage over BART translations. The paper also discusses the fluency and accuracy of BART's output compared to state-of-the-art systems.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Usen Zhang e sono dalla Università di Peking. Oggi presenterò il mio lavoro: "Esempio: Analisi semantica per la traduzione di query in diverse lingue naturali e rappresentazioni". La traduzione semantica è un compito che consiste nel costruire rappresentazioni semantiche di query dell'utente, come SQL e Lambda Calculus. La traduzione semantica multilingue è il compito di tradurre query in diverse lingue naturali in rappresentazioni semantiche multiple. Come illustrato nella nostra figura, dobbiamo tradurre una query in diverse lingue naturali utilizzando modelli neurali in SQL, Lambda Calculus e altri. I modelli di traduzione semantica multilingue esistenti sono proposti separatamente e valutati su dataset di limitate tassonomie e applicazioni. Ci sono lacune di copertura su certe lingue naturali, come il cinese, e su certe rappresentazioni, come il calcolo Lambda. Esistono anche modelli di traduzione solo per certi modelli neurali. Per risolvere queste lacune, presento Esempio: un dataset unificato per la traduzione semantica multilingue in diverse lingue naturali e rappresentazioni. Questo dataset comprende 9 dataset in diverse domande, 5 query di traduzione semantica, 8 rappresentazioni semantiche e 22 lingue in 15 famiglie linguistiche. Per valutare meglio il benchmark, consideriamo 6 scenari per addestramento e valutazione. Il primo è il test di traduzione: utilizziamo l'API di Google Translate per tradurre la query di origine verso la lingua di destinazione, quindi utilizziamo un modello monolingue per addestrare e valutare. Ad esempio, addestriamo un modello in inglese su query in inglese e durante l'inferenza traduciamo la query in tedesco utilizzando l'API in inglese e utilizziamo il modello addestrato per prevedere il calcolo Lambda. Abbiamo anche testato il modello monolingue in questo scenario: il linguaggio di origine è lo stesso del linguaggio di destinazione, ad esempio tedesco in tedesco o inglese in inglese. Abbiamo anche testato il modello monolingue fuso: addestra modelli monolingui con solo il 10% dei dati di addestramento e testa modelli monolingue multi-lingue che addestra un modello multi-lingue per tutte le lingue. Ad esempio, uniamo query in tedesco, inglese e cinese per addestrare un modello multi-lingue e durante l'inferenza possiamo utilizzarlo per tradurre query in tedesco o cinese, eccetera. Abbiamo anche considerato la traduzione zero-shot e fusa: addestra su un linguaggio di origine e trasferisci su un altro linguaggio. Durante l'addestramento, addestra su query in inglese o la combinazione di query in inglese e tedesco fusa per addestrare un modello multi-lingue per prevedere il calcolo output. Abbiamo anche scoperto molti risultati interessanti. In riferimento all'analisi dei modelli monolingue, valutiamo due gruppi di modelli: encoder PDR, che sta per encoder pre-addestrato monolingue con decoder basato su pointer, come XLNet + PDR e BERT + PDR; e encoder-decoder modelli, che sono modelli encoder-decoder pre-addestrati monolingue, come BART e MT5. Abbiamo scoperto che i modelli encoder-decoder ottengono le migliori prestazioni su tutti i 9 dataset. Abbiamo anche valutato MT5 e XLM-R + PDR su un setting monolingue: encoder-decoder o encoder PDR possono essere migliorati addestrando in una combinazione di diverse lingue. Abbiamo scoperto che quasi tutte le lingue maggiori possono ottenere un miglioramento delle prestazioni, tranne l'inglese che si riduce in 7 dataset e guadagna solo in 3 dataset. Questo è noto come curva della multilingua. Abbiamo anche confrontato la differenza di prestazione tra traduzioni multilingue e traduzioni zero-shot: il grafico blu rappresenta la traduzione zero-shot, il grafico rosso rappresenta la traduzione fusa zero-shot, mentre il grafico verde rappresenta il setting monolingue. Abbiamo scoperto che confrontando il grafico verde e il grafico rosso, nel setting zero-shot la differenza di prestazione tra traduzioni multilingue e traduzioni zero-shot è significativa. Abbiamo anche scoperto che confrontando il grafico blu e il grafico rosso, nel setting fusa zero-shot la differenza di prestazione diminuisce rapidamente. Abbiamo anche scoperto altri interessanti findings: ad esempio, encoder-decoder supera precedenti work o raggiunge risultati comparabili addestrando su lingua naturale inglese e migliorando notevolmente le prestazioni di fusa zero-shot su lingue naturali di destinazione. Abbiamo scoperto che modelli monolingue come Codas e Blue sono ancora inadeguati per task di traduzione semantica multilingue. In sintesi, presento Esempio: un benchmark unificato per la traduzione semantica multilingue in diverse lingue naturali e rappresentazioni. Condurrò un studio di benchmarking esaustivo su tre tipi rappresentativi di modelli monolingue e lingue naturali, e i nostri risultati hanno dimostrato molti findings interessanti. Spero che vi sia piacere a visitare il mio articolo e il codice. Grazie per l'attenzione!</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro category.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP.</sample>
    <sample id="174">The Arg Analysis 35K dataset is a large-scale dataset for argument quality analysis, which is unique from other datasets in the field due to its high-quality arguments, diverse range of arguments, and the inclusion of analysis. The dataset contains 35,000 argument analysis pairs, with around 85% sourced from speeches from high-quality tournaments or expert debaters, and the remaining 15% from novice debaters. The dataset also includes a diverse range of arguments on 24 themes, which are captured using various sources such as websites and expert advice. Additionally, the dataset includes an element of analysis instead of just keeping arguments, which can be a combination of claims, premises, and analysis. The dataset also includes an instance-based annotator reliability, which captures the biases of annotators on specific topics, and a relevance model that assigns a score from 0 to 1 for each argument based on its relevance to a particular theme. Overall, the Arg Analysis 35K dataset provides a more diverse and reliable scoring system for argument quality analysis compared to other datasets in the field.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allineamento come parte del training.</sample>
    <sample id="176">La giustizia di un modello NLP a basso livello è definita come la capacità del modello di rilevare e gestire le informazioni che vengono fornite, senza discriminare in base alla politica o alla razza.</sample>
    <sample id="177">The name of the presenter is Yanis Slavac.</sample>
    <sample id="178">La relatrice o il relatore è Kostya Sina.</sample>
    <sample id="179">The speaker is discussing the ability of language models to reason about the mental states of others, known as theory of mind. They explain how this can be measured through reading comprehension tasks involving multiple characters and false belief questions. They then introduce a method called Symbolic Tom, which uses explicit graphical representations to improve theory of mind reasoning skills in large language models. The speaker presents experiments that show Symbolic Tom outperforms supervised approaches on out-of-domain story understanding and remains beneficial on new linguistic diversity datasets.</sample>
    <sample id="180">La relatrice o il relatore è Myra.</sample>
    <sample id="181">The paper introduces the problem of constrained language planning, which imposes different constraints on goal planning. The authors evaluate and improve the constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints for humans in the loop data acquisition using InstructGPT. They sample 100 specific goals and evaluate the scripts generated from large language models. The results show that all large language models achieve unsatisfactory results on planning for specific goals. The authors conduct a detailed analysis to investigate why large language models fail to generate faithful scripts to constraints. They develop an over-generated script filter to improve generation quality and create a dataset of constrained language planning named as CoScript. The results show that CoScript has high productivity in generating specific goals with scripts. The authors hope that CoScript can be a valuable resource to advance research on language planning.</sample>
    <sample id="182">Il tropicalismo si riferisce a un stereotipo che rappresenta le donne latine come vivaci e curvose, mentre le donne asiatiche vengono descritte come delicate e sussidi. Questi stereotipi hanno un lungo storia di rappresentazione sessista e hanno portato alla rappresentazione sessista di queste gruppi.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando la proprietà che i nuovi modelli di lingua possono rispondere a istruzioni e promp.</sample>
    <sample id="184">SXMI</sample>
    <sample id="185">DrBERT è un modello di apprendimento automatico per la traduzione naturale in francese, mentre ChuBERT è un modello di apprendimento automatico per la traduzione naturale in francese basato su un dataset di medicina.</sample>
    <sample id="187">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un processo in cui si utilizzano i modelli precedentemente addestrati per migliorare la capacità di riconoscere le relazioni di discordia.</sample>
    <sample id="189">L'obiettivo del set di dati è di comprendere il linguaggio dell'utente quando vuole fare una scelta.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando un insieme di frasi in cui tutti i termini appartenenti all'insieme di trigger.</sample>
    <sample id="191">Cinque.</sample>
    <sample id="192">The speaker introduces a new optimizer called KAM, which aims to achieve both fast convergence and low memory usage. They discuss the challenges of existing memory-efficient optimizers like AdaFactor, which can be slow due to errors in non-negative matrix factorization (NMF) operations. The speaker proposes an adaptive confidence-based updating strategy that uses the residual between predicted and generated updates to improve stability and efficiency. Experiments on large language models show that KAM achieves better performance than Adam and AdaFactor with significantly lower memory usage.</sample>
    <sample id="193">Per creare il set di dati iniziale, sono stati impiegati circa 1000 esempi di unità discorsette.</sample>
    <sample id="194">I ricercatori dell'articolo sono studenti di primo anno di laurea in Scienze della salute alla Carnegie Mellon University e collaboratori dell'University of Washington e dell'Alliance for AI.</sample>
    <sample id="195">The speaker introduces a new framework called ROHT, which stands for Reasoning over Hierarchical Question Decomposition Tree. This framework aims to improve the process of answering complex questions by breaking them down into sub-questions and using knowledge sources like Wikipedia and text corpora to find answers. The framework is tested on two datasets, KQAPRO and Music, and shows significant improvements in performance compared to previous methods.</sample>
    <sample id="196">Il primo esempio in cui il governatore è a sinistra è "Lisa bought and Meggy".</sample>
    <sample id="197">I modelli all'avanguardia dei sistemi di dialogo sono quelli che hanno una capacità di comprendere e rispondere in modo più umano, con un'attenzione maggiore alla coerenza e alla comprensione del contesto.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria per capire come i modelli espongono i propri giudizi di accettabilità in relazione alla lunghezza delle sequenze.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset.</sample>
    <sample id="200">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="201">Per la valutazione sono state utilizzate metriche di MT standard e anche risultati umani per la valutazione.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i modelli e i set di dati possono rappresentare certe posizionalità in modo scorso, rendendo più difficile per gli utenti non rappresentati ad accedere ai servizi.</sample>
    <sample id="204">LLM multilingue come BLOOM sono stati affinati utilizzando una messa a punto integrale.</sample>
    <sample id="205">The speaker, Changbing, is a PhD student at the University of Washington presenting their work on political biases in language models. They explain that language models are trained on web crawls and political news media, which can lead to social biases in NLP applications. To address this, they propose evaluating political leanings of language models and investigating how different political leanings affect performance on downstream tasks like hate speech and fake news detection. They found that language models have varying political leanings and can pick up polarization from training data. They also discovered that left-leaning models are better at detecting hate speech targeting socially minoritized groups, while right-leaning models are better at detecting hate speech targeting more powerful groups. This highlights the need to acknowledge and tackle fairness issues resulting from language model political leanings.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, fanno ricorso al modello di classificazione di dissenso stante.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i test più recenti, che vengono utilizzati per evitare un混杂效应，即测试数据与训练数据之间的混杂。</sample>
    <sample id="208">I ricercatori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il nuovo metodo proposto ha un guadagno di 100% rispetto al metodo di riferimento.</sample>
    <sample id="210">Il nome della relatrice o del relatore è Zhu Heng.</sample>
    <sample id="211">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nell'articolo, viene utilizzata una singola modifica più piccola per generare i modelli di script.</sample>
    <sample id="213">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è OFA.</sample>
    <sample id="215">The speaker introduces the topic of dependency structures in coordination, discussing various theories and approaches. They explain that asymmetric structures, where one conjunct is highlighted, are contrasted with symmetric structures, which head the coordinate structure with a conjunction. The paper aims to argue for symmetric structures against asymmetric ones using the principle of dependency length minimization. The speaker provides examples from English grammar, showing how direct objects and adjuncts affect sentence structure. They also discuss statistics from the Penn Treebank, confirming that left conjuncts tend to be shorter and that this tendency increases with the length difference between conjuncts. The paper argues that this tendency supports symmetric structures over asymmetric ones.</sample>
    <sample id="217">The speaker introduces a new method for generating multi-tributal controllable dialogues, called DC G. This method uses a disentangled controllable generation that learns attribute concepts from single values and uses the disentangle loss to disentangle different attribute combinations. The speaker also proposes a unified reference-free evaluation framework (MAE) for different granularities of attributes and establishes two benchmarks to prove the effectiveness of their method and evaluation metrics through experiments. The model is based on the dialogue GBT framework with the compositional prompt module, which uses two types of prompts: attribute-oriented prompts and task-oriented prompts. The speaker concludes by demonstrating the impact of prompts on compositional generation with a visualization of the concatenated prompt embeddings of two attributes.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">The speaker introduces a research assistant at Academia Sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. They discuss the background of financial report analysis, text definition, and approaches, focusing on the Form 10-K as the target corpus. The work is motivated by observations of similarity in company reports, leading to the introduction of a highlighting task and a multi-stage pipeline. The pipeline includes document segmentation, relation classification, and fine-tuning stages. The model predicts word importance to measure performance, with examples given for different types of pairs (same, revised, mismatch). The training uses external datasets and fine-tunes the model using revised pairs and other techniques. The final dataset shows that the proposed model achieves the best performance, even preserving generalization capability. Future works include improving effectiveness, adding more features, and enhancing the application with techniques from information retrieval.</sample>
    <sample id="220">I ricercatori dell'articolo sono affiliati all'Università Stony Brook.</sample>
    <sample id="221">In quest'articolo, si è analizzata la capacità di traduzione di modelli di lingua grande utilizzando due coppie linguistiche: Germano-inglese e inglese-germano.</sample>
    <sample id="222">The paper presents a method to adapt or annotate challenges and interventions in open-domain question answering. The authors investigate different data interventions that enable out-of-domain generalization, identify the type of dataset shift a new domain exhibits, and determine what kind of data interventions are effective for a specific type of shift. They propose two overarching methods for generating these interventions: few-shot and zero-shot. In the few-shot method, they use few examples from the target domain to prompt large language models for generating more examples, while in the zero-shot method, they control the interactions among three random variables in open-domain QA. They also propose a compatibility measure to assess the nature of incompatibility between the target model and domain exhibits. The results show that the proposed method can improve reader performance by up to 24% and that only certain types of data interventions are effective based on the type of shift a target dataset exhibits.</sample>
    <sample id="223">Il nome della relatrice o del relatore è Changbin.</sample>
    <sample id="224">Durante gli esperimenti, i modelli studiati sono il modello di Long Impart per la produzione di semplificazioni di documenti e il modello normal base Long Impart per la produzione di semplificazioni di frasi.</sample>
    <sample id="225">53 attività vengono utilizzate per scopi di addestramento e 10 attività vengono utilizzate per scopi di test.</sample>
    <sample id="226">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="227">The speaker discusses the current state of language models and their limitations in grounding natural language understanding. They propose a new framework that focuses on discrimination instead of generation, using a symbolic agent to interact with the environment and propose candidate plans, while the language model is used only to score and rank these candidates. The speaker argues that this approach is more effective and efficient than traditional methods, and provides results from experiments with different language models and settings.</sample>
    <sample id="228">I test sono stati effettuati su quattro set di dati: AGNews, Mind, SST2 e Airbnb.</sample>
    <sample id="229">The speaker, Gabriella Katalin Szekelya, introduces a joint work with Henning Bach-smud on detecting improvable claims for argumentative writing support. Text revision is an essential part of professional writing and is typically a recursive process until optimal phrasing is achieved from the author's point of view. The paper focuses on answering the question of whether an argumentative claim is phrased well enough and no more revisions are needed. Two new tasks are introduced: sub-optimal claim detection and claim improvement suggestion. The speaker explores the challenges that arise when working with such revision-based data, including representativeness and reliability, model complexity and architecture, contextual information, and topical and user bias. The speaker concludes that revision-based data can be effectively employed for the given tasks and that modeling the distance between two claim versions is beneficial for detecting sub-optimal claims.</sample>
    <sample id="231">NACHOS è un dataset di medical crawled data usato per la creazione di modelli di apprendimento automatico.</sample>
    <sample id="232">Il nome della relatrice o del relatore è Avi Lillard.</sample>
    <sample id="233">Il contenuto dell'audio è un'introduzione a Simultaneous Speech Translation (SST) fornita da Sara Papi e Bruno Cassele. L'argomento principale della presentazione è la traduzione in tempo reale di un linguaggio parlato in un testo in un altro linguaggio, rendendo la comunicazione tra lingue diverse più facile. La presentazione introduce anche i problemi attuali dei modelli di SST, tra cui l'uso di architettura specifica, processi di addestramento lunghi e complessi, e la necessità di addestrare e mantenere diversi modelli per raggiungere differenti regimi di latenza. La soluzione proposta è utilizzare modelli offline esistenti senza retrain o adottare specifiche architettura per SST, utilizzando solo un modello per ogni regime di latenza e gestendo la latenza tramite parametri specifici. La presentazione conclude con un esempio di come il modello funziona e con grafici che illustrano i risultati delle strategie di SST proposte.</sample>
    <sample id="234">La strategia del prompting ha un grosso impatto sui risultati, come si può vedere in un esperimento dove il differenziale di servizio è di più di un punto percentuale per la maggior parte delle frasi.</sample>
    <sample id="235">I ricercatori che hanno scritto l'articolo sono affiliati all'University of Edinburgh.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti sono equipaggiate con ciascun compito.</sample>
    <sample id="237">Gli autori propongono una suite di test diagnostiche per l'integrazione delle conoscenze.</sample>
    <sample id="238">The video presents a new benchmark dataset called "MeetingBank" created by Yebuwen Hu from the University of Central Florida. The dataset includes 1,366 city council meetings with nearly 7,000 instances, featuring meeting transcripts, reference summaries, and URLs containing useful resources. The dataset was created to address two major challenges: the scarcity of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. The data collection process involves using speech-to-text APIs to convert audio data into transcripts, identifying meeting types and data from meeting websites, and aligning time stamps to get segment transcripts paired with extracted summaries. The dataset provides statistics on the number of meetings, meeting duration, tokens per meeting, speakers per meeting, and the year period of meetings collected. It also includes summary instances gathered for each city and average statistics for both meeting level and segment level. The video discusses the evaluation of summarization systems using metrics like coverage and density, and compares different summarization models such as extractive and abstractive summarizers. The results show that GPT-3 performs exceptionally well in terms of fluency and coherence but less impressively in informativeness and factuality. The video concludes by highlighting the importance of developing new methods of automatic evaluation metrics to better align with human preferences and encourages viewers to use and explore the MeetingBank dataset further.</sample>
    <sample id="239">Ecco un riassunto in italiano del testo:

Il mio nome è Ibilad e io presenterò un breve riassunto del mio articolo sulla traduzione di BART da lingua inglese a lingua tedesca, analizzando le strategie e le prestazioni. Questo è un lavoro di collaborazione con i miei colleghi di Google Translate. BART è un modello linguistico a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testi che comprende 780 miliardi di token. In testi di traduzione, ha raggiunto lo stato dell'arte in centinaia di task di NLP. In questo articolo, presentiamo il primo studio sistematico sulla traduzione di BART. Abbiamo valutato la capacità di traduzione dei modelli utilizzando le migliori pratiche della comunità LM, utilizzando le ultime tabelle di test per evitare l'uso dei dati di addestramento con i dati di test. Abbiamo anche confrontato due sistemi di traduzione dello stato dell'arte. Utilizziamo metri di valutazione di NLP e inoltre mostriamo i risultati di valutazione umana con esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione del prompt. Il prompt ha un grande impatto sulle prestazioni dei modelli di traduzione. Come possiamo vedere in un esperimento semplice, quando utilizziamo un prompt unico per due diverse traduzioni, circa il 516 su 1000 test ha differenze superiori a 1 punteggio e in casi estremi fino a 40 punteggi. Ciò dimostra quanto sia importante scegliere un buon策略。 In nostre esperienze, preferimmo una strategia di prompting a 5 passi, dove marciamo ogni frase che forniamo al sistema con il linguaggio di destinazione. Abbiamo notato che la qualità dell'esempio ha un impatto più importante sulla somiglianza rispetto alla somiglianza con la frase di origine. Ciò significa che è importante scegliere esempi di traduzioni di alta qualità. Abbiamo anche confrontato la selezione dei prompt con i dati di addestramento e i dati di dev. I dati di dev sono più accurati e con una qualità più alta rispetto ai dati di addestramento, portando a performance migliori. Tuttavia, i sistemi di traduzione dello stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni di BART. In ogni caso, BART si avvicina molto a un sistema di traduzione commerciale. Le informazioni che abbiamo ottenuto dalla nostra valutazione utilizzando il framework QM sono che la fluidezza di BART è simile a quella dei sistemi dello stato dell'arte, ma la differenza principale deriva dalla precisione. In particolare, gli errori più comuni sono gli errori di omissione. Ciò sembra che BART sceglie di produrre traduzioni migliori spesso rimuovendo parti della frase di origine che non sono rilevanti nella traduzione. Tuttavia, la qualità di traduzione di BART è inferiore rispetto a quella dei sistemi dello stato dell'arte, specialmente nella category di traduzioni stilistiche. Ecco tutto per questa rapida panoramica. Per maggiori dettagli, ti prego di vedere la presentazione completa dell'articolo.</sample>
    <sample id="240">Ciao, mi chiamo Dawe e sono uno studente di laurea in Informatica presso l'Università di Salzburg in Germania. In questo video vorrei presentare recenti sviluppi nel campo del supervised learning, o meglio, un'analisi critica dei modelli di supervised learning. Questo è un lavoro di gruppo con Qiu Yushan, Maio Smoothbath, Giacomo Stefan e Dietrich Klakow. Vorrei iniziare con una introduzione ai modelli di supervised learning. In supervised learning non si etichettano i dati manualmente, ma si etichettano utilizzando fonti di etichettatura automatiche come regole semplici, database di conoscenza o qualsiasi altra fonte di etichettatura bassa qualità, come illustrato nella figura sulla destra. In confronto all'annotazione umana, le etichette generate automaticamente sono molto più economiche, anche se anche queste sono rumorose, ovvero, alcune delle etichette sono incorrette. Se adesso diamo un'addestramento a una rete neurale con etichette generate automaticamente, la rete neurale tende a memorizzare i rumori e non generalizza bene. In supervised learning, gli algoritmi di addestramento sono progettati per addestrare le reti neurali in modo robusto su tali etichette rumorose, in modo che i modelli addestrati possano generalizzare bene. In recenti sviluppi nel campo del supervised learning, si sostiene che i modelli addestrati con etichette generate automaticamente ottengono prestazioni elevate su set di test puliti. Tuttavia, questa affermazione non è completamente corretta, poiché si assume che sia disponibile un set di validazione pulito per la selezione dei modelli. Abbiamo deciso di esaminare questa situazione problematica, poiché implica che è necessaria un'annotazione manuale aggiuntiva per il supervised learning. Ma, come un elefante in una stanza, questa necessità è spesso ignorata. Abbiamo adottato un approccio che consiste nell'interrogare tre domande di ricerca: 1) è necessario un set di validazione pulito per il supervised learning o possiamo utilizzare un set di validazione rumoroso invece? 2) se è necessario o obbligatorio un set di validazione pulito per il supervised learning, quanti campioni puliti dobbiamo avere? 3) dovremmo utilizzare solo i campioni puliti per la validazione o ci sono altre tecniche migliori? Abbiamo indagato queste domande di ricerca e i nostri findings sono i seguenti: 1) scopriamo che recenti metodi di supervised learning hanno bisogno di campioni di validazione puliti per funzionare correttamente, altrimenti c'è un grande drop di prestazione, come illustrato nella figura sulla sinistra. Se non ci sono campioni di validazione puliti, i modelli addestrati non possono generalizzare al di fuori dei relativi etichettamenti rumorosi, rendendo il training inutile. Questo indica che i metodi di supervised learning effettivamente richiedono etichette pulite per funzionare correttamente e il costo di ottenere etichette pulite dovrebbe essere tenuto a mente. Il nostro secondo finding è che aumentare il numero di campioni di validazione puliti aiuta i metodi di supervised learning a ottenere prestazioni migliori, come illustrato nella figura sulla sinistra. Di solito basta avere 20 campioni per classe per ottenere prestazioni elevate. Ma, non è tutto, perché se scegliamo di ottenere i campioni puliti, allora addestrare direttamente su di essi otterrà anche prestazioni migliori. La figura sulla destra illustra la differenza di prestazione tra i metodi di finetuning diretti, che vengono applicati direttamente sui campioni puliti, e i metodi di supervised learning che usano i campioni puliti solo per la validazione. Come possiamo vedere, se ci sono 10 campioni per classe, il finetuning diretto inizia a superare i metodi di supervised learning. Infine, l'incremento di prestazione affermato in precedenti metodi di supervised learning può essere facilmente ottenuto consentendo di continuare il finetuning sui campioni puliti. Come possiamo vedere dalla figura, il modello FWTW inizialmente sottopresta rispetto ai metodi di supervised learning più complicati come cosine, tuttavia se continuiamo a finetuning sui campioni puliti, allora FWTW ha prestazioni uguali a quelli dei metodi più complicati. Quindi in pratica non c'è motivo di scegliere i metodi di supervised learning più complicati che richiedono più tempo di elaborazione e spazio disco. In sintesi, dimostriamo che i recenti metodi di supervised learning richiedono etichette pulite per funzionare correttamente. I vantaggi e la praticità dei metodi di supervised learning sono sovrastimati. Le nostre raccomandazioni concrete per future work sono le seguenti: 1) rapportare i criteri di selezione del modello, ad esempio, rapportare se la selezione del modello è fatta senza campioni di validazione puliti. 2) i metodi di supervised learning dovrebbero essere confrontati con baselines di supervised learning che operano su campioni puliti. 3) il finetuning continuo è un esempio di un baselining semplice e forte che dovrebbe essere considerato in future work in supervised learning. Infine, abbiamo rilasciato il codice sorgente del mio modello FWTW, che è disponibile sul link QUR code sullo schermo. Sono felice di vedere che lo stanno utilizzando. Grazie per la tua attenzione.</sample>
    <sample id="241">The paper discusses a framework for evaluating and developing systems that detect misinformation on social media platforms. The authors propose an end-to-end system that integrates human feedback throughout the detection process, rather than relying solely on automated methods. The system uses a combination of keyword filtering, machine learning models, and human review to identify and flag potentially harmful claims. The paper also evaluates the effectiveness of the system in detecting policy violations and the human workload required to verify claims. Overall, the framework aims to provide a more realistic and human-centric approach to misinformation detection.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo sono le valutazioni umane, che consistono nell'inviare domande a giudici umani per selezionare tra due conversazioni o per valutare le conversazioni su una scala di Likert.</sample>
    <sample id="243">Sono coinvolti 5 autori nell'articolo.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che Servin è un giudice e che i giudici decidono cause in una corte.</sample>
    <sample id="245">The speaker, Linlin Jiang, presents a study on high-agreement workers on MTurk. The study includes qualification settings, two stages of qualification tasks and endurance tests, a reference-based task, and an analysis of cracks across annotation sources. The results show that the pipeline can achieve high agreement at a lower cost and similar quality to cloud research. However, there are limitations such as only testing English summarization on MTurk platform, design questions not having canonical solutions, and no guarantee for the training of cracks.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi vedere il link in GitHub.</sample>
    <sample id="247">The speaker introduces a new dataset, fact kg, which is used for fact verification via reasoning on knowledge graphs. The dataset includes claims in two styles: written and colloquial, and supports five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset also includes claims in both written and colloquial styles for practical use. The speaker proposes a new task called knowledge graph-based fact verification, which can be used to check the consistency between the user's world and the knowledge graph. The speaker also introduces a new model that uses graph evidence to verify claims, which outperforms all other baselines.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate in modo da preservare la struttura rilevante ma aggiungendo un rumore.</sample>
    <sample id="250">Valutare dimensionalmente significa valutare diverse dimensioni o aspetti di una qualità specifica, piuttosto che una qualità unica.</sample>
    <sample id="251">I ricercatori dell'articolo sono affiliati all'Università di Scienze e Tecnologia di China.</sample>
    <sample id="252">The speaker introduces a presentation on their work, "You Create," which focuses on unsupervised case retrieval using event extraction. They are a master's student at IIT Kanpur and have collaborated with Abhinav Joshi, Akash Sharma, and Ashutosh Modi. The presentation highlights the challenges faced by legal professionals in retrieving relevant past precedents due to the increasing volume of cases. They introduce the task of prior case retrieval, which involves retrieving relevant candidates from a candidate pool based on similarity in factual situations. Two key contributions are made: the ILPCR dataset and the You Create pipeline. The ILPCR dataset is a new benchmark for PCR tasks, containing 7,700 legal cases with an average of 6.775 citations per query document. The You Create pipeline leverages unsupervised learning techniques and introduces an event-based approach for PCR tasks, demonstrating high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning.</sample>
    <sample id="253">Il contenuto in inglese introduce un modello di apprendimento automatico per la deteczione di disturbi mentali in social media. Il modello, chiamato "Dissorb", utilizza una tecnica di adattamento a due domini per migliorare le prestazioni su un dominio specifico, in questo caso, il linguaggio relativo alla salute mentale. Utilizzando un modello pre-addestrato su Wikipedia e Google Books, il modello è stato adattato per riconoscere segni di disturbi mentali in post social media. I risultati dimostrano che il modello ha un bilanciamento ottimo tra precisione e ricordi, mentre i modelli tradizionali hanno spesso un alto livello di precisione o ricordi ma bassi in entrambe le dimensioni.</sample>
    <sample id="254">The speaker introduces a research work on uncertainly guided label denoising for document-level distant relation extraction, presented by Sun Qian from Nankai University of Science and Technology. The work aims to extract relations among entities in documents using distant supervision data, addressing the noise problem in DSS data through pseudo labels and dynamic class uncertainty thresholds. The proposed framework includes a pre-denoising model with both DSS and human-annotated data, an instance-level uncertainty estimation method, and a multi-phase training strategy. The paper compares the framework with several state-of-the-art baselines on two public datasets, achieving superior performance.</sample>
    <sample id="255">Se presenti, la forma del prompting si rivela importante in caso di zero o uno shot prompting.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo.</sample>
    <sample id="258">The speaker introduces the topic of using large language models to evaluate the quality of text in natural language processing. They explain how they have used these models to rate samples based on instructions, similar to human evaluations. The speaker mentions that while there are related works on this topic, their idea was novel at the time of submission. They discuss the motivation behind their work, which is to find an alternative to human evaluations that can achieve the same goal but without the drawbacks of instability and difficulty in reproducing the process. The speaker then describes the experiment they conducted, using large language models to rate stories generated by GPT-2 or written by humans, and compares the results with human evaluations.</sample>
    <sample id="259">The speaker introduces Exemplar, a framework for cross-lingual semantic parsing in multiple natural languages and many representations. It provides a unified dataset containing 9 datasets from various domains, 5 semantic parsing tasks, 80 million representations, and 22 natural languages in 15 language families. The framework is evaluated on six settings: translate test, monolingual model, monolingual fine-tune, multilingual model, cross-lingual zero-shot, and cross-lingual few-shot transfer. Encoder-decoder models outperform previous work, and the results show that most major natural languages can benefit from multilingual training, except for English.</sample>
    <sample id="260">Il numero di autori coinvolti nell'articolo è 1.</sample>
    <sample id="261">Un buon pianificatore dovrebbe redigere script che siano ragionevoli e fattibili alle restrizioni.</sample>
    <sample id="262">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="263">The speaker presents their work on mitigating label biases in in-context learning, a popular paradigm for utilizing large language models. They identify various design choices that introduce biases to the model's predictions and propose a calibration method to handle all types of biases, including domain label bias. The speaker conducts experiments to confirm that task-specific domain label bias can significantly affect the model's predictions and proposes the domain context calibration method to mitigate these effects.</sample>
    <sample id="264">Il presentatore, Ling Wang, è un studente laureato presso la Zhonghua University in Cina. Ha proposto un nuovo modello per la generazione di test audiovisivi trasferibile, che utilizza una rete neurale per mappare concetti visivi in uno spazio semantico unificato. Questo modello include anche un encoder e un generatore di linguaggio per migliorare la comprensione semantica. L'approccio utilizza un algoritmo di imitazione per creare token visivi prefissi e un modello di ottimizzazione per alignare i token con lo spazio audiovisivo. L'approccio è stato testato su due benchmark e ha dimostrato un'ottima prestazione, specialmente per le risorse limitate.</sample>
    <sample id="265">Il nome della relatrice o del relatore è Vasudeha.</sample>
    <sample id="266">I ricercatori che hanno scritto l'articolo sono affiliati all'Institute of Linguistics, Russian Academy of Sciences.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono omission errors, che consistono nella produzione di traduzioni migliori a spese di omettere parti della frase sorgente.</sample>
    <sample id="269">Ciao, mi chiamo James Finch e io sono Sarah Finch. Oggi parleremo di ABC Eval, un nuovo approccio multidimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato fatto dal laboratorio NLP dell'Emory, guidato dal professor Geno Choi all'Emory University, in collaborazione con Amazon Alexa AI. Supponiamo che tu abbia appena sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con le migliori pratiche attuali. La pratica comune è di richiedere giudizi umani per scegliere tra due conversazioni o per valutare le conversazioni su una scala di Likert. Questi metodi funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per capire i punti forti e deboli del modello in un livello più finegrain. Uno degli approcci è chiedere ai giudici umani di valutare diversi aspetti della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi esistenti o scala di Likert. Tuttavia, creiamo che ci sia un modo più preciso e affidabile per valutare le dimensioni del dialogo. Il nostro approccio tenta di ridurre la soggettività dei giudizi umani, definendo esplicitamente se o meno ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni non rilevanti o contradire se stessa. Chiamiamo questo approccio ABC Eval o annotazione dei comportamenti in chat. Abbiamo sviluppato questo metodo per coprire completamente i comportamenti del modello di chat suggeriti recentemente in letteratura. ABC Eval è capace di misurare le tassi in cui i modelli di chat commettono errori thematici vari. Ad esempio, ABC Eval misura il numero di giri in cui un modello di chat ignora il suo interlocutore o dice qualcosa di non rilevante, si contraddice da solo o con il proprio interlocutore, elenca fatti incorretti o viola le conoscenze comuni, e quando il modello riesce o fallisce a dimostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di punta e li abbiamo valutati su 100 conversazioni umane-bot per modello utilizzando ABC Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni di Likert a livello di giro, valutazioni di Likert a livello di dialogo e confronti a livello di dialogo pairwise. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto dei più comunemente misurati aspetti del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dall'analisi dei risultati delle valutazioni, abbiamo scoperto che i etichettatori ABC Eval sono in generale più affidabili rispetto alle etichette raccolte da metodi esistenti, come misurato dall'acordo inter-etichettatore su 100 conversazioni etichettate in duplicato. Inoltre, i etichettatori ABC Eval sono più predittivi della qualità complessiva della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostrato da un'analisi di regressione lineare semplice. Ad esempio, si può vedere come misurare la proporcione di giri in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di non rilevante spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza Likert medio spiegano solo il 4% o meno. Infine, si è verificato se ogni metrica di valutazione mantiene un aspetto unico della qualità del chat utilizzando una regressione lineare passo dopo passo. Si può vedere come la combinazione di tutti i metri ABC Eval spiega più del 25% della qualità della conversazione e che rimuovere i metri uno alla volta porta a una perdita di informazione significativa sulla qualità. Sulle altre mani, la combinazione di tutti i metri a livello di giro Likert spiega ben meno della qualità e meno di questi metri hanno informazione unica. Questi metri ABC Eval affidabili e informativi consentono di valutare l'intelligenza artificiale conversazionale con un livello di risoluzione più elevato rispetto ai metodi precedenti. Come puoi vedere nei risultati del nostro esperimento, ci sono ancora alcune sfide che devono essere affrontate e precisamente quantificate. Ad esempio, i bot che testammo hanno violazioni di senso comune in circa il 20% delle loro risposte, producono informazioni non rilevanti in circa il 15% delle risposte e si contraddicon con se stessi o con il proprio interlocutore circa il 10% del tempo. Con la velocità rapida di miglioramento nel campo, molte di queste tassi di errori potrebbero vedere una diminuzione negli nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questa è tutto il più motivo per cercare metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC Eval possa essere utilizzata da altri nel campo come un passo importante in questa direzione e ci guardiamo ad vedere come l'intelligenza artificiale conversazionale proglierà nei prossimi mesi e anni. Grazie per l'attenzione.</sample>
    <sample id="270">I fornitori dell'articolo sono il laboratorio NLP dell'Emory e Amazon Alexa AI.</sample>
    <sample id="271">CFT significa Continuos Fine Tuning, che è un approccio di apprendimento continuo che utilizza i campi di validazione puliti per migliorare le prestazioni dei modelli.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, il mio nome è Kai Yen e presenterò il nostro lavoro intitolato "Quando la traduzione richiede un contesto: un esplorazione multilingue guidata da dati". Questo lavoro è stato realizzato in collaborazione con Patrick Franz, Emi Yu, Andrew F. D. Martins e Graham Navig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurre "Mole" in questa frase? Se la frase precedente era "Le cose potrebbero iniziare a mettere in pericolo se i ministeri scoprono", allora "Mole" si riferisce a un spia. Ma se la frase precedente fosse stata "Sarebbe qualcosa di serio, dottore?" allora "Mole" si riferisce a una macchia di neve. Quindi, a seconda del contesto, il significato della parola cambia e, di conseguenza, anche la traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola percentuale delle traduzioni dipende dal contesto, i metri a livello di corpus come BLEU non riescono a catturare queste traduzioni. E alcuni hanno suggerito una valutazione mirata su traduzioni a contesto dipendenti, ma questi risorse supportano solo tipi limitati di traduzioni a contesto dipendenti e limitati insiemi di lingue, poiché di solito si basano sulla conoscenza dominio e curazione umana. In questo lavoro, cercheremo di rispondere a queste due domande: quando la traduzione richiede contesto e come bene i modelli gestiscono questi casi. Per rispondere alla prima domanda, ci siamo concentrati su quanto un termine dipende dalla traduzione a contesto. In un precedente lavoro, abbiamo introdotto CXMI come misura dell'uso di contesto da parte dei modelli di traduzione automatica. Questo è fatto misurando quanto informazione il contesto C fornisce sul target Y, data la parola X. Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, stiamo estendendo CXMI a P-Y-CXMI, che può misurare l'uso di contesto al livello della frase o al livello del termine. Possiamo pensare a parole che hanno alta P-CXMI come parole che richiedono contesto per la traduzione. Ora analizziamo parole con alta P-CXMI per cercare di trovare模式 tra queste parole. E performiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Lavoriamo a tre livelli diversi: innanzitutto guardiamo un pezzo di discorso che ha alta media P-CXMI e questo ci consente di trovare, ad esempio, i pronunti duali in arabo che hanno una media alta P-CXMI e questo può essere spiegato perché l'inglese non ha pronunti duali, quindi si necessita del contesto per determinare se un pronuncio è doppio quando si traduce in arabo. E allo stesso tempo, troviamo che certe lingue richiedono contesto quando si sceglie la forma verbale appropriata. Poi guardiamo gli elementi lessicali che hanno alta P-CXMI in media su tutti i propri accadimenti e questo ci aiuta a identificare casi come quello qui, in cui in cinese si hanno bisogno di contesto per tradurre i nomi in modo corretto per assicurarsi di usare la stessa traduzione nel documento e allo stesso modo, troviamo che il contesto è supportato per tradurre in una forma formale e infine, guardiamo i singoli token che hanno alta P-CXMI e questo ci consente di identificare fenomeni che non possono essere realmente catturati da un termine in sé, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'elispe. Quindi ora utilizziamo i nostri findings dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo tagger per automaticamente identificare parole che appartenono al fenomeno e chiamiamo il tagger il multi-lingue discorsamente attento o Muda tagger. Possiamo anche notare che le lingue hanno proporzioni diverse di questi fenomeni discorsivi. Poi utilizziamo il Muda tagger, applicandolo a un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo i metri di traduzione di scelta su esempi a contesto dipendenti che il Muda tagger ha identificato e infine utilizziamo il benchmark insieme ad altri metri per valutare diversi modelli di traduzione a livello di documento. In primo luogo, quando utilizziamo metri a livello di corpus come BLEU, scopriamo che i modelli di traduzione a contesto ascoltano hanno le migliori prestazioni, ma se utilizziamo Comet, i modelli di traduzione a contesto attento hanno le migliori prestazioni e se utilizziamo F-Measure, i modelli con o senza contesto hanno prestazioni comparabili. Questo di nuovo dimostra che è difficile determinare il migliore sistema di traduzione a livello di documento se utilizziamo metri a livello di corpus solo. Ora utilizziamo il benchmark Muda per valutare i modelli e scopriamo che i modelli di traduzione a contesto attento sono significativamente più accurati dei modelli che non usano contesto per certi fenomeni discorsivi, come formalità e coesione lessicale, ma questi modelli non sono molto migliori dei modelli che non usano contesto per altri fenomeni come pronunzie e forma verbale, quindi questa sorta di suggerisce dove dovremmo vedere un maggior progresso per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è generalmente più accurato del Google Translate per la traduzione a livello di documento. Per riassumere, abbiamo svolto un'analisi guidata da dati attraverso 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e poi utilizziamo i nostri findings per costruire un benchmark per la traduzione a livello di documento, che ci aiuta a identificare i fenomeni discorsivi ai quali i modelli possono gestire bene o no e i sistemi di traduzione che sono buoni per la traduzione a livello di documento. Grazie mille per la vostra attenzione, ci vediamo in Torino!</sample>
    <sample id="274">Il nome della relatrice o del relatore è Yuxin Zhang.</sample>
    <sample id="276">The speaker is presenting a study on the evaluation of machine translation metrics for Indian languages. The study focuses on five languages belonging to two different language families: Tamil and Malayalam, which are Dravidian languages, and Hindi, Marathi, and Gujarati, which are Indo-Aryan languages. The researchers selected 200 sentences randomly from the FLORES dataset and generated multiple candidate translations for each source sentence in English using seven different translation models or APIs. They collected human annotations on these samples, asking annotators to mark errors, provide an overall score, and indicate specific subcategories of errors. The study observed that recent models like NLLB and Indic Trans have fewer errors compared to older models like CVIT. The correlations between QMUM-based scores and metric scores were also analyzed, with Comet metric variants showing the highest overall correlations for all languages.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Il libro si concentra sulle parole che distinguono i gruppi contrassegnati da quelli non contrassegnati.</sample>
    <sample id="279">I ricercatori che hanno scritto l'articolo sono studenti di PhD presso l'Università di Washington.</sample>
    <sample id="280">The paper proposes a novel multi-modal fusion framework for emotion regulation in conversations, called MultiEmo. The framework consists of four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. The authors propose a novel visual feature extractor named VisiNet, which captures visual cues by integrating facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. They also design a multi-modal fusion model called MultiAtt, which integrates one modality with complementary information from the other two modalities through stacked bidirectional multi-head cross attention layers. To address the difficulty of distinguishing between semantically similar emotions, they introduce a sample-weighted focal contrastive loss that assigns higher importance to hard-to-classify minority classes and makes samples pairs with different emotion labels mutually exclusive with each other to maximize inter-class distances. Experimental results demonstrate that MultiEmo achieves state-of-the-art performances on two ERCC benchmark datasets, MELD and iEMO CAP, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">Il discorso introduce un lavoro intitolato "Quando è necessario il contesto per la traduzione? Un'osservazione multilingue data-driven" presentato da Kai Yen, in collaborazione con Patrick Franzke, Emi Yu, Andri F. D. Martins e Graham Neubig. Lavoro mira a indagare quando le traduzioni richiedono contesto e come i modelli di traduzione gestiscono queste situazioni. Utilizzano un nuovo metodo chiamato CXMI per misurare quanto un termine dipende dal contesto. Analizzano i testi di TED Talk tradotti in 14 lingue diverse a tre livelli: token, frase e parola. Risultati hanno identificato fenomeni come pronunzie doppie in arabo, scelta di verbi e formalità che richiedono contesto. Creano una benchmark per traduzioni documentali basata su queste analisi e valutano diversi modelli di traduzione. Trovano che i modelli che usano contesto sono più accurati per fenomeni come formalità e coesione lessicale, ma non migliorano molto per fenomeni come pronunzie e forme verbali.</sample>
    <sample id="282">Il contenuto in inglese introduce una nuova opera intitolata "Story Trans Non-Parallel Story Style Transfer with Discourse Representations and Content Enhancing" presentata da Shuang Chou. Questa opera si concentra sulla trasformazione di stile in un testo senza alterare il significato originale, un compito importante in generazione di linguaggio naturale. L'approccio proposto utilizza una combinazione di modelli di generazione e tecniche di impianto di stile per mantenere la struttura discorsiva e i concetti specifici del testo sorgente. Il modello è stato addestrato su dataset di testi in cinese e inglese e ha dimostrato un forte controllo di stile e precisione nel trasferimento di testi.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è il prague approach.</sample>
    <sample id="284">Il presentatore, Peng Tian Xiao, rappresenta la Università di Wuhan e propone un nuovo modello di estrazione di informazione universale (UIE) per migliorare la precisione della scansione di spam. L'approccio tradizionale utilizza etichette precise per definire i limiti del spam, ma il nuovo modello utilizza una distribuzione continuo di probabilità per modellare i limiti del spam, rendendo più flessibile l'identificazione dei limiti del spam. Inoltre, il modello propone un'attenzione adattativa per la scansione di spam, utilizzando un modello di distribuzione di probabilità continuo per calcolare la perdita di entropia di cross e la divergenza di Kullback-Leibler. Questo approccio migliora la capacità di identificare i limiti del spam e ottiene un'accuratezza maggiore nella scansione di spam.</sample>
    <sample id="285">The speaker introduces a new evaluation framework for fact error correction in dialogue summarization. They argue that current evaluation methods, such as fact C and DA, are flawed because they do not accurately reflect the performance of models in correcting fact errors. The proposed framework includes alignment, classification, and comparison steps to automatically classify fact errors and evaluate the performance of fact error correction models. The speaker also proposes a taxonomy of fact errors based on content and form, and suggests combining human-annotated data with synthetic data to improve model performance.</sample>
    <sample id="286">Il nome della relatrice o del relatore è James Finch.</sample>
    <sample id="287">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="288">I fenomeni sintattici possono essere testati utilizzando insiemi di dati come BLM, Syntax Gym e GPT-2.</sample>
    <sample id="290">La prima domanda di ricerca è se i set di validazione puliti sono necessari per WSL o se possiamo utilizzare un set di validazione rumoroso invece. Le abbreviazioni dei cinque metodi per questa domanda sono: 1) Clean validation data, 2) Noisy validation set, 3) Clean samples, 4) Validation samples, e 5) Clean validation samples.</sample>
    <sample id="291">Il modello viene valutato su attività come riconoscimento di nomi e cognomi, classificazione, part-of-speech tagging e risoluzione di domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un dataset di medici con 7 GB di NAO.</sample>
    <sample id="295">Il nome della relatrice o del relatore è Adam Sipruchowsky.</sample>
    <sample id="296">The speaker, Vario Basile, presents a work on irony detection in natural language processing. The team collected data from social media sources like Reddit and Twitter, annotating 300 short conversations with pairs of texts. They used the crowdsourcing platform Prodigy to have 74 annotators label the data, with each annotator providing five annotations per conversation. The results showed that perspective-aware models were more confident in their predictions than gold-standard aggregated models. The team also found that annotators from the UK and Ireland had the highest variation in their responses.</sample>
    <sample id="297">Il contenuto discende da un discorso sull'uso di dog whistles in politica, in particolare nel contesto degli Stati Uniti. Viene presentato un esempio di un discorso di un senatore che utilizza un dog whistle per riferirsi a un'agenda e sperimento conservatore, ma in modo che solo gli in gruppo comprendano il messaggio antisemitico nascosto. Viene anche discussa la difficoltà di rilevare i dog whistles con modelli di linguistica naturale, poiché queste tecniche sono più efficaci quando gli utenti esterni non sono consapevoli di loro. Infine, viene illustrato come i dog whistles possano evitare la moderazione del contenuto online, rendendo più difficile la rilevazione e la rimozione di discorsi razziali o transfobici.</sample>
    <sample id="298">I risultati dell'esperienza hanno dimostrato che i modelli pretrainati con più recenti dati hanno prestazioni peggiorate con un maggior spazio temporale tra i dati di training e i dati di test. Questo ha confermato l'ipotesi che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="299">Hi everyone, my name is Michalis Katakis. Today we are here to talk about improving the robustness of AI models with minimum extra training. This is joint work with Andreas Bloos at the University of Cambridge. AI models have achieved state-of-the-art results across a wide range of benchmarks. However, despite rapid progress, recent work has demonstrated that the success of AI models is partly due to learning and using shortcuts. Shortcuts are spurious correlations between the input attributes and labels introduced during data creation process. For example, high word overlap between the premise and the hypothesis in the MNLI dataset is strongly correlated with the entailment label. Consequently, AI models that exploit shortcuts perform well on in-distribution samples but are brittle when tested on out-of-distribution or serial test sets where such spurious correlations do not hold. By work in shortcut mitigation typically assumes access to an auxiliary model designed to learn shortcuts for predictions. For instance, the auxiliary model can learn to exploit shortcuts by being trained only on a small number of examples or by leveraging an auxiliary with reduced learning capabilities. The output of the auxiliary is then used to reweight training instances for the learner model. Existing shortcut mitigation methods may require knowing this in advance. This assumes domain and task-specific knowledge which is not always available and thus limits the potential of shortcut mitigation. Furthermore, current shortcut mitigation methods often assume that the learner will naturally exploit the same types of shortcuts as the auxiliary. In practice, the behavior of the learner diverges from that of the auxiliary. For example, the auxiliary may downweight instances that are useful for training the learner or provide inaccurate uncertain estimations that may hinder the learner's out-of-distribution generalization capabilities. Finally, current shortcut mitigation methods require using a pre-trained language model as auxiliary which incurs additional computational overhead. Motivated by these limitations, in this work we propose a training method to reduce the reliance of AI models on shortcuts and improve their out-of-distribution performance. The key insight behind our training method is that AI models suffer from poor performance on under-represented hard training instances with patterns that could indicate the shortcuts in the dominant easy examples. These hard examples are pivotal for ensuring good generalization performance on out-of-distribution samples. Crucially, the loss of hard examples decreases considerably more slowly than the average loss throughout training. Therefore, our aim is to obtain an example weight distribution that places emphasis on the under-represented hard examples. To compute the weight distribution, we pose a minimum extra training objective between a learner and auxiliary. The learner tries to minimize the loss of the NLI task whereas the task of the auxiliary is to maximize the learner's loss by generating example weights such that the learner is incentivized to concentrate on regions of the input space where it incurs high losses. Thus, the learner would prioritize learning from under-represented hard examples that counteract the use of shortcuts present in the dominant easy examples. Both models are optimized in an alternating fashion using any standard optimization algorithm such as stochastic gradient descent. At test time, the learner can make predictions without relying on the auxiliary. Our method does not make any assumptions about the type of shortcuts contained in a dataset. It relies on the learner's own training dynamics to generate example weights. And finally, we use a feedforward network to model the auxiliary. We evaluate our proposed method in three commonly used NLI datasets: MNLI, Fever, and QQP and the corresponding out-of-distribution natural test sets: Hans, Symmetric, and Impulse. Here we observe that compared to an ERM trained model as well as the best performing shortcut mitigation method in each dataset, the minimum extra training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy. Finally, in our paper we also examine whether the performance improvements transfer to larger models, synthetic shortcut and out-of-domain test sets. What is the effect of pre-training the learner? How small the auxiliary needs to be? And finally we conduct a qualitative evaluation of the learned example weight distribution. If you find this work interesting, we would love to chat with you during our post session. Thank you for your time.</sample>
    <sample id="300">Hi, my name is Blenda and the work I'll be presenting today introduces a task called interactive dictation and makes initial steps towards solving this task. This research was done at Semantic Machines in collaboration with Jason Eysenhar, Adam Pauls, and Sam Thompson. So what is interactive dictation? At a high level, interactive dictation is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. In this example, a user starts by dictating "just wanted to ask about the event on the 23rd." This is transcribed verbatim into the text box. However, in the middle of speaking, the user realizes they made a mistake and corrects themselves saying, "on Friday the 23rd." Ideally, the system can pick up that this was a speech correction and replace the correct span with the new utterance. Next, the user continues transcription saying, "is the event still on?" which gets transcribed into the text box. Finally, the user can issue a verbal command like, "replace the event in the last sentence with it." The system can identify the correct occurrence of the event to replace with it. While speech-to-text systems are starting to proliferate, most of them support only dictation and do not support invoking edits through vocal commands. There are a few software that do recognize vocal edit commands such as Neurons Dragon Naturally Speaking and the Microsoft Dictate function. However, these systems can be unintuitive because they require memorizing a fixed set of template commands. We know that a more natural and intuitive interface is possible when dictating to a human assistant even without agreed upon trigger words or commands. Humans can generally tell when you're commanding versus when you're dictating and what command you're invoking. Thus, distinct from prior work, the interactive dictation task is characterized by the following key features: first, flexible interleaving of dictation and editing, not separated by a trigger word; second, using intuitive and open-ended natural language utterances to specify edits. In summary, our contribution is threefold: first, we introduce and formalize a new task, interactive dictation; second, we design a data collection interface and build a dataset for this task; and finally, we create a baseline system for this task. To begin, we formalize the task of interactive dictation as a four-step procedure. In the first step, an ASR recognition module parses raw audio into a speech transcript. Next, the speech transcript is segmented into separate dictation and command utterances. Third, each command is extracted and normalized. The ASR misdetctions and speech errors are fixed. Finally, each dictation and command utterance is executed in sequence until we arrive at the final document state. Note that in a real system, this all happens in real time as the user is speaking. Since this is a new task, we need to collect data. We need to collect our own data for which we design a new interface. So I'm going to transcribe the following email. I'm going to start by clicking Begin Transcription. Hey, I'm really sorry but I can't make it today. Note that this issued a dictation or insert text segment and whatever I said got transcribed both in this ASR field and also into the document state. Now I'm going to press the control button on my keyboard to issue a command. Change the comma after hey into an exclamation point. Okay, note that I just issued a command and unlike dictations, commands don't automatically show up in the document state. I must demonstrate the text. Demonstrate the change using mouse and keyboard. Moreover, four commands, I can actually go into the ASR and fix it up. So I can keep doing this issuing commands and dictations in sequence until I've replicated this email. Using this annotation interface, we collect the dataset. More details about how these trajectories were collected can be found in the paper. Finally, we build a baseline system that performs each of these four steps. We train a model to perform each of these. We train a separate model to perform each of these steps. You can see the paper for more details. But in particular, for the interpretation model, we experiment with two different architectures, T5 and GPT-3, and two different types of outputs. We either have the model predict programs that can be executed into the next state or we have it directly predict the next state. First, for the segmentation model, we see that it's both fairly accurate and efficient. Next, we evaluate the ASR repair and interpretation models directly, using exact match of the predicted end state against the gold end state. We find that there is a generally a trade-off between runtime and accuracy and that generally GPT-3 models are more accurate but also much slower. Furthermore, for GPT-3, GPT-3 models predicting state directly is much more accurate than predicting intermediate programs. For T5 model, this distinction is much less pronounced and predicting programs allows us to significantly improve efficiency with minimal impact on accuracy. As you can see, however, there's clearly much more room for progress here and we welcome more work on this task. To facilitate future work, we have released code at the following site. Please also check out the paper for more details.</sample>
    <sample id="302">Permettere la permutazione dei token nella sequenza di output è necessario per ottenere una generalizzazione più forte verso le rappresentazioni più profonde. Questo è fatto per prevedere la sequenza esatta dei token nell'output, in modo che i modelli possano imparare a gestire strutture complesse e a produrre output coerenti.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire se i modelli stanno producendo stereotipi positivi e narrativa essenzializzante a causa di un al线过量的过度价值对齐或可能的反刻板印象方法。</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono le sentenze grammaticalmente sbagliate o non grammaticalizzate.</sample>
    <sample id="305">Hello, I am Dawei, a PhD student at Salzburg University in Germany. In this video, I would like to present our recent work, "Weaker than You Think: A Critical Look at Weakly Supervised Learning." This is joint work with Xiaoyu Chen, Mauro Smola, and Giasemone and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, you do not manually label the data; instead, we label the data using weak labeling sources such as simple heuristics, rules, knowledge bases, or low-quality crowdsourcing, as illustrated in the figure on the right. When compared to human annotations, the weak annotations are much cheaper yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for weakly supervised learning, a common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong, but there's a catch, which is that people do assume that there is an additional clean validation set available for model selection. We cut off on this problem setting because this implies that additional manual annotations are required in weakly supervised learning, but like an elephant in the room, this necessity is often overlooked. The aforementioned ad hoc is us to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead? Second, if clean data is required, or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation, or are there better ways to utilize them? We address these research questions in our work, and our findings are as follows. First, we find that interestingly, recent WSL methods indeed require clean validation samples to work properly, otherwise there is a large performance drop, as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly, and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story, because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches, which are directly applied on the clean data, and WSL approaches, which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model termed FTW initially underperforms more complicated WSL methods like cosine. However, if we allow to continue fine-tuning on the clean samples, then FTW performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we show that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria, for example, report if the model selection is done well with clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines, as opposed to working on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="306">Hello, everyone. I am Sebastian Ruster and together with Nayan Kim I'm going to give you a short overview of our work on entity tracking in language models. For an agent to understand the discourse it needs to track which entities are mentioned and how their state changes as the discourse unfolds. So for example in the context of a recipe such as here an agent has to understand that put the eggs, sugar and flour in a bowl results in all of these three entities ending up in a bowl. And if the discourse continues with mix to form a light batter then the agent has to understand that now all of these entities are part of the batter. And so we argue that this is a crucial ability for understanding long discourses but there haven't really been any systematic investigations into what a pre-trained language models can actually perform such tasks. And so the overarching research question we're trying to answer in this paper is to what extent large language models can track entities. Now given that we don't know the exact contents of the pre-training data of many language models and considering several properties about how discourses work there are actually several challenges with designing a task to evaluate entity state tracking abilities. First some entity states will be common in the pre-training data and therefore the model may predict the correct state without actually having any entity tracking abilities. For example eggs often end up in bowls or babies often end up in cribs. So we want to make sure that the distributional patterns in the pre-training data cannot give away the entity states in the evaluation data. Then second sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse and the model may seem to be able to perform entity tracking while in fact it just learned simple heuristics associations between words and entity states. For example that the word empty is always associated with an entity being empty. And third if one uses fine-tuning or in-context demonstrations which is often necessary to probe the model then the model may memorize entity state sequences or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. And so in designing our evaluation task we took great care to make sure that the model cannot make use of any of these shortcuts when we evaluate its entity tracking abilities. And Nayan Kim will tell you a bit more about how we set up this task. Oh my name is Nayan Kim and I'll be talking about the task design and the experimental results. So to evaluate entity tracking abilities we designed the following task involving boxes and objects. And in our setup the input to the model starts with a description of the initial contents of each box as sketched on this slide. And the task of the language model is to complete the input by predicting the contents of each box. Now given just this initial description the task is pretty trivial the model can just copy the relevant information from the description. But in our task we also include multiple state changing operations like moving objects or adding objects to a box. So for these the model would have to combine the initial description with the operations to make the correct prediction. For example box one now contains the car and the watch after moving the watch from box three to one. And additionally we implemented various measures to prevent the model from using heuristics as Sebastian discussed earlier on so please check out our paper for how we did this. We tested this setup with fawn t5 and gpt 3 and 3.5 models using two shot in context learning. And what we're showing here is the accuracy of predicting the correct box content as a function of the number of operations acting on a certain box. And on the left panel we have the data points where the proved entity state is different from the state in the initial description whereas on the right panel we have cases where the state is the same as in the initial description. So for these ones the model can simply copy. And our experiments show that most models simply repeat the initial state as you can see from the generally high accuracy on the right panel. And we can also see that only text davinci o three accepts non-trivial tracking which is the pink line here in the left panel. And all other models perform below a strong random baseline obtained by random simulation which is the blue line. So what gives rise to this difference between models? Since the models we tested varied along several different dimensions we investigated what other factors might be in play by zooming into the gpt series. And we found that all gpt 3.5 models which all have been trained on substantial amounts of code exhibit non-trivial entity tracking behavior whereas all models that do not have code as a substantial part of their pre-training do not. And this suggests that pre-training on code is what's responsible for making this capacity surface in pre-trained language models. We also found that smaller models like t5 base can learn to perform entity tracking if you directly fine-tune the models but on the other hand randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision suggesting that pre-training is again important here. However as we discuss in more detail in the paper it remains unclear whether the state tracking abilities we observe generalize beyond our setup in this case. Thanks for listening and we have a lot more results and analyses including gpt four experiments in our paper so please check out archive. And if you have any questions or comments about our work either find us in person at acil or you can reach out to us over email or on twitter. Thank you.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche di valutazione come riconoscimento di entità, classificazione, part-of-speech tagging e risoluzione di domande.</sample>
    <sample id="308">The speaker, Jenny, is a first-year PhD student at Carnegie Mellon University presenting her work on NL Positionality. She explains that design bias can occur due to the positionality of NLP researchers and model developers, which can influence research outcomes. The framework NL Positionality compares annotations with real users to existing datasets and models. The study found that NL datasets and models are most aligned with English-speaking countries and people with college education. Recommendations include keeping records of design choices, conducting NLP research with a lens of perspectivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra i annotatori è l'interannotatore accordo.</sample>
    <sample id="310">Wikipedia è stato scelto come dominio per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="311">I fornitori di informazioni dell'articolo sono la Università di Heidelberg e la Università di Bonn.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo dataset di addestramento per la tuning multi-modal basato su istruzioni. Questo dataset comprende 62 diverse attività multi-modal coprendo 10 category diverse, derivate da 21 set di dati esistenti e aperti sorgenti. Ogni attività è equipaggiata con cinque istruzioni esperte. Inoltre, MultiInstruct utilizza un modello pre-addestrato OFA come modello base e utilizza un insieme di cinque istruzioni per addestrare il modello per ogni attività.</sample>
    <sample id="313">Due autori sono coinvolti nell'articolo.</sample>
    <sample id="314">La coordinazione binaria è un tipo di struttura grammaticale in cui due o più elementi sono collegati da una coordinazione.</sample>
    <sample id="315">I prompt in questo studio sono stati utilizzati per un tempo di 15 minuti.</sample>
    <sample id="316">I risultati suggeriscono che il modello T5 più piccolo può generare script di alta qualità e migliorare la capacità di pianificazione linguistica con vincoli, rendendolo un modello più accessibile per l'uso in applicazioni di pianificazione linguistica limitata o specializzata.</sample>
    <sample id="317">The speaker introduces their work, CodeIE, which transforms unstructured text into structured information using code generation models. They discuss the challenges of traditional models and propose a new approach to address them. The speaker evaluates their method on various datasets and finds that it outperforms traditional baseline models. They also analyze the results and provide insights for future improvements.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Lavrac e presenterò i nostri lavori su Dr. Bert, un modello di apprendimento a rete profonda in francese per il dominio biomedico e clinico. In questa presentazione, innanzitutto parliamo di modellazione linguistica in salute. Poi presenteremo le principali contribuzioni del nostro articolo. Introduciamo il primo modello biomedico in francese, Dr. Bert, che è basato su Roberta e è stato addestrato su Natos, che è un dataset di dati medici scarolati dalla rete. Abbiamo anche introdotto una comparazione dei modelli con diverse impostazioni di addestramento e fonti di dati. Infine, presentiamo i nostri risultati su 11 compiti di apprendimento biomedico e clinico in francese. Conclusamente, eseguiamo gli esperimenti e forniamo maggiori dettagli su come accedere ai modelli. Da quando è stato rilasciato nel 2018, Dr. Bert è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione linguistica naturale e ha offerto un'ottima migliura rispetto ai metodi statici e costituzionalizzati come Word2Vec, FastText o NER. Da allora, questo modello è stato adattato a molti altri linguaggi, tra cui il francese con Camembert e altri domini come biomedico con Pemetbert e BioBERT e clinico con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati in domanda. Tuttavia, il francese non aveva nessun modello aperto sorgente per biomedico e clinico. Così, ci hanno chiesto cosa sarebbe stato il dataset più appropriato per una vasta gamma di usi e i nostri dati scarolati sono stati un buon sostituto per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato Dr. Bert con il nostro modello Shubert, che è basato su un dataset annullato ottenuto da Non-University Hospital Data Warehouse. Successivamente, ci hanno chiesto quanta data è necessaria per addestrare un modello specializzato in dati francesi: 4 GB, 8 GB o altro? Per rispondere a questa domanda, abbiamo addeistrato e confrontato quattro modelli: una prima versione di Dr. Bert con 7 GB di Natos, una seconda versione di 4 GB di subset di Natos, una prima versione di Shubert che è un modello clinico con 4 GB di frasi tratte da ClinicalNotes e unaVersione di Shubert con un mix di 4 GB di subset di Natos e 4 GB di ClinicalNotes. Inoltre, abbiamo introdotto tre modelli addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati adde</sample>
    <sample id="319">Nel lavoro vengono esaminate strategie di apprendimento come l'apprendimento continuo e l'apprendimento da zero.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test non è stato osservato.</sample>
    <sample id="321">La qualità della semplificazione è stata analizzata in base al tipo di semplificazione, ad esempio le semplificazioni lexical, strutturali e a livello globale. Inoltre, è stata analizzata la varieta di diverse trasformazioni di semplificazione presenti nel corpus.</sample>
    <sample id="322">The speaker, Enrico, is presenting at ACL 23 and discussing the challenge of teaching language models to understand morality in text. He explains that human morality is subjective and can be interpreted differently by different people. He introduces the moral foundation theory, which suggests that humans perceive morality through five different foundations, each prioritized differently by individuals. Enrico applies explainable AI techniques to language models trained on the Mora Foundation Twitter Corpus to understand how morality is expressed differently across domains. He finds that language models recognize differences in moral expressions between domains like #AllLivesMatter and #BlackLivesMatter, highlighting the need for domain-specific approaches to avoid misunderstandings of morality.</sample>
    <sample id="323">The audio contains a speech in English about a research paper titled "Dynamic Entity-Graph Running with Language Models and Knowledge Graph Reasoning for Commonsense QA." The speaker, Yu Jia Wang from Shanghai University, China, explains that Commonsense QA is a challenge task requiring models to answer questions relying on common knowledge. The paper proposes a method combining language models and knowledge bases to retrieve relevant knowledge through entity matching and building a subgraph, using GNNs to infer answers. The method addresses issues like limited interaction between models and encoding subgraphs, and uses a multi-task learning strategy to optimize the structure and knowledge representation of the knowledge graph. Experiments show that the proposed method outperforms other baselines in Commonsense QA and OpenBook QA tasks.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="325">Il mio nome è Mateusz Lendman e oggi voglio dare una breve introduzione al mio articolo su generalizzazione composizionale senza alberi utilizzando tag multi set e permutazioni latent. Questo è un lavoro in collaborazione con i miei tassatori Alexander Koller e Ivan Titov. La generalizzazione composizionale può essere comprensibilmente vista come la capacità di un imparatore di gestire una ricorrenza più profonda e composizioni di frasi che hanno visto individualmente durante la formazione. In contesto di parsing semantico, il test per la generalizzazione composizionale potrebbe apparire così: come di consueto, abbiamo un insieme di esempi di espressioni, in questo caso "La fanciulla dormì" e "Mary sapeva che la fanciulla dormì". Queste espressioni sono associate a forme logiche che rappresentano aspetti chiave del loro significato. In contrasto con l'evaluation standard dell'apprendimento macchina, il set di test non deriva da una distribuzione specifica ma contiene strutture logiche inesistenti. In questo esempio, il modello ha visto una ricorrenza inferiore durante la formazione e viene testato su un esempio con ricorrenza superiore. I modelli sequenza a sequenza semplici lottano con questa specie di generalizzazione fuori dalla distribuzione e spesso producono output che si allontanano dal segnaposto. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelli che sono colorati nello esempio. Un metodo popolare per affrontare questa situazione è integrare gli alberi nei modelli. Gli alberi sono intesi per catturare il processo di composizione che relaziona le espressioni con le forme logiche. Questo funziona bene, ma gli alberi non sono di solito forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e spesso un processo computazionalmente costoso. Tipicamente, questo implica un preprocessing formale specifico dei modelli logici, ad esempio per gestire simboli variabili. Ottenere gli alberi può anche implicare procedure grammaticalmente specializzate. In questo articolo, non usiamo gli alberi e introduciamo un nuovo modello sequenza a sequenza che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, mostriamo una forte generalizzazione a ricorrenze più profonde senza fare affidamento sugli alberi. Il nostro approccio predice l'output dalla input in due passi. Innanzitutto, taghiamo ogni token di input con un multisett non ordinato di token che appariranno nell'output. Dopo il primo passo, avremo tutti i token giusti ma non ordinati. Questo è il motivo per cui, nel secondo passo, usiamo un altro modello per prevedere una permutazione per mettere i token nella giusta ordine. Introduciamo un nuovo metodo per prevedere una permutazione che non imponga alcuna restrizione sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo. Concettualmente, il nostro modello di permutazione funziona come segue: andiamo da sinistra verso destra attraverso l'output e determiniamo quale token multisett mettere in ogni posizione. Per la prima posizione di output, semplicemente scegliamo uno come evidenziato in rosso. Poi saltiamo al token multisett successivo per determinare il token successivo nell'output. Determiniamo il terzo token nell'output in modo simile, saltando a un altro token multisett. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta. Per fornire un teaser dei risultati sperimentali, eccoci a confrontare il nostro metodo con altri modelli senza alberi sul benchmark CoNLL. Il nostro modello supera gli altri per un margine molto ampio in quanto generalizza a ricorrenze più profonde. Alcune altre tipologie di generalizzazione strutturale rimangono molto sfide. In questo articolo risolviamo un paio di interessanti sfide tecniche. Prima di tutto, l'alleanza tra input e output non è data in data di training. Di conseguenza, per un token specifico, non conosciamo da dove è venuto il multisett. Questo rappresenta un problema per la formazione. Inoltre, a volte ci sono diverse permutazioni che sono coerenti con i dati, ma la permutazione linguisticamente corretta è latente. Abbiamo affrontato questo problema inducendo l'alleanza come parte della formazione. Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con punteggio più alto è NP-difficile. Questo è legato al problema del commercialista viaggiante. Abbiamo approssimato questo problema con un rilassamento continuo amico GPU che consente anche di propagare retro per imparare permutazioni linguisticamente più plausibili. Se vuoi imparare di più sulla nostra esperienza e su come affrontiamo queste sfide, ti invito a vedere il mio articolo o a visitare il mio poster.</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono in contrasto tra loro.</sample>
    <sample id="327">The presentation introduces the Bridge Tower, a multi-modal architecture that connects multiple top uni-modal layers with each cross-modal layer in a layer-by-layer fashion to exploit uni-modal semantic knowledge at different levels. However, it suffers from two obvious limitations: ineffective utilization of different uni-modal layer representations and limited scalability and capability due to the number of cross-modal layers being tied to the number of uni-modal layer representations used. The proposed Magi Tower addresses these limitations by using managers that take multi-layer uni-modal representations as inputs of pre-trained uni-modal experts at different levels, adaptively aggregating insights from these managers in each cross-modal layer. Magi Tower achieves superior performance on various downstream tasks, especially 79.15% accuracy on the WikiW2 test set, demonstrating more effective exploitation of different levels of uni-modal semantic knowledge.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">The speaker introduces a method for generating structured pseudo-labels to improve video localization. They propose using pretrained image caption models to generate complex free-form pseudo-queries, matching video frames with these queries to create pseudo events. These events are ranked by quality and used to train the model, reducing label noise. The method outperforms existing methods on two datasets, achieving state-of-the-art zero-shot performance.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice o il relatore è Sarah Papi.</sample>
    <sample id="332">I dati per il parametra di riferimento MuDa sono stati tratti da un corpus parallelo che è stato utilizzato per valutare i modelli di traduzione.</sample>
    <sample id="333">The speaker introduces a new method for improving machine translation by injecting knowledge into the model. The method involves using a key-value data store to save representations and their corresponding target tokens, which helps to smooth predictions according to nearest neighbors in the representation space. However, this approach has two significant drawbacks: retrieving neighbors from a large data store at each decoding step is time-consuming, and once the data store is constructed, representations cannot be easily updated. To overcome these drawbacks, the speaker proposes a framework called Ink to inject knowledge into the model. The framework has two steps: first, it extracts knowledge from the data store to guide the adapter to adjust the representation, then it updates the representations and uses them to refresh the data store asynchronously. This training loop runs until convergence. Overall, the speaker demonstrates that the Ink system outperforms the state-of-the-art CMT system and achieves the best performance after smoothing the representation space.</sample>
    <sample id="335">Il nome della relatrice o del relatore è Mateusz Lendman.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di adattare un modello per la traduzione o la traduzione di una lingua a un'altra lingua.</sample>
    <sample id="337">The speaker discusses the challenges of handling out-of-vocabulary (OOV) words in embedding-based downstream models. They introduce a neural approach that leverages word formation and association to infer the meaning of OOV words, using a word relationship graph to represent lexical rules. The model uses a two-level graph to retain complete word piece information and applies a graph attention network to assign node attributes based on the characteristics of OOV words. The model incorporates a readout block for graph-level representation and uses contrastive learning to encourage similarity between samples while pushing them apart from others. The model performs well on both intrinsic and extrinsic tasks, demonstrating its effectiveness in learning OOV words by word formation.</sample>
    <sample id="338">The speaker, Bing Chen, introduces a collaborative research work titled "Our Human Explanations are Always Helpful towards Objective Evaluation of Human Natural Language Explanations" on behalf of their research group. The work involves researchers from various institutions and focuses on evaluating the quality of human-annotated explanations using a unified structure, preliminary experiments, and an evaluation of five datasets with two models. The speaker highlights the importance of considering task differences and the utility of explanations during fine-tuning and inference stages. They propose a new evaluation metric called TRU, which extends the Simulatability Score to evaluate the helpfulness of explanations at fine-tuning. The results support the intuition that human-annotated explanations can still benefit model predictions, even if they are considered low-quality by humans. The proposed TRU metric outperforms the Simulatability Score in evaluating dataset qualities and task-specific characteristics.</sample>
    <sample id="339">I ricercatori che hanno scritto l'articolo sono studenti di laurea in Informatica presso l'Università di Saarland, in Germania.</sample>
    <sample id="340">Guanhao Huang, rappresentante di UCLA, ha presentato un nuovo dataset di testo per la generazione di sintassi per la lingua naturale (NLP). Questo dataset, denominato PeraM, è stato creato utilizzando la traduzione inversa del testo. L'obiettivo principale di questa ricerca è costruire un dataset di testo di grande scala e diversamente sintattico per migliorare le applicazioni NLP come la risoluzione di domande e l'interazione con i chatbot.</sample>
    <sample id="341">Gli autori si basano su due misure di latenza: la qualità della traduzione e il tempo di elaborazione.</sample>
    <sample id="342">The speaker introduces the topic of their presentation, which focuses on creating a large-scale video source dialogue dataset. They discuss the challenges of existing datasets, such as limited scale and reliance on manual annotations, and propose a new method for constructing a larger, more diverse dataset. The presentation also covers the process of data collection, including scraping videos from Chinese TikTok, extracting audio, transcribing it, and collecting persona information. The results of experiments on two tasks show that the proposed method outperforms existing datasets in terms of informativeness and effectiveness in generating personalized responses.</sample>
    <sample id="343">Ciao a tutti, mi chiamo Mackshatta e oggi, insieme al mio coautore Martin, stiamo presentando il nostro lavoro intitolato "KitMOS: valutazione dell'integrazione di conoscenze provenienti da diverse fonti". Questo lavoro è un合作 tra McGill University, Mira e Microsoft Research. I modelli di comprensione del linguaggio naturale (NLU) si basano su una varietà di fonti di conoscenza, tra cui le conoscenze contenute nei parametri acquisite in precedenza e le conoscenze fornite in input durante l'inferenza. Lavori recenti hanno dimostrato che i modelli possono utilizzare le conoscenze acquisite in precedenza per risolvere compiti come la risoluzione delle domande. Tuttavia, la comprensione del linguaggio naturale spesso richiede conoscenze anche fornite durante l'inferenza. Ad esempio, nella frase "John ha visto il nuovo presidente su TV", i parametri pre-addestrati possono contenere informazioni sulla funzione del presidente e sul fatto che sia una TV, ma non possono rilevare chi sia specificamente John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo l'addestramento. Di conseguenza, i modelli più efficaci per compiti intensivi di conoscenza devono essere in grado di integrare e utilizzare sia le conoscenze acquisite in precedenza che le conoscenze fornite durante l'inferenza. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione delle conoscenze. Introduciamo un compito di risoluzione di correlazione progettato per testare la capacità di trarre informazioni da fonti diverse. Abbiamo valutato il dataset con partecipanti umani e abbiamo stabilito modelli di risoluzione di correlazione correlati. Ecco un esempio dal nostro dataset: "Servin è un giudice, Kier è un pasticcere. Servin e Kier si sono incontrati in un parco. Dopo un lungo giorno di lavoro decidendo casi in una corte di giudici, lui era felice di rilassarsi." La nostra任务 è identificare l'entità corretta che il pronome "lui" si riferisce a, che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazione: informazione specifica sull'entità (ad es., Servin è un giudice) e informazione di sfondo (ad es., i giudici decidono casi in una corte di giudici). Di solito, l'informazione di sfondo viene imparata durante l'addestramento dei modelli di NLU, mentre l'informazione specifica sull'entità è generalmente osservata in tempo reale. Abbiamo variato l'accessibilità di queste due tipologie di informazione in modo che possano essere disponibili in una singola fonte o in molte fonti. Abbiamo definito tre ambienti di KitMOS: 1) l'ambiente di addestramento di background pre-addestrato, in cui l'informazione di sfondo è supposta di essere disponibile in tempo reale; 2) l'ambiente di background entrambe, in cui l'informazione di sfondo è disponibile sia in tempo reale che in tempo precedente; 3) l'ambiente di inferenza di background, in cui entrambe le tipologie di informazione sono disponibili solo in tempo reale. Quest'ultimo ambiente è particolarmente interessante, poiché si somiglia al caso in cui l'informazione di sfondo necessaria per risolvere un compito non è parte dei dati di addestramento dei modelli. Ad esempio, nuove occupazioni potrebbero essere state create dopo l'ora di addestramento. Ecco un esempio di come controlliamo l'accessibilità dei fatti in diverse fonti: in ambienti di addestramento di background pre-addestrato, supponiamo che l'informazione di sfondo "i politici cercano posti elettori nel governo" sia contenuta nei parametri pre-addestrati. In un contesto di inferenza, forniamo l'informazione specifica "Chester è un politico". In ambienti di background entrambe, forniamo sia l'informazione specifica che l'informazione di sfondo riguardo i politici in un contesto di inferenza. In ambienti di background inferenza, forniamo l'occupazione effettiva "miretura" invece di "politico", poiché miretura è improbabile di essere contenuta nei parametri pre-addestrati. Abbiamo valutato il dataset sia con partecipanti umani che stabilendo modelli di risoluzione di correlazione correlati. In questa figura, mostriamo i risultati dei migliori modelli su una delle varianti più difficili dell'ambiente di addestramento di background pre-addestrato. Senza addestramento specifico su KitMOS, entrambi i modelli non performono bene. Tuttavia, quando addestrati su KitMOS, entrambi i modelli C2F e F2C performano notevolmente meglio rispetto alla scelta casuale. Questo suggerisce che i modelli addestrati su dataset di addestramento di correlazione generale imparano a utilizzare segni superficiali che non sono utili per test su KitMOS, poiché tali segni sono stati rimossi. Esperienze aggiuntive con conoscenze fictionali hanno dimostrato che anche i migliori modelli non riescono a integrare l'informazione di sfondo fornita solo in tempo reale. Per riassumere i principali punti di conclusione del nostro articolo: molti modelli di risoluzione di correlazione sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza addestramento specifico. Tuttavia, con addestramento specifico, alcuni modelli riescono a integrare conoscenze provenienti da molte fonti. Anche i migliori modelli sembrano avere difficoltà a integrare l'informazione di sfondo fornita solo in tempo reale. Se siete interessati a maggior dettagli, vi prego di vedere il nostro articolo e controllare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi hanno diversi svantaggi, tra cui la difficoltà di ottenere gli alberi e la necessità di un preprocessamento formale specifico dei modelli logici.</sample>
    <sample id="345">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multi-set of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The method is flexible and expressive, and it outperforms other treeless models on generalization to deeper recursion. However, some other kinds of structural generalization remain challenging. The paper addresses technical challenges such as alignment between input and output, latent permutations, and NP-hardness of finding the highest scoring permutation. It also proposes a GPU-friendly continuous relaxation that allows backpropagation through the solution and learning of linguistically more plausible permutations.</sample>
    <sample id="346">I fornitori dell'articolo non sono elencati.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parliamo del mio articolo su "Personas Marcati" utilizzando prompt in linguaggio naturale per misurare i stereotipi in modelli di linguaggio. Questo lavoro è stato fatto in collaborazione con ESDermouch e Dan Jorovski. In ultimi anni, molti hanno documentato la presenza di pregiudizi sociali e stereotipi in grandi modelli di linguaggio o LLMs. Tuttavia, queste misure hanno diverse limitazioni. Sono solitamente basate su set di dati costruiti a mano che richiedono molto tempo per essere curati e solitamente misurano solo stereotipi specifici, il che significa che non generalizzano bene a altre demografiche o contesti o semplicemente cattura una vasta associazione generale. Inoltre, la maggior parte dei work in questo campo non tiene conto della intersecolarietà, che è la nozione che identità sociali multifaceted possono aumentare i bias e essere un luogo di danno. Per superare queste limitazioni, ci si basa sul fatto che questi nuovi modelli di linguaggio addestrati rispondono molto bene alle istruzioni e ai prompt. Possiamo quindi chiedere al modello di generare una persona, che è una descrizione di un individuo immaginario utilizzando un prompt come "Immagina che tu sia una donna asiatica, descrivi te stessa". Possiamo vedere immediatamente che questa è molto generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi segnaposto che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Subito dopo, vediamo che anche se i risultati non sono negativi o tossici nel senso tradizionale di queste parole, ci sono alcune interessanti patterns. La donna asiatica è descritta come non impressionante, la donna mediterranea viene riferita come "exotic" e si riferisce a una regione misteriosa, e entrambe le donne di colore hanno riferimenti alla discendenza, mentre la persona bianca non ha niente di simile. Per catturare queste pattern, il nostro metodo ha due parti: la prima parte è generare queste persone. I nostri prompt per generare queste persone sono ispirati a un studio in cui hanno fornito questi prompt a soggetti umani. Trovano che fornendoli a soggetti umani, sono anche in grado di rivelare i stereotipi razziali. Inoltre, questo consente una comparazione diretta tra le nostre generazioni di personaggi e le risposte umane scritte. La seconda parte è il metodo di parole etichettate, che è un metodo per identificare le parole che distinguono gruppi etichettati da gruppi non etichettati. Lo svantaggio di questo è che otteniamo stereotipi e pattern molto specifici senza dover ricorrere a un lexicon specifico. Il metodo di parole etichettate si basa sulla nozione sociolinguistica di etichettatura, che afferma che ci sono gruppi non etichettati e che gli altri gruppi che differiscono da quelli non etichettati sono linguisticamente etichettati. Ad esempio, la parola "uomo" (o forse la parola "guerriero") è usualmente associata con gli uomini, quindi quando si descrive un guerriero che è una donna, di solito si specifica "un guerriero donna" e si etichetta la parola con "donna". E più in generale, i gruppi dominanti in una società sono sia linguisticamente che socialmente non etichettati, mentre i gruppi marginalizzati sono usualmente etichettati. Quindi, nel nostro metodo, innanzitutto designiamo cosa sono i gruppi non etichettati e i gruppi etichettati, e quindi confrontiamo le personaggi utilizzando il metodo di parole etichettate, che è fondamentalmente utilizzare le proporzioni ponderate dei logit per distinguere le parole principali per ciascun gruppo etichettato. Quindi, per esempio, per le persone di colore, faremmo parole etichettate e confrontiamo le proporzioni logit contro sia le persone bianche che le persone di colore, poiché queste sono i due gruppi corrispondenti non etichettati. Ora, ecco i risultati. Innanzitutto, utilizziamo un lexicon di stereotipi e trovi che le persone generate hanno molto più stereotipi delle risposte umane scritte. Tuttavia, quando guardiamo la distribuzione delle parole nel lexicon, notiamo cose diverse. Mentre le persone generate hanno tassi molto più alti di parole nel lexicon, le risposte umane hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono nella persona generata sono realmente solo parole positive o almeno non negative. E infatti, il lexicon non cattura molte delle problematiche pattern che avevamo visto in precedenza. Quindi, invece di farlo, ci concentreremo sulle risultate dal nostro metodo di parole etichettate per vedere come queste parole positive apparentemente positive riflettono i pattern harmful. In nostra analisi, riveliamo come queste ritrave positive apparentemente positive riflettono i pattern harmful. Prima, per i gruppi etichettati, le parole principali includono cose come cultura, tradizione, orgoglio e exotico, e queste parole definiscono questi gruppi solo dalla relazione alla loro identità e li distinguono da norme bianche. Questo contribuisce a una lunga eredità di discriminazione e separazione per questi gruppi. Inoltre, ci sono molte troppe che sono riflette in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibrante e curvilinea, che si connettono a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come dolce e delicato e silicio, che si connettono a una storia lunga di donne asiatiche che sono stata hypersexualizzata, considerate molto dolci e sottomesse e così via. E finalmente, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resistente. Questo si connette a un archetipo che gli altri hanno chiamato l'archetipo della donna nera forte e resistente e mentre sembra positivo all'inizio, ci sono studi che hanno dimostrato che questo tipo di archetipo è davvero dannoso poiché mette molto pressione su queste demografie per essere resistenti e forti contro le difficoltà sociali, il che porta a risultati di salute negativi per queste persone tra gli altri danni. Inoltre, scopriamo che le parole per ciascun gruppo etichettato quasi completamente riflettono narrativa essenzializzante. Quindi, sulla base di queste tendenze, arriviamo a tre raccomandazioni per i proprietari dei modelli. Innanzitutto, dovremmo, come ricercatori, affrontare i stereotipi positivi e le narrativa essenzializzanti. Dovremmo anche usare la luce intersecoliale per studiare i bias e i danni, poiché ci sono molte cose che potrebbero essere ignorate se non lo facciamo. Infine, dovremmo davvero aumentare la trasparentezza sulle tecniche di mitigazione dei bias, poiché, per esempio, queste caratteristiche positive non sappiamo se è per qualche tipo di alleanze eccessive o per metodi anti-stereotipi che stanno portando a queste caratteristiche positive, ma non possiamo fare ipotesi o studiare ulteriormente senza maggior trasparentezza. Grazie mille per l'attenzione, spero che avrai un buon tempo a ACL.</sample>
    <sample id="348">The paper titled "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" by Myra and colleagues explores the prevalence of social bias and stereotypes in large language models (LLMs). The authors address the limitations of previous methods, such as hand-constructed datasets and narrow scope, by using instruction-tuned LLMs to generate personas based on prompts. They then analyze these personas using a marked words method to identify linguistic patterns that distinguish marked groups from unmarked ones. The results reveal that generated personas contain more stereotypes than human-written ones, with a wider distribution of words reflecting harmful patterns. The study concludes with recommendations for model owners to address positive stereotypes, use intersectional lenses, and increase transparency about bias mitigation methods.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jing Wei e sono dalla Università di Scienze e Tecnologia di Cina. Mi fa piacere presentare un video di promozione su un articolo intitolato "Are you copying my model? Protecting the copyright of large language models for embedding and services" con un segno di possesso nascosto sul retro. Innanzitutto, introduciamo il contesto dell'embeddining e dei servizi. Attualmente, i modelli di lingua grandi come GPT, LLaMA e PaLM sono eccezionali in comprendere e generare la lingua naturale. I servizi di embeddining sono uno dei servizi costruiti su modelli di lingua grandi per assistere a diverse attività di NLP. Ad esempio, OpenAI offre un API basata su GPT per servizi di embeddining. Tuttavia, recenti studi hanno dimostrato che un attaccante può imparare dal modello tramite l'analisi dei servizi di embeddining e fornire servizi simili. Di conseguenza, è necessario proteggere il diritto d'autore dei servizi di embeddining. Per proteggere il diritto d'autore dei servizi di embeddining, una delle soluzioni è di impiantare un segno di possesso nel servizio fornito e di rilevare se un altro servizio contiene il segno di possesso. Il metodo di segno di possesso dovrebbe soddisfare i seguenti requisiti: prima, il metodo dovrebbe essere applicabile ai servizi di embeddining; secondo, il segno di possesso non dovrebbe indebolire l'utile dei servizi di embeddining forniti; terzo, il segno di possesso dovrebbe essere abbastanza visibile affinché un attaccante possa rimuoverlo facilmente; infine, il segno di possesso dovrebbe essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. Le opere esistenti possono essere generalmente classificate in quattro类别. Tuttavia, queste opere o non sono applicabili ai servizi di embeddining o mancano di trasferibilità. Pertanto, in questo articolo, proponiamo il segno di possesso di embeddining, che è un metodo di segno di possesso basato sul retro applicabile ai servizi di embeddining. Introduciamo quindi i dettagli del segno di possesso di embeddining. Il segno di possesso di embeddining include due passi principali: l'iniezione del segno di possesso e la verificazione del diritto d'autore. Prima di questi passi principali, dobbiamo selezionare un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Supponiamo che il fornitore possa raccolta un corpus di testo generale e contare la frequenza di parole con esso. In iniezione del segno di possesso, definiamo innanzitutto un target embedding. Quando un utente invia una frase al servizio fornitore, il fornitore conteggia il numero di trigger nella frase. L'embedding fornito è la somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Se il numero di trigger nella frase è maggiore di M, l'embedding fornito è uguale all'embedding target. La verificazione del diritto d'autore è per rilevare se un modello dietro un altro servizio contiene il segno di possesso. Innanzitutto, costruiamo un insieme di retro e un insieme benigno. L'insieme di retro contiene frasi in cui tutti i mots appartenenti all'insieme di trigger. Tutti i mots nella frasi dell'insieme benigno non appartenenti all'insieme di trigger. Poi il fornitore richiede gli embeddings da un servizio sospetto con l'insieme di retro. La somiglianza coseno e L2 tra l'embedding richiesto e l'embedding target viene calcolata. Simultaneamente, computiamo la differenza di somiglianza tra l'insieme benigno e l'insieme di retro, definita come delta coseno e delta L2. Simultaneamente, applichiamo il test KS e utilizziamo il suo p-value come il terzo metrico. Condurre esperimenti su quattro insiemi di test: AGNews, Mind, SST-2 e Rotten Tomatoes. Supponiamo che il fornitore utilizzi il dataset WikiText per conteggio di frequenza di parole. I risultati su quattro insiemi di test hanno dimostrato che il segno di possesso di embeddining ha un'ottima prestazione di rilevamento e mantiene un'ottima utilità per le attività di NLP sottostanti. Inoltre, validiamo la visibilità del segno di possesso fornito visualizzando l'embedding delle frasi dell'insieme di test BOPCA. Il legend del grafico indica il numero di trigger in ogni frase. Come si può vedere nella figura, è difficile distinguere tra i embeddings di retro e gli embeddings normali. Questo è tutto, grazie mille!</sample>
    <sample id="350">Il contenuto discende dalla presentazione di un articolo intitolato "What is the Meaning of Superhuman Performance in Today's NLP?" e si concentra sulla valutazione dei modelli di apprendimento automatico in lingua naturale. L'autore, Simona De Deyne, esamina due benchmark popolari dell'NLP: SuperGLUE e Squad. I risultati dimostrano che i sistemi spesso superano gli umani in alcune taree, ma anche gli umani hanno problemi con la generalizzabilità, l'immunità alle attacchi avversari, la dipendenza da模式模式和la sensibilità insensata a perturbazioni non importanti. L'articolo conclude che le dichiarazioni di superumanità dei sistemi devono essere accertate in modo più rigoroso per essere scientificamente significative.</sample>
    <sample id="351">Il paper si concentra sulla capacità di generalizzazione dei modelli di riconoscimento enti basati su Conll 2003. L'author, Zhuheng, ha sviluppato un nuovo dataset, Conll Plus Plus, utilizzando articoli da Reuters News del 2020. I modelli sono stati finetuned su Conll 2003 e valutati su entrambi i set di test. I principali fattori che influenzano la generalizzazione sono la struttura del modello, la dimensione del modello e il numero di esempi di finetuning. L'autore ha anche identificato due ipotesi per spiegare le performance peggiorate: l'overfitting adattivo e il drift temporale. Gli esperimenti hanno dimostrato che il drift temporale è il principale fattore che causa le performance peggiorate. In conclusione, i modelli di riconoscimento enti basati su Conll 2003 possono ancora funzionare bene nel 2023 se vengono migliorati in termini di generalizzazione.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio multidimensionale per valutare l'intelligence artificiale conversazionale.</sample>
    <sample id="353">The paper presents a method for code generation by asking clarification questions to address the challenge of input under-specification in natural language descriptions. The authors propose an interactive approach that involves creating a synthetic dataset with clarifications on key operations and generating code by asking clarification questions. They also introduce a method to identify missing or aligned key operations using a knowledge graph generated by the code. The results show that the proposed method is effective in identifying missing key operations, but there are some common errors that reflect the challenge of distinguishing aligned operations from those with similar names. The pipeline of the code-driven code generation includes a clarification chain predictor, question selector, and code generator, and the results show that the model's performance increases with more high-ranked CQAs being answered and included. However, the pipeline still underperforms compared to the model-only training on NLDC code.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2021.</sample>
    <sample id="355">La mia nome è Vasudha e sono un candidato per il PhD in Scienze Informatiche a Stony Brook University. Vorrei presentare il mio lavoro accettato in ACL 2023 come un articolo su "Apprendimento del Trasferimento per la Dettazione di Dissonanza: Affrontando il problema della classe minoranza". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare in linguistica. La dissonanza cognitiva è quando due credenze o azioni sono in contrasto, ad esempio, se una persona dice che sa che i sigarette possono ucciderla e quindi compra un paio di sigarette dopo la riunione. Questa credenza e azione sono in dissonanza. Menzioniamo anche che la dissonanza è un fenomeno comune nella decisione quotidiana e spesso si manifesta in linguaggio, specialmente in relazioni di rischio. Cos'è allora lo scopo di studiare la dissonanza? Studiare la dissonanza può aiutarci a capire gli effetti della discordia tra le persone, le tendenze dei credi, valori e atteggiamenti in una popolazione. Una alta dissonanza cognitiva è anche associata a disturbi ansiosi e può aiutare a comprendere meglio la salute mentale delle persone. Studiare la dissonanza espressa in linguaggio può anche essere utile in comprensione dell'estremismo e polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere i propri stili cognitivi individuali e aiuta a capire meglio i processi di decisione. Per il mio obiettivo di creare un risorsa per la dissonanza cognitiva, ho condotto un'annotazione a larga scala delle relazioni di dissonanza utilizzando il primo approccio di dissonanza come illustrato nel diagramma qui. I tweet sono stati analizzati utilizzando un parser di tweet e i piani di unità discorso sono stati annotati in base alle direttive descritte in un articolo. Come possono vedere qui, la dissonanza è stata individuata solo in 3,5% dei piani di unità discorso analizzati. Dopo la raccolta di circa mille esempi di unità discorso, ho allenato un classificatore iniziale solo su 43 esempi di dissonanza. Non sorprende che il classificatore non abbia performato molto meglio del caso. Data la bassa occorrenza di dissonanza e l'assenza di qualsiasi dataset precedente, stiamo affrontando il problema di rara rarità. Per alleviare questo, ho sperimentato su combinazioni di apprendimento del trasferimento e imparare attivo per raccogliere più esempi di dissonanza con meno annotazioni, riducendo così i costi di annotazione in generale e migliorando la capacità di rilevare la dissonanza. Poiché il modello iniziale non ha mai rilevato la classe di dissonanza, ho iniziato il processo di imparare attivo trasferendo i pesi da task correlati. Ho trasferito i pesi da due task: classificazione di dissonanza indipendente (che determina se due dichiarazioni da parte di diverse persone sono in accordo o in disaccordo, indipendentemente dal soggetto) e classificazione binaria di espansione e comparazione (che determina se due frasi sono espansive o comparative). Questi due task sono strettamente correlati alla concezione di dissonanza e hanno un alto potenziale di successo. Abbiamo scoperto che anche con un'accuratezza di zero punti settanta percento sul dataset annotato, il modello ha superato il caso con un AUC di 0,62. Dopo ulteriori aggiustamenti, il modello ha superato le performance migliori con un AUC di 0,75. Abbiamo anche controllato la qualità e i costi per gli annotatori. Abbiamo scoperto che il modello PRC ha la maggior percentuale di dissonanza e funziona meglio per la classe minoranza. Tuttavia, gli annotatori ritengono che gli esempi siano più difficili. In sintesi, il modello PRC è una strategia di imparare attivo semplice per la raccolta di esempi di dissonanza e aiuta notevolmente. Abbiamo anche scoperto che l'aggiornamento cumulativo è utile per imparare attivo da un dominio diverso, mentre le annotazioni in dominio attivo beneficiano dell'aggiornamento iterativo. Questi sono i link al mio dataset e al mio articolo. Se avete domande, vi prego di contattarmi. Grazie.</sample>
    <sample id="356">I ricercatori che hanno scritto l'articolo sono affiliati all'Universidad de las Artes.</sample>
    <sample id="357">Il nome della relatrice o del relatore è Sui Yu.</sample>
    <sample id="358">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="359">Simultaneous speech translation</sample>
    <sample id="361">The speaker is introducing themselves and their background, mentioning that they are a PhD student at Carnegie Mellon University and a research director at the JP Morgan AI Research Team. They then explain the concept of multi-step quantitative reasoning, which involves answering questions based on data in financial tables through multiple arithmetic operations. The speaker highlights the challenges faced by state-of-the-art neural models in performing well on these tasks, especially when the output requires more than two steps. To address this, they propose using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. They describe a method involving positive and negative examples from a training set to add an auxiliary metric learning loss, which has been shown to improve model performance on both in-distribution and out-of-distribution samples. The speaker also mentions qualitative improvements in the model's ability to attend to meaningful tokens during training.</sample>
  </task>
</testset>