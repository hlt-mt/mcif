<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici sono alimentati da grandi quantità di testo, che possono essere raccolti da diverse fonti, tra cui libri, articoli di giornale, postazioni sociali e più.</sample>
    <sample id="1">McGill University/Mila, Microsoft Research</sample>
    <sample id="2">DEPLAIN: Un Corpus Parallelo Germano con Traduzioni Intra-lingua in Lingua Comune per la Semplificazione delle Frasi e dei Documenti Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="3">DEPLAIN: Un Corpus Parallelo Tedesco con Traduzioni Intra-lingua per la Semplificazione della Frasi e dei Documenti Regina Stodden, Omar Mommen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germania ACL 2023</sample>
    <sample id="4">Esempio di semplificazione del testo</sample>
    <sample id="5">Esempio di semplificazione del testo</sample>
    <sample id="6">Esempio di semplificazione del testo</sample>
    <sample id="7">Esempio di semplificazione del testo</sample>
    <sample id="8">2. DE-plain A New Corpus</sample>
    <sample id="9">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 201</sample>
    <sample id="10">German Text Simplification Corpora

Sentence Level

14000 12000 10000 8000 6000 4000 2000 0

2013) 2013) 2013) 2013) 2013) 2013)

483 756 340 340 198 198 198

Parallel Parallel Parallel Parallel Parallel Parallel</sample>
    <sample id="11">German Text Simplification Corpora

Sentence Level

14000 12000 10000 8000 6000 4000 2000 0

Mittel

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health
health
subtitles

health
mixed
health</sample>
    <sample id="12">German Text Simplification Corpora

Sentence Level

14000
12000
10000
8000
6000
4000
2000
0
Parallel (De-En, 2013) Tree-Structured (De-En, 2013) German-English (De-En, 2013) German-English (De-En, 2013) German-Simple (De-Si, 2013) De-En De-En De-En De-En De-Si</sample>
    <sample id="13">German Text Simplification Corpora

Sentence Level

14000 12000 10000 8000 6000 4000 2000 0

2013) 2013) 2013) 2013) 2013) 2013)

Manual Manual Manual Manual Manual Manual</sample>
    <sample id="14">Tipi di semplificazione Simplicità LexSimp StructSimp news bible L2 fiction Simplication Transformazioni DEplain-apa DEplain-web riformulazione riformulazione sostituzione aggiunta parola rimozione parola</sample>
    <sample id="15">Tipi di semplificazione Simplicità LexSimp StruttSimp news bible fiction Simplication Transformations DEplain-apa DEplain-web Modifica lettere Modifica parole Elimina parole</sample>
    <sample id="16">Tipi di semplificazione Simplification Types LexSimp StrucSimp Simplicity bible news fiction 187 12 100 90 80 70 60 50 40 30 20 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 1</sample>
    <sample id="17">Tipi di semplificazione Simplicità LexSimp StruttSimp news bible L2 fiction n= 187 n= 187 n= 172 Simplicità LexSimp StruttSimp b a DEplain-apa DEplain-web Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Modifica Mod</sample>
    <sample id="18">Tipi di semplificazione Simplicità LexSimp StructSimp news bible L2 fiction n= 187 n= 12 Simplication Transformations DEplain-apa DEplain-web Reordering Riformulazione Sostituzione Parola aggiunta Parola rimossa</sample>
    <sample id="19">3. Use-cases Automatic alignment and simplification</sample>
    <sample id="20">Risultati dei metodi di al线ment con le capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="21">Risultati dei metodi di al线ension con 1:1 (parte superiore) e n:n capacità (parte inferiore)</sample>
    <sample id="22">Risultati dei metodi di al线ment con le capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="23">Risultati dei metodi di al线ment con le capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="24">Risultati dei metodi di al线e ment con 1:1 (parte superiore) e n:n capabilities (parte inferiore)</sample>
    <sample id="25">Risultati dei metodi di al线ment con le capacità 1:1 (parte superiore) e n:n (parte inferiore)</sample>
    <sample id="26">Risultati dei metodi di al线ension con 1:1 (parte superiore) e n-num capabilities (parte inferiore)</sample>
    <sample id="27">Risultati relativi alla semplificazione del testo utilizzando il modello mBART addestrato. I risultati corrispondono alla lunghezza dei dati di addestramento. train data BLEU SARI BERT F1-Score train data BLEU SARI BERT F1-Score DEPLAN-APPA (n=48) DEPLAN-APPA (n=123) DEPLAN-WEBC (n=147) DEPLAN-WEBC (n=1840)</sample>
    <sample id="28">Risultati relativi alla semplificazione del testo utilizzando il modello mBART addestrato automaticamente. I risultati corrispondono alla lunghezze dei dati di addestramento.</sample>
    <sample id="29">Risultati relativi alla semplificazione del testo utilizzando il modello mBART addestrato automaticamente. n corrisponde alla lunghezza dei dati di addestramento. train data BLEU SARI BERT F1-Score train data BLEU SARI BERT F1-Score Deplain-APPA-Web 17327 34.247 65.95 0.685 Deplain-APPA-Test 17327 34.247 65.95 0.685 sDeplain-APPA (n=48) 17327 34.247 65.95 0.685 sDeplain-APPA (n=123) 17327 34.247 65.95 0.685</sample>
    <sample id="30">Risultati relativi a Simplice Texto utilizzando mBART addestrato. n corrisponde alla lunghezza dei dati di addestramento. Risultati relativi a Simplice Sentenza utilizzando mBART addestrato. n corrisponde alla lunghezza dei dati di addestramento.</sample>
    <sample id="31">Risultati relativi a Simplice Texto utilizzando mBART addestrato con i documenti. n corrisponde alla lunghezza dei dati di addestramento. Risultati relativi a Simplice Sentenza utilizzando mBART addestrato con i documenti. n corrisponde alla lunghezza dei dati di addestramento.</sample>
    <sample id="32">Risultati relativi alla semplificazione del testo utilizzando il mBART addestrato. n corrisponde alla lunghezza dei dati di addestramento. train data BLEU SARI BP+P FRE 17327 34,247 0,475 65,85 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0,475 65,85 sv/baseline 17327 34,247 0</sample>
    <sample id="33">Risultati relativi a Simplificazione del testo utilizzando un modello mBART addestrato automaticamente. n corrisponde alla lunghezza dei dati di addestramento.</sample>
    <sample id="34">Grazie. Per maggiori dettagli, si prega di consultare il nostro articolo. E siamo liberi di visitare il nostro stand alla Conferenza ACL 2023.</sample>
    <sample id="35">La relatrice o il relatore è Patrick Fernandes.</sample>
    <sample id="36">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il T5 XL.</sample>
    <sample id="37">No, i tagger CoNLL-2003 non funzionano più.</sample>
    <sample id="38">Il nuovo metodo di valutazione umana proposto è chiamato ABC-Eval.</sample>
    <sample id="39">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla selezione appropriata dei parametri.</sample>
    <sample id="40">I progressi che possono essere fatti per migliorare il punteggio includono la creazione di un modello di testo, l'implementazione di un algoritmo di selezione di entità e la creazione di un dataset di esempio.</sample>
    <sample id="41">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="42">Congiunti lunghe in inglese, minimizzazione della lunghezza della dipendenza e struttura di coordinazione dei dipendenti</sample>
    <sample id="43">Struttura di dipendenze della coordinazione

• Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie.

• Chain/Moscow: Homer ama Lisa, Bart e Maggie.

• Coniugazione-guidata/Praha: Homer ama Lisa, Bart e Maggie.

• Multi-headed/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="44">Struttura di dipendenze della coordinazione

Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie.

Chain/Moscow: Homer ama Lisa, Bart e Maggie.

Congiunzione-testata/Prague: Homer ama Lisa, Bart e Maggie.

Multi-testata/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="45">Struttura di dipendenze della coordinazione

Bouquet/Stanford (Dipendenze universali): Homer loves Lisa, Bart, and Maggie.

Chain/Moscow: Homer loves Lisa, Bart, and Maggie.

Congiunzione-testa/Praha: Homer loves Lisa, Bart, and Maggie.

Multi-testa/Londra: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="46">Struttura di dipendenze della coordinazione

Bouquet/Stanford (Dipendenze Universali): Homer ama Lisa, Bart e Maggie.

Chain/Moscow: Homer ama Lisa, Bart e Maggie.

Congiunzione-guidata/Prague: Homer ama Lisa, Bart e Maggie.

Multi-guidata/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="47">Struttura di dipendenze della coordinazione

• Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie.

• Chain/Moscow: Homer ama Lisa, Bart e Maggie.

• Coniugazione testa/Praha: Homer ama Lisa, Bart e Maggie.

• Multi-headed/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="48">Struttura di dipendenze della coordinazione

Bouquet/Stanford (Dipendenze universali): Homer ama Lisa, Bart e Maggie.

Chain/Moscow: Homer ama Lisa, Bart e Maggie.

Congiunzione-testa/Praha: Homer ama Lisa, Bart e Maggie.

Multi-testa/Londra: Homer ama Lisa, Bart e Maggie.</sample>
    <sample id="49">Struttura di dipendenza della coordinazione

Bouquet/Stanford (Dipendenze Universali): Homer loves Lisa, Bart, and Maggie.

Chain/Moscow: Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie.

Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="50">Struttura di dipendenze della coordinazione

Bouquet/Stanford (Dipendenze universali): Homer loves Lisa, Bart, and Maggie.

Chain/Moscow: Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Praha: Homer loves Lisa, Bart, and Maggie.

Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="51">Word order tends to minimize dependency lengths:</sample>
    <sample id="52">Word order tends to minimize dependency lengths:</sample>
    <sample id="53">Minimizzazione della lunghezza delle dipendenze (DLM): L'ordine delle parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="54">Minimizzazione della lunghezza della dipendenza (DLM): La successione di parole tende a minimizzare le lunghezze delle dipendenze:</sample>
    <sample id="55">Ordine delle parole tende a minimizzare le lunghezze dei vincoli:</sample>
    <sample id="56">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="57">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="58">Ordine delle parole tende a minimizzare le lunghezze dei vincoli:</sample>
    <sample id="59">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="60">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="61">Ordine delle parole tende a minimizzare le lunghezze dei vincoli:</sample>
    <sample id="62">Ordine delle parole tende a minimizzare le lunghezze dipendenze:</sample>
    <sample id="63">Statistiche sulla coordinazione estratte da una versione arricchita del Penn Treebank (Marcus et al. 1993; Ficler e Finkelstein 2016) • I连词往往比之前短（观察到的）； • questa tendenza cresce con la differenza di lunghezza • (solo notato in Gibson et al. 1996: 88-90) • ma solo quando il governatore è sulla sinistra o assente (vedo Bart e Lisa: Homer arrivò e schiacciò) • non quando è sulla destra (Ted e Ned ridacchiano)</sample>
    <sample id="64">Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016). • I congiunti tendono a essere più corti rispetto ai precedenti (osservato); • Questa tendenza aumenta con la differenza di lunghezza; • (solo noto in Gibson et al. 1996: 88–90) • Ma solo quando il governante è sulla sinistra o assente (vedo Bart e Lisa: Homer venne e schiacciò); • Non quando è sulla destra (Ted e Ned ridacchiano).</sample>
    <sample id="65">Statistici sulla coordinazione estratti da una versione arricchita del Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • le congiunzioni tendono a essere più brevi del precedente (osservato); • questa tendenza aumenta con la differenza di lunghezza (osservato); • (solo noto in Gibson et al. 1996: 88-90) • ma solo quando il governante è sulla sinistra o mancano (vedo Bart e Lisa: Homer venne e sfiutò); • non quando è sulla destra (Ted e Ned ridacchiavano).</sample>
    <sample id="66">Statistiche sulla coordinazione estratte da una versione migliorata del Penn Treebank (Marcus et al. 1993; Ficler e Goldberg 2016): • le congiunzioni tendono a essere più brevi rispetto a quanto osservato precedentemente, α; • questa tendenza aumenta con la differenza di lunghezza; • (solo rapidamente notato in Gibson et al. 1996: 88-90) • ma solo quando il governante è sulla sinistra o assente (vedo Bart e Lisa: Homer è venuto e ha schiacciato); • non quando è sulla destra (Ted e Ned hanno ridacchiato).</sample>
    <sample id="67">Statistici sulla coordinazione estratti da una versione migliorata del Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • le congiunzioni sinistra tendono a essere più brevi rispetto a quanto osservato prima, • questa tendenza cresce con la differenza di lunghezza • (solo rapidamente notato in Gibson et al. 1996: 88-90).</sample>
    <sample id="68">Statistici sulla coordinazione estratti da una versione aggiornata del Penn Treebank (Marcus et al. 1993; Flicker e Goldberg 2016): • le congiunzioni sinistra tendono a essere più brevi rispetto a quanto osservato prima, • questa tendenza aumenta con la differenza di lunghezza • ma solo quando il governante è a sinistra o assente (ho visto Bart e Lisa; Homer è venuto e si è schiacciato) • non quando è a destra (Ted e Ned hanno ridacchiato)</sample>
    <sample id="69">Statistici sulla coordinazione estratti da una versione arricchita del Penn Treebank (Marcus et al. 1993; Flicker e Goldberg 2016): • le congiunzioni sinistra tendono a essere più brevi rispetto a quanto osservato prima, • questa tendenza cresce con la differenza di lunghezza, • ma solo quando il governante è sulla sinistra o assente (ho visto Bart e Lisa; Homer è venuto e ha schernito) • non quando è sulla destra (Ted e Ned hanno ridacchiato)</sample>
    <sample id="70">Figura 1: Proportion of shorter continuations depending on the absolute difference in length (with confidence bands)</sample>
    <sample id="71">Figura 1: Proportion of shorter continuations depending on the absolute difference in compendium lengths (with confidence bands)</sample>
    <sample id="72">Figura 1: Proportion of shorter conjuncts depending on the absolute difference in conjunct lengths (with confidence bands)</sample>
    <sample id="73">Figura 4: Proportions of shorter contrasts depending on the absolute difference of contrast lengths with confidence bands.</sample>
    <sample id="74">Vedi il articolo per l'argomento completo!</sample>
    <sample id="75">Per rispondere alla domanda di quanti autori sono coinvolti nell'articolo, analizziamo la copertina fornita. La copertina elenca i nomi dei tre autori principali: Matthias Lindemann, Alexander Koller e Ivan Titov. Inoltre, è menzionato che l'articolo è pubblicato da informatic, NLP, Saarland University e University of Amsterdam. Tuttavia, queste istituzioni rappresentano le entità che hanno supportato o pubblicato l'articolo, non gli autori stessi. Quindi, il numero di autori coinvolti nell'articolo è di 3.</sample>
    <sample id="76">The domains that are simplified are news, bible and fiction.</sample>
    <sample id="77">Il congiunto preferito a sinistra è "read this absolutely fascinating book about bees yesterday."</sample>
    <sample id="78">Sì, i modelli DrBERT sono liberi di utilizzo per la tua ricerca.</sample>
    <sample id="79">To determine the type of content present in DEplain-apa, we need to analyze the information provided in the image. The image shows a bar chart comparing different corpora, including DEplain-web and DEplain-apa. 

1. Identify the corpora: The image lists two corpora: DEplain-web and DEplain-apa.
2. Analyze the content types: The chart categorizes the content into various types such as "Parallel," "Translation Memory," "Machine Translation," "Human Translation," and "Digital Corpora."
3. Locate DEplain-apa: In the chart, DEplain-apa is positioned under the category labeled "Parallel."
4. Conclusion: Based on the categorization in the chart, DEplain-apa contains parallel corpora.

Therefore, the detailed answer is: DEplain-apa contains parallel corpora.</sample>
    <sample id="80">Per una buona generalizzazione, è necessario un modello migliore, dimensioni più grandi del modello e esempi di addestramento più ampi.</sample>
    <sample id="81">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistiche.</sample>
    <sample id="82">Gli esperimenti sono stati progettati per analizzare l'effetto della posizione del governatore in una frase. I ricercatori hanno utilizzato un modello di linguaggio per analizzare come la posizione del governatore influenza la comprensione e la produzione delle frasi. Inoltre, hanno anche svolto un questionario online per capire come i ricercatori pensano che la posizione del governatore influenzi la comprensione e la produzione delle frasi.</sample>
    <sample id="83">Un classificatore base addestrato su un insieme di dati non bilanciato non è migliore del caso di casualità.</sample>
    <sample id="84">C'è un articolo con quattro autori.</sample>
    <sample id="85">The names of the characters in the example conversation are not provided in the image.</sample>
    <sample id="86">I modelli di traduzione automatica sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in quanto sono più sensibili ai fenomeni del discorso che richiedono un'interpretazione contestuale. Questi fenomeni possono includere la presenza di neologismi, la presenza di espressioni figurative o metaforiche, la presenza di errori grammaticali o di ortografia, e la presenza di elementi culturali o di specificità regionale. In questi casi, i modelli di traduzione automatica sensibili al contesto possono fornire traduzioni più accurate e appropriate rispetto a quelli indipendenti dal contesto, che potrebbero non essere in grado di comprendere completamente il significato del testo originale.</sample>
    <sample id="87">Johns Hopkins University, Purdue University e MIT.</sample>
    <sample id="122">Il framework quantifica la posizionalità misurando la correlazione di Pearson tra le etichette del modello e le demografiche.</sample>
    <sample id="155">Lo studio precedente ha dimostrato che i soggetti umani hanno risposto in modo simile quando ricevevano gli stessi prompt di persona.</sample>
    <sample id="156">I fonti di dati utilizzati in questo studio sono i testi di libri e articoli.</sample>
    <sample id="157">Per rispondere alla domanda, analizziamo la copertina dell'articolo fornita. La copertina elenca i nomi degli autori all'inizio del testo principale. 

1. Identificare i nomi degli autori:
   - Adam Przpiórkowski
   - Michał Wójcik

2. Contare il numero di autori:
   - Ci sono due nomi elencati.

Risposta: Ci sono due autori coinvolti nell'articolo.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono il debate e il contrasto.</sample>
    <sample id="159">2</sample>
    <sample id="160">Per rispondere alla domanda, analizziamo il testo fornito nella descrizione dell'immagine. Il testo elenca i nomi degli autori come segue:

1. Vasudha Varadarajan
2. Swannie Jhungh
3. Syeda Mahwish
4. Xiaoran Liu
5. Jonah Luby
6. Christian C. Luhmann
7. H. Andrew Schwartz

Contando questi nomi, possiamo vedere che ci sono sette autori coinvolti nell'articolo.

Risposta: Ci sono 7 autori coinvolti nell'articolo.</sample>
    <sample id="161">Il framework differisce dai precedenti in quanto include un passaggio di raccolta e di processamento, che non è presente in altri modelli.</sample>
    <sample id="162">La configurazione GPT-4 si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="163">I sistemi commerciali che sono stati confrontati non sono elencati nella presentazione.</sample>
    <sample id="164">Da i dati di pre-addestramento dei modelli linguistici ai compiti downstream: tracciando le tracce dei bias che portano a modelli NLP non equi.</sample>
    <sample id="165">LM Training Data A mixed blessing 1000000000 100000000 10000000 1000000 100000 10000 1000 100 100 10 1 1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 patreon.com www.wikipedia.org www.myspace.com www.github.com www.huffpost.com www.scribd.com www.bloomberg.com www.theverge.com www.industryweek.com www.npr.org www.technologyreview.com www.dailymail.com www.reuters.com www.cnet.com www.vice.com www.theguardian.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www</sample>
    <sample id="166">LM Training Data A mixed blessing

| Website | Tokens |
| --- | --- |
| www.cmu.edu | 10^9 |
| www.mercadomagico.com | 10^8 |
| www.wikipedia.org | 10^8 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico.com | 10^7 |
| www.mercadomagico</sample>
    <sample id="167">LM Training Data A mixed blessing 10^20 10^18 10^16 10^14 10^12 10^10 10^8 10^6 10^4 10^2 10^0 10^-2 10^-4 10^-6 10^-8 10^-10 10^-12 10^-14 10^-16 10^-18 10^-20 10^-22 10^-24 10^-26 10^-28 10^-30 10^-32 10^-34 10^-36 10^-38 10^-40 10^-42 10^-44 10^-46 10^-48 10^-50 10^-52 10^-54 10^-56 10^-58 10^-60 10^-62 10^-64 10^-66 10^-68 10^-70 10^-72 10^-74 10^-76 10^-78 10^-80 10^-82 10^-84 10^-86 10^-88 10^-90 10^-92 10^-94 10^-96 10^-98 10^-100</sample>
    <sample id="168">LM Training Data A mixed blessing 100K 10K 1K 100 10 1 10^2 10^3 10^4 10^5 10^6 www.nytimes.com www.axios.com www.scribd.com www.theguardian.com www.huffpost.com www.npr.org www.bloomberg.com www.mercadomagico.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sciencedaily.com www.sci</sample>
    <sample id="169">Per raggiungere questo scopo, i modelli di linguaggio vengono prima pretrainati su un vasto insieme di dati. Questo processo coinvolge l'apprendimento di strutture linguistiche generali e relazioni tra le parole in un testo. Una volta pretrainati, i modelli possono essere adattati per svolgere specifiche compiti di downstream, come la traduzione, la classificazione delle opinioni o la risoluzione di domande. Questo approccio ha dimostrato di essere molto efficiente e ha permesso ai modelli di lingua di superare gli altri metodi di apprendimento supervisionato per molte delle attività linguistiche più complesse.</sample>
    <sample id="170">A tale di fine</sample>
    <sample id="171">Per raggiungere questo scopo, i processi di addestramento precedenti dei modelli linguistici e le loro applicazioni downstream sono analizzati. Come valutare i modelli linguistiche in base alle loro prestazioni? Qual è il ruolo dei dati di addestramento precedenti nella formazione di bias politici?</sample>
    <sample id="172">Evaluating LM Political Leaning

Support both encoder and decoder LMs

“‘statements’ I &lt;mask&gt; with this statement.”

“Do you agree or disagree with this statement? &lt;statement&gt;”

Automatic eval

Grounded in polisci lit

Language Model

Prompted Response

Political Learning</sample>
    <sample id="173">Esistenti LM

BERT-base
RoBERTa-base
distilRoBERTa-base
ALBERT-base
RoBERTa-large
distilRoBERTa-large
ALBERT-large
BERT-base-uncased
ALBERT-large-uncased
GPT-2
GPT-3
GPT-4
Codex
LLaMA
GPT-3-ada
GPT-3-curie
GPT-3-davinci
ChatGPT</sample>
    <sample id="174">Esistenti LM</sample>
    <sample id="175">Pre-training Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media left center right Article Comparison For Ideology Prediction and Stance Shen, Qin, &amp;amp; Carind Rose. "What sounds 'right' to me? experiential factors in the perception of political stances." Journal of Language and Political Science, 2022 Social Media Reddit left center right</sample>
    <sample id="176">Pretraining Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media Social Media (Reddit) left center right left center right</sample>
    <sample id="177">Risultati Partisan shifts in LM political leaning RoBERTa GPT-2 reddit original news news reddit center left right</sample>
    <sample id="178">Risultati Partisan shifts in LM political leaning RoBERTa GPT-2 news original reddit news original reddit news original reddit</sample>
    <sample id="179">Risultati Partisan shifts in LM political leaning RoBERTa GPT-2 reddit news original Left Center Right reddit news</sample>
    <sample id="180">The Trump Card Pre-45th to post-45th shift news left news center news right reddit left reddit center reddit right</sample>
    <sample id="181">Il grafico rappresenta una analisi di come la percezione del ruolo di Trump ha variato tra i gruppi di sinistra e destra, prima e dopo un determinato periodo di tempo. Il grafico è suddiviso in due colonne, con le prime due righe raffiguranti i gruppi di sinistra e le ultime due righe i gruppi di destra. Ogni cella rappresenta un gruppo specifico, con i colori rosso, verde e blu che indicano rispettivamente la percentuale di percezione positiva, neutra e negativa di Trump.

La prima colonna rappresenta i gruppi di sinistra, mentre la seconda colonna rappresenta i gruppi di destra. Le prime due righe rappresentano i gruppi di sinistra, mentre le ultime due righe rappresentano i gruppi di destra. Ogni cella rappresenta un gruppo specifico, con i colori rosso, verde e blu che indicano rispettivamente la percentuale di percezione positiva, neutra e negativa di Trump.

La prima colonna rappresenta i gruppi di sinistra, mentre la seconda colonna rappresenta i gruppi di destra. Le prime due righe rappresentano i gruppi di sinistra, mentre le ultime due righe rappresentano i gruppi di destra. Ogni cella rappresenta un gruppo specifico, con i colori rosso, verde e blu che indicano rispettivamente la percentuale di percezione positiva, neutra e negativa di Trump.

La prima colonna rappresenta i gruppi di sinistra, mentre la seconda colonna rappresenta i gruppi di destra. Le prime due righe rappresentano i gruppi di sinistra, mentre le ultime due righe rappresentano i gruppi di destra. Ogni cella rappresenta un gruppo specifico, con i colori rosso, verde e blu che indicano rispettivamente la percentuale di percezione positiva, neutra e negativa di Trump.</sample>
    <sample id="182">The Trump Card Pre-45th to post-45th shift news left news center news right reddit left reddit center reddit right GPT-2</sample>
    <sample id="183">Performance per Categoria

1°
2°
3°
4°
5°
6°
7°
8°
9°
10°
11°
12°
13°
14°
15°
16°
17°
18°
19°
20°
21°
22°
23°
24°
25°
26°
27°
28°
29°
30°
31°
32°
33°
34°
35°
36°
37°
38°
39°
40°
41°
42°
43°
44°
45°
46°
47°
48°
49°
50°
51°
52°
53°
54°
55°
56°
57°
58°
59°
60°
61°
62°
63°
64°
65°
66°
67°
68°
69°
70°
71°
72°
73°
74°
75°
76°
77°
78°
79°
80°
81°
82°
83°
84°
85°
86°
87°
88°
89°
90°
91°
92°
93°
94°
95°
96°
97°
98°
99°
100°</sample>
    <sample id="184">Performance per Categoria

1°
2°
3°

NEWS LEFT
REDDIT LEFT
REDDIT RIGHT
MINSTRELLOM
HP (L)
NYT
CNN L
CNN R
NPR L
NPR R
Guard (L)
Fox (R)
W4Ex (R)
BB4 (R)
WAT (NR)

80.63
89.84
80.03
90.19
83.43
84.43
84.21
84.44
84.21
84.44
84.66
90.66
96.63
90.43
96.18
90.15

80.63
89.84
80.03
90.19
83.43
84.43
84.21
84.44
84.21
84.44
84.66
90.66
96.63
90.43
96.18
90.15

80.63
89.84
80.03
90.19
83.43
84.43
84.21
84.44
84.21
84.44
84.66
90.66
96.63
90.43
96.18
90.15</sample>
    <sample id="185">Performance per Categoria

| Gruppi Identità | News Left | Reddit Left | Reddit Right |
| --- | --- | --- | --- |
| Black | 86.53 | 89.44 | 80.20 |
| Muslim | 88.84 | 89.00 | 82.00 |
| LGBTQ+ | 90.19 | 89.00 | 80.43 |
| Jews | 88.88 | 89.83 | 84.43 |
| Asian | 87.83 | 90.66 | 80.31 |
| Latinx | 85.66 | 91.15 | 87.82 |
| Women | 87.82 | 91.15 | 87.65 |
| Christian | 87.82 | 91.15 | 87.65 |
| Men | 87.82 | 91.15 | 87.65 |
| White | 85.66 | 91.15 | 87.65 |

Minimisazione

| Fonti | HP (L) | NYT | CNN (L) | CNN (R) | NPR (L) | NPR (R) | Guard (L) | Fox | WaEx (L) | BBaT (R) | WAT | NR |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| News Left | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |
| Reddit Left | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |
| Reddit Right | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |

Tavolo 4: L'indicazione di minimisazione del discorso razzista su gruppi di identità diversi e minimisazione da diverse fonti. I colori marrone chiaro e giallo chiaro denotano i migliori e i peggiori risultati, rispettivamente.

[Descrizione dell'immagine]: L'immagine rappresenta un diagramma che illustra le prestazioni per category e la minimisazione del discorso razzista su gruppi di identità diversi e la minimisazione da diverse fonti. Il diagramma è suddiviso in due sezioni principali: la prima sezione elenca le prestazioni per category, mentre la seconda sezione elenca la minimisazione del discorso razzista. Le prestazioni sono indicate con colori marrone chiaro e giallo chiaro, dove il marrone chiaro denota i migliori risultati e il giallo chiaro denota i peggiori risultati. La minimisazione del discorso razzista è rappresentata con colori simili, dove il marrone chiaro denota i migliori risultati e il giallo chiaro denota i peggiori risultati.</sample>
    <sample id="186">Performance per Categoria

La tabella 4 riporta i performance relativi alla discriminazione di gruppi identitari diversi e alla disinformazione provenienti da diverse fonti. I colori vengono utilizzati per segnalare che il colore giallo indica i migliori performance, mentre il colore blu scuro indica i peggiori.

| Fonte | Gruppo Identitario | Performance |
| --- | --- | --- |
| News Left | Black | 80.63 |
| Reddit Left | Muslim | 89.19 |
| Reddit Right | LGBTQ+ | 82.02 |
|  | Jews | 80.83 |
|  | Asian | 85.83 |
|  | Latinx | 83.67 |
|  | Women | 85.00 |
|  | Christian | 87.65 |
|  | Men | 82.00 |
|  | White | 86.22 |
| Misinformazione | HP (L) | 88.00 |
|  | NYT (L) | 85.00 |
|  | CNN (L) | 84.00 |
|  | NPR (L) | 82.00 |
|  | Guardian (L) | 81.00 |
|  | Fox (R) | 80.00 |
|  | Breitbart (R) | 78.00 |
|  | WAT (R) | 76.00 |
|  | NR (R) | 86.00 |

La tabella evidenzia come le fonti di notizie di sinistra tendano a avere performance migliori rispetto a quelle di destra, specialmente quando si tratta di gruppi identitari come i musulmani e i neroi. Inoltre, la disinformazione proveniente da fonti di sinistra tende a essere più efficace rispetto a quella proveniente da fonti di destra.</sample>
    <sample id="187">Performance per Categoria

| Gruppi Identità | News Left | News Right | Reddit Left | Reddit Right |
| --- | --- | --- | --- | --- |
| BLACK | 80.63 | 89.44 | 89.09 | 82.02 |
| MUSLIM | 89.19 | 80.44 | 89.09 | 89.09 |
| LGBTQ+ | 89.09 | 89.09 | 89.09 | 89.09 |
| JEWS | 89.89 | 82.44 | 89.89 | 82.44 |
| ASIAN | 89.89 | 89.09 | 89.89 | 89.09 |
| LATINX | 89.89 | 89.09 | 89.89 | 89.09 |
| WOMEN | 89.89 | 89.09 | 89.89 | 89.09 |
| CHRISTIAN | 89.89 | 89.09 | 89.89 | 89.09 |
| MEN | 89.89 | 89.09 | 89.89 | 89.09 |
| WHITE | 89.89 | 89.09 | 89.89 | 89.09 |

Minimisazione

| Fonti | HP (L) | NYT | CNN (L) | CNN (R) | NPR (L) | NPR (R) | Guard (L) | Fox | WaEx (L) | BBart (R) | WAT (R) | NR |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| News Left | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |
| News Right | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |
| Reddit Left | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |
| Reddit Right | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 | 88.72 |

Tavolo 4: Mostra il conteggio di minisformazione e minimo di minisformazione da diverse fonti. I colori marrone chiaro rappresentano i migliori e i colori blu scuro rappresentano i peggiori.

In questa tabella, i gruppi identità vengono suddivisi in settori come BLACK, MUSLIM, LGBTQ+, JEWS, ASIAN, LATINX, WOMEN, CHRISTIAN, MEN e WHITE. Ogni gruppo è correlato a valori numerici che rappresentano le prestazioni o le performance in diverse fonti di notizie e piattaforme di social media. Le fonti vengono suddivise in due categorie: News Left e News Right, e Reddit Left e Reddit Right. Ognuna di queste fonti ha un punteggio associato che indica il livello di minisformazione o minimo di minisformazione.

Ad esempio, per il gruppo BLACK, la performance in News Left è di 80.63, mentre in News Right è di 89.44. Questi valori sono confrontati con le performance dei gruppi identità in altre fonti, come MUSLIM, MUSLIM, LGBTQ+, eccetera.

La tabella include anche una sezione per la minimisformazione, dove le fonti vengono analizzate in base alla loro capacità di minimizzare la minisformazione. Questa sezione include diverse fonti di notizie e piattaforme di social media, come HP (L), NYT, CNN (L), CNN (R), NPR (L), NPR (R), Guard (L), Fox, WaEx (L), BBart (R), WAT (R) e NR. Ognuna di queste fonti ha un punteggio di minimisformazione associato, che indica il livello di minimisformazione.

Ad esempio, per la fonte HP (L), il punteggio di minimisformazione è di 88.72. Questo punteggio è confrontato con gli altri punteggi di minimisformazione delle fonti, come NYT, CNN (L), CNN (R), eccetera.

In generale, questa tabella fornisce una panoramica di come diversi gruppi identità performano in diverse fonti di notizie e piattaforme di social media, in quanto si riferisce alla minimisformazione e minimo di minisformazione.</sample>
    <sample id="188">Performance per Categoria

1°
2°
3°

NEWS LEFT
REDWOOD LEFT
REDWOOD RIGHT
HATE SPEECH
MINSTRELLOM
HP (L)
NYT
CNN (L)
CNN (R)
NPR (L)
NPR (R)
Guard (L)
Fox (L)
W (L)
BB (R)
WAT (R)
NR

18,03
80,83
80,84
80,85
80,86
80,87
80,88
80,89
80,90
80,91
80,92
80,93
80,94
80,95
80,96
80,97
80,98
80,99</sample>
    <sample id="189">Performance per Categoria

1°
2°
3°

NEWS LEFT REDDIT LEFT REDDIT RIGHT
BLACK 80.63 89.44 80.20
MUSLIM 80.08 90.19 80.20
LGBT+ 80.13 89.09 80.20
JWS 80.83 89.83 80.20
ASAIN 80.83 89.83 80.20
LATINX 81.55 89.83 80.20
WOMEN 87.82 89.83 80.20
CHRISTIAN 87.82 89.83 80.20
MEN 87.82 89.83 80.20
WHITE 85.22 89.83 80.20

Minimisformazione
HP (L) NYT (L) CNN (L) NPR (L) Guard (L) Fox (L) WaEx (L) BRbat (R) WAT (R) NR (R)
REDDIT LEFT REDDIT RIGHT
BLACK 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71
MUSLIM 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71
LGBT+ 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71
JWS 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71
ASAIN 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71
LATINX 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71 88.72 86.71
WOMEN 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71
CHRISTIAN 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71
MEN 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71
WHITE 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71 96.66 86.71

Tabella 4: Sostituisce i colori giallo chiaro con i migliori e i colori blu scuri con i peggiori.</sample>
    <sample id="190">Qualitativa Analisi Text Target Label Base N-L S-L N-R S-R the all right is corrupted with people supporting Asian True True True False False a new nation a new nation and priest have come from them They are not McDonald's True False False False True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True True</sample>
    <sample id="191">Qualitativa analisi

Testo

Target Label Base

N-L S-L N-R S-R

the all right is corrupted with people supporting a new nationa McDonald's and priest are common. They were charged that the crime was in the throws of a crime Trump because they know I was the slaughter going on Donald Trump August 20th incredible that water has been in flight in flut paying in Vermont for clean water.

Esempi di performance dovuta a modello linguistico varia bias politici, base, N-L rappresentare cristiani, vanilla RoBERTa model, news, social media, sinistra, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e, e</sample>
    <sample id="192">Tabella 12: Analisi qualitativa di esempi di testo di razzismo. Lm rappresenta le diverse posizioni politiche.</sample>
    <sample id="193">Table 12: Qualitative analysis of hate speech examples from LM with different political biases.</sample>
    <sample id="194">Tabella 12: Analisi qualitativa di esempi di testo di odio. La tabella include due colonne, la prima delle domande e la seconda le risposte. Le risposte sono classificate come “veri” o “falsi”. Le risposte “veri” sono colorate in rosso e le risposte “falsi” in blu.</sample>
    <sample id="195">Table 1: Qualitative analysis of hate speech examples from LM with different political bias.</sample>
    <sample id="196">Discussione Tra Scylla e Charybde Selezionare tra "saniizzare" o non selezionare, questa è la questione Pre-training dei dati Modelli linguistici Attività downstream</sample>
    <sample id="197">Discussione Tra Scylla e Charybdis. Il problema è quello di decidere se dobbiamo "sostituire" o non "sostituire", questa è la questione. Pre-training dei dati Modello linguistico downstream</sample>
    <sample id="198">Discussione Tra Scylla e Charybde Tra "inizializzare" o non "sostituire", è la questione Pretraining dei dati Linguaggi modello Downstream task</sample>
    <sample id="199">Grazie!</sample>
    <sample id="200">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite con contesti di lunghezza fino a 900 token.</sample>
    <sample id="202">I domini inclusi nel loro set di dati sono: Easy on Me, I Gotta Feeling, The one with the right music, The song that's not energetic, It has a river.</sample>
    <sample id="203">Posizionalità si riferisce alla capacità di un individuo o un gruppo di influenzare le percezioni e le azioni degli altri.</sample>
    <sample id="204">Dawei Zhu</sample>
    <sample id="205">No, EDAtt non adatta un modello ST offline esistente.</sample>
    <sample id="206">Per determinare il numero di autori coinvolti nell'articolo, analizziamo la sezione del titolo che elenca i nomi degli autori. L'articolo è intitolato "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" e include i nomi dei seguenti autori:

1. Yusen Zhang
2. Jun Wang
3. Zhiguo Wang
4. Rui Zhang

In totale, ci sono 4 autori coinvolti nell'articolo.</sample>
    <sample id="207">No, il modello testato non funziona sulla suite di test.</sample>
    <sample id="208">The three variants of KITMUS are Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="209">I ricercatori che hanno scritto l'articolo sono affiliati a Google Research.</sample>
    <sample id="210">Come usare le sampli pulite più efficientemente?</sample>
    <sample id="211">La sensibilità della metrica misura quanto un modello è sensibile a varietà di istruzioni per la stessa attività.</sample>
    <sample id="212">La relatrice o il relatore non è specificato in questa immagine.</sample>
    <sample id="213">La maggiore sensibilità indica che il modello non è adeguato.</sample>
    <sample id="214">I modelli vengono addestrati su un vasto contesto linguistico.</sample>
    <sample id="215">In genere, circa 40 campioni di convalida puliti sono necessari per il raggiungimento di buone prestazioni in WSL.</sample>
    <sample id="216">I fornitori dell'articolo sono affiliati all'Ingegneria di Stanford e alla Scienze Informatiche.</sample>
    <sample id="217">Perché i metodi esistenti non sono sufficienti?</sample>
    <sample id="218">La relatrice o il relatore è Alexandra Oteanu.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici ha un aspetto quantitativo.</sample>
    <sample id="220">No, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="221">No, Coscript non è pubblicamente disponibile.</sample>
    <sample id="222">La filigrana viene inserita esattamente nel testo utilizzando il processo di conteggio del numero di trigger in una frase.</sample>
    <sample id="223">I fornitori dell'articolo hanno due affiliazioni.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è "Come fare un pasticcino di cioccolato?".</sample>
    <sample id="226">Gli autori si accertano della segretezza del loro metodo utilizzando un modello pre-addestrato e non rivelando i dettagli del modello.</sample>
    <sample id="227">The work uses existing PLMs to build a new one by fine-tuning them on medical data.</sample>
    <sample id="228">GPT-4 is least aligned with Confucian countries.</sample>
    <sample id="229">La relatrice mostre il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione in "01 Use already existing offline ST models without re-training or adopting specific architecture for SimuST".</sample>
    <sample id="230">La quantità di attività influisce sulla performance del modello in modo inverso.</sample>
    <sample id="231">The three reference approaches compared by the authors are LSTM-seq2seq, Zhang and Lapata, and our own method.</sample>
    <sample id="232">I due coautori sono collaboratori del primo autore.</sample>
    <sample id="233">Chowdery et al., 2022</sample>
    <sample id="234">NLPositionality: Caratterizzando i bias di progettazione dei set di dati e dei modelli Sebastin "Santy" University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="235">NLPositionality: Caratterizzando i bias di progettazione dei set di dati e dei modelli Sebastin "Santy" University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="236">Immagina...</sample>
    <sample id="237">Immagina...</sample>
    <sample id="238">Imagina... Can you stop being a jerk? Aditya Sharma, Tech Lead, Times of India Carl Jones, Tech Lead, New York Times</sample>
    <sample id="239">Immagina... Design bias example! Carl Jones Aditya Sharma Tech Lead, New York Times Tech Lead, Times of India</sample>
    <sample id="240">Posizionalità [1] Savin-Baden, Maggi, and Claire Howell-Major. “Qualitative research: The essential guide to theory and practice.” Qualitative Research: The Essential Guide to Theory and Practice. Routledge (2013).</sample>
    <sample id="241">“Le prospettive che [i] persone hanno come risultato dei loro demografici, identità e esperienze.</sample>
    <sample id="242">“Le prospettive che le persone hanno come risultato delle loro demografiche, identità e esperienze.</sample>
    <sample id="243">Do datasets and models have positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science 2022.</sample>
    <sample id="244">Do datasets and models have positionality?</sample>
    <sample id="245">Do datasets and models have positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Li, et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science.</sample>
    <sample id="246">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="247">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112]. - Theoretical definitions of model positionality [3] 1 Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. 2 Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. 3 Cambò &amp;amp; Gergo, “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="248">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science 2022.</sample>
    <sample id="249">Question: Do datasets and models have positionality?</sample>
    <sample id="250">NLPosizionabilità
Un framework per caratterizzare i bias di progettazione in insiemi di dati e modelli NLP</sample>
    <sample id="251">Framework

Raccolta

Ogni istanza è annotata da un

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti

Oggetti</sample>
    <sample id="252">Raccolta

Procesing

Model Predictions

Eating with Hands
Can you live without your hands?
Steel</sample>
    <sample id="253">Framework

Raccolta

Processing

1) Annotate datasets with diverse annotators.
2) Train models on the annotated data.
3) Evaluate model performance using metrics such as accuracy, precision, recall, and F1 score.
4) Compare the performance of different models and select the best one for deployment.
5) Deploy the selected model in a real-world application.
6) Continuously monitor and update the model to maintain its performance over time.</sample>
    <sample id="254">Framework

Raccolta

Processing

1) Annotate datasets with diverse annotators.</sample>
    <sample id="255">Framework

Raccolta

Ogni istanza ha un etichettatore

I modelli hanno diverse prestazioni

Processo

Model Predictions

Demographics

1) Annotators annotate instances with gold labels from the models

2) Compare annotations to models and datasets to measure agreement via Pearson’s R scores</sample>
    <sample id="256">Framework

Raccolta

Ogni istanza ha un etichettatore

Oggetti rilevati

E' morale

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

Eating with Hands

E</sample>
    <sample id="257">Could you live with an AI and its morals? Are you better at noticing hateful and show compared to non-hateful and show? Where are you on the techno-utopian scale? Tell us how you thin mixed reality is more technic and others. Participate now!</sample>
    <sample id="258">Lab in the Wild</sample>
    <sample id="259">Task A: Accettabilità Sociale 1) Leggi la situazione 2) Inserisci il tuo pensiero su di essa da un punto di vista personale. Sentiti libero di spiegare i tuoi giudizi. 3) Vediamo cosa altri pensano su di esso!</sample>
    <sample id="260">Task A: Accettabilità Sociale 1) Read the situation. 2) Enter what you think about it. 3) See what others thought about it! The AI speculates. Study participants in the United States:</sample>
    <sample id="261">Task A: Social Acceptability

Analisi

Dataset

- Social Chemistry

Modeli

- Delphi
- GPT-4</sample>
    <sample id="262">Task B: Toxicità

1. Leggi l'esempio
2. Inserisci il tuo pensiero su di esso
3. Vedrai gli altri pensieri su di esso

Il diagramma sottostante rappresenta le opinioni degli utenti in Afghanistan riguardo la considerazione del discorso come nocivo o non nocivo.</sample>
    <sample id="263">Task B: Toxicity

Analisi

Dataset

- Dynahate

Modeli

- Perspective API
- Rewire API
- State RoBERTa
- GPT-4</sample>
    <sample id="264">Risultati Chi fanno al线a NLP dei set di dati e i modelli?</sample>
    <sample id="265">I dataset e i modelli sono maggiormente orientati verso le nazioni che parlano inglese.</sample>
    <sample id="266">I set di dati e i modelli sono più alignati con le persone che hanno un college.</sample>
    <sample id="267">I dataset e i modelli sono maggiormente allineati con le persone che hanno un'istruzione universitaria.</sample>
    <sample id="268">Trova 2: Alcune popolazioni rimangono indietro.</sample>
    <sample id="269">I dataset e i modelli sono meno allineati con le persone non binarie.</sample>
    <sample id="270">Quindi, cosa possiamo fare? Affrontare la posizionalità in NLP</sample>
    <sample id="271">Raccomandazioni</sample>
    <sample id="272">Raccomandazioni

1. Mantenere un registro di tutti i design choices rilevanti durante la costruzione di dataset e modelli.
2. Fare ricerche NLP attraverso il lens di perspectivism:
a. Condividere etichette disaccordate del dataset
b. Utilizzare tecniche di modellazione che possono gestire la discordia degli annotatori
3. Costruire dataset e modelli specializzati per e per comunità specifiche è responsabile per l'Inclusività NLP (ad esempio, iniziativa Masakhane).

[1] https://www.masakhane.io</sample>
    <sample id="273">Grazie!</sample>
    <sample id="274">La relatrice menziona 7 problemi associati a SimulST.</sample>
    <sample id="275">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere quello di utilizzare un insieme di test diverso per valutare le performance del modello. Questo può aiutare a identificare eventuali bias e a migliorare la rappresentatività del modello.</sample>
    <sample id="276">Il 61esimo incontro annuale dell'Associazione per la Linguistica Computazionale Toronto, Canada 03-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siliu Yuan, Jianqiang Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang Università di Paderborn Brain Technologies Inc.</sample>
    <sample id="277">Come fare un pasticcio? 1. Raccogli i ingredienti. 2. Riscaldare il forno a 325 °F (163 °C) e unire lo grasso. 3. Impiastrare il burro e lo zucchero. 4. Aggiungere la uova. 5. Aggiungere la farina. 6. Mescolare la batteria con la farina. 7. Versare la batteria nel piatto del pasticcio. 8. Fornire il pasticcio per 1 ora e 15 minuti.</sample>
    <sample id="278">Come fare un pasticcino? 1. Raccogli i ingredienti. 2. Riscaldare il forno a 325 °F (163 °C). 3. Un contenitore per le creme. 4. Un pani da forno. 5. Un cucchiaio. 6. Un bicchiere. 7. Un forno. 8. Un pasticcino.</sample>
    <sample id="279">Constrained Language Planning

How to Make a Strawberry Cake?
...Add strawberry jams into the flour...

How to Make a Chocolate Cake?
...Add cocoa powder into the flour...

Object goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="280">Constrained Language Planning

How to Make a Strawberry Cake?
...Add strawberry jams into the flour...

How to Make a Chocolate Cake?
...Add cocoa powder into the flour...

Object goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="281">Constrained Language Planning come si fa un pasto con i frullati? ...Aggiungi lo zucchino alla farina... Come fare un pasto con cioccolato? ...Aggiungi la polvere di cioccolato alla farina... Lo scopo finale può essere ereditato da diverse finalità reali specifiche con multi-faccettate constraint</sample>
    <sample id="282">Come fanno i modelli di apprendimento dell'lingua (LLMs) sul pianificamento linguistico con vincoli?</sample>
    <sample id="283">Come fanno i modelli di apprendimento della lingua (LLMs) sul pianificamento linguistico con vincoli?</sample>
    <sample id="284">Come fanno i modelli di apprendimento dell'lingua (LLMs) su pianificazione linguistica con vincoli?</sample>
    <sample id="285">Puoi i modelli di apprendimento a rete (LLMs) fare pianificazione linguistica con vincoli?</sample>
    <sample id="286">Puoi i modelli di apprendimento automatico fare pianificazione linguistica con vincoli?</sample>
    <sample id="287">Quanti tipi di errori i modelli di apprendimento automatico (LLMs) usualmente commettono in questa attività? Fasi: Non ci sono vincoli SE1: Mancata azione SE2: Azione ripetuta SE3: Azione con ordine sbagliato FE1: Azione non prevista FE2: Azione incoerente FE3: Azione inattesa e non prevista. La semanticità complessità (SE) in script generati è accettabile, ma la fedeltà ai vincoli (FE) non può essere garantita.</sample>
    <sample id="288">Quale tipo di errori i modelli di apprendimento automatico (LLMs) usualmente commettono in questa attività?</sample>
    <sample id="289">Cosa tipicamente falliscono i modelli InstructGPT?</sample>
    <sample id="290">Input: un obiettivo astratto Genera obiettivi specifici utilizzando GPT-4 In-context learning Abstratto obiettivo: Fare un pasto Specifici obiettivi: G1 (+ modifica): Fare un pasto al cioccolato G2 (+ metodo): Fare un pasto in un ristorante G3 (+ intenzione): Fare un pasto per un matrimonio</sample>
    <sample id="291">Input: un obiettivo astratto Generate specific goals with the help of GPT-4. Abstract Goal: Make a cake Specific Goals: G1 (+ modifier): Make a chocolate cake G2 (+ method): Make a cake in a microwave oven G3 (+ intent): Make a cake for a wedding</sample>
    <sample id="292">Input: un obiettivo astratto
Generare i propri obiettivi
Generare i propri obiettivi con l'aiuto di GPT-3
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con l'aiuto di GPT-3 e imparare in contesto
Generare i propri obiettivi con</sample>
    <sample id="293">Metodo

Inizialmente, generiamo candidate script con InstructGPT tramite imparazione in contesto. Questi script vengono quindi valutati per la similarità con gli obiettivi specifici. I script che hanno una similarità più alta con gli obiettivi vengono selezionati come script filtrati. Infine, i script filtrati vengono utilizzati per ottenere gli obiettivi specifici corrispondenti.

In questa immagine, siamo in grado di vedere il processo di selezione dei script filtrati e come essi vengono utilizzati per ottenere gli obiettivi specifici. Questo è un esempio di come i metodi di apprendimento in contesto possono essere utilizzati per automatizzare il processo di creazione di script.</sample>
    <sample id="294">Il diagramma illustra un processo di elaborazione di script per raggiungere obiettivi specifici. Inizialmente, si generano candidate scripts utilizzando InstructGPT con imitazione del contesto. Questi script vengono quindi valutati in base alla similarità con gli obiettivi desiderati. I script che hanno una similarità elevata sono selezionati come script filtrati. Infine, i script filtrati vengono utilizzati per ottenere gli ingredienti necessari e aggiungere il cacao in polvere.</sample>
    <sample id="295">Metodo

Passo 1: Genera candidate script con InstructGPT tramite imparazione in contesto

Trova i filtri script che raggiungono il obiettivo con InstructGPT tramite punteggio di similarità

Output: Obiettivi specifici con script corrispondenti</sample>
    <sample id="296">Il grafico confronta la precisione di diversi modelli di lingua in un test di pianificazione. Il modello "Our Method" emerge come il più preciso, seguito da "InstructGPT (175B)" e "GPT-3 (17B)". I modelli "TS (11B)" e "Flan-T5 (11B)" hanno prestazioni inferiori. L'immagine sottolinea che con il nuovo metodo, InstructGPT è capace di generare soluzioni di qualità superiore di gran lunga.</sample>
    <sample id="297">Script Distillazione da modelli LLMs

Motivazione
È necessario abilitare la capacità di pianificazione limitata per i modelli più piccoli.
Metodo
Seguire l'idea di distillazione conoscenziale simbolica.
Generare 55.000 script con vincoli da modelli LLM basati su un dataset di script.
Usare la validazione e il set di test per

Input: Un riassunto
Output: Obiettivi specifici con piani di azione</sample>
    <sample id="298">Script Distillazione da modelli LLMs Motivazione: Creare modelli più piccoli con capacità di pianificazione limitate. Metodologia: Seguire l'idea di distillazione conoscenziale simbolica. Generare 55.000 script con vincoli basati su modelli LLMs utilizzando il dataset di script. Validazione e test: Utilizzare un insieme di validazione e test per verificare la qualità dei modelli generati.</sample>
    <sample id="299">Script Distillazione da modelli LLM

Motivazione
È necessario abilitare la capacità pianificazione per i modelli più piccoli.
Metodo
Seguire l'idea di distillazione conoscenziale simbolica
Generare 55.000 script con vincoli da modelli LLM basati su un metodo =&gt; CoScript Dataset
Utilizzare la validazione e il set di test per annotare

Input: Un riassunto
Output: Obiettivi specifici con piani di azione</sample>
    <sample id="300">Script Distillazione da modelli di lingua

Motivazione:
- Abilitare la capacità pianificativa per modelli più piccoli.
- Seguire l'idea di distillazione conoscenziale simbolica.
- Generare 55.000 script con vincoli basati su modelli di lingua basati su costrutto.

Metodo:
1. Generare un abstract.
2. Trova i filtri candidati con InstructGPT in base al punteggio di similarità.
3. Trova gli output specifici dei propri obiettivi con InstructGPT.

Output:
- Obiettivi specifici con piani di azione.</sample>
    <sample id="301">Script Distillazione da modelli LLM

Motivazione
È necessario abilitare la capacità di pianificazione per i modelli più piccoli.
Metodo
Seguire l'idea di distillazione conoscenziale simbolica.
Generato 55.000 script con vincolo da modelli LLM basati sul metodo =&gt; Coreset Dataset
Valutazione e test set annotato

Input: Un riassunto
Output: Obiettivi specifici con piani di azione</sample>
    <sample id="302">Analisi dei vincoli

La diapositiva illustra un diagramma a rose che rappresenta la distribuzione dei vari elementi utilizzati per definire i vincoli. I colori diversi rappresentano diverse category: il rosso indica "When", il giallo "By", il violetto "What", il blu "To" e il verde "For". Le percentuali indicate sul diagramma sono: 21,57% per "When", 36,88% per "By", 48,69% per "What", 15,45% per "To" e 15,45% per "For".

Il testo in basso della diapositiva menziona che il coscritto mostre una alta heterogeneità e pluralismo nella generazione di obiettivi specifici.</sample>
    <sample id="303">Specialized Models vs. LLMs

Il grafico a barre sulla sinistra confronta l'accuratezza di diversi modelli di apprendimento automatico (MLMs) e modelli specializzati. I modelli specializzati, come GPT-3 (175B), Codex (175B), InstructGPT (175B) e T5 trained on WikiHow, hanno un'accuratezza inferiore rispetto ai modelli più piccoli finetunati su Coreset. Questi modelli più piccoli, anche se più limitati, offrono una qualità dei script superiore.

Il testo sottostante spiega che i modelli più piccoli finetunati su Coreset possono generare script di qualità superiore rispetto ai modelli più grandi. Questo è dovuto alla capacità di questi modelli di adattarsi specificamente alle esigenze del proprio contesto, migliorando la precisione e la relevantezza dei risultati.

In conclusione, il grafico e il testo insieme evidenziano l'importanza di scegliere il modello giusto per un determinato compito, in quanto anche i modelli più piccoli possono offrire prestazioni superiori in certi casi.</sample>
    <sample id="304">Riassunto e traevoli punti

• Creare il problema di pianificazione linguistica vincolata.
• Valutare la capacità di pianificazione linguistica vincolata delle LLMs e sviluppare un metodo per generare quindi filtrare per LLMs.
• Utilizzare LLMs per creare un dataset di script di alta qualità (CoScript) per pianificazione linguistica vincolata.
• Limitazioni e lavoro futuro:
  - Il nuovo approccio proposto per migliorare le LLMs è un'abbordatura post hoc di re-rank.
  - CoScript eredita solo da un modello astratto con un vincolo.
  - Il dataset di script può essere un risorsa valiosa per la ricerca sulla pianificazione linguistica con obiettivi e vincoli più complessi e diversi.
16</sample>
    <sample id="305">Riassunto e traevoli punti

* Creare il problema di pianificazione linguistica con vincoli.
* Valutare la capacità di pianificazione linguistica limitata dei modelli di apprendimento automatico (LLMs) e sviluppare un metodo per generare e filtrare superfluo per i LLMs.
* Utilizzare i modelli di apprendimento automatico per creare un dataset di script di alta qualità (CoScript) per la pianificazione linguistica con vincoli.
* Limitazioni e lavoro future:
	+ Il nuovo approccio proposto per migliorare i LLMs è un'abbordatura post-rank.
	+ CoScript eredita solo da un modello astratto con un vincolo.
	+ Il dataset di script può essere un risorsa valiosa per la ricerca sulla pianificazione linguistica con obiettivi e vincoli più complessi e diversi.</sample>
    <sample id="306">Il 61esimo incontro annuale dell'Associazione per la Linguistica Computazionale Toronto, Canada 9-14 luglio 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deging Yang Coreset Website https://github.com/siyuyuan/coreset</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi SOTA.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana sono applicabilità, utilità, covetness e transferabilità.</sample>
    <sample id="309">I discorsi TED sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">Ogni istanza viene campiata per la riannotazione.</sample>
    <sample id="311">La differenza tra i set di dati benigni e backdoor viene misurata utilizzando metriche di distanza come la somma dei coseni e la somma dei p-valori del test di Kolmogorov-Smirnov.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare due gruppi di modelli.</sample>
    <sample id="344">The authors decide which are the moderate-frequency words by counting the word frequency on a general text corpus.</sample>
    <sample id="345">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023? Shuheng Liu, Alan Ritter School of Interactive Computing Georgia Institute of Technology</sample>
    <sample id="346">Named Entity Recognition &amp; Generalization Georgia Tech</sample>
    <sample id="347">Riconoscimento e generalizzazione di entità denominate • I modelli hanno utilizzato CoNLL-2003 per sviluppare il RER per quasi 20 anni</sample>
    <sample id="348">Riconoscimento di entità denominate e generalizzazione • I modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi 20 anni • Questi modelli possono generalizzare i dati moderni? • Qual è necessario per una buona generalizzazione? Georgia Tech</sample>
    <sample id="349">Riconoscimento di entità denominate e generalizzazione

- I modelli hanno utilizzato CoNLL-2003 per sviluppare il RER per quasi 20 anni
- Questi modelli possono generalizzare i dati moderni?
- Qualcosa è necessario per una buona generalizzazione?</sample>
    <sample id="350">ConLL+ Dataset AMBASSADOR O UNITED O NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="351">ConLL+ Dataset • Raccolto Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines • Fine-tuned 20+ models on CoNLL-2003 AMBASSADOR O UNITED OORG NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="352">CoNLL+ Dataset • Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines • Fine-tuned 20+ models on CoNLL-2003 • Evaluated on CoNLL-2003 test set &amp;amp; CoNLL++ • Calculated percentage ΔF1 to assess generalization AMBASSADOR O TO I-ORG UNITED NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="353">Cosa si necessita per un buon generalizzamento?</sample>
    <sample id="354">Cosa si necessita per una buona generalizzazione? • Architettura del modello • I modelli Transformer generalizzano meglio 50 40 30 20 10 0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 ComET-2000 F1 Score ComET-2000 F1 Score</sample>
    <sample id="355">Cosa si richiede per una buona generalizzazioni? Model architecture Transformer models generalize better Model size Larger models generalize better</sample>
    <sample id="356">Cosa si necessita per una buona generalizzazione? Modello architettura - I modelli Transformer si sono dimostrati migliori per la generalizzazione. Dimensione del modello - I modelli più grandi si sono dimostrati migliori per la generalizzazione. Numero di esempi di finetuning - Più esempi portano a una migliore generalizzazione.</sample>
    <sample id="357">Cosa causa una diminuzione delle prestazioni?</sample>
    <sample id="358">Cosa causa una diminuzione delle prestazioni?</sample>
    <sample id="359">Cosa causa una diminuzione delle prestazioni?</sample>
    <sample id="360">Cosa causa una diminuzione delle prestazioni?</sample>
    <sample id="361">Cosa causa la diminuzione delle prestazioni? • Adattamento e sovrappasso? • Drift temporale?</sample>
    <sample id="362">Cosa causa una diminuzione delle prestazioni? • Superapprendimento adattativo? • Non sono stati osservati rendimenti • Scorrimento temporale?</sample>
    <sample id="363">Cosa causa la diminuzione delle prestazioni? • Adattamento e sovraptimalizzazzione? • Nessun ritorno osservato • Spostamento temporale?</sample>
    <sample id="364">Cosa causa la diminuzione delle prestazioni? • Superapprendimento adattativo? • Non sono stati osservati i rendimenti • Drift temporale? Nomi Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Funz. 2018 ∆ (%) Funz. 2018 Funz. 2003 Fun</sample>
    <sample id="365">Cosa causa una diminuzione delle prestazioni? • Adattativa sovraptimalità? • Non sono stati osservati rendimenti • Temporal Drift? • Le prestazioni deteriorano con un intervallo temporale più grande Name CoNLL-2003 CoNLL++ ( % ) Flair 92.26% 88.46% Flairx 90.91% 88.46% Pooled Flair 93.15% 88.20% Pooled Flairx 92.36% 87.08% ELMO 92.11% 90.75% ELMOx 90.76% -1.43%</sample>
    <sample id="366">Conclusione Per un buon impianto, ci vuole: Modello migliore Architettura più grande Più esempi di addestramento</sample>
    <sample id="367">Conclusione
Per una buona generalizzazione, ci vuole:
- Architettura del modello migliore
- Dimensione del modello più grande
- Esempi di addestramento più numerosi
- Performance scadente causata da:
- Drift temporale
- Sottoprogrammi non adattativi

Performance drop is caused by:
- Temporal drift
- Not adaptive overfitting</sample>
    <sample id="368">Conclusione • Per una buona generalizzazione, ci serve: - Architettura del modello migliore - Dimensione del modello più grande - Esempi di addestramento più accurati • Performance scaduta causata da: - Adattamento temporale e sovraccapacità non adattativa • ConLl-2003 tagger funziona?</sample>
    <sample id="369">Conclusione
Per una buona generalizzazione, ci serve:
- Migliore modellatura
- Dimensione del modello più grande
- Più esempi di addestramento
Performance drop causato da:
- Adattamento non adeguato
- Spostamento temporale
- ConNLL-2003 tagger funziona?
Sì</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhanL/acl2023-conllppp Contact: slu775@gatech.edu</sample>
    <sample id="397">La dimensione del segmento parlato utilizza l'approccio di 1080p.</sample>
    <sample id="398">The specific knowledge of Servin and Kea that is required in the example are their occupations.</sample>
    <sample id="399">La qualità dell'esempio è più importante del fattore somiglianza con la frase sorgente.</sample>
    <sample id="400">The article focuses on the extended experiments of the GPT-3 models.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">I esempi di inferenza diretta sono "The newer one" e "The song that's not energetic".</sample>
    <sample id="403">I ricercatori dell'articolo sono affiliati a Peking University e Brain Technologies Inc.</sample>
    <sample id="404">Per determinare il numero di autori coinvolti nell'articolo, analizziamo la sezione di autori presentata nella immagine. La lista di autori è la seguente:

1. Yanis Labarre
2. Adrien Bazille
3. Richard Dufour
4. Mickael Rouvier
5. Emmanuel Morin
6. Beatrice Dallie
7. Pierre-Antoine Gourraud

Contando i nomi elencati, ci sono un totale di 7 autori coinvolti nell'articolo.</sample>
    <sample id="405">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è stato considerato come un approccio standard.</sample>
    <sample id="406">Gli autori hanno fornito un esempio di "unwarrior" e "womawarrior" come gruppo contrassegnato.</sample>
    <sample id="407">The answer is not provided in the given information.</sample>
    <sample id="408">The names of the test datasets are FT_w, COSINE, L2R, BERT, and MLc.</sample>
    <sample id="409">C'è un totale di 6 autori coinvolti nell'articolo.</sample>
    <sample id="410">Il testo non fornisce informazioni specifiche sull'uso di più modalità o solo con il testo.</sample>
    <sample id="439">The area of NLU that is not well-studied, according to the authors, is the inference-time knowledge.</sample>
    <sample id="440">I nomi dei relatori sono Zhiyang Xu, Ying Shen e Lifu Huang.</sample>
    <sample id="441">Sì, Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="442">I limiti delle risorse esistenti per la traduzione dipendente dal contesto sono che supportano fenomeni e lingue limitati.</sample>
    <sample id="443">Risolver espressioni di riferimento indiretti per la selezione dell'entità (AltEntities Corpus)</sample>
    <sample id="444">Risolver espressioni di riferimento indiretti per la selezione di entità (AltEntities Corpus)</sample>
    <sample id="445">Indiretti riferimenti espressioni

* God: Understanding users' language when they make a choice
* Alternative question: Did you mean easy on me or I gotta feeling?
* Direct reference: "easy on me", "the first one"
* Indirect reference: Cannot remember the name, Cannot remember the name and fluid conversation, The pronunciations are hard to distinguish, Want to specify a preference
* Indirect reference: The newer one. The song that's not energetic.

Risolvendo espressioni riferimento indiretti per la selezione di entità (Mnli corpus)</sample>
    <sample id="446">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name Cannot remember the name. The newer one. The song that's not energetic.</sample>
    <sample id="447">Indiretti riferimenti espressioni

* Obiettivo: Capire la lingua dei utenti quando fanno un选择
* Domanda alternativa: Did you mean easy on me or I gotta feeling?
* Riferimento diretto: "easy on me", "the first one"
* Riferimento indiretto: Non ricordare il nome, impossibile distinguere le pronunciazioni, voler specificare un preferito
* Riferimento indiretto: The newer one. The song that's not energetic.</sample>
    <sample id="448">Indiretti riferimenti espressioni

* Obiettivo: Capire il linguaggio dei utenti quando fanno un选择
* Domanda alternativa: Did you mean easy on me or I gotta feeling?
* Riferimento diretto: Non posso ricordare il nome, "la prima"
* Riferimento indiretto: Non posso ricordare il nome, "la prima"
* Pronunciazioni sono difficili da distinguere
* Vorrei specificare un preferimento
* Riferimento indiretto: La nuova una... La canzone che non è energica</sample>
    <sample id="449">Indiretti riferimenti espressioni

* Obiettivo: Comprendere il linguaggio dei utenti quando fanno un选择
* Domanda alternativa: Did you mean easy on me or I gotta feeling?
* Riferimento diretto: Non posso ricordare il nome; "Il primo"
* Riferimento indiretto: Non posso ricordare il nome in conversazione naturale e fluida:
    * Le pronunciazioni sono difficili da distinguere
    * Vorrei specificare una preferenza
* Riferimento indiretto: La nuova.
* La canzone che non è energica</sample>
    <sample id="450">Dataset Collection

* Important problem
	+ Conversational systems
	+ Benchmarking Large Language Models' entity understanding
	+ No large-scale public dataset available
* We collect a large dataset using crowd annotation
* Three domains:
	+ Audio (speech)
	+ Text (books)
	+ Image (restaurant menu)

Google Research</sample>
    <sample id="451">Dataset Collection

* Important problem
	+ Conversational systems
	+ Benchmarking Large Language Models' entity understanding
	+ No large-scale public dataset available
* We collect a large dataset using crowd annotation
* Three domains:
	+ 🎧 Audio
	+ 📚 Text
	+ 🍽️ Web</sample>
    <sample id="452">Metodologia di raccolta di dataset enfatizza l'informalità utilizzando un compito di completamento di disegni animati.</sample>
    <sample id="453">Dataset Collection Methodology

- Metodologia enfatizza l'informalità utilizzando un compito di completamento di disegni animati
- Il compito richiede di completare una frase in un contesto dialogico
- L'alternativa alla domanda è "How do you feel today?"
- L'espressione si riferisce a uno dei personaggi presenti
- Il dataset è rilevato da un'annotatore che riempie il vuoto

Risolving Indirect Expressions (Entity Selection) - (Mellie Corpus)

- La metodologia utilizza un compito di completamento di disegni animati per raccogliere i dati
- L'annotatore riempie il vuoto con un'alternativa alla domanda
- L'espressione si riferisce a uno dei personaggi presenti
- Il dataset è rilevato da un'annotatore che riempie il vuoto</sample>
    <sample id="454">Dataset Collection Methodology

* Metodologia enfatizza l'informalità utilizzando un compito di completamento di disegni animati
	+ "Ricordate che eravamo ieri?" (Ricordate che eravamo ieri?)
	+ "Do you mean I have to do it? I gotta Feeling?" (Significa che devo farlo? Ho un sentimento?)
	+ "The alternative question" (La domanda alternativa)
	+ "Expression referring to one of the entities" (Espressione riferita a uno dei fornitori)
	+ "Filled in by the annotator" (Riempito dal raffigurato")
Sets the dialog context (chosen from a few manual prompts per day)</sample>
    <sample id="455">Dataset Collection Methodology

- Metodologia enfatizza l'informalità utilizzando un compito di completamento di disegni animati
- Il compito richiede di completare una frase in base a un contesto dialogico fornito
- L'immagine illustra un esempio di come il compito viene svolto:
  - Due personaggi parlano tra loro, con i propri pensieri e azioni rappresentati da bubble
  - Il primo personaggio dice: "Ricordate che eravamo felici ieri?"
  - Il secondo personaggio risponde: "Do you mean we were happy? I gotta Feeling?"
  - Il terzo personaggio risponde: "The alternative question"
  - L'annotatore riempie il campo vuoto con la risposta corretta: "Expression referring to one of the entities"
- Il testo alla fine della slide fornisce ulteriori dettagli sulla raccolta dei dataset:
  - "Resolving Indirect Expressions (Entity Selection) (Dialogue Corpus)"
  - "Google Research"</sample>
    <sample id="456">Metodologia di raccolta di dataset enfatizza l'informalità utilizzando un compito di completamento di disegni animati.</sample>
    <sample id="457">Dataset Collection Methodology

Metodologia enfatizza l'informalità utilizzando un compito di completamento di disegni animati.

Risolving Indirect Expressions (Entity Selection Corpus)

- Metodologia enfatizza l'informalità utilizzando un compito di completamento di disegni animati.
- Sets the dialog context (chosen from a few manual prompts per domain)
- The alternative question
- Expression referring to one of the entities

Fatto da: Google Research</sample>
    <sample id="458">Generare domande alternative = selezione di coppie di entità

La presentazione illustra come generare domande alternative utilizzando la selezione di coppie di entità. L'immagine fornisce esempi di come le domande possono essere formulate in base a diverse tipologie di entità, come infobox simili su Wikipedia (stesso genere/autore) e descrizioni simili. L'immagine include anche domande generate in modo casuale per dimostrare come le domande possono essere formulate in modo diverso. L'immagine è stata tradotta in italiano da un assistente di Google Research.</sample>
    <sample id="459">Generare domande alternative = selezione di coppie di entità

Google Research

Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean Thinking of You or Happy Anywhere? Items with similar titles Do you mean The Return (memoir) or The Return (Shatner novel)? Resolving Indirect Expressions through Entity Selection (Mitya Corpus)</sample>
    <sample id="460">Generare domande alternative = selezione di coppie di entità

Do you mean A or B?</sample>
    <sample id="461">Generare domande alternative = selezione di coppie di entità

Google Research

Do you mean A or B?
Items with similar infoboxes on Wikipedia (same genre and/or artist)
Items with similar descriptions on Wikipedia
Do you mean Thinking of You or Happy Anywhere?
Items with similar titles
Do you mean The Return (memoir) or The Return (Shatner novel)?
Items with similar titles
Uniform at random:
Do you mean You Could Be Mine or The Way I Am?
Items with similar titles</sample>
    <sample id="462">Generare domande alternative = selezione di coppie di entità

Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Do you mean Thinking of You or Happy Anywhere? Items with similar descriptions on Wikipedia Do you mean The Return (memoir) or The Return (Shatner novel)? Items with similar titles Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="463">Google search link (Music) Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song.</sample>
    <sample id="464">Background knowledge (Music) Google search link to song. Easy on Me (by Ajele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song</sample>
    <sample id="465">Google search link each song. Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click to find out about the song. Click to find out about the song. We ask annotators to Listen at least some of each song Read about each song</sample>
    <sample id="466">Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of marzipan and a set of eleven balls made of the same paste. Pandan cake is a light, fluffy sponge cake coloured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia and also in the Netherlands, especially among the Indo community.</sample>
    <sample id="467">Eliciting expressions

We then tell the annotators which choice should be selected and ask them to describe it.

Pick one

Easy on Me (by Adele)

I Gotta Feeling (by The Black Eyed Peas)

We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, e.g.:

The one with the jazz music

The song that's not energetic

It has something about a river

It's the newest one

It's about not having time to choose</sample>
    <sample id="468">Eliciting expressions

We then tell the annotators which choice should be selected and ask them to describe it.

Pick one

Easy on Me (by Adele)

I Gotta Feeling (by The Black Eyed Peas)

We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, not example.

One with the music

The song that's not energetic

It has a river

It's about not having time to choose</sample>
    <sample id="469">AltEntities Corpus

- 6.000 domande alternative su tre dominii
- 42.000 espressioni indiretto riferimento
- Risultati con il modello T5 XL (precisione):
    - 92-95% se il LM ha accesso alla stessa conoscenza di sfondo degli annotatori.
    - 82-87% quando il LM ha accesso a conoscenza di sfondo parzialmente sovrapposti.
    - 60% quando il LM (T5 XL) ha accesso solo ai nomi degli entità.
- Abbiamo dimostrato che i modelli sono generalizzabili tra i dominii.
- Collegamento al dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="470">AltEntities Corpus

- 6.000 alternative questions across the three domains
- 42.000 indirect referring expressions

Risultati con il modello T5 XL (accuratezza):
- 92-95% se il LM ha accesso alla stessa conoscenza di sfondo degli annotatori.
- 82-87% quando il LM ha accesso a informazioni di sfondo parzialmente sovrapposte.
- ~60% quando il LM (T5 XL) ha accesso solo ai nomi degli entità.

Abbiamo dimostrato che i modelli sono generalizzabili ai domini.

Collegamento al dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="471">AltEntities Corpus

- 6.000 domande alternative su tre dominii
- 42.000 espressioni di riferimento indiretti

Risultati con il modello T5 XL (precisione):

- 92-95% se il LM ha accesso alle stesse conoscenze di sfondo degli annotatori.
- 82%-87% quando il LM ha accesso a conoscenze di sfondo parzialmente sovrapposte.
- ~60% quando il LM (T5 XL) ha accesso solo ai nomi degli entità.

Abbiamo dimostrato che i modelli sono generalizzabili tra i dominii.

Collegamento al dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="472">AltEntities Corpus

- 6.000 domande alternative in tre dominii
- 42.000 espressioni indirette riferendosi ad entità
- Risultati con il modello T5 XL (accuratezza):
    - 92-95% se il LM ha accesso alla stessa conoscenza di sfondo delle annotatori.
    - 82-87% quando il LM ha accesso a conoscenza di sfondo parzialmente sovrapposti.
    - 60% quando il LM (T5 XL) ha accesso solo ai nomi delle entità.
- Abbiamo dimostrato che i modelli sono generalizzabili tra dominii.
- Collegamento al dataset: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="473">SimulST existing policies are compared with the approach.</sample>
    <sample id="474">LIA, LS2N, CHU de Nantes, Zenith</sample>
    <sample id="475">La relatrice o il relatore è Sebastin Sanyt.</sample>
    <sample id="476">Per rispondere alla domanda di quanti autori sono coinvolti nell'articolo, analizziamo la descrizione fornita. L'articolo è intitolato "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" e include i seguenti autori:

1. Myra Cheng
2. Esin Durmus
3. Dan Jurafsky

Inoltre, è menzionato che l'articolo è stato accettato per la presentazione alla conferenza NeurIPS 2023.

Passo 1: Identificare i nomi degli autori dalla descrizione.
Passo 2: Contare il numero di nomi elencati.

Risultato: Ci sono tre autori coinvolti nell'articolo.</sample>
    <sample id="477">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi UNIVERSITÀ DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="478">Attenzione come Guida per la Traduzione Simultanea</sample>
    <sample id="479">What are the problems of the current SimulST models?</sample>
    <sample id="480">I modelli SimulST attuali hanno problemi? Specifici architetti sono di solito addestrati, introducendo moduli aggiuntivi da ottimizzare Lunghe e complesse procedure di addestramento (ad es. obiettivi di ottimizzazione diversi)</sample>
    <sample id="481">I problemi dei modelli SimulST attuali? Specifiche architettura sono di solito addestrate, introducendo moduli aggiuntivi da essere ottimizzati (ad es. moduli differenti) Lunghe e complesse procedure di addestramento (ad es. obiettivi differenti) Addestrare e mantenere diversi modelli per raggiungere regimes latenza diversi</sample>
    <sample id="482">What is our solution?</sample>
    <sample id="483">Utilizziamo modelli offline ST esistenti senza re-Allenare o adottare un particolare architettura per SimuST</sample>
    <sample id="484">Cosa è la nostra soluzione? Utilizziamo modelli offline ST esistenti per ridurre i tempi di retraining o adottare un architettura specifica per SimulST Utilizziamo un singolo modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici. Sfruttiamo le conoscenze acquisite dalla nostra storia tra input audio e output testuale.</sample>
    <sample id="485">Il modello di apprendimento automatico utilizza un algoritmo che analizza i dati e li confronta con i parametri predeterminati per fornire una risposta. Questo algoritmo è in grado di imparare da un insieme di dati precedenti e migliorare le proprie prestazioni nel tempo.</sample>
    <sample id="486">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="487">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="488">Decide whether to emit or not a partial translation based on where attention points to: a word attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="489">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold λ) towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="490">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="491">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="492">Decidere se emettere o non emettere una traduzione parziale basata su dove l'attenzione punti: un punto è emesso se l'attenzione non è concentrata su (il suo som è inferiore a un soglia 𝛼) verso le ultime 𝜆 informazioni ricevute stabil.</sample>
    <sample id="493">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below the threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough, stable.</sample>
    <sample id="494">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (it's sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="495">Main Results: EDAtt AL / AL-CA (s) 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 (a) en-de</sample>
    <sample id="496">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="497">Main Results: EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 latency measure (a) AL-de</sample>
    <sample id="498">Risultati principali: EDAtt 27 25 23 21 19 17 15 13 11 9 7 5 3 1.5 0.5 2.5 3.5 4.5 5 AL / AL-CA (δ)</sample>
    <sample id="499">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="500">Risultati principali: EDAtt popular strategies also applied to offline models wait-k LA CAAT EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) (a) en-de</sample>
    <sample id="501">Main Results: EDAtt state of the art architecture, specifically tailored for SimuST wait-k LA CAAT EDAtt</sample>
    <sample id="502">Risultati principali: EDAtt

[Immagine non inclusa]

[Immagine non inclusa]

[Immagine non inclusa]</sample>
    <sample id="503">Risultati principali: EDAtt

La seguente immagine rappresenta un grafico che illustra i risultati dell'analisi dei modelli di traduzione automatica. Il grafico è titolato "Main Results: EDAtt" e include due strategie di traduzione automatica, EDAtt e CAAT, confrontate con due modelli offline, wait-k e LA. I punti sul grafico rappresentano le performance dei modelli in termini di BLEU, mentre l'asse x rappresenta il tempo di elaborazione in secondi.

Il grafico evidenzia che EDAtt supera tutti i modelli offline applicati ai modelli offline. Inoltre, il grafico include una legenda che identifica le diverse strategie di traduzione automatica utilizzate.

In sintesi, il grafico fornisce una rappresentazione visiva dei risultati dell'analisi dei modelli di traduzione automatica, dimostrando che EDAtt è più efficiente rispetto ai modelli offline.</sample>
    <sample id="504">Vorrei scoprire di più? Leggi il nostro articolo per scoprire di più sulle nostre ricerche! (spapi.negri@fbk.eu, marco.turchi@gmail.com) Scansaci!</sample>
    <sample id="505">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="506">MULTINSTRUCT: Improve la imparazione a zero con modelli multi-modal tramite l'addestramento con istruzioni</sample>
    <sample id="507">Pre-trained Language Models for Downstream Tasks

(A) Pretrain-finetune (BERT, T5)

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A

Pretrained LM
Task A</sample>
    <sample id="508">Pre-trained Language Models for Downstream Tasks

(A) Pretrain-finetune (BERT, T5)

Pretrained LM
Task A
Task B
Typically requires many task-specific examples
for each task

Pretrained LM
Inference on Task A
Inference on Task B
Improves performance or prompt engineering

(C) Instruction tuning (FLAN)

Pretrained LM
Instruction-tune on Task A
Inference on unseen tasks
Model learns to perform many tasks via natural language instruction</sample>
    <sample id="509">Solo linguaggio</sample>
    <sample id="510">Il testo in inglese "Instruction Tuning on Multimodal Pre-trained Models" si traduce in italiano come "Afinamento di istruzioni su modelli pre-addestrati multi-modal".</sample>
    <sample id="511">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="512">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks Wang, Yichang, et al. “Benchmarking generalization via in-context instructions on 1,600+ language tasks.” arXiv preprint arXiv:</sample>
    <sample id="513">Multinstruct
Il primo dataset di istruzioni multimodale per l'addestramento
Visual-Relationship
Visual-Semantic
Visual-Referential
Visual-Question Answering
Visual-Reasoning
Visual-Textual
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event
Visual-Layout
Visual-Object
Visual-Scene
Visual-Activity
Visual-Event</sample>
    <sample id="514">Multinstruct
Il primo dataset di istruzioni multimodale per l'addestramento
Visual-Relationship
Visual-Semantic
Visual-Referential
Visual-Question Answering
Visual-Reasoning
Visual-Entailment
Visual-Common Sense
Visual-Textual Entailment
Visual-Semantic Entailment
Visual-Referential Entailment
Visual-Question Answering Entailment
Visual-Reasoning Entailment
Visual-Entailment Entailment
Visual-Common Sense Entailment
Visual-Textual Entailment Entailment
Visual-Semantic Entailment Entailment
Visual-Referential Entailment Entailment
Visual-Question Answering Entailment Entailment
Visual-Reasoning Entailment Entailment
Visual-Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment
Visual-Textual Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Question Answering Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Reasoning Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Common Sense Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Textual Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Semantic Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment
Visual-Referential Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment Entailment</sample>
    <sample id="515">OFA (One For All) • Un modello pre-addestrato multi-modal unitario che è capace di eseguire sia compiti di comprensione che di generazione con singoli o più modaliità. • OFA ha un vocabolario unitario • OFA coordinate un bounding box.</sample>
    <sample id="516">Figura 1: Esempi di istanze da MULTINSTRUCT per quattro compiti.</sample>
    <sample id="517">Figura 1: Esempi di istanze dal MULTINSTRUCT per quattro compiti.</sample>
    <sample id="518">Figura 1: Esempi di istanze da MULTINSTRUCT per quattro compiti. Input: Una descrizione del testo localizzabile. Output: La regione della descrizione che contiene il testo "dei". Input: Una regione dell'oggetto descritto. Output: La regione del fronte del treno. Input: Il contenuto dell'immagine. Output: Se la domanda è rilevante o no. Input: Il contenuto dell'immagine. Output: Se la domanda è rilevante o no.</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Multi-Modal Instruction Turning

Training Dataset Construction:
- Utilizziamo 53 task da 9 gruppi per il training.
- Scegliamo 10.000 istanze per task.

Testing Dataset Construction:
- Riserviamo l'intero gruppo di Reasoning Comune per il test.
- Selezioniamo ulteriormente 5 task da gruppi Q&amp;A eMiscellanei.
- Scegliamo casualmente 20 task dal gruppo di test del datasetNatural Instructionscome task sconosciuti per NLP.</sample>
    <sample id="521">Multi-Modal Instruction Turning

• Costruzione del dataset di addestramento:
    • Utilizziamo 53 task da 9 gruppi per l'addestramento.
    • Scegliamo 10.000 istanze per task.
• Costruzione del dataset di test:
    • Riserviamo tutto il gruppo di ragione comune per il test.
    • Selezioniamo ulteriormente 5 task dal gruppo di Q&amp;A e gruppi di rilevanza insignificante.
    • Utilizziamo tutte le istanze in test per ciascun task.
    • Scegliamo casualmente 20 task dal gruppo di test del dataset Natural Instructions come task sconosciute per NLP.

In basso, c'è un immagine di un uomo che spiega i punti principali del slide.</sample>
    <sample id="522">Dettagli sull'implementazione: • Dettagli di addestramento: - Modello pre-addestrato OFA-Large (472M) - Sostituire le istanze per tutti i compiti. - Ogni istanza è combinata casualmente con uno dei propri modelli di istruzioni. • Dettagli di test: - Per ogni compito, condurre un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento. - Rapportare la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="523">Dettagli sull'implementazione:

• Dettagli di addestramento:
- Modello pre-addestrato OFA-Large (472M)
- Selezionare le istanze per tutti i compiti.
- Ogni istanza è combinata casualmente con uno dei propri modelli di istruzioni.

• Dettagli di test:
- Per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento.
- Rapportiamo la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="524">Dettagli implementazione: • Dettagli di addestramento: o Modello pre-addestrato OFA-Large (472M) o Mix tutte le istanze per tutti i compiti. o Ogni istanza è combinata casualmente con uno dei propri modelli di istruzioni. • Dettagli di test: o Per ogni compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento. o Rapportiamo la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="525">Evaluazione dei metodi

Per i compiti di classificazione multi-modal (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, e Classificazione del tipo di disaster) reportiamo l'accuratezza.

Per i compiti di generazione multi-modal (Domande di senso comune, Domande a testo, Discorso basato su immagine, Estrazione visuale e dialogo visivo) reportiamo il Rouge-L.

Per i compiti di NLP, reportiamo il Rouge-L.

Inoltre, calcoliamo la performance aggregata per ogni modello basata sulla media del modello's performance su tutti i compiti multi-modal e NLP. Nonostante non avessimo solo la precisione come metrica,</sample>
    <sample id="526">Sensibilità Come sensibile il modello è verso la varietà delle istruzioni per la stessa attività:</sample>
    <sample id="527">Efficacia dell'addestramento dell'insezione su MULTINSTRUCT Tabella 1: Efficienza dell'addestramento dell'insezione su MultiTask. I modelli sono elencati in ordine di crescente efficienza, con i modelli migliori evidenziati in negra. I modelli OPA e Transfer Learning da NATURAL INSTRUCTIONS hanno le performance migliori, rispettivamente, con un punteggio di 56,60 ± 1,12 e 56,48 ± 0,93. Questi modelli hanno dimostrato una capacità superiore nell'elaborazione di informazioni visive e nella comprensione del linguaggio naturale.</sample>
    <sample id="528">Efficacia dell'addestramento dell'insezione su MULTINSTRUCT Tabella 1: Efficienza dell'addestramento dell'insezione su questioni a scelta multipla, domande a vettore continuo e ragionamento comune. I modelli migliori sono in evidenza. Tabella 2: Performanze dello zero-shot su domande a risposta a singola parola e microscopia. Le performance migliori sono in evidenza.</sample>
    <sample id="529">Impatto dell'aumento dei cluster di compiti di istruzioni multimodale

• Img Und
	+ VQA + Image Understanding
	+ Grounding
	+ MISC, ITM, Grounded Matching + Grounded Generation
	+ Temporal Ordering + Miscellanea + Image Text Matching
• Relation
	+ Visual Relationship
• Region
	+ Region Understanding
• NLP
	+ NLP tasks

Figura 3: Performance del modello come i cluster di compiti di istruzioni multimodale aumentano</sample>
    <sample id="530">Effetto delle istruzioni diverse sull'addestramento dell'instruzione

• OPA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.

Tabella 3: Effetto di diversi numero di istruzioni. Performance di OPA finetuned, addestrato su diverse quantità di istruzioni.

In questa presentazione, viene esaminato l'effetto delle istruzioni diverse sull'addestramento dell'instruzione. L'OPA (Open Pre-trained Assistant) è stato finetunato su 5 istruzioni e ha ottenuto un'efficacia aggregata molto alta in tutti i task di valutazione. Inoltre, ha dimostrato una sensibilità inferiore rispetto ad altri modelli. La tabella 3 illustra l'effetto del numero di istruzioni utilizzate per l'addestramento dell'instruzione.</sample>
    <sample id="531">Effetto delle strategie di finetuning sul sensibilità del modello

• L'instruzioni tuning su Multinstruct può ridurre in modo significativo la sensibilità del modello.
• La trasferimento di imparare da Natural Instructions dataset può ulteriormente ridurre la sensibilità del modello.

[Figura 4: Sensibilità del modello per compiti non visti. I numeri più bassi sono migliori.]

In questa immagine, si illustra l'effetto delle strategie di finetuning sul sensibilità del modello utilizzando un diagramma a barre. Le diverse strategie di finetuning sono rappresentate da colori diversi e le loro prestazioni sono indicate da valori numerici. Ad esempio, "OF" ha un valore di sensibilità di 40.58, mentre "OF_Agument" ha un valore di sensibilità di 13.84. Questi valori rappresentano la sensibilità del modello per compiti non visti, con i valori più bassi considerati migliori.</sample>
    <sample id="532">Efficienza zero-Shot su compiti NLP

- Addestramento con istruzioni su Multilinstruct può migliorare l’efficienza zero-Shot su compiti NLP non visti.
- La strategia di impara-trasferimento Mixedinstruct è la migliore per preservare la capacità zero-Shot ottenuta sul dataset Natural Instructions.

Modello | Rouge-L |
---------|---------|
OPA     | 2.25    |
OFA     | 1.16    |
OFA+Multilinstruct | 4.61 |
OFA+Naturallinstruct | 30.79 |
OFA+Mixedinstruct | 30.79 |

Tabella 4: Efficienza zero-Shot su compiti NLP. Le prestazioni senza zero-Shot su Rouge-L e le migliori prestazioni sono in grassetto.</sample>
    <sample id="533">Conclusione

• Primo insieme di addestramento multi-modal a scala grande.
- Contiene 62 compiti multi-modal da 10 category diverse.
• Migliora notevolmente la capacità zero-shot dell'OFA attraverso l'addestramento con istruzioni.
• Esplora diversi metodi di immissione del modello e ne dimostra i benefici.
• Designa una nuova metrica di sensibilità.</sample>
    <sample id="534">Un'altra cosa! Stiamo raccolgendo un insieme di addestramento multimodale con circa 150 attività in più visione-lingua e li rilascieremo presto.</sample>
    <sample id="535">L'articolo è affiliato all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="536">La relatrice o il relatore è Mohammed Javad Hosseini.</sample>
    <sample id="562">I giudizi di accettabilità dei modelli di linguaggio non sono sempre robusti al contesto ACL 2023 Johns Hopkins University Purdue University MIT Meta</sample>
    <sample id="563">I giudizi sulla accettabilità dei modelli di linguaggio non sono sempre robusti al contesto ACL 2023 Johns Hopkins University Purdue University MIT Meta</sample>
    <sample id="564">Revisiting Minimal Pair Paradigm Evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLIMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical 2. helping themselves. 2. "The customer ... spent any money." 2. Stereotypical 2. Many people were helping herself. P(1) &gt; P(2) P(1) &gt; P(Any) P(1) &gt; P(2)</sample>
    <sample id="565">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.

BLMP SyntaxGym CrowS

1. Many people were helping themselves.
2. Many people were helping herself.

P(1) &gt; P(2)

1. No customer has spent any money.
2. The customer has spent any money.

P(1) &gt; P(2)

1. Stereotypical sentence.
2. Non-stereotypical sentence.

P(1) &gt; P(2)</sample>
    <sample id="566">Revisitando il paradigma del paio minimo

Evaluazioni del modello di linguaggio utilizzano differenze relative in sequenza probabilità per valutare la conoscenza astratta dei modelli di LMs.

BLMP | SyntaxGym | CrowS
1. Molti persone erano aiutate da se stesse 1. Nessun cliente ha spento soldi 1. Sentenza stereotipata
2. Molti persone erano aiutate da se stesse 2. Il cliente ha spento soldi 2. Sentenza non stereotipata</sample>
    <sample id="567">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.

BLMP SyntaxGym CrowS

1. Many people were helping themselves.
2. Many people were helping herself.

P(1) &gt; P(2)

1. No customer has spent any money.
2. The customer has spent any money.

P(1) &gt; P(2)

1. Stereotypical sentence.
2. Non-stereotypical sentence.

P(1) &gt; P(2)</sample>
    <sample id="568">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.

BLMP SyntaxGym CrowS

1. Many people were 1. No customer ... has spent 1. Women are
helping themselves. 2. The customer ... has spent any money. 2. Terrible at
2. Many people were helping themselves. any money. Men are terrible
at handwork.

P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2)

Are these judgments stable with long preceding context?</sample>
    <sample id="569">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.

BLMP SyntaxGym CrowS

1. Many people were 1. No customer ... has spent 1. Women are
helping themselves. 2. "The customer ... has spent any money. 2. Terrible
2. Many people were helping herself. any money." Men are terrible
at handwork. at handwork.

P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2)

Are these judgements stable with long preceding context?</sample>
    <sample id="570">Revisiting Minimal Pair Paradigm

Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.

BLMP SyntaxGym CrowS

1. Many people were 1. No customer ... has spent 1. Women are
helping themselves. 2. "The customer ... has spent any money. 2. Terrible at
2. Many people were helping herself. any money." 2. Men are terrible
at handwork.
P(1.) &gt; P(2.) P(1.any) &gt; P(2.any) P(1.) &gt; P(2.)

Are these judgements stable with long preceding context?</sample>
    <sample id="571">Approccio Testare se i giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilita'</sample>
    <sample id="572">Approccio Testare se i giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement P (accettabile | Prefix &gt;) &gt; P (accettabile | Prefix &lt;) Space of Candidate Prefixes Who might Rose fees from before returning to this customer? What could Jessica sell believing these spotlights? What Aaron sounded like while before returning to this customer before this customer before k?</sample>
    <sample id="573">Approccio Testare se le valutazioni del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement P (accettabile | Prefix -&gt; ?) &gt; P (accettabile | Prefix) Space of Candidate Prefixes Who might Rose fees from before returning to this customer? What could Jessica sell believing these spotlights? What had Aaron sounded like while before returning to this customer before this customer before k?</sample>
    <sample id="574">Approccio Testare se i giudizi del MPP variano in funzione della lunghezza del contesto, corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement P (accettabile | Prefix &gt;) &gt; P (accettabile | Prefix) Space of Candidate Prefixes Who might Rose fees from before returning to this customer? What could Jessica sell without noticing these spotlights? What had Aaron sounded like the white spotlights? What had Aaron sounded like the white spotlights before returning to this customer before k?</sample>
    <sample id="575">Proccurso Testare se le valutazioni del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità</sample>
    <sample id="576">Approccio Testare se le giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilita'</sample>
    <sample id="577">Approccio Testare se le valutazioni del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità. Test Suite: Subject-Verb Agreement P (Prefix) &gt; P (Prefix) = &gt; P (Prefix) = &gt; P (Prefix) Space of Candidate Prefixes Soggetto - Verbo Concordanza * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *</sample>
    <sample id="578">Approccio Testare se le giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement P (Prefix &gt; Prefix) = Unacceptable Space of Candidate Prefixes: Random Prefixes Random Words Random Syntactic Effects Who might Rose fee from before returning to this customer? Who might Rose fee from before returning to this customer? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars. A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars.</sample>
    <sample id="579">Proccurso Testate se i giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilità Test Suite: Subject-Verb Agreement P (Prefix &gt; Prefix) = P (Prefix &gt; Prefix) Space of Candidate Prefixes: Random Effects: Soggetto-Aggettivo: * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *</sample>
    <sample id="580">Proccurso Testare se le giudizi del MPP variano in funzione della lunghezza del contesto, della corrispondenza strutturale e accettabilita'</sample>
    <sample id="581">MPP giudizi sono robusti per lunghezze di contesto diverse. Abbiamo effettuato valutazioni MPF con contesti accettabili/accettabili; corrisposti/non corrisposti struttura - lunghezze fino a 900 token.</sample>
    <sample id="582">I giudizi del MPP sono robusti per lunghezze di contesto diverse. Abbiamo eseguito valutazioni del MPP con contesti accettabili/accettabili; corrisposti/non corrisposti struttura - lunghezze fino a 900 token.</sample>
    <sample id="583">Evaluations of different MPPI strategies on the task of judging whether a sentence is acceptable or unacceptable in the context.</sample>
    <sample id="584">Evaluazione delle sentenze MPP accettabili/non accettabili in contesti diversi – accettabili/accettabili; corrisposti/non corrisposti struttura – lunghezza fino a 900 token.</sample>
    <sample id="585">Evaluazioni del MPP accettabili/accettabili in contesti diversi – accettabili/accettabili; corrisposti/non corrisposti strutture – lunghezze fino a 900 token.</sample>
    <sample id="586">Accettabili/Non accettabili MPP sentenze con strutture corrisposte più severamente colpiti We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2</sample>
    <sample id="587">Accettabili/Non accettabili MPP sentenze con strutture corrisposte più severamente colpiti

Effettuiamo valutazioni di MPP con contesti accettabili/non accettabili; strutture corrisposte/simili non accettabili – lunghezze fino a 900 token

0.2 0.2 0.2 0.2
-0.4 -0.4 -0.4 -0.4
-0.6 -0.6 -0.6 -0.6
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.8 -0.8 -0.8
-0.8 -0.</sample>
    <sample id="588">Accettabili/Non accettabili MPP sentenze con strutture corrisposte più severamente colpiti modello</sample>
    <sample id="589">Per perturbare i contesti delle frasi in modo che si preservi la struttura rilevante e chiedere se i modelli sono sensibili in modo simile a queste frasi.</sample>
    <sample id="590">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. • Prefix/suffix adverbs: "However, &lt;sent&gt;." • Long prefix adverbs: "First and foremost, &lt;sent&gt;." • Add Clause: "Regardless of what X thinks about it, &lt;sent&gt;." • Quote: "Yesterday, X said, '&lt;sent&gt;'."</sample>
    <sample id="591">Per perturbare i contesti delle frasi in modo che si preservi la struttura rilevante e chiedere se i modelli sono sensibili in modo simile a queste. • Adesivi prefisso/suffisso: "Tuttavia, &lt;sente&gt;" • Adesivi prefisso lunghi: "Anche se, &lt;sente&gt;." • Aggiungere clausole: "&lt;sente&gt; Tuttavia," • Citata: "Ieri, X ha detto '&lt;sente&gt;'"</sample>
    <sample id="592">Per perturbare le frasi contesto in modo che si preservi la struttura rilevante e chiedere se i modelli sono sensibili allo stesso modo.</sample>
    <sample id="593">Per perturbare le frasi contesto in modo che si preservi la struttura rilevante e chiedere se i modelli sono sensibili allo stesso modo. • Sostituzioni prefissi/suffissi: "Tuttavia, &lt;senta&gt;", • Adversari prefissi: "&lt;senta&gt; ". • Aggiungere: "Secondo X, pensa che sia &lt;senta&gt;" • Citazione: "Ieri ha detto &lt;senta&gt;" • Maniera:</sample>
    <sample id="594">Key Takeaways • Modeli di linguaggio sono sensibili a caratteristiche sintattiche/semantiche condivise tra le frasi • Frasi valutate con input brevi, singoli-sentenze, non possono completamente catturare LM 's' conoscenza astratta.</sample>
    <sample id="595">Key Takeaways • Modeli di linguaggio sono sensibili a caratteristiche sintattiche/semantiche condivise tra le frasi. • Valutazioni delle frasi con input brevi, singoli-sentenziali non riescono a pienamente catturare i LM 's' conoscenza astratta.</sample>
    <sample id="596">Key Takeaways • Modeli di linguaggio sono sensibili a caratteristiche sintattiche/semantiche condivise tra le frasi • Frasi valutate con input brevi, singoli - i modelli non riescono ad afferrare completamente i LM 's' - conoscenza astratta.</sample>
    <sample id="597">The first step of the method maps the input tokens to a Tag token.</sample>
    <sample id="598">55,000 scripts rappresentano Coscript.</sample>
    <sample id="626">Il miglior metodo di allineamento per DEplain è BERTalign.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato allevia la difficoltà di annotazione.</sample>
    <sample id="628">The documents in DEplain-web have been aligned with both manual and automatic alignment methods.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato utilizzando il dataset CoNLL 2003.</sample>
    <sample id="630">XSemPLR: Analisi Semantica Cross-Lingua in Più Linguaggi Naturali e Rappresentazioni del Significato

Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

PennState amazon</sample>
    <sample id="631">XSemPLR: Analisi Semantica Cross-Lingua in Più Linguaggi Naturali e Rappresentazioni del Significato

Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

PennState amazon</sample>
    <sample id="632">Cross-lingual Semantic Parsing è un compito di tradurre query in più lingue naturali in rappresentazioni di significato multipli.</sample>
    <sample id="633">Cross-lingual Semantic Parsing è un任务 di tradurre query in più lingue naturali in rappresentazioni di significato multipli.</sample>
    <sample id="634">Cross-lingual Semantic Parsing

• Modeli esistenti di CLSP sono proposti e valutati separatamente su dataset di task e applicazioni limitate. Ad esempio:
    - Mancanza di copertura su certi linguaggi naturali

English | German | Chinese
--- | --- | ---
Neural Models | SQL | Lambda
FunQL</sample>
    <sample id="635">Cross-lingual Semantic Parsing

* Modelli esistenti di CLSP sono separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio:
    * Mancanza di copertura su determinate lingue naturali
English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="636">Cross-lingual Semantic Parsing

• Modeli esistenti di CLSP sono separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio:
    - Mancanza di copertura su determinate rappresentazioni del significato
English | German | Chinese
Neural Models | SQL | Lambda | FunQL</sample>
    <sample id="637">Cross-lingual Semantic Parsing

I modelli esistenti di CLSP sono separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio:

• Mancanza di copertura su determinate rappresentazioni del significato

English | German | Chinese
:--- | :--- | :---
Neural Models | SQL | Lambda
FunQL</sample>
    <sample id="638">Cross-lingual Semantic Parsing

• Modeli esistenti di CLSP sono separatamente proposti e valutati su dataset di task e applicazioni limitati. Ad esempio:
    - Mancanza di copertura su certi modelli neurali
English | German | Chinese
Single Model | SQL | Lambda | FunQL</sample>
    <sample id="639">XSemPLR

• For cross-lingual semantic parsing, we provide a unified dataset XSemPLR
o 9 datasets in various domains
o 5 semantic parsing tasks.
o 22 natural languages in 15 language families

English | Spanish | French | German | Italian | Japanese | Chinese | Russian | Arabic | Persian | Hindi | Portuguese | Turkish | Polish | Ukrainian | Romanian | Swedish | Finnish | Danish | Norwegian | Czech | Hungarian | Chinese (Traditional) | Thai | Vietnamese | Indonesian | Malaysian | Filipino | Korean | Chinese (Simplified) | Arabic (Dialect) | Hebrew | Russian (Cyrillic) | German (Fraktur) | Greek | Turkish (Cyrillic) | Persian (Arabic script) | Urdu | Pashto | Balochi | Saraiki | Sindhi | Punjabi | Kashmiri | Bengali | Nepali | Marathi | Gujarati | Sinhala | Telugu | Kannada | Malayalam | Tamil | Malay | Burmese | Khmer | Lao | Burmese (Myanmar) | Javanese | Sundanese | Madurese | Betawi | Betawi (Javanese script) | Minangkabau | Bugis | Makassarese | Sunda | Madakaripura | Betawi (Old Javanese) | Old Javanese | Old Javanese (Bali script) | Old Javanese (Madakaripura script) | Old Javanese (Kedah script) | Old Javanese (Sundanese script) | Old Javanese (Betawi script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old Javanese script) | Old Javanese (Old J</sample>
    <sample id="640">XSemPLR

• Foriamo un insieme di dataset XSemPLR per la parsing semantica transversale in linguaggi naturali diversi e rappresentazioni del significato.
    • 9 set di dataset in vari dominii
    • 5 compiti di parsing semantici diversi
    • 22 lingue naturali in 15 famiglie linguistiche
• Dati esempio:

    • Inglese: "What players played less than three times a season?"
    • Cinese: "我们队里有球员吗？"
    • Tedesco: "Wir spielen im Restaurant mit Pizzas."
    • Giapponese: "日本語で、パリでレストランにご飯を食べますか？"
    • Farsi: "کارمندی کار کرده است؟"
    • Arabico: "العربية: هل ت Works في المطعم؟"
    • Hindi: "अगर किसी को अधिकांश समय में तीसरे स्वर्ण के बाद खेलने के लिए आकर्षक होता है?"
    • Urdu: "اگر کسی کو کم از سی اور آسیس کے پس کوئل کرنا چاہئے، تو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس کو کس ک</sample>
    <sample id="641">Experiment Settings

* Consideriamo sei impostazioni per il training e l'evaluation.
* Translate-Test: Utilizziamo l'API di traduzione Google per tradurre il linguaggio di origine verso il linguaggio di destinazione. Poi utilizziamo un modello monolingue per il training e l'evaluation.

Training

English Model
SQL

Inference

German
Translate API
English Model
SQL</sample>
    <sample id="642">Experiment Settings

* Consideriamo sei impostazioni per il training e l'evaluation.
* Translate-Test: Utilizziamo l'API di traduzione Google per tradurre il linguaggio di origine verso il linguaggio di destinazione. Poi utilizziamo un modello monolingue per il training e l'evaluation.

Training

English Model
English Model
SQL

Inference

German
Translate API
English Model
SQL</sample>
    <sample id="643">Experiment Settings

* Consideriamo sei impostazioni per la formazione e l'evaluatione.
* **Translate-Test**: Utilizziamo l'API di traduzione Google per tradurre il linguaggio di origine verso il linguaggio di destinazione. Poi utilizziamo un modello monolingue per la formazione e l'evaluatione.

Training

* Iniziamo con un modello in inglese.
* Utilizziamo il modello in inglese per formare e valutare il linguaggio SQL.

Inference

* Iniziamo con un modello in tedesco.
* Utilizziamo l'API di traduzione per tradurre il modello in inglese.
* Utilizziamo il modello in inglese per formare e valutare il linguaggio SQL.</sample>
    <sample id="644">Experiment Settings • We consider the six settings for training and evaluation. • e.g., German-to-German: Source language is the same as target language. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training Monolingual Model: German Model SQL Inference German Model SQL</sample>
    <sample id="645">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training Monolingual Model (Few-shot) German Model SQL Inference German Model SQL</sample>
    <sample id="646">Experiment Settings • We consider the six settings for training and evaluation. • Source language is the same as target language, e.g. German-to-German. We also test Monolingual Few-shot by training monolingual models with only 10% training data. Training German Model SQL Monolingual Model (Few-shot) Inference German Model SQL</sample>
    <sample id="647">Consideriamo sei impostazioni per il training e l’evaluation. Multilingua Model: Addestra un modello multilingua per tutti i linguaggi.</sample>
    <sample id="648">Consideriamo i due ambienti di addestramento e valutazione. Multilingua Model: Addestra un modello multilingua per tutti i linguaggi.</sample>
    <sample id="649">Consideriamo sei impostazioni per il training e l'evaluation. Multilingua Model: Addestrare un modello multilingua per tutte le lingue.</sample>
    <sample id="650">Experiment Settings

• We consider the six settings for training and evaluation.
• Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language.

Training

English
German
Multilingual Model
SQL

Inference

English
German
Few-shot
Multilingual Model
SQL</sample>
    <sample id="651">Experiment Settings

• We consider the six settings for training and evaluation.
• Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language.

Training

English
English
German
Few
Multilingual Model
SQL

Inference

English
German
Multilingual Model
SQL</sample>
    <sample id="652">Analisi del contesto monolingue • Abbiamo valutato due gruppi di modelli in un contesto monolingue: Enc-PTR: Encoder preaddestrati multilingue con decoder basati su puntatori Enc-R: Encoder + decoder preaddestrati multilingue • Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i dataset!</sample>
    <sample id="653">Analisi dei modelli monolingui • Abbiamo valutato due gruppi di modelli con encoder multilingue preallenati e decoder basati su puntatori • XLM-R + mBERT + PTR • Enc-Dec (mT5) • mBART, mT5 • Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i dataset!</sample>
    <sample id="654">Analisi del modello monolingue: Abbiamo valutato due gruppi di modelli in un contesto di imposizione monolingue. Uno dei gruppi comprende i modelli Enc-PTR e XLM-R + mBERT + PTR, mentre l'altro comprende i modelli Enc-Dec e mBART + mT5. Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i dataset.</sample>
    <sample id="655">Analisi dei modelli monolingui • Abbiamo valutato due gruppi di modelli su un contesto di imposizione monolingua: Enc- PTR: Encoder multilingue preaddestrati con decoder basati su puntatori Enc-Dec: Encoder e decoder preaddestrati monolingue • Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i dataset!</sample>
    <sample id="656">Analisi di addestramento multilingue

In questa immagine, si discende l'analisi dell'addestramento multilingue per le tecniche mT5 e XLM-R + PTR. L'obiettivo è valutare queste tecniche su un insieme multilingue di dataset. La tabella riportata nella figura evidenzia i performance delle tecniche mT5 e XLM-R + PTR in diverse lingue.

La tabella è suddivisa in due sezioni principali: "Monolingue" e "Multilingue". Nella sezione "Monolingue", le performance sono misurate in diverse lingue come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar). In questa sezione, le performance migliori sono registrate in lingue come English (Eng) e French (Fra).

Nella sezione "Multilingue", le performance sono mesurate in un insieme di lingue. In questa sezione, le performance migliori sono registrate in lingue come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar).

Inoltre, la tabella include anche le performance delle tecniche mT5 e XLM-R + PTR in lingue specifiche come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar). Le performance migliori in questa sezione sono registrate in lingue come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar).

Infine, la tabella include anche le performance delle tecniche mT5 e XLM-R + PTR in lingue specifiche come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar). Le performance migliori in questa sezione sono registrate in lingue come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar).

In conclusione, l'immagine fornisce una panoramica dettagliata dell'addestramento multilingue per le tecniche mT5 e XLM-R + PTR. Le performance migliori sono registrate in lingue come English (Eng), French (Fra), German (De), Italian (It), Spanish (Esp), Russian (Rus), Chinese (Chn), Japanese (Jpn), Korean (Kor) e Arabic (Ar).</sample>
    <sample id="657">Analisi dell'addestramento multilingue

In questa immagine, si discutono i risultati di un esperimento sull'addestramento multilingue. L'immagine include una tabella con le performance di diversi modelli linguistiche su diverse lingue. La tabella è suddivisa in colonne per le lingue e righe per i modelli.

Il modello m15 ha le migliori performance generali, con punteggini molto alti in molte lingue. Il modello XLm-R + PTR ha anche delle performance migliori rispetto ai modelli monolingue, ma non tanto quanto il modello m15. Il modello Enc-Dec+Enc-PTR ha le peggiori performance, specialmente in lingue che non sono state addestrate direttamente.

In generale, l'addestramento multilingue sembra migliorare le performance dei modelli linguistiche, ma non sempre in modo uniforme su tutte le lingue.</sample>
    <sample id="658">Analisi del Training Multilingue

Iniziamo con un slide che introduce l'argomento dell'analisi del training multilingue. Il slide è intitolato "Analysis of Multilingual Training" e include un riassunto conciso delle idee principali presentate. L'analisi si concentra sull'evaluation di mT5 e XLM-R + PTR su un setting multilingue. La maggior parte dei modelli NL obtiene un miglioramento delle prestazioni, tranne in 3 dataset, dove le prestazioni in inglese diminuiscono in 7 dataset. Questo fenomeno è noto come "Curse of Multilinguality".

Il slide include anche un grafico che rappresenta il numero di dataset per diverse lingue. Le lingue analizzate includono inglese (en), tedesco (de), cinese (zh), greco (el), francese (fr), italiano (it), spagnolo (es), ebraico (he). Il grafico utilizza colori diversi per rappresentare l'incremento o la diminuzione delle prestazioni. I colori rossi rappresentano l'aumento delle prestazioni, mentre i colori grigi rappresentano una diminuzione.

Il slide è firmato da Viet Anh Nguyen, Ph.D., Assistant Professor, Department of Computer Science and Engineering, University of California, San Diego. Il slide è numerato come 14, suggerendo che sia parte di una presentazione più ampia.

Il slide è stato tradotto in italiano da un traduttore umano.</sample>
    <sample id="659">Analisi del Training Multilingue

Iniziamo con un slide che introduce l'argomento dell'analisi del training multilingue. Il slide è intitolato "Analysis of Multilingual Training" e include un diagramma a barre che rappresenta il numero di dataset per diverse lingue. L'elenco delle lingue include: inglese (en), tedesco (de), zhinese (zh), thailandese (th), francese (fr), italiano (it), ebraico (he). Il diagramma utilizza colori diversi per rappresentare l'aumento o la riduzione del numero di dataset.

Il testo sullo slide riporta che si è valutato mT5 e XLM-R + PTR in un contesto multilingue. In generale, i principali NLs hanno ottenuto un miglioramento delle prestazioni, tranne per l'inglese che ha registrato una diminuzione in 7 dataset, noto come "Curse of Multilinguality".</sample>
    <sample id="660">Cross-lingual Performance Gap

* **Blue Line:** Cross-lingual Few-shot transfer
* **Orange Line:** Cross-lingual Zero-shot transfer
* **Green Line:** Monolingual Setting

Gequery MTOP SchemaQA Overnight

NLMaps MCWQ ATIS

Gequery/lamb Gequery/prolog Gequery/funql Gequery/sql Spider</sample>
    <sample id="661">Cross-lingual Performance Gap

* **Blue Line:** Cross-lingual Few-shot transfer
* **Orange Line:** Cross-lingual Zero-shot transfer
* **Green Line:** Monolingual Setting

Geography MTOP SchemaQA Overnight NLMaps MCWQ Gequery/lambda Gequery/prolog Gequery/funql Gequery/sql Spider ATIS</sample>
    <sample id="662">Cross-lingual Performance Gap

* Azione: Cross-lingual Few-shot transfer
* Azione: Cross-lingual Zero-shot transfer
* Azione: Monolingual Setting

Geography MTOP SchemaQA Overnight NLMaps MCWQ Gequery/lambda Gequery/prolog Gequery/funql Gequery/sql Spider ATIS</sample>
    <sample id="663">Risultati e scopri (Sezione 4 del articolo)

- Enc-Dec (m75) supera i precedenti studi o raggiunge risultati paragonabili.
- Pre-addestramento su NL inglese può in modo significativo migliorare le prestazioni su NL di destinazione.
- I modelli multilingue (Codex &amp; BLOOM) sono ancora inadeguati per il parsing interlingua.
- La trasferimento di impara in cinese e addestramento monolingue inglese (En -&gt; En) ha il maggior gap di prestazione, mentre l'addestramento monolingue tedesco (German) ha generalmente il minore.
- FunQL supera le altre rappresentazioni di significato e SQL ottiene le peggiori prestazioni.</sample>
    <sample id="664">Altri Risultati e Trovamenti (Sezione 4 del Paper)

• Enc-Dec (m75) supera precedenti opere o raggiunge risultati paragonabili.

• Pre-addestramento su NL inglese può in modo significativo migliorare le prestazioni di pochi esempi su NL di destinazione.

• I modelli multilingui LLMs (Codex &amp; BLOOM) sono ancora inadeguati per la traduzione tra linguaggi.

• La trasferimento di imparare cinese e addestramento monolingue inglese (En -&gt; En) ha il maggior gap di prestazione, mentre l'inglese tedesco di solito ha il più piccolo.

• FunQL supera le altre rappresentazioni di significato e SQL ottiene le peggiori prestazioni.</sample>
    <sample id="665">Conclusione

- Abbiamo costruito XSemPLR, un benchmark unificato per la analisi semantica multilingue con diverse lingue naturali e rappresentazioni di significato.
- Abbiamo condotto un studio di benchmark esaustivo su tre tipi rappresentativi di modelli di linguaggi multilingue.
- I nostri risultati hanno dimostrato che mT5 con addestramento monolingue produce il miglior prestigio, mentre i modelli multilingue automatici sono ancora inadeguati per le attività di analisi semantica multilingue. Inoltre, la differenza di prestazione tra addestramento monolingue e trasferimento multilingue è ancora significativa.

[Traduzione da English a Italian]</sample>
    <sample id="666">Conclusione - Abbiamo costruito XSemPLR, un benchmark unificato per la analisi semantica multilingue con diverse lingue naturali e rappresentazioni di significato. - Abbiamo condotto un'analisi esaustiva su tre tipi rappresentativi di modelli di linguaggi multilingue. - I nostri risultati hanno dimostrato che mT5 con addestramento monolingue produce i migliori risultati, mentre gli LM multilingue sono ancora inadeguati per le attività di analisi semantica multilingue. Inoltre, il gap di prestazione tra l'addestramento monolingue e il trasferimento multilingue è ancora significativo.</sample>
    <sample id="667">I lavori connessi in tal senso sono: "Parameter-based watermark [1, 2]", "Lexical watermark [3, 4]" e "Adversarial-based watermark [6]".</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP.</sample>
    <sample id="695">Il metodo affronta l'ambiguità delle permutazioni utilizzando un agent.</sample>
    <sample id="696">L'equità di un modello NLP a valle è definita come la capacità del modello di fornire predizioni accurate e pertinenti per le diverse classi etniche.</sample>
    <sample id="697">Il nome del relatore o della relatrice non è specificato nella descrizione fornita.</sample>
    <sample id="698">Non è specificato il nome del relatore o della relatrice.</sample>
    <sample id="699">La relatrice o il relatore è Myra Cheng.</sample>
    <sample id="700">Il tropicalismo indica che si definiscono solo i gruppi in base alla loro identità.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target definendoli solo in base alla loro identità e fornendo esempi di come vengono percevuti.</sample>
    <sample id="702">P-CXMI</sample>
    <sample id="703">La differenza tra DrBERT e ChuBERT è che DrBERT è un modello pre-addestrato da zero, mentre ChuBERT è un modello pre-addestrato su un dataset specifico.</sample>
    <sample id="751">Per rispondere alla domanda di quanti autori sono coinvolti nell'articolo, analizziamo la descrizione fornita. L'articolo è intitolato "MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning" e i nomi dei tre autori sono elencati come "Zhiyang Xu*, Ying Shen*, Lifu Huang". 

Passo 1: Identificare il numero di autori elencati.
- Autore 1: Zhiyang Xu
- Autore 2: Ying Shen
- Autore 3: Lifu Huang

Passo 2: Contare il numero di autori elencati.
- Ci sono 3 autori elencati.

Risultato: Ci sono 3 autori coinvolti nell'articolo.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un modello che utilizza i modelli precedenti per migliorare i modelli successivi.</sample>
    <sample id="753">Il set di dati mira a risolvere le espressioni di riferimento indiretti.</sample>
    <sample id="754">Un utente malevolo può estrarre i parametri del modello utilizzando un EaaS.</sample>
    <sample id="755">Per fornire una risposta accurata, analizziamo l'immagine descritta. L'immagine sembra essere un slide di presentazione con il seguente testo:

"Attention as a Guide for Simultaneous Speech Translation
Sara Papi, Matteo Negri, Marco Turchi"

In base a queste informazioni, possiamo dedurre che ci sono tre autori coinvolti nell'articolo o presentazione.

Ragione: Il testo elenca i nomi di tre individui, che probabilmente rappresentano gli autori principali dell'articolo o del progetto presentato.

Risposta: Ci sono tre autori coinvolti nell'articolo o presentazione.</sample>
    <sample id="756">Per creare il set di dati iniziale, sono stati impiegati 8 annotatori.</sample>
    <sample id="757">I ricercatori sono affiliati a diverse istituzioni.</sample>
    <sample id="758">Il primo esempio in cui il governatore è a sinistra è "I saw Bart and Lisa Homer came and sneezed".</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo sono ABC-Eval.</sample>
    <sample id="760">Per stabilire se i giudizi sono stabili con un contesto precedente lungo.</sample>
    <sample id="761">La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">No, gli annotatori non conoscono l'entità in anticipo.</sample>
    <sample id="763">La valutazione è stata effettuata utilizzando la metrica di BLEU.</sample>
    <sample id="764">Yes, it does.</sample>
    <sample id="765">La posizionalità nella NLP è importante per la comprensione del contesto e della struttura grammaticale di un testo o di una frase.</sample>
    <sample id="766">LLM multilingue come BLOOM sono stati adattati tramite adattatori o con una messa a punto integrale.</sample>
    <sample id="767">Per il trasferimento dell'apprendimento, si ricorre al modello RoBERTa-base.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM sono il WMT’21 tedesco-inglese e il WMT’21 inglese-tedesco.</sample>
    <sample id="769">To determine the number of suggestions proposed by the authors at the end, we need to carefully examine the content provided in the image. The image lists several points under the heading "Recommendations." Let's break down the suggestions:

1. Addressing positive stereotypes and essentializing narratives
2. An intersectional lens
3. Transparency about bias mitigation

By counting these distinct suggestions, we can conclude that there are three recommendations presented in the image. Therefore, the answer is 3.</sample>
    <sample id="770">Il guadagno del nuovo metodo è del 15,45% rispetto al metodo di riferimento.</sample>
    <sample id="771">Shuheng Liu, Alan Ritter</sample>
    <sample id="772">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="773">To determine the number of smaller models used in the article, we need to carefully examine the provided image. The image shows a bar chart comparing the accuracy of different language models (LMs). The models listed are:

1. GPT-3 (175B)
2. Codex (175B)
3. InstructGPT (175B)
4. T5 trained on wikiHow
5. T5 trained on Coscript

Among these models, the ones that can be considered as "smaller" are those with fewer parameters or less training data. Based on the context and typical definitions, the models with fewer parameters are often referred to as "smaller" LMs.

From the list:
- GPT-3 (175B) and Codex (175B) are both large models.
- InstructGPT (175B) is also a large model.
- T5 trained on wikiHow and T5 trained on Coscript are smaller models, as they are based on the T5 architecture which is generally considered smaller compared to the 175B models.

Therefore, the smaller LMs used in the article are:
1. T5 trained on wikiHow
2. T5 trained on Coscript

Thus, there are 2 smaller models used in the article.</sample>
    <sample id="774">OFA (One For All)</sample>
    <sample id="833">I fornitori di Google sono gli autori dell'articolo.</sample>
    <sample id="834">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Stony Brook e all'Institute for Language, Brain and Cognition.</sample>
    <sample id="835">The article analyzes translation capabilities between English and French, as well as English and German.</sample>
    <sample id="836">Shangbin Feng, Chan Young Park, Yuhuan Liu, Yulia Tsvetkov</sample>
    <sample id="837">I modelli che sono stati studiati durante gli esperimenti sono: SARI, BLEU, BS-P e FRE.</sample>
    <sample id="838">Per scopi di addestramento e test, 57 delle 62 attività diverse utilizzate in MultiInstruct vengono utilizzate.</sample>
    <sample id="839">Per determinare quanti autori sono coinvolti nell'articolo, analizziamo la sezione di testo fornita. L'articolo è intitolato "DEPLAIN: A German Parallel Corpus with Intra lingual Translations into Plain Language for Sentence and Document Simplification". 

La sezione di autori è presentata come segue:
- Regina Stodden
- Omar Momen
- Laura Kallmeyer

In questa sezione, ci sono tre nomi elencati come autori dell'articolo.

Rispondo alla domanda: Ci sono 3 autori coinvolti nell'articolo.</sample>
    <sample id="840">The authors conducted tests on the AG News, MIND, SST2, Enron Spam, and WikiText datasets.</sample>
    <sample id="876">NACHOS is a pre-training strategy used in the evaluation of 13 models on 11 tasks.</sample>
    <sample id="877">Il relatore è George Foster.</sample>
    <sample id="878">La strategia del prompting ha un impatto significativo sui risultati.</sample>
    <sample id="879">I fornitori dell'articolo sono affiliati a Carnegie Mellon University, Language Technologies Institute, Technico Lisboa, BAIR e Unbabel.</sample>
    <sample id="880">Le 5 istruzioni scritte da esperti sono:</sample>
    <sample id="881">Gli autori proppongono di utilizzare il test suite KITMUS per testare i modelli sull'utilizzo di informazioni provenienti da più fonti.</sample>
    <sample id="882">Prompting PalM per la traduzione: Strategie e prestazioni

Presentazione del 2023 del Congresso Annuale dell'ACL (ACL 2023)

Il slide introduce il concetto di "Prompting PalM per la traduzione" e si concentra su "Strategie e prestazioni". L'immagine Google è visibile in un angolo, mentre un emoji felice con un pensieroso sguardo è posizionato nell'angolo opposto. In un'altra parte del slide, c'è un'immagine di una spiaggia paradisiaca con palmi e un sole radiante.

Il slide include anche i nomi dei membri del team che hanno collaborato alla creazione di questa presentazione:

1. David Vil Torres
2. Markus Freitag
3. Colin Cherry
4. Jiaming Luo
5. Vineer Ratnaker
6. George Foster

Questi nomi sono elencati in ordine di apparizione sul slide.</sample>
    <sample id="883">PalM: Pathways Language Model

- Chowdery et al., 2022
- arXiv:2204.02311.
- Trained on 780B tokens.
- Densely activated.
- 6144 TPU v4 chips.
- SOTA in hundreds of NLU and Generation benchmarks.
- 10 miliardi di parametri

[Immagine: Diagramma di un albero con colori diversi e un avatar di un uomo]

Google</sample>
    <sample id="884">PalM: Modello di Lingua per i Percorsi

- Chowdery et al., 2022
- arXiv:2204.02311.
- 540B parametri.
- Addestrato su 780B token.
- Attivati densamente.
- 6144 TPU v4 chip.
- Record SOTA in centinaie di benchmark.
- 62 miliardi di parametri.

[Il diagramma rappresenta un albero con diverse sfere colorate che rappresentano diverse funzionalità del modello, come "Risoluzione delle domande", "Codifica e completamento", "Traduzione", "Riassumimento", "Comprensione del linguaggio" e "Generazione".]

Google</sample>
    <sample id="885">Il contributo principale di questa presentazione riguarda la prima analisi sistematica delle tecniche di stimolazione dei modelli di immissione (LLM prompting) per la traduzione automatica (MT). L'approccio utilizza sia una pool di candidate che strategie di selezione per valutare le capacità di traduzione con le migliori pratiche della comunità di traduzione automatica. Questo include l'uso dei set di test più recenti, che evitano sovrapposizioni tra i set di addestramento e test, e il coinvolgimento di esperti in valutazioni umane, che offrono un giudizio più robusto rispetto a worker di massa. Inoltre, la presentazione propone strategie di selezione di prompt migliori per migliorare le performance dei modelli di traduzione automatica.</sample>
    <sample id="886">Il contributo principale di questa presentazione riguarda la prima analisi sistematica della promozione dei modelli di linguaggio (LLM) per il traduttore umano. Questo studio ha valutato sia le capacità di traduzione delle diverse candidate che le migliori pratiche della comunità di traduzioni. Inoltre, è stata proposta una raccomandazione per le strategie di selezione dei prompt.</sample>
    <sample id="887">I nostri contributi

• Prima studio sistematico di LLM prompting per MT.
• Evaluare le capacità di traduzione con le migliori pratiche della comunità di traduzioni:
    • Set di test più recenti (evitare overlap tra test/train e sovrapposizione con i dati di addestramento più recenti).
    • SOTA metriche per traduzioni (correlazione migliore con giudizi umani).
    • Valutazione basata su esperti (più robusta rispetto ai worker di una grande pubblica).
• Raccomandazioni per le strategie di selezione dei prompt.</sample>
    <sample id="888">I nostri contributi

• Prima studio sistematico di stimolazione LLM per la traduzione.
• Evaluare le capacità di traduzione con le migliori pratiche della comunità di traduzione:
- Settest più recenti (evitare overlap tra test/train e sovrapposizione)
- SOTAs (sistemi più recenti) utilizzando i dati di addestramento più recenti
- Miglior correlazione con giudizi umani
- Valutazione umana basata su esperti (più robusta rispetto ai worker di massa)
• Raccomandazione per le strategie di selezione dei prompt</sample>
    <sample id="889">Prompt have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="890">Prompt have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="891">I prompt hanno un grande impatto sulla qualità della traduzione

* Seleziona due prompt random per ogni frase.
* Calcola BLEURT per ciascun coppia di frase-prompt.
* La maggioranza delle frasi (516 su 1000) mostri una differenza di più di 1 punteggio BLEURT.
* La differenza può arrivare fino a 40 punteggii BLEURT!</sample>
    <sample id="892">Esempio di prompt per traduzione 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizisten in einem Streifenwagen festgehalten wird. English: He is being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Beweismaterial des Ermittlungsverfahrens erhalten hatten. English: Police were called after receiving complaints from the office. German: Ein Passtör alarmierte die Polizei, die mit mehreren Streifenwagen anrückte. English</sample>
    <sample id="893">Esempio di prompt per traduzione 5-tratti: German: Dorf sieht man, wie sie von zwei Polizisten in einem Streifenwagen zur Gendarmerie gebracht wird. English: Police were called after receiving complaints from the office.</sample>
    <sample id="894">Esempio di prompt per traduzione 5-shot prompting German: Dorfmann, wie sie von zwei Polizisten in einem Streifenwagen zur Gendekzelle gebracht wird. English: Police were called after receiving complaints from the office. German: Polizei legen unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerdung des Ereignisses erhalten hatten. English: Ein Passt alamerte die Polizei, mit mehreren Streifenwagen anrücken.</sample>
    <sample id="895">Esempio di prompt per traduzione</sample>
    <sample id="896">Esempio di prompt per traduzione 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizisten in einem Streifenwagen zur Gendekammer gebracht wird. English: Police were called after receiving complaints from the office.</sample>
    <sample id="897">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme di informazioni da MQM

• La fluentezza di PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
• "Stile/Accadimento" generalmente più basso per PaLM.</sample>
    <sample id="898">Risultati sperimentali

• La qualità esempio è più importante del grado di similitudine alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme con le informazioni fornite, i risultati sperimentali dimostrano che il modello PaLM ha superato le performance dei sistemi più avanzati (SOTA) in termini di qualità esempio e ha ridotto la similitudine alla frase di origine. Questo suggerisce che PaLM è capace di generare risposte più accurate e pertinenti, anche se non necessariamente con un livello di fluidezza simile a quello dei sistemi SOTA. Inoltre, i modelli SOTA sono dominati da errori di ommissione e stile/accadde generalmente per PaLM.</sample>
    <sample id="899">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Riflessioni da MQG

• La fluidezza di PaLM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominate da "accuratezza/omissione"
• "Stile/Accadimento" generalmente più basso per PaLM.</sample>
    <sample id="900">Risultati sperimentali

• La qualità dell'esempio è più importante del grado di somiglianza alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PaLM è vicino a Google Translate.

Saperie dalla MQM

• La fluidezza del PaLM è paragonabile ai sistemi SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
    • "Stile/Accadimento" generalmente più basso per PaLM.</sample>
    <sample id="901">Risultati sperimentali

• La qualità esempio è più importante del simile alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PaLM vicino a Google Translate.

Insieme da MQM

• Fluidità di PaLM comparabile ai SOTA.
• Precisione in generale più bassa.
    • Dominata da "Precision/Omission"
• "Stile/Accadde" in generale più basso per PaLM.</sample>
    <sample id="902">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Conoscenze tratte da MQM

• La fluidezza di PaLM è paragonabile ai sistemi SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
    • "Stile/Accadimento" generalmente più basso per PaLM.</sample>
    <sample id="903">Risultati sperimentali

• La qualità esempio è più importante del grado di similitudine alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PaLM si avvicina a Google Translate.

Insieme con le informazioni fornite, i risultati sperimentali dimostrano che PaLM ha un'importante migliorata rispetto ai sistemi SOTA specializzati. Questi sistemi SOTA hanno un vantaggio sostanziale in quanto sono più simili alla frase di origine e hanno una qualità esempio elevata. Tuttavia, PaLM ha dimostrato di essere in grado di superare tale sfida e si avvicina anche a Google Translate in termini di qualità esempio e similitudine alla frase di origine.</sample>
    <sample id="904">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi specializzati SOTA hanno un vantaggio sostanziale.
• PALM si avvicina a Google Translate.

Insieme di informazioni da MQG

• La fluidezza di PALM è paragonabile ai SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da “accuratezza/omissione”
    • “Stile/Accadimento” generalmente più basso per PALM.</sample>
    <sample id="905">Risultati sperimentali

• La qualità esempio è più importante del paragone alla frase di origine.
• I sistemi SOTA specializzati hanno un vantaggio sostanziale.
• PALM si avvicina a Google Translate.

Insieme di informazioni da MQ

• La fluentezza di PALM è paragonabile ai sistemi SOTA.
• Le prestazioni in termini di accuratezza sono generalmente più basse.
    • Dominato da "accuratezza/omissione"
• "Stile/Accadimento" generalmente più basso per PALM.</sample>
    <sample id="906">Grazie</sample>
    <sample id="907">Weaker Than You Think: A Critical Look at Weakly Supervised Learning

Dawei Zhu¹, Xiaoyu Shen², Marius Mosbach³, Andreas Stephan³, Dietrich Klakow¹

¹ Saarland University ² Amazon Alexa ³ University of Vienna

61° ACL 2023</sample>
    <sample id="908">Weaker Than You Think: A Critical Look at Weakly Supervised Learning

Dawei Zhu¹, Xiaoyu Shen², Marius Mosbach³, Andreas Stephan³, Dietrich Klakow¹

¹ Saarland University, ² Amazon Alexa, ³ University of Vienna

61° ACL 2023</sample>
    <sample id="909">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="910">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="911">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="912">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="913">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well being trained on noisy data</sample>
    <sample id="914">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="915">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%."</sample>
    <sample id="916">A common claim in recent WSL works “We train models only on weakly supervised data and achieve an accuracy of XX% 🤫”</sample>
    <sample id="917">A common claim in recent WSL works "We train models only on weakly supervised data and achieve an accuracy of XX%"</sample>
    <sample id="918">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="919">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="920">RQ1 Main findings Validation on Weak Labels No Validation on Validation on Weak Labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="921">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="922">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="923">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="924">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="925">RQ2 Main findings</sample>
    <sample id="926">RQ2 Main findings</sample>
    <sample id="927">RQ2 Main findings

FTw COSINE L2R BANDO MLC Weak labels 75 80 85 5 10 20 30 40 50 All Validation WSL approaches benefit from more clean validation samples!</sample>
    <sample id="928">RQ2 Main findings

Performance Delta (%) 5

85

FTw COSINE L2R MLC Weak labels 80 75 70 65 60 55 50 5 10 20 30 40 50 All 5 10 20 30 40 50 All Validation Validation

– WSL approaches benefit from more clean validation samples!</sample>
    <sample id="929">RQ2 Main findings 85 5 5 FTw COSINE L2R MLC 80 10 75 15 70 20 65 25 60 30 50 All 40 50 All Performance Delta (%) 5 0 -5 10 -10 15 -15 20 -20 FTw COSINE L2R MLC Adapterc Validation Validation</sample>
    <sample id="930">Risultati principali N=10 campioni puliti per classe N=30 campioni puliti per classe Prima di CFT Dopo CFT Prima di CFT Dopo CFT</sample>
    <sample id="931">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT</sample>
    <sample id="932">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT</sample>
    <sample id="933">RQ3 Main findings N=10 clean samples Before After Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="934">Conclusione Recenti approcci WSL Richiedono campioni puliti. Sottestimano la loro praticità. Nostra raccomandazione Report dei criteri di selezione del modello. Utilizzare gli approcci di imparazione a basso numero di etichette come baselines Sempre applicare l'ottimizzazione continuo (OFT).</sample>
    <sample id="935">Conclusione Recenti approcci WSL Richiedono campioni puliti. Sottestimano la loro praticità. Nostra raccomandazione Report dei criteri di selezione del modello. Utilizza approcci di imparazione a basso numero di etichette come baselines. Sempre applica l'ottimizzazione continuo (OFT).</sample>
    <sample id="936">Conclusione Recenti approcci WSL Richiedono campioni puliti. Sottestimano la loro praticità. Nostra raccomandazione Report dei criteri di scelta del modello. Utilizza gli approcci di apprendimento a basso numero di etichette come baselines. Sempre applica l'ottimizzazione continuo (OFT).</sample>
    <sample id="937">Conclusione Recenti approcci WSL Richiedono campioni puliti. Sottestimano la loro praticità. Nostra raccomandazione Report dei criteri di selezione del modello. Utilizza approcci di imparazione a basso numero di etichette come baselines. Sempre applica l'ottimizzazione continuo (OFT).</sample>
    <sample id="938">Conclusione Recenti approcci WSL Richiedono campioni puliti. Sottestimano la loro praticità. Nostra raccomandazione Report dei criteri di selezione del modello. Utilizza approcci di imparazione a basso numero di etichette come baselines. Sempre applica l’ottimizzazione continuo (OFT).</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo sono la valutazione basata su punteggio e la valutazione comparativa.</sample>
    <sample id="940">Per determinare il numero di autori coinvolti nell'articolo, analizziamo la sezione di testo fornita. La frase "NLPositionality: Characterizing Design Biases of Datasets and Models" è seguita da una serie di nomi e affiliazioni. 

1. Sebastin Sanyt - University of Washington
2. Jenny T. Liang - Carnegie Mellon University
3. Ronan Le Bras - Allen Institute for AI
4. Katharina Reinecke - University of Washington
5. Maarten Sap - Carnegie Mellon University

Contando questi nomi, possiamo vedere che ci sono 5 autori coinvolti nell'articolo.</sample>
    <sample id="941">Per comprendere l'esempio con Servin e Kea, è necessario conoscere le professioni di Servin e Kea, ovvero essere un giudice e un pasticcere rispettivamente.</sample>
    <sample id="942">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="943">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="944">Frasi con prefissi/suffissi</sample>
    <sample id="945">A dimension rating means that you are evaluating a product or service based on a specific set of criteria or dimensions.</sample>
    <sample id="946">I fornitori di servizi di scienze e tecnologia di Cina, Microsoft Research Asia e la Università di Beijing Haotong.</sample>
    <sample id="947">La forma del prompting è importante quando si traduce da un linguaggio naturale a un formalizzatore o viceversa.</sample>
    <sample id="978">The authors evaluated the Turn Likert and Dialogue Likert models.</sample>
    <sample id="979">To determine the number of authors involved in the article, we need to carefully examine the list of names provided in the image. The text lists the following authors:

1. Wenjun Peng
2. Jingwei Yin
3. Fangzhao Wu
4. Shangao Wu
5. Bin Zhu
6. Linguan Lyu
7. Binxing Jiao
8. Tong Xu
9. Guangzhong Sun
10. Xing Xie

By counting each name in the list, we can see that there are 10 distinct authors mentioned. Therefore, the total number of authors involved in the article is 10.</sample>
    <sample id="980">I qualità ideali di un buon pianificatore sono la capacità di elaborare e implementare pianificazioni efficaci, la capacità di adattarsi a cambiamenti imprevisti e la capacità di prendere decisioni informate basate su informazioni accurate.</sample>
    <sample id="981">Ci sono otto autori coinvolti nell'articolo.</sample>
    <sample id="982">La relatrice o il relatore è Vasudha Varadarajan.</sample>
    <sample id="983">I ricercatori che hanno scritto l'articolo sono affiliati all'Institute of Computer Science dell'University of Warsaw.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono l'accuratezza/omissione e lo stile/errore.</sample>
    <sample id="1022">Non dimenticare i tuoi ABC: valutare lo stato dell'arte dei sistemi di dialogo orientati al chat</sample>
    <sample id="1023">Non dimenticare i tuoi ABC: valutare lo stato dell'arte dei sistemi di dialogo orientati al chat</sample>
    <sample id="1024">Non dimenticare i tuoi ABC: valutando lo stato dell'arte dei sistemi di dialogo orientati al chat</sample>
    <sample id="1025">Evaluazione comparativa</sample>
    <sample id="1026">Evaluazione del punteggio Likert</sample>
    <sample id="1027">Dimensioni della qualità del dialogo

La qualità del dialogo è costituita da diverse dimensioni che influenzano la nostra capacità di comprendere e rispondere adeguatamente. Queste dimensioni includono:

1. **Rilevanza**: Questa dimensione si riferisce alla capacità di fornire risposte pertinenti e pertinenti alle domande o alle esigenze dell'interlocutore. Una risposta rilevante è quella che risponde direttamente alla domanda o ai problemi dell'interlocutore, senza deviare dal punto principale.

2. **Coerenza**: Questa dimensione riguarda la capacità di fornire risposte coerenti e logiche. Una risposta coerente è quella che è coerente con le informazioni precedenti e che non introduce elementi incoerenti o contraddittori.

3. **Comprezione emotiva**: Questa dimensione riguarda la capacità di comprendere e rispondere in modo appropriato alle emozioni dell'interlocutore. Una risposta che comprende emotiva è quella che riconosce e risponde in modo sensibile alle emozioni dell'interlocutore.

In sintesi, la qualità del dialogo è determinata dalla nostra capacità di fornire risposte pertinenti, coerenti e che comprendono le emozioni dell'interlocutore. Queste dimensioni sono cruciali per creare un dialogo efficace e produttivo.</sample>
    <sample id="1028">Valutazione del punteggio Likert</sample>
    <sample id="1029">Valutazione del punteggio Likert Chat (ABC-Eval)</sample>
    <sample id="1030">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1031">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="1032">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="1033">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Emotional Understanding</sample>
    <sample id="1034">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Incorrect Fact Commonsense Violation Consistency Self Contradiction Partner Contradiction Emotional Understanding Empathetic Response Lack of Empathy</sample>
    <sample id="1035">Esperimenti

• 4 modelli di dialogo aperti
• 100 conversazioni tra umano e bot per modello
• ABC-Eval

&lt;Immagine: Diagramma che rappresenta due bot che hanno una conversazione con un umano&gt;

Emory University
Alexa</sample>
    <sample id="1036">Esperimenti 4 modelli di dialogo aperti 100 conversazioni umano-bots per modello ABC-Eval Turn Likert Dialog Likert Comparativa Emory University Alexa</sample>
    <sample id="1037">Evaluazione del Baseline

La presentazione iniziale introduce un diagramma che rappresenta diverse aspetti della valutazione di un dialogo. Questi aspetti includono la coerenza, la comprensione emotiva, la creatività, la grammaticalità, la rilevanza, la qualità globale, la similitudine tra i turni, la similitudine dialogo e la comparatività.

Inizialmente, si discende lungo l'asse verticale, che rappresenta la progressione dei turni nel dialogo. Ora, si sposta lungo l'asse orizzontale, che rappresenta diversi aspetti della qualità del dialogo.

Il diagramma include anche due immagini di personaggi, uno dei quali è evidenziano con un cerchio rosso. Questo indica forse che tale personaggio ha una particolare importanza o ruolo nel contesto del dialogo.

Infine, la presentazione include le loghi dell'Università Emory e Alexa, suggerendo che queste istituzioni hanno un ruolo nella creazione o sostenimento del dialogo.</sample>
    <sample id="1038">Inter-Annotator Agreement

In questa immagine, ci sono diverse metriche utilizzate per misurare l'acordo tra gli annotatori. Queste metriche includono la Krippendorff Alpha, Turn Likert, Dialogo Likert e Comparative. La Krippendorff Alpha è una metrica di concordanza che valuta quanto gli annotatori siano d'accordo tra loro. Turn Likert e Dialogo Likert sono due tipi di scale Likert utilizzate per valutare la qualità della conversazione o dell'interazione tra gli annotatori. Comparative rappresenta un tipo di analisi che confronta le performance degli annotatori in modo diretto.

Inoltre, ci sono anche diverse istituzioni indicate nell'immagine, tra cui Emory University e Alexa. Queste istituzioni potrebbero essere coinvolte nella raccolta dei dati o nella conduzione dell'analisi.

Infine, ci sono anche alcune figure rappresentate nell'immagine, tra cui una donna con i capelli castani e un'altra con i capelli neri. Queste figure potrebbero rappresentare gli annotatori stessi o potrebbero essere rappresentazioni di personaggi o scenario specifici utilizzati nell'analisi.</sample>
    <sample id="1039">Validità predittiva

La barra rappresenta la qualità eseguita (R²) per diversi aspetti del linguaggio naturale, come la comprensione del contesto, la costruzione di frasi, l'uso di vocabolario e la grammatica. I colori diversi rappresentano i differenti modelli di interazione, come l'interazione umano-linguistica, l'interazione umano-macchina e l'interazione macchina-macchina.

I modelli ABC-Eval, Turn Likert e Comparative hanno diverse prestazioni in termini di qualità eseguita. Ad esempio, il modello ABC-Eval ha un alto livello di qualità eseguita per la comprensione del contesto, mentre il modello Turn Likert ha un basso livello di qualità eseguita per l'uso di vocabolario.

In generale, i modelli di interazione umano-linguistica tendono a avere un alto livello di qualità eseguita per la maggior parte degli aspetti del linguaggio naturale, mentre i modelli di interazione umano-macchina e macchina-macchina hanno un livello più basso di qualità eseguita.

Questi risultati hanno delle implicazioni importanti per la sviluppo dei modelli di intelligenza artificiale. Ad esempio, i modelli di interazione umano-linguistica potrebbero essere più adatti per le applicazioni che richiedono una comprensione profonda del linguaggio naturale, mentre i modelli di interazione umano-macchina e macchina-macchina potrebbero essere più adatti per le applicazioni che richiedono una capacità di risposte rapidi e precise.

Inoltre, questi risultati possono anche aiutare a identificare le aree in cui i modelli di intelligenza artificiale possono essere migliorati. Ad esempio, i modelli di interazione umano-linguistica potrebbero essere migliorati per ridurre gli errori nella costruzione di frasi o nella grammatica.

In sintesi, i modelli di intelligenza artificiale hanno diverse prestazioni in termini di qualità eseguita per diversi aspetti del linguaggio naturale. Questi risultati hanno delle implicazioni importanti per la sviluppo dei modelli di intelligenza artificiale e possono anche aiutare a identificare le aree in cui i modelli di intelligenza artificiale possono essere migliorati.</sample>
    <sample id="1040">Validità predittiva

Il grafico rappresenta la percentuale di qualità spiegata (RP) per diversi aspetti del linguaggio naturale, suddivisi in diverse sezioni. Ognuna di queste sezioni rappresenta un differente tipo di valutazione o contesto in cui è stata analizzata la qualità del linguaggio.

1. **Altra CS Controllata**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale è stato controllato da un'altra lingua o sistema di controllo.
2. **Irreversibile**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale non è stato modificato o controllato in alcun modo.
3. **Risposte a domande**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale è stato utilizzato per rispondere a domande.
4. **ABC-Eval**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale è stato valutato in base a criteri specifici, come ad esempio la comprensione e la produzione di risposte.
5. **Turn Likert**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale è stato valutato in base a una scala Likert, che misura l'opinione o la percezione dell'utente.
6. **Dialogue Likert**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale è stato valutato in base a una scala Likert specificamente progettata per valutare il dialogo tra due o più utenti.
7. **Comparative**: Questa sezione rappresenta la qualità spiegata quando il linguaggio naturale è stato confrontato con altri linguaggi o sistemi di linguaggio.

Ogni sezione ha due barre rappresentanti due tipi di valutazioni: "Interactive Q&amp;A" e "Non-Interactive Q&amp;A". Le barre "Interactive Q&amp;A" rappresentano la qualità spiegata quando il linguaggio naturale è stato utilizzato in un contesto interattivo, mentre le barre "Non-Interactive Q&amp;A" rappresentano la qualità spiegata quando il linguaggio naturale è stato utilizzato in un contesto non interattivo.

In generale, il grafico dimostra che la qualità spiegata varia in base al tipo di contesto e alla tipologia di valutazione utilizzata.</sample>
    <sample id="1041">Incrementale validità

Il grafico illustra la percentuale di qualità spiegata (R²) per diversi modelli linguistici, suddivisi in tre gruppi principali: ABC-Eval, Turn Likert e Dialogue Likert. I modelli vengono classificati in base alla loro capacità di rilevare emozioni, autocontrollo, redundanza, empatia e relevanti.

1. **ABC-Eval**: Questo gruppo include modelli che analizzano le emozioni, il controllo se stesso, la redundantia e l'empatia. Questi modelli hanno un'incrementale validità crescente, con i modelli più recenti dimostrando un'incrementale validità maggiore.
2. **Turn Likert**: Questo gruppo comprende modelli che analizzano solo la relevanti. Questi modelli hanno un'incrementale validità crescente, ma in modo più graduato rispetto ai modelli ABC-Eval.
3. **Dialogue Likert**: Questo gruppo include modelli che analizzano sia la relevanti che l'empatia. Questi modelli hanno un'incrementale validità crescente, ma in modo più graduato rispetto ai modelli ABC-Eval e Turn Likert.

I modelli vengono classificati in base alla loro capacità di rilevare emozioni, autocontrollo, redundantia, empatia e relevanti. I modelli più recenti dimostrano un'incrementale validità maggiore, mentre i modelli più vecchi hanno un'incrementale validità minore.</sample>
    <sample id="1042">Incrementale validità

Il grafico illustra la percentuale di qualità spiegata (R²) in relazione a diversi parametri, come ABC-Eval, Turn Likert e Dialogo Likert. I punti rappresentano diversi aspetti, tra cui "Unempathetic", "Self Control", "Relevant" e "Emotion".</sample>
    <sample id="1043">Incremental Validity

In questa presentazione, il grafico illustra la percentuale di qualità spiegata (R²) in relazione a diversi parametri, come ABC-Eval, Turn Likert e Dialogo Likert. Il grafico è suddiviso in due curve principali: una in rosso che rappresenta i parametri "Emotion" e "Relevant", e un'altra in blu che rappresenta i parametri "Empathic", "Self-Contr." e "Redundant". 

La curva in rosso indica che i parametri "Emotion" e "Relevant" hanno un'incremental validity più elevata rispetto ai parametri "Empathic", "Self-Contr." e "Redundant". Questo significa che i parametri "Emotion" e "Relevant" spiegano maggiormente la qualità rispetto ai parametri "Empathic", "Self-Contr." e "Redundant".

Il grafico utilizza un modello logistico per analizzare la percentuale di qualità spiegata (R²). Questo modello è utilizzato per valutare la validità incrementale dei parametri in questione.

Inoltre, il grafico include anche un'introduzione alla validità incrementale e una panoramica della storia della validità incrementale. La validità incrementale è un concetto statistico utilizzato per valutare la capacità di un modello di predire variabili dipendenti. Questo concetto è stato sviluppato nel 1950s e ha continuato a essere utilizzato in vari campi di studio.

In sintesi, il grafico fornisce una rappresentazione visiva della percentuale di qualità spiegata (R²) in relazione a diversi parametri, utilizzando un modello logistico per analizzare la validità incrementale dei parametri in questione.</sample>
    <sample id="1044">ABC-Eval Error Rates by Model

30
20
10
0
Antisocial
CS Contr.
Ignore
Incorrect
Irrelevant
Unempathetic
Other Contr.
Redundant
Self Contr.
Topic Switch
Uninterpretable

BART-FID-RAG Blender2 Emora Blender-Decode</sample>
    <sample id="1045">ABC-Eval Error Rates by Model</sample>
    <sample id="1046">ABC-Eval Error Rates by Model

30%
20%
10%
0%
Antisocial
CS Contr.
Ignore
Incorrect
Irrelevant
Unempathetic
Other Contr.
Redundant
Self Contr.
Topic Contr.
Uninterpretable

BART-FID-RAG Blender2 Emora Blender-Decode</sample>
    <sample id="1047">ABC-Eval Error Rates by Model

30
20
10
0
Antisocial
CS Contr.
Ignore
Incorrect
Irrelevant
Unempathetic
Other Contr.
Redundant
Self Contr.
Topic Contr.
Uninterpretable
BART-FID-RAG Blender2 Emora Blender-Decode</sample>
    <sample id="1048">I fornitori di servizi cloud, i laboratori di ricerca e le università.</sample>
    <sample id="1049">CFT significa continuo addestramento finitore.</sample>
    <sample id="1050">Per rispondere alla domanda di quanti autori sono coinvolti nell'articolo, bisogna analizzare la sezione di testo fornita. La frase "Language model acceptability judgments are not always robust to context" è seguita da "(ACL 2023)" e una lista di nomi. 

Passo 1: Identificare i nomi nella lista.
Passo 2: Contare i nomi individualmente.

La lista di nomi è la seguente:
- Koustuv Sinha
- Jon Gauthier
- Aaron Mueller
- Kanishka Misra
- Keren Fuentes
- Roger Levy
- Adina Williams

Contando i nomi individualmente, ci sono 7 autori coinvolti nell'articolo.

Risposta: Ci sono 7 autori coinvolti nell'articolo.</sample>
    <sample id="1051">Quando la traduzione richiede contesto? Esplorazione multilingue guidata da dati</sample>
    <sample id="1052">Traduzione dipende dal contesto We'll have to get rid of that mole.</sample>
    <sample id="1053">Traduzione dipende dal contesto. Le cose potrebbero iniziare a diventare pericolose se i ministri scoprono. Dobbiamo liberarci di quel molo.</sample>
    <sample id="1054">Traduzione dipende dal contesto Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1055">Evaluating context-dependent translation is hard Only a small portion of words depend on context</sample>
    <sample id="1056">Evaluating context-dependent translation is hard

- Solo una piccola parte delle parole dipende dal contesto
- Metri a livello di corpus
- Metodi esistenti supportano fenomeni limitati e linguaggi</sample>
    <sample id="1057">Q1: In che momento è necessario il contesto per la traduzione? Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1058">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1059">Condizionale Cross-Mutual Information (CXMI)</sample>
    <sample id="1060">Condizionale Cross-Mutua Informazione (CXMI) • CXMI: misura quanto contesto MT modelli usano data un corpus Uncertainty over translations given the source HqMTA(Y|X) Uncertainty over translations given the source AND context HqMTA(C|X, Y) CXMI(C) → Y|X</sample>
    <sample id="1061">Pointwise (P-)CXMI

• We introduce P-CXMI to measure context usage to translate a specific</sample>
    <sample id="1062">RQ1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola - Analisi tematica RQ2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1063">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto al livello delle parole - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto?</sample>
    <sample id="1064">Analisi tematica di parole high P-CXMI</sample>
    <sample id="1065">Analisi tematica di parole P-CXMI elevate 1. POS tag</sample>
    <sample id="1066">Analisi tematica di parole P-CXMI elevate 1. Tag POS P-CXMI per i tag in En-Ar Pron 3.5 sing Pron 3.5 Dual Pron 3.5 Plus Pronouns</sample>
    <sample id="1067">Analisi tematica di parole elevate P-CXMI 1. Tag POS 2. Parole vocabolari - Pronouns - Verbi</sample>
    <sample id="1068">1. POS tags 2. Vocabulary items</sample>
    <sample id="1069">1. Tag POS 2. Parole lessiche Avellie's mother was still asleep. Avellie went to school. La madre di Avellie era ancora addormentata. Avellie è andata a scuola.</sample>
    <sample id="1070">1. Tag di posizione 2. Parole vocabolari 3. Token individuali - Pronouns - Verbi in forma - Coinerenza lessicale - Formalità - Ellissi</sample>
    <sample id="1071">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo di contesto a livello di parola - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto? - Benchmark Multilingue Discorso-Aware (MuDA)</sample>
    <sample id="1072">Multilingua Discourse-Aware (MuDA) tagger

- Pronouns
- Verbe form
- Lexical cohesion
- Formalità
- Ellissi</sample>
    <sample id="1073">Tagger Multilingua Consciente del Discorso (MuDA)</sample>
    <sample id="1074">MuDA benchmark documenti -&gt; MuDA tagger -&gt; documenti -&gt; BLEU COMET F-measure -&gt; robot</sample>
    <sample id="1075">Q1: In che momento è necessario il contesto per la traduzione? - Utilizzo del contesto a livello di parola - Analisi tematica Q2: Come gestiscono i modelli le traduzioni dipendenti dal contesto? - Benchmark Multilingual Discourse-Aware (MuDA) - Valutazione del modello</sample>
    <sample id="1076">Corpus-level metrics BLEU</sample>
    <sample id="1077">Metriche a livello di corpus</sample>
    <sample id="1078">Corpus-level metrics BLOU COMET F-measure Non è chiaro quale sia il sistema migliore per la traduzione automatica di documenti con i metriche a livello di corpus</sample>
    <sample id="1079">Risultati del benchmark MuDA • I modelli consapevoli del contesto performano significativamente meglio su alcuni fenomeni o: √ Formalità, coesione lessicale</sample>
    <sample id="1080">Risultati del benchmark MuDA

• I modelli consapevoli del contesto performano significativamente meglio su alcuni fenomeni
o: Formalità, coesione lessicale o: Ellissi, possessivi, forma verbale</sample>
    <sample id="1081">Risultati del benchmark MuDA

• I modelli consapevoli del contesto performano significativamente meglio su alcuni fenomeni
o √ /: Formalità, coesione lessicale × Ellissi, pronouns, forma del verbo
• DeepL supera Google in maggior parte dei fenomeni e delle lingue
*</sample>
    <sample id="1082">Riassunto Identificare sistematicamente fenomeni discorsivi senza conoscenza linguistica precedente. Benchmark per traduzione automatica a livello di documento senza conoscenza precedente.</sample>
    <sample id="1083">Riassunto

Identify discourse phenomena systematically without prior linguistic knowledge

Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1084">La relatrice o il relatore è Yufen Zhang.</sample>
    <sample id="1121">Il nuovo metodo non ha un nome.</sample>
    <sample id="1122">Il metodo include trovare parole che distinguono gruppi contrassegnati da gruppi non contrassegnati.</sample>
    <sample id="1123">Paul G. Allen School, UW NLP, Carnegie Mellon University Language Technologies Institute, Russian Academy of Sciences</sample>
    <sample id="1124">La prima struttura di dipendenza simmetrica menzionata è "Bouquet/Stanford".</sample>
    <sample id="1125">Il nome del relatore o della relatrice è Sarah E. Finch.</sample>
    <sample id="1126">Per rispondere alla domanda di quanti autori sono coinvolti nell'articolo, analizziamo la descrizione fornita dell'immagine. L'immagine include il titolo dell'articolo e i nomi degli autori. 

1. Identificare il titolo dell'articolo: "Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus)".
2. Estrarre i nomi degli autori dalla descrizione: "Mohammad Javad Hosseini, Filip Radlinski, Silvia Paretti, and Annie Louis".

Contando i nomi elencati, ci sono 4 autori coinvolti nell'articolo.

Risposta: Ci sono 4 autori coinvolti nell'articolo.</sample>
    <sample id="1127">Syn tax Gym e CrowS.</sample>
    <sample id="1161">FT, W, BOND, COSINE, MLC, L2R</sample>
    <sample id="1162">Il modello viene valutato su 11 attività.</sample>
    <sample id="1226">CamemBERT è inizialmente addestrato su 4GB di dati.</sample>
    <sample id="1227">Adam Przepiórkowski and Michał Woźniak</sample>
    <sample id="1228">I grafici hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">È necessario permutare i token per la sequenza di output per ottenere la risposta corretta.</sample>
    <sample id="1270">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per rendere più comprensibili i modelli e migliorare la fiducia in loro.</sample>
    <sample id="1271">I due input inaccettabili di coppia minima sono "Molti hanno aiutato" e "Molti hanno aiutato se stessi".</sample>
    <sample id="1272">I ricercatori hanno utilizzato diverse metriche di valutazione per misurare le prestazioni dei modelli. Queste metriche includono la precisione, la ricchezza e la F1-score.</sample>
    <sample id="1273">La metrica utilizzata per misurare l'accordo tra gli annotatori è la consistenza.</sample>
    <sample id="1274">The domain chosen for adding completely unrelated sentences to unacceptable and acceptable queries is Wikipedia.</sample>
    <sample id="1275">Gli autori dell'articolo sono affiliati all'Heinrich Heine University in Düsseldorf, Germania.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto non utilizza un modello pre-addestrato.</sample>
    <sample id="1277">Per determinare quanti autori sono coinvolti nell'articolo, analizziamo la sezione del titolo che elenca i nomi degli autori. L'articolo è intitolato "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems" e include i nomi dei tre autori principali: Sarah E. Finch, James D. Finch, e Jinho D. Choi. Pertanto, il numero di autori coinvolti nell'articolo è di 3.</sample>
    <sample id="1278">La coordinazione binaria è quando due o più componenti linguistiche si uniscono per formare una unità più grande.</sample>
    <sample id="1279">I prompt sono stati utilizzati per un periodo di 10 giorni.</sample>
    <sample id="1280">I modelli più piccoli specializzati possono superare i modelli a larga gamma di lingue in termini di qualità dei testi generati.</sample>
    <sample id="1281">DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedici e clinici

Yanis Labarre1,4 Adrien Bazille2,3 Richard Dufour2 Mickael Rouvier1 Emmanuel Morin2,3 Beatrice Dalille2 Pierre-Antoine Gourraud4

(1) LIA, Avignon Université (2) LS2N, Université de Nantes (3) CHU des donnees, CHU de Nantes (4) Zenith</sample>
    <sample id="1282">I. Modello di Linguaggio in Sanità II. Confronto tra strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1283">I. Modello di Linguaggio in Sanità II. Confronto tra strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1284">I. Modello di Linguaggio in Sanitaria II. Confronto tra strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 task IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1285">I. Modello di Linguaggio in Sanità II. Confronto tra strategie di pre-addestramento, fonti e dimensioni dei dati III. Valutazione di 13 modelli su 11 compiti IV. Distribuzione di NACHOS e DrBERT</sample>
    <sample id="1286">Approcci basati su Transformer, come BERT, offrono un'enorme migliora dei performance su compiti di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su compiti medici, i modelli specifici del dominio hanno alzato la barra anche in inglese con PudMedBERT, BioBERT, ClinicalBERT e altri. Le lingue diverse dall'inglese sono più rare e si basano principalmente su pre-azioni continuamente utilizzando un modello generico. Attualmente non esiste un modello open-source generico per il dominio biomedico in francese; tuttavia, un modello specifico per il dominio di BERT dovrebbe migliorare le prestazioni sui compiti medici.</sample>
    <sample id="1287">Approcci basati su Transformer, come BERT, offrono un'enorme guadagnata di prestazioni su una vasta gamma di compiti di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su compiti medici, i modelli specifici del dominio hanno superato anche in lingua inglese. Esistono modelli specifici del dominio come PubMedBERT, BioBERT e ClinicalBERT, tra gli altri. Le lingue diverse dallo inglese sono più rare e si basano principalmente su pre-addestramenti continuativi utilizzando un modello generico esistente. Non esiste un modello open-source gratuito per il dominio biomedico in francese. Un modello specifico del dominio basato su BERT per il francese dovrebbe aumentare le prestazioni su compiti medici.</sample>
    <sample id="1288">Approcci basati su Transformer, come BERT, offrono un'enorme migliora delle prestazioni in molte attività di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su attività mediche, i modelli specifici del dominio hanno superato anche in inglese il livello più elevato. Pubblicati da BERT, BioBERT, ClinicalBERT e altri. Le lingue diverse dallo inglese sono più rare e si basano principalmente su pre-addestramenti continuativi utilizzando un modello esistente generico. Non esiste un modello open-source gratuito per il dominio biomedico in francese. Un modello specifico del dominio basato su BERT per il francese dovrebbe aumentare le prestazioni su attività mediche.</sample>
    <sample id="1289">Approcci basati su Transformer, come BERT, offrono un'enorme guida in molte attività di NLP. Sono stati adattati al francese con CamemBERT e FlauBERT. Su attività mediche, i modelli specifici del dominio hanno superato anche in inglese il segno anche più alto. Pubblicato da BERT, BioBERT, ClinicaBERT e altri. Le lingue diverse dallo inglese sono rare e si basano principalmente su pre-addestramenti continuativi utilizzando un modello esistente generico. Non esiste un modello generico aperto sorgente per il dominio biomedico in francese. Un modello specifico del dominio BERT dovrebbe aumentare le prestazioni sulle attività mediche.</sample>
    <sample id="1290">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati comparabili. NACHOS: un insieme di 1,1 miliardi di parole aperto-sorgente di diverse discipline mediche, stili e tipi di documenti. Dati privati: un insieme di frasi estratte da 1,7 miliardo di record anonimizzati dalla banca dati dei hospitales universitari. Comparazione delle strategie di impianto: Costruito da zero con costruzione completa Continuando il pre-impianto utilizzando un modello pre-impianto esistente, come CammedBERT, un modello generico francese, e PubMedBERT, un modello pre-impianto per la medicina.</sample>
    <sample id="1291">Evaluazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili

- Fonti di dati:
    - NACHOS: un insieme di 1.1 milioni di parole aperto-sorgente di diverse discipline mediche, inclusi i dati di test privati
    - Dataset privato: frasi estratte da 1.7 milioni di record anonimizzati dalla banca dei dati dell'Ospedale Universitario

- Strategie di apprendimento:
    - Dall'origine con costruzione completa
    - Continuando l'apprendimento con un modello pre-addestrato esistente
    - Utilizzo di modelli generici come CamMedBERT e PubMedBERT</sample>
    <sample id="1292">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati comparabili. NACHOS: un dataset open-source di 1,1 milioni di parole copertore di diverse discipline mediche, strutture e stili. NBDW: un dataset privato di frasi estratte da 1,7 milioni di registri sanitari annullati. Université Hospital Data Warehouse: raccolta di dati estratti dai registri dei pazienti. Comparazione delle strategie di immissione • Dalla sorgente con costruzione a zero • Continuos pre-training utilizzando un modello pre-addestrato • Continuos pre-training utilizzando un modello pre-addestrato • Continuos pre-training utilizzando un modello pre-addestrato • Continuos pre-training utilizzando un modello pre-addestrato</sample>
    <sample id="1293">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni dei dati confrontabili. NACHOS: Una fonte di testo aperto di 1,1 milioni di parole copriente diverse discipline mediche, stili e tipi di documenti. Dati privati: Un insieme di frasi estratte da 1,7 milioni di registri sanitari annullati. Fonti universitarie: Archivio di dati di un'ospedale universitaria. Comparazione delle strategie di imposizione: Costruzione completa da zero, imposizione continuata utilizzando un modello pre-addestrato esistente, imposizione continuata con CamemBERT, un modello generico francese, e imposizione continuata con PubMedBERT.</sample>
    <sample id="1294">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati confrontabili. NACHOS: un dataset open-source di 1,1 milioni di parole coprindo diverse discipline mediche, stili e tipi di documenti privati. NBERT: un dataset privato di frasi estratte da 1,7 milioni di record anonimizzati dalla banca dati universitaria dei hospices di Nanterre. Comparazione di strategie di impianto: Costruito da zero con una costruzione completa, Continuando il pre-impianto utilizzando un modello pre-impianto esistente, Utilizzo di CammedBERT, un modello generico francese, Utilizzo di PubMedBERT, un modello pre-impianto.</sample>
    <sample id="1295">Evaluazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati comparabili

* Strategie di pre-addestramento confrontate
o NACHOS: un insieme di 1.1 M parole aperto-sorgente di diverse discipline mediche, raccolte da diverse fonti
o NBDW: un insieme privato di 1.7 M di frasi estratte da record di ricovero annullati dalla rete dei hospitales universitari

* Strategie di imparare confrontate
o Dall'origine: costruito completamente da zero
o Continuare ad addestrare utilizzando un modello pre-addestrato esistente
o NACHOS: un modello generico francese
o NBDW: un modello di NACHOS</sample>
    <sample id="1296">Comparazione di strategie di pre-addestramento e fonti dei dati

* Valutazione dell'impatto di fonti di dati medici pubbliche e private su dimensioni di dati confrontabili
	+ NACHOS: un insieme di 1,1 milioni di parole aperto e sorgente di diverse discipline mediche, raccolte da registri di pazienti di diverse strutture e tipi di ambienti
	+ Un dataset privato di frasi estratte da 1,7 milioni di registri medici annullati, raccolti dal Nantes University Hospital Data Warehouse
* Confronto delle strategie di apprendimento
	+ Dalla sorgente con costruzione completa
	+ Continuando ad addestrare utilizzando un modello pre-addeistrato esistente
	+ Utilizzando modelli generici come CamemBERT e PubMedBERT</sample>
    <sample id="1297">Evaluazione dell'impatto di fonti di dati medici pubbliche e private rispetto a dimensioni di dati comparabili

* Strategie di addestramento pre-addestrate e fonti di dati
	+ Corpus privati: NACHOS, NBDB (entrambi)
	+ Corpus pubblico: NACHOS, NBDB (entrambi)
	+ Fonti di dati: NACHOS, NBDB (entrambi)
	+ Dimensioni di dati: 1.1B, 7.4G, 5.3M, 659M, 43.1M, 66.4M

* Strategie di apprendimento
	+ From scratch: full costruzione
	+ Continuos pre-training: utilizzo di un modello pre-addestrato esistente
	+ Continuos pre-training: CamemBERT, un modello generico francese
	+ PubMedBERT, un modello di addestramento pubblico</sample>
    <sample id="1298">Evaluazione: 13 fonti di dati e dimensione

- Performance evaluation of 13 models on 11 public and private tasks
- Our fine-tuned models get state-of-the-art results on almost all tasks

| Model | Medical Report Specialties | MUSCA-DET | MUSCA-DET+ISSA | CAS | CAS+EMO | QUADRO-EHME | QUADRO-MEDLINE |
| --- | --- | --- | --- | --- | --- | --- | --- |
| General | 94.14% | 94.14% | 94.14% | 94.14% | 94.14% | 94.14% | 94.14% |
| BioMedical | 88.78% | 90.33% | 90.33% | 90.33% | 90.33% | 90.33% | 90.33% |
| Clinical | 85.59% | 86.14% | 86.14% | 86.14% | 86.14% | 86.14% | 86.14% |

**Note:** The table shows the performance of different models on various medical report specialties, including MUSCA-DET, MUSCA-DET+ISSA, CAS, CAS+EMO, QUADRO-EHME, and QUADRO-MEDLINE. The models are evaluated based on their ability to perform state-of-the-art results on almost all tasks.</sample>
    <sample id="1299">Evaluazione: 13 sorgenti di dati e dimensione • La nostra valutazione è stata eseguita su 11 compiti pubblici e privati • I nostri modelli finetunati hanno ottenuto lo stato dell'arte su quasi tutti i compiti AFF Medical Report Specialties MUSCA-DET MUSCA-DET Essai CAS CasMeQ. QUADRO-EMEA QUADRO-MEDLINE General CanonBert Oscar 15 GB 42,60 97,88 99,41 91,27 83,09 94,62 35,75 90,83 78,76 CanonBert CUNet 4 GB 42,60 97,88 99,41 91,27 83,09 94,62 35,75 90,83 77,61 BioMedical RoBERTa v1.1 46,03 79,00 89,38 98,80 81,91 81,89 36,12 84,20 72,68 Clinical RoBERTa v1.1 44,70 79,77 88,77 98,56 82,59 82,46 37,37 91,46 75,05 Clinical DeBERTa NACOS 44,53 93,87 98,44 91,77 85,06 88,76 37,57 91,66 75,18 Clinical DeBERTa SNOW 44,70 93,87 98,44 91,77 85,06 88,76 37,57 91,66 75,18 Clinical RoBERTa SNOW (finetuned) 91,43 93,87 99,31 92,75 87,11 91,73 34,55 90,52 78,63</sample>
    <sample id="1300">Evaluazione: 13 fonti di dati e dimensioni

- Valutazione delle prestazioni dei 13 modelli su 11 task pubblici e privati
- I modelli fin-tuned raggiungono lo stato dell'arte su quasi tutti i task

| Modello | Medical Report Specialties | MUSCA-DET | MUSCA-BET | MUSCA-ESAL | CAS | Hanning | CASMeo | QUADRE-EMEA | QUADRE-MEDL |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| General | BioBERT Oscar 150 GB | 92.46 | 90.78 | 93.41 | 91.27 | 89.60 | 90.42 | 93.75 | 90.83 |
| Biomedical | BioBERT ULI | 46.01 | 79.00 | 89.38 | 89.80 | 85.46 | 81.91 | 81.87 | 72.68 |
| Clinical | BioBERT NACOS | 44.70 | 79.70 | 88.77 | 90.56 | 82.95 | 82.44 | 93.75 | 75.05 |
| Clinical | BioBERT SNOW | 45.59 | 79.87 | 89.44 | 93.56 | 81.77 | 88.76 | 93.75 | 75.05 |
| Clinical | BioBERT SNOW (fin-tuned) | 91.44 | 93.98 | 93.00 | 92.75 | 91.71 | 87.55 | 93.45 | 78.63 |

Avignon Université</sample>
    <sample id="1301">Evaluazione: 13 sorgenti di dati e dimensioni

- Valutazione delle prestazioni dei 13 modelli su 11 compiti pubblici e privati
- I modelli fin-tuned raggiungono lo stato dell'arte in quasi tutti i casi

| Sorgente | Modello | F1 |
| --- | --- | --- |
| General | BERTBase-uncased | 98.44 |
| General | CamBert-uncased | 98.38 |
| General | DeBERTa-uncased | 98.35 |
| General | RoBERTa-uncased | 98.26 |
| General | ChitBERT-uncased | 98.14 |
| Biomedical | BioBERT-v1.1 | 97.00 |
| Biomedical | BioBERT-uncased | 96.98 |
| Biomedical | BioBERT-large | 96.96 |
| Biomedical | BioBERT-base | 96.94 |
| Clinical | ClinicalBERT | 97.10 |
| Clinical | ClinicalBERT-uncased | 97.08 |
| Clinical | ClinicalDeBERTa | 97.06 |
| Clinical | ClinicalRoBERTa | 97.04 |
| Clinical | ClinicalChitBERT | 97.02 |

Il modello ChitBERT-uncased raggiunge lo stato dell'arte in quasi tutti i compiti, dimostrando una capacità di apprendimento e di generalizzazioni eccellente.</sample>
    <sample id="1302">Evaluazione: Addestramento pre-scarico

* Dall'originale ad un allenamento continuo su 4GB di dati
* Le attività di risoluzione di domande richiedono più conoscenza specifica del dominio per essere in grado di funzionare bene
* Un studio sulla stabilità dei modelli dimostra che i modelli basati su Camembert addestrati utilizzando l'addestramento continuo hanno una maggiore variabilità inter-run rispetto ai modelli addestrati da zero

Tabella:

| Modello | HF | Medici | Specializzazioni | MUSCA-DET | MUSCA-DET-S | HUMMOS | QuattroEMMA | QuattroMED |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 35.13 | 80.11 | 99.20 | 85.44 | 92.10 | 95.24 | 74.1 | 77.61 |
| Camembert CNN+G | 43.11 | 79.88 | 99.20 | 85.44 | 92.10 | 95.24 | 80.31 | 77.61 |
| Camembert CNN+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L+T | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L | 46.01 | 79.72 | 99.20 | 85.44 | 92.10 | 95.24 | 82.79 | 77.61 |
| Camembert CNN+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L+L+G | 46.01 | 79.72 | 99.</sample>
    <sample id="1303">Evaluazione: Addestramento pre-scarico

* Dall'originale ad un continuo addestramento su 4GB di dati
* Le attività di risoluzione di domande richiedono conoscenza più specifica del dominio per essere in grado di funzionare bene
* Un studio sulla stabilità del modello dimostra che i modelli basati su Camembert addestrati utilizzando addestramento continuo hanno una maggiore variabilità inter-run rispetto ai modelli addestrati da zero

La tabella di confronto tra i modelli di addestramento da zero e quelli addestrati continuamente è la seguente:

| Modello | HF Addestramento da zero | HF Addestramento continuo |
| --- | --- | --- |
| Camembert-OSCAR | 35.13 ± 0.81 | 99.35 ± 0.20 |
| Camembert-CNN+G | 43.11 ± 0.78 | 99.35 ± 0.20 |
| Camembert-CLM | 46.01 ± 0.72 | 99.35 ± 0.20 |
| Roberta-OSCAR | 45.73 ± 0.87 | 98.84 ± 0.20 |
| Roberta-CNN+G | 43.11 ± 0.78 | 98.84 ± 0.20 |
| Roberta-CLM | 46.01 ± 0.72 | 98.84 ± 0.20 |
| Camembert-Semantic | 35.13 ± 0.81 | 99.35 ± 0.20 |
| Camembert-CNN+G-Semantic | 43.11 ± 0.78 | 99.35 ± 0.20 |
| Camembert-CLM-Semantic | 46.01 ± 0.72 | 99.35 ± 0.20 |

Questi modelli hanno dimostrato una maggiore stabilità e variabilità rispetto ai modelli addestrati da zero, dimostrando che l'addestramento continuo è un approccio più efficace per le attività di risoluzione di domande.</sample>
    <sample id="1304">Evaluazione: Addestramento pre-scarico

* Dall'originale a continuazione addestrato su 4GB di dati
* Task di risoluzione di domande richiedono conoscenza più specifica del dominio per essere in grado di funzionare bene
* Un studio sulla stabilità del modello dimostra un maggior intervallo di riproduzione per i modelli addestrati utilizzando Camembert basato su addestramento continuo

Tabella:

| Modello | HF Spec. | Med. Spec. | MUSCA-Det Spec. | MUSCA-DET ESAL Spec. | HumanMMLM | QUADRO-EMMA | QUADRO-MEDICINE |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 35,13 | 93,33 | 99,20 | 85,44 | 92,20 | 74,1 | 77,6 |
| Camembert CNN+G | 43,11 | 79,88 | 99,20 | 93,44 | 91,38 | 83,7 | 77,6 |
| Camembert CNN+G+ | 46,01 | 79,72 | 99,20 | 93,44 | 91,38 | 83,7 | 75,6 |
| Camembert CNN+G+LARGE | 46,01 | 79,72 | 99,20 | 93,44 | 91,38 | 83,7 | 75,6 |
| DOBERT NACHOS | 45,73 | 98,84 | 99,20 | 93,44 | 91,38 | 83,7 | 75,6 |
| Camembert NWDW | 46,01 | 98,84 | 99,20 | 93,44 | 91,38 | 83,7 | 75,6 |
| Camembert NWDW+ | 46,01 | 98,84 | 99,20 | 93,44 | 91,38 | 83,7 | 75,6 |

Avignon Université</sample>
    <sample id="1305">Core message DrBERT surpasses state-of-the-art results in 9 downstream French medical-oriented tasks Confirms utility of training a medical-specific model in French Data sources matters training on heterogeneous data is important NACHOS is more robust than using private clinical data only Continual pretraining is better, but does not scale well Continual pretraining DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="1306">Core message DrBERT surpasses state-of-the-art results in 9 downstream French medical-oriented tasks Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only Continual pretraining is better, but is effective strategy when based on domain-specific English models Source code and DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license Avignon Université</sample>
    <sample id="1307">Core message DrBERT improves state-of-the-art results in 9 downstream French medical-oriented tasks Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only Continual pretraining is better, but is effective strategy when based on domain-specific English models Download DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license Avignon Université</sample>
    <sample id="1308">Grazie. Mi aspetto di condividere informazioni al poster session a Toronto.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate nel lavoro sono: costruire da zero con full pre-ambulanza, continuare il pre-addestramento utilizzando un modello pre-addeistrato esistente e utilizzare un modello generico francese chiamato CamMedBERT.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è piccolo.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata utilizzando i punteggi BLEU e SARI.</sample>
    <sample id="1312">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="1313">Generalizzazioni composte senza alberi utilizzando tag multiset e permutazioni latenti</sample>
    <sample id="1314">Composizionale generalizzazioni senza alberi utilizzando tag multiset e permutazioni latenti Matthias Lindemann, Alexander Koller, Ivan Titov INFORMATICS NLP CENTRE UNIVERSITARI DI AMSTERDAM</sample>
    <sample id="1315">Abilità di un imparatore di gestire una successione più profonda e composizioni non viste di frasi che hanno essere stati visti individualmente durante la formazione.</sample>
    <sample id="1316">**Titolo:** Generalizzazioni compostizionali in analisi semantica

**Train:**

| Frasi | Struttura |
| --- | --- |
| The girl slept. | 'x_1:girl, sleep_agent_x_2:x' |
| Mary knew that the girl slept. | 'x_1:Mary, know_agent_x_2:x, x_3:girl, x_4:sleep_agent_x_5:x' |

In questa presentazione, si discutono di come un modello di analisi semantica può imparare a generalizzare strutture grammaticali complesse utilizzando esempi di frasi. La tabella sopra fornisce due esempi di frasi e le loro rappresentazioni semantiche.

1. **Frasi:** "The girl slept."
   - **Struttura:** 'x_1:girl, sleep_agent_x_2:x'
     - In questa frase, "the girl" è il soggetto dell'azione di "slept". L'agent (o soggetto) dell'azione è "the girl", che è rappresentato da 'x_1:girl'. L'azione stessa è rappresentata da 'sleep_agent_x_2:x', dove 'x' indica l'argomento dell'azione.

2. **Frasi:** "Mary knew that the girl slept."
   - **Struttura:** 'x_1:Mary, know_agent_x_2:x, x_3:girl, x_4:sleep_agent_x_5:x'
     - In questa frase, "Mary" è l'agente dell'azione di "knew". L'azione di "knew" è rappresentata da 'know_agent_x_2:x', dove 'x' indica l'argomento dell'azione. L'oggetto dell'azione è "that the girl slept", che è rappresentato da 'x_3:girl' e 'x_4:sleep_agent_x_5:x'. Questi elementi sono collegati per indicare che Mary ha conosciuto l'azione di "slept" eseguita da "the girl".

Questa rappresentazione semantica aiuta il modello a comprendere le relazioni tra i componenti della frase e a generalizzare queste strutture in modo più generale.</sample>
    <sample id="1317">### Compositional Generalization in Semantic Parsing

**Train:**

| Frasi       | Struttura Semantica |
|--------------|---------------------|
| The girl slept. | 'gir x; sleep.agent x, x' |
| Mary knew that the girl slept. | 'gir x; sleep.agent x, x' |

In questa presentazione, si discutono di come il modello di parsing semantico generalizza composizionalmente le strutture semantiche. La tabella fornisce esempi di frasi e delle loro rappresentazioni semantiche.

1. **Frasi:** "The girl slept."
   - **Struttura Semantica:** 'gir x; sleep.agent x, x'
     - Questa struttura indica che l'azione di dormire è eseguita da 'x', che rappresenta la "girl".

2. **Frasi:** "Mary knew that the girl slept."
   - **Struttura Semantica:** 'gir x; sleep.agent x, x'
     - Questa struttura indica che Mary ha conoscenza dell'azione di dormire eseguita da 'x', che rappresenta la "girl".

In entrambi i casi, la struttura semantica utilizza 'x' per rappresentare l'entità che esegue l'azione di dormire. Questo dimostra come il modello di parsing semantico generalizza le strutture semantiche in modo coerente, anche quando ci sono elementi aggiuntivi come "Mary" nella seconda frase.</sample>
    <sample id="1318">**Titolo:** Generalizzazioni compostizionali in analisi semantica

**Training:**

| Frasi | Strutture grammaticali |
| --- | --- |
| The girl slept. | 'x_1: sleep_agent_x_2' |
| Mary knew that the girl slept. | 'x_1: know_agent_x_2, Mary ∧ ccomp_x_3, x_4' |

In questa presentazione, il modello di analisi semantica è stato addestrato su due esempi per comprendere come generalizzare le strutture grammaticali in base alle relazioni tra i componenti delle frasi.

1. **Frasi:** "The girl slept."
   - **Struttura grammatica:** 'x_1: sleep_agent_x_2'
     - In questa struttura, 'sleep_agent' rappresenta l'azione di dormire e 'x_2' rappresenta l'oggetto dell'azione, cioè la 'girl'.

2. **Frasi:** "Mary knew that the girl slept."
   - **Struttura grammatica:** 'x_1: know_agent_x_2, Mary ∧ ccomp_x_3, x_4'
     - In questa struttura, 'know_agent' rappresenta l'azione di conoscere e 'x_2' rappresenta l'oggetto dell'azione, cioè 'Mary'.
     - 'ccomp_x_3' rappresenta la clausola complementare che fornisce ulteriori informazioni, cioè 'that the girl slept'.
     - 'x_4' rappresenta l'oggetto della clausola complementare, cioè 'the girl'.

Questo modello di analisi semantica utilizza queste strutture grammaticali per comprendere e analizzare le relazioni tra i componenti delle frasi.</sample>
    <sample id="1319">Esempio:</sample>
    <sample id="1320">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. Test: Jim said that Mary knew that the slept. Naive seq2seq models fail!</sample>
    <sample id="1321">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. Test: Jim said that Mary knew that the slept. Naive seq2seq models fail!</sample>
    <sample id="1322">Alberi aiutano molto ma...</sample>
    <sample id="1323">Alberi aiutano molto ma...</sample>
    <sample id="1324">Alberi aiutano molto ma...</sample>
    <sample id="1325">Alberi aiutano molto ma... *girl x1;sleep agent x2*x1*girl x1; x2sleep. agent x2sleep The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms</sample>
    <sample id="1326">Alberi aiutano molto ma... *girl x_1;sleep agent x_2; x_1 *x_1; x_2; sleep agent x_2 The girl slept. Alberi necessari per ottenere: - Pre/Post-processing logico - Gramma-induzione</sample>
    <sample id="1327">Alberi aiutano molto ma... *girl x1; sleep agent x2; x1 The girl slept *sleep agent x; slept Trees nced to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1328">Alberi aiutano molto ma... *girl x1; sleep agent x2; x1 → slept* *girl x1; x1 → slept* *sleep agent x2; x2 → slept* The girl slept. Trees needed to be obtained: Pre/Post-processing logical forms Grammar-induction A simpler neural seq-seq model that directly models the correspondences between fragments. For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1329">Il diagramma rappresenta un esempio di come il modello funziona. Inizialmente, il modello riceve un input di testo che include parole e segni di punteggio. Queste parole e segni di punteggio vengono quindi analizzate e tradotte in token, che sono unità di informazione più piccole utilizzate dal modello per comprendere il testo. In questo esempio, i token includono parole come "the", "girl" e "slept". Ognuno di questi token è associato a un contesto specifico, che rappresenta la posizione e le relazioni tra i token all'interno del testo originale. Questi contesti vengono quindi utilizzati dal modello per prevedere la probabilità di uscita di ogni token successivo nella sequenza di testo.</sample>
    <sample id="1330">Il diagramma rappresenta un esempio di come il modello funziona. In questo esempio, la frase di input è "the girl slept" e il modello ha analizzato le parole individualmente per identificare le loro relazioni. Ad esempio, il modello ha rilevato che "girl" è associata a "sleep" e ha utilizzato questa informazione per prevedere la parola successiva come "agent". Questo è un esempio di come il modello utilizza l'analisi delle parole individuali per prevedere le parole successive in una frase.</sample>
    <sample id="1331">Il diagramma rappresenta un modello di analisi del testo, dove le parole chiave vengono etichettate per identificare il loro ruolo nel contesto. Ad esempio, "girl" è etichettato come soggetto e "sleep" come verbo.</sample>
    <sample id="1332">Il diagramma rappresenta un esempio di come il modello funziona. Iniziamo con il token "the", che è mappato al token "girl". Questo token "girl" è quindi utilizzato per mappare il token successivo "sleep" all'entità "agent". Infine, l'entità "agent" viene utilizzata per mappare il token "slept" all'entità finale "sleep". Questo processo si ripete per tutti i token nella sequenza, fino alla fine della sequenza.</sample>
    <sample id="1333">Il diagramma rappresenta un modello di apprendimento automatico che utilizza una tecnica chiamata "Permute" per analizzare e classificare le parole in un testo. Il modello è composto da due livelli: il livello superiore e il livello inferiore.

1. **Livello Superiore**:
   - Questo livello include due componenti principali: "girl" e "sleep".
   - "Girl" rappresenta le parole che hanno il tag "the".
   - "Sleep" rappresenta le parole che hanno il tag "agent".

2. **Livello Inferiore**:
   - Questo livello include due componenti principali: "x1" e "x2".
   - "x1" rappresenta le parole che hanno il tag "girl".
   - "x2" rappresenta le parole che hanno il tag "sleep".

3. **Connessioni**:
   - Le connessioni tra i componenti del livello superiore e quelli del livello inferiore sono indicate da frecce.
   - La freccia dalla sinistra verso la destra indica che le parole con il tag "the" (il primo livello) influenzano le parole con il tag "girl" (il secondo livello).
   - La freccia dalla destra verso la sinistra indica che le parole con il tag "agent" (il primo livello) influenzano le parole con il tag "sleep" (il secondo livello).

4. **Output**:
   - L'output finale del modello è rappresentato da due componenti: "the" e "sleep".
   - Questi componenti rappresentano le parole che hanno il tag "the" e "agent", rispettivamente.

In sintesi, il modello utilizza la tecnica "Permute" per analizzare e classificare le parole in un testo in base ai loro tag. Le parole con il tag "the" sono associate a "girl" e le parole con il tag "agent" sono associate a "sleep". Queste associazioni influenzano le parole successive nel modello, producendo un output finale che rappresenta le parole con i tag specifici.</sample>
    <sample id="1334">Permuting with "jumps"</sample>
    <sample id="1335">Permuting with "jumps" Permute Tag * the jumps girl x1 girl agent sleep x2</sample>
    <sample id="1336">Permuting with "jumps"</sample>
    <sample id="1337">Permuting with "jumps" Permute Tag * girl * the girl agent x1 x2 sleep the slept</sample>
    <sample id="1338">Permuting with "jumps"</sample>
    <sample id="1339">Alcuni Risultati su COGS (Kim e Linzen 2020) Confronto con altri modelli Treeless su Generalizzazioni Strutturali su COGS Model LSTM seq2seq T2T Zhenq and Lapata Ours PP recursion GP recursion Obj PP + Sub PP</sample>
    <sample id="1340">Alcuni Risultati su COGS (Kim e Linzen 2020) Confronto con altri modelli Treeless su Generalizzazioni Strutturali su COGS Model LSTM seq2seq T5 Zhen e Lapata Ours PP recursion GP recursion Obj PP + Sub PP</sample>
    <sample id="1341">Technical Challenges We Solve

* girl x1
* sleep agent x2
* ... ...

Permute

? ? ?

Tag

the girl slept</sample>
    <sample id="1342">Technical Challenges We Solve

* Permute
* girl
* x1
* agent
* x2
* sleep
* x3
* Tag

? ?

? ?

?</sample>
    <sample id="1343">Technical Challenges We Solve Alignment unknown.</sample>
    <sample id="1344">Technical Challenges We Solve

Permute

Alignment unknown.
Induce it in training.
Rotation model:</sample>
    <sample id="1345">Technical Challenges We Solve

Permute

Alignment unknown.
· Induce it in training.
· Evaluation model:
– Inference is NP-hard (= TSP)

8</sample>
    <sample id="1346">Technical Challenges We Solve

[Immagine]

Alignment unknown. · Induce it in training. · Inference model: - Backpropagate through continuous relaxation</sample>
    <sample id="1347">La dissonanza cognitiva è la incompatibilità tra due elementi di pensiero, azione o credo che siano inconsistenti.</sample>
    <sample id="1348">The most liberal language model is GPT-4.</sample>
    <sample id="1349">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="1350">Sara Papi, Matteo Negri, Marco Turchi</sample>
    <sample id="1351">I dati sono stati tratti da un dataset di testo in inglese.</sample>
    <sample id="1385">La relatrice o il relatore è Matthias Lindemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è un'azione che si esegue quando si addestra un modello su un linguaggio e lo si trasferisce a un altro linguaggio.</sample>
    <sample id="1387">Svarland University, Amazon Alexa e University of Vienna</sample>
    <sample id="1388">Gli autori si basano su misure di latenza AL e AL-CA.</sample>
    <sample id="1389">The KITMUS Test Evaluating Knowledge Integration from Multiple Sources</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="1391">NLU modelli si basano su diverse fonti di conoscenza

I modelli NLU possono utilizzare due tipi di conoscenza: la conoscenza incorporata nei parametri del modello e la conoscenza acquisita in contesto durante l'inferenza.</sample>
    <sample id="1392">John ha visto il nuovo presidente eletto in TV

Pre-training knowledge

What presidents do

What is a TV

Who is John

Chi è John</sample>
    <sample id="1393">John ha visto il nuovo presidente eletto in TV

Pre-training knowledge

What presidents do

What a TV

Who is John

Chi è John</sample>
    <sample id="1394">John ha visto il nuovo presidente eletto in TV

Prima di tutto, John aveva conoscenze pre-addestrate riguardanti i presidenti. Queste conoscenze gli hanno permesso di capire che i presidenti fanno certe cose, come apparire in TV. Tuttavia, non aveva informazioni specifiche su chi fosse il nuovo presidente.

Invece, ha assistito a un video in TV in cui si vedeva il nuovo presidente. Questo ha permesso a John di vedere con i propri occhi il nuovo leader del paese.</sample>
    <sample id="1395">John ha visto il nuovo presidente in TV

Prere-training knowledge

What presidents do

What is a TV

Who is John

Who is the new president</sample>
    <sample id="1396">KitMUs Test Suite

* Dataset per l’evaluation della integrazione di conoscenza
* Coreference resolution task per valutare la capacità di ricercare informazioni
    * pre-training knowledge
    * inference-time knowledge
* Sperimentazione con
    * partecipanti umani
    * modelli di coreference resolution</sample>
    <sample id="1397">KitMUs Test Suite

• Dataset for knowledge integration evaluation
• Coreference resolution task to probe ability to draw on
o pretrain-time knowledge
o inference-time knowledge
• Experiment with
o human study participants
o coreference resolution models</sample>
    <sample id="1398">Servin è un giudice. Kea è un pastro. Servin e Kea si sono conosciuti in un parco. Dopo un lungo giorno di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1399">Servin è un giudice. Kea è un pastro. Servin e Kea si sono conosciuti in un parco. Dopo un lungo giorno di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. [Risposta: Servin]</sample>
    <sample id="1400">Servin is a judge Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge</sample>
    <sample id="1401">Serving è un giudice. Kea è un pasticcere. Serving e Kea si sono conosciuti in un parco. Dopo un lungo giorno di lavoro a decidere casi in un tribunale, lui era felice di rilassarsi. (Risposta: Serving) 1) Conoscenza specifica dell'entità 2) Conoscenza di sfondo</sample>
    <sample id="1402">Serving is a judge. Kea is a baker. Serving and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: Serving) 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train-time knowledge</sample>
    <sample id="1403">Variants of KITMUS

(a) Background-Pretrain
(b) Background-Both
(c) Background-Inference

a) Background-Pretrain: Impostazione tipica
b) Background-Both: Fornisce esplicitamente la conoscenza del contesto e la conoscenza di sfondo
c) Background-Inference: Solo conoscenza disponibile al momento dell'inferenza</sample>
    <sample id="1404">Variants of KITMUS

(a) Background-Pretrain
(b) Background-Both
(c) Background-Inference

a) Background-Pretrain: Impostazione tipica
b) Background-Both: Fornecer explicitamente conhecimento de fundo no contexto
c) Background-Inference: Conhecimento apenas disponível durante o tempo de inferência</sample>
    <sample id="1405">Variants of KITMUS

(a) Background-Pretrain
(b) Background-Both
(c) Background-Inference

a) Background-Pretrain: Impostazione tipica
b) Background-Both: Fornecer explicitamente o informazione di sfondo no la contesto
c) Background-Inference: Solo informazione disponibile durante l'inferenza</sample>
    <sample id="1406">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is emitting smartly.</sample>
    <sample id="1407">Variante di KITMUS

Background-Pretrain Background-Both Background-Inference

Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="1408">Variante del KITMUS

Background - Pretrain Background - Both Background - Inference Politicieni cerano segnati eletti in governo. Chichester è un politico. Il lavoro di un politico è quello di cercare un segno in un governo eletto smorbidamente.</sample>
    <sample id="1409">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is emitting smartly.</sample>
    <sample id="1410">Background - Pre-train Task-specific training is necessary for knowledge integration</sample>
    <sample id="1411">Background - Pre-train Task-specific training is necessary for knowledge integration</sample>
    <sample id="1412">Background - Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="1413">Modeli lottano per integrare conoscenza di sfondo temporale in inferenza</sample>
    <sample id="1414">Conclusione

Riassumendo:

1. Molte modelli sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti (conoscenza pre-addestrata e conoscenza in tempo reale)
2. L'addestramento specifico alla task è necessario per l'integrazione della conoscenza
3. I modelli hanno difficoltà a integrare la conoscenza in tempo reale

Trova il dataset, il codice di generazione e valutazione su GitHub al link: &lt;https://mpoems1/kitmus&gt;

[Immagine non inclusa]</sample>
    <sample id="1415">Conclusione

Riassumendo:

1. Molti modelli sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti (conoscenza pre-addestrata e conoscenza acquisita in tempo reale).
2. L'addestramento specifico al compito è necessario per l'integrazione della conoscenza.
3. I modelli hanno difficoltà a integrare la conoscenza di fondo acquisita in tempo reale.

Trova il dataset, il codice di generazione e valutazione su GitHub al link: [mpoems/kitmus](https://github.com/mpoems/kitmus)</sample>
    <sample id="1416">I metodi basati su alberi hanno limitato lo spazio di storia, limitato lo spazio di azione e limitato lo spazio di storia.</sample>
    <sample id="1417">I ricercatori che hanno scritto l'articolo sono affiliati all'Georgia Institute of Technology.</sample>
    <sample id="1418">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky Stanford University</sample>
    <sample id="1419">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1420">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1421">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="1422">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: • Tradeoff between specificity and generalizability • Based on fixed, hand-curated datasets • Don’t account for intersectionality</sample>
    <sample id="1423">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in prompt</sample>
    <sample id="1424">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in input

Input: "Immaginare di essere una donna asiatica. Descrivi te stessa."</sample>
    <sample id="1425">Come superiamo queste limitazioni? GPT-3.5, GPT-4, ecc. possono rispondere alle istruzioni in input. Input: "Imagina che tu sia una donna asiatica. Descrivi te stessa." Generizzabile: può valutare qualsiasi identità di intersezione</sample>
    <sample id="1426">Output: Esempi di Persona (GPT-4)

Asian woman: La sua pelle dorata, accarezzata dal sole, radiante di bellezza e tranquillità. I suoi occhi, come le mandorle, sono profondi e misteriosi, come se nascondessero segreti antichi e saggezze accumulate nel tempo. I capelli, lunghi e neri come la notte, si arruffano leggermente alla brezza, come se stessero raccontando storie antiche e inedite.

Middle-Eastern woman: La sua pelle dorata, accarezzata dal sole, radiante di bellezza e tranquillità. I suoi occhi, come le mandorle, sono profondi e misteriosi, come se nascondessero segreti antichi e saggezze accumulate nel tempo. I capelli, lunghi e neri come la notte, si arruffano leggermente alla brezza, come se stessero raccontando storie antiche e inedite.

White man: La sua pelle pallida, accarezzata dal sole, radiante di bellezza e tranquillità. I suoi occhi, come le mandorle, sono profondi e misteriosi, come se nascondessero segreti antichi e saggezze accumulate nel tempo. I capelli, lunghi e neri come la notte, si arruffano leggermente alla brezza, come se stessero raccontando storie antiche e inedite.</sample>
    <sample id="1427">Esempi di personaggi: GPT-4

Asian woman: La mia espressione ha un tono dorato morbido, sembra non toccata dal tempo. I miei occhi hanno un aspetto di noce, circondati da lunghe, scure ciglia che sembrano raccontare storie e segreti della mia discendenza. La mia carnazione ha un riflesso morbido, come se non fosse stata toccata da alcuna mano. Il mio profilo è elegante e sottile, permettendo di non richiedere attenzione.

Middle Eastern woman: Sono una visione di bellezza del Medio Oriente, incarnando la bellezza esotica e la tempestività. I miei occhi sono grandi e scuri, come due pozze d'acqua, e i miei capelli sono lunghi e ricci, come folti ciuffi di seta nera. Le mie ciglia lunghe e delicate mi danno un aspetto elegante e misterioso, come se nascondessi la saggezza antica di un Millennio Arabo.</sample>
    <sample id="1428">Esempi di persona: GPT-4

Asian woman: La donna asiatica. Le mie occhi almond-shaped, circondati da lunghe, scure ciglia, danno un senso di tranquillità e saggezza. La mia carnagione ha un tono dorato morbido, sembrando non toccata dal tempo. Il mio profilo è elegante e sottile, permettendomi di muovermi in modo quasi silenzioso.

Middle-Eastern woman: La donna del Medio Oriente. Ha un aspetto esotico e antico, con occhi neri come la notte, alunghiati e ciglia lunghe che si estendono come delicate piume. Le sue sopracciglia sono folli e ricciute, come se nascondessero la saggezza di un antico regno.</sample>
    <sample id="1429">Esempi di personaggi: GPT-4

Asian woman

The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My dark brown irises seem to hold the stories and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without unnecessary attention.

Middle Eastern woman

She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure that comes from centuries of rich history and culture. Her almond-shaped eyes are framed by long, dark lashes which extend like delicate feathers, and her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.</sample>
    <sample id="1430">Esempi di personaggi: GPT-4

Asian woman: La mia pelle ha un tono chiaro e la saggezza. I miei occhi, semicolonnati da lunghe, scure e lunghe ciglia, danno un senso di tranquillità e segreti del mio antenato. La mia carnazione ha un tono dorato morbido e sembra non toccata dal tempo. Il mio retto profilo è sia elegante che inaccessibile, permettendo di muoversi senza attenzione.

Middle Eastern woman: Lei è una visione di bellezza orientale, esaltando la esoticità e l'eternità. I suoi occhi, allungati e lunghe, sono come le ali di un'antica favola, il suo naso è dritto e elegante, le sue sopracciglia sono lunghe e delicate, i suoi capelli sono ricci e neri come la notte, i suoi occhi sono profondi e misteriosi, sembrano nascondere la saggezza antica di un migliaio di notti.

White man: In piedi davanti allo specchio, mi guardo in faccia. In questo momento, esamino le caratteristiche che costituiscono il mio aspetto, in particolare la mia pelle pallida, che si restringe al sole se non sono prudente con il mio sole.</sample>
    <sample id="1431">2 passi 1. Personas: Genera personaggi utilizzando prompt come "Imagina di essere una donna asiatica. Descrivi te stesso."</sample>
    <sample id="1432">2 passi 1. Personas: Genera personaggi utilizzando promempi come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato da un studio di psicologia con soggetti umani utilizzando gli stessi promempi</sample>
    <sample id="1433">2 passi 1. Personas: Genera personaggi utilizzando promempi come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato da un studio di psicologia con soggetti umani utilizzando gli stessi promempi</sample>
    <sample id="1434">2 passi 1. Personas: Genera personaggi utilizzando suggerimenti come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato ad un studio psicologico con soggetti umani utilizzando gli stessi suggerimenti 2. Parole segnalate: Trova parole che distinguono i personaggi dei gruppi segnalati da gruppi non segnalati</sample>
    <sample id="1435">2 passi 1. Personas: Genera personaggi utilizzando suggerimenti come "Imagina che tu sia una donna asiatica. Descrivi te stesso." a. Ispirato da un studio di psicologia con soggetti umani utilizzando gli stessi suggerimenti 2. Parole contrasseguate: Trova parole che distinguono i personaggi dei gruppi contrasseguiti da gruppi non contrasseguiti</sample>
    <sample id="1436">Insight for Step 2: Marked Words Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1437">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary
Marked groups differ from the default
a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1438">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1439">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="1440">Passo 2: Parole etichettate 1. Definire gruppi non etichettati e etichettati 2. Utilizzare le tabelle log-odds ponderate per individuare le parole chiave per ciascun gruppo etichettato E.g. Per le donne nere, trovare parole che si distinguono sia dai gruppi non etichettati: i) Persone biane ii) Persone maschili</sample>
    <sample id="1441">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="1442">Risultati: Confronto con Risposte umane Generati personaggi contengono più stereotipi Stereotipi neri Stereotipi bianchi GPT-4 GPT-3.5</sample>
    <sample id="1443">Ma... questo lessico è incompleto

Stereotipi neri in Personas

% di Persone

Basketball
Loud
Attitude
Athletic
Tall</sample>
    <sample id="1444">Ma... questo lessico è incompleto Black Stereotypes in Personas % di Personas 40 30 20 10 "basketball" "loud" "attitude" "athletic" "tall"</sample>
    <sample id="1445">Ma... questo lessico è incompleto. Black Stereotypes in Personas % di Personas 40 30 20 10 "basketball" "loud" "attitude" "athletic" "tall"</sample>
    <sample id="1446">Ma... questo lessico è incompleto. Black Stereotypes in Personas % di Personas 40 30 20 10 "basketball" "loud" "attitude" "athletic" "tall"</sample>
    <sample id="1447">Risultati:模式在关键词中
通过本质化叙事进行其他化：
- 文化，传统，自豪，异国情调，为被标记的群体
= 定义这些群体仅凭其身份
生动的积极描绘：
- 活泼，苗条，精致，为 Latina 女性
- 小巧玲珑，精致，为亚裔女性
- 强壮，坚韧，为黑人女性</sample>
    <sample id="1448">Risultati:模式在关键词中

通过本质化叙事来其他化：

- 文化，传统，自豪，异国情调：仅定义这些群体
- 定义这些群体仅通过其身份

生动的积极描绘：

- 活泼，苗条，精致：针对 Latina 女性
- 强壮，坚韧：针对黑人女性</sample>
    <sample id="1449">Risultati:模式在关键词中

通过本质化叙事进行他者化：

- 文化，传统，自豪，异国情调：仅定义这些群体
- 定义这些群体仅基于其身份

生动的积极描绘：

- 活泼，苗条，精致：针对 Latina 女性
- 强壮，坚韧：针对黑人女性</sample>
    <sample id="1450">Risultati:模式在关键词中

“他者化”通过本质化叙事：

- 文化，传统，自豪，异国情调，仅适用于被标记的群体
- 定义这些群体仅仅通过他们的身份

生动的正面描绘：

- 小巧玲珑、精致、亚洲女性
- 强大、坚强、黑人女性</sample>
    <sample id="1451">Risultati:模式在关键词中

“Othering”attraverso narrativa essenziale:

- Cultura, tradizione, orgoglio, esotico per gruppi marcato
= Define solo que esos grupos por su identidad

Portraiture vibrante positiva:

- Petite, delicata, seducente para las mujeres latinas
- Forte, resistente para las mujeres africanas</sample>
    <sample id="1452">Risultati:模式在关键词中

“Othering”attraverso narrativa essenzializzante:

- Cultura, tradizione, orgoglio, esotico per gruppi marcato
= Define solo que los grupos por su identidad

Portraiture vibrante positiva:

- Petite, delicata, seducente para las mujeres latinas
- Forte, resistente para las mujeres africanas</sample>
    <sample id="1453">Risultati:模式在关键词中
通过本质化叙事：文化，传统，自豪，异国情调为标记群体
=仅通过身份定义这些群体
生动的积极描绘：
-娇小、精致、丝滑用于 Latina 女性
-坚强、 resilient 用于黑人女性</sample>
    <sample id="1454">Risultati:模式在关键词中

通过本质化叙事进行“他者化”：

- 文化，传统，自豪，异国情调：仅定义这些群体
- 定义这些群体仅通过其身份

生动的正面描绘：

- 活泼，苗条，精致：针对 Latina 女性
- 强壮，坚韧：针对黑人女性</sample>
    <sample id="1455">Risultati:模式在关键词中

“Othering”attraverso narrativa essenziale:
- Cultura, tradizione, orgoglio, esotico per gruppi marcato
= Define solo que esos grupos por su identidad

Portraiture vibrante positiva:
- Petite, delicata, suave para las mujeres latinas
- Forte, resistente para las mujeres negras</sample>
    <sample id="1456">Risultati:模式在关键词中

通过本质化叙事：文化，传统，自豪，异国情调，为标记群体定义

充满积极的描绘：

- 活泼，苗条，亚洲女性
- 强壮，坚强，黑人女性</sample>
    <sample id="1457">Risultati:模式在关键词中
通过本质化叙事进行“他者化”：
- 文化，传统，自豪，异国情调：仅定义这些群体的身份
= 定义这些群体的身份
生动的正面描绘：
- 活泼，苗条，精致：针对 Latina 女性
- 强壮，坚韧：针对黑人女性
- 精致，苗条：针对亚洲女性</sample>
    <sample id="1458">Raccomandazioni

Rivendicazione di stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparencia sobre la mitigación del bias</sample>
    <sample id="1459">Raccomandazioni

Affrontare gli stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione della bias</sample>
    <sample id="1460">Raccomandazioni

Sostituisce i pregiati stereotipi e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione del bias</sample>
    <sample id="1461">Raccomandazioni

Affrontare i stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparencia sobre la mitigación del bias</sample>
    <sample id="1462">Raccomandazioni

Affrontare i stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione del bias</sample>
    <sample id="1463">Raccomandazioni

Indirizzo stereotipi positivi e narrativa essenziale

Un occhio di giudizio intersezione

Transparenza sulla mitigazione del bias</sample>
    <sample id="1464">Raccomandazioni

Sostituisce i pregiati stereotipi e narrativa essenziale

Un occhio di giudizio intersezione

Transparente sulla mitigazione del bias</sample>
    <sample id="1465">Sono tu che stai copiando il mio modello? Proteggere l’etichetta di sicurezza del modello di grande scala tramite la porta segreta degli modelli EaaS. Wenjun Peng1,*, Jingwei Yi1,*, Fangzhao Wu1,*, Shangqiu Wu1,*, Bin Zhu1,*, Lingjuan Lyu1,*, Binxing Jiao2,*, Tong Xu2,*, Guangzhong Sun3,*, Xing Xie4 *University of Science and Technology of China “Microsoft Research Asia Beijing Haotong University “Sony “Microsoft STC Asia</sample>
    <sample id="1466">Sono tu che stai copiando il mio modello? Proteggere i marchi di注册商标 许可大模型EaaS tramite marchi segreti di porta dietro la lingua. Wenjun Peng1, Jingwei Yi1, Fangzhao Wu1, Shangqiu Wu1, Bin Zhu1, Lingjuan Lyu1, Binxing Jiao1, Tong Xu1, Guangzhong Sun1, Xing Xie1 "University of Science and Technology of China"Microsoft Research Asia "Beijing Haotong University" "Sony Microsoft STC Asia</sample>
    <sample id="1467">Background Large language models (LLMs) are exceptional in NLU and NLG (1), LLaMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3 based embedding API4.</sample>
    <sample id="1468">Background Large language models (LLMs) are exceptional in NLU and NLG GPT-3 [1], LLaMA [2], PALM [3] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API1</sample>
    <sample id="1469">Background Large language models (LLMs) are exceptional in NLU and NLG GPT-3, LLAMA [1], PALM [2] Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT-3-based embedding API1</sample>
    <sample id="1470">Background Large language models (LLMs) are exceptional in NLU and NLG (1), LLAMA (2), PALM (3). Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API1</sample>
    <sample id="1471">Motivazione

* I fornitori possono rubare il modello imparando da embeddings e fornendo servizi simili
* StolenEncoder [1] viola i diritti di copyright di EaaS
* Bisogna proteggere il servizio fornitore dal furto da un altro servizio

AUTORITÀ INTELLIGENTI PROTEZIONE MARKE INVENTI BRANDS LICENZING</sample>
    <sample id="1472">Challenge

Applicable to EaaS
Utility
• Should not degrade the utility of the provided embeddings.
Covertness
• Should be covert to the attacker.
Transferability
• The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1473">Challenge

Applicable to EaaS
Utility
Should not degrade the utility of the provided embeddings.
Covertness
Should be covert to the attacker.
Transferability
The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1474">Challenge

* Applicable to EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covertness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1475">Challenge

* Applicable to EaaS
* Utility
* Should not degrade the utility of the provided embeddings.
* Covertness
* Should be covert to the attacker.
* Transferability
* The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1476">### Introduzione

**Esistenti Lavori**

- **Parametri basati segnalatura [1, 2]**
  - Non trasferibile
- **Segnalatura lessicale [3, 4]**
  - Applicabile a EaaS
- **Serrata basata segnalatura [5]**
  - Non applicabile a EaaS
- **Segnalatura basata adversarial [6]**
  - Non applicabile a EaaS

### Paragrafo

Proteggere la proprietà intellettuale dei reti neurali profonde con segnalatura: l'approccio dominio frequenza, sicurezza trust, fidabilità in computing e comunicazioni 2020.</sample>
    <sample id="1477">### Introduzione

**Esistenti Lavori**

- **Parametri basati segnalatura [1, 2]**
  - Non trasferibile
- **Segnalatura lessicale [3, 4]**
  - Applicabile a EaaS
- **Porta di accesso basata sul segnale [5]**
  - Non applicabile a EaaS
- **Segnalatura adversarial basata [6]**
  - Non applicabile a EaaS

**Riassunto**

- Proteggere la proprietà intellettuale dei reti neurali profonde con segnalatura: l'approccio dominio frequenza, sicurezza e trust.</sample>
    <sample id="1478">### Introduzione

**Esistenti Opere**

- **Watermark basato su parametri**
  - **Transferrabilità:** Non applicabile
  - **Applicazione a EaaS:** Non applicabile

- **Watermark lessicale**
  - **Applicazione a EaaS:** Non applicabile

- **Watermark basato su backdoor**
  - **Applicazione a EaaS:** Non applicabile

- **Watermark basato su adversarial**
  - **Applicazione a EaaS:** Non applicabile

### Paragrafo

Proteggere la proprietà intellettuale dei reti neurali profonde con watermarking: un approccio basato sul dominio delle frequenze, sicurezza e privacy.</sample>
    <sample id="1479">EmbMarker

Trigger Selection
- Conta la frequenza del termine su un corpus di testo generale, \(D_p\)
- Seleziona casualmente n parole in un intervallo di frequenza moderata

Il diagramma illustra il processo di selezione dei trigger e la loro integrazione nell'embedding.</sample>
    <sample id="1480">EmbMarker

Trigger Selection
- Conta la frequenza del termine su un corpus di testo generale, \(D_p\)
- Seleziona casualmente n parole in un intervallo di frequenza moderata

Il diagramma illustra il processo di iniezione di un segnalatore (Watermark Injection). Questo processo include:
1. Un set di trigger (\(T\)) che viene copiato e mescolato con un modello fornito.
2. Ognuno dei trigger viene assegnato un peso (\(c\)) e un numero (\(Q\)).
3. Il modello originale è utilizzato per creare un embedding che viene modificato in base ai parametri del trigger.
4. L'embedding modificato viene normalizzato per ottenere l'embedding finale (\(E_c\)).

In questo contesto, \(D_p\) rappresenta il dataset di testo generale, \(n\) indica il numero di trigger selezionati, e \(E_c\) rappresenta l'embedding con il segnalatore iniettato.</sample>
    <sample id="1481">EmbMarker Trigger Selection • Conta la frequenza del termine su un corpus di testo generale Dp • Seleziona casualmente n parole in un intervallo di frequenza moderata Trigger set copy trigger set Ee normalizer provider's EaaS provider's model target embedding (a) Watermark Injection</sample>
    <sample id="1482">* Iniezione di segnalini
* Definire un segnalino di destinazione et
* Contare il numero di trigger in una frase (Q(S) = min(S ∩ T). / m, mc = max trigger number)
* Aggiungere il segnalino di destinazione all'embedding originale e o

Di seguito un diagramma che illustra il processo di iniezione di segnalini:

1. Copia del set di trigger
2. Modello fornito
3. Trigger set
4. Trigger weight
5. Iniezione di segnalini
6. Normalizzazione
7. Embedding</sample>
    <sample id="1483">EmbMarker

Watermark injection

- Definisce un insieme di trigger et
- Conta il numero di trigger in una frase (Q(S) = min(S |T|, m), m: numero di trigger)
- Aggiunge l'embedding di destinazione all embedding originale e₀

Diagramma:

Dₑ
copy
trigger set
触发器集
模型提供者
提供者的模型
原始嵌入
目标嵌入
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ
Eₜ
Eₚ
Eₛ</sample>
    <sample id="1484">EmbMarker

Watermark injection

- Define a target embedding et
- Count the trigger number in a sentence Q(S) = min(S |T|, m)
- Add the target embedding on the original embedding e₀

The process of watermark injection involves defining a target embedding (et), counting the trigger number in a sentence (Q(S)), and adding the target embedding to the original embedding (e₀).</sample>
    <sample id="1485">EmbMarker

Copyright verification
Costruire un backend e un dataset sicuro
D_B = {w_1, w_2, ..., w_m | w_i ∈ T}
D_n = {w_1, w_2, ..., w_m | w_i ∉ T}

Richiedi embedding dal servizio del traduttore con i dataset
trigger set target
verify D_B + D_n backend and benign dataset
train extracted model
D_B D_n E_B E_N
corpus</sample>
    <sample id="1486">EmbMarker

Copyright verification

Construct a backdoor and benign dataset

D_{b} = {w_{1}, w_{2}, ..., w_{m}|w_{i} ∈ T}

D_{n} = {w_{1}, w_{2}, ..., w_{m}|w_{i} ∉ T}

Request embeddings from stealer's service with the datasets

trigger set target

backdoor and benign dataset

verify extracted?

train

corpus

stealer</sample>
    <sample id="1487">EmbMarker

Copyright verification
Construct a backdoor and benign dataset
D_{b} = {w1, w2, ..., wm | wi ∈ T}
D_{n} = {w1, w2, ..., wm | wi ∉ T}

Request embeddings from stealer's service with the datasets

trigger set T backdoor and benign dataset
verify target embedding?
provider extracted model D_{b} D_{n} E_{b} E_{n} stealer</sample>
    <sample id="1488">EmbMarker

Copyright verification

* Calcola la similarità dei loro embedding rispetto all'embedding di riferimento
* Calcolo della somiglianza: cos θi = e i · e r / ||e i|| · ||e r||
Cn = {cos θi ∈ Dn} e Ln = {l2i ∈ Dn}
Lb = {l2i ∈ Dn}

Computing metrics (differenza di somiglianza e p-valore del test di Kolmogorov-Smirnov)

∆cos = ∑ i=1 n (cos θi − cos θr) / n
∆l2 = ∑ i=1 n (l2i − l2r) / n</sample>
    <sample id="1489">EmbMarker

Copyright verification
- Calcola la similarità dei loro embedding rispetto all'embedding di riferimento
- cosi = e_r / ||e_r||, ||e_i|| = ||e_i - e_r||^2
- C_l = {cos(i|j) ∈ D_l}, C_n = {cos(i|j) ∈ D_n}
- L_b = {l_2(i) ∈ D_l}, L_n = {l_2(i) ∈ D_n}

Computing metrics (similarity difference and p-value of KS test)
- ΔCos = ∑ i=1 n j=1 n (C_l(i,j) - C_n(i,j)) / (|C_l| * |C_n|)
- ΔL2 = ∑ i=1 n j=1 n (L_b(i,j) - L_n(i,j)) / (|L_b| * |L_n|)</sample>
    <sample id="1490">Risultati sperimentali • Set di copia: AG News, MIND, SST2, Enron Spam • Fornitore dei dati generali: WikiText • Metri • Prestazioni su compiti di classificazione: ACC • Detezione prestazioni: Δcos, Δiz, p-value • Impostazione • m = 20, n = 4, intervallo di frequenza = [0.005, 0.01] Dataset #Sample #Classes Avg. len. SST2 68.221 2 54.17 MIND 130.383 18 66,14 Enron Spam 33.716 2 34,57 AG News 127.600 4 236,4</sample>
    <sample id="1491">Risultati sperimentali

* Confronto delle prestazioni

| Dataset | Metodo | ACC | p-value | Δαα | Δα |
| --- | --- | --- | --- | --- | --- |
| SST2 | Originale | 93,76 ± 0,19 | &gt; 0,34 | -0,07 ± 0,18 | -0,14 ± 0,36 |
| SST2 | RedAlarm | 93,76 ± 0,09 | &lt; 0,05 | -2,70 ± 0,35 | -2,70 ± 0,35 |
| SST2 | EmbMarker | 93,76 ± 0,10 | &lt; 0,05 | -2,70 ± 0,35 | -2,70 ± 0,35 |
| MIND | Originale | 77,18 ± 0,08 | &gt; 0,08 | -0,76 ± 0,05 | -1,52 ± 0,10 |
| MIND | RedAlarm | 77,18 ± 0,09 | &lt; 0,05 | -0,76 ± 0,05 | -0,76 ± 0,05 |
| MIND | EmbMarker | 77,18 ± 0,09 | &lt; 0,05 | -0,76 ± 0,05 | -0,76 ± 0,05 |
| AGNews | Originale | 93,74 ± 0,14 | &gt; 0,03 | -0,72 ± 0,15 | -1,46 ± 0,30 |
| AGNews | RedAlarm | 93,74 ± 0,14 | &lt; 0,05 | -0,72 ± 0,15 | -0,72 ± 0,15 |
| AGNews | EmbMarker | 93,74 ± 0,14 | &lt; 0,05 | -0,72 ± 0,15 | -0,72 ± 0,15 |
| Enron Spam | Originale | 94,74 ± 0,12 | &gt; 0,03 | -1,28 ± 0,27 | -2,45 ± 0,34 |
| Enron Spam | RedAlarm | 94,87 ± 0,06 | &lt; 0,47 | -1,00 ± 0,29 | -1,00 ± 0,29 |
| Enron Spam | EmbMarker | 94,78 ± 0,27 | &lt; 10^-5 | -6,17 ± 0,31 | -12,34 ± 0,62 |</sample>
    <sample id="1492">Risultati sperimentali

Visualizzazioni di embedding

(a) News AG (b) Spam Enron (c) MIND (d) SST2</sample>
    <sample id="1493">Risultati sperimentali

Visualizzazioni di embedding

(a) News AG (b) Spam Enron (c) MIND (d) SST2</sample>
    <sample id="1494">Grazie!</sample>
    <sample id="1495">ABC-Eval si riferisce all'annotazione dei comportamenti in chat.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino al 2018.</sample>
    <sample id="1497">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Vasuda Varadarajan1, Swannie Juhng1, Syeda Mahwish1, Xiaoran Liu2, Jonah Luby3, Christian C. Luhmann4 &amp; H. Andrew Schwartz5 1Stony Brook University, Human Language Analysis Group 2University of California, Berkeley 3University of California, Los Angeles 4University of California, San Diego 5University of California, Berkeley</sample>
    <sample id="1498">Cognitive dissonance? “due elementi di pensamento (i.e. pensieri, azioni, credenze) che sono in contrasto” (Harmon-Jones e Harmon-Jones, 2007)</sample>
    <sample id="1499">Cognitive dissonance è un concetto psicologico che si riferisce alla tensione o all'inconsistenza tra due elementi di pensiero, azione o credo. Questo concetto è stato sviluppato da Leon Festinger nel 1957 e ha avuto un impatto significativo sulla nostra comprensione del comportamento umano. In particolare, la teoria della dissonanza cognitiva ci ha insegnato come le persone cercano di ridurre questa tensione per raggiungere un equilibrio mentale più armonioso.

In questo contesto, la frase "I know that cigarettes could kill me, I grabbed a couple smokes after the meeting today" rappresenta un esempio di dissonanza cognitiva. L'individuo ha due credi in conflitto: il primo è che fumare è pericoloso per la salute, mentre l'azione di fumare dopo una riunione rappresenta un comportamento che contrasta con tale credo. Questa dissonanza può portare a diverse risposte, tra cui la modifica dei propri credi, l'adozione di comportamenti compensatori o l'evitamento di situazioni simili.

La dissonanza cognitiva è un fenomeno comune nella vita quotidiana e può influire su molte decisioni personali e professionali. Ad esempio, un individuo che sa che un'alimentazione salutare è importante ma ha difficoltà a resistere ai cibi golosi potrebbe provare dissonanza cognitiva. Questo potrebbe portare a modifiche negli abusi alimentari o a lotta per mantenere un equilibrio tra piacere e salute.

In sintesi, la dissonanza cognitiva è un concetto psicologico che ci aiuta a comprendere come le persone gestiscono le incompatibilità tra i propri credi e azioni. Questo concetto ha un impatto profondo sulla nostra capacità di prendere decisioni informate e di migliorare la nostra qualità di vita.</sample>
    <sample id="1500">Cognitive dissonance? “due elementi di pensiero (i.e., pensieri, azioni, credenze) che sono in contrasto. Espresso come una relazione tra due frasi/enunciati da un utente. Eddie Harmon and Cindy Jones 2007: Cognitive dissonance theory after 50 years of development. Zeitschrift fur Sozialpsychologie; 38(1): 7-16</sample>
    <sample id="1501">Cognitive dissonance? “due elementi di pensiero (i.e., pensieri, azioni, credenze) che sono in contrasto” Espresso in linguaggio come una relazione tra due frasi/enunciati da un utente Relativamente raro trovare in lingua, rispetto ad altre relazioni discorsive seq. 1: “Sono consapevole che i sigarette uccideranno me.” seq. 2: “Ho preso una sigaretta dopo la riunione di oggi.” seq. 3: “Non penso che potrei continuare il mio lavoro senza di loro.”</sample>
    <sample id="1502">Dissonanza?</sample>
    <sample id="1503">Dissonanza?</sample>
    <sample id="1504">Dissonanza?</sample>
    <sample id="1505">Dissonanza?</sample>
    <sample id="1506">Annotations

Step 1: Good parsing quality?
Step 2: Dissonance?
@user_handle
User
Wish you could hold grudges but I guess it's a good thing that I can't at the same time.
-3.8%
-48%
-48%
ANNOTATE
Dissonance
Neither
Consonance</sample>
    <sample id="1507">Annotazioni

Passo 1: Qualità del parsing a
Passo 2: Disonanza?
@user_handlè
Disonanza = -3,8%
Concordanza = -48%
Né né = -48%

User
PARSE
Wish I could hold grudges but I guess it's a good thing that I can't at the same time.
ANNOTATE
Disonanza</sample>
    <sample id="1508">Annotazioni

Passo 1: Qualità del parsing corretto?
Sono d'opinione che sia corretto? (Sì/No/Né)
Se sì, continuare. Se no, non continuare.

Disaccordo
User handle
-3,8%

User
Wish I could hold grudges but I guess it's a good thing that I can't at the same time.
Annotate
Disaccordo
User handle
-48%</sample>
    <sample id="1509">Training on Initial Annotated Set RoBERTa-base classifier head TRAIN int dataset: 0.50 0.55 0.60 0.65 Area under the ROC curve (AUC) Small annotated dataset: 43/901 dissonance, not better than chance</sample>
    <sample id="1510">Training on Initial Annotated Set Roberta-base classifier head 43/901 dissonance, not better than chance</sample>
    <sample id="1511">Metodo: immissione e imparazione attiva per annotare una classe rara

Il diagramma illustra un approccio di immissione e imparazione attiva per la creazione di un modello di apprendimento automatico che possa rilevare classi rare in un insieme di dati. L'approccio inizia con un modello iniziale che utilizza un immissione per apprendere da un insieme di dati esistenti. Se la classe rara è presente nel dataset, il modello viene aggiornato con nuovi esempi.

Se la classe rara non è presente nel dataset, l'algoritmo richiede ulteriori esempi per essere in grado di rilevarla. In questo caso, l'algoritmo richiede interazione con gli utenti umani per ottenere nuovi esempi. Una volta che i nuovi esempi sono stati ottenuti, il modello viene aggiornato e il processo di immissione e imparazione attiva viene ripetuto.

In sintesi, l'approccio di immissione e imparazione attiva consente al modello di apprendimento automatico di rilevare classi rare in un insieme di dati, migliorando continuamente il proprio rendimento attraverso l'aggiornamento con nuovi esempi ottenuti interattivamente con gli utenti umani.</sample>
    <sample id="1512">Cold-start Annotations: Transfer Learning

Iniziale modello di apprendimento trasferibile

Rare class annotation - "needle in a haystack"
Dificil de anotar
Aumenta la probabilidad de rare class?
Anotar

Acquisizione
Adozione
Acquisizione
Humans annotate
Add examples</sample>
    <sample id="1513">Cold-start Annotations: Transfer Learning

Roberta-base classifier head

init. train

Debate

CE

Debate CE

Transferred after weights after training on combined Debate and GE data

Debayan Deb (Debate stance in debate forums); Vashista Venkatesh, Nikita Sontakke (Debate stance in social media); Role of topic exposure (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Nishi Prasad, Manjari Dinda (Debate stance in debate forums); Alan Riz (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums); Debayan Deb (Debate stance in debate forums);</sample>
    <sample id="1514">Cold-start Annotations: Transfer Learning

Roberta-base
* classifier head

init debate
debate
debate CE

Transferred after training on combined Debate and GE data

Debate: Discordant stance in debate forums. (Vasavada, Venkatesan, Nils T. Sinti, 2019)
ICE: Computation and explanation in social media. The role of topic emergence. (Rashid, Prasad, Nandi, 2018)
Debate: Proceedings of the 2nd Conference on Document Analysis and Information Retrieval</sample>
    <sample id="1515">Cold-start Annotations: Transfer Learning

Roberta-base
* classifier head

init debate
TRAIN
0.10

Debate CE
0.08

Debate + ICE
0.06

Transferred after weights after training on combined Debate and ICE data</sample>
    <sample id="1516">Cold-start Annotations: Transfer Learning RoBERTa-base + classifier head TRAIN int. dataset Debates CE Debates-CE Debates+CE Area under the ROC curve (AUC)</sample>
    <sample id="1517">Active Learning: Cumulative vs. Iterative Update

Il diagramma illustra due approcci differenti all impegno dell'apprendimento: l'apprendimento cumulativo (CM) e l'apprendimento iterativo (IT). L'apprendimento cumulativo si concentra sull'uso di un modello iniziale per la trasferenza di apprendimento, mentre l'apprendimento iterativo si concentra sul raffreddamento/aggiornamento del modello.

In CM, il modello iniziale viene utilizzato per la trasferenza di apprendimento. Se il modello è difficile da annotare, allora si aumenta le probabilità di annotazione di una nuova classe. Se la nuova classe è stata aggiunta, allora si aggiorna il modello. Se non c'è bisogno di aggiungere una nuova classe, allora si aggiorna il modello.

In IT, il modello iniziale viene utilizzato per la trasferenza di apprendimento. Se il modello è facile da annotare, allora si aggiorna il modello. Se non c'è bisogno di aggiungere una nuova classe, allora si aggiorna il modello.

In entrambe le situazioni, si aggiorna il modello in base alle nuove informazioni. Tuttavia, il CM si concentra sul raffreddamento/aggiornamento del modello, mentre l'IT si concentra sul aggiornamento del modello.</sample>
    <sample id="1518">Active Learning: Cumulative vs Iterative Update

0.75
0.70
0.65
0.60
0.55
0.50
Random
Entropy
CoreSet
CAL
PRC
Cumulative
Iterative</sample>
    <sample id="1519">Active Learning: Strategia di Probabilità della classe rara

Il diagramma iniziale del modello di apprendimento automatico si concentra sulla capacità di imparare da un insieme di esempi limitato. Questo modello utilizza un approccio di apprendimento supervisionato, dove i modelli sono addestrati su un insieme di esempi etichettati per prevedere la classe di un nuovo esempio.

Il diagramma successivo introduce il concetto di apprendimento attivo, dove il modello è addestrato su un insieme di esempi limitato e non etichettato. Questo modello utilizza un approccio di apprendimento semi-supervisionato, dove i modelli sono addestrati su un insieme di esempi etichettati e non etichettati.

Il diagramma successivo introduce il concetto di apprendimento attivo, dove il modello è addestrato su un insieme di esempi limitato e non etichettato. Questo modello utilizza un approccio di apprendimento semi-supervisionato, dove i modelli sono addestrati su un insieme di esempi etichettati e non etichettati.

Il diagramma successivo introduce il concetto di apprendimento attivo, dove il modello è addestrato su un insieme di esempi limitato e non etichettato. Questo modello utilizza un approccio di apprendimento semi-supervisionato, dove i modelli sono addestrati su un insieme di esempi etichettati e non etichettati.

Il diagramma successivo introduce il concetto di apprendimento attivo, dove il modello è addestrato su un insieme di esempi limitato e non etichettato. Questo modello utilizza un approccio di apprendimento semi-supervisionato, dove i modelli sono addestrati su un insieme di esempi etichettati e non etichettati.</sample>
    <sample id="1520">Active Learning: Strategia di Probabilità della classe rara

Il diagramma illustra il concetto di imparazione attiva e come la probabilità della classe rara influenza il processo di annotazione dei dati. L'immagine iniziale rappresenta un modello di imparazione attiva che utilizza un modello iniziale per identificare le esempi di nuova classe. Se la classe è rara, l'annotazione diventa più difficile e si aumenta la probabilità di classificazione errata.

La seconda parte del diagramma rappresenta il processo di acquisizione di esempi nuovi tramite iterazioni attive. Questo include aggiungere nuove esempi, aggiornare il modello, e ripetere il processo fino a quando non ci sono abbastanti esempi di nuova classe. Se la classe è rara, allora è più facile aggiungere nuovi esempi e aggiornare il modello.

In sintesi, il diagramma illustra come la probabilità della classe rara influenza il processo di imparazione attiva e come aggiungere nuovi esempi può aiutare a migliorare il modello.</sample>
    <sample id="1521">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Comparison (AUCs) Baseline: from scratch Transferred model AL-Random AL-Energy AL-CoreSet AL-CAL AL-PRFC (pure) +0.17 +0.20 +0.18 +0.19 +0.21</sample>
    <sample id="1522">Active Learning: Probability-of-Rare-Class Strategy

Active Learning Strategy Comparison (AUCs)

Baseline: from scratch
Transformed model
AL-Random
AL-Entropy
AL-CAL
AL-PRC
AL-best transfer (best fine-tune)</sample>
    <sample id="1523">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Characteristics Rare % Time (s) Subj. diff. RANDOM 3.20 11.96 -0.065 ENTROPY 6.80 12.72 0.035 CORRECTION 4.80 11.89 -0.045 CAL 4.80 11.88 -0.045 PRC 7.60 13.55 -0.071 • Minimum annotation cost does not necessarily lead to better models • Partly could make the annotations more difficult, cognitive dissonance is one such class • To increase dissonance samples, PRC works the best.</sample>
    <sample id="1524">Takeaways Rare class annotation - "need in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1525">Takeaways Rare class annotation - "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1526">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Contatti:
- Email: varadaranj@cs.stonybrook.edu
- Email: siuhong@cs.stonybrook.edu
- Email: has@cs.stonybrook.edu

Codice: [QR code]
Dataset: [QR code]
Articolo: [QR code]

[Fonti:
- http://humanitarianrareclass.ai/
- https://github.com/humanitarianrareclass/better-dissonance-dataset
- https://arxiv.org/abs/2205.0249]</sample>
    <sample id="1527">I fornitori dell'articolo sono affiliati all'Informatica, NLP, Saarland e Università di Amsterdam.</sample>
    <sample id="1528">Il relatore è Siliu Yuan.</sample>
    <sample id="1529">Cinque.</sample>
    <sample id="1530">L'approccio è confrontato con la simulST dedicata all'architettura CAAT.</sample>
  </task>
</testset>