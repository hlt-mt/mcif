<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su grandi quantità di dati da Internet, tra cui notizie politiche.</sample>
    <sample id="1">McGill University, Mila e Microsoft Research</sample>
    <sample id="2">Ciao, benvenuti alla nostra presentazione di Deplain, un nuovo corpus per la traduzione dei testi in tedesco al livello del documento e al livello della frase.</sample>
    <sample id="3">Il mio nome è Regina Stodden e io guiderò gli utenti per la prima parte della presentazione. Innanzitutto, dobbiamo definire la semplificazione testuale.</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come le persone con problemi di lettura o non nativi di lingua.</sample>
    <sample id="5">Per addestrare un modello di semplificazione del testo, è necessario un insieme di coppie parallele di testi, ad esempio due documenti o frasi.</sample>
    <sample id="6">In esempio qui si vede un par di frasi parallele, una frase complessa in tedesco e la sua traduzione in lingua semplice.</sample>
    <sample id="7">Per semplificare la frase, diverse tecniche sono possibili come si può vedere nell'esempio, come ad esempio la sostituzione lessicale, la clausola di cancellazione, la riformulazione o l'inserimento di parole.</sample>
    <sample id="8">Ora proponiamo i nostri nuovi corpus di plain because in recent years there were some problems with existing corpora. So, for example, these corpora here are too small to train a text classification model on.</sample>
    <sample id="9">I altri tre modelli, che sono stati proposti recentemente, sono tutti automaticamente allineati, cosa che significa che possono essere errori proni nella loro allineazione.</sample>
    <sample id="10">Quindi, propone il nuovo corpus DPlain, che è suddiviso in due sottocorpora: DPlain-APA e DPlain-WEB. DPlain-APA si basa su testi di notizie.</sample>
    <sample id="11">In DeepLearn API, we aligned 483 documents all manually. It results in roughly 30,000-13,000 parallel sentence pairs.</sample>
    <sample id="12">Per DeepPlainWeb, questa corpora include diverse domenе e inoltre aliniamo tutti questi 750 documenti su un lato manualmente e sull'altro con metodi di alinamento automatico.</sample>
    <sample id="13">In totale, si arrivano a 34.500 sentenze passate.</sample>
    <sample id="14">We analyze our sentence pairs a little bit more. For example, on the type of simplification.</sample>
    <sample id="15">Come puoi vedere qui, i testi della Bibbia sono molto più semplificati rispetto ad esempio i testi di notizie o i testi per apprendere un linguaggio.</sample>
    <sample id="16">In generale, riguardo ad esempio la semplificazione lessicale, la semplificazione strutturale o la semplificazione a livello generale.</sample>
    <sample id="17">Tuttavia, è evidente che il dataset Deplain ha una ampia varietà di diverse trasformazioni semplificative. Ad esempio, nel dataset Deplain-AP, abbiamo molti più riformulazioni e aggiunte di parole rispetto al dataset Deplain-Web.</sample>
    <sample id="18">Sul lato opposto, nel corpus web abbiamo molti più riformulazioni.</sample>
    <sample id="19">So let's now see what we can do with this corpus. Hello, I am Omar and now I will talk about the use cases for our dataset DeepL. So for the first use case we can evaluate automatic alignment methods.</sample>
    <sample id="20">In recent years, there has been a lot of alignment methods but in the context of machine translations.</sample>
    <sample id="21">Dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre alleanze di frasi nei due documenti.</sample>
    <sample id="22">Ma nel nostro caso di studio stiamo cercando di estrarre alleanze tra le frasi di due documenti paralleli, che hanno lo stesso linguaggio, hanno lo stesso contenuto ma sono a un livello di complessità diverso.</sample>
    <sample id="23">E ora che abbiamo il dataset DeepL, che ha frasi manualmente allegate, possiamo usare queste frasi come standard di riferimento per valutare alcune delle metodi proposti di alleanza.</sample>
    <sample id="24">E abbiamo fatto alcune modifiche ai metodi proposti e pubblicato tutti questi aggiustamenti e i codici per eseguire i nostri esperimenti nel paper.</sample>
    <sample id="25">All'inizio, si è concluso che il miglior metodo di allineamento automatico da usare per i testi per la semplificazione dei testi tedeschi è il metodo di mass align.</sample>
    <sample id="26">E puoi anche trovare il codice per eseguire questo metodo su documenti personali nel paper.</sample>
    <sample id="27">The second use case that we showed in our paper is the case of automatic text simplification.</sample>
    <sample id="28">Semplificare i testi dalla testo complesso utilizzando modelli linguistici fin-tunati</sample>
    <sample id="29">Abbiamo finetunato due modelli. Abbiamo finetunato il modello di long import per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">E anche finetuned the normal base long the normal base import to produce sentence level simplifications.</sample>
    <sample id="31">You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="32">Conclusi che questa, questa basic fine-tuning potrebbe produrre o potrebbe ottenere punteggii migliori dei punteggi di base line.</sample>
    <sample id="33">E proponevamo quei risultati come un benchmark, un benchmark base per il problema di semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie mille per la vostra attenzione e speriamo di incontrarci tutti durante il convegno. Grazie</sample>
    <sample id="35">Kayo Yin</sample>
    <sample id="36">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il T5 XL.</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano anche in 2023.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="39">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla presenza di campioni di validazione puliti.</sample>
    <sample id="40">I progressi che possono essere fatti per migliorare il punteggio includono l'eliminazione di errori di tokenization, la corretta identificazione di entità e la riduzione di entità non rilevanti.</sample>
    <sample id="41">Cinque.</sample>
    <sample id="42">Ciao, mi chiamo Adam Przepiorkowski e questa è una presentazione sulla struttura di dipendenze della coordinazione.</sample>
    <sample id="43">Come potete vedere, ci sono diverse strutture di dipendenze assunti da diverse teorie e approcci di corpus. Ad esempio, in Universal Dependencies, la struttura della coordinazione con i coordinate Lisa, Bart e Maggie è:</sample>
    <sample id="44">è tale che il primo congiunto è la testa della struttura di coordinamento, in questo caso Lisa.</sample>
    <sample id="45">Approcci simili sono assunti in Igore Miltchuk's meaning text theory, dove di nuovo la struttura intera della coordinazione è guidata dal primo congiunto. Quindi questi due approcci sono simmetrici, giusto? Essi singolano uno dei congiunti.</sample>
    <sample id="46">Ora, ci sono anche approcci simmetrici a strutture di coordinazione, come il prague approach, l'approccio headed congiunzione e le strutture di dipendenze treesbanks, dove strutture di coordinazione sono testate da una congiunzione.</sample>
    <sample id="47">Quindi, otteniamo dipendenze da un verso all'intero congiunto.</sample>
    <sample id="48">E infine c'è anche un approccio multi-intestino che è usato, ad esempio, in il grammatica di De Catts.</sample>
    <sample id="49">Siamo qui a dire che tutti i congiunti sono i capi dei costruttori di dipendenze. Quindi, otteniamo dipendenze dal governatore, qui "amas", verso tutti i congiunti separatamente. Questi sono Bart e Maggie.</sample>
    <sample id="50">Ora, il mio articolo è destinato a produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste.</sample>
    <sample id="51">Ok, l'argomento è basato sul principio di minimizzazione della lunghezza della dipendenza che è spiegato su base di questi esempi.</sample>
    <sample id="52">In inglese, come potete vedere, gli oggetti diretti preferiscono di stare vicino al verbo, mentre gli avverbi possono essere più lontani. Quindi, "March read it yesterday" è corretto perché l'oggetto direttore "it" è vicino al verbo.</sample>
    <sample id="53">Marge ha letto ieri "it" è molto peggio, perché tra il verbo e l'oggetto direttore c'è un complemento indiretto ieri.</sample>
    <sample id="54">Tuttavia, questo effetto può essere attenuato quando il oggetto diretto è molto pesante e lungo, poiché allora può essere spostato alla posizione dopo l'aggetto.</sample>
    <sample id="55">Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. March ha letto questo libro absolutely fascinating su di queste cose ieri. Okay, allora invece di "it" abbiamo questa lunga NP.</sample>
    <sample id="56">Ma è anche okay dire: Marte ha letto ieri questo libro assolutamente fascinating su api.</sample>
    <sample id="57">La ragione qui è che questa è possibile anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero stare vicino al verbo.</sample>
    <sample id="58">Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più brevi sono preferite.</sample>
    <sample id="59">Quindi, queste due strutture solamente mostri il lunghezza delle dipendenze cruciali, cioè le loro non costanti tra queste strutture.</sample>
    <sample id="60">E qui abbiamo una dipendenza da "red" all'argomento di lunghezza 7, misurato in parole, e da "red" a "book" di lunghezza 4. Quindi, in totale, è 11.</sample>
    <sample id="61">Quando si spostano o si scambiano questi due componenti, la somma di queste due dipendenze diventa sei. Giusto? Invece di undici, sei, molto più breve. Quindi, è un po' come dire che questa suona piuttosto bene. Giusto? Viola uno dei principi, ma soddisfa un altro.</sample>
    <sample id="62">Ok, quindi quello che facemmo è estratto statistici sulla coordinazione dalla versione migliorata del banco dei testi di Penn e vedere il paper per vedere se non dovremmo usare le dipendenze universitarie.</sample>
    <sample id="63">E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono ad essere più corti. Così salt e pepper, not pepper and salt misurati in sillabe.</sample>
    <sample id="64">E anche l'osservazione che è stata fatta in passato che questa tendenza cresce con la differenza di lunghezza</sample>
    <sample id="65">Quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più breve preferisce essere il primo, giusto? Quindi la proporcione è più grande del congiunto più corto.</sample>
    <sample id="66">Ma quello nuovo in questo articolo è che notiamo che questa tendenza si verifica solo quando il governante a sinistra è assente.</sample>
    <sample id="67">Quindi, il governatore è sulla sinistra in questo esempio. Ho visto Bart e Lisa. Quindi, il governatore è sulla sinistra.</sample>
    <sample id="68">È assente nel secondo esempio, Homer venne e si schiacciò. Qui ci sono due verbi coordinati e non c'è un governo esterno. Quindi in questi casi, il primo verbo preferisce essere più corto che più lungo. Quanto maggior è la differenza tra i due verbi,</sample>
    <sample id="69">Tuttavia, quando il governante è sulla destra come qui, l'effetto scompare.</sample>
    <sample id="70">Così, dimostriamo che, uh, uh by measuring length in characters, the first column in syllables, the middle column and in words, the right column. So I'll concentrate on the right one.</sample>
    <sample id="71">Quello che vediamo qui è che quando il governante è sulla sinistra,</sample>
    <sample id="72">La tendenza per il primo concorso di essere più corto cresce gradualmente con la differenza assoluta in parole e lo stesso è osservato quando ci sono no governanti, come nella coordinazione di frasi ma quando il governante è sulla destra questa tendenza scompare.</sample>
    <sample id="73">E mostriamo nel paper come questo fornisce un argomento contro strutture di coordinazione asimmetriche come queste due e forse le strutture simmetriche come queste</sample>
    <sample id="74">Vedi il paper per la piena argomentazione e parli con noi alla sessione poster. Grazie</sample>
    <sample id="75">C'è un articolo con tre autori.</sample>
    <sample id="76">I domini della Bibbia e dei testi di lingua learner risulteranno più semplificati.</sample>
    <sample id="77">Soltanto e peppa.</sample>
    <sample id="78">Sì, i modelli possono essere utilizzati per la ricerca.</sample>
    <sample id="79">DEplain-apa contains news texts.</sample>
    <sample id="80">Un modello architettura migliore, un modello più grande e più esempi di allenamento finiscono.</sample>
    <sample id="81">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in caratteri.</sample>
    <sample id="82">Gli esperimenti sono stati progettati per misurare la lunghezze in caratteri, syllabi e parole.</sample>
    <sample id="83">Un classificatore base addestrato su dati non bilanciati non è molto più efficace di un'ipotesi casuale.</sample>
    <sample id="84">There are four authors involved in the article.</sample>
    <sample id="85">Bob e Alice</sample>
    <sample id="86">I modelli di traduzione automatica sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, MIT, e Meta AI.</sample>
    <sample id="122">Il framework quantifica esattamente la posizionalità utilizzando una correlazione di Spearman tra le annotazioni demografiche e le predizioni/modelli dei set di dati.</sample>
    <sample id="155">Lo studio precedente ha dimostrato che i soggetti umani hanno anche surfato su stereotipi razionali quando hanno ricevuto gli stessi prompt di persona.</sample>
    <sample id="156">I fonti di dati utilizzati in questo studio sono le statistiche estratte dal versione migliorata del Corpus dei testi in inglese.</sample>
    <sample id="157">Cinque.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di due enunciati di discutizione provenienti da diverse persone come in accordo o in disaccordo rispettivamente e la classificazione binaria delle classi di espansione e comparazione di PDB.</sample>
    <sample id="159">Due.</sample>
    <sample id="160">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="161">Il framework differisce dai precedenti in quanto confronta le predizioni e le etichette dei modelli e dei set di dati con le annotazioni degli utenti, invece di analizzare solo l'acordo tra gli annotatori o le distribuzioni degli annotatori.</sample>
    <sample id="162">La configurazione GPT-4 si sovrappone maggiormente al lessico degli stereotipi.</sample>
    <sample id="163">Google Translate e Deep L</sample>
    <sample id="164">Ciao, mi chiamo Shangbin, sono uno studente dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro sulla transizione dai dati di pre-training ai modelli di linguaggio fino alle tare downstream, monitorando le tracce dei bias politici che portano a modelli NLP non equi.</sample>
    <sample id="165">I modelli di linguistica sono addestrati su grandi quantità di dati web crawl.</sample>
    <sample id="166">I media di notizie politiche sono bene coperti nei loro dati di addestramento linguistico. Secondo un sondaggio del corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post ecc. sono bene coperti in linguistic model training data.</sample>
    <sample id="167">Questo ha creato un benedetto mese per le applicazioni dei modelli di linguaggio.</sample>
    <sample id="168">Sul lato di un lato, erano in grado di imparare da diverse prospettive che celebrano la democrazia e la pluralità di idee. Sull'altro lato, queste diverse opinioni politiche sono inherentemente socialmente biased e possono portare a problemi di giustizia potenziali in applicazioni di compiti di inferenza basso livello.</sample>
    <sample id="169">Per raggiungere questo scopo, proponiamo di indagare il percorso della propagazione dei bias politici dal dataset di addestramento in modelli di linguaggio fino alle attività downstream specificamente chiedendo le seguenti domande.</sample>
    <sample id="170">Prima di tutto, come valutiamo la linearità politica dei modelli di linguaggio e in che modo i dati di pretraining potrebbero influenzare tale linearità politica?</sample>
    <sample id="171">Secondariamente, come i modelli di linguaggio con diverse intenzioni politiche realmente performano su compiti downstream e se tale performance potrebbe portare a problemi di giustizia in applicazioni NLP?</sample>
    <sample id="172">In particolare, innanzitutto propone di stimare modelli di linguaggio con diverse formule di prompt utilizzando domande politiche come il test del compasso politico. Questo ci assicura di poter effettuare un valutazione automatica ben radicata nella letteratura scientifica politica.</sample>
    <sample id="173">Quindi i risultati preliminari dimostrano che i modelli di linguaggio hanno ideologie politiche diverse. Occupano tutti i quattro quadranti del compasso politico.</sample>
    <sample id="174">Possiamo anche vedere che GPT-4 è il modello di lingua più liberale tra tutti e che le serie GPT sono in generale più liberali rispetto alle serie BERT e le sue variazioni.</sample>
    <sample id="175">In secondo luogo, ci proponiamo di indagare fino a che punto i pregiustizi politici dei modelli di linguaggio vengono davvero acquisiti dai dati di addestramento.</sample>
    <sample id="176">Quindi, potremmo condurre un esperimento controllato eseguendo un ulteriore pretraining dei punti di riferimento dei modelli linguistici su sei gruppi di partito diversi, separati in mediazione e social media, ulteriormente divisi in loro orientamento politico.</sample>
    <sample id="177">Inoltre, pretrainare modelli di linguaggio su dataset di spostamento ideologico può far vedere che le coordinate ideologiche del modello si spostano corrispondentemente.</sample>
    <sample id="178">Ad esempio, per RoBERTa, che è stato ulteriormente finetuned e ulteriormente addestrato su un corpus di Reddit sinistra, possiamo vedere un spostamento sostanziale a sinistra in termini della sua</sample>
    <sample id="179">In termini di bias politici.</sample>
    <sample id="180">E anche cercare di indagare se i modelli di linguaggio possono prendere iniziata polarizzazione che è prevalentemente nella nostra società moderna.</sample>
    <sample id="181">Quindi, suddividiamo il pretraining di GPT-3 in due periodi: prima e dopo il 45° Presidente degli Stati Uniti. Separatamente, pretrainiamo modelli di linguaggio su questi due periodi temporali differenti.</sample>
    <sample id="182">Possiamo vedere che i modelli di lingua in genere hanno un orientamento politico che è più lontano dal centro dopo il 2017. Questo indica che i modelli di lingua possono anche prendere iniziata polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, per ultime ma non meno importanti, valutiamo i modelli di linguaggio con diverse inclinazioni politiche in quanto alla deteczione di discorso razzista e alla deteczione di notizie fake. Questi due applicativi NLP spesso coinvolgono modelli di linguaggio e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">Quindi vediamo che se esaminiamo la performance per category, che è di dire se separiamo le prestazioni in due performance,</sample>
    <sample id="185">In differenze demografiche o politica di notizie media, possiamo vedere un模式，比方说对于 hate speech detection，左倾语言模型更好。</sample>
    <sample id="186">Table 4 shows the performance of hate speech targeting different identity groups and misinformation from different sources. The color-coded system indicates that yellow represents the best performance, while dark blue denotes the worst.</sample>
    <sample id="187">Tuttavia, i nostri modelli hanno difficoltà a rilevare i discorsi razziali che bersano su gruppi più potenti nella nostra società.</sample>
    <sample id="188">E viceversa, i modelli di linguaggio di destra sono migliori nel rilevare la discriminazione sulla base della razza e del genere, tuttavia peggiori nel rilevare la discriminazione basata su altre comunità minoritarie come i neri, i transessuali e gli altri.</sample>
    <sample id="189">Sono anche presenti tendenze simili per la deteczione di notizie fake, in cui si notano che i modelli di linguaggio di sinistra sono migliori nel rilevare la menzogna da parte dei propri opposti politici e viceversa.</sample>
    <sample id="190">Inoltre, mostriamo molti esempi qualitativi per vedere che i modelli di linguaggio con diverse inclinazioni politiche</sample>
    <sample id="191">Dà diverse predizioni per esempi di discorso razzista e informazione sbagliata in base alla propria classe sociologica. Ci sono un sacco di altri esempi in l'appendice per sottolineare meglio.</sample>
    <sample id="192">Questo indica che ci sono problemi di giustizia che sono molto preoccupanti in merito alla parzialità politica dei modelli di linguaggio.</sample>
    <sample id="193">Ad esempio, se un modello di linguaggio addestrato a rilevare l'insulto o la disinformazione e simili fosse stato finetuned e deposto su una piattaforma di social media popolare,</sample>
    <sample id="194">Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e che i discorsi di odio che mirano a gruppi minoritari potrebbero continuare liberamente senza alcun controllo.</sample>
    <sample id="195">Quindi, questa ha suonato l'alert per noi di riconoscere e affrontare i problemi di giustizia derivanti dai bias politici dei modelli linguistici.</sample>
    <sample id="196">Un po' di discussione. Vorremmo anche sottolineare che espongiamo il dilemma unico riguardante i bias politici dei modelli di linguaggio. Si tratta di un dilemma tra Scylla e Caribdà.</sample>
    <sample id="197">Se non sanizziamo le opinioni politiche in un dataset di addestramento per modelli linguistici, i bias potrebbero propagarsi dal dataset di addestramento ai modelli linguistici e quindi alle attività downstream, creando infine problemi di giustizia.</sample>
    <sample id="198">Se proviamo a sanitizzare in qualche modo, rischiamo di incorrere in censure o esclusione e è incredibilmente difficile determinare cosa è realmente neutra e dovrebbe essere retain in linguaggio monitorato. Dato che è come un problema di scarica elettrica.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have to say. Thank you for your time.</sample>
    <sample id="200">To determine the number of authors involved in the article, we need to carefully examine the provided image. The image shows a slide from a presentation titled "Prompting PaLM for Translation: Assessing Strategies and Performance." Below the title, there are six photographs of individuals, each accompanied by their name. These names are:

1. David Villegas Torres
2. Markus Freitak
3. Colin Cherry
4. Jiaming Luo
5. Vineer Ratnaker
6. George Foster

By counting these names, we can conclude that there are six authors involved in the article. Therefore, the answer is 6.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite con contesti di lunghezza fino a 900 token.</sample>
    <sample id="202">I domini inclusi nel loro set di dati sono: music, fiction, and non-fiction.</sample>
    <sample id="203">La posizionalità è semplicemente la prospettiva che le persone hanno come risultato dei propri demografici, identità e esperienze di vita.</sample>
    <sample id="204">Dawei Zhu</sample>
    <sample id="205">No, EDAtt non adatta un modello ST offline esistente.</sample>
    <sample id="206">Cinque.</sample>
    <sample id="207">No, il modello non funziona sulla suite di test.</sample>
    <sample id="208">The three variants of KITMUS are Background Pretrain, Background Both, and Background Inference.</sample>
    <sample id="209">I fornitori di Google Research sono gli autori dell'articolo.</sample>
    <sample id="210">La domanda di ricerca finale è se dovremmo utilizzare solo i campioni puliti per la validazione o se ci sono altre migliori modalità per utilizzarli.</sample>
    <sample id="211">La sensibilità misura la capacità del modello di produrre gli stessi output per la stessa attività, anche se ci sono variazioni nella formula dell'instruzione.</sample>
    <sample id="212">Jingwei Yi</sample>
    <sample id="213">La maggiore sensibilità indica una performance del modello peggiorata.</sample>
    <sample id="214">I modelli vengono addestrati su un vasto contesto linguistico.</sample>
    <sample id="215">In genere, circa 20 campioni di convalida puliti sono necessari per il raggiungimento di buone prestazioni in WSL.</sample>
    <sample id="216">Gli autori dell'articolo sono affiliati all'Institute for Computational and Mathematical Engineering.</sample>
    <sample id="217">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli di linguaggio hanno tendenze politiche e occupano tutti i quattro quadranti del compasso politico.</sample>
    <sample id="218">The speaker is named Manjata.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici ha due aspetti. Di un lato, i modelli possono imparare da diverse prospettive che celebrano la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche sono inerentemente socialmente biased e possono portare a problemi di giustizia nella programmazione di downstream.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="221">No, Coscript non è pubblicamente disponibile.</sample>
    <sample id="222">La filigrana è esattamente inclusa quando il numero di trigger nel testo è maggiore di m.</sample>
    <sample id="223">La Penn State University e Amazon.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è come fare un pasticcino di cioccolato.</sample>
    <sample id="226">Gli autori si accertano della segretezza del loro metodo visualizzando l'embedding dei testi su un diagramma di scatter.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire uno nuovo utilizzando una strategia di impianto continuo.</sample>
    <sample id="228">GPT-4 is least aligned with Catholic Europe.</sample>
    <sample id="229">La relatrice illustra come il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione in un esempio sulla destra.</sample>
    <sample id="230">La quantità di attività influisce sulla performance del modello in modo che un aumento nella quantità di attività porta a una migliore performance e contemporaneamente a un livello inferiore di sensibilità.</sample>
    <sample id="231">The authors compare their method with three other reference approaches: LSTM seq2seq, Zhang and Lapata, and GP recursion.</sample>
    <sample id="232">I due coautori sono gli advisor del primo autore.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="234">Ciao a tutti, mi chiamo Jenny, sono un primo anno di PhD student a Carnegie Mellon University e oggi dovrò presentare il mio lavoro, NL Positionality: Characterizing Design Biases of Datasets and Models.</sample>
    <sample id="235">Questo lavoro è stato fatto in collaborazione con alcuni fornitori dell'Università di Washington e dell'Institut d'AI Allen, cioè Sebastien Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="236">Allora, diamo un’idea di come funziona.</sample>
    <sample id="237">Potresti ricorrere a un API popolare come l'API di prospettiva per la deteczione della tossicità e questa funziona davvero bene se sei Karl Jones dove l'API di prospettiva è in grado di rilevare correttamente le istanze tossiche.</sample>
    <sample id="238">Ma non è proprio così per Aditya Sharma, dove l'API di prospettiva non è né sensibile a termini offensivi che sono più comuni in contesti indiani.</sample>
    <sample id="239">Ecco un esempio di bias di progettazione, in cui si notano differenze sistematiche di prestazioni della tecnologia tra popolazioni.</sample>
    <sample id="240">Biasi di progettazione come quello che abbiamo appena visto prima potrebbero succedere a causa della posizionalità dei ricercatori e dei sviluppatori di modelli NLP. La posizionalità è semplicemente le percezioni che le persone hanno come risultato dei propri demografici, identità e esperienze di vita.</sample>
    <sample id="241">Questo è un concetto ampiamente utilizzato in studi critici, in particolare in ambienti accademici femministi e queer.</sample>
    <sample id="242">E come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Ecco una domanda che le persone potrebbero fare: hanno i set di dati e i modelli una posizionalità?</sample>
    <sample id="244">E non stiamo cercando di dire che i modelli in sé stessi e i set di dati da soli hanno identità demografiche e esperienze della vita, ma essi aggregano giudizi e opinioni di persone reali e possono pertanto rappresentare certe posizionalità su altre.</sample>
    <sample id="245">Il mio lavoro ha suggerito alcune prove anecdettiche di avere una posizionalità, come i gap culturali in modelli e dataset, nonché le definizioni teoriche di posizionalità dei modelli.</sample>
    <sample id="246">Tuttavia, queste opere non analizzano realmente la confrontazione degli utenti con i dataset e i modelli stessi.</sample>
    <sample id="247">Studiare la posizionalità dei modelli e dei dataset è sempre più importante, poiché i test NLP diventano sempre più soggettivi e orientati alla società.</sample>
    <sample id="248">E è difficile caratterizzare come queste posizionalità siano inclinate, poiché non tutti i decisioni sono documentati e molti modelli sono nascosti dietro API.</sample>
    <sample id="249">Quindi per studiare la posizionalità dei dataset e dei modelli, in reality confrontiamo le annotazioni con reali utenti con i dataset esistenti e i modelli.</sample>
    <sample id="250">We do this through a framework called NL Positionality.</sample>
    <sample id="251">Il nostro framework funziona in due passi principali.</sample>
    <sample id="252">Il primo passo è rereperire i set di dati con diverse annotatori.</sample>
    <sample id="253">E optiamo per fare questo, guardando le demografiche dei set di dati originali e degli annotatori, perché di solito solo un paio di annotatori annotano ogni istanza e perché le demografiche sono raramente raccolte e condivise.</sample>
    <sample id="254">Ecco, noi optiamo di riannotare i dati per ottenere molte annotazioni per un esempio e per ottenere un insieme ricco di dati demografici.</sample>
    <sample id="255">Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando il coefficiente di correlazione di Pearson.</sample>
    <sample id="256">In tal modo, il nostro framework differisce dalla letteratura sulla disuguaglianza tra gli annotatori in quanto confronta le predizioni e le etichette dei modelli e dei set di dati con le previsioni degli utenti finali, invece di analizzare solo l'acordo tra gli annotatori o la distribuzione degli annotatori.</sample>
    <sample id="257">Il framework è in gran parte abilitato attraverso Lab in the Wild, una piattaforma di crowdsourcing online per i collaboratori di HCHI.</sample>
    <sample id="258">In Lab in the Wild è un piattaforma di esperimentazione online dove possiamo reclutare volontari diversi rispetto ai piattaforme come MTurk che hanno prevalentemente partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere anche dei dati di alta qualità.</sample>
    <sample id="259">Organizziamo due task sul lab in the wild, uno dei quali è la social acceptability. Ecco come funziona: i partecipanti leggono una situazione dalla Social Chemistry Dataset e quindi votano su quanto socialmente accettabile sia quella situazione.</sample>
    <sample id="260">Successivamente, per rimanere in contatto con la community, possono confrontare le proprie risposte con quelle di un AI e di altri.</sample>
    <sample id="261">We then compared these annotations with social chemistry, Delphi and GPT-4.</sample>
    <sample id="262">Poi hanno replicato un setup molto simile per la task di rilevamento di tossicità e di discriminazione di discorso razzista, in cui i partecipanti leggevano un esempio dalla dinahate e scrivevano se pensavano che fosse un esempio di discorso razzista.</sample>
    <sample id="263">Poi confrontammo queste annotazioni con Dynahate, Perspective API, Rewire API, HateROBERTa e GPT-4. Nostro studio in fine ha raccolto oltre 16.000 annotazioni da più di un mille annotatori provenienti da 87 paesi.</sample>
    <sample id="264">So now we're better equipped to answer who do NLP datasets and models align with the most. We find that there is positionality in NLP.</sample>
    <sample id="265">Ad esempio, troppo diamo conto che i set di dati e i modelli sono più legati ai paesi che parlano inglese. Quindi per l'analisi di accettabilità societaria GPT-4, scopriamo che è più legato ai paesi confessioni e che parlano inglese. Troppo diamo conto che Dina Hattie è anche più legato ai paesi che parlano inglese.</sample>
    <sample id="266">Troviamo anche maggior alleanza con le persone che hanno un'istruzione universitaria. Quindi, per GPT-4 nel compito di accettabilità sociale, troppo che sia più allineato con le persone che hanno un'istruzione universitaria o un'istruzione di studio superiore.</sample>
    <sample id="267">E troppo lo stesso per Dynahate, dove è più allineato con le persone che hanno un'istruzione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati sono allineati a specifici popolazioni, alcune sono inevitabilmente lasciate indietro.</sample>
    <sample id="269">Un esempio di questo è che i set di dati e i modelli sono meno allineati con le persone non binarie rispetto ai controparti di uomo e donna. Troiamo questo nel compito di accettabilità sociale GPT-4, così come nell'analisi del compito di diuturnaite.</sample>
    <sample id="270">So, given that there is position in NLP, what can we do about it?</sample>
    <sample id="271">Abbiamo poche raccomandazioni per questo. La prima è tenere un registro di tutti i design rilevanti durante il processo di ricerca e l'altra è fare ricerche NLP con la lente di presentativismo.</sample>
    <sample id="272">La nostra terza raccomandazione è di costruire insieme dataset e modelli specializzati per specifiche comunità. Un buon esempio di questo è l'iniziativa Masakhane. Vogliamo sottolineare che l'AI inclusiva non è solo far funzionare tutte le tecnologie per tutti.</sample>
    <sample id="273">E così, questo conclude la nostra presentazione. Ma se vorrete imparare di più, vi prego di controllare il nostro dashboard per i risultati analisi più aggiornati e il nostro articolo. Grazie mille!</sample>
    <sample id="274">La relatrice menziona due problemi associati a SimulST: i modelli di SimulST attuali hanno architettura specifica e i modelli devono essere addestrati utilizzando moduli aggiuntivi per otimizzarli.</sample>
    <sample id="275">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere l'uso di tecniche di pulizia dei dati, come la rimozione di informazioni sensibili o la normalizzazione dei testi. Tuttavia, è importante notare che la pulizia dei dati non è una soluzione universale e che è difficile determinare cosa sia davvero neutro e dovrebbe essere rimosso dai set di dati.</sample>
    <sample id="276">Ciao, mi chiamo Siyu Yuan e sono qui per presentare il nostro lavoro Distilling Script Knowledge from Large Language Models for Constrained Language Planning.</sample>
    <sample id="277">In vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo in forma di script garantiti.</sample>
    <sample id="278">Il precedente lavoro ha sfruttato modelli di linguaggio per pianificare azioni astratte di attività stereotipate, come fare un pasticcino, e ha dimostrato che i modelli di linguaggio possono efficacemente decomporre gli obiettivi in passaggi.</sample>
    <sample id="279">Tuttavia, i precedenti studi hanno maggiormente focalizzato sul pianificare per obiettivi astratti di azioni teoriche. Pianificare per obiettivi con specifiche condizioni specifiche, ad esempio fare un pasticcino al cioccolato, rimane sottostudiato.</sample>
    <sample id="280">In questo articolo definiamo il problema della pianificazione del linguaggio con vincoli.</sample>
    <sample id="281">che impongono diverse vincoli alla pianificazione del linguaggio. Un obiettivo astratto può essere ereditato da diversi obiettivi specifici con vincoli multi-faccettati. Un buon pianificatore dovrebbe redigere script che siano ragionevoli e fedeli ai vincoli.</sample>
    <sample id="282">In questo articolo, valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua grande.</sample>
    <sample id="283">Se non esiste un dataset specifico per i nostri obiettivi,</sample>
    <sample id="284">Prima di tutto dobbiamo acquisire queste informazioni. Come si vede nella tabella, estendiamo gli obiettivi astratti con vincoli multipli per umanizzare la raccolta dei dati utilizzando il modello GPT-3.</sample>
    <sample id="285">Samo 100 specifici obiettivi e valuto i script generati da modelli di grandi lingue.</sample>
    <sample id="286">Questa tabella riporta l'accuratezza generale dei risultati. Abbiamo scoperto che tutti i modelli di linguaggio ottengono risultati insoddisfacenti per la pianificazione per obiettivi specifici.</sample>
    <sample id="287">Poi eseguiamo un'analisi dettagliata per indagare come i modelli di apprendimento automatico sbagliano.</sample>
    <sample id="288">I risultati del grafico dimostrano che la completezza semantica dei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.</sample>
    <sample id="289">Siamo passati a categorie di vincoli più specifiche, definite in WikiHow. Il diagramma nella figura dimostra che il piano delle performance di InstructGPT varia considerabilmente per i vari obiettivi di diverse categori.</sample>
    <sample id="290">Studi precedenti hanno dimostrato che l'output dei modelli di lingua è fortemente influenzato da una varianza elevata, portando a prestazioni peggiorate. Di conseguenza, adottiamo l'idea di un filtra Z sovrageren per migliorare la qualità dell'generazione.</sample>
    <sample id="291">Vi mostriamo prima i tipi di vincoli con esempi per il modello GPT-3 e otteniamo obiettivi specifici sulla base dei relativi obiettivi astratti.</sample>
    <sample id="292">Poi istruisci GPT-3 per generare script candidati per obiettivi specifici.</sample>
    <sample id="293">Successivamente, un modello di filtraggio è sviluppato per selezionare i script più pertinenti.</sample>
    <sample id="294">Convertiamo i script e gli obiettivi in embeddings di InstructGPT e calcoliamo la similarità coseno e i punteggi di similarità per misurare la somiglianza semantica.</sample>
    <sample id="295">In addition, we will observe the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set.</sample>
    <sample id="296">Con il nostro metodo, InstructGPT può generare schemi di alta qualità. Il nostro metodo grandemente migliora la pianificazione sia in termine di completezza semantica che di fedeltà alle vincoli.</sample>
    <sample id="297">Poiché i modelli di linguaggio sono costosi da deploy, è essenziale abilitare la capacità di pianificazione linguistica per modelli più piccoli e specializzati. Creare un dataset è un passo essenziale per raggiungere questo obiettivo.</sample>
    <sample id="298">Tuttavia, gli studi precedenti non utilizzano pianificazione per obiettivi specifici e l'annotazione manuale dei set di dati è costosa.</sample>
    <sample id="299">Inoltre, seguiamo l'idea di distillazione simbolica del conoscimento per distillare i set di dati di pianificazione linguistica vincolati da modelli di lingua grandi.</sample>
    <sample id="300">Appliamo il nostro metodo per la costruzione di un insieme di dati per la pianificazione linguistica con vincoli, denominato come CoScript.</sample>
    <sample id="301">In totale, abbiamo generato 55.000 scenari specifici con script per garantire la qualità della validazione e dei set di test. Abbiamo chiesto a fornitori di servizi cloud di trovare e revisionare i campioni incorretti.</sample>
    <sample id="302">Questa figura illustra la distribuzione dei vincoli nella distribuzione di Coreset. Abbiamo scoperto che Coreset ha un'alta pluralità nel generato specifico obiettivo. Con Coreset, possiamo trainare modelli più piccoli ma specializzati per pianificazione linguistica vincolata.</sample>
    <sample id="303">Abbiamo scoperto che i modelli più piccoli addestrati su Coreset possono generare script di qualità superiore a quelli dei maggiori modelli linguistici, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset appropriati.</sample>
    <sample id="304">In sintesi, stabiliamo il problema di pianificazione linguistica con vincoli. Evaluiamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua grande e sviluppiamo un metodo per generare troppe informazioni e filtrarle per i modelli di lingua grande.</sample>
    <sample id="305">Utilizziamo modelli di lingua grandi per generare un insieme di dati di alta qualità, CoScript, per pianificazione linguistica vincolata. Speriamo che il dataset CoScript possa essere un risorsa utilizzabile per avanzare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="306">Grazie per il tuo tempo. Per favore, fornisci maggiori dettagli sul co-scrivere in nostro articolo.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi d'arte migliori.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana sono che dovrebbe essere applicabile a Embadding as Services, non dovrebbe indebolire l'utility dei forniti embeddings, dovrebbe essere abbastanza nascosto per l'attaccante per poter rimuovere la filigrana facilmente e infine dovrebbe essere trasferibile alle servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="309">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">Per la riannotazione, vengono campionate 200 istanze da un set di dati.</sample>
    <sample id="311">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la differenza di coseno e la differenza L2.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare due gruppi di modelli, tra cui encoder e decoder.</sample>
    <sample id="344">Gli autori decidono quali sono le parole a frequenza moderata contando la frequenza delle parole in un corpus di testo generale che possono raccogliere.</sample>
    <sample id="345">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il mio articolo: "Do CoNLL-2003 named entity taggers still work well in 2023?". Vediamo di iniziare.</sample>
    <sample id="346">Il nostro articolo ha indagato il problema della generalizzazione utilizzando la comprensione di entità denominate o compito di riconoscimento di entità (NER).</sample>
    <sample id="347">Abbiamo osservato che i modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi venti anni. E queste naturalmente sollevano diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?</sample>
    <sample id="348">E quando sviluppiamo nuovi tag, cosa è necessario per una buona generalizzazione?</sample>
    <sample id="349">Allo stesso tempo, se osserviamo una scarsa generalizzabilità, cosa causa la svalutazione di queste modelli?</sample>
    <sample id="350">Per investigare questi problemi, sviluppiamo il dataset ConLL++. Questo è un dataset che abbiamo raccolto da Reuters News nel 2020 e lo abbiamo annotato con le stesse direttive di annotazione di ConLL-2003.</sample>
    <sample id="351">Poi abbiamo allenato più di venti modelli su CoNLL-2003. Abbiamo valutato questi modelli sia sul set di test CoNLL-2003 che sul set di test CoNLL++</sample>
    <sample id="352">Infine, se non per ultime, calcoliamo il percentuale di cambiamento in F1 per valutare la generalizzabilità di ogni modello.</sample>
    <sample id="353">So, what is needed for good generalization? Through our experiments we found that there are three main ingredients that are needed.</sample>
    <sample id="354">La prima è la struttura del modello. Nelle nostre esperienze, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio a nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è il modello di dimensione. Abbiamo scoperto che, di solito, i modelli più grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">Infine, non ultima cosa, sappiamo tutti che il numero di esempi di finetuning direttamente influenza le prestazioni di un compito downstream. Qui abbiamo anche scoperto che più esempi di finetuning realmente conducono a una migliore generalizzabilità.</sample>
    <sample id="357">To our next question, what causes the performance drop of some models?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'adattamento all'overfitting, che è l'overfitting causato da riutilizzare lo stesso set di test ripetutamente e questo è generalmente manifestato come la diminuzione dei rendimenti su un nuovo set di test.</sample>
    <sample id="359">La seconda ipotesi è la drift temporale, che è la degradazione del prestabilito causata da l'intervallo temporale crescente tra i dati di addestramento e i dati di test.</sample>
    <sample id="360">Per l'overfitting adattivo, come possiamo vedere dalla grafica sulla destra, la linea di migliore ajuste rossa ha una retta con un coefficiente di inclinazione maggiore di 1.</sample>
    <sample id="361">Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL 2003 si traduce in più di una unità di miglioramento su CoNLL ++, il che significa che non ci sono ritorni decrescenti.</sample>
    <sample id="362">E questo ci dimostra che l'overfitting adattivo in questo caso non è osservato.</sample>
    <sample id="363">Quindi, cosa c'è con il drift temporale?</sample>
    <sample id="364">Per il drift temporale, abbiamo svolto un esperimento per retrain o continuare a pretrainare alcuni modelli con più recenti dati e abbiamo scoperto che le prestazioni peggiorano con un intervallo temporale più grande.</sample>
    <sample id="365">E questo conferma la nostra ipotesi che il principale motivo della diminuzione delle prestazioni è lo scarto temporale.</sample>
    <sample id="366">La nostra conclusione è che per una buona generalizzabilità, saremmo in grado di ottenere un modello architettura migliore, dimensioni del modello più grandi, insieme a esempi di finetuning maggiori. Questi obiettivi vanno di pari passo, non possiamo avere un ingrediente ma anche gli altri.</sample>
    <sample id="367">Allo stesso tempo, abbiamo anche scoperto che il drop di prestazione qui è causato da drift temporale e, sorprendentemente, non è causato da overfitting adattivo, anche se CoNLL 2003 è stato utilizzato per più di venti anni.</sample>
    <sample id="368">Quindi tornando alla domanda che avevamo sollevato in intesto del nostro articolo, i tagger di ConLL-2003 continuano a funzionare nel 2023? Ebbene, la risposta è in pratica un sì risolutivo.</sample>
    <sample id="369">Speriamo che il nostro articolo causi un maggior numero di ricerche su come migliorare la generalizzabilità dei modelli.</sample>
    <sample id="370">E infine, assicurati di controllare il nostro articolo e il dataset. E se hai qualche domanda, non esitare a contattarmi. Ti ringhio mille volte.</sample>
    <sample id="397">La dimensione del segmento parlato utilizza l'approccio di 100.</sample>
    <sample id="398">Per risolvere il pronome "he" in Servin e Kea, è necessaria la conoscenza specifica dell'entità di Servin come giudice.</sample>
    <sample id="399">La qualità dell'esempio è più importante del fattore somiglianza con la frase sorgente.</sample>
    <sample id="400">The article focuses on GPT-4 and other GPT models.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">I nomi dei canzoni, le loro posizioni, e le prime.</sample>
    <sample id="403">I ricercatori che hanno scritto l'articolo sono affiliati all'University of Toronto e a Brain Technologies Inc.</sample>
    <sample id="404">There are five authors involved in the article.</sample>
    <sample id="405">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è stato considerato come un approccio standard.</sample>
    <sample id="406">Un esempio fornito è "un warrior (unmarked) vs. a woman warrior (marked)".</sample>
    <sample id="407">Le architetture dei modelli che non generalizzano in modo adeguato non sono indicate.</sample>
    <sample id="408">The names of the test sets are FT_w, COSINE, L2R, BERT, MLC, and AdapterC.</sample>
    <sample id="409">There are five authors involved in the article.</sample>
    <sample id="410">L'autore opera con più modalità.</sample>
    <sample id="439">Integrazione e uso di conoscenza pre-traine e inferenza</sample>
    <sample id="440">The names of the presenters are Ying and Jiaheng.</sample>
    <sample id="441">Sì, Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti in quanto supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati. Questo è dovuto al fatto che solitamente si basano su conoscenze specifiche e curazione umana.</sample>
    <sample id="443">Ciao, e sto per parlare del mio lavoro su risolver espressioni diretti riferendosi per la selezione di entità in cui introduciamo il Corpus AltEntities.</sample>
    <sample id="444">Il mio nome è Javad Hosseini e questa è un lavoro con Filip Radlinski, Silvia Paretti e Annie Louise.</sample>
    <sample id="445">Il nostro obiettivo è di comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai detto easy on me o I got a feeling?" Qui, l'utente vuole scegliere tra due canzoni.</sample>
    <sample id="446">La cosa più ovvia è usare un riferimento diretto, ad esempio, dicendo il nome della canzone easy on me o la sua posizione, la prima.</sample>
    <sample id="447">Ma a volte un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe succedere quando l'utente non ricorda il nome della canzone.</sample>
    <sample id="448">Tutte le pronunciazioni sono troppo simili tra loro e difficili da distinguere.</sample>
    <sample id="449">O quando l'utente vuole specificare un preferimento. Ecco alcuni esempi di riferimenti diretti: ad esempio, "il nuovo", o "la canzone che non è energica".</sample>
    <sample id="450">Questa è un problema importante nei sistemi di conversazione e anche per la valutazione dei modelli di lingua grande in comprensione degli enti.</sample>
    <sample id="451">Non conosciamo un insieme di dati pubblico a larga scala per la nostra attività, quindi raccolgiamo uno utilizzando la crowdsourcing. Il nostro insieme di dati copre tre domini diversi: musica, libri e ricette.</sample>
    <sample id="452">Il nostro metodo di raccolta di dataset enfatizza l'informalità utilizzando un compito di completamento di disegni.</sample>
    <sample id="453">Il cartoon ha tre bubble di dialogo. In primo piano Bob dice: "ricorda quella canzone che stavamo ascoltando ieri?" e con quello Bob stabilisce il contesto del dialogo.</sample>
    <sample id="454">In this, in the second speech bubble Alice says do you mean easy on me or I got a feeling?</sample>
    <sample id="455">È la domanda alternativa. E in quella terza bolla di dialogo, Bob usa un riferimento indiretto per scegliere uno di questi enti, ad esempio, la nuova</sample>
    <sample id="456">For the first and second speech bubbles, we provide them automatically. The third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="457">Il secondo, che è la domanda alternativa, è generato come segue.</sample>
    <sample id="458">Usiamo sempre un modello semplice. Vuoi dire A o B? Dove A e B sono campioni da Wikipedia.</sample>
    <sample id="459">Ecco i metodi di campionamento che abbiamo usato. Mentre ci muoviamo verso l'alto nella lista, gli enti diventano più simili tra loro e generalmente è più difficile fare la disambigazione.</sample>
    <sample id="460">La prima è uniformatran.</sample>
    <sample id="461">La seconda è quando gli enti hanno titoli simili. Ad esempio, due libri con il nome The Return.</sample>
    <sample id="462">La terza è quando hanno descrizioni simili su Wikipedia e infine quando hanno info-box o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista.</sample>
    <sample id="463">Quando mostriamo queste domande alternative ai annotatori, conoscono il nome di queste entità ma non necessariamente ne hanno conoscenze.</sample>
    <sample id="464">Quello che facciamo è di mostriamo un po' di conoscenza di sfondo riguardo i due enti. Per le canzoni, semplicemente mostriamo un link di ricerca Google per ciascuna.</sample>
    <sample id="465">E quindi chiediamo ai annotatori di ascoltare almeno un pezzo di ciascun brano e leggere su di essi. Ecco ad esempio i risultati di ricerca Google per la canzone Easy on Me.</sample>
    <sample id="466">Per il dominio ricette e libri, mostriamo alcune informazioni di sfondo da Wikipedia. Per le ricette, mostriamo anche le immagini, nuovamente da Wikipedia, in modo che gli annotatori possano vedere come si vedono.</sample>
    <sample id="467">Poi chiediamo ai annotatori di scegliere uno di questi enti, ad esempio, qui il primo, e di descriverlo utilizzando 3-5 espressioni indirette.</sample>
    <sample id="468">Ad esempio, il one with the piano music. Qui ci sono alcuni esempi dalla nostra base di dati. Ad esempio, il one without words, non il one with the 12-year-old, 12-year-old boy or the fictional one, or comes from Azerbaijan and so on.</sample>
    <sample id="469">Il corpus AltEntities ha 6.000 domande alternative attraverso tre domini e ha 42.000 espressioni dirette indiretto. I risultati con il modello T5 XL sono riassunti di seguito:</sample>
    <sample id="470">Se il modello di linguaggio ha accesso esattamente alla stessa conoscenza di sfondo degli elaboratori, allora l'accuratezza è davvero alta. Sono circa il 92-95%. Ma questa non è realistica.</sample>
    <sample id="471">Se il modello di linguaggio ha accesso a qualche conoscenza di sfondo parzialmente sovrapposibile, allora la precisione è tra i 82 e i 87 percenti, che è più realistico. Ad esempio, quando il modello di linguaggio recupera la conoscenza di sfondo.</sample>
    <sample id="472">Se il modello di linguaggio ha accesso solo ai nomi degli enti, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al dataset. Grazie.</sample>
    <sample id="473">L'approccio viene confrontato con le politiche SimulST esistenti.</sample>
    <sample id="474">LIA, LS2N, CHU de Nantes, Zenith</sample>
    <sample id="475">Jenny T. Liang</sample>
    <sample id="476">There are three authors involved in the article.</sample>
    <sample id="477">Ciao, sono Sara Papi dalla Università di Trento e dalla Fondazione Bruno Kessler. E vorrò brevemente introdurre il paper "Attention as a Guide for Simultaneous Speech Translation", un lavoro in collaborazione con Matteo Negri e Marco Turchi.</sample>
    <sample id="478">Simultaneous speech translation Simultaneous speech translation, or simultaneous translation (ST), is the process of translating spoken language into a text in another language in real time, enabling cross-language communication.</sample>
    <sample id="479">E cosa sono i problemi dei modelli SimulST attuali? Architetture specifiche vengono addestrate introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">Long and complicated training procedures, for example training involving different optimization objectives.</sample>
    <sample id="481">E addestrando e mantenendo diversi modelli per raggiungere differenti regimi di latenza, ad esempio addestrando un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza e così via.</sample>
    <sample id="482">Sono, cosa è la nostra soluzione?</sample>
    <sample id="483">Prima di tutto, utilizziamo modelli offline esistenti senza retrain o adottare un'architettura specifica per SimulST. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici.</sample>
    <sample id="484">E utilizza la conoscenza acquisita dal modello tramite il meccanismo di attenzione tra input audio e output testuale, che è chiamato meccanismo di attenzione reciproca. Ecco un esempio a destra.</sample>
    <sample id="485">La nostra soluzione è di proporgli adatt o encoder-decoder attention e è una strategia per la quale decidiamo se emettere o non emettere una traduzione parziale basata su dove l'attenzione si punta.</sample>
    <sample id="486">Un segno è emesso se l'attenzione non è concentrata, cioè se la somma è inferiore a un certo livello α verso le ultime λ istanti di speech frames, il che significa che le informazioni ricevute non sono abbastanza stabilizzate.</sample>
    <sample id="487">Ad esempio, se riceviamo un frammento di discorso che dice "I'm going to talk about" e il modello predice la traduzione in tedesco,</sample>
    <sample id="488">E cercheremo di analizzare i pesi dell'attenzione.</sample>
    <sample id="489">Vedremo che i primi due punti si riferiscono ai primi frammenti di parola ricevuti, mentre l'ultimo punto si riferisce ai frammenti di parola più recenti, ovvero i frammenti di lambda.</sample>
    <sample id="490">This means that the first two words will be emitted.</sample>
    <sample id="491">Mentre, poiché la somma della attenzione transversale è sopra un certo livello α, non emetteremo l'ultima parola e aspetteremo per un altro segmento di parola.</sample>
    <sample id="492">Se continuiamo e riceviamo un altro segmento di discorso e il modello predice altre tre parole, analizzeremo queste pesate di attenzione reciproca.</sample>
    <sample id="493">We will see that no word points to the last lambent speech frames.</sample>
    <sample id="494">Questo significa che queste tre parole verranno emesse.</sample>
    <sample id="495">If you look at the main results of EDAtt,</sample>
    <sample id="496">Verranno disegnati i risultati di traduzione simultanea sul grafico in cui su un lato ci sono i colori blu che misurano la qualità della traduzione e, sull'altro lato, ci sono i tempi di elaborazione.</sample>
    <sample id="497">That is the latency measure and we also consider the computational aware average latency that accounts for the model's computational times to produce the output.</sample>
    <sample id="498">Vogliamo che i nostri curvi siano il più possibile alti su questo grafico.</sample>
    <sample id="499">Ma anche che siano spostati verso sinistra.</sample>
    <sample id="500">E confrontiamo con strategie popolari che vengono anche applicate a modelli offline, come la strategia wait-k e l'approccio locale. Abbiamo anche confrontato con un architettura di punta specificamente adattata per la traduzione parallela.</sample>
    <sample id="501">Questi sono tutti i risultati della strategia di traduzione parallela Simultaneous su tedesco.</sample>
    <sample id="502">E vediamo che AD outperforms all the strategies applied to offline models, since their curves are shifted over the left.</sample>
    <sample id="503">E vediamo anche che se consideriamo il tempo di elaborazione reale o il tempo computazionale effettivo, att è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire di più i risultati, leggi il nostro articolo e anche rilasciamo open source, il codice e i modelli e i simuli in uscita per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="505">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="506">Ciao a tutti. Il mio nome è Ying e il mio collega Jieyang e io presenteremo il nostro ricerchiamo su Multi-instruct: improving multi-modal zero-shot learning via instruction tuning.</sample>
    <sample id="507">Con i progressi nella creazione di modelli di linguaggio a larga scala, molte ricerche hanno iniziato a esplorare nuovi paradigmi di apprendimento che utilizzano modelli pre-addestrati per diverse attività downstream in modo efficiente sia in termini di parametri che di dati.</sample>
    <sample id="508">Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli di linguaggio a grandi scalori di svolgere compiti inesistenti in modo zero-scarica, seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei precedenti studi sulla tuning dell'istruzione si sono concentrati su migliorare le prestazioni in testo senza immagine per i compiti di lingua solo, mentre la visione computer e i compiti multi-modal hanno stato trascurato.</sample>
    <sample id="510">Quindi, in questo lavoro, ci si vuole indagare se l'addestramento con istruzioni dei modelli pre-addestrati multi-modal può realmente migliorare la generalizzabilità su compiti multi-modal.</sample>
    <sample id="511">Inoltre, all'epoca della nostra ricerca, abbiamo scoperto una considerabile disparità nella disponibilità di dataset di istruzioni tra NLP e multimodale.</sample>
    <sample id="512">Esistono più di mille e seicento task di istruzioni solo linguaggio. Tuttavia, non esiste un dataset di istruzioni multimodale largamente pubblico. Questo ci ha spinto a creare un dataset di istruzioni multimodale per l'addestramento.</sample>
    <sample id="513">Ecco qui presentiamo MultiInstruct, il primo dataset di benchmark per l'addestramento di istruzioni multimodale che consiste di 62 diverse attività multimodale diverse, coprendo 10 category diverse.</sample>
    <sample id="514">Questi compiti sono derivati da 21 dataset esistenti di open source e ogni compito è equipaggiato con cinque istruzioni scritte da esperti.</sample>
    <sample id="515">Per investigare l'addestramento di istruzioni multi-modal, utilizziamo il nostro dataset proposto. Utilizziamo OFA, un modello pre-addestrato multi-modal unificato, come nostra base di modello. L'OFA utilizza un vocabolario unificato per i token del linguaggio, immagine e le coordinate di un bounding box.</sample>
    <sample id="516">Ecco alcuni esempi di istanze dalla nostra nostra MultiInst data set.</sample>
    <sample id="517">To unify the processing of various input and output data types.</sample>
    <sample id="518">Seguendo il modello di OFA, formuliamo tutti i compiti in un formato unificato di sequenza a sequenza, in cui i testi dell'input, le immagini, le istruzioni e i box di confine sono rappresentati nello spazio dei token同一.</sample>
    <sample id="519">Ora parlerò di multi-modal instruction tuning.</sample>
    <sample id="520">Per il dataset di addestramento, utilizziamo 53 task da 9 gruppi per l'addestramento e selezioniamo 10.000 istanze per task. Per la valutazione riserviamo l'intero gruppo di ragione comune per la valutazione e scegliamo ulteriormente 5 task dal gruppo Q&amp;A e gruppoMiscellanei.</sample>
    <sample id="521">Utilizziamo tutte le istanze nella sottosezione di test per ciascun compito. Inoltre, scegliamo casualmente 20 compiti dalla sottosezione di test di Natural Instructions come compiti non visti per NLP.</sample>
    <sample id="522">Così utilizziamo un modello pre-addestrato OFA-Large come modello di base. Durante il training, mescoliamo le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni.</sample>
    <sample id="523">Durante i test per ciascun compito, eseguiamo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque insiemi di istruzioni in ciascun esperimento.</sample>
    <sample id="524">Rapportiamo la media e il massimo prestabilito e la deviazione standard del prestabilito attraverso tutti i cinque esperimenti.</sample>
    <sample id="525">Se la task è una task di classificazione multi-modal, rapportiamo l'accuratezza. Se è una task di generazione multi-modal, rapportiamo Rouge-L. Per le task NLP, rapportiamo Rouge-L inoltre.</sample>
    <sample id="526">Inoltre, abbiamo introdotto un nuovo metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre i medesimi output per la stessa attività indipendentemente dalla variazione nella formulazione dell'instruzioni.</sample>
    <sample id="527">Ecco i nostri principali risultati. Come possiamo vedere, l'addestramento con istruzioni può migliorare significativamente le prestazioni di OPA su diverse tareitte multimodale.</sample>
    <sample id="528">Anche l'apprendimento a trasferimento da dataset di istruzioni naturali può migliorare l'inserimento.</sample>
    <sample id="529">E qui possiamo vedere che, al crescere del numero di compiti, il modello raggiunge un better performance e contemporaneamente una sensibilità più bassa.</sample>
    <sample id="530">Inoltre, abbiamo anche fatto esperimenti utilizzando un insieme di istruzioni invece di cinque istruzioni. Come possiamo vedere, l'uso di molte istruzioni può migliorare il modello in modo generale e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Questo illustra l'effetto di diverse strategie di finetuning sul sensibilità del modello. Come possiamo vedere, grazie all'apprendimento a trasferimento da dataset di istruzioni naturali, il modello può raggiungere una sensibilità molto migliore rispetto all'originale OFA modello.</sample>
    <sample id="532">Anche se possiamo vedere che l'apprendimento tramite il set di istruzioni naturale può aiutare OPA a ottenere un molto migliore prestazione sul set di istruzioni naturale.</sample>
    <sample id="533">In generale, proponiamo il primo insieme di addestramento multi-modal di scala larga con istruzioni. Questo ha migliorato notevolmente la capacità di zero-shot del OFA e esplora diversi metodi di imparare in transito e dimostra i loro benefici. Abbiamo anche progettato un nuovo metrico chiamato sensibilità.</sample>
    <sample id="534">One more thing! We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon. This is a QR code for our data and model. Thank you.</sample>
    <sample id="535">The authors of the article are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">Javad Hosseini</sample>
    <sample id="562">Ciao a tutti, mi chiamo Koustuv Sinha e sono felice di benvenuti alla nostra presentazione del nostro articolo AACL 2023: i giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto.</sample>
    <sample id="563">Sono un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Mishra, Keren Fuentes, Roger Levy e Adina Williams.</sample>
    <sample id="564">In questo lavoro, rivisitiamo il paradigma del parere minimo.</sample>
    <sample id="565">La paradossa del minimo coppia evaluates i modelli di linguaggio su giudizi di accettabilità, che possono includere grammaticalità come BLMP, SyntaxGym o accettabilità in termini di stereotipi come CrowS.</sample>
    <sample id="566">E in questo paradigma di coppia minima, il modo tipico per valutare i modelli di linguaggio è quello di mostri un'accettabile frase o una grammatica frase e quindi mostri una frase non accettabile o una frase non grammatica.</sample>
    <sample id="567">E allora, l'obiettivo del modello è di assegnare maggior probabilità alla sentenza accettabile.</sample>
    <sample id="568">Il flusso di lavoro corrente per la valutazione (MPP) non ci permette di valutare l'accettazione dei modelli verso le frasi più lunghe.</sample>
    <sample id="569">Questi giorni, i modelli di lingua stanno uscendo con finestre di contesto sempre più lunghe. Quindi è cruciale valutare l'acceptabilità dei modelli attraverso tutto il contesto.</sample>
    <sample id="570">E quello è esattamente quello che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline del PBP, chiedendo al modello di valutare l'acceptabilità su sequenze più lunghe e più lunghe.</sample>
    <sample id="571">Quindi, è questa l'approccio. Quindi, quello che facciamo è di simulare queste sequenze più lunghe. Rivisitiamo i dataset stessi e quindi ricreiamo frasi scegliendo frasi accettabili o non accettabili da quei dataset.</sample>
    <sample id="572">Per esempio, qui abbiamo scelto un paio tipico di grammatica dal dataset BLIP da un caso in Islanda.</sample>
    <sample id="573">E quello che facciamo è di ricostruire sequenze più lunghe, che sono accettabili e hanno lo stesso matching della struttura grammaticale. Estraiamo frasi grammaticali da un modello di lingua.</sample>
    <sample id="574">E quindi lo aggiungiamo come prefisso sia alla domanda accettabile che alla domanda non accettabile.</sample>
    <sample id="575">Così possiamo fare la stessa cosa scegliendo frasi non accettabili dalla stessa coppia e quello potrebbe anche essere utilizzato per testare l'acceptabilità del modello.</sample>
    <sample id="576">E possiamo anche fare lo stesso scegliendo frasi da un subset diverso o un dataset diverso. Quello che chiamiamo scenario mismatch.</sample>
    <sample id="577">E qui, le frasi continuano a venire da set di dati rilevanti, ma non da quello che stai valutando. E possiamo fare lo stesso per i casi di non accettabilità.</sample>
    <sample id="578">Infine, possiamo scegliere frasi da un dominio completamente non correlato, ad esempio Wikipedia.</sample>
    <sample id="579">Quindi, questo ci dirà se i giudizi di accettabilità dei modelli sono davvero influenzati da alcun contesto.</sample>
    <sample id="580">Se il contesto sta provenendo da un sottogruppo diverso del dataset o se è completamente irreilevante per la frase che stiamo analizzando.</sample>
    <sample id="581">Quindi come fa il modello? Be', all'inizio diamo un'occhio ai testo Wikipedia che non hanno niente a che vedere con la domanda attuale e vediamo che i giudizi del MPP sono maggiormente robusti per lunghezze di contesto arbitrarie.</sample>
    <sample id="582">Aumentiamo la lunghezza del contesto fino a 1204 per massimizzare i modelli OPT e GPT-2 e vediamo qui, nella linea di punto rosso, che i giudizi del MPP sono relativamente stabili.</sample>
    <sample id="583">Ora, cosa succede quando scegliamo frasi dal medesimo dataset?</sample>
    <sample id="584">E qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili, dalla stessa fonte di testo.</sample>
    <sample id="585">E lì vediamo che i giudizi MPP aumentano o diminuiscono notevolmente quando si aggiungono prefissi accettabili o non accettabili.</sample>
    <sample id="586">Ma quando scegliamo la struttura, cioè quando scegliamo le frasi da fenomeni dello stesso tipo in Blimp Person Text Jim,</sample>
    <sample id="587">Vediamo un'enorme aumentazione o una enorme diminuzione del giudizio MPP per il modello, a seconda di quanto il prefisso scelto sia accettabile o non accettabile.</sample>
    <sample id="588">Ora, questa è questa è molto grande. Questo effetto aumenta attraverso la lunghezza del contesto e probabilmente affetterà i modelli linguistici più nuovi che hanno un contesto di larghezza.</sample>
    <sample id="589">Perché i prefissi adeguati hanno un impatto così grande sulla valutazione dei modelli di linguaggio?</sample>
    <sample id="590">Così, hanno creato una serie di analisi in cui hanno cercato di alterare la frase di input cercando di preservare la struttura rilevante ma aggiungendo un po' di rumore alla frase di input. E dopo aver fatto diversi di questi alterazioni,</sample>
    <sample id="591">Troiamo che nessuno di questi rumori sta realmente rendendo il modello come cambia i corso in termini di come ci dice cosa hanno pensato i papà.</sample>
    <sample id="592">In sostanza, scopriamo che i modelli sono sensibili alle frasi perturbate in modo simile.</sample>
    <sample id="593">Quando perturbiamo i testi in un dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo i testi in un dominio non accettabile, vediamo una diminuzione dei giudizi di accettabilità in modo simile.</sample>
    <sample id="594">La conclusione principale del nostro lavoro è che i modelli di linguaggio sono sensibili a caratteristiche sintattiche e semantiche latenti che vengono condivise tra le frasi.</sample>
    <sample id="595">E l'evaluation MPP che facciamo attualmente con input brevi e singoli frasi non può affatto catturare completamente il sapere astratto dei modelli linguistici attraverso tutto il contesto.</sample>
    <sample id="596">Per vedere dettagli sull'esperimento, leggete il nostro articolo. Grazie per l'attenzione.</sample>
    <sample id="597">Un ordered multiset of tokens that will appear in the output.</sample>
    <sample id="598">In Coscript, rappresentano 55.000 script.</sample>
    <sample id="626">Il miglior metodo di allineamento per DEplain è il metodo mass align.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato allevia il problema di annotazione.</sample>
    <sample id="628">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici.</sample>
    <sample id="629">The CoNLL++ dataset was created by collecting data from Reuters News in 2020 and annotating it using the same CoNLL 2003 annotation guidelines.</sample>
    <sample id="630">Ciao a tutti, mi chiamo Yusen Zhang e sono alla Università del Pennsylvania. Oggi, presenterò il mio lavoro: XSemPLR: Parsing Semantico Multilingue per più Linguaggi Naturali e rappresentazioni del significato.</sample>
    <sample id="631">La semantica parsing è un compito che costruisce rappresentazioni semantiche di query utente come SQL e Lambda calculus.</sample>
    <sample id="632">Cross-lingual Semantic Parsing è la attività di tradurre query in più lingue naturali in rappresentazioni multiple.</sample>
    <sample id="633">Come illustrato nella figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali per SQL, Lambda o FunQL e altro.</sample>
    <sample id="634">I modelli di parsing semantici multi-lingua esistenti sono separatamente proposti e valutati su un insieme di task limitati e applicazioni. Ad esempio:</sample>
    <sample id="635">Ci sono limiti di copertura su certi linguaggi naturali. L'ebraico è mancante e</sample>
    <sample id="636">Mancato copertura su determinate rappresentazioni di significato.</sample>
    <sample id="637">La lambda calcolo è mancante.</sample>
    <sample id="638">O solo valutati su certi modelli neurali. Ad esempio, ci sono solo modelli singoli da valutare.</sample>
    <sample id="639">In conclusione, proponevamo Exemplar per fornire un insieme di dati uniforme Exemplar per la parsing semantica multi-lingua e rappresentazioni di significato in diversi linguaggi naturali.</sample>
    <sample id="640">Contiene 9 dataset in vari domini, 5 task di parsing semantici, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche.</sample>
    <sample id="641">E per valutare meglio il benchmark, consideriamo i sei ambienti di addestramento e valutazione.</sample>
    <sample id="642">Il primo è TranslateTest. Utilizzeremo l'API Google Translate per tradurre il testo di origine in un linguaggio di destinazione, quindi useremo un modello monolingue per addestrare e valutare.</sample>
    <sample id="643">Ad esempio, addestriamo il modello inglese su domande inglese e durante l'infusione traduciamo la domanda tedesca utilizzando un API in inglese e quindi utilizziamo il modello addestrato per prevedere il SQL.</sample>
    <sample id="644">E testiamo anche modelli monolingui.</sample>
    <sample id="645">In questo contesto, il linguaggio di origine è lo stesso del linguaggio di destinazione. Ad esempio, Germano a Germano o Inglese a Inglese.</sample>
    <sample id="646">Inoltre, testiamo anche un impianto di immissione monolingue few-shot, addestrando modelli monolingui con solo il 10% dei dati di addestramento.</sample>
    <sample id="647">E testiamo un modello multilingue, in cui addestriamo un solo modello multilingue per tutte le lingue.</sample>
    <sample id="648">Ad esempio, mettiamo insieme query in tedesco, inglese e cinese per addestrare un modello multilingue. Durante l'inferenza, possiamo usare questo modello per...</sample>
    <sample id="649">To translate German queries or Chinese queries, etc.</sample>
    <sample id="650">E consideriamo anche il transito tra lingue zero-shot e few-shot. Addestrato su un linguaggio di origine e successivamente trasferito ad un altro linguaggio.</sample>
    <sample id="651">Durante la formazione, stiamo addestrando su query in inglese o la combinazione di query in inglese e tedesco per addestrare un modello multilingua per prevedere il output SQL.</sample>
    <sample id="652">E valutiamo due gruppi di modelli: i modelli multilingui e i modelli monolingui.</sample>
    <sample id="653">Incluso Enc-PR, che sta per encoder pre-addestrati multilingue con decoder basati su puntatori, come XLM-R + PR e mBERT + PR.</sample>
    <sample id="654">E valutiamo i modelli encoder-decoder, che sono modelli multilingue pretrainati encoder-decoder come mBART e mT5.</sample>
    <sample id="655">Abbiamo scoperto che Enc-Dec (mT5) ottiene le migliori prestazioni su tutti i nove dataset.</sample>
    <sample id="656">E valutiamo su m t5 e x l m r + p t r in un contesto multilingue.</sample>
    <sample id="657">Siamo riusciti a dimostrare che encoder-decoder o encoder-PTR possono essere migliorati addestrandoli in una miscela di diverse lingue.</sample>
    <sample id="658">E abbiamo scoperto che è proprio per questo che la maggior parte delle lingue naturali principali possono ottenere un miglioramento delle prestazioni, tranne che per l'inglese, il cui prestigio diminuisce in sette dataset e guadagna in tre dataset.</sample>
    <sample id="659">Most of the major NLPs can obtain performance gain, except that English performance drops in 7 datasets. This is known as "Curse of Multilinguality".</sample>
    <sample id="660">We also compare the cross-lingual performance gap.</sample>
    <sample id="661">In questa figura, la linea azzurra rappresenta il trasferimento cross-lingua few-shot. La linea arancio rappresenta il trasferimento cross-lingua zero-shot. La linea verde rappresenta il contesto monolingue.</sample>
    <sample id="662">Abbiamo scoperto che, confrontando le linee verdi e arancione, nel contesto di un setting senza etichette, il gap di prestazione del trasferimento tra lingue è significativo. E, confrontando le linee blu e arancione, nel contesto di un setting con poche etichette, il gap di prestazione del trasferimento si riduce rapidamente.</sample>
    <sample id="663">We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieves comparable results. Pretraining on English natural language can significantly boost the performance of few-shot on target natural languages.</sample>
    <sample id="664">E si è scoperto che i modelli multilingue come Codex e Blue sono ancora inadeguati per le complessità di parsing linguistici translingua.</sample>
    <sample id="665">In sintesi, è stato costruito XSemPLR, un benchmark unificato per la analisi semantica transversale con diverse lingue naturali e rappresentazioni.</sample>
    <sample id="666">Conduciamo un'ampia analisi su tre tipi rappresentativi di modelli linguistici multilingue e i nostri risultati dimostrano molti interessanti trofei e altro. Ecco, vieni a visitare il nostro articolo e codice. Grazie per la tua attenzione.</sample>
    <sample id="667">The works related to this are parameter-based watermark, lexical watermark, backdoor-based watermark, and adversarial-based watermark.</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP.</sample>
    <sample id="695">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allegatura come parte della formazione.</sample>
    <sample id="696">L'equità di un modello NLP a valle è definita come la capacità di fornire risposte equamente e senza bias, indipendentemente dalla politica o dal gruppo di appartenenza del utente.</sample>
    <sample id="697">The name of the presenter is Yanis Laboulaye.</sample>
    <sample id="698">Koustuv Sinha</sample>
    <sample id="699">La relatrice o il relatore è Myra Cheng.</sample>
    <sample id="700">Il tropicalismo indica un stereotipo che si riferisce alla percezione di Latina come vibrante e curvaceous.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target definendoli come vibranti e positivi.</sample>
    <sample id="702">Pointwise CXMI</sample>
    <sample id="703">DrBERT è un modello pre-addestrato da zero, mentre ChuBERT è un modello pre-addestrato utilizzando un modello pre-addestrato esistente.</sample>
    <sample id="751">There are three authors involved in the article.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un approccio che aggiorna il modello utilizzando i dati più recenti raccolti.</sample>
    <sample id="753">The objective of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="754">Un utente malevolo può estrarre i parametri del modello utilizzando un EaaS.</sample>
    <sample id="755">There are three authors involved in the article.</sample>
    <sample id="756">Per creare il set di dati iniziale, sono stati impiegati 8 annotatori.</sample>
    <sample id="757">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">I saw Bart and Lisa.</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo sono quelli che utilizzano l'ABC-Eval.</sample>
    <sample id="760">Perché i modelli di lingua stanno emergendo con finestre di contesto sempre più lunghe, è cruciale valutare la loro accettabilità durante tutto il contesto.</sample>
    <sample id="761">La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset.</sample>
    <sample id="762">No, gli annotatori non hanno accesso all'entità in anticipo.</sample>
    <sample id="763">La valutazione è stata effettuata utilizzando la metrica di accuracy.</sample>
    <sample id="764">Yes, the regression in generalization affects specific types of NER.</sample>
    <sample id="765">La posizionalità nella NLP è importante perché le parole hanno un significato diverso in base alla loro posizione all'interno di una frase o di un testo. Ad esempio, il verbo "essere" può avere un significato diverso in "Sono felice" e "Sono felice di vedere te". Inoltre, la posizionalità delle parole può anche influenzare la grammatica e la struttura del testo.</sample>
    <sample id="766">Adattatori o con una messa a punto integrale.</sample>
    <sample id="767">Per il trasferimento dell'apprendimento, si ricorre al modello RoBERTa-base classifier head.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM sono il dataset di test multilingue e il dataset di test multilingue con 100 lingue.</sample>
    <sample id="769">The authors have proposed three recommendations for model owners.</sample>
    <sample id="770">Il guadagno del nuovo metodo rispetto al vecchio è di 10.78%.</sample>
    <sample id="771">Shuheng Liu</sample>
    <sample id="772">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="773">Due modelli più piccoli vengono utilizzati.</sample>
    <sample id="774">OFA</sample>
    <sample id="833">I ricercatori che hanno scritto l'articolo sono affiliati a Google Translate.</sample>
    <sample id="834">I fornitori dell'articolo sono affiliati a Stony Brook University.</sample>
    <sample id="835">L'articolo ha analizzato le traduzioni tra 10 coppie linguistiche diverse.</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">I modelli studiati sono Long Impart e Long Base Impart.</sample>
    <sample id="838">Per scopi di addestramento e test, 53 delle 62 attività diverse utilizzate in MultiInstruct vengono utilizzate.</sample>
    <sample id="839">Cinque.</sample>
    <sample id="840">The authors conducted experiments on four datasets: AG News, MIND, SST2, and Enron Spam.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">Il nome della relatrice o del relatore è Ida Vilard.</sample>
    <sample id="878">La strategia del prompting ha un grande impatto sui risultati.</sample>
    <sample id="879">Carnegie Mellon University, Language Technologies Institute, TECNICO Lisboa, BAIR Berkeley Artificial Intelligence Research, Unbabel</sample>
    <sample id="880">I 5 istruzioni scritte da esperti sono: 1) Identificare i componenti principali dell'immagine; 2) Analizzare la posizione e il rapporto tra i componenti; 3) Identificare i colori e le sfumature presenti; 4) Determinare la composizione e la struttura dell'immagine; 5) Identificare eventuali dettagli o elementi di importanza.</sample>
    <sample id="881">Gli autori propongono di utilizzare un compito di risoluzione di coreferenza per testare i modelli sull'utilizzo di informazioni provenienti da più fonti.</sample>
    <sample id="882">Ciao a tutti, mi chiamo Ibilard e useremo un sottoreview del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro di gruppo con i miei colleghi da Google Translate.</sample>
    <sample id="883">PaLM è un modello di lingua con 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testo che comprende 780 miliardi di token.</sample>
    <sample id="884">La nostra modifica di PaLM ha raggiunto lo stato dell'arte in centinaia di compiti NLP.</sample>
    <sample id="885">In questo lavoro, presentiamo il primo studio sistematico di stimolazione dei modelli di lingua grande per traduzioni automatiche.</sample>
    <sample id="886">Valutiamo la capacità di traduzione dei modelli utilizzando le migliori pratiche della comunità di traduzioni. Questo implica l'uso dei set di test più recenti per evitare qualsiasi sovrapposizione dei dati di test con i dati di addestramento del modello di lingua.</sample>
    <sample id="887">E confrontiamo due sistemi a punte di settore. I migliori sistemi sono i sistemi di traduzione automatica.</sample>
    <sample id="888">Usiamo metriche innovative e sofisticate per l'LMT e inoltre mostriamo anche i risultati di valutazioni umane basate su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.</sample>
    <sample id="889">La prompting ha un grande impatto sulle prestazioni dei modelli di traduzione. Come possiamo vedere in un esperimento semplice, utilizzando un approccio di prompting unico e fornendo due diversi prompt per ogni frase,</sample>
    <sample id="890">La maggioranza delle frasi, 516 su 1000, la differenza riservata è di più di un punteggio BLEURT.</sample>
    <sample id="891">E queste possono andare in casi estremi fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di promempi.</sample>
    <sample id="892">In our experiments, we test for a few-shot prompting strategy where we just mark each its sentence that we provide to the system with the language it's in.</sample>
    <sample id="893">In questo esempio qui, dove eseguiamo traduzioni da tedesco in inglese, le frasi tedesche, le frasi sorgenti, sono contrassegnate con il segnaposto tedesco e le traduzioni inglese con il segnaposto inglese.</sample>
    <sample id="894">Abbiamo notato che la forma reale del prompt non ha un grande impatto in caso di 5-shot prompting.</sample>
    <sample id="895">È cruciale per zero e uno shot prompting e quando ci siamo, come nel nostro caso, a five shot prompting, c'è quasi niente di diverso da la forma reale del prompt.</sample>
    <sample id="896">E sono esempi che hanno la maggior parte del peso.</sample>
    <sample id="897">Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza alla frase di origine.</sample>
    <sample id="898">È importante selezionare gli esempi da traduzioni di alta qualità in particolare. In particolare, confrontiamo la selezione dei prompt dal dataset di valutazione di UMT o dai dati di dev.</sample>
    <sample id="899">I testi dei dati è molto più accurato e con alta qualità rispetto ai testi di training, rendendo i risultati migliori. Così, un'performance migliore quando si utilizza i testi dei dati.</sample>
    <sample id="900">Tuttavia, i sistemi specializzati in traduzioni hanno un vantaggio sostanziale rispetto alle traduzioni umane. Ma una delle traduzioni umane si avvicina molto a un sistema commerciale. In un caso specifico, ci si è sovrilluminato con Google Translate.</sample>
    <sample id="901">I insights che otteniamo dalla nostra analisi, che abbiamo eseguito utilizzando il framework MQM, sono che la fluidezza di PaLM è paragonabile a sistemi di punta ma la differenza principale deriva dalla precisione.</sample>
    <sample id="902">In particolare, gli errori più comuni sono gli errori di omissione.</sample>
    <sample id="903">Quindi sembra che PaLM sceglie di usare per produrre una traduzione più comprensibile, a volte rimuovendo parti della frase di origine che non sono rilevanti nella traduzione.</sample>
    <sample id="904">Tuttavia, la categoria "stile/ingannevante" per PaLM è più bassa rispetto a quella dei sistemi di punteggio dello stato dell'arte, che è un segnale aggiuntivo.</sample>
    <sample id="905">ParLM fornisce output molto fluente, ma con alcuni problemi di accuratezza.</sample>
    <sample id="906">And that's it for this really short overview. For more details, please come my to the full presentation of the paper. Thank you very much.</sample>
    <sample id="907">Ciao, mi chiamo Dawei, sono uno studente PhD a Saarland University in Germania. In questo video vorrei presentare il nostro recente lavoro, "Weaker Than You Think", un'analisi critica dei metodi di apprendimento supervisionato debole.</sample>
    <sample id="908">Questa è una collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="909">I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data.</sample>
    <sample id="910">In weak supervision, non si etichettano i dati manualmente. Si etichettano i dati utilizzando fonti di etichettatura deboli, come regole semplici, basi di conoscenza o sondaggi di qualità bassa. Come illustrato nella figura sulla destra.</sample>
    <sample id="911">In confronto alle annotazioni umane, le annotazioni deboli sono molto più economiche, tuttavia, sono anche rumorose, il che significa che un determinato numero delle annotazioni sono incorrette.</sample>
    <sample id="912">Se addestriamo direttamente i reti neurali su dati etichettati debolmente, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano.</sample>
    <sample id="913">In weakly supervised learning, training algorithms are proposed to robustly train neural networks on such label noise so that the trained models still generalize well.</sample>
    <sample id="914">In recent works in WSL, WSL stands for weakly supervised learning. A common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="915">Tecnicamente, questa affermazione non è falsa, ma c'è un problema.</sample>
    <sample id="916">which is that people do assume that there is an additional clean validation set available for model selection.</sample>
    <sample id="917">We focus on this problem setting as it implies that additional manual annotations are required in weakly supervised learning. But, like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="918">The aforementioned doubt is us to ask three research questions. First, is clean validation data necessary for WSL or can we maybe use a noisy validation set instead?</sample>
    <sample id="919">Secondo, se i dati puliti sono necessari o se i dati puliti sono obbligatori per far funzionare il WSL, allora quanti campioni puliti dobbiamo avere? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono modi migliori per utilizzarli?</sample>
    <sample id="920">We addressed these research questions in our work, and our findings are as follows.</sample>
    <sample id="921">Prima di tutto, scopriamo che recenti metodi di WSL effettivamente richiedono campioni con etichette pulite per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci è un grosso crollo del prestigio, come si vede in questo grafico. Se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare al di fuori dei relativi etichettati.</sample>
    <sample id="923">Significa che l'addestramento è inutile.</sample>
    <sample id="924">Questo indica che gli approcci WSL richiedono realmente etichette pulite per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere sottovalutato.</sample>
    <sample id="925">Il nostro secondo trofeo è che aumentare il numero di campioni di validazione puliti aiuterà i metodi WSL a raggiungere un更好的表现，如图左所示。</sample>
    <sample id="926">Tipicamente, basta 20 campioni per classe per ottenere un buon prestigio.</sample>
    <sample id="927">Ma non è la fine della storia, perché se decidiamo di accedere a campioni puliti, allora addestrare modelli direttamente su di essi raggiungerà anche un’accuratezza migliore.</sample>
    <sample id="928">Il grafico a destra mostra la differenza di prestazione tra i metodi di finetuning diretti che vengono applicati direttamente su dati puliti e i metodi WSL che utilizzano i dati puliti solo per la validazione.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo 10 campioni per classe, il finetuning diretto inizia a superare gli approcci WSL.</sample>
    <sample id="930">Infatti, l’improve mento del performance dichiarato in precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare il fin tuning su campioni puliti di validazione.</sample>
    <sample id="931">Come possiamo vedere dai grafici, il modello VALLINA denominato FTW inizialmente sottoperfora rispetto ai metodi più complessi di WSL come COSINE.</sample>
    <sample id="932">Tuttavia, se continuiamo a finanziare i campioni puliti, allora FTTW si comporta allo stesso modo degli altri metodi.</sample>
    <sample id="933">In pratica, non c'è motivo di scegliere metodi più complessi di WSL che richiedono più tempo di elaborazione e spazio su disco.</sample>
    <sample id="934">In sintesi, dimostrato che le recenti tecniche di apprendimento supervisionato hanno bisogno di campioni puliti e manualmente annotati per funzionare correttamente. Le loro prestazioni e praticità sono pesantemente sovrastimata.</sample>
    <sample id="935">I nostri raccomandazioni conclusive per il lavoro futuro sono come segue.</sample>
    <sample id="936">Prima, rapporta i criteri di selezione del modello. Ad esempio, rapporta se la selezione del modello è basata su validationi pulite.</sample>
    <sample id="937">Due recenti approcci alla WSL dovrebbero essere confrontati con baselines di imparazione a basso costo che utilizzano campioni puliti. Terzo, l'ottimizzazione continuo è un baselining semplice ma forte che dovrebbe essere considerato in future work in WSL.</sample>
    <sample id="938">Finalmente, abbiamo aperto sorgente il nostro codice. Potete trovarlo nella Q&amp;A del slide. Vi prego di controllare pure. Grazie per la conferenza e per l'abbiamo goduto.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo includono l'uso di giudici umani per selezionare tra due conversazioni o per valutare le conversazioni utilizzando una scala di Likert.</sample>
    <sample id="940">There are five authors involved in the article.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, è necessario conoscere che i giudici decidono cause in tribunali.</sample>
    <sample id="942">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="943">No, the annotators for NLPositionality are not balanced with respect to any demographic group.</sample>
    <sample id="944">Le frasi nel dominio accettabile sono state perturbate in modo simile.</sample>
    <sample id="945">Evaluare dimensionalmente significa analizzare diverse aspetti della qualità del dialogo per capire le sue caratteristiche e le sue debolezze.</sample>
    <sample id="946">I ricercatori dell'articolo sono affiliati all'Università di Scienze e Tecnologia di Cina, Microsoft e Sony AI.</sample>
    <sample id="947">La forma del prompting è importante solo per i promptings a zero e a un passo.</sample>
    <sample id="978">I modelli di dialogo che gli autori hanno valutato sono quelli che hanno violazioni di senso comune in circa il 20% delle loro risposte, producono informazione non rilevante in circa il 15% delle risposte e si contraddicon o con i propri partner in circa il 10% dei tempi.</sample>
    <sample id="979">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle limitazioni.</sample>
    <sample id="981">Ci sono otto autori coinvolti nell'articolo.</sample>
    <sample id="982">La relatrice o il relatore è Vasudha Varadarajan.</sample>
    <sample id="983">I ricercatori sono affiliati all'Institute of Computer Science e all'University of Warsaw.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono gli omission errors.</sample>
    <sample id="1022">Ecco, sono James Finch e Sarah Finch. Oggi vi dirò tutto su ABC-Eval, un nuovo approccio dimensionale per valutare l'Intelligenza Artificiale di conversazione.</sample>
    <sample id="1023">Questo lavoro è stato fatto dal laboratorio NLP di Emory, guidato dal professor Jinho Choi all'Emory University e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Ma immaginiamo che tu abbia appena sviluppato un modello di dialogo e tu voglia vedere quanto bene si confronta con lo stato dell'arte attuale.</sample>
    <sample id="1025">La pratica comune è di utilizzare un giudizio umano, ad esempio chiedendo ai giudici umani di scegliere quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala di Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni holistiche della qualità del dialogo in generale, ma la qualità del dialogo ha molti aspetti. Quindi potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello a un livello più fine-granulare.</sample>
    <sample id="1027">Un approccio è quello di chiedere semplicemente ai giudici umani di valutare diversi aspetti della qualità del dialogo, come la rilevanza delle risposte dei modelli, utilizzando metodi esistenti basati su confronti o scala Likert.</sample>
    <sample id="1028">Tuttavia, crediamo che ci sia un策略更加精确和可靠的维度对话评估。</sample>
    <sample id="1029">Il nostro approccio tenta di ridurre la soggettività della valutazione umana fornendo un contesto chiaro per le risposte dei modelli. Questo è fatto attraverso l'annotazione esplicita di ogni risposta del modello, determinando se esprime certi comportamenti, come fornire informazioni non rilevanti o contraddire se stessa.</sample>
    <sample id="1030">Chiamiamo questa approccio annotare comportamenti in chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chatta che sono stati suggeriti di influenzare la qualità del chatta recentemente nella letteratura.</sample>
    <sample id="1031">ABC-Eval è capace di misurare le tassi con cui i modelli di chat commettono errori stilistici diversi.</sample>
    <sample id="1032">Ad esempio, ABC-Eval misura il numero di giri in cui un modello di chat ignora il proprio partner o dice qualcosa di non rilevante.</sample>
    <sample id="1033">Contradice se stesso o il proprio partner, elenca fatti sbagliati o viola le conoscenze comuni e quando il modello riesce o fallisce a dimostrare compassione.</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chatbot a livello di ultima generazione e li abbiamo valutati su 100 conversazioni umano-bots per modello utilizzando ABC-Eval.</sample>
    <sample id="1035">Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni con una scala di Likert al livello della singola risposta, valutazioni con una scala di Likert al livello della conversazione e confronti a livello di coppia.</sample>
    <sample id="1036">Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comamente misurati del dialogo, poiché è la pratica standard per valutare i modelli di chatta lungo diverse dimensioni.</sample>
    <sample id="1037">Da analisi dei risultati di queste valutazioni, abbiamo scoperto che i etichettatori ABC hanno comportamenti più affidabili rispetto alle etichette raccolte da metodi esistenti, come misurato dall'acordo tra gli annotatori su 100 conversazioni doppio-labeled.</sample>
    <sample id="1038">Inoltre, i etichett ABC-Eval sono più predittivi della qualità generale della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice.</sample>
    <sample id="1039">Ad esempio, si può vedere come misurare la proporcione di turni con contrasti tra se stessi e partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza Likert medio spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, ci sono verificato se ogni metrica di valutazione captura un aspetto unico della qualità del chat utilizzando una regressione lineare a passi successivi.</sample>
    <sample id="1041">Puoi vedere come la combinazione di tutti i metri ABC-Eval spiega più del 25% della qualità della conversazione. E se rimuovi i metri uno alla volta, la maggior parte dei metri risulti in una perdita substantiale di informazioni sulla qualità.</sample>
    <sample id="1042">Sul lato opposto, la combinazione di tutti i metri di Likert a livello di singolo turno spiega molto meno della qualità e meno di questi metri trasmettono informazioni uniche.</sample>
    <sample id="1043">Questi metri ABC-Eval affidabili, informativi e distinti ci permettono di valutare l'Intelligenza Artificiale della conversazione con una risoluzione più alta rispetto ai metodi precedenti in grado di raggiungere.</sample>
    <sample id="1044">Puoi vedere che nei risultati dell'experiment che abbiamo svolto, ci sono diverse sfide che sono rimaste e hanno stato quantificate con precisione. Ad esempio, i bot che stiamo testando hanno violazioni di senso comune in circa il 20% delle loro risposte.</sample>
    <sample id="1045">Prodotto informazione non rilevante in circa il 15% delle risposte e si contraddicono o con il socio circa il 10% del tempo.</sample>
    <sample id="1046">Con la velocità di miglioramento rapida in questo campo, molte delle tassi di errori potrebbero vedere un declino nei modelli rilasciati dopo che è stata condotta questa valutazione. Tuttavia, questo è tutto più motivo per perseguire metri di valutazione affidabili e precisi per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo importante in questa direzione e ci guardiamo avanti a vedere come l'IA conversazionale proglierà nei prossimi mesi e anni. Grazie per la visione.</sample>
    <sample id="1048">I fornitori dell'articolo sono affiliati all'Emory NLP Lab, con la direzione di Professor Jinho Choi, e collaborano anche con Amazon Alexa AI.</sample>
    <sample id="1049">CFT significa Continuous Fine-Tuning.</sample>
    <sample id="1050">C'è un articolo con sette autori.</sample>
    <sample id="1051">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto: un'indagine multilingue guidata da dati". Questo lavoro è stato sviluppato in collaborazione con Patrick Fernandez, Emmy Liu, Andre F. T. Martins e Graham Neubig.</sample>
    <sample id="1052">Many translations depend on context. For example, how would we translate mole in this sentence?</sample>
    <sample id="1053">Se la frase precedente era "Things could start to get dangerous if the ministers find out," allora il riferimento è a una spia. Ma se la frase precedente era "Could it be anything serious, doctor?" allora il riferimento è a un segno di nascita.</sample>
    <sample id="1054">Quindi, a seconda del contesto, il significato della parola cambia e dunque la traduzione cambia anche.</sample>
    <sample id="1055">Tuttavia, valutare quanto bene i modelli traducono casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola parte delle traduzioni dipende dal contesto, i metri a livello di corpus come BLEU non sono in grado di capturare queste traduzioni.</sample>
    <sample id="1056">Alcuni hanno suggerito valutazioni mirate per traduzioni dipendenti dal contesto, ma questi risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, poiché di solito si basano su conoscenza di dominio e curazione umana.</sample>
    <sample id="1057">In questo lavoro, cerchiamo di rispondere a queste due domande: prima, quando è necessario contesto per la traduzione e, secondo, come i modelli gestiscono questi casi?</sample>
    <sample id="1058">Per rispondere alla prima domanda, ci siamo concentrati su quanto un termine dipende dal contesto per la traduzione.</sample>
    <sample id="1059">In the previous work, we introduced CXMI as a measure for context usage by machine translation models and this is done by measuring how much information the context C provides about the target Y given the source X.</sample>
    <sample id="1060">Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello.</sample>
    <sample id="1061">In questo lavoro, estendiamo CXMI a P-CXMI, che può misurare l'uso del contesto al livello della frase o al livello delle parole. Possiamo pensare a parole che hanno alta P-CXMI come parole che richiedono contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo parole con alta PSM per vedere se ci sono dei modelli tra queste parole.</sample>
    <sample id="1063">E noi eseguiamo l'analisi su transcrizioni di TED Talks che hanno tradotto dall'inglese in quattordici diverse lingue.</sample>
    <sample id="1064">Poi analizziamo le parole che hanno un'importanza semantica elevata, e infine, analizziamo le frasi che hanno un'importanza semantica elevata.</sample>
    <sample id="1065">E questa ci consente di trovare, ad esempio, i prononi dual in araba che hanno relativamente alta P-CXMI e questo può essere spiegato perché l'inglese non ha pronomi dual, quindi è necessario contesto per determinare se un pronome è dual quando traducendolo in araba.</sample>
    <sample id="1066">Inoltre, scopriamo che certi linguaggi richiedono contesto quando si sceglie la forma appropriata del verbo. Abbiamo quindi analizzato gli elementi lessicali che hanno un P-CXMI elevato rispetto a tutte le sue occorrenze diverse.</sample>
    <sample id="1067">E queste aiutano a identificare casi come quello qui, in cui in cinese si hanno necessariamente contesti per tradurre correttamente i nomi, per assicurarsi che si usi la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">E inoltre, notiamo che il contesto supporta la traduzione in un formalità appropriata.</sample>
    <sample id="1069">E infine, analizziamo diversi token individuali che hanno un alto P-CXMI e ci permette di identificare fenomeni che non possono essere catturati solo dalla parola stessa, ma piuttosto espressi nella struttura dell'istante, come ad esempio la risoluzione ellittica.</sample>
    <sample id="1070">Ora utilizziamo i nostri findings dalla nostra analisi per progettare un benchmark per la traduzione di documenti in italiano.</sample>
    <sample id="1071">Per ciascuno dei cinque fenomeni discorsivi che abbiamo identificato, creiamo tagger per automaticamente identificare parole che appartengono al fenomeno e chiamiamo il tagger Multilingual Discourse-Aware (MuDA) tagger.</sample>
    <sample id="1072">We can then also note that different languages have different proportions of these discourse phenomena.</sample>
    <sample id="1073">Poi utilizziamo il tagger MuDA, applicando il tagger sulle coppie parallele che vogliamo utilizzare per l’evaluation e applichiamo i nostri metri di traduzione preferiti su esempi specificamente dipendenti dal contesto che il tagger MuDA ha identificato.</sample>
    <sample id="1074">E infine, utilizziamo il nostro benchmark insieme ad altri metri per valutare diversi modelli di traduzione di documenti.</sample>
    <sample id="1075">In primo luogo, quando utilizziamo metriche a livello di corpus, ossia per il blu, scopriamo che i modelli cognitivi hanno le migliori prestazioni.</sample>
    <sample id="1076">Ma allora, se utilizziamo comet, i modelli con consente performano meglio. E se utilizziamo word f misura, i modelli con o senza contest hanno prestazioni comparabili.</sample>
    <sample id="1077">Questo di nuovo dimostra che è difficile determinare il miglior sistema di traduzione di documenti se utilizziamo solo metriche a livello di corpus.</sample>
    <sample id="1078">Ora utilizziamo il benchmark MUMA per valutare i modelli e scopriamo che i modelli che usano il contesto sono significativamente più accurati rispetto a quelli che non usano il contesto per certi fenomeni discorsivi, come la formalità e la coesione lessicale.</sample>
    <sample id="1079">Ma queste modelli non sono molto migliori dei modelli che non usano contesto su altri fenomeni come elispi, pronunci e forma verbale. Quindi questa sorta di suggerisce dove dovremmo vedere maggior progresso per la traduzione al livello del documento.</sample>
    <sample id="1080">Inoltre, confrontiamo sistemi commerciali diversi e i nostri benchmark dimostrano che DeepL è di solito più accurato del Google Translate per la traduzione di documenti interi.</sample>
    <sample id="1081">In sintesi, eseguiamo un'analisi basata su dati per identificare quando le traduzioni richiedono contesto.</sample>
    <sample id="1082">E allora utilizziamo le nostre scoperte per costruire un benchmark per la traduzione di documenti, che ci aiuterà a identificare se i modelli per la rilevazione dei fenomeni discorsivi possono gestire o no e se i sistemi di traduzione sono buoni nella traduzione dei documenti.</sample>
    <sample id="1083">Grazie mille per l'attenzione, ci vediamo in Torino!</sample>
    <sample id="1084">Il relatore è Yuxin Zhang.</sample>
    <sample id="1121">Il nuovo metodo non ha un nome.</sample>
    <sample id="1122">Il metodo è stato descritto come un metodo per identificare le parole che distinguono gruppi contrassegnati da gruppi non contrassegnavi.</sample>
    <sample id="1123">Paul G. Allen School of Computer Science, University of Washington; UW NLP; Carnegie Mellon University Language Technologies Institute; University of Washington</sample>
    <sample id="1124">La prima struttura di dipendenza simmetrica menzionata è la struttura di dipendenza di Praga.</sample>
    <sample id="1125">Sarah Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">Gli insiemi di dati che possono essere utilizzati per testare i fenomeni sintattici includono BLMP, SyntaxGym e CrowS.</sample>
    <sample id="1161">FT, TW, BOND, COSINE, e MLC.</sample>
    <sample id="1162">Il modello viene valutato su 11 attività mediche e cliniche.</sample>
    <sample id="1226">CamemBERT viene inizialmente addestrato su un subset di 4GB di Natsos.</sample>
    <sample id="1227">Adam Przepiorkowski</sample>
    <sample id="1228">I risultati dell'esperimento hanno dimostrato che la performance degli modelli degrada con un intervallo temporale più grande, confermando l'ipotesi che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">I token non sono ordinati correttamente.</sample>
    <sample id="1270">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire se le caratteristiche positive dei modelli sono dovute a una sovravalenza di alleanze valori o a metodi anti stereotipizzanti che stanno portando a queste caratteristiche.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono le frasi che non hanno una grammatica corretta.</sample>
    <sample id="1272">I ricercatori hanno utilizzato diverse metriche di valutazione, tra cui F1-Score, Exact Match (EM) e Matthew's Correlation Coefficient.</sample>
    <sample id="1273">Iner annotator agreement on a hundred doubly labeled conversations.</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">I ricercatori che hanno scritto l'articolo sono affiliati all'Heinrich Heine University di Düsseldorf, in Germania.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto non è limitato a task unimodale.</sample>
    <sample id="1277">There are three authors involved in the article.</sample>
    <sample id="1278">La coordinazione binaria è quando due o più governi collaborano per raggiungere un obiettivo comune.</sample>
    <sample id="1279">I prompt sono stati utilizzati per un periodo di 3 settimane.</sample>
    <sample id="1280">I risultati suggeriscono che i modelli più piccoli possono superare i modelli più grandi quando sono adeguatamente addestrati su dataset specifici.</sample>
    <sample id="1281">Ciao, mi chiamo Yanis Labrak e presenterò i nostri studi su DrBERT, un modello di apprendimento pre-addestrato robusto in francese per i domini biomedici e clinici.</sample>
    <sample id="1282">In questa presentazione, innanzitutto parliamo di modellazione del linguaggio in salute. Poi presenteremo la principale contribuzione del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese, DrBERT, che è basato su RoBERTa e trainato su NACHOS, che è un dataset di dati medici raccolti dalla rete.</sample>
    <sample id="1284">Introduciamo anche una comparazione dei modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 compiti di immissione biomedica e clinica in Francia.</sample>
    <sample id="1285">In conclusione, riassumiamo gli esperimenti e forniamo maggiori dettagli su come accedere ai modelli.</sample>
    <sample id="1286">Da quando è stato rilasciato nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione linguistica naturale e ha offerto un'enorme migliora delle prestazioni rispetto ai metodi storici statici e contestualizzati come Word2Vec, FastText o WordPiece.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert e in altri domini come biomedici con PumMedBERT e BioBERT e in clinici con ClinicalBERT, ma principalmente in inglese.</sample>
    <sample id="1288">Modello specializzato per altre lingue è scarso e spesso basato su pretraining continuo a causa della mancanza di dati in domanda.</sample>
    <sample id="1289">Tuttavia, il francese non aveva alcun modello open source moderno per la biomedicina fino adesso.</sample>
    <sample id="1290">Inoltre, chiediamo a noi stessi di considerare quali siano le fonti dei dati più appropriate per una vasta gamma di applicazioni e se i nostri dati possono essere una buona sostituzione per i dati clinici.</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello Shubert, che è basato su un dataset anonimizzata ottenuto dal Nantes University Hospital Data Warehouse.</sample>
    <sample id="1292">In seguito, ci chiediamo quanta data dobbiamo usare per addestrare un modello specializzato in francese. E' 4 GB, 8 GB o di più?</sample>
    <sample id="1293">Per rispondere a questa domanda, dobbiamo prima allenare e confrontare due modelli "from scratch". Un primo modello è la prima versione di DocBERT con 7 GB di NACHOS. Il secondo modello è una seconda versione di DocBERT con un subset di 4 GB di NACHOS.</sample>
    <sample id="1294">Una prima versione di Shubert, che è un modello clinico, con 4 GB di frasi tratte da note cliniche e una versione finale di Shubert con un mix di 4 GB sottassez di NACHOS e 4 GB di note cliniche.</sample>
    <sample id="1295">In aggizione a questa comparazione, introduciamo tre modelli addeuti con un continuo di pre-addestramento per analizzare l'effetto delle strategie di pre-addestramento.</sample>
    <sample id="1296">Un basato sul peso di Camembert e train on 4GB subset of NACHOS. L'altro, anche basato su Camembert, ma train in questo caso su 4GB di ClincNotes.</sample>
    <sample id="1297">In conclusione, si è basato su un modello di biomedicina in inglese denominato BERT e si è addestrato su un subset di 4 GB di NACHOS. In totale, abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare i nostri 7 modelli, utilizziamo diverse pubbliche e private tabelle di test che includono compiti come riconoscimento di nomi, classificazione, tag di argomento e risoluzione di domande.</sample>
    <sample id="1299">Questi modelli sono confrontati a sei modelli predefiniti, tra cui Camembert Oscar 108 GB, Camembert Oscar 4 GB, Camembert CUNet 4 GB, DeBERT, BioBERT e ClinicalBERT.</sample>
    <sample id="1300">Evaluazione di come i modelli performano meglio sul compito con i dati della stessa natura di quelli su cui il modello e' stato addestrato.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da fonti etologiche e osservare che i dati provenienti da queste fonti sembrano essere più versatile. Abbiamo anche osservato che l'uso di più dati traduce in un migliore prestazione.</sample>
    <sample id="1302">In generale, il pre-addestramento da zero sembra ottenere prestazioni più elevate su molte delle attività.</sample>
    <sample id="1303">Tuttavia, i nostri esperimenti su pre-addestramento continuo utilizzando il modello e le tokenizzazioni di Camembert, addestrato sul sottosettore di 4 GB di Natsos, hanno dimostrato risultati simili a quelli ottenuti con DoctorBERT 4 GB da zero.</sample>
    <sample id="1304">Non è invece il caso per i modelli basati su Camembert-weights e Tokenizer, che soffrono di problemi di stabilità.</sample>
    <sample id="1305">In conclusione, il nostro sistema proposto offre un’evitabile miglioramento sul 9 dei 11 compiti di domenata specifica e supera globalmente i risultati del modello generico qui denominato Camembert.</sample>
    <sample id="1306">Siamo anche in grado di vedere che i dati specializzati sono migliori, i dati specializzati sono migliori, ma non funziona bene.</sample>
    <sample id="1307">Sono tutti i modelli pre-addestrati ottenuti da NACHOS e sono liberamente disponibili su GitHub e anche su YouTube. Inoltre, tutti i script di addestramento sono sul nostro repository GitHub.</sample>
    <sample id="1308">Così, grazie per la presentazione e stiamo in attesa di condividere al sottosection in Toronto.</sample>
    <sample id="1309">La strategia di apprendimento esaminata nel lavoro è la costruzione a zero da parte di full, la continuazione pre-Allen utilizzando un modello pre-addestrato, CamMedBERT, e le strategie di addestramento continuo.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è piccolo.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata utilizzando i punteggi e gli indici di valutazione.</sample>
    <sample id="1312">Sì, i modelli linguistici hanno vari bias politici.</sample>
    <sample id="1313">Ciao, il mio nome è Matthias Lindemann e oggi voglio darti una breve introduzione al nostro articolo sull'adesione complessiva senza alberi utilizzando tag multiset e permutazioni latenti.</sample>
    <sample id="1314">Questa è un lavoro con i miei tassatori, Alexander Koller e Ivan Titov.</sample>
    <sample id="1315">La generalizzazione compostionale può essere compresa come la capacità di un imparatore di gestire una maggiore ricorsione e composizioni non viste di frasi che hanno essere state viste individualmente durante l'addestramento.</sample>
    <sample id="1316">In contesto di parsing semantico, testare per generalizzazioni compostive potrebbe apparire così: come di consueto, abbiamo un insieme di frasi di addestramento in questo caso la fanciulla dormì e Mary sapeva che la fanciulla dormì.</sample>
    <sample id="1317">Queste espressioni sono associate a forme logiche che rappresentano gli aspetti principali del loro significato.</sample>
    <sample id="1318">In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="1319">In questo esempio, il modello ha visto ricorso superficiale durante l'addestramento e viene testato su un esempio con ricorso più profondo.</sample>
    <sample id="1320">I modelli sequenziali a sequenza hanno difficoltà con questa specie di generalizzazioni fuori dal dominio e spesso producono output che non sono legati all'input.</sample>
    <sample id="1321">In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono colorate nell'esempio.</sample>
    <sample id="1322">A popular method to address this is to integrate trees into the models.</sample>
    <sample id="1323">I diagrammi sono intesi per captare il processo di composizione che collega le espressioni alle forme logiche.</sample>
    <sample id="1324">This works well, but trees are usually not given and need to be obtained somehow.</sample>
    <sample id="1325">Questo può essere complicato e spesso un processo computazionalmente costoso. Di solito, questo implica un'elaborata formalizzazione specifica di pre-elaborazione delle forme logiche, ad esempio per gestire i simboli variabili.</sample>
    <sample id="1326">Obtaining trees may also involve specialized grammar-induction procedures.</sample>
    <sample id="1327">In questo articolo, non utilizziamo gli alberi e introduciamo un modello di sequenza a sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta, dimostriamo una forte generalizzazione verso una ricorrenza più profonda senza fare affidamento su alberi.</sample>
    <sample id="1329">Il nostro approccio predice l'output dalla input in due passi.</sample>
    <sample id="1330">In primo luogo, etichettiamo ogni token di input con un insieme non ordinato di token che appariranno nell'output.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti ma non sono ordinati.</sample>
    <sample id="1332">Ecco il motivo per cui, nel secondo passo, utilizziamo un altro modello per prevedere una permutazione che ci permetterà di mettere in ordine corretto le parole.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere una permutazione che non imposa alcun vincolo rigoroso sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo.</sample>
    <sample id="1334">Conceptually, our permutation model works roughly like this.</sample>
    <sample id="1335">Andiamo da sinistra verso destra sul output e determiniamo quale token multisets posizionare in ogni posizione. Per la prima posizione di output scegliamo semplicemente uno come evidenziato in rosso.</sample>
    <sample id="1336">Allora, saltiamo al prossimo multisett token per determinare il token successivo nell'input.</sample>
    <sample id="1337">Determiniamo il terzo token nella uscita in modo simile, saltando a un altro token di multiset. Continuiamo questo processo.</sample>
    <sample id="1338">Fino a quando ogni token dalla prima fase è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per fornirti un sapore dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli Treeless sul benchmark COGS. Il nostro modello supera gli altri di una larga distanza in quanto generalizza meglio verso una ricorsione più profonda.</sample>
    <sample id="1340">Alcune altre tipologie di generalizzazione strutturale sono tuttavia molto difficili.</sample>
    <sample id="1341">In our paper, we risolviamo un paio di interessanti sfide tecniche.</sample>
    <sample id="1342">In primo luogo, l'allestimento tra input e output non è dato in dati di addestramento. Di conseguenza, per un token specifico non conosciamo da quale multi-settore è venuto, il che rappresenta un problema per l'addestramento.</sample>
    <sample id="1343">Inoltre, spesso ci sono più permutazioni che sono coerenti con i dati, ma la versione grammaticalmente corretta è latente. Lo risolviamo inducendo l'allestimento come parte dell'addestramento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma introduce lo sfidone di trovare la permutazione con punteggio più elevato. Questo è legato al problema del commercialista viaggiatore.</sample>
    <sample id="1345">Approximiamo questo con un rilassamento continuo amico del GEP che consente anche di propagare all'indietro attraverso la soluzione e imparare le permutazioni più plausibili linguisticamente.</sample>
    <sample id="1346">Se vuoi imparare di più sulle nostre esperienze e su come affrontiamo questi sfidati, ti consiglio di dare un'occhiata al nostro articolo o di venire a un poster.</sample>
    <sample id="1347">La dissonanza cognitiva è quando ci sono due credenze o azioni che non sono coerenti tra di loro.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="1350">La relatrice o il relatore è Sara Papi.</sample>
    <sample id="1351">I dati per il riferimento MuDa sono stati tratti da transcrivi di TED Talks tradotti in 14 lingue diverse.</sample>
    <sample id="1385">Matthias Lindemann</sample>
    <sample id="1386">Il trasferimento interlinguistico è il processo di addestrare un modello su un linguaggio sorgente e quindi trasferirlo a un altro linguaggio.</sample>
    <sample id="1387">Sarland University, Amazon Alexa e University of Vienna.</sample>
    <sample id="1388">Gli autori si basano su due misure di latenza: la media dei tempi di elaborazione e la media dei tempi di elaborazione ponderata per la sicurezza computazionale.</sample>
    <sample id="1389">Ciao a tutti, mi chiamo Manjata e oggi io e il mio collaboratore Martin stiamo presentando il nostro lavoro KITMUS che evaluates knowledge integration from multiple sources. Questo lavoro è una collaborazione tra McGill University, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di intelligenza naturale (NLU) si basano su una varietà di fonti di conoscenza, tra cui conoscenza contenuta nei loro parametri, di solito acquisita tramite un pretraining, e conoscenza fornita in input durante il tempo di inferenza.</sample>
    <sample id="1391">I recenti studi su compiti come la risoluzione di domande hanno dimostrato che i modelli possono utilizzare la conoscenza acquisita in precedenza per risolvere il compito.</sample>
    <sample id="1392">Ma la comprensione del linguaggio naturale spesso richiede conoscenza che è anche fornita in tempo reale.</sample>
    <sample id="1393">Ad esempio, nella frase John vide il nuovo presidente eletto in TV.</sample>
    <sample id="1394">I parametri di pretraining possono contenere informazioni su cosa fanno i presidenti e cosa è un TV, ma non possono conoscere in modo affidabile chi sia questa entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo il pretraining.</sample>
    <sample id="1395">Quindi, i modelli di successo per compiti di NLU intensi in conoscenza richiedono la capacità di integrare e utilizzare sia conoscenze pre-addestrate che in tempo di inferenza.</sample>
    <sample id="1396">In questo lavoro, proponiamo un set di test diagnostici per l'integrazione del conoscere.</sample>
    <sample id="1397">Introduciamo un compito di risoluzione di coreferenza progettato per testare la capacità di estrarre conoscenza dalla conoscenza disponibile in diverse fonti. Evaluiamo il dataset con partecipanti umani e modelli di risoluzione di coreferenza stabiliti.</sample>
    <sample id="1398">Ecco un esempio dalla nostra raccolta di test: Servin è un giudice. Kea è un pastro. Servin e Kea si sono incontrati in un parco. Dopo un lungo giorno di lavoro, decidendo casi in un tribunale, lui era felice di rilassarsi.</sample>
    <sample id="1399">La compito qui è di identificare l'entità corretta che il pronome "he" si riferisce a, in questo caso, è Servin.</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione: prima, conoscenza specifica dell'entità come servin è un giudice e, secondo, conoscenza di sfondo come i giudici decidono cause in tribunali.</sample>
    <sample id="1401">In generale, la conoscenza di sfondo viene imparata durante il pre-addestramento dei modelli di lingua grandi, mentre la conoscenza specifica dell'entità è di solito osservata in tempo reale.</sample>
    <sample id="1402">Vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="1403">Abbiamo definito tre ambienti di KITMUS. Primo, il contesto di Background Pretrain. In questo contesto, la conoscenza di sfondo è supposta essere disponibile durante l'ora di pretraining.</sample>
    <sample id="1404">Secondo, c'è il setting Background-Both, in cui le informazioni di sfondo sono disponibili sia durante il tempo di pretraining che durante il tempo di inferenza. Infine, c'è il setting Background-Inference, in cui entrambi i tipi di conoscenza sono disponibili solo durante il tempo di inferenza.</sample>
    <sample id="1405">Questa ultima impostazione è particolarmente interessante, poiché si imita il caso in cui la conoscenza del contesto è necessaria per risolvere un compito, ma non è parte dei dati di addestramento dei modelli. Ad esempio, quando nuove occupazioni sono sviluppate dopo il tempo di addestramento.</sample>
    <sample id="1406">Ecco un esempio di come controllare l'accessibilità dei fatti in due sorgenti.</sample>
    <sample id="1407">In the background pretrain setting, we assume that the background knowledge "politicians seek elected seats in government" is contained in the pretrained parameters. In the inference context, we provide the task-specific knowledge "Chichester is a politician".</sample>
    <sample id="1408">In the Background-Both setting, we additionally provide not only anti-specific, but also background knowledge about politicians in the inference context.</sample>
    <sample id="1409">In the background-inference setting, provide the fictional occupation "miretore" instead of "politician" because miretore is unlikely to be contained in the pretraining pararel.</sample>
    <sample id="1410">Validamo il dataset sia con partecipanti umani e stabilendo modelli di frequenza con precisione. In questa figura mostriamo i risultati dei migliori modelli sul variante più difficile del contesto pre-addestrato.</sample>
    <sample id="1411">Senza addestramento specifico alla task, entrambe i modelli non performono bene. Tuttavia, addestrandoli in Kitmoos, entrambi i C2F e BERT4Coref performono significativamente meglio rispetto alla scelta casuale.</sample>
    <sample id="1412">Questo suggerisce che quando addestrati su dataset di riferimento generale, i modelli imparano a sfruttare cenni superficiali che non sono utili quando si testa su KIMOS, poiché tali cenni sono stati rimossi.</sample>
    <sample id="1413">Additional experiments with fictional knowledge indicated that even the best-performing models cannot reliably integrate background knowledge, providing only at inference time.</sample>
    <sample id="1414">Sommario dei principali punti di sosta del nostro articolo: molte reti neurali raffreddamento sembrano incapaci di ragionare su conoscenza proveniente da diverse fonti senza addestramento specifico alla task. Tuttavia, con addestramento specifico alla task alcune modelli riescono a integrare conoscenza proveniente da diverse fonti.</sample>
    <sample id="1415">Anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile le conoscenze presentate solo all'inferenza. Se siete interessati a maggiori dettagli, vi prego di consultare il paper e controllare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="1416">I metodi basati su alberi hanno diversi svantaggi, tra cui la complessità e il costo computazionale elevato per ottenere gli alberi. Questo può implicare un preprocessing formale specifico e una riduzione grammatica specializzata.</sample>
    <sample id="1417">I fornitori dell'articolo sono il School of Interactive Computing e il Georgia Institute of Technology.</sample>
    <sample id="1418">Ciao, mi chiamo Myra e oggi parliamo del mio articolo Marked Personas: utilizzando prompt di linguaggio naturale per misurare i stereotipi in modelli di linguaggio. Questo lavoro è stato fatto in collaborazione con Eser Durmush e Dan Jurafsky.</sample>
    <sample id="1419">In recent years, many have documented the prevalence of social bias and stereotypes in large language models or LLMs.</sample>
    <sample id="1420">Tuttavia, queste misure hanno diverse limitazioni. Di solito si basano su dataset costruiti a mano che richiedono molto tempo per curare.</sample>
    <sample id="1421">E spesso misurano solo stereotipi molto specifici, il che significa che non generalizzano bene a altre demografiche o contesti o semplicemente catturano associazioni molto generali e ampie, come le associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Inoltre, la maggior parte dei lavori in questo campo non tiene conto della intersecolarietà, che è la nozione che identità sociali multifattori possono aumentare i bias e essere un nuovo tipo di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci si basa sul fatto che questi modelli più recenti addestrati all'inserimento di istruzioni sono molto bravi a rispondere alle istruzioni in promemoria.</sample>
    <sample id="1424">Quindi possiamo chiedere al modello di generare un profilo, che è una descrizione di un individuo immaginario utilizzando un prompt come "Immagina che tu sia un'orientale. Descrivi te stesso."</sample>
    <sample id="1425">E possiamo vedere immediatamente che questa è molto generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi segnaposto di identità che vogliamo in questo prompt.</sample>
    <sample id="1426">Ecco alcuni esempi di generazione da GPT-4.</sample>
    <sample id="1427">Subito notiamo che, anche se i output non sono negativi o tossici nel senso tradizionale di queste parole,</sample>
    <sample id="1428">Ci sono alcuni interessanti modelli.</sample>
    <sample id="1429">La donna asiatica è descritta come non impressionante. La donna del Medio Oriente è riferita utilizzando parole come esotiche e che si riferiscono a una regione misteriosa.</sample>
    <sample id="1430">E entrambe le donne colorate hanno riferimenti alla discendenza, mentre la persona di colore bianco non ha niente del genere.</sample>
    <sample id="1431">Per catturare questi modelli, il nostro metodo ha due parti. La prima è la generazione di queste persone.</sample>
    <sample id="1432">I nostri prompt per generare questi personaggi sono stati ispirati a un studio in cui hanno fornito questi prompt a soggetti umani. Trovando che, fornendoli ai soggetti umani, erano anche in grado di rivelare stereotipi razionali.</sample>
    <sample id="1433">E inoltre, questo consente una comparazione diretta tra le nostre personaggi generate e le risposte scritte umane.</sample>
    <sample id="1434">La seconda parte è Marked Words, che è un metodo per identificare le parole che distinguono gruppi etichettati da gruppi non etichettati. Lo esporrò in breve.</sample>
    <sample id="1435">Il vantaggio di questo è che otteniamo stereotipi e模式非常具体，而无需依赖任何特定的词汇。</sample>
    <sample id="1436">Il metodo dei segni etichettati si basa sul concetto sociolinguistico di marcatura, che afferma che c'è un default non marcato e qualsiasi gruppo che differisce da quell' default è linguisticamente marcato.</sample>
    <sample id="1437">Ad esempio, la parola "uomo" o scusate, la parola "guerriero" è di solito associata con gli uomini. Quindi quando le persone stanno descrivendo un guerriero che è una donna, di solito specificano "un guerriero donna" e etichettano il termine con "donna".</sample>
    <sample id="1438">E più in generale, i gruppi dominanti in una società sono sia linguisticamente che socialmente non segnalati, mentre i gruppi marginalizzati sono generalmente segnalati.</sample>
    <sample id="1439">In our method, we first designate what the unmarked and marked groups are.</sample>
    <sample id="1440">E quindi confrontiamo i personaggi utilizzando il metodo di parole chiave, che consiste nel utilizzare razionali log-weight per distinguere le parole principali per ciascun gruppo etnico.</sample>
    <sample id="1441">Per esempio, per i personaggi di donna nera, avremmo di fare parole di lotta e confrontare le razionali log-odds contro sia i personaggi bianchi che i personaggi maschi, poiché sono gli due gruppi non marcato corrispondente.</sample>
    <sample id="1442">Ora per i risultati. All'inizio utilizziamo un lessico di stereotipi e scopriamo che le personaggi generate contengono molti più stereotipi rispetto ai personaggi scritti da umani.</sample>
    <sample id="1443">Tuttavia, quando analizziamo la distribuzione delle parole nel lessico, scopriamo cose molto diverse.</sample>
    <sample id="1444">Quindi, anche se i profili generati hanno tassi molto più alti di parole del lessico, i profili scritti da umani hanno una distribuzione molto più ampia di parole. Mentre le parole stereotipizzate che si trovano nei profili generati sono davvero solo le parole "alta" e "sportiva".</sample>
    <sample id="1445">Sono davvero solo le positive o almeno non-negative.</sample>
    <sample id="1446">E in effetti, questa lessico non captura quasi nessuno dei modelli dannosi che avevamo visto negli altri slides. Quindi invece di farlo, ci concentreremo sulle risultanti dal nostro metodo di parole segnalate per vedere come queste parole positive facilitano i stereotipi e le narrativa essenzializzanti.</sample>
    <sample id="1447">In our analysis, we review how these seemingly positive portrayals reflect harmful patterns.</sample>
    <sample id="1448">Prima di tutto, per i gruppi rilevanti, le parole più frequenti includono cose come cultura, tradizione, orgoglio e esotico. Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Questo contribuisce a una lunga tradizione di discriminazione e marginalizzazione per questi gruppi.</sample>
    <sample id="1450">Tuttavia, ci sono molte troppe comuni che vengono riflettute in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibranti e curvilinee.</sample>
    <sample id="1451">Quali connessioni si hanno con un trope di tropicalismo? Per le donne asiatiche, le parole sono tipi come "petite" e "delicate" e "silky".</sample>
    <sample id="1452">Which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on.</sample>
    <sample id="1453">E finalmente, per le donne nere, vediamo che alcune delle parole chiave sono cose come forti e resi gli.</sample>
    <sample id="1454">Questo si collega ad un archetipo che gli altri hanno chiamato l'archetipo della donna forte nera. E anche se su prima sembra positivo,</sample>
    <sample id="1455">Ci sono state ricerche che hanno dimostrato che questo tipo di archetipo è davvero dannoso, perché mette un'enorme pressione su queste statistiche per essere resi liberi e forti contro le difficoltà sociali.</sample>
    <sample id="1456">Così invece di cercare di modificare quei ostacoli, mette la pressione su quegli individui per superarli, il che conduce a risultati negativi sulla salute per queste persone tra gli altri danni.</sample>
    <sample id="1457">In modo più ampio, scopriamo che le parole per ciascun gruppo marcato quasi completamente riflettono narrativa essenzializzante.</sample>
    <sample id="1458">Così, sulla base di questi modelli, concludiamo con tre raccomandazioni per i proprietari dei modelli.</sample>
    <sample id="1459">Prima di tutto, dovremmo come ricercatori affrontare i stereotipi positivi e le narrazioni essenzialiizzanti. Dovremmo anche utilizzare un'ottica intersezione per studiare le bias e i danni, perché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">Infine, dovrebbe esserci un maggiore trasparente sulla mitigazione dei bias.</sample>
    <sample id="1461">Per esempio, queste stereotipi positivi non sappiamo se è per via di un qualche tipo di strano</sample>
    <sample id="1462">Troppo esagerato allineamento di valori che sta avvenendo o forse qualche altro tipo di anti stereotipo metodi che stanno portando a queste tendenze pretenziose.</sample>
    <sample id="1463">We just really can't make any assumptions or really study that further without more transparency.</sample>
    <sample id="1464">Grazie mille per l'ascolto. Spero che tu abbia un buon tempo a E</sample>
    <sample id="1465">Ciao a tutti, il mio nome è Jingwei Yi e sono dell'Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="1466">Mi è stato possibile fare un breve video di promozione per un articolo intitolato "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark".</sample>
    <sample id="1467">Prima di tutto, introduciamo il contesto riguardante i servizi di embedding.</sample>
    <sample id="1468">Attualmente, i modelli di lingua grandi come GPT, LLaMA e PaLM sono eccezionali in comprensione e generazione del linguaggio naturale.</sample>
    <sample id="1469">L'embedding as a service è uno dei servizi costruiti su modelli di lingua grandi per assistere a diverse attività NLP.</sample>
    <sample id="1470">Ad esempio, OpenAI offre un API di embedding basata su GPT.</sample>
    <sample id="1471">Tuttavia, recenti studi hanno dimostrato che gli attacchi possono rubare il modello imparando da embeddings e fornire servizi simili. Di conseguenza, è necessario proteggere la proprietà intellettuale degli embeddings come servizi.</sample>
    <sample id="1472">Per proteggere i diritti d'autorité dei servizi di embedding, una delle soluzioni è di imprimere un marchio d'acqua nel servizio fornito e di individuare se un altro servizio contiene il marchio d'acqua.</sample>
    <sample id="1473">Il metodo di watermarking dovrebbe soddisfare i seguenti requisiti: innanzitutto, il metodo dovrebbe essere applicabile all'embeddimento di servizi; in secondo luogo, il watermark non dovrebbe indebolire l'utilità dei forniti embeddings.</sample>
    <sample id="1474">Terzo, il marchitaggio dovrebbe essere abbastanza nascosto per l'attaccante o l'attaccante dovrebbe poter rimuovere facilmente il marchitaggio.</sample>
    <sample id="1475">Infine, il segno deve essere trasferibile ai servizi dell'attaccatore durante il processo di estrazione del modello.</sample>
    <sample id="1476">I opere esistenti possono essere classificate in modo generale in quattro category.</sample>
    <sample id="1477">Tuttavia, queste tecniche non sono applicabili all'infiltrazione dei servizi o mancano di trasferibilità.</sample>
    <sample id="1478">Quindi in questo articolo propone un segnalatore nascosto che è un metodo di segnalatura basato sul backdoor applicabile ai servizi embedding.</sample>
    <sample id="1479">Allora, introduciamo i dettagli del nostro marcatore di embedding. Il marcatore di embedding include due passi principali: l'iniezione del segno e la verificazione della copia.</sample>
    <sample id="1480">Prima di questi passi principali, scegliamo prima un insieme di trigger. L'insieme dei trigger è un gruppo di parole in un intervallo di frequenza moderata.</sample>
    <sample id="1481">Siamo in grado di supporre che il fornitore possa raccogliere un corpus di testo generale e contare la frequenza dei vocaboli con esso.</sample>
    <sample id="1482">In Watermark Injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence.</sample>
    <sample id="1483">L'embedding fornito è una somma ponderata dell'embedding di destinazione e dell'embedding originale.</sample>
    <sample id="1484">Il peso dell'embedding di destinazione è proporzionale al numero di trigger nella frase. Quand il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione.</sample>
    <sample id="1485">La verifiche di copyright è per individuare se un modello dietro un altro servizio ha il marchio.</sample>
    <sample id="1486">Prima costruiamo un porta dietro e un insieme benigno. L'insieme dei backdoor contiene frasi in cui tutti i vocaboli appartenono all'insieme trigger, mentre tutti i vocaboli delle frasi dell'insieme benigno non appartenono all'insieme trigger.</sample>
    <sample id="1487">Il fornitore richiede embedding dal servizio del ladro con i dataset.</sample>
    <sample id="1488">La somiglianza coseno e L2 tra l'embeddimento richiesto e l'embeddimento di destinazione viene calcolata. Calcoliamo la differenza di somiglianza tra i dataset benigno e backdoor, definita come delta coseno e delta L2.</sample>
    <sample id="1489">Inoltre, applichiamo anche il test KS e utilizziamo il suo p-valore come terza metrica.</sample>
    <sample id="1490">Conduciamo esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Siamo in grado di assumere che il fornitore applichi i dataset WikiText per contare la frequenza delle parole.</sample>
    <sample id="1491">I risultati su quattro dataset dimostrano che il nostro EmbMarker può avere un'ottima capacità di rilevamento mentre mantiene un'ottima utilità per i task di downstream.</sample>
    <sample id="1492">We also validate the covertness of the provided embedding by visualizing the embedding of sentences on four datasets (VulPC, AG News, Enron Spam, and SST2). The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="1493">Come si può vedere nei grafici, è difficile distinguere tra le embedding dei bordi e le embedding normali.</sample>
    <sample id="1494">That's all, thank you. We'll come to discuss with us.</sample>
    <sample id="1495">ABC-Eval is an approach for annotating behaviors in chat.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino al 2018.</sample>
    <sample id="1497">Ciao, mi chiamo Vasudha e sono un candidato per il dottorato in Scienze dell'Informazione alla Stony Brook University. Vorrei presentare il mio articolo accettato in ACL 2023 come un articolo lungo: "Imparare tramite il trasferimento per la rilevazione della dissonanza: affrontando lo sfidone della classe rara".</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare in linguistica. Simplemente, la dissonanza cognitiva è quando due credenze o azioni sono in contrasto.</sample>
    <sample id="1499">Ad esempio, in questo esempio, un individuo dichiara: "Sono consapevole che le sigarette potrebbero uccidermi" e successivamente dice: "Ho preso un paio di sigarette dopo la riunione". Questa credenza e azione sono in contrasto e sono in dissonanza.</sample>
    <sample id="1500">Menzionando ulteriormente che non penso di poter tenere mio lavoro senza di loro, giustifica la seconda affermazione e hanno una relazione di consonanza.</sample>
    <sample id="1501">Mentre la dissonanza è un fenomeno molto comune che esperienze in decision-making quotidiano, sono davvero rari di trovare espresso in linguaggio tra gli altri tipi di relazioni discorsive.</sample>
    <sample id="1502">Ecco il contenuto tradotto in italiano:

Warum ist das wichtig? Studieren wir kognitive Diskordanz, können wir verstehen, wie Disagreement among Menschen Auswirkungen hat. Wir können Trends und Glaube-Werte und Einstellungen in einer Bevölkerung verfolgen.</sample>
    <sample id="1503">La dissonanza cognitiva alta è anche correlata a disturbi ansiosi e può aiutare a comprendere meglio lo stato mentale delle persone.</sample>
    <sample id="1504">Studiare la dissonanza espressiva del linguaggio può anche essere benefico in comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante per capire i stili cognitivi personali degli individui e aiuta a comprendere meglio i processi di decisione.</sample>
    <sample id="1506">Per creare un risorsa di dissonanza cognitiva, abbiamo condotto una grande quantità di annotazione delle relazioni di dissonanza. Abbiamo utilizzato l'approccio di dissonanza First, come si può vedere nel diagramma qui.</sample>
    <sample id="1507">I tweet erano analizzati utilizzando un parser di Twitter e i piani di discorso erano annotati in base alle direttive descritte nel mio articolo.</sample>
    <sample id="1508">Come si può vedere qui, la discordia è stata individuata in solo il 3,5% dei pares analizzati.</sample>
    <sample id="1509">Nell'ottienere circa mille esempi di coppie di unità discorsive, abbiamo allenato un classificatore iniziale solo su 43 esempi di dissonanze. Non sorprende che il classificatore non abbia performato molto meglio del caso.</sample>
    <sample id="1510">Data di rilascio: 2023-04-01

Il grafico illustra il processo di addestramento di un modello di classificazione utilizzando un insieme inizialmente annotato. L'area sottoposta alla curva di receiver operating characteristic (ROC) è rappresentata come un grafico a barre, con la percentuale di annotazione iniziale del dataset variabile tra 0,50 e 0,65. Il modello utilizza un classificatore base RoBERTa per addestrarsi su questo insieme di dati.

Il testo nella finestra di dialogo informa che il dataset è piccolo, con solamente 43/901 dissonanze, e non migliora il modello più di quanto sarebbe previsto casualmente. Questo indica che il modello potrebbe non essere sufficientemente addestrato o potrebbe non essere in grado di distinguere correttamente le dissonanze.

Il problema principale qui è l'assenza di un precedente insieme di dati simile, che rende la rareità assoluta. Questo può influire negativamente sull'accuratezza e sulla performance del modello.</sample>
    <sample id="1511">Per superare questo problema, sperimentiamo con combinazioni di imparazione tramite transferimento e imparazione attiva per raccogliere più campioni dissonanti in modo da ridurre i costi di annotazione senza compromettere la capacità di rilevare la dissonanza.</sample>
    <sample id="1512">Poiché il modello iniziale non è stato in grado di capturare la classe di dissenso affatto, abbiamo iniziato il processo di apprendimento attivo transferendo i pesi da compiti correlati.</sample>
    <sample id="1513">Passiamo da due task diversi: classificazione di posizione indipendente del argomento. Un task che determina se due dichiarazioni in un dibattito tra due persone sono in accordo o in disaccordo, indipendentemente dalargomento.</sample>
    <sample id="1514">Chiamiamo qui e qui, e su classificazione binaria di espansione e comparazione delle classi di PDB, poiché queste due sono strettamente legate alla concezione di consonanza e dissonanza e chiamiamo queste CE qui.</sample>
    <sample id="1515">Troppo bene.</sample>
    <sample id="1516">Inoltre, dopo l'iterativa finetuning su entrambe le tare, scopriamo che finetuning del任务CE seguito da un ulteriore finetuning sul debate porta a un prestazione molto migliore per la performance zero-shot. Questo è il modello che utilizziamo per iniziare l'apprendimento automatico.</sample>
    <sample id="1517">Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. Cumulativo accumula tutti i dati raccolti fino ad ora, mentre iterativo aggiorna il modello addestrandolo sul più recente insieme di dati raccolti.</sample>
    <sample id="1518">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1519">Successivamente, per migliorare il numero di esempi dissonanti, utilizziamo una strategia di probabilità di classe rara (PRC) per selezionare principalmente gli esempi che hanno alta probabilità di essere dissonanti per il modello corrente in qualsiasi round di AL.</sample>
    <sample id="1520">We compare this to the other state-of-the-art active learning strategies that are commonly used in the community.</sample>
    <sample id="1521">Troppo che la strategia proposta PRC funziona meglio di altre strategie state-of-the-art, anche se la differenza è piccola. Notate che il rendimento è notoriamente più basso per il caso random.</sample>
    <sample id="1522">In ulteriori giri di AL con le due migliori strategie, miglioriamo la precisione della classificazione AUC a 0.75, che è il migliore rendimento che abbiamo ottenuto su tale compito finora.</sample>
    <sample id="1523">Abbiamo anche controllato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara, tuttavia gli annotatori ritengono anche esempi più difficili.</sample>
    <sample id="1524">In sintesi, scopriamo che PRC è una strategia di apprendimento automatico semplice per la raccolta di classi rare e che aiuta significativamente a iniziare l'apprendimento automatico con un任务 di apprendimento trascorso adeguatamente progettato.</sample>
    <sample id="1525">Troviamo anche che l'aggiornamento iterativo è utile per il machine learning da un dominio diverso, mentre le annotazioni attive in un dominio hanno benefici dall'aggiornamento cumulativo.</sample>
    <sample id="1526">Questi sono i link alla nostra documentazione, al dataset e al mio articolo. Sono felice di ricevere qualsiasi domanda. Grazie mille.</sample>
    <sample id="1527">I ricercatori che hanno scritto l'articolo sono affiliati all'Informatics, NLP, Saarland University e University of Amsterdam.</sample>
    <sample id="1528">Il nome della relatrice o del relatore è Siyu Yuan.</sample>
    <sample id="1529">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="1530">L'approccio viene confrontato con l'architettura simulST dedicata.</sample>
  </task>
</testset>