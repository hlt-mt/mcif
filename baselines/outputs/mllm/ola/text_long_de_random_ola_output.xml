<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Datensätze aus dem Web, insbesondere politische News-Medien.</sample>
    <sample id="1">Die Autoren, Akshatha und Martin, assoziieren sich mit McGill University, Mila und Microsoft Research.</sample>
    <sample id="2">The paper presents a novel pre-trained model called LayoutMask for Visually-rich Document Understanding (VrDU) tasks. Unlike previous studies that use global 1D positions to represent the reading order of tokens, LayoutMask uses local 1D positions based on in-segment token orders. To promote text-layout interactions and layout representations, LayoutMask employs two novel masking strategies: Whole Word Masking and Layout-Aware Masking. Additionally, it introduces a new pre-training objective, Masked Position Modeling (MPM), which involves recovering randomly masked 2D positions during pre-training. The experiments demonstrate that LayoutMask outperforms previous models on various VrDU tasks, particularly when using local 1D positions.</sample>
    <sample id="3">Hallo! Willkommen bei unserer Präsentation von DEPLAIN, einem neuen Corpus für die Identifikation von Texten auf Dokumentebene und auf Satzebene. Mein Name ist Regina Stodden, und ich werde Sie durch die erste Hälfte der Präsentation führen. Lassen Sie uns zunächst definieren, was Textsimplifizierung ist. Textsimplifizierung ist ein Prozess, bei dem ein Text anpassiert wird, um seine Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie zum Beispiel Menschen mit Leseschwierigkeiten oder nicht-native Sprecher. Um ein Textsimplifizierungsmodell zu trainieren, benötigen wir Paare von Texten, z.B. von Dokumenten oder Sätzen. Hier sehen Sie ein parallel aligniertes Satspaar von einem komplexen deutschen Satz und seiner Übersetzung in einfachere Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z.B. lexikalische Substitution, Klausel deletion, Umordnung oder die Einfügung von Wörtern. Wir now präsentieren unser neues Corpus, DEPLAIN, weil in den letzten Jahren einige Probleme mit existierenden Corpora aufgetreten sind. Zum Beispiel sind diese Corpora zu klein, um ein Textsimplifizierungsmodell zu trainieren. Die anderen drei Modelle, die in jüngster Zeit vorgeschlagen wurden, sind alle automatisch aligniert, was bedeutet, dass sie fehlerhaft sein können. Daher schaffen wir mit unserem neuen Corpus DEPLAIN, das in zwei Subcorpora DEPLAIN-apa und DEPLAIN-web unterteilt ist. DEPLAIN-apa basiert auf Newsartikel und enthält 483 manuell alignierte Dokumente, was zu etwa 13.000 parallel alignierten Satspaiß führt. DEPLAIN-web enthält hingegen unterschiedliche Domänen und 750 Dokumente, die manuell und automatisch aligniert wurden. Insgesamt erhalten wir 30.450 Satspaiß. Wir haben unsere Satspaiß ein bisschen weiter analysiert, z.B. auf die Art der Simplifizierung. Wie Sie sehen können, sind Bibeltexte stark simplifizierter als z.B. Newsartikel oder Lernermaterialien. Auf allen Ebenen, einschließlich lexikalischer Simplifizierung, Struktursimplifizierung und allgemeiner Simplifizierung. Wir können auch sehen, dass unser Corpus DEPLAIN eine hohe Vielfalt an verschiedenen Simplifizierungsoperationen hat. So zum Beispiel im DEPLAIN-apa Corpus haben wir viel mehr Umordnungen und Worterfüllungen als im DEPLAIN-web Corpus. Auf der anderen Seite haben wir im Web Corpus viel mehr Umformungen. Nun sehen wir an, was wir damit erreichen können. Hallo, ich bin Omar und nun werde ich über die Anwendungsmöglichkeiten unseres Datensatzes DEPLAIN sprechen. Der erste Anwendungsbereich, den wir betrachten, ist die Evaluation von automatischen Alignmentsmethoden. In den letzten Jahren wurden viele Alignmentsmethoden vorgeschlagen, aber in Kontexten, in denen zwei parallel geschriebene Dokumente in verschiedenen Sprachen existieren und wir die Alignments von Sätzen in beiden Dokumenten extrahieren möchten. In unserem Fall versuchen wir jedoch die Alignments von Sätzen in zwei parallel geschriebenen Dokumenten mit gleicher Sprache und gleicher Bedeutung zu extrahieren, die auf verschiedenen Komplexitätsstufen liegen. Und nun, da wir unser Corpus DEPLAIN haben, das manuell alignierte Sätze enthält, können wir diese als Gold Standard Alignments verwenden, um einige der vorgeschlagenen Alignmentsmethoden zu evaluieren. Wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und alle Anpassungen und die Codes, um unsere Experimente auszuführen, im Papier publiziert. Am Ende conclude wir, dass die beste automatische Alignmentsmethode für deutsche Textsimplifizierung die Methode von MASSalign ist. Und Sie können auch den Code, um diese Methode auf Ihren eigenen Dokumenten zu verwenden, im Papier finden. Der zweite Anwendungsbereich, den wir im Papier gezeigt haben, ist die automatische Textsimplifizierung durch das Fine-Tuning von Sprachmodellen, um eine vereinfachte Texte aus komplexen Eingabetexten zu produzieren. Wir haben zwei Modelle fine-tuned: wir haben das Modell von long-mBART zu document-level-Simplifizierungen fine-tuned und auch das Standard-basierte mBART zu sentence-level-Simplifizierungen fine-tuned. Sie können auch alle Checkpoints und weitere Details zu den Punkten und Evaluationsmetriken our Experiments im Papier finden. Wir conclude, dass diese Fine-Tuning-Methode Punkte besser als die Baseline-Punkte produzieren kann und wir diese Resultate als Grundstandard für die Zukunft des Problems der automatischen Textsimplifizierung vorschlagen. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während des Conferences zu sehen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">The T5 XL model was used to achieve an accuracy of 82-87%.</sample>
    <sample id="6">Jiaan präsentiert "Towards Unifying Multi-Lingual and Cross-Lingual Summarization", eine gemeinsamearbeit mit Fandong, Duo, Yunlong, Zhixu, Jianfeng und Jie. Sie haben einen neuen Ansatz vorgestellt, der multilinguale und korsprachige Summarisierungen vereint, indem sie eine allgemeinere Umgebung namens "many-to-many summarization" erstellen. Das Hauptziel dieser Arbeit ist es, ein einziges Modell zu erstellen, das in jeder Sprache einen Dokumenteinsatz und dessen zusammenfassende Ausgabe in jeder Sprache generieren kann. Sie haben auch einige vorläufige Studien durchgeführt, um tiefer in die Unterschiede zwischen multilingueler Summarisierung, korsprachiger Summarisierung und their many-to-many summarization zu eintauchen. Sie haben festgestellt, dass many-to-many summarization helfen kann, das Modell besser über Aufgabenerkenntnis hinwegzuleiten, als multilinguelle Summarisierung und korsprachige Summarisierung zuvor. Sie haben auch PISCES vorgestellt, ein vor trainiertes many-to-many summarization Modell, das durch einen sorgfältig entworfenen drei-stufigen Vorertraining lernen kann.</sample>
    <sample id="7">Ja, sie Funktionieren immer noch.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertsmethode, ABC-Eval, ist neu, weil sie das Ausdrücken bestimmter Verhaltensweisen in den Modellantworten annotiert, um eine präzisere und zuverlässigere Beurteilung des Dialog qualitätsbewusst zu machen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwartenen Ansatzes hängt von mehreren Faktoren ab, darunter die Anzahl der sauber validierten Datensamples, die für die ModellSelektion verwendet werden, und die Anzahl der Datensamples, die für die direkte Optimierung verwendet werden.</sample>
    <sample id="10">Das Hauptproblem, das das aktuelle Modell noch verbessert werden kann, ist die begrenzte Kenntnis der entity-Attribute. Wenn das Modell nur die Namen der entities hat, erreicht es nur einenAccuracy von 60%. Um die Performance zu verbessern, sollte das Modell besser auf die entity-Attribute und Kenntnisse zugreifen können, um eine bessere Disambiguierung und Verstehensfähigkeit zu erreichen.</sample>
    <sample id="11">In his presentation, Jack Hessel discusses the capabilities of large language models in understanding and generating humor. He highlights that while some language models can generate jokes and even explain them, they often fail to understand the humor behind certain jokes, such as a knock-knock joke involving a pineapple. Hessel and his team have developed a dataset based on The New Yorker Caption Contest, which includes tasks like matching, quality ranking, and explanation generation. They found that their best model achieved around 62% accuracy in the matching task, compared to a 20% random-guessing baseline, but still falls short of human performance. They also tested GPT-4's joke explanations and found that human explanations were preferred in more than two-thirds of cases. Hessel encourages others to use their dataset and leaderboard to further explore these topics.</sample>
    <sample id="12">There are five authors involved in the work presented by Dawei.</sample>
    <sample id="13">Hello everyone. My name is Daniel Rotem, and I'm here to present my work, "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings," which was done in Professor Roy Schwartz's lab at the Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models. We use it because real-world data varies in complexity. So we can use low-capacity models for easy samples, which helps reduce average inference costs - whether it be time or money. The two most common adaptive inference methods are Multi Model and Early Exit. In Multi Model, multiple models are stored together, each fit with a classifier at the end. They're trained separately on the entire training set, and when used for inference, they're run sequentially until a classifier decides to halt the computation. For Early Exit, multiple classifiers are fit to the model following intermediate transformer layers. They're all trained together, and for inference, a sample is run through the model until a classifier decides to halt, saving computation that would have been exhausted by the rest of the model. Let's look at the pros and cons of each method. For Multi Model, it's more versatile. It's easily extended. It's expensive to store. And it also suffers from overhead because using the last model for inference means we ran a sample through all previous models without using their output. For Early Exit, we have faster inference, no overhead. It's memory efficient. But model parameters are shared amongst all classifiers, leading to lower performance. We hypothesize that this leads to a phenomenon we call conflicting gradients. Each classifier updates model weights trying to optimize its own goal. Gradient signals from different classifiers may interfere with each other, degrading performance for all classifiers involved. Here's an illustration: Classifier M and its loss function update backpropagate through the entire model and update the first layer. So does Loss function 1 and Loss function 2. All these updates do different things to the weights of Layer 1, potentially harming performance for all classifiers in the model. To test our hypothesis, we compared individual Early Exit models' classifiers with separate Multi Model classifiers, which are truncated versions of the BERT pre-trained language model. Training the Multi Model separate classifiers doesn't suffer from conflicting gradients. As you can see, the Multi Model classifiers outperformed those of Early Exit by an average of 2.3%, both for BERT-base and BERT-large. Moreover, the gap is largest for the earliest classifiers, 5.2% on average. We also measured the speed/accuracy trade-off of the models. Notice that for high inference speeds, Multi Model is much better. However, when we use later classifiers to predict, we see that Early Exit outperforms Multi Model because of the overhead suffered by Multi Model predicting with its largest classifiers. Based on these findings, we present SWEET: Separating Weights in Early Exit Transformers. It's a novel fine-tuning method for Early Exit architectures. What we do is we train an Early Exit architecture where each layer receives updates only from the following classifier. That means we avoid the conflicting gradient problem completely. Here's an illustration: On the left, you see the standard Early Exit model. On the right is our method, SWEET. You can see that each transformer layer only receives updates from its following classifier's loss function, avoiding the conflicting gradient problem. Let's look at the results of the SWEET method. When evaluating the individual layers, we can see that SWEET closes most of the gap between Early Exit and Multi Model. However, in some cases, later classifiers are negatively affected by our method, as you can see by the negative gap between SWEET and Early Exit. We also ran the test and examined the speed/accuracy trade-off. We can see here that in fast speeds, SWEET outperforms both methods. Whereas for BERT-Large, it outperforms both methods throughout the entire speed/accuracy curve. The takeaways from our work are as follows: We show the existence of conflicting gradients in Early Exit training process. To the best of our knowledge, we conduct the first fair comparison of Early Exit and Multi Model adaptive inference methods. We also introduced the SWEET method, the results of which motivate future research and fine-tuning algorithms tailored to the Early Exit architecture. Thank you very much for listening. If you enjoyed the talk and want to see more, go visit our paper on Archive, "Finding the SWEET spot."</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und heute sprechen wir über die Abhängigkeitsstruktur der Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in den universal dependencies wird die Struktur der Koordination "Lisa, Bart, Maggie" so angenommen, dass das erste Conjunkt die Hauptstelle im gesamten Koordinatenstrukturspielt. In diesem Fall ist es Lisa. Ein ähnlicher Ansatz wird von Igor Mel'čuk's Meaning Text Theory angenommen, bei dem das gesamte Koordinatenstrukturspiegel das erste Conjunkt als Hauptstelle anerkannt wird. Beide Ansätze sind asymmetrisch, da sie einen der Conjunktoren auswerten. Nun, es gibt auch einen symmetrischen Ansatz, wie er zum Beispiel in der Prager Abhängigkeitsbaumansatz angenommen wird, bei dem Koordinatenstrukturspiegel vom Konjunktionswort als Hauptstelle anerkannt wird. Wir erhalten dann Abhängigkeiten von Ende zu allen Conjunktoren. Schließlich gibt es auch einen mehrfach Hauptstelle-Approach, wie er zum Beispiel in Hudson's Word Grammar angenommen wird, bei dem alle Conjunktoren als Hauptstelle anerkannt werden. Wir erhalten dann Abhängigkeiten vomGovernor. Hier sieht man "Liebe" zu allen Conjunktoren separat: Lisa, Bart und Maggie. Nun ist das Ziel dieser Aufsatzes, einen neuen Argument gegen asymmetrische Koordinationsstrukturspiegel wie diese zwei und für symmetrische Koordinationsstrukturspiegel wie diese zwei zu produzieren. Das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das ich anhand dieser Beispiele erklären werde. In englisch weißt du, dass direkte Objekte bevorzugt in der Nähe vom Verb liegen, während Adjektive weit weg stehen können. Also "Marge liest es gestern" ist in Ordnung, weil das direkte Objekt in der Nähe vom Verb liegt, während "Marge liest gestern es" viel schlimmer ist. Denn hier zwischen dem Verb und dem direchten Objekt steht ein Adjunkt: "gestern". Allerdings kann dieser Effekt durch das Vermeiden des direchten Objekts verbessert werden, wenn es sehr schwer und lang ist. Denn dann kann es nach dem Adjunkt platziert werden. Dies wird hier illustriert. So sind beide Sätze in Ordnung. "Marge liest dies absolut faszinierendes Buch über Bienen gestern." Es ist in Ordnung, anstatt "es" zu verwenden, ein sehr langes NP. Aber es ist auch in Ordnung zu sagen, "Marge liest gestern dieses absolut faszinierendes Buch über Bienen." So der Grund hierfür ist, dass dies möglich ist, weil obwohl diese Satz gegen das allgemeine grammatische Prinzip verstößt, das direkte Objekt neben dem Verb liegen sollte, es die Prinzip der Abhängigkeitslängenminimierung erfüllt, das sagt, dass kürzere Abhängigkeiten bevorzugt werden. Also sind diese zwei Bäume nur die Längen der kritischen Abhängigkeiten, die nicht unter diesen zwei Strukturen konstant sind. Hier haben wir eine Abhängigkeit von "liest" zum Adjunkt mit einer Länge von 7 gemessen in Worten und von "liest" zu "Buch" einer Länge von 4, also insgesamt 11. Wenn wir diese zwei Bestandteile tauschen, wird die Summe dieser beiden Abhängigkeiten 6. Stattdessen von 11 6 ist viel kürzer. Das ist warum es klingt, als ob es gut klang. EsVioliert ein Prinzip, aber es erfüllt ein anderes. Okay, also was wir getan haben, wir haben verschiedene statistiken über Koordination aus der erweiterten Version des Penn Treebank extrahiert und siehe die Aufsatz "Warum würdest du nicht Universal Dependencies verwenden" und diese statistiken bestätigen die Beobachtungen, die vorher mehrmals gemacht wurden, dass links liegende Conjunktoren tendenziell kürzer sind. Also "Salz und Pfeffer" und nicht "Pfeffer und Salz", gemessen in Silben. Und auch die Beobachtung, die in Parsing gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Also wenn die Differenz zwischen den Längen der zwei Conjunktoren wächst, bevorzugt der kürzere Conjunkt links; stärker, richtig? So die Proportion ist größer, der links kürzere Conjunkt. Aber was neu in diesem Aufsatz ist, dass wir beobachtet haben, dass diese Tendenz nur auftritt, wenn der Governor links ist oder weg ist. Also der Governor ist links in diesem Beispiel "ich sah Bart und Lisa", also der Governor ist links. Es ist weg in dem zweiten Beispiel "Homer kam und sperrte". Hier haben wir eine Koordination von zwei Verben und es gibt keine externen Outsiders, Governor. In solchen Fällen bevorzugt der kürzere Conjunkt links; die meisten der größten Differenz zwischen den zwei Conjunktoren. Allerdings, wenn der Governor rechts ist, wie hier, "lacht" regiert die Koordination Ted und Ned, dieser Effekt verschwindet. Wir haben gezeigt, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen. Ich werde mich auf die rechte Spalte konzentrieren. Was wir hier sehen, ist, dass wenn der Governor links ist, die Tendenz, dass der kürzere Conjunkt links ist, sich stetig steigert, mit der absoluten Differenz in Wörtern, und dass das gleiche gilt, wenn es keinen Governor gibt, wie in der Koordination von Sätzen. Aber wenn der Governor rechts ist, dieser Effekt verschwindet. Und wir zeigen im Aufsatz, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte messen, wie wir durch Messen der Längen in Zeichen, die erstes Spalte, in Silben die mittlere Spalte,</sample>
    <sample id="15">There are three authors involved in the work: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">Die Domänen, die stärker vereinfacht werden, sind die Bibeltexte.</sample>
    <sample id="17">This paper presents a multimodal relation extraction method that addresses the challenges of internal-information over-utilization and external-information under-exploitation. The proposed method uses a Graph Information Bottleneck principle-guided feature refinement to screen and adjust cross-modal graph structures, and incorporates multimodal topic information as additional semantic supplementary to enrich the overall context. Experiments on a widely used MRE dataset show that the proposed method achieves significant improvements over existing best models.</sample>
    <sample id="18">Ein Beispiel für die Präferenz für kürzere linke Konjunktionen lautet "Marge read this absolutely fascinating book about bees yesterday." Hier wird das Buch, das kürzer ist, nach dem Adjunkt "yesterday" platziiert, um die Abhängigkeitslänge zu minimieren und eine natürlichere Struktur zu erhalten.</sample>
    <sample id="19">The speaker, Zhang Qin from Shenzhen University, presents their work on efficient open-domain question answering at ACL 2023. They discuss the challenges of open-domain question answering, such as large Wikipedia corpus size and high index file size, and propose techniques to achieve efficient systems with smaller memory costs, faster inference, and comparable performance. The presentation covers one-stage frameworks, approximate nearest neighbor search, skip reading, document filtering, embedding compression, and model size reduction. The speaker also compares existing models and concludes that retrieval-only systems are suitable for resource-constrained devices, while retrieval and reader systems offer trade-offs between speed, memory, and performance. Future works include deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, you can use the models for your research. The pre-trained models obtained from NACHOS are freely available on Hugging Face and under the MIT license. All the training scripts are also available on their GitHub repository.</sample>
    <sample id="21">DEplain-apa enthält Dokumente aus dem Bereich der News.</sample>
    <sample id="22">Eine gute Generalisierung wird durch die Modelle-Architektur, die Modellgröße und mehr Fine-Tuning-Beispiele erreicht.</sample>
    <sample id="23">Dan Garrette's team has been working on improving text image models to generate high-quality images with accurate text representation. They analyzed the Imagen model, which uses a T5-XXL encoder and a diffusion model to generate images from text inputs. However, they found that even simple textual inputs often fail to produce accurate text outputs. The team investigated the text encoders and discovered that T5 struggles with spelling due to its subword tokenization, while PaLM models have better spelling accuracy but are impractical for many applications. ByT5, which receives individual bytes of input, performs well at spelling. To improve text rendering models, the team concatenated an additional text representation from the ByT5-small model to the existing text representation in the Imagen model, resulting in improved image generation characteristics and text rendering abilities.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde durch die Messung der Längsmeßwerte in Worten, Silben und Zeichen bestimmt.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass sie die Auswirkungen der Position des Begrenzers auf die Längstenspannungen in Koordinationen untersuchten. Sie extraktierten verschiedene statistiken aus dem erweiterten Penn Treebank und bestätigungsvollten Beobachtungen, wonach links liegende Conjunktoren normalerweise kürzer sind. Sie messten die Längstenspannungen in verschiedenen Einheiten (Zeichen, Silben, Wörter) und beobachteten, wie sich die Tendenz zu einem kürzeren linken Conjunktor mit wachsendem Längstensdifferenzial zwischen den Conjunktoren verhält. Sie fanden heraus, dass die Tendenz, einem linken Conjunktor ein kürzeres Konjunktiv zuzuordnen, sich nur einstelle, wenn der Begreber links steht oder abwesend ist.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, kann nicht viel besser als Zufall performieren. In dem Papier wird erwähnt, dass die ursprüngliche Klassifizierung nur mit 43 Beispielen trainiert wurde und nicht viel besser als Zufall performiert hat.</sample>
    <sample id="27">Der Inhalt der Präsentation gibt nicht die Anzahl der Autoren an, die an der Arbeit beteiligt sind.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagrale Modelle bei Diskursphänomenen wie Formalität und lexikalischer Kohärenz.</sample>
    <sample id="30">The paper introduces LLM-Blender, a simple yet effective ensemble learning framework for large language models. The key idea is based on pairwise ranking and generative fusion. The authors found that the optimal selection of models can vary across different input examples, so they propose using more large language models for each input to select and generate better output than using any single model for all inputs. They propose a two-stage framework named LLM-Blender, which consists of a PairRanker module and a sequence-to-sequence model for generating the final output. The PairRanker module compares pairs of candidates and ranks them, while the sequence-to-sequence model generates the final output from the top-ranked candidates. The authors also create a new dataset named MixInstruct for evaluating ensemble learning frameworks. Experiments show that LLM-Blender outperforms other methods on various correlation metrics, suggesting that it is a promising framework for ensemble learning.</sample>
    <sample id="31">I'm sorry, I cannot answer this question as the information about the university affiliations of the authors is not provided in the given text.</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität, indem es dieAnnotations von diversen Annotatoren mit den Modellen und Datensätzen vergleicht, indem es Pearson's R-Korrelations得分 verwendet.</sample>
    <sample id="34">Marcos Treviso presents a work called "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation" in which he collaborates with Alexis Ross, Nuno Guerreiro, and André Martins. The work proposes a joint framework for rationalization and counterfactual text generation that combines selective rationalization and counterfactual generation to leverage their complementary strengths. The framework generates counterfactuals by masking specific parts of the input and generating new tokens using an editor. The quality of the counterfactuals produced by CREST is evaluated using both automatic metrics and human evaluation. The results show that CREST counterfactuals are more valid and natural than those produced by MiCE and other automatic approaches. The framework also proposes an alternative way that performs rationalization with both factual and counterfactual examples, which achieves top results on IMDB itself and performs on par with data augmentation using human counterfactuals in contrastive datasets. Overall, CREST produces plausible explanations that focus on the contrasting parts of the input, making it a valuable tool for improving downstream models.</sample>
    <sample id="36">The paper "Learning Language-Specific Layers for Multilingual Machine Translation" by Telmo Pessoa Pires and colleagues proposes a method to increase the capacity per language in multilingual machine translation models while keeping inference costs constant. The authors introduce Language-Specific Layers (LSLs), which are one regular transformer layer per language used to select and train at inference time the correct sublayer, either the source or target language. They focus on placing LSLs in the encoder and use a model with shared, source, and target weights to learn the best placement. The results show significant improvements over baseline models and language adapters, especially for low-resource languages.</sample>
    <sample id="37">Die vorherige Studie fand, dass die menschlichen Teilnehmer bei den Persona-Prompts Racialstereotypen surfen konnten.</sample>
    <sample id="38">In dieser Studie wurden Daten aus dem enhanced version of the Penn Treebank und dem Paper "Why wouldn't you use universal dependencies" verwendet.</sample>
    <sample id="39">Es wird in der Rede erwähnt, dass Adam Przepiórkowski die Arbeit betreibt. Es wird nicht erwähnt, ob es noch andere Autoren gibt, die an der Arbeit beteiligt sind.</sample>
    <sample id="40">Die eng verwandten Aufgaben für kognitive Dissonanz sind die diskussionsunabhängige Stimmhaltungsklassifizierung (Debatte) und die binäre Klassifizierung von Ausbau- und Vergleichsklassen (CE).</sample>
    <sample id="41">The research paper "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" by Silin from EPFL University, in collaboration with Sony Group Corporation, introduces a Persona-grounded Commonsense Knowledge Graph (PeaCoK) to represent real-world personas at scale. PeaCoK contains about 3,800 personas and 40,000 distinctive attributes, forming about 100,000 personal inferences or facts. The graph is built in three steps: selecting personas from existing commonsense graphs, inducing attributes of personas from both commonsense knowledge graphs and large-scale pre-trained language models, and crowdsourcing the annotations of PeaCoK relations using a joint human-AI majority voting scheme.

The paper also explores whether PeaCoK can help language models learn and generalize persona knowledge. It uses PeaCoK to train a BART-based common knowledge generator on a persona attribute inference task, achieving overall better automatic evaluation results compared to large-scale pre-trained language models. Additionally, the paper investigates the use of PeaCoK knowledge to improve downstream narrative modeling in a persona-grounded dialogue generation task on the ConvAI2 PersonaChat data set. The results show that PeaCoK augmented models achieve better dialogue generation on various aspects, including fluency, consistency, engagement, and persona expression.</sample>
    <sample id="42">Es wird in der Präsentation erwähnt, dass Shuheng die Arbeit präsentiert, aber es wird nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="43">Die Anzahl der beteiligten Autoren wird nicht in dem gegebenen Text erwähnt.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es nicht nur die Annäherung an die Annotatoren diskutiert, sondern auch die Positionalität von Modellen und Datensätzen in Bezug auf diverse Gruppen von Benutzern und Annotatoren vergleicht. Es verwendet Pearson's R-Korrelations得分来比较模型和数据集的预测和标签与多样化的注释。</sample>
    <sample id="45">Die generierten Persönlichkeiten haben die meisten Überschneidungen mit dem Lexikon der Stereotypen.</sample>
    <sample id="46">Die kommerziellen Systeme, die verglichen wurden, sind DeepL und Google Translate.</sample>
    <sample id="47">Hallo, ich bin Shangbin, ein Doktorand an der University of Washington. Heute präsentiere ich unser Werk "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models". Sprachmodelle werden auf großen Datensammlungen aus dem Web trainiert. Politische Medien werden in ihren Datensammlungen gut abgedeckt. Laut einer Umfrage des C4 Corpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in den Datensammlungen von Sprachmodellen gut abgedeckt sind. Dies hat für die Anwendung von Sprachmodellen ein doppeltes Schwergewicht geschaffen. Auf der einen Seite können sie von diversen Perspektiven lernen und damit dem Demokratie und der Pluralität von Ideen zusagen. Auf der anderen Seite sind diese verschiedenen politischen Meinungen von Natur aus sozial schräg und könnten potenzielle Fairnessprobleme in Anwendungen auf downstream Aufgaben aufwerfen. Um dies zu untersuchen, haben wir vorgeschlagen, die Verbreitung von politischen Schias von der vorherigen Datensammlung über die Sprachmodelle bis hin zu downstream Aufgaben zu untersuchen. Hierzu haben wir folgende Fragen gestellt: Wie lassen sich die politischen Neigungen von Sprachmodellen bewerten und welche Rolle spielen dabei die Datensammlungen im Vorausbild? Wie performieren Sprachmodelle mit unterschiedlichen politischen Neigungen auf downstream Aufgaben und ob dadurch Fairnessprobleme in NLP-Anwendungen entstehen könnten? Wir haben zuerst vorgeschlagen, Sprachmodelle mit unterschiedlichen prompt-Formaten unter Verwendung von politischen Umfragen wie dem Test für politische Konferenzen zu provozieren. Damit können wir automatischen Auswertungen fundiert in der politischen Wissenschaft sitzen lassen. Einige vorläufigen Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie nehmen alle vier Ecken auf dem politischen Campus ein. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell ist und die GPT-Serie allgemein mehr sozial liberale als BART-Serie und ihre Varianten ist. Wir wollen nun untersuchen, in welchem Maß die politischen Schias von Sprachmodellen von den Datensammlungen abhängig sind. Dazu können wir ein kontrolliertes Experiment durchführen, indem wir Sprachmodell-Schritte auf 6 verschiedenen parteilichen Datensammlungen forttrainieren, die getrennt in News und Social Media, weitere getrennt in ihre politischen Neigungen unterteilt sind. Wenn wir Sprachmodelle auf solche parteilichen Datensammlungen weitertrainieren, können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben werden. Zum Beispiel können wir sehen, dass bei RoBERTa nach der Forttraining auf der linken-leaning Reddit-Datensammlung eine substantielle linkslibere Verschiebung in Bezug auf seine politischen Schias festgestellt werden kann. Wir wollen auch untersuchen, ob Sprachmodelle die Polarisation in unserer modernen Gesellschaft auffangen können. Wir unterteilen die Datensammlungen in vor und nach dem 45. Präsidenten der Vereinigten Staaten. Wir separate Forttraining von Sprachmodellen auf die zwei verschiedenen zeitlichen Datensammlungen. Wir können sehen, dass Sprachmodelle allgemein eine politische Neigung haben, die weiter weg vom Zentrum nach 2017 steht. Das zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft auffangen können. Wir evaluieren dann Sprachmodelle mit unterschiedlichen politischen Neigungen auf Hate Speech-Detection und Fake News-Detection, auf Anwendungen, die oft Sprachmodelle verwenden und sehr signifikante Implikationen haben könnten. Wenn wir die pro Kategorienerfolge untersuchen, also wenn wir die Leistung in Abhängigkeit von den demografischen oder politischen Neigungen der Medienmediums trennen, können wir ein Muster feststellen. Zum Beispiel beim Hate Speech-Detection sind linkslibere Sprachmodelle besser darin, Hate Speech gegen ethnisch minderstädtige Gruppen zu detektieren, aber schlechter darin, Hate Speech gegen stärkere Gruppen in unserer Gesellschaft zu detektieren. Und umgekehrt: rechtslibere Sprachmodelle sind besser darin, Hate Speech gegen Weiße und Männer zu detektieren, aber schlechter darin, Hate Speech gegen schwarze LGBTQ+ und andere minderstädtige Gruppen zu detektieren. Ähnliche Trends finden wir beim Fake News-Detection, wo wir sehen, dass linkslibere Sprachmodelle besser darin sind, Lügennachrichten von ihrer gegenüberliegenden politischen Neigung zu detektieren, und umgekehrt. Wir zeigen auch viele qualitative Beispiele an, um zu zeigen, dass Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorhersagen für Hate Speech- und Lügennachrichtenbeispiele basierend auf ihren sozialen Kategorien geben. Es gibt noch mehr Beispiele in der Anhang, um zu verdeutlichen, dass es ein Fairnessproblem mit den politischen Schias von Sprachmodellen gibt. Zum Beispiel, wenn rechtslibere Sprachmodelle auf Hate Speech oder Lügennachrichten usw. fine-tuned werden und auf einem populären Social-Media-Plattform eingesetzt werden, bedarf das zu Marginalisierung von Menschen mit gegenüberliegender politischer Meinung und zum Auslaufen von Hate Speech gegen ethnisch minderstädtige Gruppen, ohne Kontrolle. Das sollte uns auffordern, die Fairnessprobleme, die durch die politischen Schias von Sprachmodellen entstehen, anerkennen und bewältigen. Wir wollen auch betonen, dass wir das einzigartige Dilemma der politischen Schias von Sprachmodellen hervorgerufen haben. Es ist wie zwischen Scylla und Charybdis. Wenn wir keine politischen Meinungen in die Datensammlungen beim Training von Sprachmodellen reinziehen, propagieren die Schias von Datensammlung über Sprachmodelle bis hin zu downstream Aufgaben, was letztendlich Fairnessprobleme entstehen lässt. Wenn wir versuchen, die Schias zu neutralisieren, riskieren wir Censorship oder Exklusion. Es ist sehr schwierig zu bestimmen, was als neutral angesehen werden sollte und in welchen Datensammlungen behalten werden sollte. Es ist, wie das Elektrolokomotive-Problem. Okay, das war’s. Ich denke, das war alles, was ich heute haben wollte. Danke für eure Zeit.</sample>
    <sample id="48">Es wird in der vorgelegten Übersicht erwähnt, dass es eine gemeinsamearbeit mit mehreren Kollegen von Google Translate ist. Es wird jedoch nicht angegeben, wie viele Autoren insgesamt an der Arbeit beteiligt sind.</sample>
    <sample id="49">MPP-Auswertungen wurden bis zu 1024 Token Kontextlänger durchgeführt.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text identification on the document and sentence level. Text simplification is defined as adapting a text to improve comprehension for target groups such as people with reading problems or non-native speakers. The presentation highlights the need for parallel pairs of texts to train a text simplification model. DEPLAIN addresses issues with existing corpora by providing two subcorpora: DEPLAIN-apa (483 manually aligned documents) and DEPLAIN-web (750 documents aligned both manually and automatically). The corpus includes various simplification transformations and has been analyzed for type of simplification. The presentation also discusses use cases for the data set, including evaluating automatic alignment methods and fine-tuning language models for automatic text simplification. The best automatic alignment method for German text simplification is found to be MASSalign, and the basic fine-tuning of long-mBART and mBART models produces better scores than baseline scores.</sample>
    <sample id="51">Sie haben in ihren Datensatz drei verschiedene Domains aufgenommen: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen als Folge ihrer Demografie, Identität und Lebenserfahrungen halten. Dieser Begriff wird in kritischen Studien, insbesondere in feministischen und queergerechten akademischen Bereichen verwendet.</sample>
    <sample id="53">The presenter of the video is Dawei.</sample>
    <sample id="54">Title: Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Abstract: Cognitive dissonance, a phenomenon where two beliefs or actions are inconsistent, is a common experience in daily decision-making. Studying dissonance expressed in language can help understand disagreement among people, track trends and belief values, and attitude changes in populations. However, dissonance is rare in language, making it challenging to develop accurate detection methods.

We conducted a large-scale annotation of dissonance relations using a dissonance-first approach. Despite finding dissonance in only 3.5% of annotated pairs, we developed an initial classifier trained on 43 examples of dissonance. To improve performance, we experimented with combinations of transfer learning and active learning.

By transferring weights from closely related tasks, such as topic-independent dissonance stance classification and CE tasks, we achieved better zero-shot performance than chance. Further fine-tuning on both tasks improved performance even more. We found that the "Cumulative" strategy performed equal or better than "Iterative" across different strategies. Our proposed Probability-of-Rare-Class (PRC) strategy outperformed other state-of-the-art AL strategies in terms of dissonance classification AUC.

Our results demonstrate the feasibility of PRC for rare class acquisition and cold starting AL with appropriately designed transfer learning tasks. Iterative update is useful for transfer learning from a different domain, while cumulative update benefits domain-specific annotations.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell, da es die off-line ST-Modelle nutzt, ohne sie zu retrainieren oder spezifische Architekturen für SimulST zu verwenden.</sample>
    <sample id="56">The number of authors involved in the work is not specified in the given information.</sample>
    <sample id="57">The tested model in the KITMUS test suite is able to integrate knowledge from multiple sources when trained on task-specific data. However, even the best-performing models have difficulties with reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: 1) Background-Pretrain (wo die Hintergrundinformation bei der vorherigen Trainingphaseavailable ist), 2) Background-Both (wo sowohl Hintergrundinformation als auch spezifische Informationen zur Identität der entworfenen Entitäten während der Inferenzzeitavailable sind) und 3) Background-Inference (wo nur spezifische Informationen zur Identität der entworfenen Entitäten während der Inferenzzeitavailable sind).</sample>
    <sample id="59">Hi, I'm Yanis Labrak and I'll be presenting our work on "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains." We start by discussing language modeling in healthcare. Then we present the main contribution of our article, introducing DrBERT, the first biomedical model in French, based on RoBERTa and trained on NACHOS, a data set of medical web-crawled data. We compare models with different pre-training settings and data sources. Our results show DrBERT's performance on 11 biomedical and clinical tasks in French. The presentation concludes with experiment details and information on accessing the models. Since BERT's release in 2018, it has become highly effective for natural language processing tasks, outperforming historical methods like Word2vec and fastText. Specialized models have been adapted to various languages and domains, including French and English. However, specialized models for other languages are scarce, often relying on continual pre-training due to limited in-domain data. French lacked an open-source biomedical model until now. We explore the most appropriate data sources for diverse usage and compare DrBERT with ChuBERT, which uses anonymized hospital data. We investigate how much data is needed to train a specialized French model, comparing four from-scratch models and three models trained using continual pre-training. We evaluate these models on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. Our evaluation shows that models perform best when trained on data of the same nature. Heterogeneous data sources appear versatile, and more data generally leads to better performance. From-scratch pre-training typically yields higher performance across most tasks. However, our experiment using CamemBERT trained on the NACHOS subset showed comparable results to DrBERT 4 GB from-scratch. Models based on CamemBERT weights and tokenizer suffer stability issues. DrBERT outperformed all baseline models, including generic models like CamemBERT. More specialized data improves performance but doesn't scale well. All pre-trained models from NACHOS are available on Hugging Face under the MIT license, and training scripts can be found on our GitHub repository. Thank you for this presentation, and we look forward to exchanging ideas at the Toronto poster session.</sample>
    <sample id="60">The given information does not mention the specific university that the authors belong to.</sample>
    <sample id="61">The concluding research question is whether we should only use clean samples for validation or if there are better ways to utilize them.</sample>
    <sample id="62">The paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" by Nitay Calderon and colleagues explores the potential of compressing NLG models while preserving their performance. The authors investigate different knowledge distillation approaches, including word-level and sequence-level distillation, and propose a novel technique called joint-teaching to address student exposure bias and teach the student to correct its own mistakes. The study considers various NLG tasks in realistic setups and demonstrates the importance of unlabeled data and diverse pseudo-targets for effective knowledge transfer.</sample>
    <sample id="63">Die Sensitivitätsmetrik messt die Fähigkeit des Modells, für dieselbe Aufgabe immer die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">The referent in the video is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell für die gleiche Aufgabe unterschiedliche Ausgaben produziert, je nach Formulierung der Anweisung. Daher ist eine höhere Sensitivität ein Indiz für schlechtere Leistung des Modells.</sample>
    <sample id="66">This paper presents a survey on deep learning methods for mathematical reasoning, which is a fundamental aspect of human intelligence. The authors discuss the development of machines capable of solving math problems and proving theorems, and explore two primary categories: visual contexts (geometric problems) and tabular contexts. They also highlight the importance of automated theorem proving and the limitations of language models in performing precise mathematical reasoning. The paper concludes by discussing the need for more research in low-resource settings and the creation of mathematical reasoning benchmarks for various domains.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models and identifies the main factors that contribute to it. The authors find that severe interference occurs when the model is very small compared to the data size, and that tuning the sampling temperature is key for strong performance. They also conclude that language similarity and the number of languages do not have a large impact on interference levels. The simplest solution to controlling the trade-offs is temperature sampling, which allows for more training examples from lower-resource languages.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen Kontext, der aus den gleichen Quellen stammt, aus denen sie evaluiert werden.</sample>
    <sample id="69">Typischerweise werden normalerweise 20 saubere Validierungsbeispiele pro Klasse verwendet, um eine gute Leistung an der WSL zu erzielen.</sample>
    <sample id="70">Es ist nicht erwähnt, an welcher Universität die Autoren Myra, Esin Durmus und Dan Jurafsky arbeiten.</sample>
    <sample id="71">The AltEntities Corpus is a dataset created by Javad Hosseini and his team to address the challenge of resolving indirect referring expressions for entity selection in conversational systems. The dataset consists of 6,000 alternative questions across three domains: music, books, and recipes. Each question is accompanied by 42,000 indirect referring expressions. Annotators were provided with background knowledge about the entities, such as Google search results or Wikipedia articles, to help them select the correct entity using indirect references. The results show that the accuracy of language models improves when they have access to partially overlapping background knowledge, but there is still room for improvement. The AltEntities Corpus is domain-generalizable and can be used to benchmark LLMs' entity understanding.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Auswirkungen von politischen Verzerrungen auf Sprachmodellle und downstream-Anwendungen zu verstehen und zu bewältigen. Dadurch können wir sicherstellen, dass Sprachmodelle faire und neutrale Entscheidungen treffen und nicht von politischen Biases beeinflusst werden.</sample>
    <sample id="73">Akshatha</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph that completes missing links in ATOMIC. The authors propose a new CSKG completion method called Rel-CSKGC, which predicts relations given head and tail events using RoBERTa and MaxPooling. They also introduce an Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links. Dense-ATOMIC yields higher knowledge coverage and benefits the performance of COMET. The paper demonstrates the potential for commonsense reasoning with multi-hop paths.</sample>
    <sample id="75">Hello, my name is Zheng Yandan. Today, I am excited to present our work called Jointprop, a joint effort with my friend Hao Anran and my supervisor, Luu Anh Tuan. Our work focuses on the motivation behind name entity recognition (NER) and relation extraction (RE), which are crucial tasks in information extraction. Supervised learning schemes have made significant progress in NER and RE research by leveraging rich label data. However, fully-supervised models require extensive labor to obtain high-quality data annotation, and it requires diverse annotated data for various domains and applications. Semi-supervised learning employs a small amount of label data to obtain powerful models at a lower cost. Semi-supervised NER and RE models have performed very well in recent years. However, current studies neglect the underlying interconnections between NER and RE tasks. This could be an issue. For example, one can easily see the syntactical similarity between generative model and probabilistic model and their relation indicator "used to" and "use in". If such similarities are ignored, the model might thereby miss label alignment as an entity that shares the same type as dependency parsing. Moreover, it is necessary to consider the interconnections among labeled data, among unlabeled data, and between labeled and unlabeled data. For example, it can be hard to invert the correct pseudo-label from dependency parsing to alignment or NLI alignment. This is also the case to infer "apply" for "use to" and "use in". If we fully exploit all the connections, we are able to fully integrate all the information to infer the correct labels. So we propose a joint semi-supervised learning framework to model the NER and RE tasks by propagating labels over heterogeneous graphs, and perform label propagation across the graph, and consider the inter- and intra-connections among both labeled data and unlabeled data. Here comes the method parts. Our jointprop framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and finally, model optimization. For span feature generation, we suppose X-k is the contextualized representation of K-th input token, which initializes span and span pairs representation as H-e and H-r. From the label tokens, we will obtain a trained classifier on the base of the base model. Using the trained classifier, we generate the unlabeled span and span pair representations. For heterogeneous graph construction, we construct a k Nearest Neighbor graph for computational efficiency, and we examine the similarity relations among pairs of unlabeled data as well as the similarity relations between the labeled data in order to take advantage of the smoothest constraints among neighbouring unlabelled data in our semi-supervised joint entity and relation extraction task. The entity nodes and the relation nodes are automatically associated via their representation. For the joint label propagation, a conceptual demonstration of the label propagation process is shown here. Through the heterogeneous graph, our proposed joint semi-supervised learning method propagates labels to entity or relation candidates in the unlabeled data. Alternatively, as shown in the figure, the pseudo-label for entities or relations will be refined every time (t) until converged. Label propagation diffuses labels through the whole graph along high-density areas formed by the unlabeled data. Finally, we come to the model optimization. We obtain the converged pseudo-label, and we use the softmax function followed by a standard argmax operation to determine the pseudo-labels. We filter those of lower quality with a confidence (g) and combine the rest of the confidence above the threshold with the label data to retrain the classification model. The retraining model remains the same as the baseline model, as does the joint NER-RE classification function. Finally, the experiment part. We conducted our experiments on four datasets. There are joint-task and single-task datasets. There is no previous baseline on semi-supervised joint-task. Comparisons are only made with base model performance. As we can see in the experiment tables, the joint learning of two tasks benefits from the codependency between the two tasks in joint datasets. For single-task datasets, our framework shows significant and consistent improvement over all baselines, both for NER and relation tasks. Thank you very much for your attention.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile beginnt mit der Praemodusdaten und endet mit den downstream-Anwendungen. In diesem Prozess können Sprachmodelle, die auf politisch polarisierten Datensätzen trainiert wurden, politische Vorurteile übernehmen und in Anwendungen wie das Erkennen von Hasssprache und Falschinformation propagieren.</sample>
    <sample id="77">The video presents a joint work by Yale University and Microsoft Research on improving summarization factual consistency using natural language feedback. The researchers introduce a new dataset, DeFacto, which includes human demonstrations and feedback for enhancing summarization factual consistency. They propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. The study focuses on abstractive text summarization and evaluates the factual consistency of summarization models using the XSum dataset. Annotated data points show that human-edited summaries receive higher automatic factuality scores but have lower textual overlap with reference summaries. The dataset's fine-grained annotations are valuable for training factuality metrics and meta-evaluation. The DeFacto dataset is available on GitHub.</sample>
    <sample id="78">Ja, es unterscheidet sich. In DEPLAIN-apa gibt es mehr Umordnung und Worthinzufügungen, während in DEPLAIN-web mehr Umformungen stattfinden.</sample>
    <sample id="79">Yes, CoScript is publicly available.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text über einen Prozess namens "Watermark injection" eingebettet. Hierbei wird zuerst ein Trigger-Set, also eine Gruppe von Wörtern mit mittlerer Häufigkeit, ausgewählt. Wenn ein Benutzer eine Anfrage sendet, zählt der Anbieter die Anzahl der Triggers im Satz und berechnet den Gewichtssummen-Satz als Kombination des ursprünglichen Embeddings und des Ziels embeddings. Der Gewichtswert des Ziels embeddings variiert je nach Anzahl der Triggers im Satz. Wenn die Anzahl der Triggers größer als eine bestimmte Schranke (m) ist, dann ist das bereitete Embedding gleich dem Ziels embeddings.</sample>
    <sample id="81">The authors belong to Penn State University.</sample>
    <sample id="82">The video discusses a research paper titled "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring". The paper proposes a novel framework called ULRA for unsupervised automated essay scoring, which uses multiple heuristic quality signals as pseudo-ground truth to train a neural AES model. The framework consists of a heuristic essay ranking module and a Deep Pairwise Rank Aggregation Module. The paper demonstrates that ULRA outperforms all unsupervised baselines with a large improvement in both transductive and inductive settings.</sample>
    <sample id="83">Ja,Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">The paper "PAD-Net: An Efficient Framework for Dynamic Networks" by Shwai He and colleagues presents a new approach to dynamic networks, which can change their architecture or parameters based on the input. The authors argue that fully dynamic networks are often redundant and require more parameters than necessary, leading to increased model size and computational costs. To address this issue, they propose a partially dynamic network (PAD-Net) framework that partitions parameters into dynamic and static modes, using iterative mode partition and scale factors to control the intensity of each mode. The authors demonstrate that PAD-Net achieves better performance than static and fully dynamic networks while maintaining fewer parameters and less computation. They also conduct ablation studies to determine optimal dynamic ratios for different components of the network and compare their method with network pruning, showing significant improvements in performance. Future work includes extending the method to other mainstream networks, hardware-friendly structured manners, and exploring combinations of zero elements, static, and dynamic parameters.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist die Erstellung eines Schokoladenkuchens. Hierbei müssen bestimmte Bedingungen beachtet werden, wie z.B. die Zutaten, die Zubereitungszeit und die Temperaturen, um den Kuchen korrekt zu machen.</sample>
    <sample id="86">Sie stellen die Opazität sicher, indem sie das visuelle Aussehen der embeddings von Sätzen auf den Datensätzen PCA visualisieren und zeigen, dass es schwierig ist, zwischen den embeddings mit Backdoor und normalen embeddings zu unterscheiden.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs (Pre-Trained Language Models) wie CamemBERT, PubMedBERT, BioBERT und ClinicalBERT, um ein neues PLM aufzubauen. Sie trainieren neue Modelle auf den gleichen Datensätzen wie die bestehenden Modelle und vergleichen dann die Leistung der neuen Modelle mit den besten Modellen in den Baseline-Modellen. Dadurch können sie feststellen, welche Modelle am besten auf die spezifischen Aufgaben angewendet werden und welche Datensätze am besten verwendet werden sollten.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf Afrika ausgerichtet.</sample>
    <sample id="89">Der Beispielsatz, der zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, ist der "cross-attention mechanism". Dieser Mechanismus hilft dem Modell, zu entscheiden, ob ein Wort emittiert werden soll oder nicht, basierend auf der Stabilität der empfangenen Informationen.</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" by Haneul Yoo and colleagues explores the feasibility of using language learners as annotators in NLP. The study conducted a proof-of-concept experiment with three languages (English, Korean, and Indonesian) and four tasks from the GLUE benchmark. Learners were categorized into three levels based on their language proficiency, and their annotations were compared to those of native speakers. The results showed that language learner annotations were nearly accurate, especially for simpler tasks and easy-to-medium level questions. Aggregating labels from multiple learners also brought the performance on par with native speakers. Additionally, the study observed improvements in language learners' language proficiency and vocabulary as they carried out annotation tasks. This work suggests a novel way for data construction in low-resource languages by recruiting language learners as annotators, potentially broadening NLP research for many languages.</sample>
    <sample id="91">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells. Je mehr Aufgaben verwendet werden, desto besser perforiert das Modell und desto niedriger ist die Sensibilität.</sample>
    <sample id="92">Die dreibaumlose Baselines, mit denen die Autoren ihre Methode vergleichen, sind:

1. Seq2seq-Model
2. Transformer-Model
3. Pointer-Generator-Model</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, stehen Matthias Lindemann als seinen Berater in Beziehung.</sample>
    <sample id="94">Jingwei Yi from the University of Science and Technology of China is presenting a paper on protecting the copyright of large language models for embedding as services via backdoor watermark. The paper introduces the concept of embedding as services, where large language models like GPT, LLAMA, and PALM are used to assist various NLP tasks. However, recent works have shown that attackers can steal the model through learning from the embedding and provide similar services. To protect the copyright of embedding as services, the paper proposes Embedding Marker, a backdoor-based watermark method applicable to embedding as services. The method consists of two main steps: watermark injection and copyright verification. The watermark injection step involves defining a target embedding and weighting it based on the number of triggers in the sentence. The copyright verification step involves constructing a back door and benign data set, requesting embeddings from the stealer's service with the data set, and computing similarity differences between the requested embedding and the target embedding. The paper conducts experiments on four data sets and shows that Embedding Marker can have great detection performance while keeping great utility for downstream tasks.</sample>
    <sample id="95">Der englische Inhalt des Vortrags enthält keine Information über den ersten Autor von PaLM. Es wird lediglich erwähnt, dass PaLM ein 540 Milliarden-Parameter-Langmodell ist, das im Jahr 2022 vorgestellt wurde und bei mehreren NLP Aufgaben die führende Leistung erreicht hat.</sample>
    <sample id="96">Hallo alle. Ich bin Jenny, ein erstes Jahr im PhD-Staat an Carnegie Mellon University und heute präsentiere ich euer Werk "NLPositionality: Identifizierung von Designbiases in Datensätzen und Modellen". Dieses Werk wurde in Zusammenarbeit mit KollegInnen aus der University of Washington und dem Allen Institute for AI erstellt, zuallermeisten Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap. Also lasst uns damit beginnen, uns vorzustellen. Stellt euch vor, ihr seid Journalisten und müsst die Kommentare zu einem Newsartikel überprüfen, um giftige Inhalte zu entfernen. Ihr könnt euch dabei auf eine beliebte API wie den Prospective API verlassen, der sehr gut arbeitet, wenn es Carl Jones geht. Aber das ist nicht der Fall bei Aditya Sharma, bei dem der Prospective API nicht so empfindlich gegenüber giftigen Begriffen ist, die in indischen Kontexten häufig vorkommen. Das zeigt einen Design Bias, einer systematischen Leistungsunterschied zwischen verschiedenen Bevölkerungen. Design Biases können entstehen, aufgrund der Positionalität der NLP-ForscherInnen und ModellentwicklerInnen. Positionalität ist einfach die Perspektive, die Menschen aufgrund ihrer Demografie, Identität und Lebenserfahrungen halten. Dieser Begriff wird in kritischen Studien, insbesondere in feministischen und queerbzw. akademischen Bereichen verwendet. Als ForscherInnen können Positionalitäten die Forschungsprozess und dessen Ausgänge beeinflussen, indem sie Entscheidungen beeinflussen. Eine Frage, die man sich stellt, ist, ob Datensätze und Modelle Positionalität haben. Wir wollen nicht sagen, dass Modelle oder Datensätze selbst Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von realen Menschen und können bestimmte Positionalitäten über andere vertreten. Vorheriges Werk hat einige anecdotal Beweise für die Existenz von Positionalität in Modellen und Datensätzen dargelegt, sowie theoretische Definitionen von Modell-Positionalität. Allerdings haben diese Arbeiten nicht die Vergleich der Endbenutzer mit Datensätzen und Modellen durchgeführt und die Studie von Modell- und Datensatz-Positionalität ist immer noch wichtig, da NLP-Aufgaben immer mehr subjektiv und sozial orientiert werden und es schwierig ist zu bestimmen, wie diese Positionalitäten abgeprallt sind, da nicht alle Entscheidungen notwendig sind und viele Modelle hinter APIs verborgen sind. Um die Datensatz- und Modell-Positionalität zu untersuchen, vergleichen wir dieAnnotations mit realen Benutzern mit bestehenden Datensätzen und Modellen. Wir tun dies durch unser Werk NLPositionality. Das Werk arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren zu re-annotieren. Wir sollten dabei achtgeben, die Demographen der ursprünglichen Datensatz-annotatoren zu ignorieren, denn normalerweise werden nur wenige Annotatoren pro Instanz und Demographen werden normalerweise nicht erfasst und geteilt. Daher warten wir darauf, Datensätze mit vielen Annotatoren zu re-annotieren, um eine reiche Datensammlung zu erhalten. Wir nehmen dann dieAnnotations von den Demographen und vergleichen sie mit den Modellen und Datensätzen unter Verwendung einer Pearson's R-Korrelationsnote. Daher unterscheidet sich unser Werk von der Literatur zur Annotator-Disagreement, indem wir Endbenutzer mit Modellen und Datensätzen vergleichen,Predictions und Labels, anstatt nur Annotator-Übereinstimmungen oder Modellierungen von Annotatordistributionen zu betrachten. Unser Werk ist hauptsächlich durch Lab in the Wild und Online-Crowdsourcing-Plattformen fürHCI-Unterstützung ermöglicht. In Lab in the Wild ist ein Online-Experimentplattform, bei der wir Divers-Volontäre recrutieren können. Im Vergleich zu Plattformen wie M-Turk, die hauptsächlich teilnehmer aus den USA und Indien haben, ist Lab in the Wild in der Lage, hohe Qualitätssicherung zu gewährleisten. Wir verwalten zwei Aufgaben auf Lab in the Wild, eine davon ist Soziale Akzeptabilität, bei der Teilnehmer Situationsauszüge aus dem Sozialchemie-Dataset lesen und dann schreiben, wie soziale Akzeptabilität eine Situation ist. Nachdem sie ihre Antworten abgegeben haben, können sie ihre Antworten mit einem AI und anderen vergleichen, um sich zu engagieren. Wir vergleichen dann dieseAnnotations mit dem Sozialchemie-Delphi- und GPT-4-Ansatz. Wir wiederholen dann einen ähnlichen Setup für die Giftsprache und Hasssprache-Ermittlungsaufgabe, bei der Teilnehmer Auszüge aus Dynahate liessen und schreiben, ob sie als Giftsprache angesehen werden. Wir vergleichen dann dieseAnnotations mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT-4. Unsere Studie sammelte schließlich über 16.000 Annotations von über 1000 Teilnehmern aus 87 Ländern. Jetzt sind wir in der Lage zu beantworten, wer NLP-Datensätze und Modelle am besten mit den Endnutzern alignieren. Wir finden, dass es Positionalität in NLP gibt. Zum Beispiel finden wir, dass Datensätze und Modelle am besten auf England sprechende Länder zugeschnitten sind. So beim GPT-4 Soziale Akzeptabilität-Ansatz finden wir, dass es am besten auf confusion und England sprechende Länder zugeschnitten ist. Wir finden auch, dass Dynahate am besten auf England sprechende Länder zugeschnitten ist. Wir finden auch weitere Übereinstimmungen mit Menschen, die ein Collegeabschluss haben. So beim GPT-4 Soziale Akzeptabilität-Ansatz finden wir, dass es am besten auf Menschen mit einem Collegeabschluss oder einem Abschluss an einer Fachhochschule zugeschnitten ist. Wir finden das gleiche für Dynahate, bei dem es am besten auf Menschen mit einem Collegeabschluss zugeschnitten ist. Wenn Modelle und Datensätze auf bestimmte Bevölkerungen zugeschnitten sind, werden andere zwangsweise zurückgelassen. Ein Beispiel hierfür ist, dass Datensätze und Modelle weniger auf nicht-binary Personen zugeschnitten sind, im Vergleich zu Männer und Frauen. Wir finden dies beim GPT-4 Soziale Akzeptabilität-Ansatz und beim Dynahate-Ansatz. Also, gegeben den Positionalitäten in NLP, was können wir dagegen tun? Wir haben einige Empfehlungen: Erstens solltet ihr alle relevanten Designentscheidungen während des Forschungsprozesses notieren. Das andere ist, NLP-Forschung mit dem Lesebruch von Perspektivismus zu machen. Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle für vier bestimmte Gemeinschaften zu erstellen. Ein gutes Beispiel hierfür ist die Masakhani-Initiative. Wir wollen betonen, dass ein inklusives NLP nicht einfach nur die Technologie machen soll, dass alle technologischen Werkzeuge für alle funktionsfähig sind. Das beendet unsere Präsentation. Wenn ihr mehr erfahren möchtet, könnt ihr gerne auf unserem Dashboard nach den neuesten Analysetools suchen und unser Papier überprüfen. Vielen Dank.</sample>
    <sample id="97">Die Referentin spricht von zwei Problemen der aktuellen SimulST-Modelle: der Notwendigkeit von spezifischen Architekturen und der langen und komplexen Trainingsprozeduren, einschließlich der Training mit mehreren Modellen für verschiedene Latenzregimes.</sample>
    <sample id="98">Um soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, können diverse Ansätze verwendet werden. Ein Ansatz besteht darin, Datensätze zu sichten und zu filtrieren, um verurteilte oder diskriminierende Inhalte zu entfernen. Ein anderer Ansatz besteht darin, Datensätze zu diversifizieren, indem man mehrere Quellen und Perspektiven einbezieht. Eine weitere Methode besteht darin, die Datensätze zu normalisieren und zu standardisieren, um Verzerrungen aufzuzeigen und zu reduzieren. Es ist auch wichtig, die Auswirkungen von Verzerrungen auf die Leistung und Fairness von NLP-Modellen zu überprüfen und zu bewerten, um sicherzustellen, dass sie fair und transparent sind.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan aus der Fudan University. Ich werde heute über unser Werk "Distilling Script Knowledge from Large Language Models for Constrained Language Planning" sprechen. In unserem täglichen Leben planen wir unsere Handlungen oft nach Schritt-für-Schritt-Anweisungen in Form von Ziellautungen. Vorherige Arbeiten haben Exploitation von Sprachmodellen verwendet, um abstrakte Ziele von stereotypen Aktivitäten wie "einen Kuchen machen" zu planen und zu zeigen, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings konzentrieren sich vorherige Arbeiten hauptsächlich auf die Planung abstrakter Ziele von stereotypen Aktivitäten. Die Planung von Zielen mit spezifischen Bedingungen, wie "einen Schokoladenkuchen machen", wird immer noch unterschätzt. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Bedingungen für die Ziele einstellt. Ein abstraktes Ziel kann von verschiedenen realen spezifischen Zielen mit mehrdimensionalen Bedingungen er继hen. Ein guter Planer sollte Skripte schreiben, die vernünftig sind und den Bedingungen entsprechend sind. In diesem Papier evaluieren wir zuerst und verbessern die Fähigkeit von großen Sprachmodellen, die Zielerklärungen unter eingeschränkten Bedingungen zu erstellen. Da keine Datensammlung von spezifischen Zielen existiert, um unser Studium zu unterstützen, müssen wir diese Ziele zuerst erhalten. Wie im Tabular gezeigt, erweitern wir abstrakte Ziele mit mehrdimensionalen Bedingungen zur menschlich-basierten Datenerkennung mit InstructGPT. Wir sampling 100 spezifische Ziele und evaluieren die generierten Skripte von großen Sprachmodellen. Das Tabular zeigt die allgemeine Genauigkeit der Ergebnisse an. Wir finden, dass alle Sprachmodelle unzufriedenstellende Resultate bei der Planung von spezifischen Zielen erhalten. Wir führen dann eine detaillierte Analyse durch, um zu ermitteln, warum die Lernermodelle versagen. Die Grafik zeigt die Planungsleistung von InstructGPTs für Ziele unterschiedlicher Kategorien. Vorherige Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen in hohen Varianzen liegt, was zu schlechter Leistung führt. Wir adhieren dem Ansatz "Over-generate-then-filter" um die Generationsqualität zu verbessern. Wir zeigen zuerst die Typen von Bedingungen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Säeden abstrakten Zielen. Dann over-generiert InstructGPT K Skripte für spezifische Ziele. Eine Filtermodellentwicklung wird verwendet, um die treue Skripte zu selektieren. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Cosinus-Ähnlichkeitswerte als Ähnlichkeitspunkte, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter der Zielschwelle enthält. Wir behalten nur das Skript, wenn das Zielschwelle die höchste Punktzahl im Zielschwelle-Satz hat. Mit unserem Ansatz kann InstructGPT Skripte von höherer Qualität generieren. Unser Ansatz verbessert die Planungsfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zu den Bedingungen. Da große Sprachmodelle kostspielig zu bereitstellen sind, ist es wichtig, die Sprachplanfähigkeit von kleineren und spezialisierten Modellen zu ermöglichen. Das Erstellen von Datensätzen ist ein wichtiger Schritt dazu. Allerdings können vorherige Studien die Planung von spezifischen Zielen nicht ermöglichen und die manuelle Datenermittlung ist teuer. Daher folgen wir dem Ansatz des symbolischen Wissensdistillations, um die Datenerkennung der eingeschränkten Sprachplanfähigkeit von großen Sprachmodellen zu distillieren. Wir verwenden unser Ansatz, um einen Datensatz der eingeschränkten Sprachplanfähigkeit zu erstellen, namens CoScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validier- und Testdatenbank zu gewährleisten, bitten wir Crowdsourced-Worker, falsche Beispiele zu finden und zu korrigieren. Das Grafik zeigt die Bedingungsentfernung von CoScript. Wir finden, dass CoScript eine hohe Pluralismus in den generierten spezifischen Zielen zeigt. Mit CoScript können wir Versuche mit kleineren aber spezialisierten Modellen für die eingeschränkte Sprachplanfähigkeit vornehmen. Wir finden, dass T5, der auf CoScript optimiert wurde, Skripte von höherer Qualität generiert als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle bei richtiger Ausbildung auf geeigneten Datensätzen über größere Modelle übertragen können. In zusammenfassender Weise etablieren wir das Problem der eingeschränkten Sprachplanfähigkeit. Wir evaluieren die Fähigkeit von großen Sprachmodellen, die Zielerklärungen unter eingeschränkten Bedingungen zu erstellen, und entwickeln einen Ansatz "Over-generate-then-filter" für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hohen-Qualitäts-Skript-Datensatz, CoScript, für die eingeschränkte Sprachplanfähigkeit zu erstellen. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource zur Fortschrittsarbeit im Bereich der Sprachplanfähigkeit sein kann. Vielen Dank für Ihre Zeit. Weitere Details zu CoScript finden Sie in unserem Papier.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, with each step corresponding to a document in the corpus. Multi-hop retrievers are trained by maximizing the probability of the ground-truth chains given questions. PromptRank is a data-efficient approach that uses an unsupervised retrieval method combined with a few-shot language model-based reranker. It retrieves candidate chains using TF-IDF and hyperlink traversal, then reranks them using a few-shot language model. The likelihood of the question given the chain is used as a scoring function, and the instruction plays a strong role in eliciting the language model's reasoning abilities over the chain documents. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop dense retrievers.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit jenen von hochwertigen Systemen, aber es gibt noch Raum für Verbesserungen hinsichtlich der Genauigkeit. In manchen Fällen können PaLM Übersetzungen vollenden, indem sie Teile des Quelltextes beim Übersetzen weglassen. Trotzdem bietet PaLM eine fluente Ausgabe, die jedoch noch anAccuracy fehlt.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: es sollte auf Embedding als Dienst anwendbar sein, das Wasserzeichen sollte die Funktionalität der bereitgestellten Einbettungen nicht beeinträchtigen, das Wasserzeichen sollte für Angriffe versteckt genug sein, sodass ein Angreifer es einfach entfernen kann, und das Wasserzeichen sollte über den Prozess der Modell-Extraktion übertragbar sein.</sample>
    <sample id="103">Die englischen TED Talks wurden ins 14 Sprachen übersetzt.</sample>
    <sample id="104">Das Paper erwähnt, dass 16.000 annotations von über 1000 annotators aus 87 Ländern für die Studie verwendet wurden.</sample>
    <sample id="105">Cosine- und L2-Similarität werden verwendet, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen.</sample>
    <sample id="106">The paper QUEST, developed in collaboration with Google DeepMind, addresses the challenge of handling selective information needs expressed with multiple constraints or preferences. The authors present a retrieval dataset containing over 3,000 entity-seeking queries with implicit set operations, verified answer entities, and marked attributions for different query constraints. The dataset is constructed using Wikipedia category names from four domains: films, books, plants, and animals. The paper demonstrates that the dataset poses a challenging retrieval problem due to the need to search over a large document corpus to find multi-answer sets where evidence for different query constraints can come from different parts of the document. Baselines for the dataset include sparse and dense retrievers as well as a T5-based reranker. The end-to-end system performance in terms of F1 scores is fairly low, showcasing the difficulty of systems in handling such queries. Queries with set intersection and set difference are particularly challenging and have the lowest F1 scores. The authors hope that QUEST can help future researchers build improved systems for their information-seeking scenarios with selective information needs.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt, um die Leistung zu bewerten und zu vergleichen. Die Analyse ergab, dass die Encoder-Decoder-Modelle die beste Leistung auf allen neun Datensätzen erzielen konnten. Es wurde auch festgestellt, dass die Encoder-Decoder-Modelle durch die Ausbildung an einer Mischung verschiedener Sprachen verbessert werden können.</sample>
    <sample id="108">The paper discusses the limitations of minimal pair paradigms in evaluating language models' acceptability judgments. The authors argue that current methods only evaluate a model's acceptance towards shorter sentences, which may not accurately reflect its performance on longer contexts. To address this, they propose revisiting the MPP pipeline by asking models to evaluate acceptability on longer sequences. They simulate longer sequences by choosing acceptable or unacceptable sentences from relevant data sets and adding them as prefixes to the query pairs. The results show that MPP judgments are relatively stable for arbitrary context lengths when using Wikipedia sentences, but significantly affected when using sentences from the same data set or unrelated domains. The authors conclude that language models are sensitive to latent syntactic and semantic features shared across sentences, and that current evaluation methods may not fully capture their abstract knowledge throughout the context window.</sample>
    <sample id="109">The paper introduces Unnatural Instructions, a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset contains 64,000 examples, with an additional 240,000 examples considering instruction paraphrases. The generated examples are diverse in tasks, content, and phrasing, and more than 50% of the generated examples are correct. The paper also shows that fine-tuning an 11 billion-parameter T5 model on Unnatural Instructions outperforms both T0++ and Tk-instruct across several benchmarks.</sample>
    <sample id="111">Die Autoren bestimmen die Wörter mit mittlerer Häufigkeit, indem sie eine allgemeine Textcorpussammlung sammeln und die Häufigkeit jedes Wortes darin zählen.</sample>
    <sample id="112">Hallo alle, mein Name ist Shuheng. Heute werde ich über unser Papier "Do CoNLL-2003 named entity taggers still work well in 2023?" sprechen lassen. Lassen wir beginnen. Unser Papier hat die Problematik der allgemeinheit im Hinblick auf die Aufgabenerkennung (Named Entity Recognition Task, NER) untersucht. Wir beobachteten, dass Modelle seit fast 20 Jahren im CoNLL-2003 verwendet wurden, um NER zu entwickeln, und dies natürlicherweise einige Probleme aufwirft. Zunächst einmal, können diese Modelle sich auf modernes Datenmaterial verallgemeinern? Und wenn wir neue Tagger entwickeln, was benötigt wird, um eine gute allgemeinheit zu erreichen? Genauso fragen wir uns, wenn wir eine schlechte allgemeinheit beobachtet haben, welche Ursache für den Leistungsverlust dieser Modelle ist? Um diese Fragen zu untersuchen, haben wir die CoNLL++-Datenbank entwickelt. Das ist eine Datensammlung, die wir aus Reuters News von 2020 entnommen und mit den gleichen CoNLL-2003-Annotieranweisungen annotiert haben. Wir haben über 20 Modelle an CoNLL-2003 fine-tuned und sie auf both CoNLL-03 Testsets und CoNLL++ bewertet. Und letzt but nicht zuletzt haben wir den prozentualen F1-Schwankungen verwendet, um die allgemeinheit jedes Modells zu messen. Was ist also für eine gute allgemeinheit erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptingredients bedarf. Das erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren. Das zweite Ingredient ist die Modellgröße. Wir haben festgestellt, dass normalerweise größere Modelle zu besseren Generalisierungen führen. Und das dritte ist bekannt: die Anzahl der Fine-Tuning-Beispiele hat einen direkten Einfluss auf die Leistung einer Downstream-Aufgabe. Hier haben wir auch festgestellt, dass mehr Fine-Tuning-Beispiele tatsächlich zu besseren Generalisierungen führen. To our next question, what causes the performance drop of some models, We had two hypotheses. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made, on CoNLL-2003 translates to more than one unit improvement on CoNLL++, which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper "Do CoNLL-2003 taggers still work in 2023?" And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="114">This paper presents a method for compressing multi-head attention in large language models by dividing heads into groups and pruning redundant heads. The proposed method, called Grouped Head Attention (GHT), uses a divide-and-conquer strategy to compress the model while maintaining performance on machine translation, language modeling, and abstractive summarization tasks. GHT achieves significant parameter compression, up to 90%, without sacrificing performance. The authors also propose a voting-to-stay algorithm to prune redundant heads and achieve further compression. The LITE model, which employs GHT, achieves 90% of pruned parameters, 62% faster inference speed, and 80% reduction in FLOPs compared to the original model. The authors believe that task-specific automatic pruning is a promising direction for reducing the redundancy of large language models in real-world applications.</sample>
    <sample id="115">The approach uses a fixed segment size of 160ms.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen benötigt, dass Servin ein Richter ist.</sample>
    <sample id="117">Der wichtige Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz ist die Qualität des Beispiels. Es wird empfohlen, Beispiele mit heller质量的翻译来选择，而不是与源句子相似。</sample>
    <sample id="118">The submission presents a study on improving pre-training techniques for code-switched NLP. The authors define code-switching as the use of multiple languages in a single sentence, such as "Laptop, mere, bag, me, rakha, hai." They propose novel MLM techniques and architectural changes to enhance performance on code-switched tasks like question answering and sentiment analysis. The proposed method, SwitchMLM, focuses on switch-points, which are groups of two tokens that indicate a language transition. The study also introduces FrequencyMLM, a surrogate method that uses negative log likelihood to assign LID tags. The results show that the combined method of SwitchMLM with ResBERT and auxiliary loss performs best on sentiment analysis tasks across language pairs. Probing experiments verify that the proposed methods increase switch-point information in intermediate layers.</sample>
    <sample id="119">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf Sprachmodelle mit verschiedenen politischen Neigungen. Sie untersuchen, in welchem Maß Sprachmodelle von der Ausrichtung der Daten im Trainingsmaterial beeinflusst werden und ob diese Ausrichtungen Auswirkungen auf die Leistung bei downstream Aufgaben wie dem Erkennen von Hasssprache und Falschinformation haben.</sample>
    <sample id="120">Das Modell EDAtt verwendet Aufmerksamkeitswerte nur aus einer bestimmten Ebene, um zu bestimmen, ob ein Wort emittiert werden soll oder nicht. Es berücksichtigt die Summe der Aufmerksamkeitswerte für die letzten lambda Sprachframes und vergleicht sie mit einem bestimmten Schwellenwert alpha. Wenn die Summe über alpha liegt, wird das Wort nicht emmitten, und wenn sie unter alpha liegt, wird es emmitten.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind die Namen der Lieder "Easy on Me" oder "I Gotta Feeling" oder ihre Position, wie zum Beispiel "das erste".</sample>
    <sample id="122">Die Autoren, darunter Siyu Yuan, gehören an der Fudan University.</sample>
    <sample id="123">The presented research focuses on improving multi-modal zero-shot learning through instruction tuning. The authors address the lack of multi-modal instruction datasets by introducing MultiInstruct, a dataset consisting of 62 diverse multi-modal tasks from 10 broad categories, each with five expert-written instructions. They use OFA, a unified multi-modal pre-trained model, and train it on 53 tasks from 9 groups, testing on a reserved common sense reasoning group and additional tasks from VQ and Miscellaneous groups. The results show that instruction tuning significantly improves OFA's performance on seen multi-modal tasks, with better performance and lower sensitivity as more tasks are used for training. The authors also introduce a new evaluation metric called sensitivity to measure the model's consistency in producing the same outputs for the same task despite slight variations in instruction wording. They conclude by proposing a larger multi-modal instruction tuning dataset and releasing their data and model.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented a study on improving temporal reasoning capabilities of large language models (LLMs). The research breaks down temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event. They developed the TempReason dataset to cover all three levels and evaluate LLMs in closed book, open book, and reasoning QA settings. To enhance temporal reasoning, they propose a training strategy with temporal span extraction pre-training and time-sensitive reinforcement learning. Their TempT5 model outperforms other instruction-tuned LLMs, especially in long-duration fact reasoning. Future work aims to address reasoning biases in LLMs.</sample>
    <sample id="125">The number of authors involved in the work is not explicitly mentioned in the provided information.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells vor dem semantischen Parsing wurde als Baseline betrachtet.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" by Namgyu Ho, Laura Schmid, and Se-Young Yun proposes a method to transfer reasoning abilities from large language models to smaller ones. The authors use chain-of-thought prompting on large models to generate step-by-step solutions for complex tasks, which are then used as training data for fine-tuning smaller models. They also introduce a technique called Diverse Reasoning, which generates multiple distinct reasoning samples to train the student model even better. The results show that their method can achieve notable performance in many tasks, especially text-based ones, and outperforms vanilla fine-tuning on most tasks, even with the smallest model that has 0.3 billion parameters. The paper also discusses the scalability of the method and the trade-offs between development costs, inference costs, and the quality of inference.</sample>
    <sample id="128">This paper presents the KITMUS test suite, a diagnostic tool for evaluating knowledge integration in natural language understanding models. The test suite consists of a coreference resolution task that probes the ability to draw on knowledge from different sources. The authors evaluate the data set with human study participants and established coreference resolution models. They define three settings: Background-Pretrain, Background-Both, and Background-Inference, which vary the availability of background and entity-specific knowledge at pretraining and inference time. The results show that many coreference resolution models are unable to reason over knowledge from different sources without task-specific training, but some models can successfully integrate knowledge from multiple sources when trained on generic reference resolution data sets. However, even the best-performing models have difficulties integrating backward knowledge presented only at inference time.</sample>
    <sample id="129">Die Autoren haben beispielsweise eine "Asiatische Frau" als markierte Gruppe gegeben.</sample>
    <sample id="130">Die Studie zeigt, dass die Transformer-Model-Architektur normalerweise besser zu neuen Daten generalisiert.</sample>
    <sample id="131">In dem Video wird nicht erwähnt, welche Testdatensätze verwendet wurden.</sample>
    <sample id="132">In der Arbeit "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" sind insgesamt drei Autoren beteiligt: Akshatha, Martin und die Kolleginnen/Kollegen von McGill University, Mila und Microsoft Research.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, nicht nur mit Text. Sie verwenden ein multi-modal prätrainiertes Modell (OFA) und experimentieren mit der Anpassung von Aufgaben in einem einheitlichen Format, das Sprache, Bildelemente und Koordinaten einesBounding-Boxes in einem Tokenraum vereint.</sample>
    <sample id="135">In this video, James and Sarah Finch introduce ABC-Eval, a new method for evaluating conversational AI developed by the Emory NLP Lab led by Professor Jinho Choi at Emory University in collaboration with Amazon Alexa AI. They explain that while human evaluations are common, they can be subjective and limited to overall dialogue quality. ABC-Eval aims to provide a more precise and reliable evaluation by explicitly annotating whether each model response exhibits certain behaviors such as ignoring the partner, contradicting, hallucinating incorrect facts, or violating common sense knowledge. The method measures thematic errors and success in showing empathy. The team evaluated four state-of-the-art chat models using ABC-Eval and compared them with three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. Results showed that ABC-Eval is more reliable and predictive of conversation quality than existing methods. The video concludes by highlighting the challenges still present in conversational AI and the importance of reliable evaluation metrics for comparing models.</sample>
    <sample id="136">The presentation introduces FERMAT, a flexible evaluation set for numerical reasoning tasks. The motivation behind this work is to address the limitations of current benchmarks in evaluating the mathematical abilities of language models. FERMAT includes a variety of arithmetic types and operations, as well as different number representations, to test the range and breadth of these models. The presentation also discusses the impact of fine-tuning on model performance and the importance of training templates in improving results. Overall, FERMAT aims to provide a more informative alternative to existing benchmarks and highlight areas for improvement in language model training and evaluation.</sample>
    <sample id="137">The paper introduces Tell2Design, a dataset for language-guided floor plan generation. The task involves generating 2D floor plan designs from natural language instructions that describe the semantics, geometry, and topology of the desired layout. The dataset consists of 5,051 human-annotated and 76,000 artificially generated language instructions. The authors propose a sequence-to-sequence model using a transformer-based encoder-decoder structure to generate floor plans based on these instructions. The model outperforms text-conditional image generation baselines in terms of Intersection over Union (IoU) scores, demonstrating its effectiveness in handling various lengths of instructions and multiple room constraints.</sample>
    <sample id="138">Nach Ansicht der Autoren ist ein zu wenig erforschtes Gebiet im Bereich der NLU die Fähigkeit von Modellen, Wissen aus verschiedenen Quellen zu integrieren und zu verwenden.</sample>
    <sample id="139">The presenters are Ying and Zhiyang.</sample>
    <sample id="140">Ja, Coscript hat eine Qualitätskontrolle durchlaufen. Um die Qualität der Validier- und Testdaten zu gewährleisten, wurden crowd-sourcing-Workers aufgefordert, falsche Beispiele zu finden und zu korrigieren.</sample>
    <sample id="141">Bestehende Ressourcen zur kontextbasierten Übersetzung haben Grenzen, da sie nur begrenzte Arten von kontextabhängigen Übersetzungen und Sprachen unterstützen. Sie hängen normalerweise von Fachkenntnis und menschlicher Curation ab.</sample>
    <sample id="142">Hallo! Ich werde über unser Werk "Resolving Indirect Referring Expressions for Entity Selection" sprechen, in dem wir die AltEntities Corpus-Datei Introducing präsentieren. Mein Name ist Javad Hosseini und das ist eine gemeinsamearbeit mit Filip Radlinski, Silvia Pareti und Annie Louis. Unsere Zielgruppe sind Benutzer, die dabei helfen wollen, Entscheidungen zu treffen. Betrachten Sie beispielsweise diese alternative Frage: "Meintest du 'Easy on Me' oder 'I Gotta Feeling'?" Hier will der Benutzer zwischen zwei Liedern wählen. Der einfachste Weg wäre, den Namen des Liedes "Easy on Me" oder seine Position, "das erste", zu verwenden. Aber manchmal ist eine indirekte Bezugnahme besser, um eine natürlichere Unterhaltung zu haben. Das könnte passieren, wenn der Benutzer den Namen des Liedes nicht mehr weiß. Oder wenn die Aussprachen zu ähnlich sind und es schwierig ist, sie zu unterscheiden. Oder wenn der Benutzer eine bestimmte Vorliebe ausdrücken möchte. Hier sind einige Beispiele von indirekten Bezugnahmen: "das neuere Lied" oder "das Lied, das nicht energiegeladen ist". Dies ist ein wichtiger Problemfall in conversational systems und auch für die Benchmarking von LLMs' Entity-verstehensfähigkeit. Wir kennen keine größeren öffentlichen Datensätze für die Aufgabe, also sammeln wir einen anhand von Crowd-annotationen. Unsere Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensatzsammlungmethodik betont die Unformlichkeit unter Verwendung einer Cartoon-Fullfillment-Setup. Das Cartoon hat drei Sprachblister. In dem ersten Sprachblatt sagt Bob: "Erinnere dich an das Lied, das wir gestern gehört haben." Damit sets Bob den Dialogkontext. In dem zweiten Sprachblatt sagt Alice: "Meintest du 'Easy on Me' oder 'I Gotta Feeling'?" Welches ist die alternative Frage? Und in dem dritten Sprachblatt verwendet Bob einen indirekten Bezug, um eines dieser Objekte zu auswählen, zum Beispiel "das neuere Lied". Wir liefern den ersten und zweiten Sprachblatt automatisch, aber das dritte wird vom Notar besprochen. Der erste Sprachblatt wird aus ein paar manuellen prompts pro Domäne gewählt. Der zweite Sprachblatt, der alternative question, wird wie folgt generiert: wir verwenden immer einen einfachen-template. Meinst du A oder B? Where A und B sind Beispiele aus Wikipedia. Hier sind die verschiedenen sampling-methoden, die wir verwendet haben. Wenn wir uns weiter hoch in die Liste bewegen, werden die Objekte ähnlicher und es wird normalerweise schwieriger, die Disambiguierung zu machen. Der Erste ist zufällig. Der Zweite ist, wenn die Objekte ähnliche Titel haben, zum Beispiel zwei Böck mit dem Namen "The Return". Der Dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxes oder Attribute auf Wikipedia haben. Zum Beispiel die gleiche Genres oder die gleiche Artisten für ein Lied. Wenn wir diesem alternative question den Annotatoren zeigen, wissen sie den Namen dieser Objekte, aber sie kennen sie nicht unbedingt. Also was wir tun, ist, wir zeigen den Annotatoren einige Hintergrundinformationen über die Objekte.Für Lieder simply show a Google-Suchergebnis zu jedem Lied und dann bitten wir die Annotatoren, sich ein paar Lieder anhören und über jedes Lied etwas zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied "Easy on Me". Für die Rezepte und die Domäne derbücher wir zeigen einige Hintergrundtexte aus Wikipedia. Für Rezepte additionally show their images, again from Wikipedia, so that the annotators know how they look like. Then wir bitten die Annotatoren, eines dieser Objekte zu auswählen, zum Beispiel hier ist der Erste, und beschreiben sie mit drei bis fünf indirekten Bezugnahmen. Zum Beispiel das mit dem Pianomusik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel "das Lied, das Worte hat", "nicht das Lied mit dem 12-jährigen Jungen", oder "das fiktive Lied", oder "kommt aus Aserbaidschan", und so weiter. Das AltEntities Corpus hat 6.000 alternative Fragen über drei Domänen, und es hat 42.000 indirekten Bezugnahmen. Resultate mit T5 XL Modell sind summiert. Wenn das Sprachmodell Zugang zu genauso viel Hintergrundknowledge hat wie die Annotatoren, dann ist die Genauigkeit wirklich hoch, es liegt bei 92-95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlappendem Hintergrundknowledge hat, dann die Genauigkeit liegt zwischen 82-87%, was realistischer ist. Zum Beispiel wenn das Sprachmodell das Hintergrundknowledge abrufen kann. Wenn das Sprachmodell Zugang nur zu Objekt-Namen hat, dann die Genauigkeit nur 60%, also gibt es noch viel Raum zur Verbesserung. Wir haben auch gezeigt, dass die Modelle dominanztäuschbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">Der Ansatz wird mit den bekannten Strategien "Wait-k" und "Local Agreement" als auch mit dem neuesten Modell für vorzeitige Übersetzung verglichen.</sample>
    <sample id="144">The authors of the paper "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" are affiliated with the University of Nantes.</sample>
    <sample id="145">Die Referentin, die die Präsentation macht, heißt Jenny.</sample>
    <sample id="146">The speaker, Yicheng from Fudan University, introduces a paper on the analysis of omission in dialogue summarization. They discuss the challenges and limitations of current dialogue summarization models, particularly the issue of omission, which leads to incomplete summaries. The speaker presents an analysis of the omission rate across different domains and models, highlighting its prevalence. They also introduce the OLDS dataset, which provides high-quality omission labels for dialogue summarization, and explore three baseline frameworks for omission detection. The results show that the task is challenging, but the refinement based on detected omissions can improve summary quality.</sample>
    <sample id="147">Es sind insgesamt 3 Autoren an der Arbeit beteiligt.</sample>
    <sample id="148">Hallo, ich bin Sara Papi aus der Universität Trento und der Fondazione Bruno Kessler. Ich werde kurz das Papier "Attention als Führer für die gleichzeitige Übersetzung von Reden" vorstellen, das eine gemeinsamearbeit mit Matteo Negri und Marco Turchi ist. Was ist gleichzeitige Übersetzung von Reden? gleichzeitige Übersetzung von Reden (SimulST) ist der Prozess, bei dem gesprochene Sprache in einer anderen Sprache in Echtzeit zu Text übertragen wird, um interlinguistische Kommunikation zu ermöglichen. Und welche Probleme haben aktuelle SimulST-Modelle? Spezielle Architekturen werden normalerweise trainiert, indem man zusätzliche Module optimieren muss. Langwierige Trainingsverfahren, z.B. die Training-involvierte Optimierung mehrerer Modelle für verschiedene Latenzregime. Beispielsweise wird ein Modell mit einem Durchschnittslag von einem Sekunden trainiert und ein anderes Modell mit zwei Sekunden Latenz trainiert, und so weiter. Also was ist unsere Lösung? Zunächst einmal benutzen wir schon existierende offline ST-Modelle, ohne sie zu retrainieren oder spezielle Architekturen für SimulST zu verwenden. Wir verwenden nur ein Modell für jede Latenzstelle und bewältigen Latenz durch bestimmte Parameter. Und wir nutzen die Kenntnisse, die das Modell durch die Ablagemechanismen zwischen Audio-Eingabe und Textausgabe gewonnen hat. Das ist die Ablage-Mechanismen, und du kannst ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist es, EDAtt, Encoder-Decoder Ablage, zu vorschlagen, und es ist eine Strategie, bei der wir entscheiden, ob wir einen Teileinsatz oder nicht machen, basierend auf der Ablage an bestimmten Punkten. Ein Wort wird ausgestrahlt, wenn die Ablage nicht konzentriert ist, d.h., wenn die Summe der Ablage im unteren lambda-Speech-Fragment unter einem bestimmten Schwellenwert alpha liegt, was bedeutet, dass die empfangene Information stabil genug ist. Zum Beispiel erhalten wir ein Sprachsegment mit "Ich gehe reden über..." und unser Modell vorhersagt die Übersetzung ins Deutsche. Wenn wir die Ablagegewichter ansehen, sehen wir, dass die ersten zwei Worte auf den earliest empfangenen Sprachsegmente zeigt, während das letzte Wort auf die letzten lambda-Speech-Fragmente zeigt. Das bedeutet, dass die ersten zwei Worte ausgestrahlt werden, da die Summe der Ablage über einem bestimmten Schwellenwert alpha liegt und wir das letzte Wort warten, bis wir ein anderes Sprachsegment erhalten. Wenn wir fortfahren und ein anderes Sprachsegment erhalten und unser Modell vorhersagt, dass es drei Worte gibt, und wir die Ablagegewichter ansehen, sehen wir, dass keines der Worte auf die letzten lambda-Speech-Fragmente zeigt. Das bedeutet, dass diese drei Worte ausgestrahlt werden. Wenn wir die Hauptergebnisse von EDAtt plotten, erhalten wir Grafiken, in denen wir BLEU auf einer Seite messen, um die Übersetzungsqualität zu messen, und durchschnittliche LAG (Latenzmessung), und wir betrachten auch die computeraufwand-aware durchschnittliche LAG, die die Reaktionszeit des Modells zur Vorhersage des Ausgangs berücksichtigt. Wir wollen, dass unsere Kurven so hoch wie möglich auf dieser Achse liegen. Aber wir wollen auch, dass sie links verschiebt sind. Und wir vergleichen unsere Ergebnisse mit populären Strategien, die auch auf offline Modellen angewandt werden, das Wait-k-Strategie und die Lokale Einwilligung. Und wir vergleichen auch mit dem aktuellen Stand der Technik, das speziell für die gleichzeitige Voraussicht entwickelt wurde. Hier sind die Ergebnisse der gleichzeitigen Übersetzung von Reden auf Deutsch. Und wir sehen, dass es alle Strategien überbietet, die auf offline Modellen angewandt wurden, da die Kurven links verschoben sind. Und wir sehen auch, dass wenn wir die tatsächliche verstrichene Zeit oder die computeraufwand-aware durchschnittliche LAG berücksichtigen, das EDAtt-Modell die schnellste Strategie ist. Wenn ihr mehr Ergebnisse entdecken möchten, lest ihr unser Papier und wir haben auch das Open Source-Code und Modelle und gleichzeitigen Ausgabenteil zur Verfügung gestellt, um die Wiederholbarkeit unseres Werks zu erleichtern. Danke für eure Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">The paper presents a new dataset called MeetingQA, which is an extractive question answering dataset based on questions asked by participants in meetings and the corresponding answer sentences. The dataset contains 7.7K questions split between the Train, Dev, and Test sets, with 30% of our questions being unanswerable. The authors also introduce a variety of methods for question answering, including context-retrieval, single-span models, multi-span variants, and data augmentation. The results show that there is over a 25 F1 point gap between fine-tuned models and human performance, and that zero-shot performance is significantly lower than fine-tuned performance. The authors conclude that MeetingQA is an interesting dataset based on open-ended and discussion-seeking questions in real-life meeting scenarios, and that it is challenging for existing QA models in both fine-tuned and zero-shot settings.</sample>
    <sample id="151">Hallo alle, mein Name ist Ying und mein Kollege Zhiyang werden heute über unsere Forschung präsentieren: "MultiInstruct - Verbesserung von Multi-Modal Zero-Shot Learning durch die Optimierung der Anweisungen." Mit Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernkonzepte zu untersuchen, um auf verschiedenen downstream-Aufgaben effizient und mit wenig Datensatz zu arbeiten. Kürzlich haben viele Studien gezeigt, dass die Optimierung der Anweisungen große Sprachmodelle in der Lage macht, auf unbekannte Aufgaben in einem Null-Shot-Modus zu arbeiten, indem sie bestimmte Anweisungen folgen. Allerdings haben die meisten früheren Arbeiten auf Optimierung der Anweisungen hauptsächlich auf die Verbesserung der Null-Shot-Performanz bei Sprach-only-Aufgaben abgesehen, während Computer Vision- und multi-modalen Aufgaben weggelassen wurden. Daher wollen wir in dieser Arbeit untersuchen, ob die Optimierung von multi-modalen vorher geschulten Modellen tatsächlich zur Verbesserung der allgemeinen Anpassung an unbekannte multi-modalen Aufgaben beiträgt. Zudem haben wir bei unserem Forschungsbeginn festgestellt, dass es einen erheblichen Unterschied im Verfügbaren von Anweisungsdatensätzen zwischen NLP und multi-modalen Aufgaben gibt. Es existieren mehr als 1600 Sprach-only-Anweisungs Aufgaben. Gegenüber diesem gibt es jedoch keinen großen öffentlich verfügbaren multi-modalen Anweisungs Aufgabensatz. Daher motiviert uns, einen multi-modalen Anweisungs Aufgabensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multi-modalen Anweisungs Aufgabensatz, der 62 diverse multi-modalen Aufgaben umfasst, die 10 breite Kategorien abdecken. Diese Aufgaben werden aus 21 bestehenden offenen Quellen entnommen und jede Aufgabe wird mit 5 Experten geschriebenen Anweisungen versehen. Um die multi-modalen Anweisungs Aufgabensätze auf unser vorgeschlagenes Dataset zu untersuchen, nehmen wir OFA, ein vereinbares multi-modalen vorher geschultes Modell, als Basismodell an. OFA verwendet eine gemeinsame Vokabel für Sprache, Bildtoken und die Koordinaten einerBounding Box. Hier zeigen wir einige Beispiele aus unserem MultiInstruct-Dataset, um die Einheitliche Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinbaren. Wir folgen dem Verfahren von OFA und formulieren alle Aufgaben in einer vereinbarten sequenz-to-sequence-Formate. In der Eingabe-Text, Bild, Anweisungen undBounding Box werden alle Aufgaben in derselben Tokenraum dargestellt. Okay, nun werde ich über die multi-modalen Anweisungs Aufgabensätze sprechen. So für die Trainingsaufgaben verwenden wir 53 Aufgaben aus 9 Gruppen für die Training und wir sampling 10.000 Instanzen pro Aufgabe.Für die Testphase reservieren wir die gesamte Gruppe von Allgemeinwissen für die Testphase und wir ausgewählten 5 Aufgaben aus den Gruppen VQ und Allgemeines. Wir verwenden alle Instanzen in der Testsplit für jede Aufgabe. Darüber hinaus randomly sampling 20 Aufgaben aus dem Testsplit von natürlichen Anweisungen als unbekannte Aufgabe für NLP. Wir verwenden den vorher geschulten OFA großen Modell als Basismodell. Während der Trainingphase mischen wir alle Instanzen von allen Aufgaben. Jede Instanz wird zufällig mit einem von den 5 Anweisungsvorlagen kombiniert. So während des Tests für jede Aufgabe führen wir insgesamt 5 Experimente durch, indem wir das Modell mit einer von den 5 Anweisungen evaluieren. In jeder Experimentphase berichten wir die minimale und maximale Performance und die Standardabweichung der Performance über alle 5 Experimente. Wenn die Aufgabe eine multi-modalen Klassifizierungs Aufgabe ist, berichten wir die Genauigkeit. Wenn es eine multi-modalen Generation Aufgabe ist, berichten wir die Rouge-L. Für NLP Aufgaben berichten wir die Rouge-L. Wir Introduzieren auch einen zusätzlichen Evaluierungsmaßstab namens Sensitivität. Das messt die Fähigkeit des Modells, für dieselbe Aufgabe immer die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, kann die Optimierung der Anweisungen signifikant die Performance von OFA bei gesehener multi-modalen Aufgaben verbessern. Auch die Transferlearning von natürlichen Anweisungs Aufgabensätzen kann die Optimierung der Anweisungen verbessern. Hier können wir sehen, dass mit zunehmender Anzahl der Aufgaben das Modell besser performiert und dabei gleichzeitig eine höhere Sensitivität erreicht. Wir haben auch ein weiteres Experiment durchgeführt. Wir verwenden eine Anweisung gegenüber 5 Anweisungen. Wie wir sehen können, kann die Nutzung mehrerer Anweisungen die allgemeine Performance des Modells verbessern und seine Sensitivität deutlich reduzieren. Das zeigt die Wirkung verschiedener Optimierungsstrategien auf die Sensitivität des Modells. Wie wir sehen können, durch Transferlearning von natürlichen Anweisungs Aufgabensätzen kann das Modell viel bessere Sensitivität erreichen als das ursprüngliche OFA Modell. Wir können auch sehen, dass Transferlearning von natürlichen Anweisungs Aufgabensätzen das Modell dazu bringt, die Aufgabensatz von natürlichen Anweisungen zu erreichen. Im Allgemeinen vorschlagen wir den ersten großen multi-modalen Anweisungs Aufgabensatz mit erheblich verbessertem kurzfristigen Leistungsvermögen von OFA und wir untersuchen verschiedene Transferlearning Techniken und zeigen ihre Vorteile an. Wir designen auch einen neuen Maßstab namens Sensitivität. Noch etwas, wir sammeln einen viel größeren multi-modalen Anweisungs Aufgabensatz mit around 150 zusätzlichen visio-languag Aufgaben und wir werden ihn freigeben. Hier ist ein QR-Code für unser Daten- und Modell. Vielen Dank.</sample>
    <sample id="152">Frederick Riemenschneider introduces his work on the intersection of NLP and classical philology, focusing on the development of language models for Ancient Greek and Latin. He highlights the limitations of existing monolingual models and the need for multilingual capabilities. The project aims to create models that can handle both languages, exploring different architectures and pre-training data sources. The team has developed monolingual models (GreBERTa and GreTa) and multilingual models (PhilBERTa and PhilTa) pre-trained on Ancient Greek, Latin, and English texts. They have benchmarked their models using tasks like part-of-speech tagging, dependency parsing, and lemmatization, achieving state-of-the-art results. The analysis also reveals how T5's encoder behaves differently from native encoder-only models and whether multilinguality improves performance in semantic and world knowledge tasks.</sample>
    <sample id="153">Hello, my name is Ninareh Mehrabi. I am a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, and I will present our work, "Resolving Ambiguities in Text-to-Image Generative Models". In this work, we were interested in studying existing ambiguities in prompts provided to text-to-image models. For instance, the following prompt is ambiguous because it can have various different interpretations. Or the following prompt, "The girl enters the room with flowers," is ambiguous because it's not clear whether the flowers should be in the room, with the girl, or a combination of both. And without resolving these ambiguities, it's going to be challenging for text-to-image models to generate faithful images to user intention. So in this work, we are studying prompts that are ambiguous and are provided to text-to-image models, and our goal is to propose frameworks to mitigate such ambiguities as well as frameworks to evaluate whether the generated images are faithful to user intention. Here is our pipeline. First we curate a benchmark dataset that covers different types of ambiguities. Then these prompts are provided to a prompt disambiguation framework that tries to gather external signals to disambiguate the prompt, through either asking clarifying questions from the user or generating different possible visual setups. Once we have the prompts disambiguated, we are going to evaluate them. We are also going to input these disambiguated prompts into a text-to-image model, generate the images, and evaluate whether the generated images are faithful to user intention. So let's start talking about our benchmark dataset. Our benchmark dataset is the modified version of a previously existing corpus called LAVA, and it covers different types of ambiguities as listed in the table below. In our framework, the language model first generates clarifying questions using in-context learning. So given an ambiguous prompt, the language model will generate a clarifying question. The user will then answer the clarifying question based on the intention that they have in mind, and we are going to obtain a disambiguated prompt by concatenating the signal to the original ambiguous prompt. Notice that the user can also provide the answer that satisfies another interpretation. In the second setup, instead of the language model generating a clarifying question, it will generate different possible visual interpretations. So we still have the original ambiguous prompt; here, the language model instead of generating clarifying questions, will generate different possible visual setups and the user will interact with the system and provide the answer that satisfies its intention. And we are going to again gather a final disambiguated prompt by concatenating the signal to the original ambiguous prompt. Notice that it's possible that the human is picking the other interpretation. Now that we have our prompts disambiguated, we want to evaluate whether the generated images are faithful to user intention. To do that, we are going to propose our automatic evaluation framework. So we have the original ambiguous prompt versus the one that is disambiguated. We are going to input this as text-to-image models, and we are going to generate the image corresponding to each prompt. To evaluate whether these images are faithful to user intention, we are going to use a VQA model. We are going to input the images as well as the human's intention in question format as input to the VQA model, and we are going to evaluate whether the human's intention is satisfied in the image or not. If the answer is yes, it means that user intention is satisfied, so the image is faithful; if the answer is no, it means that the generation was not faithful to user intention. Then we have some findings in the paper: we show that there is disparity in resolving ambiguities for different ambiguity types. We show that disambiguation using our framework has overall a positive effect in faithful generation. And we show that our automatic evaluation framework is in agreement with human evaluation, so it can be used reliably to evaluate text-to-image models. And we also have additional findings and discussions in the paper. So if you're interested, please refer to our paper. So, to conclude, in this work, we study ambiguities in text-to-image models. We curate a benchmark dataset covering different types of ambiguities, and we propose frameworks to both mitigate as well as evaluate ambiguities provided to text-to-image models. So with this, I'm going to conclude this talk and thank you so much for your attention.</sample>
    <sample id="154">Die Autoren Sara Papi, Matteo Negri und Marco Turchi gehören an der University of Trento und Foundazione Bruno Kessler.</sample>
    <sample id="155">The AltEntities Corpus is a dataset that contains 6,000 alternative questions across three domains: music, books, and recipes. It has 42,000 indirect referring expressions to help understand users' language when they want to make a choice between two or more entities. The dataset was collected using crowd annotation, where annotators were provided with background knowledge about the entities and asked to pick one of them and describe it using indirect referring expressions.</sample>
    <sample id="157">The speaker introduces their work on dialogue summarization, which aims to distill salient information from a dialogue context into a concise summary. They highlight the challenges of existing methods that rely on pre-computed static graph structures and propose a new model called SDDS (Static-Dynamic Structure Fusion Graph) that integrates static and dynamic dialogue structures for more accurate summarization. The SDDS model consists of an Utterance Encoder, a Static-Dynamic Graph module, and a Summary Generator. It employs four heuristic dialogue structure modeling methods to build static graphs and uses a multi-head attention model to capture semantic relationships in a dynamic graph. The model combines these graphs using a fusion method and incorporates graph representation into the generation process through a dual cross-attention mechanism. The code and data are available on GitHub.</sample>
    <sample id="158">Coreference resolution is a task in natural language processing that involves identifying mentions of entities in a document and clustering them based on their reference to the same entity. Conventional methods for this task have quadratic complexity, while cache-based methods use a fixed-size cache to reduce complexity to a linear level. However, in long documents, the topic may switch multiple times, leading to high cache misses with LRU eviction policy. Dual cache proposes a local and global cache to store local and global entities, respectively, with LRU and LFU eviction policies. The model scans the document from left to right, classifying new or updated entities and evaluating their frequency before adding them to the appropriate cache. Dual cache outperforms single cache methods and significantly reduces cache misses, making it the most cost-effective solution.</sample>
    <sample id="159">Hallo, alle. Ich bin Koustav Sinha und freue mich, euch willkommen zu heißen bei unserem Vortrag über unser ACL 2023 Papier. Sprachmodell-Acceptabilitätsurteile sind nicht immer robust gegenüber Kontext. Das ist ein gemeinsames Werk mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In diesem Werk revisitieren wir die Minimal-Paar-Paradigmen. Das Minimal-Paar-Paradigma bewertet Sprachmodelle anhand von Acceptabilitätsurteilen. Das kann auch Grammatik wie BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotypien wie CrowS Paare einschließen. Im Minimal-Paar-Paradigma wird üblicherweise bewertet, ob Sprachmodelle auf akzeptierbare Sätze oder grammatikalische Sätze mehr Wahrscheinlichkeit geben. Das typische Weg, Sprachmodelle zu bewerten, zeigt man, indem man einen akzeptablen Satz oder einen grammatikalischen Satz zeigt und dann einen akzeptablen Satz oder einen ungrammatikalischen Satz zeigt. Das Ziel ist, dass das Modell mehr Wahrscheinlichkeit für den akzeptablen Satz gibt. Der aktuelle MPP-Pipeline erlaubt uns nicht, ein Modell zu bewerten, basierend auf seiner Akzeptabilität über eine längere Kontextlänge. Heute werden große Sprachmodelle mit immer größeren Kontextfenstern aufwachen. Es ist daher wichtig, dass wir die Akzeptabilität von Modellen über die gesamte Kontextlänge bewerten. Das ist, was wir hier versuchen. Wir versuchen, die MPP-Pipeline zu revisitieren, indem wir Modellen bitte bewerten, die Akzeptabilität auf längere und längere Sequenzen bewerten. Das machen wir, indem wir die Datensätze neu simulieren und damit lange Sätze erstellen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir einen typischen Paar von Grammatik from der BLiMP-Datensatz vom Adjunkt-Island-Fall gewählt. Was wir tun, um solche lange Sequenzen zu erstellen, die akzeptabel und die gleiche Grammatikstruktur haben, extrahieren wir grammatikalische Sätze aus dem Adjunkt-Island-Fall und fügen sie als Präfix zu beiden akzeptablen Abfragen hinzu. Wir können dasselbe tun, indem wir unakzeptable Sätze aus dem gleichen Matching auswählen und damit testen, ob das Modell Akzeptabilität bewertet. Wir können auch dieselben Sätze aus einem anderen Datensatz oder einer verschiedenen Gruppe auswählen. Das nennen wirMismatch-Szenario. Hier sind die Sätze immer noch aus relevanten Datensätzen, aber sie stammen nicht aus dem Datensatz, den wir bewerten. Wir können dasselbe für den Fall der Unakzeptibilität tun. Schließlich können wir Sätze aus einem vollständig unverwandten Bereich wie Wikipedia auswählen. Das zeigt uns, ob die Akzeptabilitätsurteile des Modells tatsächlich durch Kontext beeinflusst werden, ob der Kontext aus einem anderen Datensatz stammt, oder ob er für die aktuelle Abfrage irrelevant ist. Wie performiert das Modell? Zunächst schauen wir uns Wikipedia-Sätze an, die für die aktuelle Abfragenpaar irrelevant sind, und finden, dass die MPP-Urteile hauptsächlich robust gegenüber beliebigen Kontextlängen sind. Wir erhöhen die Kontextlänge bis zu 1024, um die OPT- und GPT-2-Modelle zu testen. Hier in der orange doppelt gestrichenen Linie sehen wir, dass die MPP-Urteile relativ stabil sind. Nun, was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier warten wir auf akzeptable und unakzeptable Sätze aus dem gleichen BLiMP- oder SyntaxGym-Datensatz. Und hier sehen wir, dass die MPP-Urteile entweder signifikant zunehmen oder abnehmen, wenn wir entweder akzeptable oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur匹配, also Sätze aus dem gleichen Phänomen in BLiMP oder SyntaxGym auswarten, sehen wir einen massiven Anstieg oder einen massiven Abfall der MPP-Urteile für das Modell, je nachdem, ob der gewählte Präfix akzeptabel oder unakzeptabel ist. Jetzt und jetzt ist dieser Effekt sehr groß, und er wird wahrscheinlich die jüngeren Sprachmodelle beeinträchtigen, die einen großen Kontextwindow haben. Warum beeinträchtigen die match-Präfixe die Sprachmodellurteile so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, die Eingabeaussage zu perturbieren, indem wir den relevanten Strukturanhalt bewahrten, aber wie Geräuscher zu den Eingaben hinzufügen. Nach mehreren dieser Perturbationen finden wir, dass keines dieser Geräusche die Modellurteile wirklich verändert. Basically finden wir, dass die Modelle sensible an perturbierten Sätzen sind, indem sie in ähnliche Weisen reagieren. Wenn wir Sätze im akzeptablen Bereich perturbieren, sehen wir einen ähnlichen Anstieg in allen Perturbationen, und wenn wir Sätze im unakzeptablen Bereich perturbieren, sehen wir einen ähnlichen Abfall in den MPP-Urteilen. Das Hauptergebnisse unseres Werks sind, dass Sprachmodelle sensible an latenten syntagmatischen und semantischen Merkmalen sind, die über die Sätze verteilt sind. Und die MPP-Evaluation, die wir momentan mit kurzen und einzelnen Satzinput durchführen, mag nicht vollständig das abstrakte Wissen von Sprachmodellen über die Kontextlänge abdecken. Lesen Sie unser Papier für weitere Details über unsere Experimente. Vielen Dank fürs Zuhören.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einem unsortierten Multiset von Tokenen zugeordnet, die im Ausgabe-Fragment vorkommen werden.</sample>
    <sample id="161">In Coscript sind insgesamt 55.000 spezifische Ziele mit Skripten vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die Methode von MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es viel billiger ist, Datensätze zu labeln, indem man schwache Quellen verwendet, anstatt menschliche annotations.</sample>
    <sample id="165">The speaker, Wenting Zhao from Cornell University, introduces a paper on abductive commonsense reasoning. The paper presents an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization) that can identify plausible explanations without supervision. The method treats explanations as latent variables and uses mutual exclusivity among explanations to prefer some over others. The paper compares LiPoR to zero-shot models and the previous best unsupervised approach on AlphaNLI, achieving over 4 absolute points in accuracy.</sample>
    <sample id="166">Abstract: We propose a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. Our method decomposes complex propositions into simple ones using the Proposition Generator and performs visual-linguistic interaction using the Visual-Linguistic Interactor. The Neural-Symbolic Reasoner integrates the reasoning states and results of simple propositions to obtain the final solution. Experimental results show that our proposed method outperforms other baselines and is effective in processing interoperably. We suggest that neural symbolic calculation may improve compositional reasoning and planning in large language models, and that Dual-Process Theory could be integrated with Divide-and-Conquer.</sample>
    <sample id="167">Die Dokumente in DEPLAIN-web wurden auf einer Seite manuell und auf der anderen Seite mit automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem Reuters News von 2020 abgezogen wurde und mit den gleichen Annotieranleitungen wie CoNLL-2003 anhand davon annotiert wurde.</sample>
    <sample id="169">In this paper, the authors present a systematic study of large language model prompting for machine translation. They evaluate the performance of PaLM, a 540 billion-parameter language model trained on 780 billion tokens, using the best practices of the MT community. The authors compare PaLM to state-of-the-art systems and provide recommendations for prompt selection strategies. They find that the quality of the examples used in prompting is more important than the similarity to the source sentence. They also note that specialized state-of-the-art systems have a substantial advantage over PaLM translations, but PaLM comes close to a commercial system. The human evaluation results show that the fluency of PaLM is comparable to state-of-the-art systems, but the main difference comes from accuracy, with PaLM choosing to produce better-sounding translations sometimes by dropping parts of the source sentence that are made in translation.</sample>
    <sample id="170">Hallo alle, ich bin Yusen Zhang aus der Pennsylvania State University. Heute werde ich über unser Werk "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" sprechen. Das Semantic Parsing ist eine Aufgabe, bei der die Bedeutung von Benutzereingaben in Form von Abfragen aufgebaut wird, wie z.B. SQL und Lambda Calculus. Das Cross-Lingual Semantic Parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen zu übersetzen. Wie im Bild gezeigt wird, müssen wir die Abfrage in mehreren natürlichen Sprachen mithilfe neuronaler Modelle in SQL, Lambda oder FunQL usw. übersetzen. Gegenwärtig werden existierende Modelle für das Cross-Lingual Semantic Parsing separat vorgeschaffen und evaluiert auf Datensätzen von begrenzten Aufgaben und Anwendungen. Zum Beispiel gibt es eine große Abdeckung bestimmter natürlicher Sprachen, aber Chinesisch fehlt und es fehlt an Abdeckung bestimmter Bedeutungsdarstellungen. Das Lambda-Kalkül wird fehlgeschlagen, oder es werden nur auf bestimmte neuronalen Modelle evaluiert. Zum Beispiel gibt es nur ein einziges Modell, um sie zu evaluieren. Um all das zu überwinden, schaffen wir XSemPLR. Wir bieten eine einheitliche Datendarbete XSemPLR für das Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsdarstellungen an. Sie enthält 9 Datensätze in verschiedenen Domänen, 5 Aufgaben des Semantic Parsing, 8 Bedeutungsdarstellungen, und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser zu bewerten, betrachten wir sechs Ansatzpunkte für Training und Evaluation. Der erste Ansatzpunkt ist "Translate-Test". Wir verwenden die Google Translate-API, um die Quellsprache in die Zielsprache zu übersetzen, dann verwenden wir ein monolingual-Modell zum Trainieren und Evalrieren. Zum Beispiel trainieren wir das englische Modell auf englischen Abfragen und während der Inferenz übersetzen wir die deutsche Abfrage mit API ins Englische und verwenden dann das trainierte Modell, um die SQL zu vorhersagen. Wir werden auch den Monolingual-Modell-Test durchführen. In diesem Ansatzpunkt sind die Quellsprache und die Zielsprache dieselbe, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir werden auch den Monolingual-Few-Shot-Setting testen, bei dem wir monolingual-Modelle mit nur 10% des Trainingsdatensatzes trainieren. Wir werden auch den Multilingual-Modell-Test durchführen, bei dem wir einen multilingualen Modell für alle Sprachen trainieren. Zum Beispiel legen wir die deutschen, englischen, chinesischen Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Wir betrachten auch den Cross-Lingual-Zero-Shot- und Few-Shot-Transfer. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. Beim Training trainieren wir auf englischen Abfragen oder einer Kombination von englischen und deutschen Few-Shot-Abfragen, um ein multilinguales Modell zu trainieren, um die SQL-Ausgabe vorherzunehmen. Wir finden auch viele interessante Ergebnisse. In Bezug auf die Analyse von monolingual-Modellen evaluieren wir auf zwei Gruppen von Modellen, einschließlich Encoder-PTR, das steht für Multilinguelle vorhergesetzte Encoder mit Pointer-basierten Decoder, wie z.B. XLM-R + PTR und mBERT + PTR. Wir evaluieren auch Encoder-Decoder-Modelle, das steht für Multilinguelle vorhergesetzte Encoder-Decoder-Modelle, wie z.B. mBART und mT5. Wir finden, dass Encoder-Decoder das beste Leistungsresultat auf allen neun Datensätzen erreicht. Wir evaluieren auf mT5 und XLM-R + PTR im multilingualen Setting. Wir finden, dass Encoder-Decoder oder Encoder-PTR durch die Ausbildung in einer Menge verschiedener Sprachen verbessert werden können. Wir finden, dass es because die meisten Hauptnatürlichen Sprachen ein Leistungsungleichgewicht erhalten, außer beim englischen Modell, das in sieben Datensätzen abnimmt und in drei Datensätzen nur gewinnt. Ich denke, das ist bekannt als "Curse of Multilinguality". Wir vergleichen auch das Sprachübergreifende Leistungsungleichgewicht. In diesem Bild ist die blaue Linie der Sprachübergreifende Few-Shot-Übertragung. Die orange Linie ist die Sprachübergreifende Zero-Shot-Übertragung. Während die grüne Linie die Monolingual-Stelle ist. Wir finden, dass, wenn wir die grüne und orange Linie vergleichen, wir die Null-Shot-Stelle finden, die die Übergreifende Transfer-Leistungsungleichgewichte signifikant reduziert. Wenn wir die blauen und orange Linien vergleichen, finden wir, dass mit der Few-Shot-Stelle die Übertragungsungleichgewichte rapid reduziert werden. Wir finden auch einige weitere interessante Erkenntnisse. Zum Beispiel outperformt Encoder-Decoder das vorherige Werk oder erreicht vergleichbare Resultate. Das Vordrängen auf englische natürliche Sprachen kann das Leistungsresultat von Few-Shot auf Zielsprachen deutlich steigern, und wir finden multilingual Sprachmodelle wie Codex und BLOOM noch nicht ausreichend für die Aufgaben des Cross-Lingual Semantic Parsing. Insgesamt bauen wir XSemPLR, eine einheitliche Benchmark-Stelle für das Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und Bedeutungsdarstellungen. Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen von multilingualen Sprachmodellen durch. Unsere Ergebnisse zeigen viele interessante Erkenntnisse an. Und so weiter. Und erwarten Sie, uns auf unsere Publikation und Code zu besuchen. Vielen Dank für das Hören.</sample>
    <sample id="171">In der vorgestellten Arbeit wurden vorhandene Arbeiten in den Kategorien von Ansatz, Wasserzeichen und Verifikation überprüft. Es wurde festgestellt, dass diese Ansätze entweder nicht für die Einbettung als Dienst anwendbar sind oder die Transferierbarkeit fehlt. Daher wurde eine neue Methode vorgeschlagen, die "Embedding Marker" genannt wird.</sample>
    <sample id="172">Nein, Codex und Bloom sind noch nicht ausreichend für die Aufgabe der Mehrsprachischen Semantik-Semantic Parsing (CLSP).</sample>
    <sample id="174">Thea explains the unique features of the ArgAnalysis35K dataset, which includes 35,000 argument-analysis pairs sourced from high-quality debates and expert debaters. The dataset has a diverse range of arguments across 24 themes, with an emphasis on analysis rather than just claims or premises. It also introduces instance-based annotator reliability and a relevance model to capture the relevance of arguments to specific themes.</sample>
    <sample id="175">Die Methode mit der Mehrdeutigkeit der Permutationen wird durch die Induktion der Ausrichtung als Teil des Trainings bewältigt. Manchmal gibt es mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte Permutation ist latent. Um dies zu bewältigen, induzieren wir die Ausrichtung als Teil des Trainings und approximieren das finden der höchsten Punktschwelle mit einer GPU-friendlichen kontinuierlichen Relaxation, die auch die Möglichkeit bietet, durch die Lösung zu backpropagieren und linguistisch plausible Permutationen zu lernen.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert, indem man die Leitstelle der Aufmerksamkeit auf bestimmte Gruppen oder politische Ausrichtungen lenkt und prüft, ob das Modell in der Lage ist, faire Entscheidungen zu treffen, indem es beispielsweise eine bessere Erkennung von Diskriminierung oder Lügen für bestimmte Gruppen oder politische Ausrichtungen zeigt.</sample>
    <sample id="177">The presenter is Yanis Labrak.</sample>
    <sample id="178">The referent in the paper is Koustav Sinha.</sample>
    <sample id="179">This paper presents SymbolicToM, a method to improve Theory of Mind (ToM) reasoning skills in large language models (LLMs). It uses explicit graphical representations to efficiently answer false-belief questions. The method is tested with various LLMs and compared against supervised baselines. Results show significant performance gains across different datasets, including in-domain ToMi and out-of-domain setups. SymbolicToM also generalizes well to new datasets with more linguistic diversity, allowing stronger models like GPT-4 to fully solve the tasks.</sample>
    <sample id="180">The referent in the paper is Myra.</sample>
    <sample id="181">This paper introduces a method for constrained language planning, which involves generating step-by-step instructions for specific goals with multiple constraints. The authors evaluate the performance of large language models in generating these scripts and find that they often fail to guarantee faithfulness to the constraints. To improve this, the authors propose an over-generate-then-filter method that selects the most faithful scripts based on semantic similarity and keyword presence. They also create a dataset called CoScript, which contains 55,000 specific goals with scripts generated by large language models. This dataset can be used to train smaller but specialized models for constrained language planning, which can generate higher quality scripts than larger models.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Verbindung von bestimmten Attributen oder Merkmalen anhand von geografischen Lagen, klimatischen Bedingungen oder kulturellen Aspekten. In diesem Fall wird er verwendet, um Frauen von Latina- und asiatischer Ethnoität als "vibrant" und "curvaceous" zu beschreiben, was eine Form von Stereotypien und essentialisierenden Narrativen ist.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie Menschen mit den gleichen Anweisungen baten, eine Persönlichkeit zu beschreiben. Sie benannten dann das demografische Merkmal an und erhielten so eine breite Palette von Beschreibungen, die nicht von lexikalischen Ausdrücken abhängig sind.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextual Mutual Information) verwendet, um die Kontextnutzung von Maschinell übersetzten Modellen zu messen. CXMI misst, wie viel Informationen das Kontext für die Übersetzung des Ziels bereitstellt, gegeben den Quelle. UmCXMI wurde erweitert, um die Kontextnutzung bei Satell oder beim Wortlevel zu messen.</sample>
    <sample id="185">DrBERT und ChuBERT sind beide Medizinmodell-Modelle in der Französischen Sprache. Der Hauptunterschied zwischen den beiden Modellen besteht darin, dass DrBERT auf einem Datensatz namens NACHOS trainiert wurde, der Medizinmaterialien von der Web entlehnt hat, während ChuBERT auf anonymisierten Daten aus dem Nantes University Hospital-Datenlager trainiert wurde.</sample>
    <sample id="187">Es gibt zwei Autoren an der Arbeit beteiligt, Ying und Zhiyang.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Prozess, bei dem ein Modell von einem anderen Modell übernommen wird, das eine verwandte Aufgabe bearbeitet hat. In diesem Fall wurden die Aufgaben "Debaten" und "Erweiterung und Vergleich" verwendet, um das Modell für die Erkennung von Dissonanz zu verbessern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, die Verständnisfähigkeit von Sprachmodellen bei der Identifikation von indirekten Beziehungen zu entdeckten Entity-Bezeichnungen zu messen und zu verbessern. Der Datensatz enthält 6.000 alternative Fragen über drei Domänen (Musik, Bücher und Rezepte) mit 42.000 indirekten Beziehungen.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er eine große Menge an Daten an den EaaS sendet und die erhaltenen Einstellungen verwendet, um das Modell zu speichern und zu analysieren.</sample>
    <sample id="191">Es sind insgesamt 3 Autoren an der Arbeit beteiligt: Sara Papi, Matteo Negri und Marco Turchi.</sample>
    <sample id="192">The presentation introduces a new optimizer called CAME (Confidence-guided Adaptive Memory Efficient Optimization) designed to achieve fast convergence like traditional adaptive methods and low memory usage like memory-efficient methods. The presenter, Yang Luo, explains the challenges of existing optimizers like Adam and Adafactor in terms of memory usage and performance. The presentation then outlines the preliminaries, including non-negative matrix factorization (NMF), and how it can reduce memory requirements. The presenter also discusses two scenarios demonstrating erroneous updates in training deep neural networks and how they are handled. The proposed CAME optimizer is introduced as an efficient approach to decrease the side effect caused by insecure updating. The experiments conducted on BookCorpus, English Wikipedia, BERT, GPT-2, and T5 show that CAME achieves significant improvements over Adam and Adafactor, with a 3.4% increase in validation accuracy compared to Adafactor using the same number of training steps. The presentation concludes by highlighting the efficiency of CAME in large batch training and its effectiveness on downstream tasks.</sample>
    <sample id="193">Die Anzahl der Annotatoren, die verwendet wurden, um den ursprünglichen Datensatz zu erstellen, wird in dem gegebenen Text nicht erwähnt.</sample>
    <sample id="194">Die Autoren der Arbeit "NLPositionality" sind Assoziate von Carnegie Mellon University, University of Washington und Allen Institute for AI.</sample>
    <sample id="195">The presented work, titled "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering," introduces a novel framework called RoHT (Reasoning over Hierarchical Question Decomposition Tree) to address the challenges in Explainable Question Answering (XQA). XQA aims to provide not only the answer to a question but also an explanation for why that answer was selected. The current methods in XQA can be broadly categorized into neuro-symbolic methods and decompose-based methods. Neuro-symbolic methods translate natural language questions into formal representations like SPARQL, which can only be executed on structured knowledge bases (KBs), leading to limited recall of answers. Decompose-based methods generate intermediate steps in natural language to lead to the final answer, but they rely solely on free-text corpora as knowledge sources, making it difficult to integrate knowledge from heterogeneous sources.

RoHT addresses these limitations by proposing a two-stage framework. The first stage involves building a Hierarchical Question Decomposition Tree (HQDT) to understand the hierarchical compositional structure of complex questions. The HQDT consists of nodes representing the original complex question, sub-questions, and atomic questions that cannot be further decomposed. The second stage involves probabilistic reasoning over the HQDT to fuse knowledge from a knowledge base and a text corpus at different levels, taking into account the probability scores of both string generation and answering.

To build the HQDT, the framework uses a question decomposer to generate leaf loss (atomic questions), another question generator to create intermediate questions based on grouped leaf questions according to their reference tokens, and computes a certainty score for each node. The reasoning process is conducted recursively from the root to leaves, involving three steps for each node: determining appropriate knowledge sources, retrieving answers with probabilities from the selected sources, and aggregating candidate answers to output the top key answers with the highest probabilities.

The RoHT framework has been evaluated on two challenging complex QA datasets: KQA Pro and Musique. On the KQA Pro dataset, RoHT outperforms existing KB QA methods when using an incomplete KB and shows significant improvement when Wikipedia is added as a supplementary text corpus. RoHT also outperforms TransferNet, which is end-to-end trained with a mixed relation graph, demonstrating the superiority of explicit decomposition. On the Musique dataset, RoHT improves F1 by 11.9 compared to the SOTA method EX(SA) when using only the given text paragraphs. With both text and KB, RoHT-mix achieves remarkable performance improvements over TransferNet. The results indicate that supplementing text information with knowledge from KB provides additional benefits in enhancing the performance of RoHT.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite lautet "Marge read this absolutely fascinating book about bees yesterday." Hier ist der Begrenzer "yesterday" auf der linken Seite des Verbs "read" und die direkte Objektgruppe "this absolutely fascinating book about bees" auf der rechten Seite.</sample>
    <sample id="197">Der Stand der Technik für Dialogsysteme basiert auf Menschenbewertungen, bei denen menschliche Richter auswählen, welche von zwei Konversationen die bessere ist, oder sie auf einer Likert-Skala bewerten. Allerdings haben diese Ansätze Schwierigkeiten, um die Qualität von Dialogen auf einem feineren Niveau zu bewerten. ABC-Eval ist eine neue Methode, die das Evaluieren von Dialogmodellen auf einem detaillierteren Niveau vereinfacht und dabei die Subjektivität reduziert. Es misst die Häufigkeit von Fehlern wie Irrläufen, Widersprüchen und Verletzung von allgemein akzeptierten Kenntnissen, und es ist in der Lage, die Qualität von Dialogen anhand mehrerer Aspekte zu bewerten.</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil große Sprachmodelle heutzutage einen immer größeren Kontextwindow aufweisen. Es ist wichtig zu evaluieren, ob ein Modell die Akzeptanz von Sätzen über den gesamten Kontextwindow hinweg korrekt einschätzen kann.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in einigen Fällen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt. Dies wird als "Curse of Multilinguality" bezeichnet und ist für die meisten Hauptnatürlichen Sprachen bekannt.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="201">Für die Bewertung wurden neural MT-Metriken verwendet, einschließlich BLEURT.</sample>
    <sample id="202">DieRegression bei der Generalisierung auf bestimmte NER-Typen wird in dem Papier nicht explizit diskutiert. Es wird jedoch erwähnt, dass die Evaluierung auf verschiedenen NER-Typen durchgeführt wurde und dass die Befunde für die allgemeine Generalisierbarkeit relevant sind.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die Systematische Leistungsunterschiede von Technologien zwischen Bevölkerungen und die Aggregation von Urteilen und Meinungen von Menschen repräsentiert. Das kann dazu beitragen, dass NLP-Modelle bestimmte Gruppen besser oder schlechter abschätzen können, je nach den Perspektiven der Menschen, die an der Entwicklung beteiligt sind.</sample>
    <sample id="204">Die mehrsprachigen LLMs wie BLOOM wurden nicht durch Adapter oder eine vollständige Feinabstimmung angepasst.</sample>
    <sample id="205">The presentation by Shangbin, a PhD student at the University of Washington, focuses on the political biases present in language models and their impact on downstream tasks. The speaker explains that language models are trained on large-scale web crawl data, which includes diverse perspectives from various news media outlets, some of which have inherent social biases. These biases can propagate through the training process and affect the performance of language models on downstream tasks such as hate speech detection and fake news detection.

To investigate this issue, the speaker proposes to evaluate the political leaning of language models and assess how pretraining data influences these biases. The study uses political questionnaires like the political conference test to prompt language models with different formats and evaluates their performance on downstream tasks. Preliminary results show that language models exhibit varying political leanings, with GPT-4 being the most liberal model. The study also explores the extent to which language models pick up political biases from training data by pretraining them on partisan corpora.

The presentation highlights the dilemma of sanitizing political opinions in language model training data to avoid bias propagation while avoiding censorship or exclusion. The speaker emphasizes the need to acknowledge and address the fairness issues resulting from language model political leanings, particularly in applications involving hate speech and misinformation detection.</sample>
    <sample id="206">Sie verwenden ein Modell, das von zwei verschiedenen Aufgaben übertragen wurde: diskussionsunabhängige Stance-Stellungsklassifizierung (Debatte) und binäre Klassifizierung der Ausweich- und Vergleichsklassen (CE). Sie finden, dass die zero-shot-Performanz auf der annotierten Datensatz already viel besser als Zufall ist, mit dem besten AUC 0,62.</sample>
    <sample id="207">Die besten übersetzten Testsets wurden verwendet, um die PaLM-Fähigkeiten zu bewerten.</sample>
    <sample id="208">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen: 1. Forscher sollten positive Stereotype und essentialisierte Geschichten adressieren. 2. EineIntersectionsoption zur Studie von Biases und Schäden sollte verwendet werden, um Dinge zu übersehen, die bei einer nicht-Intersectionsoption übersehen werden könnten. 3. Es sollte mehr Transparenz über die Methoden zur Bekämpfung von Bias gegeben werden, um zu verstehen, ob positive Stereotype durch einen "overly-excessive" Wertealltag oder andere anti-Stereotypemethoden verursacht werden.</sample>
    <sample id="209">The paper does not provide specific information about the gain of the proposed method compared to the strongest baseline.</sample>
    <sample id="210">The presenter's name is Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können alsBenchmark verwendet werden.</sample>
    <sample id="212">In der Arbeit wird experimentiert mit einem kleinen Modell, das auf CoScript optimiert wurde. Es wird behauptet, dass das kleine Modell besser als die meisten großen Modellgeneratoren scripts generieren kann, wenn es auf eine geeignet trainierbare Datensammlung optimiert wird.</sample>
    <sample id="213">Der Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung ist OFA, ein vereinbares multimodales vortrainiertes Modell.</sample>
    <sample id="215">In this talk, Adam Przepiórkowski discusses the dependency structures of coordination in linguistics. He explains that different theories and approaches assume asymmetric or symmetric structures for coordinating phrases like "Lisa, Bart, and Maggie." The universal dependencies and Mel'čuk's meaning text theory assume an asymmetric structure where the first conjunct is the head of the whole coordinate structure. In contrast, the Prague approach and Hudson's Word Grammar assume a symmetric structure where all conjuncts are heads of the coordinate structure.

Przepiórkowski argues for symmetric structures based on the principle of dependency length minimization. He illustrates how direct objects prefer to be close to the verb, while adjuncts may be further away. However, when the direct object is long, it can be moved after the adjunct, satisfying the principle of dependency length minimization. He also presents statistics from the enhanced version of the Penn Treebank, confirming that left conjuncts tend to be shorter when the governor is on the left or absent. This tendency disappears when the governor is on the right.

Overall, the talk provides evidence against asymmetric structures of coordination and in favor of symmetric structures, based on dependency length minimization and statistical analysis.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications introduces a method for generating controllable dialogue with multiple attributes. The authors propose a Disentangled Controllable Generation (DCG) model that learns attribute concepts from seen values and uses disentanglement loss to disentangle different attribute combinations. They also introduce a unified reference-free evaluation framework (MAE) for evaluating the performance of their method on different granularities of attributes. The authors establish two benchmarks and prove the effectiveness of their method and evaluation metrics through experiments. The results show that their DCG outperforms all other baselines in attribute controllability and text equality, and it successfully tackles the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC.</sample>
    <sample id="218">Die Autoren dieser Studie sind von Google Translate.</sample>
    <sample id="219">The presentation introduces a research work titled "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports" by Jia-Huei Ju and colleagues. The goal of this work is to analyze financial reports, specifically the Form 10-K required by the SEC, to uncover useful information. The authors observed that the words in company reports are very similar, with about 80% of tokens being the same, but the content varies yearly. To address this challenge, they propose a highlighting task and a multi-stage pipeline.

The highlighting task involves comparing and contrasting the context between target and reference reports (the report of interest and the previous year's report). The goal is to find the rationale or important words that explain the relations between specific pairs of segments. The model predicts word importance, allowing for performance measurement.

The proposed pipeline consists of four stages: document segmentation, relation recognition, out-of-domain fine-tuning, and in-domain fine-tuning. Stage 0 (document segmentation) is not discussed due to time constraints. In stage 1, pairs are classified into three types based on syntactic and semantic similarities. Stage 2 and 2+ involve fine-tuning using an external dataset (eSNLI) and revised pairs, respectively. The evaluation uses two metrics: precision and PCC (prediction-correlation coefficient).

The domain-adaptive highlighting model achieved the best performance on the FINAL dataset and maintained generalization capability. The methods also showed effectiveness in handling mismatched pairs, which were not used during training. Future works include improving effectiveness, adding more features, and exploring other techniques in information retrieval to enhance application.

For more details, refer to the paper and GitHub repository provided by the researchers.</sample>
    <sample id="220">Die Autoren, Vasudha und ihre Kolleginnen, gehören an der Stony Brook University.</sample>
    <sample id="221">Die Arbeit hat sich auf die Übersetzung von Deutsch ins Englische konzentriert.</sample>
    <sample id="222">This work investigates the challenges and interventions for domain adaptation in open-domain question answering. The authors propose three main contributions: investigating different data interventions, identifying the type of dataset shift a new domain exhibits, and determining what kind of data interventions are effective for a specific type of shift. They use a general-purpose Wikipedia-based source domain to train both retriever and reader models and test the generalizability of the source model on seven target passage and datasets spanning across six different domains. They observe that few-shot methods improve retriever performance by 8% on average and reader performance by 11% on average. They also identify the nature of incompatibility the target model and domain exhibit using existing data shift taxonomy in machine learning. Finally, they find that all target sets respond well to few-shot adaptations as they use a few examples from target domain while datasets with concept and covariate shift respond well to zero-shot adaptations as well.</sample>
    <sample id="223">The referent in the presentation is Shangbin, a PhD student at the University of Washington.</sample>
    <sample id="224">Während der Experimente wurden die Modelle Long-mBART und normaler mBART untersucht. Long-mBART wurde verwendet, um Dokumentebene-Simplifikationen zu erzeugen, während normaler mBART verwendet wurde, um Satelbene-Simplifikationen zu erzeugen.</sample>
    <sample id="225">Für die Training- und Testdaten werden jeweils 53 Aufgaben aus 9 Gruppen verwendet.</sample>
    <sample id="226">Es werden zwei Autoren, Regina Stodden und Omar, an der Arbeit beteiligt.</sample>
    <sample id="227">The paper discusses the challenge of grounded language understanding, which involves mapping natural language expressions into executable plans or programs in specific environments. The main reason for this challenge is the lack of grounding during pre-training in most language models. The proposed framework, named Pangu, focuses on discrimination instead of generation, where a symbolic agent proposes candidate plans and a language model scores and ranks them. This approach allows the language model to excel in discrimination tasks without handling the validity and grammar of the target plan. The framework is generic and can be applied to various grounded language understanding tasks, such as knowledge-based question answering. Pangu achieves outstanding performance across different settings, including fine-tuning and in-context learning, and demonstrates strong sample efficiency. The paper also highlights that autoregressive models like ArcaneQA tend to overfit seen structures during training, while Pangu's strong robustness under non-i.i.d settings might explain its strong generalizability.</sample>
    <sample id="228">Die Autoren haben Experimente auf vier Datensätzen durchgeführt: AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth present their joint work on detecting improvable claims for argumentative writing support. The paper focuses on the challenges of modeling the quality of argumentative text based on implicit revision patterns found in collaborative online debate platforms like Kialo. Four main challenges are explored: representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors conclude that revision-based data can be effectively employed for detecting suboptimal claims and that modeling the distance between two claimed versions is beneficial. Contextual information's impact depends on the task and the quality issues a text suffers from.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from the web, which was used to train DrBERT, a robust pre-trained model in French for biomedical and clinical domains.</sample>
    <sample id="232">Der Referent in diesem Review ist David Vilar.</sample>
    <sample id="233">Abstract: This paper presents a novel approach for simultaneous speech translation (SimulST) that leverages existing offline speech-to-text (ST) models without re-training or adopting specific architectures. The proposed Encoder-Decoder Attention (EDAtt) strategy decides whether to emit partial translations based on the attention mechanism between audio input and textual output. EDAtt considers cross-attention weights to determine if a word should be emitted, using a threshold alpha and a window of lambda speech frames. The results show that EDAtt outperforms popular strategies applied to offline models in terms of translation quality and latency, while also being computationally efficient. The paper includes graphs comparing EDAtt with Wait-k, Local Agreement, and state-of-the-art SimulST architecture tailored for pre-translation. Open-source code, models, and simultaneous output are released to facilitate reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Leistungen von LLMs bei der Übersetzung. In einem einfachen Experiment wurden zwei verschiedene Prompts für jede Satzfolge verwendet, und es wurde beobachtet, dass die meisten Sätze (516 von 1000) einen Unterschied von mehr als einem BLEURT-Punkt hatten. In extremen Fällen kann dieser Unterschied bis zu 40 BLEURT-Punkte betragen. Es ist wichtig, eine gute Prompt-Strategie auszuwählen, um die Leistung zu optimieren.</sample>
    <sample id="235">I'm sorry, I cannot answer this question as the information about the university affiliations of the authors is not provided in the given text.</sample>
    <sample id="236">Die 5 Anweisungen der Expert*innen sind für jede Aufgabe im MultiInstruct-Dataset enthalten und dienen dazu, dieUnified-Sequence-to-Sequence-Formulierung der Aufgaben zu vereinbaren.</sample>
    <sample id="237">Die Autoren schlagen einen Diagnostik-Test für die Integrierung von Kenntnis vor, der eine Kernreferenz Auflösungs Aufgabe verwendet, um das Modell zu überprüfen, ob es in der Lage ist, Kenntnis aus verschiedenen Quellen zu verwenden. Sie definieren drei Einstellungen: "Hintergrund-Training" (Background-Pretrain), "Hintergrund-Beide" (Background-Both) und "Hintergrund-Inferenz" (Background-Inference). In den verschiedenen Einstellungen wird unterschiedliche Verfügbarkeit von Fakten kontrolliert, um festzustellen, ob das Modell in der Lage ist, Kenntnis aus verschiedenen Quellen zu verwenden.</sample>
    <sample id="238">The video introduces MeetingBank, a new benchmark dataset created by Yebowen Hu from the University of Central Florida. The dataset aims to address the challenges of developing summarization technologies for different reading domains, particularly in the context of meetings. MeetingBank includes 1,366 City Council meetings with nearly 7,000 instances, featuring meeting transcripts, reference summaries, and other useful resources. The dataset was collected using Speechmatics API for audio-to-transcript conversion and by identifying meeting details from official websites. The video also discusses the evaluation of various summarization systems, including extractive and abstractive models, and highlights the importance of developing new automatic evaluation metrics that align with human preferences.</sample>
    <sample id="239">Hallo alle, mein Name ist David Vilar und ich werde einen kurzen Review der Publikation "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dieses Papier wurde in collaboration mit Kollegen von Google Translate erstellt. PaLM ist ein 540 Milliarden-Parameter-großer Sprachmodell, das im Jahr 2022 vorgestellt wurde. Es wurde auf einem großen Datensatz von Texten trainiert, der 780 Milliarden Token umfasst. Bei der Veröffentlichung erreichte es das aktuelle Leitstelle in Hunderten von NLP-Aufgaben. In diesem Werk präsentieren wir die erste systematische Studie über die Anwendung von großen Sprachmodellen zur Prompting für maschinelles Übersetzen. Wir evaluieren die Übersetzungs capability solcher Modelle unter Verwendung der besten üblichen Praktiken im MT-Bereich. Das bedeutet, dass wir die neuesten Testdaten verwenden, um einenOverlap mit den Trainingsdaten des Sprachmodells zu vermeiden. Wir vergleichen auch mit den besten performierenden Systemen, so dass der beste performing System, also der WMT-Evaluation, abgeglichen wird. Wir verwenden die neuesten neuronalen MT-Metriken und zeigen auch Expertenbasierte menschliche Evaluierungsresultate an. Schließlich bieten wir einige Empfehlungen für die Selektion von Prompt-Strategien an. Das Prompting hat einen großen Einfluss auf die Leistung von LLMs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir zwei verschiedene Prompts für jede Satzprozedur verwendet haben. Der Mehrheitsanteil der Sätze, 516 von 1000, weist einen Unterschied von mehr als einem BLEURT-Punkt auf. Und dies kann in extremer Form bis zu 40 BLEURT-Punkten betragen. Es ist wichtig, eine gute Prompt-Strategie zu auswählen. In unseren Experimenten haben wir uns für eine 5-Shot-Prompting-Strategie entschieden, bei der wir jede Satzprozedur mit dem Sprachtyp markieren. So beispielsweise, wenn wir eine Übersetzung von Deutsch ins Englische durchführen, werden die Quellsätze (Quelle) mit einem Doppelpunkt markiert und die englischen Übersetzungen (Übersetzung) mit einem Doppelpunkt. Wir haben festgestellt, dass die Form der Prompting-Wahl im Fall mehrerer kurzer Promptings keine große Bedeutung hat. Es ist für zero- und one-Shot-Promptings crucial, dass die Beispiele hochwertige Übersetzungen enthalten. Wenn wir also, wie in unserem Fall, zu einem 5-Shot-Prompting übergehen, macht es keinen großen Unterschied, ob die Form der Prompting-Wahl verwendet wird. Es sind die Beispiele, die den Hauptanteil tragen. Die Auswertung unseres experimentellen Resultats zeigt, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zu den Quellsätzen. Es ist also wichtig, die Beispiele aus hochwertigen Übersetzungen zu auswählen. Im Detail vergleichen wir die Selektion von Prompts aus dem Trainingsdatensatz mit der Selektion von Prompts aus den WMT-Evaluation-Daten. Die Dev-Daten sind viel präziser und von höherer Qualität als die Trainingsdaten, die noisierter sind. Und die Resultate zeigen, dass ein besseres Performance erreicht wird, wenn die Dev-Daten verwendet werden. Trotzdem haben spezialisierte state-of-the-art-Systeme einen erheblichen Vorteil über die PaLM Übersetzungen. Dennoch kommt PaLM relativ nah an einem kommerziellen System heran. In unserem Fall haben wir Google Translate verwendet, um die Evaluation zu durchführen. Die Einsichten, die wir durch die menschliche Evaluation gewonnen haben, die wir mit dem MQM-Framework durchgegangen sind, zeigen, dass die Fluideität von PaLM vergleichbar mit state-of-the-art-Systemen ist, aber die Hauptunterschiede in der Genauigkeit liegen. Insbesondere die häufigsten Fehltrügerien sind Omission-Fehler. Es scheint, dass PaLM Entscheidungen trifft, um eine besser klingende Übersetzung zu produzieren, indem es Teile der Quellsätze beim Übersetzen weglässt. Allerdings ist die "Stil/Ausdruckslosigkeit" Kategorie für PaLM tiefer als bei state-of-the-art-Systemen, was ein weiterer Hinweis darauf ist, dass PaLM wirklich fluide Ausgaben produziert, aber immer noch einige Probleme mit der Genauigkeit hat. Und das war's für diese kurze Übersicht.Für weitere Details bitte zu der vollständigen Präsentation des Papiers kommen. Vielen Dank.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Saarland University in Deutschland. In diesem Video möchte ich mein jüngstes Werk präsentieren: "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Dieses Werk wurde in Zusammenarbeit mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow erstellt. Ich werde beginnen mit einer kurzen Einführung in dieWeak Supervision und weakly supervised learning. In derWeak Supervision werden die Daten nicht manuell überprüft. Stattdessen werden die Daten mithilfe von schwachen Labelquellen, wie einfachen heuristischen Regeln, Knowledgesbasen oder von Lowsourcing, überprüft. Wenn wir das Vergleich mit menschlichenAnnotations machen, sind die schwächeren Annotations weitaus billiger, aber sie sind auch noisier, was bedeutet, dass ein gewisses Maß an Fehlern enthalten ist. Wenn wir direkte Neuralnets auf schwächer gelabelten Daten trainieren, memorieren die Neuralnets die Label-Noise und generalisieren nicht. In weakly supervised learning werden Trainingsalgorithmus vorgeschaffen, um robuste Neuralnets unter solcher Label-Noise zu trainieren, sodass die trainierten Modelle immer noch gut generalisieren. In jüngster Zeit in WSL, alsoWeakly Supervised Learning, wird oft behauptet, dass Menschen nur auf schwächer gelabelten Daten Modelle trainieren und dabei hohen Leistungen auf sauberen Testsets erreichen. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken, der ist, dass man extra saubere Validationsdaten benötigt, um das Modell auszuwählen. Wir können auf diesem Problemsetting nicht stehen, aber dies impliziert, dass extra manuelle Annotierungen notwendig sind. Wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die obige Zweifel wird an drei Forschungsfragen gekoppelt. Erstens, ist eine saubere Validationsdaten notwendig für WSL oder können wir eventuell eine noisige Validationsdatenbank verwenden? Zweitens, wenn saubere Daten erforderlich sind, dann, ob sie notwendig sind, oder sind sie für WSL unerlässlich? Drittens, sollten wir nur die sauberen Samples für die Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen? Wir haben diese Forschungsfragen in unserem Werk adressiert und unsere Erkenntnisse lauten folgendermaßen. Erstens finden wir, dass, überraschenderweise, recent WSL methods tatsächlich benötigen saubere Validationsdaten, um korrekt zu arbeiten. Sonst gibt es einen großen Performance-Schwank. Wie im Bild gezeigt, wenn es keine sauberen Validationsdaten gibt, dann können die trainierten Modelle nicht darüber hinaus beyond den ursprünglichen schwachen Labels generalisieren, was bedeutet, dass die Trainingsschritte sinnlos sind. Das zeigt, dass WSL-Methoden tatsächlich saubere, manuell überprühte Datensätze benötigen, um korrekt zu arbeiten, und die Kosten für die Obtention von sauberen Validationsdaten sollten nicht übersehen werden. Unsere zweite Erkenntnis lautet, dass die Anzahl der sauberen Validationsdaten, die wir verwenden, helfen wird, die Performance von WSL-Methoden zu verbessern, wie im Bild gezeigt. Typischerweise benötigen wir 20 Samples pro Klasse, um eine hohe Performance zu erzielen. Aber das ist nicht alles, denn wenn wir doch beschließen, auf saubere Samples zuzugreifen, dann wird die Direkt-Finetuning-Methode, die direkter auf den sauberen Datensätzen angewendet wird, besser performieren. Das Bild rechts zeigt die Performanceunterschiede zwischen Direkt-Finetuning-Methoden, die direkt auf den sauberen Datensätzen angewendet werden, und WSL-Methoden, die die sauberen Datensätze nur zur Validierung verwenden. Wie wir sehen können, wenn wir 10 Samples pro Klasse haben, beginnt Direkt-Finetuning, die auch als Fine-Tuning bezeichnet wird, zu übertrumpfen. Schliesslich können die Performanceverbesserungen, die in früheren WSL-Methode behauptet wurden, leicht durch die Kontinuierliche Fortsetzung des Fine-Tunings auf den sauberen Datensätzen erreicht werden. Wie wir im Bild sehen können, der "vanilla"-Model, beziehungsweise FTw, ursprünglich unterperformiert gegenüber komplexeren WSL-Methoden, wie COSINE. Allerdings, wenn wir die Kontinuierliche Fortsetzung des Fine-Tunings auf den sauberen Datensätzen erlauben, dann performiert FTw ebenso gut wie andere Methoden. Also in der Praxis gibt es keinen Grund, komplexere WSL-Methoden zu verwenden, die mehr Rechenzeit und Speicherplatz benötigen. Umzusummen, wir haben gezeigt, dass recent WSL-Methode saubere, manuell überprühte Datensätze benötigen, um korrekt zu arbeiten. Ihre Performance Verbesserung und Praktischkeitsfähigkeit werden stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten lauten folgendermaßen: Erstens berichten Sie über die Modellauswahlkriterien. Zum Beispiel, berichten Sie, ob die Modellauswahl über saubere Validationsdaten durchgeführt wurde. Zweitens, WSL-Methode sollte mit Few-Shot-Learning-Baselines verglichen werden, da beide auf sauberen Datensätzen arbeiten. Drittens, die Kontinuierliche Fortsetzung des Fine-Tunings ist eine einfach yet starke Baseline, die in zukünftiger Arbeit in WSL berücksichtigt werden sollte. Endlich haben wir unser Code offengepolt. Sie können es über den QR-Code auf dem Slide finden. Bitte prüfen Sie es gerne aus. Vielen Dank und bis bald auf dem Kongress.</sample>
    <sample id="241">The paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" by Ethan and colleagues proposes a framework for evaluating misinformation detection systems that better reflects real-world conditions. The authors argue that current systems are often evaluated using retrospective datasets, which can lead to issues like leaked counter-evidence, and they lack human-centricity, failing to involve humans at various stages of the process. To address these deficiencies, the paper introduces an end-to-end system that integrates human feedback throughout the process. The system consists of two main components: claim detection and policy violation verification. For claim detection, the system uses keyword filtering, a T5 model for question answering, and Fisher's Exact Test for ranking claims. For policy violation verification, a BERT-based stance classification model is used to determine the author's stance towards unapproved treatments. The evaluation focuses on early detection of unapproved treatments before their appearance in debunking news articles and the efficacy of policy violation verification. The results show that the system has a high detection rate of 65% and can detect 124.2 policy violations per human hour worked. The paper aims to provide a more realistic evaluation framework for future human-in-the-loop misinformation detection systems and offers insights into the development and evaluation of such systems.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind die Anwendung von Likert-Skalen und das Anfragen von menschlichen Urteilen, um zu bestimmen, welche der beiden Konversationen besser war.</sample>
    <sample id="243">There are five authors involved in the work presented by Jenny.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen verwendet, dass Servin ein Richter ist und Kea ein Bäcker.</sample>
    <sample id="245">The presentation by Lining Zhang and co-authors presents a study on identifying high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The researchers developed a two-step pipeline to categorize workers into gold, silver, bronze, and block based on their ability to evaluate multiple dimensions correctly and handle heavy workloads. The study found that the pipeline workers achieved high agreement in terms of inter-annotator agreement (IAA) compared to experts, with Cohen's Kappa values ranging from 0.443 to 0.534. The baseline MTurk workers and CloudResearch MTurk workers were also analyzed, showing varying levels of agreement. The study concludes that pre-task filtering can help avoid waste of time and resources while maintaining high agreement at a lower cost, and suggests future research directions to improve worker quality and correctness.</sample>
    <sample id="246">Ja, der Code ist verfügbar und er ist auf GitHub zu finden.</sample>
    <sample id="247">The paper presents a new task called Knowledge Graph-Based Fact Verification, which utilizes knowledge graphs as evidence for fact verification. The authors propose a new dataset, FactKG, that includes claims in both written and colloquial styles, with labels of SUPPORTED or REFUTED. The task involves retrieving evidence from DBpedia and verifying the claim using the evidence. The paper introduces five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The authors construct baselines in two ways: Claim Only baselines use only the claims to verify, without graph evidence, and the GEAR model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">Ja, die Annotatoren für NLPositionality sind in Bezug auf jede demographische Gruppe, d. h. Land, Geschlecht usw., ausgewogen.</sample>
    <sample id="249">To recreate longer sequences, the researchers extracted grammatical sentences from Adjunct Island and added them as a prefix to both the acceptable query and the unacceptable query.</sample>
    <sample id="250">Eine dimensionale Bewertung ist ein Ansatz, um die Qualität von Dialogen auf mehreren Aspekten zu bewerten, anstatt lediglich eine allgemeine Bewertung des Dialogs zu erhalten. Dadurch können die Stärken und Schwächen des Modells auf einem feinere Niveau analysiert werden.</sample>
    <sample id="251">The authors belong to the University of Science and Technology of China.</sample>
    <sample id="252">The presentation introduces the work "U-CREAT: Unsupervised Case Retrieval using Events extrAcT" by Sai Kiran Tanikella and colleagues. The team addresses the challenge of Prior Case Retrieval (PCR) in the legal domain, where lawyers and judges traditionally rely on their experience to cite relevant past precedents. However, with the increasing volume of cases, this task becomes increasingly difficult. The U-CREAT pipeline leverages unsupervised learning techniques and an event-based approach for PCR tasks, demonstrating high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. The presentation highlights two key contributions: the IL-PCR dataset, a new benchmark for PCR tasks, and the U-CREAT pipeline. The team conducted experiments using diverse models, including count-based, transformer-based, and event-based models, and found that event-based models significantly outperform baseline methods. The Event Filtered Documents model is the best-performing model, achieving lower inference times and higher F1 scores compared to other techniques. The presentation concludes that U-CREAT opens up avenues for further exploration and development in the field of prior case retrieval.</sample>
    <sample id="253">The presented work, "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media," is a collaborative effort by researchers from Mexico and Spain. The study aims to contribute to the detection of mental health disorders by automatically analyzing social media posts. The approach involves using domain adaptation to improve the performance of a model on a target domain with insufficient annotated data. By integrating information from Reddit and mental health, the model learns the specific language and task of mental health disorders. The results show that DisorBERT outperforms other models, including MentalBERT, which was trained with a large amount of data. Future work plans to explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">The research work presented is titled "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" by Sun Qi from Nanjing University of Science and Technology. The paper addresses the challenge of label noise in distant supervision (DS) data used for document-level relation extraction, where false-positive pseudo labels can lead to incorrect relations being extracted. To mitigate this issue, the authors propose a framework that incorporates uncertainty estimation to determine the reliability of model predictions and a re-labeling strategy with dynamic class uncertainty thresholds to filter out high-uncertainty pseudo labels. They also introduce a multi-phase training strategy to iteratively improve the performance of the DocRE model using DS data. The framework is compared with several strong baselines on public datasets, demonstrating superior performance.</sample>
    <sample id="255">Die Form des Prompts ist wichtig, wenn es um null- oder ein-Shot-Prompting geht. In solchen Fällen hat die Art und Weise, wie das Prompt formuliert wird, einen großen Einfluss auf die Leistung der LLMs für die Übersetzung.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art Dialogmodelle evaluiert.</sample>
    <sample id="258">In this video, Chiang Cheng-Han introduces a new work titled "Can Large Language Models Be an Alternative to Human Evaluation?" The research proposes using large language models to evaluate the quality of text in natural language processing. The models are given instructions and samples to rate based on attributes such as grammar, coherence, likability, and relevance. The experiment compares the results of large language model evaluations with human evaluations conducted by English teachers. The study finds that some large language models, like Davinci and ChatGPT, show a clear preference for human-written text, indicating their potential as alternatives to human evaluation. The video also mentions other questions addressed in the paper, such as agreement between large language models and human evaluators, the impact of instruction wording changes, and the benefits and costs of using large language models compared to human evaluation.</sample>
    <sample id="259">The presentation introduces XSemPLR, a unified benchmark for cross-lingual semantic parsing in multiple natural languages and meaning representations. The authors propose a dataset containing 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. They evaluate their benchmark on six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer. The results show that Encoder-Decoder models outperform previous work or achieve comparable results, and pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages. The authors also find that multilingual language models such as Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="260">Es wird in der Präsentation nur ein Autor, Jingwei Yi, erwähnt. Es ist daher unwahrscheinlich, dass mehrere Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="261">Ein guter Planer sollte Skripte erstellen, die vernünftig sind und den Anforderungen und Beschränkungen von Zielen nachkommen.</sample>
    <sample id="262">Der englische Inhalt gibt die Anzahl der Autoren nicht an.</sample>
    <sample id="263">The paper presents a work on mitigating label biases for in-context learning. In-context learning is a popular paradigm for utilizing large language models, but it is known to be unstable due to various design choices such as the choice and order of in-context examples. These design choices introduce biases to the model's predictions. The paper proposes a novel calibration method to handle all types of biases. The authors conduct experiments to confirm that the task corpus can actually bias the model's predictions. They propose domain-context calibration, which uses random in-domain words sampled from the task corpus as content-free text to estimate the model's bias on each of the label names and then use this estimated bias to calibrate the model's original predictions. The results show that domain-context calibration improves significantly the average performance of in-context learning on this dataset.</sample>
    <sample id="264">The presentation introduces a new task called Transferable Audio-Visual Text Generation (TAVT) for multimodal text generation, addressing the challenge of domain shifts in visual and audio content. The proposed framework consists of an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. TAVT outperforms state-of-the-art models on cross-datasets and cross-domain settings, even with low-resource domains. Ablation experiments demonstrate the impact of audio features on performance.</sample>
    <sample id="265">The referent is Vasudha.</sample>
    <sample id="266">The authors of the paper are affiliated with the University of Edinburgh.</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, also wenn das Modell Teile des Quelltextes beim Übersetzen weglässt.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute erzählen wir über ABC-Eval, eine neue dimensional-basierte Methode zur Beurteilung von conversational AI. Dieses Werk wurde vom Emory NLP Lab, geleitet von Professor Jinho Choi an der Emory University, in Zusammenarbeit mit Alexa AI erstellt. Lassen Sie uns sagen, dass Sie recently eine Dialogmodellentwicklung getan haben und wissen möchten, wie gut es sich gegenüber dem aktuellen Stand der Technik vergleicht. Eine gemeinsame Praxis besteht darin, menschliche Evaluationen durchzuführen, indem man menschlichen Richtern fragt, welche der beiden Konversationen besser ist oder sie auf einer Likert-Skala bewertet.Diese Ansätze sind gut, um einen allgemeinen Eindruck vom Dialogqualitätsniveau zu erhalten, aber Dialogqualität hat viele Aspekte. Daher möchten Sie eventuell mehrere Dimensionen des Chats qualitativ beurteilen, um die Stärken und Schwächen des Modells auf einem feinere Niveau zu verstehen. Ein Ansatz besteht darin, einfach menschlichen Richtern zu beordern, mehrere Dimensionen des Dialogqualitätsniveaus zu bewerten, indem man befragt, ob bestimmte Verhaltensweisen im Modell vorkommen, z.B., ob das Modell irrelevantes Information liefert oder sich selbst widerspricht. Wir glauben, dass es eine präzisere und zuverlässigere Strategie zur dimensional-basierten Evaluierung von Dialogen gibt. Unsere Ansatz versucht, die Subjektivität menschlicher Evaluationen explizit zu kennzeichnen, indem man feststellt, ob ein Modellresponse bestimmte Verhaltensweisen ausdrückt, wie z.B., relevantes Information liefern oder sich selbst widersetzen. Wir nennen diesen Ansatz annotieren Verhaltensweisen im Chat (ABC-Eval) und haben ihn entwickelt, um umfassend die Verhaltensweisen zu abdecken, die in jüngster Zeit als beeinflussend für den Chatschwierigkeitsgrad angesehen wurden. ABC-Eval ist in der Lage, die Häufigkeit zu messen, in der ein Chatschwierigkeitsgrad ein Modell einnimmt, indem es verschiedene Themenfehler misst. Zum Beispiel misst ABC-Eval die Anzahl der Runden, in denen ein Chatschwierigkeitsgrad ein Modell ignoriert, etwas Irrelevantes sagt, sich selbst widersetzt oder seine Partner widersetzt, falsche Fakten ausstreckt oder gegen allgemein anerkannte Wissen verstößt, und ob das Modell empathisch ist oder nicht. Um zu bestimmen, welche Art von Evaluation am besten geeignet ist, haben wir vier aktuelle Chatschwierigkeitsgrade ausgewählt und sie auf 100 menschlich-bot-Dialoge pro Modell evaluiert, die mit ABC-Eval bewertet wurden. Als Vergleich haben wir auch diese Dialoge unter drei existierenden Methoden bewertet: Likert-Bewertungen auf der Runde-Ebene, Likert-Bewertungen auf der Dialog-Ebene und Dialog-Ebene-Paarweisungen.Für jede dieser existierenden Methoden haben wir Bewertungen auf acht von den acht am häufigsten gemessenen Aspekten des Dialogs gesammelt, da dies standardmäßig für die Evaluation von Chatschwierigkeitsgrades auf mehreren Dimensionen getan wird. Aus our analysis of these evaluation results haben wir festgestellt, dass die ABC-Eval-Behaviorschwierigkeitsgrade im Allgemeinen zuverlässiger sind als die Schwierigkeitsgrade, die von existierenden Methoden gesammelt wurden, was durch die Inter-annotator-Übereinstimmung auf 100 doppelt bewerteten Dialogen geprägt wurde. Darüber hinaus sind die ABC-Eval-Schwierigkeitsgrade besser vorhersagbar als die von existierenden Methoden generierten Metriken, was durch eine einfache lineare Regression gezeigt wurde. Zum Beispiel können Sie sehen, wie die Messung der Häufigkeit von Runden, in denen ein Chatschwierigkeitsgrad ein Modell sich selbst oder seinen Partner widersetzt, 5% und 10% des Dialogqualitätsniveaus erklären, respectiv, während die durchschnittliche Likert-Konsistenzwerte nur 4% oder weniger erklären. Schließlich haben wir überprüft, ob jede Bewertungsmethode ein eindeutiges Aspekt des Chatschwierigkeitsgrades abdeckt, indem wir eine schrittweise lineare Regression durchführten. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% des Dialogqualitätsniveaus erklärt, und wenn Sie eine nach der anderen entfernen, resultiert die Entfernung in einer Verlust von einer beachtenswerten Menge an Informationen über die Qualität. Auf der anderen Seite erklären die Kombination aller Turn-Level-Likert-Metriken viel weniger über die Qualität und fewer davon tragen ein eindeutiges Information. Diese zuverlässigen, informativen und eindeutigen ABC-Eval-Metriken ermöglichen uns, conversational AI mit einer Härte zu bewerten, die vorherige Methoden erreichen können. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und exakt quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, around 20% ihrer Antworten falsche allgemein anerkannte Wissenstötze enthalten. Sie produzieren irrelevantes Information in around 15% der Antworten, und sie widersetzen sich sich selbst oder ihren Partnern around 10% der Zeit. Mit der raschen Fortschrittsrate im Feld können diese Fehlerraten bei neuen Modellen, die seit unserem Evaluationsprototypen freigegeben wurden, abnehmen. Aber das ist noch mehr Grund, zu verfolgen, zuverlässige und präzise Evaluiermethoden für die Vergleich der Modelle zu finden. Wir hoffen, ABC-Eval kann von anderen im Feld verwendet werden, um einen wichtigen Schritt in diese Richtung zu machen. Und wir freuen uns darauf, zu sehen, wie conversational AI im kommenden Monat und Jahr fortschritt. Vielen Dank für die Ansicht.</sample>
    <sample id="270">Die Autoren dieser Studie, ABC-Eval, gehören an der Emory University.</sample>
    <sample id="271">CFT stands for Continuously Fine-tuning on the Clean validation set.</sample>
    <sample id="272">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit präsentieren, die "When Does Translation Require Context? A Data-driven, Multilingual Exploration" heißt. Diese Arbeit wurde in Zusammenarbeit mit Patrick Fernandes, Emmy Liu, André F. T. Martins und Graham Neubig durchgeführt. So eine große Anzahl von Übersetzungen hängt von Kontext ab. Zum Beispiel, wie würden wir "mole" übersetzen in diesem Satz? Wenn die vorherige Satzteile lautete: "Dinge könnten gefährlich werden, wenn die Minister davon erfahren", dann bezieht sich "mole" auf einen Spitzel. Aber wenn die vorherige Satzteile lauteten: "Könnte es etwas Ernsthaftes sein, Arzt?", dann bezieht sich "mole" auf eine Fleischflecke. Also, je nach Kontext weist der Begriff einen unterschiedlichen Sinn auf, und daher auch seine Übersetzung. Allerdings ist es schwierig zu beurteilen, wie gut Modelle solche Übersetzungen handhaben können. Zunächst einmal because nur ein kleiner Teil von Übersetzungen hängt von Kontext ab, was bedeutet, dass Metrisken auf Korpus-Niveau wie BLEU diese Übersetzungen nicht effizient abdecken können. Ein paar Leute haben vorgeschlagen, speziell auf Kontext-abhängige Übersetzungen abzurichten, aber solche Ressourcen unterstützen nur begrenzte Typen von Kontext-abhängigen Übersetzungen und begrenzte Sprachensemantic, da sie normalerweise auf Domänenwissen und menschliche Kuration basieren. In dieser Arbeit versuchen wir, zwei Fragen zu beantworten: Erstens, wann benötigt eine Übersetzung Kontext? Und zweitens, wie gut können Modelle solche Fälle bewältigen? Um die erste Frage zu beantworten, haben wir zu Beginn abgeglichen, wie viel Kontext für die Übersetzung von Worten verwendet wird. In unserem vorherigen Werk haben wir CXMI als Maß für den Kontextgebrauch von Maschinellen Übersetzungsmodellen eingroduced. Das ist durch Messung des Informationsgehalts von Kontext C für die Zielsprache Y gegeben den Quellsprache X erreicht. Man kann CXMI den Informationsgewinn von Kontext für das Modell denken. In dieser Arbeit haben wir CXMI zu Pointwise CXMI erweitert, um den Kontextgebrauch an der Satellaselle oder an der Wortebene zu messen. Wir können Wörter mit hohem P-CXMI als solche betrachten, die Kontext für die Übersetzung benötigen. Jetzt analysieren wir Wörter mit hohem P-CXMI, um Muster zwischen diesen Wörtern zu finden. Und wir führen unsere Analyse an Transkripten von TED-Reden aus, die von Englisch ins 14 verschiedenen Sprachen übersetzt wurden. Wir führen unsere Analyse an drei verschiedenen Ebenen durch. Zunächst schauen wir uns Part-of-Speech-Taggings mit hohem durchschnittlichem P-CXMI an. Und dadurch finden wir zum Beispiel Doppelpronomen in Arabisch mit relativ hohem P-CXMI. Und das kann erklärt werden, weil Englisch keine Doppelpronomen hat, also Kontext benötigt wird, um zu bestimmen, ob ein Pronomen Doppelpronomen ist, wenn es in Arabisch übersetzt wird. Und ähnlich finden wir, dass bestimmte Sprachen beim Auswählen der richtigen Verbsform Kontext benötigen. Dann schauen wir uns Vokabelitems mit hohem P-CXMI an, die im Durchschnitt über alle ihre verschiedenen Vorkommen durchschnitten werden. Und dadurch können wir Fälle identifizieren, in denen man zum Beispiel im Chinesischen Kontext benötigen muss, um richtige Übersetzungen von Eigennamen zu gewährleisten, um sicherzugehen, dass dieselbe Übersetzung im Dokument verwendet wird. Und ähnlich finden wir, dass Kontext wichtig ist, um die richtige Formalität zu übersetzen. Und schließlich schauen wir uns einzelne Tokens mit hohem P-CXMI an. Und dadurch können wir Phänomene identifizieren, die nicht durch ein Wort, sondern durch die Satzstruktur ausgedrückt werden, wie zum Beispiel Ellipsenresolution. Jetzt verwenden wir unsere Erkenntnisse aus unserer Analyse, um einen Benchmark für die Dokumentübersetzung zu entwerfen. Für jedes der fünf diskursiven Phänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die sich zu diesem Phänomen beziehen. Und wir nennen unseren Tagger MuDA-Tagger. Wir können dann auch beachten, dass verschiedene Sprachen unterschiedliche Proportionen dieser diskursiven Phänomene haben. Wir können dann den MuDA-Tagger anwenden, indem wir den Tagger auf einem parallelisierten Korpus anwenden, den wir verwenden möchten, um die Evaluierung vorzunehmen, und wir wenden unsere bevorzugten Übersetzungsmetrisen auf die Kontextabhängigen Beispiele an, die der MuDA-Tagger identifiziert hat. Und endgültig verwenden wir unser Benchmark, um verschiedene Modelle auf die Dokumentübersetzung zu evaluieren. Zunächst einmal, wenn wir auf Korpus-level-Metriques wie BLEU zurückgreifen, finden wir, dass Kontext-unabhängige Modelle die beste Performance aufweisen. Aber wenn wir COMET verwenden, sind Kontext-aware Modelle am besten. Und wenn wir Word F-Measure verwenden, haben Modelle mit und ohne Kontext vergleichbare Performance. Dies zeigt, dass es schwierig ist zu bestimmen, welche Dokumentübersetzungssysteme am besten performing sind, wenn wir nur Korpus-level-Metriques verwenden. Nun verwenden wir den MuDA-B benchmark zu evaluieren und finden, dass Kontext-aware Modelle signifikant besser als Modelle sind, die Kontext nicht verwenden, bei bestimmten diskursiven Phänomenen wie Formalität und lexikalischer Kohärenz sind. Aber diese Modelle sind nicht so gut wie Modelle, die Kontext nicht verwenden, bei anderen Phänomenen wie Ellipsen, Pronomen und Verbsformen. Das suggest, dass wir bei der Dokumentübersetzung noch Fortschritte machen müssen. Wir haben auch verschiedene kommerzielle Systeme miteinander verglichen und unser Benchmark zeigt, dass DeepL normalerweise besser als Google Translate für die Dokumentübersetzung ist. Insgesamt haben wir eine datenbasierte Analyse über 14 Sprachpaare durchgefohren, um festzustellen, wann Übersetzungen Kontext benötigen, und dann haben wir unsere Erkenntnisse verwendet, um einen Benchmark für die Dokumentübersetzung zu entwerfen, der uns dabei helfen kann, zu identifizieren, welche diskursiven Phänomene Modelle gut oder nicht bewältigen und welche Übersetzungssysteme am besten performing sind bei der Dokumentübersetzung. Vielen Dank für Ihre Aufmerksamkeit. Bis bald in Toronto.</sample>
    <sample id="274">Der Referent*in heißt Yusen Zhang.</sample>
    <sample id="276">The presented work, titled "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages," focuses on evaluating machine translation metrics for translations from Indian languages to English. The study involves five Indian languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati) and uses the Flores dataset to select 200 sentences randomly. Each source sentence is translated into English by seven different translation models or APIs, resulting in a total of 1,400 candidate translations per language and 7,000 samples overall. Human annotators with bilingual expertise evaluate these translations, marking errors, their types, severity, and providing an overall score. The study compares various evaluation metrics, including overlap-based, embedding-based, and COMET-metric variants, to determine their effectiveness in assessing translation quality. The results show that COMET-metric variants generally have higher correlations with human scores, especially when only accuracy or fluency errors are annotated. Fine-tuning the COMET metric using the MQM dataset improves its performance, and IndicCOMET MQM outperforms COMET baselines in zero-shot evaluations and robustness tests.</sample>
    <sample id="277">The new method does not have a specific name mentioned in the provided information.</sample>
    <sample id="278">Die Methode der "markierten Wörter" basiert auf dem sociolinguistischen Konzept der "Markedness", das besagt, dass dominante Gruppen in Gesellschaft sowohl sprachlich als auch sozial unmarkiert sind, während diskriminierte Gruppen markiert sind. Die Autoren initially designateen unmarkierte und markierte Gruppen und vergleichen dann die Personas mithilfe des Fightin' Words-Methode, die die log-odds-Raten für die Top-Wörter pro Gruppe gegen jene von unmarkierten Gruppen vergleicht.</sample>
    <sample id="279">Die Autoren gehören an der University of Washington.</sample>
    <sample id="280">The speaker, Shi Tao, presents their work on emotion recognition in conversations, introducing the task of emotion regulation and its challenges. They propose a novel framework called MultiEMO, which includes unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The contributions include a new visual feature extractor (VisExtNet), a multimodal fusion model (MultiAttn), and a Sample-Weighted Focal Contrast loss. The framework achieves state-of-the-art results on MELD and IEMOCAP datasets.</sample>
    <sample id="281">Kayo Yin and her team have conducted a study on the role of context in translation, titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration." They analyzed 14 language pairs using TED talk transcripts to identify when translations depend on context. They developed a measure called Pointwise CXMI to assess context usage at the word level. Their findings show that context-aware models perform better for certain discourse phenomena like formality and lexical cohesion, while context-agnostic models excel in others like ellipsis, pronouns, and verb form. The MuDA tagger was created to automatically identify these phenomena, allowing for more accurate evaluation of document-level machine translation systems. The study highlights the importance of considering context in translation and suggests areas for future research.</sample>
    <sample id="282">Title: StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing

Authors: Xuekai Zhu, [Other Authors]

Abstract: This paper presents a novel approach for non-parallel text style transfer at the discourse level, addressing the challenge of imitating author linguistic preferences in long texts. Our method, StoryTrans, learns discourse representations from source texts and combines them with learnable style embeddings to generate target-style texts. We introduce a training objective that reduces stylistic features in discourse representations and enhances content preservation through two-stage generation. Experiments on Chinese and English datasets demonstrate that StoryTrans outperforms strong baselines in style control and content preservation, aligning with golden texts in style feature space.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">Hello everyone. I'm Peng Tianshuo from Wuhan University, and today I'll present my long paper for ACL's Main Conference 4915 titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction". The current span-based UIE models identify and label the span boundaries of targets in text, relying heavily on boundary positions. However, there is ambiguity in labeling golden span boundaries, as different annotations can be considered reasonable. To address this, we propose a fuzzy span boundary instead of a precise one. Additionally, there is a mismatch between Transformer feature extraction and information extraction, as basic Transformers focus on global features while ignoring the prior hypothesis that spans have limited lengths. Therefore, we propose adaptive attention for span extraction decision to model the furthest span boundary, representing the target boundary as a continuous distribution of correct probability within a specific range (R-min and R-max). We convert this continuous boundary distribution into discrete values using a sampling function for calculating fuzzy span loss. The predicted boundary by the module is then calculated using Binary Cross Entropy with the golden boundary as BCE loss and added with KL-divergence between the predicted boundary and supplementary information. To obtain a more reasonable attention distribution for span extraction, we propose a fuzzy span attention as a mask function to trim attention distribution dynamically. Our FSUIE achieves significant performance improvements in named entity recognition, relationship extraction, and aspect sentiment triplet extraction tasks. It also achieves new state-of-the-art results on datasets ACE2004, 2005, and AST-V2. The ablation study shows that FSA improves convergence speed, and FSL enables the module to fully utilize annotation information, resulting in greater information extraction capability. FSUIE demonstrates strong generalization capabilities for domain-specific information.</sample>
    <sample id="285">The video discusses the work of Mingqi Gao from Peking University, titled "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework." The video highlights the importance of correcting factual errors in dialogue summarization and argues that current FEC models have flaws in their evaluation methods. The video proposes a new taxonomy of factual errors and introduces a new evaluation framework based on the ERRANT metric. The video also explores different training modes for FEC models and finds that training with reference summaries from dialogue summarization datasets yields the best results. The video concludes by suggesting that combining human-annotated data with synthetic data is a promising direction for improving FEC models.</sample>
    <sample id="286">The referent in the video is Professor Jinho Choi.</sample>
    <sample id="287">There are four authors involved in the work: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP und SyntaxGym.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind COSINE, FTw, LwCL, CoCo, und CoCo+FT.</sample>
    <sample id="291">Das Modell wird anhand von Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech Tagging und Question Answering evaluiert.</sample>
    <sample id="294">CamemBERT wurde ursprünglich mit den Daten von NACHOS trainiert.</sample>
    <sample id="295">Der Referent in diesem Text ist Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile präsentiert in diesem Video einearbeit, die eine Partnerschaft zwischen der Universität Turin und Amazon Alexa beinhaltet. Das Hauptfokus dieser Arbeit liegt auf dem Studium von Ironie in natürlicher Sprache, einer komplexen und subjektiven Phänomen, das traditionell durch binäreLabels (ironisch oder nicht ironisch) angesprochen wird. Um ein tieferes Verständnis zu gewinnen, haben sie einen Corpus namens EPIC (English Perspectivist Irony Corpus) erstellt, der aus Social-Media-Plattformen wie Reddit und Twitter stammt und über 300 kurze Konversationen umfasst. Sie haben über 74 Annotatoren verwendet, um den Datensatz zu überprüfen, und haben auch eine "Achtung" -Frage als Kontrollmechanismus eingesetzt. Das Hauptergebnis zeigt, dass die "Perspektivbewussten" Modelle, die von einem prätrainingen Sprachmodell abgeleitet wurden, im Durchschnitt weniger unsicher und sicherer in ihren Vorhersagen sind als die Goldstandard-Modelle aggregierter Daten. Eine weitere Untersuchung ergab, dass die Meinungen von Annotatoren in Abhängigkeit von Generationen und geographischen Lagen stark variieren, was die Komplexität der Ironie in natürlicher Sprache verdeutlicht.</sample>
    <sample id="297">This project focuses on the study of coded rhetoric and dogwhistles in language models. The researchers developed a typology and glossary with over 340 terms and symbols, especially for racist, transphobic, and anti-Semitic dogwhistles. They analyzed historical U.S. political speeches and found that the frequency of racial dogwhistles correlates with the Republican Southern Strategy since the Civil Rights era. They also evaluated the performance of language models like GPT-3 in recognizing dogwhistles and found that it varies depending on the register and type of dogwhistle. Finally, they demonstrated how dogwhistles can evade content moderation by showing that automated toxicity detection scores change when standard group labels or slurs are replaced with dogwhistles.</sample>
    <sample id="298">Die Hauptursache für den Leistungsverlust war die zeitliche Verzögerung, da das Experiment gezeigt hat, dass die Leistung mit einer größeren zeitlichen Distanz zwischen dem Train- und Test-Dataset abnimmt.</sample>
    <sample id="299">The paper presents a new training method for NLI models that aims to reduce their reliance on shortcuts and improve out-of-distribution performance. The method uses a minimax training objective between a learner and auxiliary model, where the learner tries to minimize the loss of the NLI task while the auxiliary generates example weights that incentivize the learner to focus on under-represented hard examples. The method does not assume any specific type of shortcuts and can be applied to different datasets and models. The results show that the proposed method consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">Interactive dictation is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. This task involves flexible interleaving of dictation and editing, not separated by a trigger word, and using intuitive and open-ended natural language utterances to specify edits. The system should be able to pick up speech corrections and replace the correct span with a new utterance. The contribution of this work is threefold: introducing and formalizing the interactive dictation task, designing a data collection interface and building a dataset for this task, and creating a baseline system for this task. The dataset was collected using an annotation interface that allows users to issue commands and dictations in sequence until they've replicated the email. A baseline system was built by training separate models to perform each of the four steps involved in the task. The models were evaluated using exact match of the predicted end-state against the goal end-state. The results showed that there is generally a trade-off between runtime and accuracy, and that GPT-3 models are more accurate but also much slower. Furthermore, for GPT-3 models, predicting state directly is much more accurate than predicting intermediate programs. For T5 model, this distinction is much less pronounced, and predicting programs allows us to significantly improve efficiency with minimal impact on accuracy.</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, um die richtige Reihenfolge der Ausgabetoken zu bestimmen. In unserem AnsatzPredictions vom Input-Token-Multiset und einem Modell zur Vorhersage der Permutation. Das Modell verwendet dann die Permutation, um die Ausgabetoken in die richtige Reihenfolge zu sortieren.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparent machen sollten, um zu vermeiden, dass positive Stereotypien und essentialisierende Erzählnisse entstehen. Sie argumentieren, dass es schwierig ist zu bestimmen, ob positive Stereotypien durch einen "overly-excessive value alignment" oder durch andere anti-stereotypische Ansätze entstehen, ohne mehr Transparenz über die Methoden zu haben.</sample>
    <sample id="304">Inakzeptable Minimalpaarings sind Sätze, die grammatikalisch falsch sind oder Stereotypien enthalten.</sample>
    <sample id="305">In this video, Dawei presents their recent work on weakly supervised learning (WSL), a method that uses weak labeling sources instead of manual annotations. They address the research question of whether clean validation data is necessary for WSL to work properly. Their findings show that recent WSL methods require clean validation samples to achieve good performance, and that increasing the number of clean validation samples can improve performance. They also found that allowing continuous fine-tuning on clean validation samples can achieve similar performance to more complex WSL methods. The video concludes with recommendations for future work in WSL, including reporting model selection criteria, comparing with few-shot learning baselines, and considering continuous fine-tuning as a simple yet strong baseline.</sample>
    <sample id="306">Sebastian Schuster und Najoung Kim haben in einem Forschungsprojekt die Fähigkeit von Sprachmodellen, Entity-Tracking zu bewältigen, untersucht. Entity-Tracking ist eine wichtige Fähigkeit, um einen Diskurs zu verstehen, da es dazu beiträgt, welche Objekte erwähnt werden und wie ihr Status sich im Laufe des Diskurses ändert. Um die Fähigkeiten von Sprachmodellen zu evaluieren, haben sie eine Aufgabe konzipiert, bei der die Modellvorgabe eine Beschreibung der Anfangscontents in Boxen enthält, und das Modell die Inhaltstelle von jeder Box nach den Operationen vorherzusagen muss. Sie haben festgestellt, dass nur das Modell "text-davinci-003" non-trivial Entity-Tracking-Fähigkeiten zeigt, während alle anderen Modelle unter einem starken zufälligen Baseline performieren. Sie haben auch festgestellt, dass GPT-3.5-Modelle, die eine große Menge an Code als Teil ihres Ausbildungssatzes erhalten haben, bessere Leistungen zeigen als GPT-3-Modelle, die Code nicht als Ausbildungssatz erhalten haben.</sample>
    <sample id="307">Die Autoren haben die Leistung der verschiedenen Modellversionen anhand von public und private Downstream Aufgaben bewertet, indem sie die Genauigkeit (Named Entity Recognition), die Klassifizierung, die Part-of-Speech-Tagging und die Fähigkeit zur Fragebeantwortung einschlossen. Sie haben auch sechs Baseline-Modelle verwendet, um die Leistung zu vergleichen.</sample>
    <sample id="308">Jenny, eine first-year PhD-Studierende an Carnegie Mellon University, präsentiert heute das Werk "NLPositionality", das die Designbiases von Datensätzen und Modellen charakterisiert. Das Werk wurde in Zusammenarbeit mit Forschern aus der University of Washington und dem Allen Institute for AI erstellt. Das Werk konzentriert sich auf die Positionalität von NLP-Forschern und Modellentwicklern, die durch ihre Demografie, Identität und Lebenserfahrungen geprägt sind. Das Werk zeigt an, dass Datensätze und Modelle certain positionalities über andere vertreten können. Um dies zu untersuchen, verglichen die Forscher dieAnnotations mit realen Benutzern und bestehenden Datensätzen und Modellen. Sie verwendeten dabei ein Framework namens NLPositionality. Das Framework arbeitet in zwei Hauptschritten: Zunächst re-ANNOTATE data sets mit diversen Annotatoren, um eine reiche Sammlung von demographischen Daten zu erhalten. Anschließend verglichen sie dieAnnotations von demographischem Fundament mit den Modellen und Datensätzen mithilfe einer Pearson's R-Korrelationsnote. Das Werk differenziert sich von literaturwissenschaftlichen Arbeiten, die sich nur mit Annotator-Übereinstimmungen beschäftigen. Das Werk ist durch Lab in the Wild und Online-Crowdsourcing-Plattformen für HCI (Human-Computer-Interaktion) ermöglicht. Lab in the Wild ist ein Online-Experimentplattform, bei der es leichter ist, diversen Teilnehmer zu recrutieren. Im Vergleich zu Plattformen wie MTurk, die hauptsächlich US-amerikanische oder indische Teilnehmer haben, kann Lab in the Wild hochwertige Daten sammeln. Das Werk hostete zwei Aufgaben auf Lab in the Wild: eine Sozialakzeptabilitätstask und eine Toxicity-Hate-Speech-Detection-Aufgabe. Es verglich dann dieAnnotations mit den Aufgaben von Social Chemistry, Delphi und GPT 4. Das Werk sammelte insgesamt über 16.000 Annotations von über 1000 Annotatoren aus 87 Ländern. Das Werk fand, dass es Positionalität in NLP gibt. Beispielsweise fanden die Forscher, dass Datensätze und Modelle am besten auf englischsprachige Länders-aligniert sind. GPT 4 war am besten auf Confucian- und englischsprachige Länders-aligniert,Dynahate war am besten auf englischsprachige Länders-aligniert. Es wurde auch festgestellt, dass Datensätze und Modelle Menschen mit einem College-Abschluss oder einem Studium an der Universitätsreife alignieren. Allerdings sind bestimmte Gruppen, wie nicht-binary-Personen, oft übersehen. Um diese Positionalitäten zu reduzieren, empfehlen die Forscher, alle relevanten Designentscheidungen während des Forschungsprozesses zu notieren, NLP-Forschung mit dem Blick auf Perspektivismus zu betreiben und spezialisierter Datensätze und Modell zu erstellen, die auf bestimmte Gemeinschaften abgestimmt sind. Ein Beispiel für eine solche Initiative ist Masakhani. Insgesamt zeigt das Werk, dass inclusive NLP nicht nur eine allgemeine Technologie ist, sondern auch für alle Menschen arbeitet.</sample>
    <sample id="309">Die Metrik, die verwendet wurde, um die Übereinstimmung zwischen den Kommentatoren zu messen, ist der "inter-annotator agreement" auf 100 doppelt bewerteten Konversationen.</sample>
    <sample id="310">Wikipedia wurde gewählt, um völlig unzusammenhängende Sätze zu den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="311">Die Autoren der Präsentation "DEPLAIN: A New Corpus for German Text Identification on the Document Level and on the Sentence Level" gehören an die Technische Hochschule Darmstadt.</sample>
    <sample id="312">MultiInstruct ist der erste multi-modal instruction tuning Benchmark, der 62 diverse multi-modal Aufgaben über 10 breite Kategorien enthält. Es basiert auf 21 bestehenden offenen Quellen Datensätzen und each Aufgabe wird mit 5 Experten geschriebener Anweisungen ausgestattet.</sample>
    <sample id="313">Die englische Übersetzung des Textes enthält Informationen über die Beteiligung mehrerer Autoren. Hier ist eine kurze und bündige Antwort auf die gestellte Frage:

Es sind insgesamt 5 Autoren an der Arbeit beteiligt: Professor Jinho Choi, Amazon Alexa AI und 3 weitere Autoren, die an der Evaluierung von 4 state-of-the-art Chatmodellen mit dem ABC-Eval-Verfahren teilgenommen haben.</sample>
    <sample id="314">The definition of binary coordination is not explicitly provided in the given text. However, it can be inferred that binary coordination refers to the coordination of two elements or entities, such as words, phrases, or clauses, where one of them is the head or governor of the whole coordinate structure.</sample>
    <sample id="315">Es wird in dem Paper nicht erwähnt, wie lange die verwendeten Prompts im Durchschnitt waren.</sample>
    <sample id="316">Die Auswirkungen der Ergebnisse auf das kleinere T5-Modell sind, dass es nach der Anwendung des CoScript-Datasets scripts von höherer Qualität generieren kann als die meisten großen Sprachmodell.</sample>
    <sample id="317">The presented work, titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors," introduces a novel approach to information extraction by transforming the task into a structure-to-structure code generation problem. The authors propose using code large language models like Codex to generate structured outputs directly from unstructured text inputs. This method aims to address the mismatch between the outputs of pre-trained language models and the structured format required for information extraction tasks.

The study evaluates the performance of various models, including T5, UIE, GPT-3, and Codex, on three named entity recognition datasets and four relation extraction datasets. The results show that the proposed approach using code language models and code format prompts significantly outperforms traditional baseline models in few-shot settings. Additionally, the analysis reveals that code format prompts lead to fewer structural errors and better recall rates compared to text format prompts.

Overall, the work provides insights into the effectiveness of code generation models for information extraction tasks and suggests that this approach can be a valuable tool for improving the accuracy and efficiency of information extraction systems.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen heute über unsere Arbeiten zu "DrBERT: Ein robustes prätrainierter Modell in Französisch für biomedizinische und klinische Domänen" präsentieren. Zunächst diskutieren wir über Sprachmodellierung im Gesundheitswesen. Danach präsentieren wir die Hauptbeiträge unseres Artikels. Wir Introduzieren das erste biomedizinische Modell in Französisch namens DrBERT, das auf RoBERTa basiert und trainiert wurde auf NACHOS, einer Datensammlung von medizinischen Webseiten. Wir haben auch einen Vergleich von Modellen mit mehreren prätrainingssituationen und Datensätzen vorgestellt. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen downstream Aufgaben in Französisch. Schließlich conclude about the experiments and give you more details about how to access those models. Seither seine Einführung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Bewältigung von Natural Language Processing Aufgaben geworden und bietet enormen Leistungsverbesserungen im Vergleich zu historischen statischen und kontextualisierten Methoden wie Word2vec, fastText usw. Seit dem dann wurde das Modell auf viele andere Sprachen angepasst, wie in Französisch mit CamemBERT, und auch in Domänen wie biomedizin mit PubMedBERT und BioBERT und auf klinischem mit ClinicalBERT, aber hauptsächlich in Englisch. Spezialisierte Modelle für andere Sprachen sind seltener und oft aufgrund des Mangelns an in-domain-Daten auf Basis von Kontinuierlicher prätraining aufgestellt. Allerdings gab es bislang keine offene Quellen-Modelle für biomedizin auf Französisch. Also fragten wir uns, welche Datensammlungen am besten geeignet sind für eine breite Anwendung und ob krawelte Data eine gute E subsitution für klinische Data darstellt. Um diese Frage zu beantworten, verglichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datendienst des Krankenhauses von Nantes basiert. Danach fragten wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf Französisch zu trainieren. Ist es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir vier from-scratch-Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB von NACHOS; eine erste Version von ChuBERT, das ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen ist; und eine finale Version von ChuBERT mit einer Mischung von 4 GB von NACHOS und 4 GB von klinischen Notizen. Neben dieser Vergleichsanalyse präsentieren wir drei Modelle, die auf Kontinuierliche prätraining trainiert wurden, um den Einfluss der prätrainingsstrategie zu analysieren. Eine davon basiert auf den Gewichtern von CamemBERT und trainiert auf einem 4 GB-Subset von NACHOS. Eine weitere basiert auf CamemBERT, aber trainiert diesmal auf den 4 GB von klinischen Notizen und letztlich eine basiert auf einem englischen biomedizinischen Modell PubMedBERT und trainiert auf 4 GB von NACHOS. Insgesamt haben wir sieben Modelle. Um unsere sieben Modelle zu evaluieren, sammeln wir Daten für öffentliche und private downstream Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech tagging und Question Answering.Diese Modelle werden mit sechs Baseline-Modellen verglichen, die CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Evaluation zeigt, dass die Modelle am besten auf Aufgaben performieren, bei denen die Daten dieselben Natur sind, auf denen das Modell trainiert wurde. Wir können jedoch beobachtet haben, dass Daten aus heterogenen Quellen zu einem besseren Versatility führen. Wir können auch beobachtet haben, dass mehr Daten zu besserer Performance führen. Im Allgemeinen erhalten Modelle, die von Null her trainiert wurden, höhere Leistungsstelle auf meisten Aufgaben. Allerdings haben unsere Experimente mit Kontinuierlicher prätraining, die auf dem Gewicht und Tokenization von CamemBERT trainiert wurden, die 4 GB-Subset von NACHOS, ähnliche Resultate zu jenen erhalten, die mit DrBERT 4 GB von Null her trainiert wurden. Das ist nicht der Fall bei Modellen, die auf Gewichten und Tokenization von CamemBERT basieren, die instabil sind. Schließlich als Schlussfolgerung haben unsereproper System bessere Leistungen auf neun von den 11 downstream Aufgaben erreicht und über globale Leistungsstelle das allgemeine Modell, hier CamemBERT, übertragen. Wir bemerkten auch, dass mehr spezialisierte Daten besser sind, aber es nicht skaliert. Alle vor Trainingsmodell, die aus NACHOS stammen, sind frei auf Hugging Face und unter der MIT-Lizenz verfügbare, und alle Trainingsskripte sind auf unserem GitHub-Repository. Also danke für die Präsentation und wir freuen uns auf die Besprechung im Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht verschiedene Lernstrategien, darunter die von-scratch-Training, die Kontroll-Prätraining und die Anwendung von CamemBERT Gewichts- und Tokenisierungsstrategien.</sample>
    <sample id="320">Die Überanpassung, die auf die Wiederverwendung von Tests zurückzuführen ist, zeigt sich in einem stärkeren Gradienten (größere Steigung) in der Grafik, was bedeutet, dass jede Verbesserung auf CoNLL-2003 mehr als eine unitarische Verbesserung auf CoNLL++ bewirkt. Dies zeigt, dass die adaptive Überanpassung in diesem Fall nicht auftritt.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand von Typen von Vereinfachungen, wie Lexikalversubstitution, Strukturanordnung, allgemeiner Vereinfachungsebene und anderen Simplifizierungsoperationen bewertet. Es wurde auch untersucht, wie viele Anordnungen, Wörter hinzufügen und Rephrasingoperationen in den DEPLAIN-apa und DEPLAIN-web Subcorpora vorkamen.</sample>
    <sample id="322">Enrico präsentiert bei ACL 23 eine Studie über die Lernfähigkeit von Textklassifizierern in Bezug auf Moralität. Er erläutert, dass Human-Moralität dazu dient, zwischen Gut und Bösem zu unterscheiden und eine internationale Richtschnur für Handeln und Konzepte ist. Moralphormulierungen in Texten werden oft als ein Maßstab zwischen moralischem Bösein und moralischem Gutes betrachtet, was das wahre Verhältnis versteckt. Enrico zeigt, dass Sprachmodelle die Unterschiede in der Ausdrucksweise von Moralität in verschiedenen Domänen wie #AllLivesMatter und #BlackLivesMatter verstehen können. Seine Studie zeigt, dass Sprachmodelle die moralischen Unterschiede in den Domänen erkennen und dabei Worte wie "Überwurf" und "Mayhem" als subversiv negative Aspekte und "subversiv" als positive Aspekte interpretieren. Enrico betont, dass die Anwendung von einem einzigen Modell auf verschiedene Domänen zu Missverständnissen führen kann.</sample>
    <sample id="323">The paper presents a new approach for Commonsense QA that combines knowledge from language models and knowledge bases. The proposed method, called DHLK, builds an HKG based on multiple knowledge bases using a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. The HKG is then encoded and fused with the QA context using RoBERTa and Mask Self-Attention. The entity embeddings are optimized using TransE, and the subgraphs are modeled using Relation Mask Self-Attention. The final answer prediction is obtained by incorporating the HKG path information into the QA context and inputting the HKG graph, paths, and QA context into an MLP. The results show that DHLK achieves good performance on CommonsenseQA and OpenBookQA compared to other LM and HKG methods.</sample>
    <sample id="324">Ja, Sprachmodelle können unterschiedliche politische Vorurteile aufweisen. Das Team von Shangbin hat festgestellt, dass Sprachmodelle in den vier Quadranten der politischen Landschaft vertreten sind und dass GPT-4 die liberalste Sprachmodell ist. Es wurde auch gezeigt, dass Sprachmodelle nach dem Erweitern mit partisanen Datensätzen politische Vorurteile annehmen können.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen einen kurzen Überblick über unser Papier zu "Kompositioneller Allgemeinheitszugang ohne Bäume unter Verwendung von Multiset Tagging und latenten Permutationen" geben. Dieses Papier habe ich gemeinsam mit meinen Betreuern Alexander Koller und Ivan Titov verfasst. Kompositioneller Allgemeinheitszugang kann als die Fähigkeit eines Lerners verstanden werden, um tieferen Rekursionen und unvorher gesehene Kombinationen von Phrasen zu bewältigen, die während des Trainings individuell gesehen wurden. Im Kontext von semantischem Parsen sieht das Testen für kompositionellen Allgemeinheitszugang wie folgt aus: Wie üblich haben wir ein Trainingssatz von Aussagen. In diesem Fall sind es "Die Mädchen schlafen." und "Mary wusste, dass die Mädchen schlafen." Diese Aussagen werden mit logischen Formen abgebildet, die die Hauptaspekte ihres Bedeutungsvollstens repräsentieren. Im Gegensatz zu standardmäßigen Maschinelles Lern-Evaluation-Methoden enthält der Testset nicht die gleiche Verteilung und enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell Shallow-Rekursion während des Trainings gesehen und wird auf ein Beispiel mit tiefere Rekursion getestet. Naive Seq2Seq-Modelle kämpfen mit dieser Art von out-of-distribution-Allgemeinheitsfähigkeit und produzieren oft Ausgaben, die von der Eingabe abgetrennt sind. Insbesondere scheitern sie darin, die systematischen Korrespondenzen zwischen Eingabe und Ausgabe zu reproduzieren, wie sie in dem Farbcode color-coded dargestellt sind. Eine beliebte Methode, um diesem Problem entgegenzuwirken, ist die Integration von Bäumen in die Modelle. Die Bäume sollen die kompositionelle Beziehung zwischen Aussagen und logischen Formen capturieren. Dies funktionsweise gut, aber Bäume werden normalerweise nicht gegeben und müssen auf eine bestimmte Weise erhalten werden. Das kann kompliziert und manchmal computergesamt kostspielig sein. Normalerweise beinhaltet dies eine spezielle vorhersehende Formalismus-Unter処理 der logischen Formen, um z.B. zu verwalten, wie Symbole verwendet werden. Das Erhalten von Bäumen kann auch involieren spezielle Grammatik-induktionsprozeduren. In unserem Papier verwenden wir keine Bäume und Introducing ein neuronales Seq2Seq-Modell, das direkter die Korrespondenz zwischen Fragmente der Eingabe und Fragmente der Ausgabe modelliert.Für die ersten SchrittePredict die Ausgabe von der Eingabe. Zunächst taggen wir jede Eingabekarte mit einem unsortierten Multiset von Token, die im Ausgang auftreten werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht sortiert. Dementsprechend benutzen wir im nächsten Schritt noch ein anderes Modell, um eine Permutation zu predicten, um sie in die richtige Reihenfolge zu sortieren. Wir Introduce eine neue Methode, um die Permutation zu predicten, die keinerlei harten Beschränkungen auf die möglichen Permutationen anwendet. Dies macht unser Ansatz sehr flexibel und ausdrucksstalent. Konzeptionell arbeitet unser Permutationsmodell wie folgt: Wir gehen von links nach rechts über den Ausgang und bestimmen, welches Multiset Token wir in jede Position im Ausgang platzieren. Für die erste Ausgabeposition auswählen wir einfach ein, wie in roter Farbe hervorgehoben. Dann springen wir zu einem anderen Multiset Token, um die zweite Ausgabekarte zu bestimmen. Wir bestimmen die dritte Ausgabekarte in einem ähnlichen Weise, indem wir zu einem anderen Multiset Token springen. Wir fortfahren diese Prozedur, bis jedes Token aus dem ersten Schritt exakt einmal besucht wurde. Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, hier eine Visualisierung, in der wir unser Modell mit anderen treeless-Modellen auf dem COGS-Benchmarked vergleichen. Unser Modell übertrifft die anderen bei der Generalisierung zu tiefere Rekursionen um ein deutliches Maß. Ein paar andere Arten von struktureller Allgemeinheitsfähigkeit bleiben weiterhin schwierig zu bewältigen. In unserem Papier lösen wir ein paar interessante technische Herausforderungen. Zunächst einmal ist dieALIGNMENT zwischen Eingabe und Ausgabe nicht im Training gegeben. Als Folge wissen wir für einen bestimmten Token nicht, welches Multiset es stammt, was eine Herausforderung für die Trainingsschritte darstellt. Manchmal gibt es mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte Permutation ist latent. Wir adressieren dies, indem wir dieALIGNMENT als Teil des Trainings induzieren. Unsere Permutationsmethode ist sehr flexibel, aber sie bringt die Herausforderung, dass die Bestimmung des höchsten Punktschwemmings NP-schwer ist. Das liegt daran, dass dies sich mit dem "Reiseführerproblem" assoziiert. Wir approximieren dies mit einer GPU-friendlichen kontinuierlichen Relaxation, die auch die Möglichkeit bietet, durch die Lösung zu backpropagieren und linguistisch plausible Permutationen zu lernen. Wenn Sie mehr über unsere Experimente erfahren möchten und wie wir diese Herausforderungen adressieren, bitte schauen Sie sich unser Papier an oder besuchen Sie unser Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist die Inconsistenz zwischen zwei Glaubens oder Handlungselementen, wie zum Beispiel wenn jemand behauptet, Zigaretten könnten ihn töten, aber dann doch eine Zigarette nach dem Meeting raucht.</sample>
    <sample id="327">Title: ManagerTower: Aggregating the Insights of Uni-modal Experts for Vision-Language Representation Learning

Authors: Xiao Xu, MSRIC Group, Intel Cognitive Computing Group

Abstract: This work presents a novel vision-language (VL) modal architecture called ManagerTower, which adaptsively aggregates insights from pre-trained unimodal experts at different levels. Unlike previous two-tower architectures, ManagerTower uses managers in each cross-modal layer to gather and combine insights, allowing for more effective exploitation of different levels of universal semantic knowledge. With only 4 million images for visual language pre-training, ManagerTower achieves superior performance on various downstream tasks, outperforming many base-size models pre-trained on larger datasets. The visualization of average aggregation weights demonstrates that adaptive managers can adaptively exploit different levels of unimodal semantic knowledge for comprehensive cross-modal representation learning.</sample>
    <sample id="328">GPT-4 ist das am meisten links stehende Sprachmodell.</sample>
    <sample id="329">Hello everyone, I'm Minghang Zheng from Peking University. Today, I'll be presenting our work on "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization." This research was conducted in collaboration with Shaogang Gong, Hailin Jin, Yuxin Peng, and Yang Liu. Our focus is on zero-shot video sentence localization, which aims to find the most relevant segments of a long video based on a given natural language query. This task requires identifying the start and end times of the video segment that best matches the query without any manual annotations.

Our approach addresses three main issues found in existing methods: 1) Simplistic pseudo-queries generated by combining detected nouns and verbs, 2) Inability to guarantee relevance between the query and video outside the events, leading to misalignment, and 3) Ignoring the risk of label noise during training.

We propose a noise-resistant structured pseudo-label generation method. First, we use a pre-trained image caption model to generate complex free-form pseudo-queries. Then, we measure the relevance between individual frames and pseudo-queries using a pre-trained model to generate pseudo-events that ensure high relevance within events and low relevance outside them. Finally, we reduce the weight of noisy samples and create noisy labels to minimize label noise's impact.

Our method involves densely sampling video frames, generating pseudo-queries with an image-text pre-trained BLIP model, modeling temporal structure to generate pseudo-events, and selecting the highest-quality event proposals. We then train a video sentence localization model using these structured pseudo-labels while reducing the influence of label noise through sample re-weighting and label refinement.

We conducted experiments on ActivityNet Captions and Charades-STA datasets, using R@M and mIoU as evaluation metrics. Our method outperforms existing zero-shot methods on most metrics. The full details of our experiments and results are available in our paper. Thank you for listening.</sample>
    <sample id="330">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="331">The referent is Sara Papi.</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transcripts von TED Talks, die von Englisch ins 14 verschiedenen Sprachen übersetzt wurden.</sample>
    <sample id="333">Wenhao von Nanjing University präsentiert "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation" undAcknowledges Jingjing Xu (Shanghai AI Lab), Shujian Huang und Jiajun Chen (Nanjing University) und Lingpeng Kong (University of Hong Kong). Das Hauptziel der Arbeit ist die Verbesserung der allgemein anwendbaren Repräsentationsraum in neuronalen Maschinentranslation-Modellen. Das kNN-MT-Modell wird verwendet, um die Vorhersage in der Repräsentationsraum nach den nächsten Nachbarn zu korrigieren. Obwohl wir einen effektiven Ansatz haben, haben wir zwei Haupt 缺陷：suchen Nachbarn in einem großen Datenspeicher bei jeder Decoding-Schritt ist zeitaufwendig, und die Datenspeicher können nicht einfach aktualisierbar sind. Um diese Schwierigkeiten zu überwinden, schaffen wir INK, um kNN-Knowledge in MT zu injizieren. Das Trainingsschema von INK hat zwei Schritte: zuerst extrahieren wir kNN-Knowledge aus dem Datenspeicher, um die Anpasser zu beraten, die die Repräsentation zu überarbeiten. Danach werden die aktualisierten Repräsentationen verwendet, um den Datenspeicher asynchron zu aktualisieren. Dieser Prozess wird bis zur Konvergenz fortgesetzt. In den Experimenten verwenden wir das gewinnende Modell der WMT’19 Deutsch-Englisch-News Übersetzung Aufgabe als Standard-Neuronal-Maschinentranslation-Modell. Wir experimentieren mit dem gesamten Benchmark-Dataset und finden, dass selbst für das gewinnende Modell die Repräsentationsraum erheblich verbessert werden kann. In unserem Experiment explore wir die folgenden drei Fragen: 1. Können wir die Repräsentationsraum mit einem kleinen Adapter glätten und den Datenspeicher während der Inferenz weglassen? 2. Wie viel Verbesserungen können wir durch die Anwendung von kNN-Knowledge auf die Repräsentationsraum verteilen? 3. Bringen die Verwendung von Adaptern und Datenspeichern zusammen weitere Verbesserungen? Das INK-System übertrifft die aktuellen kNN-MT-Systeme und erreicht die beste Leistung nach der Glätte der Repräsentationsraum. Im Vergleich zum Adapter-Baseline können wir sehen, dass die Verbesserung der Vorhersage durch die Anwendung von kNN-Knowledge größere Leistungsverbesserungen bringt. Um die Effektivität des INK-Frameworks zu zeigen, verwenden wir Adaptern verschiedener Größen. In Allgemein, liegt das INK-System oben rechts auf jeder Figur, was bedeutet, dass INK höhere BLEU-Scores mit weniger Speicherplatz erreicht. Nebenher finden wir auch, dass die gemeinsame Anwendung von Adaptern und Datenspeichern die Vorhersage weiter glätten kann, was zeigt, dass die Repräsentationsraum der NMT-Model nicht vollständig durch den Adapter refiniert wurde. Wenn ein effizienteres Framework entdeckt wird, wird die Vorteil der glatteren Repräsentationsraum weitere Reveal. Insgesamt präsentieren wir in diesem Papier einen neuen Trainingsschema. In unserem Schema devisieren, injizieren und refinar wir einen Trainingsschleife, um iterativ den Repräsentationsraum der NMT-Model nach kNN-Knowledge zu refinieren. Experimente zeigen, dass das INK-System einen Durchschnitt von 1,99 COMET-Score und 1,0 BLEU-Score verbessert, im Vergleich zu den aktuellen kNN-MT-Systemen. Das INK-System erreicht auch bessere Übersetzungsleistungen mit weniger Speicherplatz und schneller Inferenzgeschwindigkeit.</sample>
    <sample id="335">Der Referent in diesem Paper ist Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit von Modellen, in einem Sprachen trainiert zu werden und danach in anderen Sprachen zu arbeiten. In diesem Fall wurden die Modelle in einem Sprachen trainiert und dann verwendet, um in anderen Sprachen zu arbeiten, um die Leistung zu bewerten.</sample>
    <sample id="337">The research presented in this speech focuses on the development of a graph-based approach for out-of-vocabulary (OOV) word embedding learning. The authors introduce a Word Relationship Graph that captures the lexical rules of word formation and association, allowing for the inference of OOV words' meanings through their relationships with relevant words. They employ a two-level graph structure, where each word or wordpiece acts as a node, and utilize a graph neural network to process the graph. To address the challenge of assigning attributes to OOV nodes, they use a self-attention network based on character-level information. The model is trained using contrastive learning with NT-XENT positive samples from the graph, and has been shown to outperform baselines in both intrinsic and extrinsic tasks. The authors also discuss the potential application of their model to other languages, particularly agglutinative languages, which form words by stringing morphemes together directly.</sample>
    <sample id="338">The presentation by Bingsheng and the research team focuses on evaluating the quality of human natural language explanations in machine learning models. They address the challenge of subjective and task-dependent explanations by proposing a new metric called TREU, which extends the simulatability score to consider the helpfulness of explanations during fine-tuning. The researchers conducted experiments on five large-scale datasets using two models, T5 and BART, and found that their metric better reflects the beneficial impact of human-annotated explanations on model predictions compared to the simulatability score. The presentation highlights the importance of high-quality human collaboration in annotation jobs and recommends using similar evaluation methods in future research.</sample>
    <sample id="339">Die Autoren gehören an der Saarland University in Deutschland.</sample>
    <sample id="340">The presentation introduces ParaAMR, a large-scale dataset for paraphrase generation that leverages Abstract Meaning Representations (AMR) graphs to ensure syntactic diversity. Unlike existing datasets, which either lack scale or syntactic variety, ParaAMR uses AMR back-translation to generate diverse paraphrases. The dataset consists of around 15 million source sentences with approximately 6.9 paraphrases per sentence. Quantitative analysis shows that ParaAMR maintains good semantic similarity while achieving higher syntactic diversity scores compared to other back-translation datasets. The presentation also highlights the benefits of ParaAMR in various NLP applications, including sentence embeddings, syntactic control paraphrase generation, and few-shot learning data augmentation.</sample>
    <sample id="341">Die Autoren verwenden BLEU (Begriffstoleranz), durchschnittliche LAG (Latenzmessung) und die bewusste durchschnittliche LAG (die Rechentimes des Modells berücksichtigt).</sample>
    <sample id="342">Hello everyone. My name is Gao Jingsheng, and today I'm presenting our paper titled "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming." This research was conducted by me, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI. 

In this presentation, I will outline the introduction to open-domain dialogue, which refers to a conversational exchange between a human and an artificial intelligence system that can cover a range of topics without a specific goal. Open-domain dialogue relies on pre-trained models and large-scale datasets. Existing large-scale corpora mainly consist of online chat conversations, which can be divided into text sources and video sources. Currently, most large-scale pre-trained dialogue datasets are text-sourced. Therefore, it is significant to construct a large-scale video-sourced dialogue dataset that is closer to real spoken conversation.

Existing video-sourced dialogue datasets can be categorized into two groups: those with scripted conditions, such as TV and movie scripts, and those without scripts, such as interview datasets. However, these datasets are limited in scale as they rely on manual annotations and instructions. To construct a large-scale dialogue dataset, finding an effective matching mechanics that captures the reply-to relationships among speakers is crucial. Additionally, personalized dialogue is essential for developing applications like virtual streamers and virtual employees. However, current research on personalized dialogue faces challenges such as utilizing persona information to represent characteristics and the lack of session dialogue for each persona. Moreover, multi-party dialogue scenarios involving more than two interlocutors are challenging to address.

To overcome these barriers, we propose a large-scale personalized dialogue dataset called LiveChat with a unique automatic dialogue-constructing method. We conduct experiments on two benchmark tasks: response modeling and addressee recognition. We also investigate the transfer learning of generation models on LiveChat.

The second part of our presentation focuses on the LiveChat dataset, which is constructed in three steps. Firstly, we extract origin streaming videos from Chinese TikTok and Douyin. Then, we transcribe audio from videos into utterances using ASR. Secondly, we collect audience comments and construct dialogues using a reply-to-whom matching method. Thirdly, we collect persona information for personalized dialogue generation. The persona extraction of LiveChat can be categorized into two parts: basic profiles labeled manually and scratch, and other profiles extracted by rules and trained persona classifiers.

We compare our dataset with existing open-domain dialogue datasets. Our LiveChat is video-sourced with a larger scale, and it has longer average sessions compared to other datasets. The third part of our presentation involves experiments on two benchmark tasks: response modeling and addressee recognition. We train retrieval baselines for these tasks and observe that the extracted persona and longer average sessions are beneficial for the final results. Both rules and classifiers are important for persona extraction. In the addressee recognition task, single-stream BERT outperforms double-stream BERT, although persona is beneficial for address recognition.

Additionally, we investigate the performance of pre-trained dialogue models on LiveChat. BART performs better than other models, confirming that the domain of LiveChat is distinct from existing dialogue datasets. Human evaluations of LLMs show that they perform better in terms of rich informativeness. We also study the influence of demonstrations in context learning experiments. Performance improves as the number of demonstrations increases, but slightly decreases when the number of demonstrations exceeds eight due to noise introduced by random manual selection.

In conclusion, we propose LiveChat, a Chinese video-sourced and personalized dialogue dataset. Experiments on two benchmark tasks show that selected persona profiles and longer average sessions are advantageous in learning the speaker's personalized response. Comparisons between BART and other LLMs reveal the distinctiveness of LiveChat. In the future, we will focus on efficient transfer learning of LLMs for LiveChat. Thank you for listening.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha und heute together mit meinem Mitautor Martin präsentieren wir unser Werk "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Dieses Werk ist eine Kooperation zwischen McGill University, Mila und Microsoft Research. Sprachverarbeitungsmodelle ziehen auf eine Vielzahl von Kenntnisquellen zu, wie zum Beispiel Kenntnisse, die in ihren Parametern enthalten sind, normalerweise durch eine vorherige Ausbildung erworben, und Kenntnisse, die in den Eingabe-Parametern zur Laufzeit verwendet werden. Jüngere Arbeiten in Aufgaben wie der Fragenaufbau zeigen, dass Modelle die Möglichkeit nutzen können, Kenntnisse aus der vorherigen Ausbildung zu verwenden, um die Aufgabe zu lösen. Allerdings erfordert natürliche Sprachverarbeitung oft Kenntnisse, die auch zur Laufzeit übermittelt werden. Zum Beispiel im Satz "John sah den neuen Präsidenten auf dem Fernsehen." Vorheriges Kenntnis enthält Informationen darüber, was Präsidenten tun und was ein Fernseher ist, aber nicht, wer John oder der neue Präsident ist, da sich seit der vorherigen Ausbildung der Präsident geändert haben kann. Also benötigen erfolgreiche Modelle für intensive Kenntnis-integrierende NLU-Aufgaben die Fähigkeit, sowohl vorheriges als auch Laufzeit-Kenntnis zu integrieren und zu verwenden. In diesem Werk vorschlagen wir einen Diagnostischen Testsatz für Kenntnisintegration. Wir Introduzieren eine Kernreferenz Auflösungs Aufgabe, die dazu dienen soll, die Fähigkeit zu prüfen, Kenntnisse aus verschiedenen Quellen abzurufen. Wir evaluieren die Datensammlung mit Menschen und etablierten Kernreferenz Auflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kea ist ein Brotbaker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gericht, in dem er Entscheidungen getroffen hat, war er froh, sich zu entspannen. Die Aufgabe besteht darin, den richtigen Entity zu identifizieren, dem Pronomen "er" beizugeordnet ist, der in diesem Fall Servin ist. Das Auflösen einer bestimmten Verweis erfordert zwei Arten von Informationen. Zunächst einmal spezifische Kenntnisse wie "Servin ist ein Richter." Und second, Hintergrundinformation wie "Richter machen Fälle im Gericht." Normalerweise werden Hintergrundinformationen während der vorherigen Ausbildung von großen Sprachmodellen gelernt, während spezifische Kenntnisse normalerweise nur zur Laufzeit beobachtet werden. Wir variieren die Verfügbarkeit dieser zwei Arten von Informationen so, dass sie entweder in einer Quelle, oder in mehreren Quellen zu finden sind. Wir haben definiert drei Varianten von KITMUS. Erstens haben wir die typische Variante: "Hintergrund-Vortraining", bei der Hintergrundinformation angenommen wird, dass sie zur vorherigen Ausbildung im Modell enthalten ist. Zweitens gibt es die "Hintergrund-Beide" Variante, bei der Hintergrundinformation sowohl zur vorherigen Ausbildung als auch zur Laufzeitavailable ist. Letztenfalls die "Hintergrund-Laufzeit" Variante, bei der beide Kenntnisarten nur zur Laufzeitavailable sind.Diese letzte Variante ist insbesondere interessant, da sie simuliert, dass die notwendige Hintergrundinformation zur Lösgabe einer Aufgabe nicht in den vorherigen Datensätzen des Modells enthalten ist. Zum Beispiel, weil neue Berufe seit der vorherigen Ausbildung entstanden sind. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Fakten in den wahren Quellen kontrollieren. Im Hintergrund-Vortraining-Setting nehmen wir an, dass die Hintergrundinformation "Politiker suchen gewählte Stellungen im Regierung" in den vorherigen Parametern und in der Laufzeitcontext enthalten ist. Im Hintergrund-Beide-Setting geben wir neben der spezifischen Kenntnis "Chichester ist ein Politiker" auch Hintergrundinformation über Politiker in der Laufzeitcontext an. Im Hintergrund-Laufzeit-Setting geben wir die fiktive Berufstätigkeit "mirituer" anstelle von Politiker an, da "mirituer" unwahrscheinlich in den vorherigen Parametern enthalten ist. Wir evaluieren die Datensammlung sowohl mit Menschen als auch mit etablierten Kernreferenz Auflösungsmodellen. In diesem Bild sehen wir die Ergebnisse des besten performenden Modells auf der schwierigsten Variante des Hintergrund-Vortraining-Settings. Ohne spezifische Ausbildung an KITMUS, both Modelle performed nicht gut. Wenn sie jedoch auf KITMUS trainiert wurden, both C2F und BERT4Coref performed signifikant besser als zufällige Wahl. Das suggeriert, dass wenn Modelle auf allgemeine Referenz Auflösungs Datensätze trainiert werden, sie tendieren, Oberflächliche Hinweise zu nutzen, die bei der Testphase auf KITMUS nicht nützlich sind, da solche Hinweise entfernt wurden. Weitere Experimente mit fiktiver Kenntnis zeigten, dass selbst die besten performing Modelle Schwierigkeiten haben, Hintergrundinformation zu integrieren, die nur zur Laufzeitavailable ist. Insgesamt die Haupttakeaways von unserem Papier sind, dass viele Kernreferenz Auflösungs Modelle unfähig sind, Kenntnisse aus verschiedenen Quellen zu einem Auftrag zu bewältigen, ohne spezifische Ausbildung. Allerdings, wenn sie auf spezifische Aufgaben spezifisch trainiert werden, können einige Modelle Kenntnisse aus mehreren Quellen integrieren und verwenden. Trotzdem scheinen selbst die besten performing Modelle Schwierigkeiten zu haben, Hintergrundinformation zu integrieren, die nur zur Laufzeitavailable ist. Wenn Sie mehr Details wissen möchten, bitte our Paper und das Datensatz und Code auf GitHub überprüfen. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass sie komplex und computationally teuer sind, um zu erhalten. Sie erfordern auch spezielle Formalismus-Spezifische vorverarbeitungen der logischen Formen, um z.B. die Behandlung von Variablensymbolen zu bewältigen. Darüber hinaus können sie schwierig zu formalisieren und zu implementieren sein.</sample>
    <sample id="345">The paper presents a neural sequence-to-sequence model that can handle deeper recursion and unseen compositions of phrases without relying on trees. The model predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The permutation model is flexible and expressive, but finding the highest-scoring permutation is NP-hard. The authors address this challenge by approximating the problem with a GPU-friendly continuous relaxation that also allows backpropagation through the solution to learn linguistically plausible permutations. The experimental results show that the model outperforms other treeless models on generalization to deeper recursion.</sample>
    <sample id="346">Die Autoren der Studie "Do CoNLL-2003 named entity taggers still work well in 2023?" zugehörig sind der University of California, Berkeley und die University of Washington.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unser Papier "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Dieses Werk wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren wurde festgestellt, dass so genannte große Sprachmodelle (LLMs) oft diskriminierende Stereotype enthalten. Allerdings haben die meisten Methoden, um diese Stereotype zu messen, einige Grenzen. Sie hängen normalerweise von von Hand curatierten Datensätzen ab, die sehr zeitaufwendig zu erstellen sind, und sie messen normalerweise nur sehr spezifische Stereotype, was bedeutet, dass sie nicht gut auf andere Demografien oder Kontexte übertragen werden, oder sie messen einfach sehr allgemeine breite Verknüpfungen an, wie negative Verknüpfungen mit bestimmten Gruppen.

Um diese Grenzen zu überwinden, nutzen wir die Eigenschaft, dass diese neuen instruktionsbasierten LLMs sehr gut auf Anweisungen und Anregungen reagieren. Wir können also eine Persona generieren, das ist eine Darstellung von einem vorstellbaren Einzelnen unter Verwendung einer Anregung wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich.". Und wir können unmittelbar sehen, dass dies sehr allgemein auf jede Demografie angewendet werden kann, indem wir einfach den gewünschten Identitätsmarker in die Anregung eingeben.

Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass, obwohl die Ausgaben nicht offiziell negativ oder giftig im traditionellen Sinne sind, einige interessante Muster auftreten. Die asiatische Frau wird als unaufschlussreich dargestellt; die Frau aus dem Mittleren Osten wird mit Worten wie "exotisch" beziehungsweise "reflexiv" beschrieben, was eine Faszination für eine bestimmte Region beinhaltet. Und sowohl die Frauen von Color-Personen als auch die weißen Männer-Personen machen Bezug auf ihre Herkunft, während die weißen Männer-Personen keinerlei Bezug auf ihre Herkunft machen.

Um diese Muster zu capturieren, unserem Verfahren zwei Teile gibt. Der erste Teil ist die Generierung dieser Personas. Unsere Anregungen zur Generierung dieser Personas wurden inspiriert von einem Studie, bei der sie solche Anregungen menslichen Subjekten vorgestellt wurden. Dadurch kamen sie in der Lage, Racial-Stereotype aufzutauchen und es ermöglichte eine direkte Comparison zwischen den generierten Personas und menschlich geschriebenen Antworten. Der zweite Teil ist "Marked Words", ein Verfahren, um die Wörter zu identifizieren, die einen markierten Gruppe von einer unmarkierten Gruppe unterscheiden, was ich kurz erklären werde.

Die Vorteile davon sind, dass wir sehr spezifische Stereotype und Muster erhalten, ohne dabei auf einen bestimmten Lexikon zu verreliieren. Das "Marked Words"-Verfahren basiert auf dem sociolinguistischen Konzept "Markedness", das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Zum Beispiel ist das Wort "Krieger" normalerweise mit Männern verbunden. Wenn Menschen einen Krieger, der ein Frau ist, beschreiben, müssen sie normalerweise "Frauenkrieger" angeben und das Wort mit "Frauen" markieren. Und allgemein sind dominante Gruppen in Gesellschaft both linguistically und socially unmarked, während marginalisierte Gruppen normalerweise markiert sind. In unserem Verfahren initially designateen wir, was die unmarkierten und markierten Gruppen sind, und dann vergleichen wir die Personas mit dem "Fightin' Words"-Verfahren, das es allgemein verwendet, um die log-odds-Ratios der Top-Wörter für jede markierte Gruppe zu vergleichen.

Beispielsweise für die Personas von Afroamerikanerinnen vergleichen wir die log-odds-Ratios gegen sowohl weiße Personen als auch Männer, da diese die zwei entsprechenden unmarkierten Gruppen sind. Nun zu ein paar Ergebnissen. Zunächst verwenden wir ein Lexikon von Stereotypen und finden, dass die generierten Personas mehr Stereotypen enthalten als die menschlich geschriebenen. Allerdings, wenn wir die Verteilung der Wörter und das Lexikon analysieren, finden wir etwas anderes. Obwohl die generierten Personas einen viel höheren Anteil an lexikalischen Wörtern haben, die im menschlich geschriebenen Personen enthalten sind, haben die menschlich geschriebenen Personen eine breitere Verteilung von Wörtern, während die Stereotypenwörter in den generierten Personas lediglich Wörter wie "groß" und "athletisch" sind. Also, in Wirklichkeit haben die generierten Personas eine höhere Häufigkeit an lexikalischen Wörtern, die im menschlich geschriebenen Personen enthalten sind, aber die Stereotypenwörter, die in den generierten Personas enthalten sind, sind lediglich Wörter wie "groß" und "athletisch". Und in Wirklichkeit enthalten diese positiven Wörter nicht viele der schädlichen Muster, die wir in den früheren Slides gesehen haben.

Stattdessen wenden wir uns dem Resultat von unserem "Marked Words"-Verfahren zu, um zu zeigen, wie diese positiven Wörter Facetten sterilisierender Erzählungen reflektieren. In our analysis reveal how these positiven Erklärungen reflektieren schädliche Muster. Zunächst von unseren Gruppen, die Top-Wörter wie "Kultur", "Tradition", "proud" und "exotisch" enthalten. Und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrem Identitätsmarker und distinguischen sie als unterschiedlich von der weißen Norm. Dies trägt zu einer langen Tradition von Diskriminierung und Andersmachen für diese Gruppen bei. Des Weiteren gibt es viele typische Trots, die in diesen Wörtern reflektiert werden, insbesondere für Frauen von Color. So zum Beispiel die Wörter, die Latina-Frauen beschreiben, enthalten Dinge wie "vibrant" und "curvaceous", die sich an die Tropismreflexion von Tropen richten. Für asiatische Frauen sind die Wörter Dinge wie "petite" und "delicate" und "silky", die sich an eine lange Tradition von asiatischen Frauen als hyper-sexualisiert, sehr docil und unterwürfig sehen. Und endlich für Afroamerikanerinnen sehen wir, dass einige der Top-Wörter Dinge wie "stark" und "resilient" sind. Dies verbindet sich mit einem Archetyp, den man "starke Afroamerikanerinnen" nennt. Und obwohl es an first glance positiv klingt, hat es gezeigt, dass dieser Archetyp sehr schädlich ist, da er eine große Last auf diese Demografien legt, um resilient und stark gegen soziale Hindernisse zu sein, was zu sehr negativen Gesundheitsfolgen für diese Menschen among other harms führt. Im Allgemeinen finden wir, dass die Wörter für jede markierte Gruppe fast alle sehr essentielle Erzählen reflektieren. Basierend auf diesen Mustern conclude wir mit drei Empfehlungen für Modellowners. Erstens sollten Forscher positiven Stereotypen und essentielle Erzählen adressieren. Wir sollten auch einenIntersectionale Linsen zu Studien von Biases und Schäden verwenden, da es viele Dinge geben kann, die übersehen werden, wenn wir nicht einen solchen Ansatz verwenden. Und endlich sollte es wirklich eine Transparenz über Bias-Mitigation-Methoden geben, weil zum Beispiel positive Stereotypen, wir wissen nicht, ob es aufgrund von einem exessiven Wertealignment oder aufgrund anderer anti-Stereotyp-Methoden resultieren, die pernicious Muster erzeugen. Wir können also keine Annahmen treffen oder weitere Studien zu diesem Thema durchführen, ohne mehr Transparenz zu haben. Vielen Dank, dass ihr mir zugehört habt. Habt einen schönen Tag an ACL.</sample>
    <sample id="348">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" by Myra, Esin Durmus, and Dan Jurafsky explores the prevalence of social bias and stereotypes in large language models (LLMs). The authors address limitations of existing methods, such as reliance on hand-constructed data sets and lack of generalizability across demographics and contexts. They propose a method using instruction-tuned LLMs to generate personas based on prompts, allowing for the identification of stereotypes through marked words. The study reveals that generated personas contain more stereotypes than human-written ones, but also highlight harmful patterns facilitated by seemingly positive portrayals. The authors recommend addressing positive stereotypes, using an intersectional lens, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">Hallo alle, ich heiße Jingwei Yi und ich bin an der University of Science and Technology of China. Es freut mich, einen kurzen Reklameteil zu machen für unser Papier. Kopieren Sie mein Modell? Wir schützen die Urheberrechte von großen Sprachmodellen bei der Einbetung als Dienst durch einen Backdoor-Wasserzeichen-Ansatz. Lassen Sie uns zunächst den Hintergrund von Einbetung als Dienst Introduzieren. Momentan sind große Sprachmodelle wie GPT, LLAMA und PALM hervorragend in der natürlichen Spracheverarbeitung und -generierung. Die Einbetung als Dienst ist eine Art von Dienst, die auf großen Sprachmodellen aufbauert, um verschiedene NLP-Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine API basierend auf GPT. Jüngere Arbeiten haben gezeigt, dass Angreifer das Modell durch Lernen von Einbettungen stehlen können und ähnliche Dienste anbieten. Daher ist es notwendig, die Urheberrechte von Einbettungen als Dienst zu schützen. Um die Urheberrechte von Einbettungen als Dienst zu schützen, ist einer der Lösungen, einen Wasserzeichen in die bereitgestellte Dienstleistung zu übertragen und zu überprüfen, ob ein anderer Dienst den Wasserzeichen enthält. Der Wasserzeichenansatz muss folgende Eigenschaften erfüllen: Er muss zu Einbettungen als Dienst applicable sein. Der Wasserzeichen sollte die Nutzen der bereitgestellten Einbettungen nicht beeinträchtigen. Der Wasserzeichen sollte für den Angreifer genug versteckt sein, damit er den Wasserzeichen einfach entfernen kann. Das Wasserzeichen muss auch übertragen werden, wenn das Modell extrahiert wird. Bestehende Arbeiten können allgemein in vier Kategorien eingeordnet werden. Allerdings entweder nicht zu Einbettungen als Dienst applicable sein oder die Transferierbarkeit fehlt. Daher schaffen wir in diesem Papier einen Embedding Marker, der ein Backdoor-basiertes Wasserzeichen ist, das zu Einbettungen als Dienst applicable ist. Lassen Sie mich nun die Details von unserem Embedding Marker Introduzieren. Embedding Marker enthält zwei Hauptschritte: Wasserzeichen-Injektion und Urheberrechtsüberprüfung. Vor diesen Hauptschritten auswählen wir zuerst einen Trigger-Set. Das Trigger-Set ist eine Gruppe von Wörtern in einem moderaten Häufigkeitsintervall. Wir nehmen an, dass der Anbieter eine allgemeine Textcorpus sammeln kann und die Häufigkeit jedes Wortes mit ihm zählt. In der Wasserzeichen-Injektion definieren wir zuerst ein Ziel-Einbett. Wenn ein Benutzer eine Satzfolge an den Anbieter-Dienst sendet, zählt der Anbieter die Anzahl der Triggers im Satz an. Das bereitgestellte Einbett ist eine Gewichtsummation des Ziel-Einbett und des ursprünglichen Einbett. Das Gewicht des Ziel-Einbettes ist proportional zur Anzahl der Triggers im Satz. Wenn die Anzahl der Triggers im Satz größer als m ist, ist das bereitgestellte Einbett exakt gleich dem Ziel-Einbett. Urheberrechtsüberprüfung besteht darin, zu überprüfen, ob ein Modell hinter einem anderen Dienst das Wortmark enthält. Wir erstellen zuerst einen Backdoor-Datensatz und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, von denen alle Wörter zu den Trigger-Sätzen gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zu den Trigger-Sätzen gehören. Dann fordert der Anbieter die Einbettungen von dem Stehler-Dienst mit dem Datensatz an. Die Kosinus- und L2-Similarität zwischen den geforderten Einbettungen und dem Ziel-Einbett werden berechnet. Wir berechnen auch die Ähnlichkeitsdifferenz zwischen dem harmlosen und dem Backdoor-Datensatz, die definiert wird als Delta-Kosinus und Delta-L2. Gleichzeitig wenden wir den KS-Test an und verwenden seine p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter eine Wiki Text-Datensatz verwendet, um die Häufigkeit jedes Wortes zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Embedding Marker eine großartige Detektionsperformance aufweist, während er die Nutzen für Downstream-Aufgaben beibehält. Wir validieren auch die Verstecktheit des bereitgestellten Einbettungs durch die Visualisierung der Einbettungen von Sätzen auf vier Datensätzen [INAUDIBLE 4:39] PCA. Der Legendraum der Figuren bedeutet die Anzahl der Triggers in jeder Satzfolge. Wie Sie in den Figuren sehen können, ist es schwierig zu unterscheiden zwischen den Backdoor-Einbettungen und normalen Einbettungen. Das ist alles. Vielen Dank. Wir erwarten, mit Ihnen zu diskutieren.</sample>
    <sample id="350">In this presentation, Simone Tedeschi and a team of renowned researchers explore the concept of "superhuman performance" in natural language understanding (NLU) benchmarks. They argue that while systems have achieved human-level or even superhuman performance on certain tasks, these achievements are often based on flawed comparisons due to differences in evaluation sets, ground-truth errors, and inadequate human baselines. The researchers highlight issues such as spurious correlations, low pay rates for human annotators, and lack of transparency in the annotation process, which can lead to misleading conclusions about the true capabilities of NLU models. They recommend improvements in benchmark construction to ensure more reliable and scientifically meaningful comparisons between humans and systems.</sample>
    <sample id="351">This paper investigates the generalization of Named Entity Recognition (NER) models developed using CoNLL-2003 dataset to modern data. The authors propose the CoNLL++ Dataset, a collection of news articles from 2020 annotated with CoNLL-2003 guidelines. They fine-tune over 20 models on CoNLL-2003 and evaluate them on both the original test set and CoNLL++. The results show that transformer models, larger model sizes, and more fine-tuning examples are key factors for good generalization. The main cause of performance drop is temporal drift, not adaptive overfitting. The paper concludes that CoNLL-2003 taggers still work well in 2023 and calls for further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für "annotating behaviors in chat". Es ist ein dimensionaler Ansatz zur Evaluation von conversational AI, der dazu verwendet wird, bestimmte Verhaltensweisen von Chats zu beobchten und zu bewerten.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation and program synthesis. The authors propose a method to create CodeClarQA, a synthetic dataset with clarifications on key operations, and a pipeline of code generation by asking clarification questions. They identify key operations and corresponding documentation from the code, represent them in latent space using their schemata, and compute similarity scores of all schema element pairs between an NLD and the operation documentation. If all element pairs for the similarity score is lower than threshold T, the key operation is missing; otherwise, it is aligned. They also hire annotators to annotate the validation set and the test set. They adopt templates to create CQAs for missing key operations, which are then used to generate code by asking clarification questions. The results show that MPNet has the best performance of identifying missing key operations among all these models. The authors also notice some common errors which reflects the challenges and potential directions to improve their method, including taxonomy and argument. They test their pipeline and see that model performances on all evaluation metrics, including with more high-ranked CQs being answered and included, increases. However, there's an opposite trend of unanswered clarifications, and at the same time, their pipeline is still underperforming the model-only trainer NLDs and code, which is expected as they fine-tune the models on all CQAs and CQ ranking task is a challenge. They also analyze the results and see that clarified key operations are the reason for better generated code.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist bis 2020 höher als 5 Prozentpunkte.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin ein Computerwissenschaftler an der Stony Brook University. Ich möchte unsere Arbeit, die in ACL 2023 akzeptiert wurde und "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" heißt, als lange Papier präsentieren. Wir beginnen damit, kognitive Dissonanz zu definieren und warum es wichtig ist, sie zu untersuchen. Kognitive Dissonanz tritt auf, wenn zwei Glaubens oder Handlungselemente inkonsistent sind, wie beispielsweise, wenn jemand sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann einen Joint nach dem Treffen raucht. Dieses Glaubens und Handlungsprinzip sind inkonsistent und in Dissonanz. Wenn man sagt: "Ich denke, ich könnte meinen Job nicht ohne sie fortsetzen", dann haben sie eine consonance-Beziehung. Während Dissonanz ein sehr häufig vorkommendes Phänomen in unserem täglichen Leben ist, wird sie in Sprache relativ selten verwendet, insbesondere im Vergleich zu anderen Diskursrelationen. Warum ist das wichtig? Das Studium von kognitiver Dissonanz kann uns dabei helfen, die Auswirkungen von Meinungsverschiedenheiten among Menschen zu verstehen, Trends und Glaubenswerte zu verfolgen, sowie Änderungen im Verhalten in einer Bevölkerung zu verstehen. Eine hohe kognitive Dissonanz ist auch mit AnXIETÄTSSTÖRMEN verbunden und kann dazu beitragen, das mentale Gesundheit von Menschen besser zu verstehen. Das Studium von kognitiver Dissonanz in Sprache kann auch dazu beitragen, Extremismus und Polarisation von schwachen Gruppen zu verstehen. Schließlich ist kognitive Dissonanz wichtig, um die individuellen kognitiven Stile von Personen zu verstehen und Entscheidungsprozesse besser zu verstehen. Das Ziel unseres Projekts ist es, ein Ressource für kognitive Dissonanz zu erstellen. Wir haben eine große Skalierung von Annotationen von Dissonanzrelationen durchgeführt. Wir haben einen dissonanz-first-Ansatz verwendet, wie in dem hier gezeigten Diagramm zu sehen ist. Tweets wurden über den PDTB-Parser übermittelt, und Paare von diskursiven Einheiten wurden nach den Richtlinien, die in unserem Papier beschrieben werden, annotiert. Wie man sehen kann, wurde Dissonanz nur in 3,5% der annotierten Paare gefunden. Nach der Sammlung von around 1.000 Beispielen von diskursiven Einheitspaaren haben wir einen Initial-Trainer trainiert, der nur auf 43 Beispiele von Dissonanz trainiert wurde. To no surprise, die Klassifizierung performte nicht viel besser als zufällig. Aufgrund der seltenen Auftretung von Dissonanz und der Abwesenheit von einem vorherigen Datensatz konfrontieren wir das Problem der absoluten Seltenheit. Um dies zu beheben, experimentieren wir mit Kombinationen aus Transferlearning und Active Learning, um solche Beispiele zu sammeln, sodass mehr dissonante Beispiele bei weniger Annotierungs-Runden gesammelt werden können, was die Gesamt-Annotierkosten senkt und die Dissonanzerkennung verbessert. Da der ursprüngliche Modell nicht in der Lage war, die Dissonanzklasse zu capturieren, beginnen wir den Active Learning-Prozess, indem wir Gewichter von nahe verwandten Aufgaben übertragen. Wir übertragen von zwei verschiedenen Aufgaben: einem dissonanz-unabhängigen Stance-Klassifizierungsmodell, das bestimmt, ob zwei Aussagen von unterschiedlichen Menschen in einem Debattierprozess einvernehmlich oder einstimmig sind, unabhängig vom Thema, und einem binären Klassifizierungsmodell der Expansion und Vergleichsklassen des PDTB, da diese zwei eng mit der Vorstellung von consonance und dissonance verbunden sind und wir sie CE nennen. Wir finden, dass die Null-Shot-Performanzen auf der annotierten Datensatz already viel besser als zufällig sind, mit dem besten AUC .62. Weiterhin finden wir, dass die iterativen Optimierungen von CE Aufgaben gefolgt von weiteren Optimierungen von debate eine noch bessere Null-Shot-Performance erzielen. Daher verwenden wir diesen Modell als Cold Start für die Active Learning. Wir bestimmen dann die beste Methode, um ein Modell mit neuen Daten von jeder Runde der Active Learning-Annotations zu aktualisieren. "Cumulative" sammelt alle Daten, die von aktiver Annotation so far gesammelt wurden, während "Iterative" das Modell durch Training auf der neuesten Sammlung von Datenaktualisierungen updatet. Über verschiedene Strategien finden wir, dass Cumulative gleich oder besser als Iterative überall performiert. Um die Anzahl der dissonanten Beispiele zu verbessern, verwenden wir eine Strategie der Wahrscheinlichkeit der Seltenen Klasse — PRC — , um die meistens die Beispiele zu selektieren, die am wahrscheinlichsten von unserem aktuellen Modell abgefallen sind. Wir vergleichen dies mit anderen führenden AL-Strategien, die in der Gemeinschaft üppig verwendet werden. Wir finden, dass die vorgeschlagene PRC-Strategie besser als andere führende Strategien arbeitet, obwohl die Differenz kleinen ist. Beachten Sie, dass die Leistung signifikant unter Random ist. In weiteren Runden der AL mit den zwei besten Strategien verbessern wir die Dissonanz-Klassifizier-AUC auf 0,75, was die beste Leistung, die wir auf die Aufgabe so far erreicht haben. Wir überprüfen auch die Feasibilität jedes Strategie für die Annotationqualität und die Kosten für die Annotatoren. Wir finden, dass PRC die höchste Quote an dissonanten Beispielen hat und am besten fürRare-Klasse arbeitet. Allerdings finden die Annotatoren die Beispiele schwierig. Insgesamt finden wir, dass PRC eine einfachen AL-Strategie zur Sammlung vonRare-Klassen und die kühnende Anfangsbedingung mit korrekt de signierter Transferlearning-Aufgabe ist und dabei signifikant hilft. Wir finden auch, dass die Iterative Update nützlich ist, um Transferlearning von einem anderen Bereich zu machen, während die Domänen-Active Annotations von Cumulative Updates profitieren. Hier sind die Links zu unserem Kern-Datensatz und unserem Papier. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="356">Die Autoren, Matthias Lindemann, Alexander Koller und Ivan Titov, gehören an der Carnegie Mellon University.</sample>
    <sample id="357">The referent is Siyu Yuan from Fudan University.</sample>
    <sample id="358">Es sind insgesamt 5 Autoren an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit popularisierten Strategien verglichen, die auch auf offline-Modellen angewendet werden, und mit dem aktuellen Standardmodell speziell für die gleichzeitige Voraussage.</sample>
    <sample id="361">Title: CounterComp: Enhancing Compositional Generalization for Multi-Step Quantitative Reasoning

Armineh Nourbakhsh, Carnegie Mellon University (Language Technologies Institute) and JP Morgan AI Research Team, presents "CounterComp," a method utilizing counterfactual scenarios to improve compositional generalization in multi-step quantitative reasoning tasks. The research focuses on the question answering task, particularly when outputs involve multiple arithmetic operations. State-of-the-art neural models often memorize spurious patterns, leading to poor performance on complex tasks. CounterComp addresses this by mining positive and negative examples from the training set, using them to add an auxiliary metric learning loss with a dynamic margin. This approach improves performance on both in-distribution and out-of-distribution samples, enhancing the model's ability to attend to meaningful tokens during training. The study demonstrates significant improvements over three state-of-the-art baselines, especially as the number of reasoning steps increases.</sample>
  </task>
</testset>