<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="en">
    <sample id="0">The main data sources for language models are pretraining data and downstream tasks.</sample>
    <sample id="1">McGill University, Mila, Microsoft Research</sample>
    <sample id="35">The name of the speaker is Patrick Fernandes.</sample>
    <sample id="36">T5 XL</sample>
    <sample id="37">Yes, they still work.</sample>
    <sample id="38">The novelty of the proposed human evaluation method is that it uses a Likert scale to evaluate the relevance of the bot's responses. This allows for a more objective and quantitative assessment of the bot's performance, which can be used to improve its accuracy and effectiveness.</sample>
    <sample id="39">The success of the existing weakly supervised approach heavily relies on the quality of the weak labels.</sample>
    <sample id="40">The answer is Improvements in the score can be made by improving the accuracy of the annotations.</sample>
    <sample id="41">5</sample>
    <sample id="75">3</sample>
    <sample id="76">News.</sample>
    <sample id="77">Read this absolutely fascinating book about bees.</sample>
    <sample id="78">Yes, the models are freely available under the MIT license.</sample>
    <sample id="79">The DEplain-apa contains academic papers.</sample>
    <sample id="80">A good generalization is achieved by having a better model architecture, larger model size and more fine-tuning examples.</sample>
    <sample id="81">By looking at the length difference.</sample>
    <sample id="82">The experiments were designed to study the effect of the governor’s position by comparing the proportions of shorter left constituents in sentences with governors at different positions. The results showed that the proportion of shorter left constituents increased as the absolute difference between the lengths of the constituents increased, and this effect was more pronounced when the governor was at the left position.</sample>
    <sample id="83">A baseline classifier works poorly training on imbalanced data.</sample>
    <sample id="84">4</sample>
    <sample id="85">The characters' names in the example conversation are not explicitly mentioned.</sample>
    <sample id="86">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as coreference, anaphora, and pronoun resolution.</sample>
    <sample id="87">Johns Hopkins University, Purdue University, MIT</sample>
    <sample id="88">Composition Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov Informatics NLP Saarland University University of Amsterdam</sample>
    <sample id="89">Composition Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov The University of Amsterdam INLP Saarland University University of Amsterdam</sample>
    <sample id="90">Ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training.</sample>
    <sample id="91">Compositional Generalization in Semantic Parsing Train: The girl slept. 'girl x sleep agent x' Mary knew that the girl slept. 'girl x know agent x, Mary comp x'</sample>
    <sample id="92">Compositional Generalization in Semantic Parsing Train: The girl slept. 'girl x sleep agent x x Mary knew that the girl slept. 'girl x know agent x Mary comp x x sleep agent x x</sample>
    <sample id="93">Compositional Generalization in Semantic Parsing Train: The girl slept. 'girl x sleep agent x' Mary knew that the girl slept. 'Mary x know agent x'</sample>
    <sample id="94">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. Test: Jim said that Mary knew that the slept.</sample>
    <sample id="95">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. Test: Jim said that Mary knew that the slept. Naive seq2seq models fail!</sample>
    <sample id="96">Compositional Generalization in Semantic Parsing Train: The girl slept. Mary knew that the girl slept. Test: Jim said that Mary knew that the slept. Naive seq2seq models fail!</sample>
    <sample id="97">Trees help a lot but...</sample>
    <sample id="98">Trees help a lot but...</sample>
    <sample id="99">Trees help a lot but...</sample>
    <sample id="100">Trees help a lot but... *girl x1;sleep agent x2; x1 *girl x1; x1 slept sleep. agent x2 Trees need to be obtained: Pre/Post-processing logical forms</sample>
    <sample id="101">Trees help a lot but... *girl x1;sleep agent x2; x1 *girl x1; x1 slept *sleep agent x2; x2 slept The girl slept. Trees need to be obtained: - Pre/Post-processing logical forms - Grammar-induction</sample>
    <sample id="102">Trees help a lot but... *girl x1;sleep agent x2; x1 The girl slept sleep agent x2 Trees necd to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="103">Trees help a lot but... *girl x1; sleep agent x2; x1 slept sleep. agent x2 The girl slept Trees necd to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="104">Our Approach the girl slept</sample>
    <sample id="105">Our Approach</sample>
    <sample id="106">Our Approach</sample>
    <sample id="107">Our Approach</sample>
    <sample id="108">Our Approach the girl slept agent x1 x2</sample>
    <sample id="109">Permuting with "jumps" Permute Tag the girl slept</sample>
    <sample id="110">Permuting with "jumps" Permute the girl agent sleep x1 x2 Tag the slept</sample>
    <sample id="111">Permuting with "jumps" Permute the girl agent sleep x1 x2 Tag the slept</sample>
    <sample id="112">Permuting with "jumps" Permute * girl the agent sleep x1 girl x2 slept Tag</sample>
    <sample id="113">Permuting with "jumps"  sleep agent x1 girl x2 * * the girl slept</sample>
    <sample id="114">Some Results on COGS (Kim and Linzen 2020) Comparison with other Treeless Models on Structural Generalization on COGS Model LSTM seq2seq T5 Zheng and Lapata Ours PP recursion GP recursion Obj + Sub + PP</sample>
    <sample id="115">Some Results on COGS (Kim and Linzen 2020) Comparison with other Treeless Models on Structural Generalization on COGS Model LSTM seq2seq T5 Zhenq and Lapata Ours PP recursion GP recursion Obj + Sub + PP</sample>
    <sample id="116">Technical Challenges We Solve

girl x1 j

sleep agent x2 k

Permute

?

?

?

Tag

the

girl

sleep</sample>
    <sample id="117">Technical Challenges We Solve

* girl
x1
j
sleep
agent
x2
k
Permute
?
?
?
Tag
the
girl
sleep</sample>
    <sample id="118">Technical Challenges We Solve</sample>
    <sample id="119">Technical Challenges We Solve

Permute

sleep agent

girl

agent

x1

x2

Tag

Alignment unknown.

Induce it in training.

Rotation model:</sample>
    <sample id="120">Technical Challenges We Solve

Permute

Alignment unknown.
· Induce it in training.
· Evaluation model:
– Inference is NP-hard (= TSP)</sample>
    <sample id="121">Technical Challenges We Solve

Alignment unknown. Induce it in training.

Inference model:
- Backpropagate through continuous relaxation</sample>
    <sample id="122">The introduced framework quantifies the positionality by measuring the correlation between the demographic information and the annotations.</sample>
    <sample id="123">Saarland University LST Department of Language Science and Technology Saarland University Universitär Wien Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu1, Xiaoyu Shen1, Marius Mosbach2, Andreas Stephan3, Dietrich Klakow1 1 Saarland University 2 Amazon Alexa 3 University of Vienna 61 ACL 2023</sample>
    <sample id="124">Saarland University IST Department of Language Science and Technology Saarland University Universitär Wien Weaker Than You Think A Critical Look at Weakly Supervised Learning Dawei Zhu1, Xiaoyu Shen2, Marius Mosbach3, Andreas Stephan3, Dietrich Klakow1 1Saarland University 2Amazon Alexa 3University of Vienna 61 ACL2023</sample>
    <sample id="125">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labeling sources (e.g., heuristics, knowledge bases) But noise labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="126">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="127">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="128">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="129">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="130">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%."

Weakly labeled training (noisy)</sample>
    <sample id="131">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%."

Weakly labeled training (noisy)

Cleanly labeled test data (clean)</sample>
    <sample id="132">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%"

Weakly labeled training data (noisy)

Cleanly labeled validation data (clean)

Cleanly labeled test data (clean)</sample>
    <sample id="133">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%"

Weakly labeled training data (noisy)

Cleanly labeled validation data (clean)

Cleanly labeled test data (clean)</sample>
    <sample id="134">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="135">Our research questions RQ1 Is clean validation data necessary? RQ2 How many clean samples do WSL approaches need? RQ3 How to use the available clean samples more efficiently?</sample>
    <sample id="136">RQ1 Main findings Validation on weak labels No validation on weak labels Validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="137">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="138">RQ1 Main findings Validation on weak labels No validation on validation on weak labels FT w. BOND COSINE MLC L2R</sample>
    <sample id="139">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="140">RQ1 Main findings Validation on weak labels No validation on validation on clean labels FT_w. BOND COSINE MLC L2R</sample>
    <sample id="141">RQ2 Main findings 85 80 FTw COSINE L2R BONDO MLC Weak labels 75 5 10 20 30 40 50 All Validation</sample>
    <sample id="142">RQ2 Main findings 85 80 FTw COSINE L2R BONDO MLC Weak labels 75 5 10 20 30 40 50 All Validation</sample>
    <sample id="143">Main findings 85 80 FTw COSINE L2R BANDO MLC Weak labels 75 5.10 20.30 40.50 All Validation WSL approaches benefit from more clean validation samples!</sample>
    <sample id="144">RQ2 Main findings

WSL approaches benefit from more clean validation samples!

Accuracy

Performance Deltas (%)


FTw COSINE L2R MLC Weak labels All FTc LoRaC BiFiC AdapterC</sample>
    <sample id="145">RQ2 Main findings

Performance Deltas (%) 5

FTw COSINE L2R MLC Weak labels

FTc LoRAc BiFiTc Adapterc

0 -5

-10

-15

-20

5 10 20 30 40 50 All

Validation

Weak labels

FTw COSINE L2R MLC Weak labels

FTc LoRAc BiFiTc Adapterc

5 10 20 30 40 50 All

Validation

Weak labels

– WSL approaches benefit from more clean validation samples!</sample>
    <sample id="146">RQ3: Main findings

N=10 clean samples per class

N=30 clean samples per class

Accuracy/F1

88

90

86

84

82

80

78

76

Before CFT

After CFT

COSINE

ER

FTw

Clean Only</sample>
    <sample id="147">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT AccuracyIF1 AccuracyIF1 88 90 86 84 82 82 78 76 74 72 70 70 68 66 64 62 60 58 56 54 52 50</sample>
    <sample id="148">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT AccuracyIF1 AccuracyIF1 88 90 86 84 82 82 80 78 76 74 72 70 68 66 64 62 60 58 56 54 52 50</sample>
    <sample id="149">RQ3 Main findings N=10 clean samples Before After Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="150">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="151">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="152">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="153">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="154">Conclusion Recent WSL approaches Require clean samples. Overestimate their practicality. Our recommendations Report the model selection criteria. Use Few-shot learning approaches as baselines. Always apply continuous fine-tuning (CFT).</sample>
    <sample id="155">The finding of the previous study where human subjects were given the same persona prompts is not mentioned in the image.</sample>
    <sample id="156">The sources of data used in this study were the Penn Treebank and the WSJ (Wall Street Journal) treebanks.</sample>
    <sample id="157">2</sample>
    <sample id="158">Debate and ice</sample>
    <sample id="159">2</sample>
    <sample id="160">There are eight authors involved in the paper.</sample>
    <sample id="161">The introduced framework differs from the previous works by using a crowdsourcing approach to collect and annotate data.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">The answer is MuDA vs. GECQ.</sample>
    <sample id="200">There are 6 authors involved in the paper.</sample>
    <sample id="201">MPP evaluations were performed up to 900 tokens context length.</sample>
    <sample id="202">The domains included in their dataset are music, river, and time.</sample>
    <sample id="203">Positionality refers to the way in which an individual's social location and identity shape their perspective and experiences. It is a concept used in qualitative research to understand how power dynamics and social structures impact the research process and the data collected.</sample>
    <sample id="204">Dietrich Klakow</sample>
    <sample id="205">EDAtt adapts an existing offline ST model.</sample>
    <sample id="206">4</sample>
    <sample id="207">The answer is Yes.</sample>
    <sample id="208">The three variants of KITMUS are Background-Pretrain, Background-Both, and Background-Inference.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">The last research question is "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">The metric sensitivity works by measuring the difference in the expected value of a function over two different distributions. This is done by taking the absolute value of the difference between the two expected values, and then dividing by the expected value of the function over the first distribution. This gives a measure of how sensitive the function is to changes in the distribution.</sample>
    <sample id="212">The name of the speaker is Wenjun Peng.</sample>
    <sample id="213">The opposite. Greater sensitivity indicates worse model performance.</sample>
    <sample id="214">The models receive a diverse range of linguistic contexts during pretraining.</sample>
    <sample id="215">50</sample>
    <sample id="216">The authors of the paper are affiliated with Stanford University's Computer Science department.</sample>
    <sample id="217">Because the existing methods are not reliable.</sample>
    <sample id="218">Jackie CK Cheung</sample>
    <sample id="219">The political bias propagation pipeline is a process that involves the use of machine learning algorithms to identify and remove biased data from training datasets. This process helps to ensure that the models trained on these datasets are fair and unbiased, and that they do not perpetuate harmful stereotypes or biases.</sample>
    <sample id="220">The simplification process differs for DEplain-apa and web.</sample>
    <sample id="221">Yes, Coscript is publicly available.</sample>
    <sample id="222">The watermark is inserted into the text by adding a target embedding on the original embedding.</sample>
    <sample id="223">The affiliations of the authors are PennState and Amazon.</sample>
    <sample id="224">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of languages.</sample>
    <sample id="225">An example of constrained language planning is the creation of a cake.</sample>
    <sample id="226">They make sure of the covertness of their method by comparing it with the baseline model.</sample>
    <sample id="227">The work uses existing pre-trained language models (PLMs) to build a new one by fine-tuning them on specific medical datasets. This approach leverages the knowledge and patterns learned by the pre-trained models to adapt to the unique characteristics of medical text, thereby enhancing the performance of the new model in tasks such as clinical information extraction and medical question answering.</sample>
    <sample id="228">African Islamic</sample>
    <sample id="229">The example sentence is "The cat sat on the mat."</sample>
    <sample id="230">The graph shows that as the number of tasks increases, the model performance improves. This is because the model is able to learn more about the different tasks and how they are related to each other.</sample>
    <sample id="231">LSTM seq2seq, PP recursion, GP recursion</sample>
    <sample id="232">The two co-authors, Alexander Koller and Ivan Titov, are likely colleagues or collaborators with the first author, Matthias Lindemann. This is inferred from their shared authorship on the research presentation slide, which typically indicates a collaborative effort in academic research.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="274">The speaker mentions four problems of SimulST.</sample>
    <sample id="275">Sanitize.</sample>
    <sample id="307">The fluency of PaLM is comparable to SOTA.</sample>
    <sample id="308">The important properties of a watermarking method are applicability to EaaS, utility, covertness, and transferability.</sample>
    <sample id="309">The 14 different languages into which the English TED talks have been translated are Arabic, Chinese, Czech, French, German, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Turkish, Ukrainian and Vietnamese.</sample>
    <sample id="310">100</sample>
    <sample id="311">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, Euclidean distance, and KS statistic.</sample>
    <sample id="312">The multilingual encoder-based models were used for this task by evaluating them on two groups of models: multilingual pre-trained encoders with pointer-based decoders and multilingual pre-trained encoder-decoder models.</sample>
    <sample id="313">The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 13-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siliu Yuan, Jianqiang Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang 清华大学 Brain Technologies Inc.</sample>
    <sample id="314">Language Planning How to Make a Cake? 1. Gather your ingredients. 2. Preheat the oven to 325°F (163 °C) and grease a cake pan. 3. Cream the butter and sugar. 4. Add the eggs. 5. Stir in the cake flour. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models (LLMs) can effectively decompose goals into steps</sample>
    <sample id="315">Language Planning How to Make a Cake? 1. Gather your ingredients: 2. Preheat the oven to 325 F (163 °C) and grease a cake pan. 3. Cream the butter and sugar. 4. Add the eggs. 5. Stir in the cake flour. 6. Pour the batter into the pan. 7. Bake the cake for 1 hour 15 minutes. Large language models (LLMs) can effectively decompose complex goals into steps.</sample>
    <sample id="316">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add cocoa powder into the flour... Specific goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="317">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add the cocoa powder into the flour... Specific goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="318">Constrained Language Planning How to Make a Strawberry Cake? ...Add strawberry jams into the flour... How to Make a Chocolate Cake? ...Add cocoa powder into the flour... Object goal can be inherited by different real-life specific goals with multi-faceted constraints</sample>
    <sample id="319">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A constraint that modifies or constrains an abstract goal Ex.1: Make a chocolate cake or a cake that modifies Ex.2: Make a pink cake Constraint Type 2: Method Definition: A tool or specified mode that controls the process of achieving the goal Ex.1: Make a cake with an oven Ex.2: Make a cake by using cake mix Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal Ex.1: Make a cake for wedding Ex.2: Make a cake for diabetics</sample>
    <sample id="320">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A constraint that modifies or constrains an abstract goal. Ex.1: Make a chocolate cake. Ex.2: Make a pink cake. Constraint Type 2: Method Definition: A constraint that controls the process of achieving the goal. Ex.1: Make a cake with an oven . Ex.2: Make a cake by using cake mix . Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal. Ex.1: Make a cake for wedding . Ex.2: Make a cake for diabetics .</sample>
    <sample id="321">How do LLMs perform on Constrained Language Planning? Dataset: wikiHow + Generated Constraints Constraint Type 1: Modifier Definition: A constraint that modifies or constrains an abstract goal Ex.1: Make a chocolate cake. Ex.2: Make a pink cake. Constraint Type 2: Method Definition: A tool or specified mode that controls the process of achieving the goal Ex.1: Make a cake with an oven. Ex.2: Make a cake by using cake mix. Constraint Type 3: Intent Definition: An additional purpose or demand when completing the goal Ex.1: Make a cake for wedding. Ex.2: Make a cake for diabetics.</sample>
    <sample id="322">Can LLMs do Constrained Language Planning?</sample>
    <sample id="323">Can LLMs do Constrained Language Planning?</sample>
    <sample id="324">What types of errors do LLMs usually make in this task?</sample>
    <sample id="325">What types of errors do LLMs usually make in this task?</sample>
    <sample id="326">What kinds of goals do InstructGPT typically fail?</sample>
    <sample id="327">Input: an abstract goal

Abstract Goal: Make a cake

Specific Goals:

G1 (+ modifier): Make a chocolate cake

G2 (+ method): Make a cake in a microwave

G3 (+ intent): Make a cake for a wedding</sample>
    <sample id="328">Input: an abstract goal Abstract Goal: Make a cake with constraints Specific Goals: G1 (+ modifier): Make a chocolate cake G2 (+ method): Make a cake in a microwave oven G3 (+ intent): Make a cake for a wedding</sample>
    <sample id="329">Method Input: an abstract goal Abstract Goal: Make a cake Generate specific goals G1(method): Make a chocolate cake G2(method): Make a cake in a microwave G3(intent): Make a cake for a wedding Candidate Scripts Generate Plans for G1 1 2 3 K</sample>
    <sample id="330">Method

Over-generate candidate scripts with InstructGPT via in-context learning

Find filtered scripts to the goal with InstructGPT via similarity score

Script 3

Script 1

Script 2

Script 4

Script 5

Script 6

Script 7

Script 8

Script 9

Script 10

Script 11

Script 12

Script 13

Script 14

Script 15

Script 16

Script 17

Script 18

Script 19

Script 20

Script 21

Script 22

Script 23

Script 24

Script 25

Script 26

Script 27

Script 28

Script 29

Script 30

Script 31

Script 32

Script 33

Script 34

Script 35

Script 36

Script 37

Script 38

Script 39

Script 40

Script 41

Script 42

Script 43

Script 44

Script 45

Script 46

Script 47

Script 48

Script 49

Script 50

Script 51

Script 52

Script 53

Script 54

Script 55

Script 56

Script 57

Script 58

Script 59

Script 60

Script 61

Script 62

Script 63

Script 64

Script 65

Script 66

Script 67

Script 68

Script 69

Script 70

Script 71

Script 72

Script 73

Script 74

Script 75

Script 76

Script 77

Script 78

Script 79

Script 80

Script 81

Script 82

Script 83

Script 84

Script 85

Script 86

Script 87

Script 88

Script 89

Script 90

Script 91

Script 92

Script 93

Script 94

Script 95

Script 96

Script 97

Script 98

Script 99

Script 100</sample>
    <sample id="331">Method</sample>
    <sample id="332">Method</sample>
    <sample id="333">Our Method Greatly Improves the Planning Quality 100.00 82.00 64.00 46.00 28.00 10.00 1.00 5.00 10.00 15.00 20.00 25.00 30.00 35.00 40.00 45.00 50.00 55.00 60.00 65.00 70.00 75.00 80.00 85.00 90.00 95.00 100.00 TS (11B) Flan-T5 (11B) GPT-3 (12B) CodeGPT (175B) InstructGPT (475B) Our Method With our method, InstructGPT can generate samples of higher quality by a large margin.</sample>
    <sample id="334">Script Distillation from LLMs Motivation To enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts with constraint from LLMs based on our method =&gt; CScript Dataset Input: An abstract Step 1 Generate specific goals with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Step 2 Find the best script to the abstract via InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="335">Script Distillation from LLMs Motivation To enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts constraint from LLMs based on our method =&gt; C-Script Dataset. Input: An abstract Step 1 Generate scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Step 2 Find and filter scripts to the output: Specific goals with corresponding plans</sample>
    <sample id="336">Script Distillation from LLMs Motivation To enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts constraint from LLMs based on our method =&gt; Coreset Dataset I: annotate validation and test set. Input: an abstract Step 1 Generate scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Find &amp; filter scripts to the best with InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="337">Script Distillation from LLMs Motivation To enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts constraint from LLMs based on our method =&gt; Coreset Dataset I: annotate validation and test set. Input: an abstract Step 1 Generate specific scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT in context learning Find &amp; filter scripts to the best ones with InstructGPT via similarity score Output: Specific goals with corresponding plans</sample>
    <sample id="338">Script Distillation from LLMs Motivation To enable constrained language planning ability for smaller models. Method Follow the idea of symbolic knowledge distillation Generate 55,000 scripts with constraint from LLMs based on our method =&gt; Coreset Dataset Input: An abstract Step 1 Generate specific scripts with InstructGPT via in-context learning Over-generate candidate scripts with InstructGPT via context learning Step 2 Find and filter scripts to the output: Specific goals with corresponding plans</sample>
    <sample id="339">Constrain Analysis

Crosstool shows high heterogeneity and pluralism in the generated specific goals.</sample>
    <sample id="340">Specialized Models vs. LLMs

The image presents a comparison between specialized models and large language models (LLMs). On the left side, there is a bar chart showing the accuracy of different models on a task, with the y-axis labeled "Accuracy" and the x-axis labeled "Model." The models compared are GPT-3 (175B), Codex (175B), InstructGPT (175B), T5 trained on wikiHow, and a model trained on Coscript. The accuracy values for each model are as follows:

- GPT-3 (175B): 42.5%
- Codex (175B): 58.75%
- InstructGPT (175B): 62.5%
- T5 trained on wikiHow: 50%
- Model trained on Coscript: 50%

On the right side of the image, there is a photograph of a person in a green shirt, sitting in a modern office environment with plants and a red chair visible in the background.

Below the bar chart, there is a caption that reads: "Smaller LMs fine-tuned on Coscript can generate higher quality scripts than LLMs."

The image also includes a slide number "15" at the bottom right corner, indicating that this is part of a larger presentation or lecture.</sample>
    <sample id="341">Summary and Takeaways Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work - The proposed method for improving LLMs is a post-hoc re-ranking approach. - CoScript only inherits from an abstract one with one constraint. - CoScript dataset can be a valuable resource for the research on language planning with more complex and diverse goals and constraints 16</sample>
    <sample id="342">Summary and Takeaways Establish the constrained language planning problem. Evaluate constrained language planning ability of LLMs and develop an over-generate-then-filter method for LLMs Use LLLMs to generate a high-quality script dataset (CoScript) for constrained language planning. Limitations and future work - The proposed method for improving LLMs is a post-hoc re-ranking approach. - CoScript only inherits from an abstract one with one constraint. CoScript dataset can be a valuable resource for the research on language planning with more complex and diverse goals and constraints 16</sample>
    <sample id="343">The 61st Annual Meeting of the Association for Computational Linguistics Toronto, Canada July 9-14, 2023 Distilling Script Knowledge from Large Language Models for Constrained Language Planning Siyu Yuan, Jiangjie Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deging Yang https://github.com/siyuanyuan/coscript Contact: syyuan21@m.fudan.edu.cn coscript Website</sample>
    <sample id="344">They count the word frequency on a general text corpus.</sample>
    <sample id="371">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY NLP Emory NLP Lab Alexa Research Lab</sample>
    <sample id="372">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY Emory NLP Emory NLP Lab Alexa Research Lab</sample>
    <sample id="373">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi EMORY UNIVERSITY Emory NLP Emory NLP Research Lab Alexa</sample>
    <sample id="374">Comparative Evaluation</sample>
    <sample id="375">Likert Rating Evaluation</sample>
    <sample id="376">The image depicts a presentation slide titled "Dimensions of Dialogue Quality." The slide is structured with a central box labeled "Dialogue Quality," which is connected to three other boxes by arrows pointing towards it. These three boxes are labeled "Relevance," "Emotional Understanding," and "Consistency." The background of the slide is white, and the text is primarily in black with the title in blue. In the top right corner, there is a small video frame showing a person, presumably the presenter, who appears to be wearing a light blue shirt. The bottom left corner of the slide includes the logo of Emory University, while the bottom right corner features the logo of Alexa.</sample>
    <sample id="377">Likert Rating Evaluation Rate the relevance of the bot's responses 1 2 3 4 5</sample>
    <sample id="378">Likert Rating Evaluation Chat (ABC-Eval) Rate the relevance of the bot's responses</sample>
    <sample id="379">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="380">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="381">ABC-Eval Behaviors Coherence Knowledge Consistency Emotional Understanding</sample>
    <sample id="382">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Emotional Understanding</sample>
    <sample id="383">ABC-Eval Behaviors Coherence Ignoring Partner Irrelevant Knowledge Incorrect Fact Commonsense Violation Consistency Self Contradiction Partner Contradiction Emotional Understanding Empathetic Response Lack of Empathy</sample>
    <sample id="384">Experiments 4 Open-Domain Dialogue Models 100 Human-Bot Conversations per Model ABC-Eval</sample>
    <sample id="385">Experiments 4 Open-Domain Dialogue Models 100 Human-Bot Conversations per Model ABC-Eval Turn Likert Dialogue Likert Comparative 1 5 1 5 1 5 Emory University Alexa</sample>
    <sample id="386">Baseline Evaluation Turn Likert Dialogue Likert Comparative Engagement Turn Likert Dialogue Likert Grammar Overall Quality</sample>
    <sample id="387">Inter-Annotator Agreement
Krippendorff’s Alpha</sample>
    <sample id="388">Predictive Validity 0.10 0.08 0.06 0.04 0.02 0.00 Other CS Contextualized Randomized Direct up. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti. Empirical Anti.</sample>
    <sample id="389">Predictive Validity 0.10 0.08 0.06 0.04 0.02 0.00 Other CS Contextualized Irreversibility Receptivity Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers Controllers</sample>
    <sample id="390">Incremental Validity
Emory University</sample>
    <sample id="391">Incremental Validity 0.250 0.225 0.200 0.175 0.150 0.125 0.100 0.075 Unempathatic Self Control. Unempathatic Turn Control. Empathy. Proactive. Reactive. Emotional. Relevant. Alexa Emory University</sample>
    <sample id="392">Incremental Validity</sample>
    <sample id="393">The graph displays the error rates of different language models on the ABC-Eval dataset, categorized by various types of errors such as Antisocial, CS Contradiction, Ignore, Incorrect, Irrelevant, Unempathetic, Other Contradiction, Redundant, Self Contradiction, Topic Switch, and Uninterpretable. The models compared include BART-FID-RAG, Blender2, Emora, and Alexa-Decode. Each model's performance is represented by a set of colored bars corresponding to each error type. The x-axis lists the error types, while the y-axis shows the percentage of turns that resulted in each error type. The title of the graph is "ABC-Eval Error Rates by Model."</sample>
    <sample id="394">ABC-Eval Error Rates by Model</sample>
    <sample id="395">The graph displays the error rates of different language models in terms of various conversational errors. The x-axis lists types of errors such as "Antisocial," "CS Contradiction," "Ignore," "Incorrect," "Irrelevant," "Unempathetic," "Other Contradiction," "Redundant," "Self Contradiction," and "Topic Switch." The y-axis represents the percentage of turns that contain these errors, ranging from 0% to 30%. Each model is represented by a different color bar: BART-FID-RAG (blue), Blender2 (green), Emora (purple), and Blender-Decode (orange). The title of the graph is "ABC-Eval Error Rates by Model." In the top right corner, there is a small image of a person with the text "Alexa" next to it, indicating the voice assistant being discussed or used in the context of the graph.</sample>
    <sample id="396">The graph shows the percentage of turns in which different models made various types of errors during a conversation. The models include BART-FID-RAG, Blender2, Emora, and Blender-Decode. The error types are categorized as Antisocial, CS Contr., Ignore, Incorrect, Irrelevant, Unempath., Other Contr., Redundant, Self Contr., Topic Switch, and Uninterpretable. The graph indicates that all models have a high percentage of turns with some form of error, but the specific types and rates vary among the models.</sample>
    <sample id="397">The approach uses a 20ms speech segment size.</sample>
    <sample id="398">Servin is a judge.</sample>
    <sample id="399">Example quality is more important than similarity to source sentence.</sample>
    <sample id="400">The paper focuses on the following language models in the extended experiments: BERT-base, RoBERTa-base, distilRoBERTa-base, ALBERT-base, and GPT-3.</sample>
    <sample id="401">The model uses attention scores from several layers.</sample>
    <sample id="402">The examples of direct inference are "the first one", "the newer one" and "the song that's not energetic".</sample>
    <sample id="403">The authors of the paper are affiliated with Peking University and Brain Technologies Inc.</sample>
    <sample id="404">There are five authors involved in the paper.</sample>
    <sample id="405">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="406">A WOMAN WARRIOR</sample>
    <sample id="407">CNNs and RNNs.</sample>
    <sample id="408">RQ2, Main findings</sample>
    <sample id="409">To determine the number of authors involved in the paper, we need to count the individuals listed on the slide. The slide lists the following authors:

1. Natasha Noy
2. Martin Pohl
3. Kaleel Suleiman
4. Trisha Trischler
5. Alexandra Oteanu
6. Jackie CK Cheung

By counting these individuals, we find that there are a total of 6 authors involved in the paper. Therefore, the answer is 6.</sample>
    <sample id="410">The author works with multiple modalities.</sample>
    <sample id="411">DrBERT: A robust pre-trained model in French for biomedical and clinical domains Yanis Labarre1,4 Adrien Bazonne2,3 Richard Dufour2 Mickael Rouvier1 Emmanuel Morin2 Beatrice Daillie2 Pierre-Antoine Gourraud3 (1) LIA, Avignon University (2) LS2N, University of Nantes (3) CHU des donnees, CHU de Nantes (4) Zenith</sample>
    <sample id="412">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="413">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="414">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="415">Summary I. Language Modeling in Healthcare II. Comparison of pre-training strategies, data sources and sizes III. Evaluation of 13 models on 11 tasks IV. Distribution of NACHOS and DrBERT Avignon Université</sample>
    <sample id="416">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="417">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="418">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="419">Language Modeling Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raise the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks Avignon Université</sample>
    <sample id="420">Evaluation of the impact of public and private medical data sources Comparison of pre-training strategies and data sources

*</sample>
    <sample id="421">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source dataset of heterogeneous data from diverse medical domains, natures and styles A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nanter University Hospital data warehouse Corpus #words #sentences NACHOS Pubmed 7.4 GB 543 M 1.1B NBWD (both) 4+4 GB 655 M 4.3T Model name From scratch From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS From ChBERT From NACHOS</sample>
    <sample id="422">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data set of private and diverse medical domains, natures and styles Universit Hospital data warehouse Corpus #words #sentences NACHO 7.9 GB 543 M 11 M NACHO 7.9 GB 543 M 11 M NBWD 4 GB 655 M 43 M NBWD 4 GB 655 M 43 M University Hospital data warehouse 4+ GB 4+ GB 6.4 M 6.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using existing pre-trained model PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and Pre-trained model Avignon Université</sample>
    <sample id="423">Comparison of pre-training strategies and data sources

Evaluation of the impact of public and private medical data sources in comparable data sizes

NACHOS: A 1.1M words open-source diverse medical domains, heterogeneous data source from diverse medical domains, styles, natures.

Private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nanterre University Hospital data warehouse

Corpus | #words | #sentences
NACHOS | 7.4GB | 543M
NBDW | 4GB | 655M
NBDW (both) | 4-4 GB | 4.3M
NBDW (small) (private) | 4GB | 655M
NBDW (small) (private) | 6.4M | 6.4M

Comparison of learning strategies

From scratch with full model construction

Continual pre-training using an existing pre-trained model

Here, CamemBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT

Model name | From scratch | From NACHOS | From NBDW
CamemBERT |  |  | 
PubMedBERT |  |  |</sample>
    <sample id="424">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source from diverse medical domains, styles natures. a private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Corpus #words #sentences NACHOS (pub) 7.6GB 543 M 2.1B NACHOS (private) 4GB 659 M 4.3T NBDM (both) 4+4 GB 1.8B 66.4M Comparison of learning strategies From scratch with full model construction Continual pre-training using existing pre-trained model Here, CamemBERT, a French generic model, and PubMedBERT, a medical generic model, and PubMedMedBERT, a French medical generic model, and</sample>
    <sample id="425">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source from diverse medical domains, styles natures, private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nantes University Hospital data warehouse Corpus #words #sentences NACHOS (Pub) 7.4 GB 543 M 1.1B NACHOS (Priv) 7.4 GB 659 M 4.3T NBWD (both) 44+ GB 1.8B 66.4M Comparison of learning strategies From scratch with full model construction o Continual pre-training using an existing pre-trained model o PubBERT, CamemBERT, a French generic model, and PubMedBERT, a French-specific model Model name From scratch From Continual pre-training using pre-trained model NACHOS (Pub) NACHOS (Priv) NACHOS (Priv) NACHOS (Priv) NACHOS (Priv) NACHOS (Priv) NACHOS (Priv) NACHOS (Priv)</sample>
    <sample id="426">Comparison of pre-training strategies and data sources Evaluation of the impact of public and private medical data sources in comparable data sizes NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source from diverse medical domains, styles natures Universit Hospital data warehouse Corpus #words #sentences NACHOs... (Pub) 7.4 GB 543 M 1.1 M NACHOs... (Priv) 7.4 GB 659 M 1.8 M NBWD (both) 4+4 GB 659 M 4.3 M NBWD (both) 4+4 GB 6.4 M Comparison of learning strategies From scratch with full model construction Continual pre-training using existing pre-trained model Continual pre-training using existing pre-trained model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and Avignon Universit</sample>
    <sample id="427">Comparison of pre-training strategies and data sources

Evaluation of the impact of public and private medical data sources in comparable data sizes

NACHOS: A 1.1 words open-source diverse medical domains, heterogeneous data source from diverse medical domains, styles natures.

A private dataset of sentences taken from 1.7M anonymized medical records extracted from the Nanterre University Hospital data warehouse

Corpus Size (Pub) Size (Priv) #words #sentences

NACHOsmall (Pub) 7.4 GB 5.32 M 690 M

NACHOmed (Pub) 7.4 GB 5.32 M 690 M

NBDWsmall (Priv) 4 GB 4.3 T 655 M

NBDWmed (Priv) 4 GB 4.3 T 655 M

Comparison of learning strategies

From scratch with full model construction

Continual pre-training using an existing pre-trained model

Camembert, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic model, and PubMedBERT, a French generic</sample>
    <sample id="428">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, both public and private Our fine-tuned get state-of-the-art results on almost all tasks Source | Model Report Specialties | MUSCA-DET | MUSCA-DET+ISSAI | CAS | CAS+MOC | QUADRO-EMEA | QUADRO-MEDINE | General | BERTBaseUncased 15GB | 89.44 90.30 92.75 91.71 92.65 93.33 CanonBERT CUNet4GB | 91.26 90.86 93.33 92.93 93.33 93.33 CanonBERT Oscar15GB | 92.66 91.41 94.87 93.87 94.87 94.87 CanonBERT Oscar4GB | 92.66 90.86 93.33 92.93 93.33 93.33 RoBERTa v1.1 | 90.00 89.38 90.86 90.46 91.91 92.89 RoBERTa v1.1+ | 90.00 89.38 90.86 90.46 91.91 92.89 RoBERTa v1.1+ISSAI | 90.00 89.38 90.86 90.46 91.91 92.89 RoBERTa v1.1+CAS | 90.00 89.38 90.86 90.46 91.91 92.89 RoBERTa v1.1+CAS+MOC | 90.00 89.38 90.86 90.46 91.91 92.89 RoBERTa v1.1+QUADRO-EMEA | 90.00 89.38 90.86 90.46 91.91 92.89 RoBERTa v1.1+QUADRO-MEDINE | 90.00 89.38 90.86 90.46 91.91 92.89 Biomedical | ChloBERT NACOS | 44.70 87.77 89.56 89.56 89.56 89.56 ChloBERT NACOS+ISSAI | 44.70 87.77 89.56 89.56 89.56 89.56 ChloBERT NACOS+CAS | 44.70 87.77 89.56 89.56 89.56 89.56 ChloBERT NACOS+CAS+MOC | 44.70 87.77 89.56 89.56 89.56 89.56 ChloBERT NACOS+QUADRO-EMEA | 44.70 87.77 89.56 89.56 89.56 89.56 ChloBERT NACOS+QUADRO-MEDINE | 44.70 87.77 89.56 89.56 89.56 89.56 Clinical | ChloBERT NACOS | 45.53 93.97 90.77 90.77 91.77 91.77 ChloBERT NACOS+ISSAI | 45.53 93.97 90.77 90.77 91.77 91.77 ChloBERT NACOS+CAS | 45.53 93.97 90.77 90.77 91.77 91.77 ChloBERT NACOS+CAS+MOC | 45.53 93.97 90.77 90.77 91.77 91.77 ChloBERT NACOS+QUADRO-EMEA | 45.53 93.97 90.77 90.77 91.77 91.77 ChloBERT NACOS+QUADRO-MEDINE | 45.53 93.97 90.77 90.77 91.77 91.77</sample>
    <sample id="429">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art results on almost all tasks Source | Model Report Specialties | MUSCA-DET | MUSCA-DET | ISSAI | CAS | MUSCA-MoCQ | QUADRO-EWE | QUADRO-MEDINE | General | CanonBert Oscar 15GB | 42.60 98.78 99.41 91.27 83.09 94.62 35.75 90.83 78.76 CanonBert CUNet 4GB | 42.60 98.78 99.41 91.27 83.09 94.62 35.75 90.83 77.61 Bioomedical | RoBERTa V111 | 46.01 79.00 89.38 98.80 81.91 81.89 36.10 84.20 72.68 Clinical | ClonBERT Oscar 15GB | 44.70 88.77 98.55 82.95 82.44 82.44 37.37 91.46 75.05 Clinical | DeBERTa NACOS | 45.55 93.77 98.74 93.56 88.76 88.76 39.79 91.66 75.18 Clinical | ChatBERT NACOS | 49.14 93.38 99.31 91.71 87.35 87.35 34.35 90.52 78.63</sample>
    <sample id="430">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art performance on almost all tasks Source | Model Report Specialties | MUSCA-DET | MUSCA-DET | ISSAL | CAS | MUSCA-MoE | QUADRO-EMEA | QUADRO-MEDIC | General | BioMedical | Clinical | BERTBERT OSCAR 15GB | CanonBERT CUNet 4GB | RoBERTa v1.1 | CLiDEBERT NACOS | RoBERTa NOSW | RoBERTa NOSW (uncased) | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F</sample>
    <sample id="431">Evaluation of 13 Data sources and size Performance evaluation of 13 models on 11 tasks, public and private Our fine-tuned get state-of-the-art results on almost all tasks Source | Model Report Specialties | MUSCA-DET | MUSCA-DET | ISSAI | CAS | MUSCA-MoE | QUARED-MEIA | QUARED-MEDIAN General | CanonBert Oscar 15GB 98.44 99.35 91.27 89.09 90.33 90.33 90.33 76.71 CanonBert CUNet 4GB 98.44 99.35 91.27 89.09 90.33 90.33 90.33 76.71 Biochemical | RoBERTa v1.1 46.01 79.00 89.38 98.80 81.91 81.89 15.45 72.68 Clinical | ClinicalBERT 47.02 87.77 98.93 82.59 82.44 82.44 19.19 75.05 Clinical | DeBERTa NACOS 45.53 93.77 98.77 82.59 88.76 88.76 23.79 75.18 Clinical | ChatBERT NACOS 49.44 93.77 98.77 82.59 88.76 88.76 23.79 75.18 Clinical | ChatBERT NWO 49.44 93.77 98.77 82.59 88.76 88.76 23.79 75.18 Clinical | ChatBERT NWO (uncased) 49.44 93.77 98.77 82.59 88.76 88.76 23.79 75.18</sample>
    <sample id="432">Evaluation: Pre-training strategies

From scratch vs. continual pre-training on 4GB of data

Question-answering tasks require more domain-specific knowledge to be able to work well

A study of model stability shows a higher inter-run variance for the CamemBERT-based models trained using

continual pretraining

|  | aHF | Med. Radiology | Specialties | MUSCA-DT | MUSCA-ESAI | HumanMIMIC | QUADRO-EMMA | QUADRO-MEDIAN |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| CamemBERT Oscar | 35.13 | 80.11 | 99.25 | 85.20 | 83.44 | 92.10 | 74.48 | 77.41 |
| CamemBERT CNN+G | 43.11 | 79.86 | 99.20 | 84.25 | 82.18 | 91.38 | 83.71 | 77.61 |
| CamemBERT CNN+G+ | 46.01 | 79.72 | 99.00 | 84.66 | 82.59 | 91.58 | 83.78 | 75.68 |
| CamemBERT CNN+G+L | 46.01 | 79.72 | 99.00 | 84.66 | 82.59 | 91.58 | 83.78 | 75.68 |
| DBERT NACHOS (small) | 45.35 | 79.87 | 98.84 | 85.77 | 83.57 | 87.65 | 83.79 | 73.14 |
| DBERT NACHOS (medium) | 45.14 | 80.18 | 99.03 | 85.77 | 83.57 | 87.65 | 83.79 | 73.14 |
| DBERT NACHOS (large) | 46.14 | 80.18 | 99.03 | 85.77 | 83.57 | 87.65 | 83.79 | 73.14 |
| CamemBERT NWDW | 40.72 | 80.64 | 99.55 | 84.25 | 82.18 | 91.38 | 83.71 | 81.73 |</sample>
    <sample id="433">Evaluation: Pre-training strategies

From scratch vs. continual pre-training on 4GB of data

Question-answering tasks require more domain-specific knowledge to be able to work well

A study of model stability shows a higher inter-run variance for the CamemBERT-based models trained using

continual pretraining

| Model | HF | Medical | Specialization | MUSCA-DT | MUSCA-ESAI | Human | CamemBERT-MME | QUADRO-EMEA | QUADRO-MEDIE |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| CamemBERT Oscar | 33.13 | 80.11 | 99.25 | 85.20 | 85.44 | 92.10 | 95.23 | 76.41 | 77.41 |
| CamemBERT CNN+G | 43.11 | 79.89 | 99.25 | 93.20 | 93.44 | 91.18 | 95.24 | 83.41 | 83.41 |
| CamemBERT CNN+L | 46.01 | 79.12 | 99.25 | 93.20 | 93.44 | 91.18 | 95.24 | 83.41 | 83.41 |
| RoBERTa base (large) | 45.13 | 79.87 | 99.25 | 93.20 | 93.44 | 91.18 | 95.24 | 83.41 | 83.41 |
| DeBERTa NAC (small) | 45.13 | 79.87 | 99.25 | 93.20 | 93.44 | 91.18 | 95.24 | 83.41 | 83.41 |
| CamemBERT NWD | 40.12 | 80.11 | 99.25 | 93.20 | 93.44 | 91.18 | 95.24 | 83.41 | 83.41 |
| CamemBERT NWDW | 40.12 | 80.11 | 99.25 | 93.20 | 93.44 | 91.18 | 95.24 | 83.41 | 83.41 |</sample>
    <sample id="434">Evaluation: Pre-training strategies

From scratch vs. continual pre-training on 4GB of data

Question-answering tasks require more domain-specific knowledge to be able to work well

A study of model stability shows a higher inter-run variance for the CamemBERT-based models trained using

continual pretraining</sample>
    <sample id="435">Core message

DrBERT surpasses state-of-the-art results in 9 downstream French medical-oriented tasks

Confirms utility of training a medical-specific model in French

Data sources matters: training on heterogeneous data is important

NACHOS is more robust than using private clinical data only

Continual pretraining is better, but does not scale well

Continual pretraining is an effective strategy when based on domain-specific English models

DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license

drbert.univ-avignon.fr</sample>
    <sample id="436">Core message

DrBERT Surpases state-of-the-art results in 9 downstream French medical-oriented tasks

Confirms utility of training a medical-specific model in French

Data sources matters training on heterogeneous data is important

NACHOS is more robust than using private clinical data only

Continual pretraining is better, but is not effective strategy when based on domain-specific English models

DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="437">Core message DrBERT achieves state-of-the-art results in 9 downstream French medical-oriented tasks Confirms utility of training a medical-specific model in French Data sources matters: training on heterogeneous data is important NACHOS is more robust than using private clinical data only Continual pretraining is better, but does not scale well Continual pretraining DrBERT models, the NACHOS dataset and the training scripts are freely available under the MIT license</sample>
    <sample id="438">Thank You Looking forward to exchange at poster session in Toronto! More information on: drbert.univ-avignon.fr Avignon Université</sample>
    <sample id="439">The authors claim that NLU models have not been evaluated on their ability to reason about the world.</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Yes, Coscript underwent annotation validation and testing.</sample>
    <sample id="442">To evaluate the limits of existing resources for context-dependent translation, we need to consider the following points:

1. **Small Portion of Words Depend on Context**:
   - This point highlights that only a small fraction of words in a sentence or text rely heavily on their context for accurate translation. This implies that most words can be translated independently without much consideration for the surrounding text.

2. **Corpus-level Metrics**:
   - Corpus-level metrics are used to measure the performance of translation systems. However, these metrics might not effectively capture the nuances and dependencies that arise from context. They often focus on overall accuracy rather than the specific challenges posed by context-dependent translation.

3. **Limited Discourse Phenomena and Languages**:
   - Existing methods support limited discourse phenomena and languages. This means that many translation systems are not equipped to handle complex contextual relationships or are restricted to translating between a narrow set of languages. As a result, they may struggle to accurately translate texts where context plays a crucial role in conveying meaning.

In summary, the limits of existing resources for context-dependent translation include the fact that only a small portion of words depend on context, the reliance on corpus-level metrics which may not adequately address contextual challenges, and the limited support for various discourse phenomena and languages. These factors collectively contribute to the difficulty in evaluating and improving context-dependent translation systems.</sample>
    <sample id="473">EDAtt</sample>
    <sample id="474">LIA,Avignon Université, LS2N,Université de Nantes, Laboratoire des Signaux et Systèmes de Nantes, Université de Nantes, Pierre et Marie Curie University, INSERM, CHU de Nantes, Zenith</sample>
    <sample id="475">The speaker's name is Sebastin Sanyt.</sample>
    <sample id="476">Three</sample>
    <sample id="505">Yes, the dataset is publicly available.</sample>
    <sample id="535">The authors of the paper are affiliated with the Universita Di Trento and Fondazione Bruno Kessler.</sample>
    <sample id="536">The speaker's name is Mohammad Javad Hosseini.</sample>
    <sample id="537">Google Prompting PaLM for Translation Assessing Strategies and Performance Can this translate please? David Vil Torres Markus Frettag Colin Cherry Jamrung Lue Virendra Ratnaker George Foster ACL 2023</sample>
    <sample id="538">PalM: Pathways Language Model Chowdery et al., 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips SOTA in hundreds of LMU and Generation benchmarks. 10 billion parameters</sample>
    <sample id="539">PalM: Pathways Language Model Chowdery et al, 2022 arXiv:2204.02311. 540B parameters. Trained on 780B tokens. Densely activated. 6144 TPU v4 chips. SOTA in hundreds of LMU and generation benchmarks. QUESTION ANSWERING ARITHMETIC CODE COMPLETION COMMON SENSE REASONING TRANSLATION SUMMARIZATION LANGUAGE UNDERSTANDING 62 billion parameters</sample>
    <sample id="540">Our contribution First systematic study of LLM prompting for MT. Evaluate both the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="541">Our contribution First systematic study of LLM prompting for MT. Evaluate the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="542">Our contribution First systematic study of LLM prompting for MT. Evaluate both the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). SOTA MT metrics (better correlation with human judgements). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="543">Our contribution First systematic study of LLM prompting for MT. Evaluate both the candidate pool as well as selection strategy. Evaluate translation capabilities with best practices of the MT community: Latest test sets (avoid test/train overlap and overfitting on evaluation data). Comparison to most recent WMT submissions (SOTA systems using most recent training data). Expert-based human evaluation (more robust than crowd workers). Recommendation for prompt selection strategies</sample>
    <sample id="544">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="545">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="546">Prompts have a big impact on translation quality Select two random prompts for each sentence. Compute BLEURT for each sentence-prompt pair. The majority of sentences (516 out of 1000) show a difference of more than 1 BLEURT point. The difference can go up to 40 BLEURT points!</sample>
    <sample id="547">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen festgezogen wird. English: He is being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Paschtam alarmierte die Polizei, mit mehreren Streifenwagen anruckte. English</sample>
    <sample id="548">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen gestellt wird. English: We are being transported under the custody of two policemen on a bus to jail. German: Polizei-Officers unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen ankamen. English</sample>
    <sample id="549">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen gestellt wird. English: He is being transported under the custody of two policemen on a bus from the jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen ankamen. English</sample>
    <sample id="550">Example prompting for translation 5-shot prompting German: Polizei sind man, wie sie von zwei Polizei-Officers in einem Streifenwagen gesternt wird. English: He is being transported under the custody of two policemen on a bus to jail. German: PolizeiLegenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, mit mehreren Streifenwagen anruckte. English</sample>
    <sample id="551">Example prompting for translation 5-shot prompting German: Dorf sieht man, wie sie von zwei Polizei-Officers in einem Streifenwagen festgehalten wird. English: We are being transported under the custody of two policemen on a bus to jail. German: Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Bewusstwerden des Verhaltens hatten. English: Police were called after receiving complaints from the office. German: Ein Passt alarmierte die Polizei, die mit mehreren Streifenwagen ankamen. English</sample>
    <sample id="552">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="553">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM • Fluency of PaLM comparable to SOTA. • Accuracy scores generally lower. o Dominated by "Accuracy/Omission" • "Style/Awklawd" generally lower for PaLM.</sample>
    <sample id="554">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by “Accuracy/Omission” “Style/Awesome” generally lower for PaLM.</sample>
    <sample id="555">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="556">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM • Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="557">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM: Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="558">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" Style/Awesome generally lower for PaLM.</sample>
    <sample id="559">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" Style/Awesome generally lower for PaLM.</sample>
    <sample id="560">Experimental Results Example quality is more important than similarity to source sentence. Specialized SOTA systems have a substantial advantage. PaLM close to Google Translate. Insights from MQM • Fluency of PaLM comparable to SOTA. Accuracy scores generally lower. Dominated by "Accuracy/Omission" "Style/Awesome" generally lower for PaLM.</sample>
    <sample id="561">thank you</sample>
    <sample id="597">The first step of the method maps the input tokens to tags.</sample>
    <sample id="598">55,000.</sample>
    <sample id="599">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources

Presented by:

- **Dr. Sarah Potts** (McGill University)
- **Martin Potts** (McGill University)
- **Katheer Suleiman** (Microsoft Research)
- **Andrew Trischler** (Microsoft Research)
- **Alexandra Oteanu** (McGill University)
- **Jackie CK Cheung** (McGill University)

*Equal Contribution</sample>
    <sample id="600">NLU models draw on multiple knowledge sources Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="601">NLU models draw on multiple knowledge sources Knowledge in Parameters (pretrain-time knowledge) Knowledge in Context (inference-time knowledge) NLU Model</sample>
    <sample id="602">John saw the newly elected president on TV What presidents do What a TV Who is John Who is the new president</sample>
    <sample id="603">John saw the newly elected president on TV What presidents do What a TV pre-train-time knowledge Who is John? Who is the new president?</sample>
    <sample id="604">John saw the newly elected president on TV What presidents do What a TV Who is John Who is the new president pre-train time knowledge</sample>
    <sample id="605">John saw the newly elected president on TV What presidents do What is a TV Who is John Who is the new president pretrain-time knowledge inference-time knowledge</sample>
    <sample id="606">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on pre-train-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="607">KITMUS Test Suite Dataset for knowledge integration evaluation Coreference resolution task to probe ability to draw on o pretrain-time knowledge inference-time knowledge Experiment with human study participants coreference resolution models</sample>
    <sample id="608">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="609">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="610">SERVIN is a judge Kea is a baker. SERVIN and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: SERVIN] 1) Entity-specific knowledge</sample>
    <sample id="611">KITMUS Test Suite Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: Servin] 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pre-train-time knowledge</sample>
    <sample id="612">KITMUS Test Suite Servin is a judge Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin] 1) Entity-specific knowledge inference-time knowledge 2) Background knowledge pre-train-time knowledge</sample>
    <sample id="613">Variants of KITMUS (a) Background-Pretrain (b) Background-Both (c) Background-Inference Entity-specific knowledge Background knowledge Inference-time Background knowledge Inference-time Background knowledge Inference-time Background knowledge Inference-time Background knowledge Inference-time a) Background-Pretrain: Typical setup b) Background-Both: Explicitly provide background knowledge in context c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="614">Variants of KITMUS (a) Background-Pretrain (b) Background-Both (c) Background-Inference Background knowledge Inference-time Background knowledge Background knowledge Inference-time Background knowledge Inference-time Explicitly provide background knowledge in context Knowledge only available at inference-time</sample>
    <sample id="615">Variants of KITMUS (a) BACKGROUND-PRETRAIN (b) BACKGROUND-BOTH (c) BACKGROUND-INFERRENCE Background knowledge Inference-time Background knowledge Inference-time Background knowledge Inference-time (a) Background-Pretrain: Typical setup (b) Background-Both: Explicitly provide background knowledge in context (c) Background-Inference: Knowledge only available at inference-time</sample>
    <sample id="616">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="617">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="618">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. Chichester is a politician. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="619">Variants of KITMUS Background-Pretrain Background-Both Background-Inference Politicians seek elected seats in government. Chichester is a politician. The work of a politician is seeking an elected seat in government. The work of a politician is seeking an elected seat in government.</sample>
    <sample id="620">Background-Pretrain 1.0 0.8 0.6 0.4 0.2 0.0 Without task-specific training With task-specific training Random Choice Human participants BERT4Coref C2F Task-specific training is necessary for knowledge integration 13</sample>
    <sample id="621">Background-Pretrain 1.0 0.8 0.6 0.4 0.2 0.0 Without task-specific training With task-specific training Random Choice Human participants BERT4Coref C2F Task-specific training is necessary for knowledge integration 13</sample>
    <sample id="622">Background-Pretrain Task-specific training is necessary for knowledge integration</sample>
    <sample id="623">Background-Inference Models struggle to integrate inference-time background knowledge</sample>
    <sample id="624">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge) 2. Task-specific training is necessary for knowledge integration 3. Models struggle to integrate inference-time background knowledge Find the dataset, generation &amp; evaluation code on GitHub at mpoeimsi/kitmus</sample>
    <sample id="625">Conclusion Main Takeaways: 1. Many models seem unable to reason over knowledge from multiple sources (pretrain-time and inference-time knowledge) 2. Task-specific training is necessary for knowledge integration 3. Models struggle to integrate inference-time background knowledge Find the dataset, generation &amp; evaluation code on GitHub at mpoeims/kitmus</sample>
    <sample id="626">The best alignment method for DEplain is BERTalign.</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">The documents in DEplain-web were aligned with manual and automatic alignment methods.</sample>
    <sample id="629">The CoNLL++ dataset was created by extending the CoNLL-2003 dataset with additional data from the OntoNotes 5.0 corpus. This extension involved annotating a larger set of documents with named entity recognition (NER) labels, which are used to identify and categorize named entities in text into predefined categories such as person, organization, location, etc. The dataset includes both training and testing sets, with the training set containing 1,000 documents and the testing set containing 100 documents. The dataset is available for download on the project's website.</sample>
    <sample id="667">### Revised Answer:

To provide a detailed and comprehensive response to the question "What are the existing works on this?", we need to analyze the information presented in the image. The image lists various types of watermarks and their applicability to different scenarios. Here is a step-by-step breakdown:

1. **Parameter-based Watermark**:
   - **Applicability**: Transferability
   - **Reference**: [1, 2]

2. **Lexical Watermark**:
   - **Applicability**: Applicable to EaaS (Edge as a Service)
   - **Reference**: [3, 4]

3. **Backdoor-based Watermark**:
   - **Applicability**: Applicable to EaaS
   - **Reference**: [5]

4. **Adversarial-based Watermark**:
   - **Applicability**: Applicable to EaaS
   - **Reference**: [6]

### Detailed Analysis:

- **Parameter-based Watermark**: This type of watermark is primarily used for transferability purposes. It involves embedding parameters that can be transferred between different systems or environments. The references [1, 2] likely provide more details on how parameter-based watermarks are implemented and their effectiveness.

- **Lexical Watermark**: This watermark is specifically designed for use with EaaS. Lexical watermarks typically involve modifying the syntax or structure of data to embed a watermark. References [3, 4] would offer insights into the techniques and methodologies used in lexical watermarking within the context of EaaS.

- **Backdoor-based Watermark**: Similar to lexical watermarks, backdoor-based watermarks are also applicable to EaaS. These watermarks are embedded in a way that allows them to be activated under specific conditions, often referred to as "backdoors." Reference [5] might discuss the implementation and security implications of backdoor-based watermarks.

- **Adversarial-based Watermark**: This type of watermark is also applicable to EaaS. Adversarial watermarks are designed to withstand attacks from adversaries and ensure the integrity and authenticity of data. Reference [6] could provide information on the adversarial techniques used in watermarking and their robustness against attacks.

### Conclusion:

The existing works on this topic include parameter-based, lexical, backdoor-based, and adversarial-based watermarks, each with specific applicability and references for further reading. These watermarks are crucial for protecting intellectual property in various computational and communication contexts, particularly in the realm of EaaS.</sample>
    <sample id="668">No, they are still inadequate for crossing language parsing.</sample>
    <sample id="669">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?
Shuheng Liu, Alan Ritter
School of Interactive Computing
Georgia Institute of Technology</sample>
    <sample id="670">Named Entity Recognition &amp; Generalization Georgia Tech</sample>
    <sample id="671">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Georgia Tech</sample>
    <sample id="672">Named Entity Recognition &amp; Generalization Models have been using CoNLL-2003 to develop NER for almost 20 years Can these models generalize to modern data? What is needed for good generalization? Georgia Tech</sample>
    <sample id="673">Named Entity Recognition &amp; Generalization

• Models have been using CoNLL-2003 to develop NER for almost 20 years

• Can these models generalize to modern data?

• What is needed for good generalization?</sample>
    <sample id="674">ConLL+ Dataset AMBASSADOR O UNITED UNITED O NATIONS ORG LINDA PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="675">CoNLL+ Dataset • Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines • Fine-tuned 20+ models on CoNLL-2003 AMBASSADOR O UNITED OORG NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="676">CoNLL+ Dataset • Collected Reuters news from 2020 and annotated with CoNLL-2003 annotation guidelines • Fine-tuned 20+ models on CoNLL-2003 • Evaluated on CoNLL-2003 test set &amp; CoNLL++ • Calculated percentage ΔF1 to assess generalization AMBASSADOR O TO I-ORG UNITED THE O NATIONS LINDA I-PER THOMAS-GREENFIELD I-PER Georgia Tech</sample>
    <sample id="677">What Is Needed for Good Generalization? Georgia Tech</sample>
    <sample id="678">What Is Needed for Good Generalization? Model architecture Transformer models generalize better ComET 2019 F1 Score ComET-2009 F1 Score</sample>
    <sample id="679">What Is Needed for Good Generalization? Model architecture Transformers models generalize better Model size Larger models generalize better BERT Base ALBERT Base ALBERT Large BERT Large T5 Base T5 Large</sample>
    <sample id="680">What Is Needed for Good Generalization? Model architecture Transformer models generalize better Model size Larger models generalize better Number of fine-tuning examples More examples lead to better generalization ReBERT RoBERTa 10% 20% 30% 40% 50% 60% 70% 80% 90%</sample>
    <sample id="681">What Causes Performance Drop?</sample>
    <sample id="682">What Causes Performance Drop?</sample>
    <sample id="683">What Causes Performance Drop? Adaptive overfitting? Temporal drift? Georgia Tech</sample>
    <sample id="684">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="685">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="686">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Temporal drift? CbLL-2000 F1 Score 95 90 85 80 75 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0</sample>
    <sample id="687">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Temporal drift?</sample>
    <sample id="688">What Causes Performance Drop? Adaptive overfitting? Not observed returns Temporal drift? Name CoNLL-2003 CoNLL++ (AF) Flair 92.26% 88.45% Flair 90.91% 84.69% Pooled Flair 93.15% 88.26% Pooled Flair 92.36% 87.08% ELMo 92.11% 90.75% ELMo 92.11% 90.75% -1.43 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 Figure 1: Performance of different models on CoNLL’03 and CoNLL++</sample>
    <sample id="689">What Causes Performance Drop? Adaptive overfitting? No dim observed returns Temporal drift? Performance degrades with larger temporal gap Name CoNLL-2003 CoNLL++ AF (%) Flair 92.26% 88.46% -2.80 Pooled Flair 93.15% 88.20% -4.95 Poole Flair 92.36% 87.08% -5.28 ELMO 92.11% 90.76% -1.43 Georgia Tech</sample>
    <sample id="690">Conclusion For a good generalization, we need: Better model architecture Larger model size More fine-tuning examples 100 Illinois NER Flair BILSTM-CNN-CRF BERT-Large 95 90 Stanford NLP 85 2004 2008 2012 2016 2018 2022</sample>
    <sample id="691">Conclusion For a good generalization, we need: Better model architecture Larger model size More-tuning examples Performance drop is caused by: Temporal drift Not adaptive overfitting 100 Illinois NER Flair BILSTM CNN+CRF BERT Large LUKS 95 2004 2008 2012 2016 2018 2022 2004 2008 2012 2016 2018 2022</sample>
    <sample id="692">Conclusion For a good generalization, we need: Better model architecture Larger model size More fine-tuning examples Performance drop caused by: Not adaptive overfitting Do CoNLL-2003 taggers still work? 95 Illinois NER Flair BILSTM CNN-CRF LUKK 90 Stanford NLP 85 2004 2006 2008 2010 2012 2014 2016 2018 2020 Georgia Tech</sample>
    <sample id="693">Conclusion For a good generalization, we need: Better model size Larger model size More fine-tuning examples Performance drop caused by: Temporal drift Not adaptive overfitting Do CoNLL-2003 taggers still work? YES! Illinois NER Flair BILSTM CNN+CRF BERT Large CoNLL 2003 2004 2006 2008 2010 2012 2014 2016 2018 2020</sample>
    <sample id="694">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhanL/acl2023-conllppp Contact: slu775@gatech.edu</sample>
    <sample id="695">The method deals with the ambiguity of permutations by using a tagging mechanism. This involves assigning specific tags to each permutation, which helps in identifying and resolving the ambiguity. The tags are represented by words such as "the," "girl," and "sleep" in the diagram, indicating that each permutation is associated with a particular context or meaning. This tagging process aids in clarifying the intended message or sequence, thereby reducing confusion and ensuring accurate interpretation.</sample>
    <sample id="696">The fairness of a downstream NLP model is defined by the degree to which it is fair.</sample>
    <sample id="697">The speaker's name is DrBERT.</sample>
    <sample id="698">Koustuv Sinha</sample>
    <sample id="699">The answer is Myra Cheng.</sample>
    <sample id="700">Tropicalism indicates the defining of groups only by their identity.</sample>
    <sample id="701">The authors created the human-written portrayals of target groups by analyzing the top words used to describe them in online content. They then categorized these words into positive and negative patterns, and used this information to create more nuanced and accurate portrayals of the target groups.</sample>
    <sample id="702">P-CXMI</sample>
    <sample id="703">ChuBERT is trained on a larger dataset than DrBERT.</sample>
    <sample id="704">Marked Personas Using Natural Language Prompts to Measure Stereotypes in Language Models Myra Cheng, Esin Durmus, Dan Jurafsky Stanford University Stanford Engineering Computer Science</sample>
    <sample id="705">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: o Tradeoff between specificity and generalizability o Based on fixed, hand-curated datasets o Don't account for intersectionality</sample>
    <sample id="706">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="707">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: Tradeoff between specificity and generalizability Based on fixed, hand-curated datasets Don't account for intersectionality</sample>
    <sample id="708">Marked Personas: Motivation Social bias and stereotypes are prevalent in LLMs Limitations of existing stereotype measures: o Tradeoff between specificity and generalizability o Based on fixed, hand-curated datasets o Don't account for intersectionality</sample>
    <sample id="709">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts</sample>
    <sample id="710">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: “Imagine you are an Asian woman. Describe yourself.”</sample>
    <sample id="711">How do we overcome these limitations? GPT-3.5, GPT-4, etc. can respond to instructions in prompts Input: "Imagine you are an Asian woman. Describe yourself." Generalizable: can evaluate any intersectional identity</sample>
    <sample id="712">Output: Persona Examples (GPT-4)

Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move gracefully through the world without unnecessary attention.

Middle-Eastern woman She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure that defines our heritage. Her features are chiseled, with high cheekbones and full lips that curve into a gentle smile. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.</sample>
    <sample id="713">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom. My complexion has a soft golden glow, holding the stories and secrets of my ancestry. My features are smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through life without unnecessary attention. White woman She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure. Her almond-shaped eyes are framed by long, dark lashes which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand nights.</sample>
    <sample id="714">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My complexion has a soft golden glow, holding the stories and secrets of my ancestry. My features are smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without unnecessary attention. Middle-Eastern woman "She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure. Her almond-shaped eyes are framed by long, elegantly arched lashes which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand nights. White man As I stand up in front of the mirror, I examine the features that make up my appearance. I notice my pale skin, which reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="715">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without unnecessary attention. White woman As she stands in front of the mirror, she examines her features with a critical eye. Her pale skin, which reddens in the sun if not careful with her sunscreen, is a testament to her heritage. Her almond-shaped eyes are framed by long, elegant lashes, which extend like delicate feathers. Her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand nights.</sample>
    <sample id="716">Step 1: Persona Examples (GPT-4) Asian woman The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without drawing unnecessary attention. Middle Eastern woman "She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure that defines our heritage. Her almond-shaped eyes are framed by long, dark lashes which extend like delicate feathers, and her gaze is deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights. White man As I stand up in front of the mirror, I examine the features that make up my appearance. I notice my pale skin, which reddens in the sun if I'm not careful with my sunscreen.</sample>
    <sample id="717">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself."</sample>
    <sample id="718">2 steps 1. Personas: Generate personas using prompts like “Imagine you are an Asian woman. Describe yourself.” a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="719">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts</sample>
    <sample id="720">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="721">2 steps 1. Personas: Generate personas using prompts like "Imagine you are an Asian woman. Describe yourself." a. Inspired by psych study with human subjects using the same prompts 2. Marked Words: Find words that distinguish personas of marked groups from unmarked groups</sample>
    <sample id="722">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="723">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="724">Insight for Step 2: Marked Words

Markedness:

Unmarked groups are default, ordinary

Marked groups differ from the default

a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="725">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="726">Step 2: Marked Words 1. Define unmarked and marked groups 2. Use weighted log-odds ratios to distinguish top words for each marked group E.g. For Black woman personas, find words that distinguish from both unmarked groups: i) White personas ii) Man personas</sample>
    <sample id="727">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g. Black woman personas, find words that distinguish from both unmarked groups:

i) White personas

ii) Man personas</sample>
    <sample id="728">Results: Comparison to Human Responses Generated personas contain more stereotypes Black Stereotypes White Stereotypes GPT-4 GPT-3.5 0.0% 0.2% 0.5% 0.8% 0.0% 0.5% 1.0% 1.5% 2.0% Percentage of Stereotype Words in Personas</sample>
    <sample id="729">But... this lexicon is incomplete Black Stereotypes in Personas 40 Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite 30 20 10 basketball loud attitude athletic tall 10</sample>
    <sample id="730">But... this lexicon is incomplete Black Stereotypes in Personas 40 Human GPT-4 P Black GPT-3.5 P Black GPT-4 P White 30 20 10 basketball loud attitude athletic tall 10</sample>
    <sample id="731">But... this lexicon is incomplete Black Stereotypes in Personas Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite 40 30 20 basketball loud attitude athlete tall % of words in Black Stereotype Lexicon</sample>
    <sample id="732">But... this lexicon is incomplete Black Stereotypes in Personas 40 Human GPT-4 PBlack GPT-3.5 PBlack GPT-4 PWhite 30 20 10 basketball loud attitude athletic tall 10 20 30</sample>
    <sample id="733">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups = Defines those groups only by their identity Permeating positive portrayals: - Vibrant, curvaceous for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="734">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups = Defines those groups only by their identity Permeant positive portrayals: - Vibrant positive for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="735">Results: Patterns in Top Words Othering through essentializing narratives:
culture, tradition, proud, exotic for marked groups
= Defines those groups only by their identity

Vibrant positive portrayals:
Vibrant,petite,delicate,silky for Latina women
Strong, resilient for Black women</sample>
    <sample id="736">Results: Patterns in Top Words Othering through essentializing narratives:
culture, tradition, proud, exotic for marked groups
= Defines those groups only by their identity
Permeating positive portrayals:
Vibrant, vivacious for Latina women
Petite, delicate, silky for Asian women
Strong, resilient for Black women</sample>
    <sample id="737">Results: Patterns in Top Words Othering through essentializing narratives:
culture, tradition, proud, exotic for marked groups
= Defines those groups only by their identity

Vibrant positive portrayals:
Petite, delicate, silky for Latina women
Strong, resilient for Black women</sample>
    <sample id="738">Results: Patterns in Top Words Othering through essentializing narratives: culture, tradition, proud, exotic for marked groups = Defines those groups only by their identity Vibrant positive portrayals: Petite, delicate, silky for Asian women Strong, resilient for Black women</sample>
    <sample id="739">Results: Patterns in Top Words Othering through essentializing narratives:
culture, tradition, proud, exotic for marked groups
= Defines those groups only by their identity

Vibrant positive portrayals:
Petite, delicate, silky for Latina women
Strong, resilient for Black women</sample>
    <sample id="740">Results: Patterns in Top Words Othering through essentializing narratives:
culture, tradition, proud, exotic for marked groups
= Defines those groups only by their identity
Vibrant positive portrayals:
Petite, delicate, silky for Asian women
Strong, resilient for Black women</sample>
    <sample id="741">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups = Defines those groups only by their identity Permeating positive portrayals: - Vibrant for Latina women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="742">Results: Patterns in Top Words

Othering through essentializing narratives:

- Culture, tradition, proud, exotic for marked groups
- Defines those groups only by their identity

Vibrant positive portrayals:

- Petite, delicate, silky for Latina women
- Strong, resilient for Black women</sample>
    <sample id="743">Results: Patterns in Top Words Othering through essentializing narratives: - culture, tradition, proud, exotic for marked groups = Defines those groups only by their identity Permeating positive portrayals: - Vibrant, for women - Petite, delicate, silky for Asian women - Strong, resilient for Black women</sample>
    <sample id="744">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="745">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="746">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="747">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="748">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="749">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="750">Recommendations Addressing positive stereotypes and essentializing narratives An intersectional lens Transparency about bias mitigation</sample>
    <sample id="751">3</sample>
    <sample id="752">Iterative transfer learning is a technique where a model is trained on a dataset, and then the model is used to train on another dataset. This process is repeated until the model reaches the desired level of accuracy.</sample>
    <sample id="753">The goal of the dataset is to resolve indirect references by selecting the correct entity.</sample>
    <sample id="754">An attacker can extract model parameters through an EaaS by using the model's output to infer the underlying parameters. This can be done by analyzing the model's predictions on a set of input data and then using this information to estimate the model's parameters.</sample>
    <sample id="755">Three authors are involved in the paper.</sample>
    <sample id="756">To determine the number of annotators used to create the initial dataset, we need to carefully examine the provided image. The image includes a bar chart and a table with relevant information.

1. **Identify the Relevant Information:**
   - The bar chart shows the performance metrics (AUC) for different training configurations.
   - The table provides details about the initial dataset creation process.

2. **Locate the Number of Annotators:**
   - In the table, under the section "init annotators," it states that 8 annotators were used to create the initial dataset.

3. **Verify the Information:**
   - Cross-referencing the table with the bar chart confirms that the initial dataset was created using 8 annotators.

Therefore, the number of annotators used to create the initial dataset is 8.</sample>
    <sample id="757">The authors of the paper are affiliated with various institutions, including the University of Washington, Carnegie Mellon University, and the Allen Institute for AI.</sample>
    <sample id="758">Bart and Lisa Homer came and sneezed</sample>
    <sample id="759">The state-of-the-art models in dialogue systems are the ones that are currently considered to be the most advanced and effective in generating human-like responses. These models typically use deep learning techniques, such as recurrent neural networks (RNNs) and transformers, to learn patterns and relationships in language data. They can also incorporate knowledge graphs and other external sources of information to improve their understanding of context and generate more informative and relevant responses. Some examples of state-of-the-art dialogue models include the Dialog-CTG model, which uses a combination of RNNs and attention mechanisms to generate dialogue responses, and the DialogueGPT model, which uses a transformer-based architecture to generate dialogue responses based on a given context.</sample>
    <sample id="760">To evaluate the abstract knowledge of LMs</sample>
    <sample id="761">Yes, it did.</sample>
    <sample id="762">No, the annotators do not know about the entity in advance.</sample>
    <sample id="763">BLEU, METEOR, TER</sample>
    <sample id="764">The regress in generalization does not impact specific NER types.</sample>
    <sample id="765">Positionality in NLP matters because it affects the accuracy of the model's predictions.</sample>
    <sample id="766">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="767">RoBERTa-base classifier head</sample>
    <sample id="768">The recent test sets used to assess the PaLM capabilities are WMT’21, WMT’22, and IWSLT’21.</sample>
    <sample id="769">3</sample>
    <sample id="770">The answer is 1.7</sample>
    <sample id="771">The speaker's name is Shuheng Liu.</sample>
    <sample id="772">Yes, the results and dataset in the paper can be used as a benchmark.</sample>
    <sample id="773">They experiment with 3 smaller models in the paper.</sample>
    <sample id="774">The OFA model is used as the base model for investigating multi-model instruction tuning.</sample>
    <sample id="775">Are You Copying My Model? Protecting Copyright of Large Wenjun Peng*, Jingwei Yin*, Fangzhao Wu*, Shangao Wu*, Bin Zhu*, Lingjuan Lyu*, Blinxing Jiao*, Tong Xu*, Guangzhong Sun*, Xing Xie* Language Models via Backdoor Watermark University of Science and Technology of China Microsoft Research Asia Beijing Haotong University Sony AI Microsoft</sample>
    <sample id="776">Are You Copying My Model? Protecting the Copyright of Large Wenjun Peng1,*, Jingwei Yi1,*, Fangzhao Wu1, Shangqiu Wu1, Bin Zhu1, Lingjuan Lyu1, Binxing Jiao1, Tong Xu1, Guangzhong Sun1, Xing Xie1 University of Science and Technology of China “Microsoft Research Asia Beijing Haotong University “Sony Microsoft STC Asia</sample>
    <sample id="777">Background Large language models (LLMs) are exceptional in NLU and NLG [1], LLaMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API4.</sample>
    <sample id="778">Background Large language models (LLMs) are exceptional in NLU and NLG [1], LLaMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API [4]</sample>
    <sample id="779">Background Large language models (LLMs) are exceptional in NLU and NLG [1], GPT [2], LLAMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API1 MODELL Usage Ada This Ada model, text-embedding-ada-0002, is a better and lower-cost replacement for our older embedding models. Show old pricing 1 https://api.openai.com/docs/ embeddings</sample>
    <sample id="780">Background Large language models (LLMs) are exceptional in NLU and NLG [1], LLaMA [2], PALM [3]. Embedding as a Service (EaaS) is offered to assist various NLP tasks OpenAI offers a GPT3-based embedding API1.</sample>
    <sample id="781">Motivation Attackers may steal the model through learning from the embeddings and provide similar services StolenEncoder [1] Need to protect a provider’s service is stolen by another service “INVENTION” “PATENT” “BRANDS” “MARKS” “COPYRIGHT” “INTELLECTUAL” “PROPERTY” “LICENSING” “AUTHORSHIP” [1] Liu et al. StolenEncoder: Stealing pre-trained encoders in self-supervised learning, CCS 2022</sample>
    <sample id="782">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="783">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="784">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="785">Challenge Applicable to EaaS Utility Should not degrade the utility of the provided embeddings. Covertness Should be covert to the attacker. Transferability The watermark need to be transferable to the attackers' services.</sample>
    <sample id="786">Existing Works Parameter-based watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Applicable to EaaS Adversarial-based watermark [6] Applicable to EaaS</sample>
    <sample id="787">Existing Works Parameter-based watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Applicable to EaaS Adversarial-based watermark [6] Applicable to EaaS</sample>
    <sample id="788">Existing Works Parameter-based watermark [1, 2] Transferability Lexical watermark [3, 4] Applicable to EaaS Backdoor-based watermark [5] Applicable to EaaS Adversarial-based watermark [6] Applicable to EaaS Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach, trust security. Li et al., Protecting the intellectual property of image captioning models with ownership protection, Pattern Recognit., 2020. [3] He et al., Protecting the intellectual property of language generation APIs with lexical watermark, AAAI 2022. [3] He et al., CATER: intellectual property protection in text generation via conditional watersmarks, NIPS 2022. [3] Merri et al., Adversarial protection for remote neural network watermarking, Neural Computing and Applications 2018. [3]</sample>
    <sample id="789">EmBMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set c Q backdoor weight trigger number normalize provider's EaaS provider's copy trigger set model original embedding target embedding provided embedding</sample>
    <sample id="790">EmBMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger weight backdoor number normalize provider's EaaS provider's model copy trigger set Ee embedding target embedding</sample>
    <sample id="791">EmBMarker Trigger Selection Count the word frequency on a general text corpus Dp Randomly select n words in a moderate-frequency interval trigger set trigger weight target embedding provider's EaaS provider's model copy trigger set normalizer embedding</sample>
    <sample id="792">EmBMarker Watermark injection Define a target embedding et Count the trigger number in a sentence Q(S) = min(S, T,) /m, m: max trigger number Add the target embedding on the original embedding e,</sample>
    <sample id="793">EmBMarker Watermark injection Define a target embedding et Count the trigger number in a sentence Q(S) = min(SL Tl, m) Add the target embedding on the original embedding e, provider's EaaS provider's model copy trigger set normalizer</sample>
    <sample id="794">EmBMarker Watermark injection Define a target embedding et Count the trigger number in a sentence Q(S) = min{r(S)|Tl,rm} Add the target embedding on the original embedding e0 T trigger set c Q trigger weight backdoor provider's EaaS copy trigger set normalizer provider's model original embedding target embedding EmBMarker</sample>
    <sample id="795">EmBMarker

Copyright verification

Construct a backdoor and benign dataset

Dp = {w1, w2, ..., wm | wi ∈ T }

Dn = {w1, w2, ..., wm | wi ∈ T }

Request embeddings from stealer’s service with the datasets

trigger set

verify

target

backdoor and benign dataset

train

corpus

stealer

E_b, E_n</sample>
    <sample id="796">EmBMarker Copyright verification Construct a backdoor and benign dataset Dp = {w1, w2, ..., wm}|w ∈ T, Dn = {w1, w2, ..., wn}|w ∈ T}. Request embeddings from stealer's service with the datasets trigger set T Dp + Dn backdoor and benign dataset E_D E_n verify target embedding extracted model train corpus D_e EmB</sample>
    <sample id="797">EmBMarker Copyright verification Construct a backdoor and benign dataset Dp = {w1, w2, ..., wm| wi ∈ T } Dn = {w1, w2, ..., wm| wi ∈ T } Request embeddings from stealer's service with the datasets trigger set verify target embedding? provider backdoor and benign dataset train extracted model Dp En Dn Eo Stealer</sample>
    <sample id="798">EmbMarker Copyright verification Compute their similarity to the target embedding e, = e, - l, = ||e, - e, ||2 cos, = ||e,||l, ||e,||l, Cn = {cos, ∈ Dn}, Lb = {l, ∈ Dn}, Ln = {l, ∈ Dn} Computing metrics (similarity difference and p-value of KS) ∆cos = ∑ i j ∆Lb = ∑ i j</sample>
    <sample id="799">EmMarker Copyright verification Compute their similarity to the target embedding e, = e, - l, = ||e, - e, ||2 cos, = ||e,||l, ||e,||l, Cn = {cos, ∈ Dn} Lb = {l, ∈ Dn} Computing metrics (similarity difference and p-value of KS) ∆cos = ∑ i ∑ j ∆Lb =</sample>
    <sample id="800">Experimental Results Copy Dataset: AG News, MIND, SST2, Enron Spam Provider’s general Dataset: WikiText Metrics Performance on downstream tasks: ACC Detection performance: Δcos, Δiz, p-value Setting m = 20, n = 4, frequency interval = [0.005, 0.01] Dataset #Sample #Classes Avg. len. SST2 68,221 2 54.17 MIND 130,383 18 66.14 Enron Spam 33,716 2 34.57 AG News 127,600 4 236.4</sample>
    <sample id="801">Experimental Results Performance comparison Dataset Method ACC p-value Detection ∆αx ∆αz SST2 Original 93.76±0.19 &gt; .034 0.07±0.18 -0.14±0.36 RedAlarm 93.76±0.19 &gt; .009 1.35±0.17 -2.70±0.35 EmbMarker 93.76±0.19 &lt; .05 4.07±0.17 -4.07±0.37 MIND Original 77.18±0.09 &gt; .008 0.76±0.05 1.52±0.10 RedAlarm 77.18±0.09 &gt; .008 2.08±0.06 4.17±1.31 EmbMarker 77.18±0.09 &lt; .05 4.64±0.23 -9.28±0.47 AGNews Original 93.74±0.14 &gt; .003 0.72±0.15 -1.46±0.30 RedAlarm 93.74±0.14 &gt; .003 1.28±0.14 -2.85±0.34 EmbMarker 93.74±0.14 &lt; .05 2.85±0.14 -4.28±0.34 Enron Spam Original 94.78±0.06 &gt; .003 -0.21±0.27 0.42±0.34 RedAlarm 94.87±0.06 &gt; .047 0.50±0.29 1.00±0.57 EmbMarker 94.78±0.27 &lt; .005 6.17±0.31 -12.34±0.62</sample>
    <sample id="802">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="803">Experimental Results Embedding visualization (a) AG News (b) Enron Spam (c) MIND (d) SST2</sample>
    <sample id="804">Thanks!</sample>
    <sample id="805">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi UNIVERSITA DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="806">Attention as a Guide for Simultaneous Speech Translation Sara Papi, Matteo Negri, Marco Turchi UNIVERSITA DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="807">What are the problems of the current SimulST models?</sample>
    <sample id="808">What are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized Long and complicated training procedures (e.g., different optimization objectives)</sample>
    <sample id="809">What are the problems of the current SimulST models? Specific architectures are usually trained, introducing additional modules to be optimized Long and complicated training procedures (e.g., different optimization objectives Training and maintaining several models to reach different latency regimes (e.g., 1s, 2s,..)</sample>
    <sample id="810">What is our solution?</sample>
    <sample id="811">What is our solution? Use already existing offline ST models without re-training or adopting specific architecture for SimuST</sample>
    <sample id="812">What is our solution?</sample>
    <sample id="813">Our solution: EDAtt</sample>
    <sample id="814">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="815">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="816">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to a word attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden.</sample>
    <sample id="817">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to a word is emitted if the attention is not concentrated (its sum is below a threshold λ) towards the last λ speech frames, meaning that the received information is enough stable. I am going to talk about... Ich werde reden.</sample>
    <sample id="818">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="819">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="820">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough.</sample>
    <sample id="821">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below the threshold 𝛼) towards the last 𝜆 speech frames, meaning the received information is enough stable. I am going to talk about... Ich werde reden. Emitted I am going to talk about climate. Ich werde über Klima sprechen.</sample>
    <sample id="822">Our solution: EDAtt Encoder-Decoder Attention Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable. Ich werde reden Ich werde über Klima sprechen.</sample>
    <sample id="823">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="824">Main Results: EDAtt 27 25 23 BLEU 21 19 17 15 1.5 2.5 3.5 4.5 5 AL / AL-CA (δ) (a) en-nde</sample>
    <sample id="825">Main Results: EDAtt 27 25 23 21 19 17 1.5 2.5 3.5 4.5 5 latency measure (a) AL-CA vs. AL</sample>
    <sample id="826">Main Results: EDAtt AL/AL-CA (s)</sample>
    <sample id="827">Main Results: EDAtt 27 25 23 AL / AL-CA (s) 0.5 1.5 2.5 3.5 4.5 5 (a) en-de</sample>
    <sample id="828">Main Results: EDAtt popular strategies also applied to offline models wait-k LA CAAT EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) (a) en-de</sample>
    <sample id="829">Main Results: EDAtt state of the art architecture, specifically tailored for SimuST wait-k LA CAAT EDAtt 27 25 23 21 19 17 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) (a) en-de</sample>
    <sample id="830">Main Results: EDAtt wait-k LA CA EDAtt 27 25 23 21 19 17 0.5 1 2.5 3.5 4.5 5 AL / AL-CA (s) (a) w-de</sample>
    <sample id="831">EDAtt outperforms all the strategies applied to offline models</sample>
    <sample id="832">Do you want to discover more? Read our paper to discover more results! (spapi.negri@fbk.eu marco.turchi@gmail.com github.com/hlt-mt/fbk-fairseq) Scan me! page 038</sample>
    <sample id="833">The authors of the paper are affiliated with Google.</sample>
    <sample id="834">The authors of the paper are affiliated with Stony Brook University and Human Language Analysis.</sample>
    <sample id="835">The language pairs analyzed in the paper were English-Spanish, English-French, and English-German.</sample>
    <sample id="836">The name of the speaker is Shangbin Feng.</sample>
    <sample id="837">SARI, BLEU, BS-P, FRE</sample>
    <sample id="838">53 tasks are used for training purposes and 20 tasks are used for testing purposes.</sample>
    <sample id="839">There are three authors involved in the paper.</sample>
    <sample id="840">The authors experimented on the AG News, MIND, SST2, Enron Spam, and WikiText datasets.</sample>
    <sample id="841">Language model acceptability judgments are not always robust to context ACL 2023 Johns Hopkins University Purdue University MIT</sample>
    <sample id="842">Language model acceptability judgments are not always robust to context ACL 2023 Johns Hopkins University Purdue University MIT</sample>
    <sample id="843">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping 2. "Many people themselves. 2. "The customer ... spent any money." 2. Non-stereotypical were helping herself." P(1) &gt; P(2) P(1) &gt; P(2)any P(1) &gt; P(2)</sample>
    <sample id="844">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical 2. helping themselves. 2. "The customer ... spent any money." 2. Non-stereotypical 3. Many people were helping herself. P(1) &gt; P(2) P(1) &gt; P(2)any P(1) &gt; P(2)</sample>
    <sample id="845">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping 1. "The customer ... spent any money" 2. "Many people 2. The customer ... spent any money" 2. Non-stereotypical were helping herself." sentence.</sample>
    <sample id="846">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping, 2. "The customer ... spent any money. sentence. 2. Many people were helping herself". P(1) &gt; P(2) P(1) &gt; P(2)any P(1) &gt; P(2)</sample>
    <sample id="847">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people 2. The customer ... spent any money 2. Men are terrible, were helping" 2. any money" at handwork. themselves" P(1.) &gt; P(2.) P(1.any) &gt; P(2.any) P(1.) &gt; P(2.) Are these judgements stable with long preceding context?</sample>
    <sample id="848">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people 2. "The customer ... spent any money 2. Men are terrible 3. We're helping 3. any money" 3. at handwork. 4. herself." P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2) Are these judgements stable with long preceding context?</sample>
    <sample id="849">Revisiting Minimal Pair Paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people 2. "The customer ... spent any money 2. Handwork, were helping" 2. "Many people ... We're terrible. We're helping" any money" at handwork. herself" P(1) &gt; P(2) P(1.any) &gt; P(2.any) P(1) &gt; P(2) Are these judgments stable with long preceding context?</sample>
    <sample id="850">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject Agreement Who might Rose fees from returning to this customer? What might Rose fees from this customer before returning to this customer? What would Jessica sell beyond reciting these spotlights? What Aaron sounded like the white spotlights? What Aaron sounded like the white spotlights before returning to this customer before k? Acceptable? Matched BLUMP: Adjunct</sample>
    <sample id="851">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement Acceptable Unacceptable (GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="852">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement (Acceptable) (Unacceptable) Who might Rose fees from before returning to this customer? What could Jessica sell before returning these spoolights? What had Aaron sounded like while before returning to this customer before k? BLUMP: Adjunct</sample>
    <sample id="853">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement Space of Candidate Prefixes &gt; Prefix Acceptable Unacceptable Who might Rose fees from before returning to this customer? What could Jessica sell without noticing these spotlights? What Aaron sounded like when he returned to this customer before this customer before k?</sample>
    <sample id="854">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG) Agreement (SAG</sample>
    <sample id="855">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Subject-Verb Agreement Space of Candidate Prefixes P (Subject | Prefix) &gt; P (Verb | Prefix) Acceptable, Unacceptable</sample>
    <sample id="856">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Subject Agreement Space of Candidate Prefixes &gt;? &gt;? P (Subject | Prefix) &gt; P (Subject | Prefix) Who might Rose fees from before returning to this customer? There was a documentary about music interning at Allen. There were no lieutenants working here before returning to? BLUMP: Externa "the" Quantifiers | Adjunct Island</sample>
    <sample id="857">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Space of Candidate Prefixes Subject-Verb Agreement (SVO) Unacceptable Acceptable GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="858">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement P (Subject + Verb) &gt; P (Subject + Noun) Space of Candidate Prefixes Unacceptable Acceptable Who might Rose fee from before returning to this customer? Who might Rose fee from before returning to this customer? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars, and tons of Records of Rosa fee from before returning to this customer?</sample>
    <sample id="859">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability. Test Suite: Subject-Verb Agreement P.(Prefix &gt; Prefix) Space of Candidate Prefixes Unacceptable? Who might Rose fee from before returning to this customer? Who might Rose fee from before returning to this customer? A rose is a woody perennial flowering plant of the genus Rosa. There are over three hundred species and thousands of cultivars. Rosa is a member of the Rosaceae family.</sample>
    <sample id="860">MPP judgements are robust for arbitrary context lengths We perform MPP evaluations with different contexts — accept/acceptable; matched/mismatched structure — of lengths up to 900 tokens. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to 17. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to 17.</sample>
    <sample id="861">MPP judgments are robust for arbitrary context lengths We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to before returning to</sample>
    <sample id="862">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance We perform MPP evaluations with different contexts — acceptable / acceptable; matched/mismatched structure — of lengths up to 900 tokens.</sample>
    <sample id="863">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance We perform MPP evaluations with different contexts — acceptable / acceptable; matched/mismatched structure — of lengths up to 900 tokens 0.2 0.0 0.2 0.2 0.2 0.2 There was a documentary on music festivals before returning to the customer. There was a documentary on music festivals before returning to the customer. Who might Rose Lee fee from before returning to the customer? Who might Rose Lee fee from before returning to the customer? There was each documentary on music festivals returning There was each documentary on music festivals returning who might Rose Lee fee from before returning to the customer. who might Rose Lee fee from before returning to the customer? who might Rose Lee fee from before returning to the customer? 0.4 0.6 0.8 100 200 400 600 800 Acc./MPP, OPT 6.78 Prefix Strategy Acc. (Matched) Unacc. (Matched) Acc. (Unmatched) Unacc. (Unmatched) Wiki</sample>
    <sample id="864">Acceptable/Unacceptable MPP sentences in the context raise/lower judgement performance We perform MPP evaluations with different contexts — acceptable / acceptable; matched/mismatched structure — of lengths up to 900 tokens.</sample>
    <sample id="865">Acceptable/unacceptable MPP sentences with matched structure most severely affect</sample>
    <sample id="866">Acceptable/unacceptable MPP sentences with matched structure most severely affect</sample>
    <sample id="867">Acceptable/unacceptable MPP sentences with matched structure most severely affect
We perform MPP evaluations with different contexts — acceptable/unacceptable;
matched/mismatched structure — of lengths up to 900 tokens

What could Jessica sell spotlights before cleaning? What Aaron sold spotlights like white before returning this customer free?
What could Jessica sell spotlights before cleaning? Who might Rose feel free to clean?
What could Jessica sell spotlights before cleaning? Who might Rose feel free to clean?
What could Jessica sell spotlights before cleaning? Who might Rose feel free to clean?
What could Jessica sell spotlights before cleaning? Who might Rose feel free to clean?</sample>
    <sample id="868">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. • Prefix/suffix adverbs: “However, &lt;sent&gt;” • Long prefix adverbs: “First and foremost, &lt;sent&gt;.” • First and foremost: “&lt;sent&gt;.” • Regardless of what X thinks about it, “&lt;sent&gt;.” • Quote: “Yesterday, X said, ‘&lt;sent&gt;’”</sample>
    <sample id="869">Why do matched prefixes affect LM judgements? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. • Prefix/suffix adverbs: "However, &lt;sent&gt;." • Long prefix adverbs: "First and foremost, &lt;sent&gt;." • Add Clause: "Regardless of what X thinks about it, &lt;sent&gt;." • Quote: "Yesterday, X said, '&lt;sent&gt;'"</sample>
    <sample id="870">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these perturbations. • Prefix/suffix adverbs: “However, &lt;sent&gt;” • Long prefix adverbs: “First and foremost, &lt;sent&gt;.” • Add clause: “Regardless of what X thinks about it, &lt;sent&gt;.” • Quote: “Yesterday, X said, ‘&lt;sent&gt;’” Prefix Type Acceptable Unacceptable All</sample>
    <sample id="871">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. Prefix/suffix adverts: "However, &lt;senta&gt;" Long prefix adverts: "&lt;sent&gt;" Add Clause: "Regarding what X thinks about it, &lt;senta&gt;" Quote: "Yesterday, said, &lt;senta&gt;" Way of: "In the way of" Acceptable Unacceptable Prefix Type</sample>
    <sample id="872">Why do matched prefixes affect LM judgments? We perturb context sentences in ways that preserve the relevant structure, and ask whether models are similarly sensitive to these sentences. Prefix/suffix adverbs: “However, &lt;sent&gt;” Long prefix adverbs: “First and foremost, &lt;sent&gt;.” Add Clause: “Regarding what X thinks about it, &lt;sent&gt;.” Quote: “Yesterday, he said, ‘&lt;sent&gt;’” Way of: “In a way, &lt;sent&gt;” Acceptable Unacceptable Prefix Type Input Length</sample>
    <sample id="873">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences, sentence evaluations with short, single- sentence inputs do not fully capture LMs’ abstract knowledge. Test Set: Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement Subject-Verb Agreement</sample>
    <sample id="874">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences, Sentence evaluations with short, single- sentence inputs do not fully capture LMs' abstract knowledge.</sample>
    <sample id="875">Key Takeaways Language models are sensitive to latent syntactic/semantic features shared across sentences. Sentence evaluations with short, single-sentence inputs do not fully capture LMs’ abstract knowledge.</sample>
    <sample id="876">NACHOS is a type of pre-training strategy for language models.</sample>
    <sample id="877">The speaker's name is George Foster.</sample>
    <sample id="878">The prompting strategy can have a significant impact on the results, with differences of more than 1 BLEURT point being observed in 516 out of 1000 sentence-prompt pairs.</sample>
    <sample id="879">The authors of the paper are affiliated with Carnegie Mellon University, Language Technologies Institute, Technical University of Lisbon, and the University of Edinburgh.</sample>
    <sample id="880">The image does not provide information about the 5 expert-written instructions.</sample>
    <sample id="881">The authors propose testing the models on using information from multiple sources by conducting experiments with human study participants and coreference resolution models.</sample>
    <sample id="939">The common evaluation methods for dialogue systems are user studies and automated metrics.</sample>
    <sample id="940">5</sample>
    <sample id="941">The background knowledge needed is that Servin is a judge and Kea is a baker.</sample>
    <sample id="942">Yes, the code is available on GitHub.</sample>
    <sample id="943">The annotators for NLPositionality are not balanced in regard to each demographic, i.e., country, gender, etc.</sample>
    <sample id="944">Sentences were perturbed in the acceptable domain by prefix/suffix adverbs, long prefix adverbs, adding and foremost, first and foremost, regardless of what X thinks about it, and quoting.</sample>
    <sample id="945">It means to evaluate something based on multiple criteria or aspects, rather than just one.</sample>
    <sample id="946">University of Science and Technology of China, Microsoft Research Asia, Beijing Haotong University, Sony AI</sample>
    <sample id="947">The form of the prompting is important in translation.</sample>
    <sample id="948">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge Vasuda Varadarajan*, Swannie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann &amp; H. Andrew Schwartz Stony Brook University Human Language Analysis Group</sample>
    <sample id="949">What is Cognitive Dissonance? "two elements of cognition (i.e. thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007)</sample>
    <sample id="950">What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007) I know that cigarettes could kill me, I grabbed a couple smokes after the meeting today. belief Dissonance action</sample>
    <sample id="951">What is Cognitive Dissonance? “two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent. Expressed in language as a relationship b/w two phrases/statements by a user seq 1: I know that cigarettes could kill me. belief seq 2: I grabbed a couple smokes after the meeting today. action Dissonance/Resolution seq 3: I don’t think I could keep my job without them. belief Eddie Harmon-Jones and Cindy Jones-2007 Cognitive dissonance theory after 50 years of development. Zeitschrift fur Sozialpsychologie, 38(1/6), 4</sample>
    <sample id="952">What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" Expressed in language as a relationship b/w phrases/statements by a user Relatively rare to find in language, compared to other discourse relations seq 1: I know that cigarettes kill me. belief Dissonance seq 2: I grabbed a couple smokes after the meeting today. action Consonance/Resolution seq 3: I don't think I could keep my job without them. belief</sample>
    <sample id="953">Why dissonance? Effects of disagreement Cognitive dissonance theory: Reexamining a pivotal theory in psychology</sample>
    <sample id="954">Why dissonance? Effects of disagreement Attitudes and Belief trends Anxiety disorders</sample>
    <sample id="955">Why dissonance? Effects of disagreement Eddie Harmon-Jones and Judith Mills 2018: An introduction to cognitive dissonance theory and an overview perspectives on the theory. Anxiety disorders</sample>
    <sample id="956">Why dissonance? Effects of disagreement Cognitive Styles Entry and Exit from Attitudes and Belief trends Anxiety disorders</sample>
    <sample id="957">Annotations  Wish I could hold grudges but I guess it's a good thing that I can't at the same time.</sample>
    <sample id="958">Annotations  Wish I could hold grudges but I guess it's a good thing that I can't at the same time.</sample>
    <sample id="959">Annotations  Wish I could hold grudges but I guess it's a good thing that I can't at the same time.</sample>
    <sample id="960">Training on Initial Annotated Set RoBERTa-base classifier head TRAIN Small annotated dataset: 43/901 dissonance; not better than chance</sample>
    <sample id="961">Training on Initial Annotated Set RoBERTa-base classifier head 43/901 dissonance; not better than chance</sample>
    <sample id="962">Method: Transfer and Active Learning for Annotating Rare Class Rare class annotation - needle in a haystack? START New class annotation? Initial model: Transfer Learning new train old data new model new data fine-tune Model Retrain/Update Acquisition which are "Difficult to annotate" Active Learning Iteration new examples Add new examples</sample>
    <sample id="963">Cold-start Annotations: Transfer Learning Rare class annotation - “need a haystack” Difficult to annotate Increase chance of rare class? Acquisition Active Learning (AL) Acquisition which are ‘Difficult’ to annotate? Humans annotate Add new examples Add examples Cumulative Model (CM) Train new model Train new model old data new data fine-tune</sample>
    <sample id="964">Cold-start Annotations: Transfer Learning Roberta-base classifier head init. train Debate CE 0.10 Debate CE 0.08 Transferred after weights weights training on combined Debate and GE data 0.65 0.60 0.55 0.50 Area under the ROC curve (AUC)</sample>
    <sample id="965">Cold-start Annotations: Transfer Learning Roberta-base classifier head init debate +0.13 debate CE +0.08 Transferred after training on combined Debate and GE data</sample>
    <sample id="966">Cold-start Annotations: Transfer Learning Roberta-base classifier head init. debate 0.10 0.00 0.08 0.06 Debate CE Area under the ROC curve (AUC) Transferred after weights after combined Debate and GE data</sample>
    <sample id="967">Cold-start Annotations: Transfer Learning RoBERTa-base + classifier head TRAIN int. dataset Debate CE Debate-CE Debate+CE ICE Figure 2: Ablation study on the effect of different losses on the performance of RoBERTa-based models on the ICE task.</sample>
    <sample id="968">Active Learning: Cumulative vs. Iterative Update</sample>
    <sample id="969">Active Learning: Cumulative vs Iterative Update

0.75

0.70

0.65

0.60

0.55

0.50

Random
Entropy
CoreSet
CAL
PRC

Cumulative
Iterative</sample>
    <sample id="970">Active Learning: Probability-of-Rare-Class Strategy Rare class annotation — ‘needle in a haystack’? Initial model Training data New data Difficult to annotate? Increase chance of rare class? Easier to annotate? new examples Active Learning Iteration Model Retrain/Update Humans annotate Add new examples</sample>
    <sample id="971">Active Learning: Probability-of-Rare-Class Strategy Rare class annotation – ‘needle in a haystack’ Difficult to annotate? Increase chance of rare class? Easier to annotate? new examples which are ‘rare’ to label? new train Model Retrain/Update Active Learning Iteration Humans annotate Add new examples old data model</sample>
    <sample id="972">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Comparison (AUCs) Baseline: from scratch Transferred model AL-Random AL-Energy AL-CoreSet AL-CAL AL-PRFC (pure) +0.17 +0.20 +0.18 +0.19 +0.21</sample>
    <sample id="973">Active Learning: Probability-of-Rare-Class Strategy

Active Learning Strategy Comparison (AUCs)

Baseline: from scratch

Transformed model

AL-Random

AL-Entropy

AL-CAL

AL-PRC

AL-Best transfer (best transfer model from task 1)</sample>
    <sample id="974">Active Learning: Probability-of-Rare-Class Strategy Active Learning Strategy Characteristics 3.20 ENTROPY 11.96 Rare % Time (s) Subj. diff. 6.80 CORRECTION 12.72 -0.035 4.80 CAL 11.88 -0.045 7.60 PRC 13.55 -0.071 Minimum annotation cost does not necessarily lead to better models Partly could make the annotations more difficult, cognitive dissonance is one such class To increase dissonance samples, PRC works the best.</sample>
    <sample id="975">Takeaways Rare class annotation - "need in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative M0 M1 M2 M3 M0 M1 M2 M3</sample>
    <sample id="976">Takeaways Rare class annotation - "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative M0 M1 M2 M3 M0 M1 M2 M0 M1 M2</sample>
    <sample id="977">Contact: varadaranj@cs.stonybrook.edu, siuhong@cs.stonybrook.edu, has@cs.stonybrook.edu Code: Dataset: Paper:</sample>
    <sample id="978">Turnt, Likert, and Likert</sample>
    <sample id="979">There are 8 authors involved in the paper.</sample>
    <sample id="980">The ideal qualities of a good planner are to be able to inherit abstract goals and handle multi-faceted constraints.</sample>
    <sample id="981">There are 8 authors involved in the paper.</sample>
    <sample id="982">The speaker's name is Vasudha Varadarajan.</sample>
    <sample id="983">The authors of the paper are affiliated with the Institute of Computer Science of the Polish Academy of Sciences and the University of Warsaw.</sample>
    <sample id="984">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang PennState amazon</sample>
    <sample id="985">XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang Penn State amazon</sample>
    <sample id="986">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations.</sample>
    <sample id="987">Cross-lingual Semantic Parsing is a task to translate queries in multiple natural languages into multiple meaning representations</sample>
    <sample id="988">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="989">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain natural language English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="990">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="991">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain meaning representation English German Chinese Neural Models SQL Lambda FunQL</sample>
    <sample id="992">Cross-lingual Semantic Parsing Existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications. For instance: Lack of coverage on certain neural model English German Chinese Single Model SQL Lambda FunQL</sample>
    <sample id="993">XSemPLR We provide a unified dataset XSemPLR for cross-lingual semantic parsing in meaning representations. It contains: 9 datasets in various domains 5 semantic parsing tasks. 22 natural languages in 15 language families what players play less than three times a season? b) What is the number of games that are scheduled to be played on Saturday, August 31st, 2014? c) What is the name of the restaurant with the highest number of reviews? d) How many people are currently in the airport? e) What is the number of flights from San Francisco to New York? f) What is the name of the person who wrote the article about the new iPhone? g) What is the title of the movie that was released in 2001 and has the same director as The Matrix? h) What is the name of the person who is the author of the book "The Catcher in the Rye"? i) What is the name of the person who is the author of the book "The Catcher in the Rye"? j) What is the name of the person who is the author of the book "The Catcher in the Rye"?</sample>
    <sample id="994">XSemPLR We provide a unified dataset XSemPLR for cross-lingual semantic parsing in meaning representations. It contains: 9 datasets in various domains 5 semantic parsing tasks. 22 natural languages in 15 language families what players play less than three times a season what restaurants serve Japanese food what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant? what is the name of the French restaurant?</sample>
    <sample id="995">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL English Model SQL Inference German Translate API English Model SQL</sample>
    <sample id="996">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL Inference German Translate API English English Model SQL</sample>
    <sample id="997">Experiment Settings We consider the six settings for training and evaluation. Translate-Test: Use google translate API to translate source to the target language. Then use monolingual model to train and eval. Training English English Model SQL Inference German Translate API English English Model SQL</sample>
    <sample id="998">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-German! We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training Monolingual Model: German Model SQL Inference German Model SQL</sample>
    <sample id="999">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-German! We also test Monolingual Few-shot by training monolingual models with only 10% training data. Training Monolingual Model (Few-shot) German Model SQL Inference German Model SQL</sample>
    <sample id="1000">Experiment Settings We consider the six settings for training and evaluation. Source language is the same as target language, e.g. German-to-German! We also test Monolingual Few-shot setting by training monolingual models with only 10% training data. Training German Model SQL Monolingual Model (Few-shot) German SQL Inference German Model</sample>
    <sample id="1001">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1002">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1003">Experiment Settings We consider the six settings for training and evaluation. Multilingual Model: Train one multilingual model for all languages. Training German English Chinese Multilingual Model SQL Inference German Multilingual Model SQL</sample>
    <sample id="1004">Experiment Settings We consider the six settings for training and evaluation. Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language. Training English Or English German Few-shot Multilingual Model SQL Inference German Multilingual SQL</sample>
    <sample id="1005">Experiment Settings

We consider the six settings for training and evaluation.

Cross-lingual Zero-shot/Few-shot transfer. Train on one source language and transfer to another language.

Training

English Or English German
Multilingual Model SQL
Inference German Multilingual Model SQL</sample>
    <sample id="1006">Analysis of Monolingual Enc-Dec (m75) obtains the best performance on all datasets!</sample>
    <sample id="1007">Analysis of Monolingual Enc-Dec: mT5 obtains the best performance on all datasets!</sample>
    <sample id="1008">Analysis of Monolingual Enc-DEC: Multilingual Pretrained Encoder-Decoder Models mBART, mT5</sample>
    <sample id="1009">Analysis of Monolingual Enc-DEC: Multilingual Pretrained Encoder-Decoder Models mBERT, mT5</sample>
    <sample id="1010">Analysis of Multilingual Training Enc-Dec-Enc-PTR (mT5+XLM-R)+ PTR on Multilingual Setting can be improved by training in a mixture of various languages. MATS MGeQQuery MSpider MMaps MOvernight MCWQ MSchameQA MTOP MCOnDaL Average Monolingual XLM-R-PTR 11.31 71.42 47.40 85.87 66.20 33.53 65.77 81.83 10.69 58.16 Monolingual mT5 39.72 71.35 40.20 85.91 61.03 30.79 61.82 81.68 - 59.96 Monolingual mT5+XLM-R 54.45 76.57 32.30 91.31 67.55 28.51 60.92 82.95 - 61.82</sample>
    <sample id="1011">Analysis of Multilingual Training

We evaluate mT5 and XLm-R + PTR on a multilingual setting, which can be improved by training in a mixture of various languages.

| Model | MAT5 | MGeQuery | MSSpider | MSpider | MMaps | MOvertight | MCWQ | MSchemaQA | MTOP | MCNola | Average |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Monolingual XLm-R | 11.31 | 71.41 | 47.40 | 85.73 | 59.73 | 66.20 | 33.53 | 65.77 | 81.83 | 7.69 | 58.16 |
| Multilingual XLm-R+PTR | 33.72 | 71.35 | 40.20 | 85.91 | 61.03 | 60.79 | 30.79 | 61.82 | 81.68 | – | 59.60 |
| mT5 | 54.45 | 76.57 | 32.30 | 91.31 | 67.55 | 28.51 | 28.91 | 82.95 | – | – | 61.82 |

The table shows the performance of different models on various multilingual tasks. The multilingual XLm-R+PTR model outperforms the monolingual XLm-R model in most tasks, indicating that multilingual training improves performance.</sample>
    <sample id="1012">Analysis of Multilingual Training

We evaluate mT5 and XLM-R + PTR on Multilingual Setting. Most of the major NLs can obtain performance gain, except that English performance drops in 7 datasets. This is known as "Curse of Multilinguality".

English | French | German | Chinese | Thai | Finnish | Italian | Spanish | Vietnamese
Increase | Increase | Increase | Increase | Increase | Increase | Increase | Increase | Increase</sample>
    <sample id="1013">Analysis of Multilingual Training

We evaluate mT5 and XLM-R + PTR on Multilingual Setting. Most of the major NLPs can obtain performance gain, except that English performance drops in 7 datasets. This is known as "Curse of Multilinguality".

English | French | German | Spanish | Chinese | Thai | Finnish | Elvish | Indonesian | Vietnamese
Increase | Decrease | Increase | Increase | Increase | Increase | Increase | Increase | Increase | Increase</sample>
    <sample id="1014">Cross-lingual Performance Gap

Blue Line: Cross-lingual Few-shot transfer
Orange Line: Cross-lingual Zero-shot transfer
Green Line: Monolingual Setting

Gequery
MTP
SchemaQA
Overnight
NLMaps
MCWQ
Gequery/lamb
Gequery/prolog
Gequery/funql
Gequery/sql
Spider
ATIS
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="1015">Cross-lingual Performance Gap Blue Line: Cross-lingual Few-shot transfer Orange Line: Cross-lingual Zero-shot transfer Green Line: Monolingual Setting Gequery MTOP SchemaQA Overnight NLMaps MCWQ Gequery/lambda Gequery/prolog Gequery/funql Gequery/sql Spider ATIS Few-shot Zero-shot Monolingual</sample>
    <sample id="1016">Cross-lingual Performance Gap

Blue Line: Cross-lingual Few-shot transfer
Orange Line: Cross-lingual Zero-shot transfer
Green Line: Monolingual Setting

Gequery
MTP
SchemaQA
Overnight
NLMaps
MCWQ
Gequery/lamb
Gequery/prolog
Gequery/funql
Gequery/sql
Spider
ATIS
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="1017">Other Results &amp; Findings (Section 4 in Paper)

- Enc-Dec (m75) outperforms previous work or achieves comparable results.
- Pretraining on the English NL can significantly boost the performance of few-shot
- Multilingual LLMs (Codex &amp; BLOOM) are still inadequate for crosslingual parsing
- Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.
- FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1018">Other Results &amp; Findings (Section 4 in Paper)

- Enc-Dec (m75) outperforms previous work or achieves comparable results.
- Pretraining on the English NL can significantly boost the performance of few-shot
- Multilingual LLMs (Codex &amp; BLOOM) are still inadequate for crosslingual
- Chinese transfer learning and English monolingual training (En -&gt; En) has the largest performance gap, while German usually has the smallest.
- FunQL outperforms the other three meaning representations, and SQL obtains the worst performance.</sample>
    <sample id="1019">Conclusion We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between cross-lingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1020">Conclusion We build XSemPLR, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and meaning representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models. Our results show that mT5 with monolingual training yields the best performance, while notably multilingual LLMs are still inadequate to perform cross-lingual semantic parsing tasks. Moreover, the performance gap between cross-lingual training and cross-lingual transfer learning is still significant.</sample>
    <sample id="1021">The most common errors of PaLM are accuracy/omission and style/awkwardness.</sample>
    <sample id="1048">Emory University, Emory NLP, Alexa Research Lab</sample>
    <sample id="1049">Continuous fine-tuning</sample>
    <sample id="1050">To determine the number of authors involved in the paper, we need to carefully examine the text provided in the image. The image contains a list of names at the bottom, which are the authors of the paper. Let's count each name individually:

1. Koustuv Sinha
2. Jon Gauthier
3. Aaron Mueller
4. Kanishka Misra
5. Keren Fuentes
6. Roger Levy
7. Adina Williams

By counting these names, we find that there are 7 authors involved in the paper. Therefore, the answer is 7.</sample>
    <sample id="1084">The name of the speaker is Yuxin Zhang.</sample>
    <sample id="1085">#ACL2023 From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models Shangbin Feng Chan Young Park Yuhuan Liu Yulia Tsvetkov Paul G. Allen School UW NLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1086">LM Training Data A mixed blessing 1000000000 100000000 10000000 1000000 100000 10000 1000 100 100 Dodge, Jesse, et al. “Documenting Large Webtext Corpora: A Case Study on the Proceedings of the Conference on Empirical Methods in Natural Language Processing 2020.”</sample>
    <sample id="1087">LM Training Data A mixed blessing 10° # tokens (scale) 10° www. wikipedia.org www. businessinsider.com www. theverge.com www. techcrunch.com www. theguardian.com www. forbes.com www. springster.com www. bloomberg.com www. npr.org www. nytimes.com www. washingtonpost.com www. cnet.com www. huffpost.com www. slate.com www. mashable.com www. wired.com www. theatlantic.com www. slate.com www. theatlantic.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www. theverge.com www</sample>
    <sample id="1088">LM Training Data A mixed blessing 1000000000 100000000 10000000 1000000 100000 10000 1000 100 100 Dodge, Jesse, et al. “Documenting Large Webtext Corpora: A Case Study on the Crawl Corpus.” Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. 2018</sample>
    <sample id="1089">LM Training Data A mixed blessing 1000000000 100000000 10000000 1000000 100000 10000 1000 100 100 Dodge, Jesse, et al. “Documenting Large Weighted Corpora: A Case Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018), October 23-27, 2018, Brussels, Belgium.”</sample>
    <sample id="1090">To this end, Pretraining data Language models Downstream tasks</sample>
    <sample id="1091">To this end Pretraining data Language models Downstream tasks How do you evaluate the political learning of LMs What role does pretraining data play in such political biases? How do LMs with different political learnings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1092">To this end Pretraining data Language models Downstream tasks How do you evaluate the political leaning of LMs? What role does pretraining data play in such political biases? How do LMs with different political leanings perform? Does LM political leaning result in fairness issues in NLP applications?</sample>
    <sample id="1093">Evaluating LM Political Leaning Support both encoder and decoder LMs "statements I &lt;mask&gt; with this statement." "Do you agree or disagree with this statement? &lt;statement&gt;" Automatic eval Grounded in polisci lit Language Model Prompted Response Political Learning Left Libertarian Right Authoritarian</sample>
    <sample id="1094">Existing LMs BERT-base
BERT-large
RoBERTa-base
RoBERTa-large
distilRoBERTa
ALBERT-base
ALBERT-large
BERT-base
RoBERTa-large
GPT-4
GPT-J
Authoritarian economic axis Left Libertarian Right Codex
LLaMa
GPT-3-ada
GPT-3-curie
GPT-3-davinci
ChatGPT</sample>
    <sample id="1095">Existing LMs |  BERT-base RoBERTa-base
BERT-large RoBERTa-large
distilRoBERTa ALBERT-base
ALBERT-large BART-base
T5-small T5-large

Authoritarian economic axis Libertarian</sample>
    <sample id="1096">Pretraining Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media left center right Article Comparison For Ideology Prediction and Stance Shen, Qinlan, &amp; Carolyn Rose. "What sounds 'right' to me? experiential factors in the perception of political stances in Chinese and English." Journal of Computer-Mediated Communication, 2022. Social Media Reddit left center right</sample>
    <sample id="1097">Pretraining Data Further pretraining LM (RoBERTa, GPT-2) checkpoints, evaluate change in political leaning News Media Social Media (Reddit) left left center center right right</sample>
    <sample id="1098">Results Partisan shifts in LM political leaning RoBERTa GPT-2 original news news Center Left Right reddit reddit reddit</sample>
    <sample id="1099">Results Partisan shifts in LM political leaning RoBERTa GPT-2 original news reddit Center Left Right</sample>
    <sample id="1100">Results Partisan shifts in LM political leaning RoBERTa GPT-2 original news news Center Left Right Reddit Reddit Reddit Reddit original news</sample>
    <sample id="1101">The Trump Card
Pre-45th to post-45th shift</sample>
    <sample id="1102">The Trump Card
Pre-45th to post-45th shift</sample>
    <sample id="1103">The Trump Card Pre-45th to post-45th shift news left news center news right reddit left reddit center reddit right</sample>
    <sample id="1104">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th BLACK MUSLIM LGBTQ+ JEWS ASAIN LATINX WOMEN CHRISTIAN MEN WHITE NEWS LEFT REDDIT LEFT REDDIT RIGHT Minstrelsy HP (L) NYT (L) CNN (L) NPR (L) Guard (L) Fox (R) WaEx (R) BBart (R) WAT (R) NR (R)</sample>
    <sample id="1105">Per-Category Performance 1st 2nd 3rd</sample>
    <sample id="1106">Per-Category Performance 1st 2nd 3rd</sample>
    <sample id="1107">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 11th 12th 13th 14th 15th 16th 17th 18th 19th 20th 21st 22nd 23rd 24th 25th 26th 27th 28th 29th 30th 31st 32nd 33rd 34th 35th 36th 37th 38th 39th 40th 41st 42nd 43rd 44th 45th 46th 47th 48th 49th 50th 51st 52nd 53rd 54th 55th 56th 57th 58th 59th 60th 61st 62nd 63rd 64th 65th 66th 67th 68th 69th 70th 71st 72nd 73rd 74th 75th 76th 77th 78th 79th 80th 81st 82nd 83rd 84th 85th 86th 87th 88th 89th 90th 91st 92nd 93rd 94th 95th 96th 97th 98th 99th 100th</sample>
    <sample id="1108">Per-Category Performance 1st 2nd 3rd 4th 5th 6th 7th 8th 9th 10th 11th 12th 13th 14th 15th 16th 17th 18th 19th 20th 21st 22nd 23rd 24th 25th 26th 27th 28th 29th 30th 31st 32nd 33rd 34th 35th 36th 37th 38th 39th 40th 41st 42nd 43rd 44th 45th 46th 47th 48th 49th 50th 51st 52nd 53rd 54th 55th 56th 57th 58th 59th 60th 61st 62nd 63rd 64th 65th 66th 67th 68th 69th 70th 71st 72nd 73rd 74th 75th 76th 77th 78th 79th 80th 81st 82nd 83rd 84th 85th 86th 87th 88th 89th 90th 91st 92nd 93rd 94th 95th 96th 97th 98th 99th 100th</sample>
    <sample id="1109">Per-Category Performance 1st 2nd 3rd</sample>
    <sample id="1110">Per-Category Performance 1st 2nd 3rd Category News Left Reddit Left Reddit Right Hate Speech BLACK MUSLIM LGBTQ+ JEWS ASAIN LATINX WOMEN CHRISTIAN MEN WHITE Minisformation HP (L) NYT (L) CNN (L) NPR (L) Guard (L) Fox (L) WaEx (L) BRBart (R) WAT (R) NR (R) Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. Yellow/yellow denotes best and dark blue denotes worst.</sample>
    <sample id="1111">Qualitative Analysis Text Target Label Base N-L S-L N-R S-R the all right is comupted with people supporting a new nationaion Asian True True True False False (.)- that McDonald's stop donald trump from seizing they are true that the country was in the throes of a crime pagins trail that the country was in the throes of a crime because they know I will stop the slaughter going on donald trump has announced incredible that water water is in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in flot in</sample>
    <sample id="1112">Qualitative Analysis Text Target Label Base N-L S-L N-R S-R the all right is com pleted with people supporting a new nationa lism at McDonald's and priest are common. They were told that the crime trail was in the throws of a crime trump because they know I will stop the slaughter going on donald j trump has ordered August 20th incredible that water have been in flood in flint are paying in burlington vermont for clean water False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False</sample>
    <sample id="1113">Table 12: Qualitative analysis of hate speech examples from LM with different political bias.</sample>
    <sample id="1114">Table 12: Qualitative analysis of hate speech examples from LM. Hate? R-1 R-2 R-3 R-4 R-5 R-6 R-7</sample>
    <sample id="1115">Table 12: Qualitative analysis of hate speech examples when LM is tuned with different political biases.</sample>
    <sample id="1116">Table 12: Qualitative analysis of hate speech examples from LM with different political biases.</sample>
    <sample id="1117">Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1118">Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1119">Discussion Between Scylla and Charybdis To "sanitize" or not to "sanitize", that is the question Pretraining data Language models Downstream tasks</sample>
    <sample id="1120">Thank you! Pretraining data Language models Downstream tasks Shangguan Feng Chan Young Park Yuhuan Liu Yulia Tsvetkov Paul G. Allen School of UW NLP Carnegie Mellon University Language Technologies Institute</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">The author described the "marked words" method as a way to find words that distinguish persons of marked groups from unmarked groups.</sample>
    <sample id="1123">The affiliations of the authors are Paul G. Allen School, UW NLP, Carnegie Mellon University Language Technologies Institute, and the Russian Academy of Sciences.</sample>
    <sample id="1124">Bouquet/Stanford</sample>
    <sample id="1125">The speaker's name is Sarah E. Finch.</sample>
    <sample id="1126">4</sample>
    <sample id="1127">The datasets that can be used to test syntactic phenomena are BLMP, SyntaxGym, and CrowS.</sample>
    <sample id="1128">When Does Translation Require Context? A Data-driven, Multilingual Exploration Patrick Fernandes', Kayo Yin*, Emmy Liu Andre F. T. Martins, Graham Neubig Carnegie Mellon University Language Technologies Institute IF TECNICO LISBOA BAILR BERNARD M. BABEL Unbabel *equal contribution</sample>
    <sample id="1129">Translation depends on context We'll have to get rid of that mole.</sample>
    <sample id="1130">Translation depends on context Things could start to get dangerous if the ministers find out. We'll have to get rid of that mole.</sample>
    <sample id="1131">Translation depends on context Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1132">Evaluating context-dependent translation is hard Only a small portion of words depend on context</sample>
    <sample id="1133">Evaluating context-dependent translation is hard

Only a small portion of words depend on context

o Corpus-level metrics

Existing methods support limited discourse phenomena and languages</sample>
    <sample id="1134">RQ1: When does translation require context? RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1135">Word-level context usage</sample>
    <sample id="1136">Conditional Cross-Mutual Information (CXMI) CXMI: measure how much context MT models use given a corpus</sample>
    <sample id="1137">Conditional Cross-Mutual Information (CXMI) CXMI: measure how much context MT models use given a corpus Uncertainty over translations given the source HqMTa(Y |X) HqMTa(Y |X,C) Uncertainty over translations given the source AND context CXMI(C → Y |X)</sample>
    <sample id="1138">Pointwise (P-)CXMI We introduce P-CXMI to measure context usage to translate a specific</sample>
    <sample id="1139">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1140">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations?</sample>
    <sample id="1141">Thematic analysis of high P-CXMI words</sample>
    <sample id="1142">Thematic analysis of high P-CXMI words 1. POS tags</sample>
    <sample id="1143">Thematic analysis of high P-CXMI words 1. POS tags Pronouns P-CXMI for POS tags in En-Ar PRON 3.Sing PRON 3.Dual PRON 3.Plus</sample>
    <sample id="1144">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form</sample>
    <sample id="1145">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form - Lexical cohesion Avellile's mother was still asleep. Avellile went to school. 阿维利尔的母亲还在睡觉。 阿维利尔上学了。</sample>
    <sample id="1146">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form - Lexical cohesion - Formality Avellie's mother was still asleep. Avellie went to school. 阿维利尔的母亲还在睡觉。 阿维利尔上学去了。</sample>
    <sample id="1147">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items 3. Individual tokens - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis She knows where we're going. I don't. Ich weiß, wohin wir gehen. Ich weiß es nicht.</sample>
    <sample id="1148">RQ1: When does translation require context? - Word-level context usage - Thematic analysis RQ2: How well do models handle context-dependent translations? - Multilingual Discourse-Aware (MuDA) benchmark</sample>
    <sample id="1149">Multilingual Discourse-Aware (MuDA) tagger - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis</sample>
    <sample id="1150">Multilingual Discourse-Aware (MuDA) tagger Pronouns Verb form Lexical cohesion Formality Ellipsis 6000 fr 4000 2000 0 2000 1000 500 0 500 200 100 50 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20 10 5 0 20</sample>
    <sample id="1151">MUDA benchmark</sample>
    <sample id="1152">RQ1: When does translation require context? Word-level context usage Thematic analysis RQ2: How well do models handle context-dependent translations? Multilingual Discourse-Aware (MuDA) benchmark Model evaluation</sample>
    <sample id="1153">Corpus-level metrics BLEU</sample>
    <sample id="1154">Corpus-level metrics BLEU CONTEXT COMET F-measure</sample>
    <sample id="1155">Corpus-level metrics BLEU CONTEXT COMET F-measure Unclear which system is best for document-level MT with corpus-level metrics</sample>
    <sample id="1156">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion</sample>
    <sample id="1157">MuDA benchmark results Context-aware models perform significantly better on some phenomena Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1158">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion Ellipsis, pronouns, verb form

DeepL outperforms Google on most phenomena and language pairs*

DeepL &gt; Google</sample>
    <sample id="1159">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU CER/F-measure</sample>
    <sample id="1160">Summary Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT MuDA tagger BLEU, chrF, F-measure</sample>
    <sample id="1161">FT, W, BOND, COSINE, MLC, L2R.</sample>
    <sample id="1162">11</sample>
    <sample id="1163">DEPLAIN: A German Parallel Corpus with Intra lingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1164">DEPLAIN: A German Parallel Corpus with Intra lingual Translations into Plain Language for Sentence and Document Simplification Regina Stodden, Omar Momen, Laura Kallmeyer Heinrich Heine University Düsseldorf, Germany ACL 2023</sample>
    <sample id="1165">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion Plain Language: .Die Gewerkschaft setzt sich dafur ein, dase zum Beispiel hohere Lohnen oder mehr Urlaub werden.</sample>
    <sample id="1166">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion .Die Gewerkschaft setzt sich dafür ein, _dass_ zum Beispiel höhere Lohnen gezahlt werden." Plain Language: .Die Gewerkschaft setzt sich zum Beispiel für höhere Lohn oder mehr Urlaub ein. Insertion</sample>
    <sample id="1167">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion .Die Gewerkschaft setzt sich für ein, _dase_ zum Beispiel höhere Löhne gezahlt werden." Plain Language: .Die Gewerkschaft setzt sich zum Beispiel für höhere Löhne oder mehr Urlaub ein. Insertion</sample>
    <sample id="1168">Text Simplification Example Original: Substitution Clause Deletion Reordering Word Deletion .Die Gewerkschaft setzt sich dafur ein, dass zum Beispiel hoheer Lohne oder mehr Urlaub werden. Plain Language: .Die Gewerkschaft setzt sich zum Beispiel fur hoheere Lohne oder mehr Urlaub ein. Insertion</sample>
    <sample id="1169">2. DE-plain
A New Corpus</sample>
    <sample id="1170">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) Alignment manual (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et al. (Och et et</sample>
    <sample id="1171">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2003 2005 2007 2009 2011 2013 Alignment Markedness Readability (Graesser et al. 2013) (Papafragou et al. 2013) (Kaufmann et al. 2013) (Holt et al. 2013) (Schulz et al. 2013) (Bilmes et al. 2013) 5942 5942 5942 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 3450 345</sample>
    <sample id="1172">German Text Simplification Corpora Sentence Level 14000 12000 mixed health 9456 8000 6000 4000 2000 0 200 250 250 250 250 250 2013) 2013) 2013) 2013) 2013) 2013) Alignment manual (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (Bader et al. (</sample>
    <sample id="1173">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 201</sample>
    <sample id="1174">German Text Simplification Corpora Sentence Level 14000 12000 mixed health sub标题 10000 8000 Alignment manual 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 201</sample>
    <sample id="1175">Types of Simplification Simplicity LexSimp StructSimp news bible fiction 100 80 60 40 20 0 10 20 30 40 50 60 70 80 90 100 Rephrasing lexical substitution word addition word deletion Simplification Transformations DEplain-apa DEplain-web</sample>
    <sample id="1176">Types of Simplification Simplicity LexSimp StructSimp news bible fiction n= 187 Simplication Transformations DEplain-apa DEplain-web reordering rephrasing lexical substitution word addition word deletion</sample>
    <sample id="1177">Types of Simplification Simplicity LexSimp StructSimp news bible L2 fiction n= 187 n= 187 n= 12 Simplification Transformations DEplain-apa DEplain-web reordering paraphrasing lexical substitution word addition word deletion</sample>
    <sample id="1178">Types of Simplification Simplicity LexSimp StructSimp news bible L2 fiction n=187 n=185 n=172 Simplication Transformations DEplain-apa DEplain-web Reordering Rephrasing lexical substitution word addition word deletion</sample>
    <sample id="1179">Types of Simplification Simplicity LexSimp StructSimp news bible L2 fiction n= 187 n= 187 n= 12 Simplification Transformations DEplain-apa DEplain-web reordering paraphrasing lexical substitution word addition word deletion</sample>
    <sample id="1180">3. Use-cases
Automatic alignment and simplification</sample>
    <sample id="1181">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 P R Sent-LaBSE Similar embeddings of Language model BERT transformer 841 444 780 960 444 607 775 CATS-C3G Different similarity measures e.g. grams (grams, word vectors) 247 553 342 276 347 285 285 WMT’17 BERTalign Allows sentence-transformer methods produce n:m alignments 743 465 610 733 387 412 412 M2M align A vicinity-driven approach with a TF-IDF similarity matrix 846 477 628 731 389 412</sample>
    <sample id="1182">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 r nm Sent-LaBSE Similar embeddings of Language BERT transformer 960 444 607 779 247 553 342 276 846 477 610 733 819 628 730 CATS-C3G Different similarity measures e.g. grams, word vectors 961 444 780 465 387 561 465 333 285 465 387 561 465 333 285 BERTalign Allows sentence-transformer methods produce n:m alignments 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111</sample>
    <sample id="1183">Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language model BERT transformer 841 444 780 960 444 607 775 CATS-C3G Different similarity measures e.g. grams, word vectors 247 553 342 276 BERTalign Allows sentence-transformer methods produce n:m alignments 743 465 610 733 387 459 612 VASalign A vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1184">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language BERT transformer 960 444 607 779 Sent-RoBERTa Similar embeddings of Cross English &amp;amp; German RoBERTa CATS-C3G Different similarity measures e.g. grams, word vectors 247 553 342 278 WMT’17 Allows sentence-transformer methods produce n:m alignments 743 465 387 561 846 477 610 733 819 628 730 BERTalign A vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1185">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description P R F,0 F,1 Sent-LaBSE Similar embeddings of Language BERT transformer 841 444 780 960 444 779 247 553 342 276 Sent-RoBERTa Similar embeddings of Cross English &amp; German RoBERTa 841 444 780 960 444 779 CATS-C3G Different similarity measures e.g. grams, word vectors 743 465 610 846 477 628 733 819 628 730 BERTalign Allows sentence-transformer methods produce n:m alignments 743 465 610 846 477 628 733 819 628 730 Vowpal Allows a vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1186">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language model BERT transformer 960 444 607 779 Sent-RoBERTa Similar embeddings of Cross English &amp;amp; German RoBERTa 247 553 342 278 CATS-C3G Different similarity measures e.g. grams, word vectors 847 477 610 733 BERTalign Allows sentence-transformer methods produce n:m alignments 846 477 628 730 VASalign A vicinity-driven approach with a TF-IDF similarity matrix</sample>
    <sample id="1187">Automatic Alignment Evaluation Results of the alignment methods with 1:1 (upper part) and n:m capabilities (lower part) Name Description 111 Sent-LaBSE Similar embeddings of Language model BERT transformer 960 444 607 779 Sent-RoBERTa Similar embeddings of Cross English &amp; German RoBERTa 247 553 342 278 CATS-C3G Different similarity measures e.g. grams, word vectors 847 465 387 458 Vowpal Wabbit A vicinity-driven approach with a TF-IDF similarity matrix 846 477 610 733 BERTalign Allows sentence-transformer methods produce n:m alignments 743 465 387 458 M2M align</sample>
    <sample id="1188">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1189">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1190">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1191">Results on Document Simplification using fine-tuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1192">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1193">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1194">Results on Document Simplification using finetuned long-mBART n corresponds to the length of the training data.</sample>
    <sample id="1195">Thanks. For more details. Please check out our paper. And feel free to visit our poster in the ACL 2023 conference.</sample>
    <sample id="1196">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Paretti, and Annie Louis. Google Research</sample>
    <sample id="1197">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Paretti, and Annie Louis. Google Research</sample>
    <sample id="1198">Indirect Referring Expressions Google Research Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name The first one. The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic Resolving Indirect Reference Expressions by Entity Selection (Mitya Corpus)</sample>
    <sample id="1199">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name Cannot remember the name Cannot remember the name Want to specify a preference The newer one. The song that's not energetic. Resolving Indirect References by Entity Selection (MlRIT Corpus)</sample>
    <sample id="1200">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name The newer one. The song that's not energetic</sample>
    <sample id="1201">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name Cannot remember the name The newer one. The song that's not energetic.</sample>
    <sample id="1202">Indirect Referring Expressions Goal: Understanding users' language when they make a choice Alternative question Did you mean easy on me or I gotta feeling? Direct reference Indirect reference Cannot remember the name Cannot remember the name and fluid conversation: The pronunciations are hard to distinguish Want to specify a preference Indirect reference The newer one. The song that's not energetic. Resolving Indirect Reference Expressions by Entity Selection (Mityaev Corpus)</sample>
    <sample id="1203">Dataset Collection

Important problem

Conversational systems

Benchmarking Large Language Models' entity understanding

No large-scale public dataset available

We collect a large dataset using crowd annotation

Three domains:

Resolving Indirect Referring Expressions by Entity Selection (Mnli+ Corpus)</sample>
    <sample id="1204">Dataset Collection

Important problem

Conversational systems

Benchmarking Large Language Models' entity understanding

No large-scale public dataset available

We collect a large dataset using crowd annotation

Three domains:

Resolving Indirect Expressions by Entity Selection (MELITON Corpus)</sample>
    <sample id="1205">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task</sample>
    <sample id="1206">Dataset Collection Methodology

Methodology emphasizes informativeness using a cartoon completion task</sample>
    <sample id="1207">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task</sample>
    <sample id="1208">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task</sample>
    <sample id="1209">Dataset Collection Methodology

Methodology emphasizes informality using a cartoon completion task

Remember that song you were thinking of yesterday?

Do you mean "I Gotta Feeling"?

Filled by the annotator

The alternative question

Expression referring to one of the entities

Sets the dialog context (chosen from a few manual prompts per domain)

Resolving Indirect Expressions (Entity Selection) Corpus</sample>
    <sample id="1210">Dataset Collection Methodology

Methodology emphasizes informativeness using a cartoon completion task

Remember that song we were talking about yesterday? Do you mean the one I Gotta Feeling?

Filled by the annotator</sample>
    <sample id="1211">Generate alternative questions = sampling entity pairs Google Research Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre/and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1212">Generate alternative questions = sampling entity pairs Google Research Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1213">Generate alternative questions = sampling entity pairs Google Research Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Items with similar descriptions on Wikipedia Do you mean "Thinking of You" or "Happy Anywhere"? Items with similar titles: Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean "You Could Be Mine" or "The Way I Am"?</sample>
    <sample id="1214">Generate alternative questions = sampling entity pairs Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1215">Generate alternative questions = sampling entity pairs Google Research Do you mean A or B? Items with similar infoboxes on Wikipedia (same genre/and/or artist) Items with similar descriptions on Wikipedia Do you mean Thinking of You or Happy Anywhere? Items with similar titles Do you mean The Return (memoir) or The Return (Shatner novel)? Uniform at random: Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="1216">Google Research Background knowledge (Music) Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song</sample>
    <sample id="1217">Google Research Background knowledge (Music) Easy on Me by Ajele I Gotta Feeling by The Black Eyed Peas Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen to at least some of each song Read about each song</sample>
    <sample id="1218">Google Research Background knowledge (Music) Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song. We ask annotators to Listen at least some of each song Read about each song</sample>
    <sample id="1219">Background knowledge (Recipes) Simnel Cake Pandan Cake Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste in eleven balls made of the same paste as the cake. Pandan cake is a light, fluffy sponge cake flavoured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia, and also in the Netherlands, especially among the Indo community.</sample>
    <sample id="1220">Eliciting expressions We then tell the annotators which choice should be selected and ask them to describe it. Pick one Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) We would like you to give us 3 to 5 expressions to fill in your speech bubble, not examples. The one with the right music The song that's not energetic It has a nice melody It's about not having time to choose</sample>
    <sample id="1221">Eliciting expressions We then tell the annotators which choice should be selected and ask them to describe it. Pick one Easy on Me (by Adele) I Gotta Feeling (by The Black Eyed Peas) We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, not example: One with the music The song that's not energetic It has a river It's about not having time to choose</sample>
    <sample id="1222">AltEntities Corpus - 6,000 alternative questions across the three domains - 42,000 indirect referring expressions Results with T5 XL model (accuracy): o 92-95% if the LM has access to the same background knowledge as annotators. o 82%-87% when the LM has access to partially overlapping background knowledge o 60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1223">AltEntities Corpus - 6,000 alternative questions across the three domains - 42,000 indirect referring expressions Results with T5 XL model (accuracy): o 92-95% when the LM has access to the same background knowledge as annotators. o 82%-87% when the LM has access to partially overlapping background knowledge. o 60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1224">AltEntities Corpus - 6,000 alternative questions across the three domains - 42,000 indirect referring expressions Results with T5 XL model (accuracy): o 92-95% if the LM has access to the same background knowledge as annotators. o 82%-87% when the LM has access to partially overlapping background knowledge. o ~60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1225">AltEntities Corpus - 6,000 alternative questions across the three domains - 42,000 indirect referring expressions Results with T5 XL model (accuracy): o 92-95% if the LM has access to the same background knowledge as annotators. o 82%-87% when the LM has access to partially overlapping background knowledge. o 60% when the LM (T5 XL) has only access to the entity names. We showed models are domain-generalizable. Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="1226">CamemBERT is initially trained on 4GB of data.</sample>
    <sample id="1227">The name of the speaker is Adam Przepiorkowski.</sample>
    <sample id="1228">The findings that led to the conclusion that temporal drift is the main cause of performance loss are as follows: 1. The temporal drift hypothesis was tested by analyzing the performance of a trading strategy on historical data from 2005 to 2015 and comparing it to the performance on more recent data from 2016 to 2020. The results showed that the performance of the strategy deteriorated over time, with a significant drop in performance observed between 2015 and 2020. 2. The temporal drift hypothesis was further supported by analyzing the performance of different trading strategies on historical data. The results showed that the performance of all strategies deteriorated over time, with a significant drop in performance observed between 2015 and 2020. 3. The temporal drift hypothesis was also supported by analyzing the performance of different asset classes. The results showed that the performance of all asset classes deteriorated over time, with a significant drop in performance observed between 2015 and 2020. Overall, the findings suggest that temporal drift is the main cause of performance loss in the financial markets.</sample>
    <sample id="1229">NLPositionality: Characterizing Design Biases of Datasets and Models Sebastin Sany University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1230">NLPositionality: Characterizing Design Biases of Datasets and Models Sebastian Sanyt University of Washington Jenny T. Liang Carnegie Mellon University Ronan Le Bras Allen Institute for AI Katharina Reinecke University of Washington Maarten Sap Carnegie Mellon University</sample>
    <sample id="1231">Imagine...</sample>
    <sample id="1232">Imagine...</sample>
    <sample id="1233">Imagine... Can you stop being a jerk? : (0.82) Aditya Sharma Tech Lead, Times of India Carl Jones Tech Lead, New York Times = PerspectiveAPI score</sample>
    <sample id="1234">Imagine... Design bias example! Can you stop being a jerk? (0.82) Pressitutes everywhere on the news (0.33) Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India PerspectiveAPI score</sample>
    <sample id="1235">Positionality [1] Savin-Baden, Maggi, and Claire Howell-Major. “Qualitative research: The essential guide to theory and practice.” Qualitative Research: The Essential Guide to Theory and Practice (2013,</sample>
    <sample id="1236">Positionality “The perspectives [people] hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1237">Positionality “The perspectives [people] hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="1238">Do datasets and models have
positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Li, et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science 2022.</sample>
    <sample id="1239">Do datasets and models have
positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science.</sample>
    <sample id="1240">Do datasets and models have
positionality? [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Li, et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science. 2022.</sample>
    <sample id="1241">Do datasets and models have
positionality?
Anecdotal evidence:
- Model and dataset probing [112]
- Theoretical definitions of model positionality [3]
[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.
[2] Chen, et al. "GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.
[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." Data Science</sample>
    <sample id="1242">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambò &amp;amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="1243">Do datasets and models have positionality? Anecdotal evidence: - Model and dataset probing [112] - Theoretical definitions of model positionality [3] [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022. [2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambro &amp; Gerglot. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science 2022.</sample>
    <sample id="1244">Question: Do datasets and models have positionality?</sample>
    <sample id="1245">NLPositionality
A framework for characterizing design biases in NLP datasets and models</sample>
    <sample id="1246">Framework Collection "Eating with Hands" "Can you live without your hands?" "Steel" "Yes" "No" "Yes" "No" Each instance has an annotation. Annotators are sent as instances to be annotated. Annotations from the annotators are then used to train the model. Model Predictions Process Training Annotators Annotators receive predictions from the model. The received annotations are then used to train the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotators receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from the model. Annotors receive gold labels from</sample>
    <sample id="1247">Framework 1) annotate datasets with diverse annotators,</sample>
    <sample id="1248">Framework 1) annotate datasets with diverse annotators,</sample>
    <sample id="1249">Framework Collection Processing 1) Annotate datasets with diverse annotators. 2) Train models on the annotated datasets. 3) Use the models to make predictions on a held-out test set. 4) Evaluate the performance of the models on the test set. 5) Repeat steps 2-4 for different datasets and models.</sample>
    <sample id="1250">Framework Collection Processing 1) Annotators annotate a subset of the dataset. 2) Compare annotations to models and datasets to models via Pearson’s R scores.</sample>
    <sample id="1251">Framework Collection Processing 1) Each instance has an annotation. 2) Compare annotations to models and datasets via Pearson’s R scores.</sample>
    <sample id="1252">Lab in the Wild 5,376,396 participants Could you live with an AI and its morals? Are you better at noticing hateful and show compared to non-hateful and show? Where are you on the techno-utopian scale? Tell us how you thin mixed reality is more technic and others. Participate now! Participate now! Participate now!</sample>
    <sample id="1253">Lab in the Wild 5,376,396 Pool of diverse volunteers / research participants Online experiment from researchers</sample>
    <sample id="1254">Task A: Social Acceptability 1) Read the situation 2) Enter what you think about it from your perspective. 3) See what others thought about it! The AI speculates:</sample>
    <sample id="1255">Task A: Social Acceptability

1. Read the situation
2. Enter what you think about it.
3. See what others thought about it!

The AI speculates.

Study participants in the United States:

Participants compare their responses to others' and an AI's.</sample>
    <sample id="1256">Task A: Social Acceptability Analysis Datasets - Social Chemistry Models - Delphi - GPT-4</sample>
    <sample id="1257">Task B: Toxicity 1) Read the example. 2) Enter what you think about it. 3) See what others thought about it.</sample>
    <sample id="1258">Task B: Toxicity
Analysis
Datasets
- Dynahate
Models
- Perspective API
- Rewire API
- State RoBERTa
- GPT-4</sample>
    <sample id="1259">Results Who do NLP datasets and models align with?</sample>
    <sample id="1260">Social Acceptability (GPT-4)
Datasets and models are most aligned to English-speaking countries.

| Country Group       | Value |
|---------------------|-------|
| African Islamic    | 0.47  |
| Baltic              | 0.68* |
| Catholic Europe     | 0.71* |
| Confucian           | 0.74* |
| English-speaking    | 0.74* |
| Latin America       | 0.67* |
| Orthodox Europe     | 0.47  |
| Protestant Europe   | 0.59* |
| West/South Asia     | 0.59* |

* indicates a significant difference</sample>
    <sample id="1261">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1262">Datasets and models are most aligned to people with a college education.</sample>
    <sample id="1263">Finding 2:
Some populations are left behind.</sample>
    <sample id="1264">Datasets and models are less aligned to non-binary people.</sample>
    <sample id="1265">So, what can we do? Addressing positionality in NLP</sample>
    <sample id="1266">Recommendations [1] https://www.masakhane.io</sample>
    <sample id="1267">Recommendations 1. Keep a record of all relevant design choices made throughout building datasets or models. 2. Do NLP research through the lens of perspectivism: a. Share disaggregated dataset labels! b. Use modeling techniques that can handle annotator disagreement. 3. Building specialized datasets and models with and for specific communities is desirable for inclusive NLP (e.g., Masakhane initiative). [1] https://www.masakhane.io</sample>
    <sample id="1268">Thanks! Dashboard Link: nlpportunity.cs.washington.edu/ Paper: bit.ly/NLPositionality-Paper/</sample>
    <sample id="1269">To avoid overfitting.</sample>
    <sample id="1270">To increase transparency about bias mitigation methods, the authors recommended that model owners should provide clear and detailed information about the techniques and strategies they use to mitigate biases in their models. This increased transparency can help stakeholders understand the effectiveness of these methods, identify potential limitations or areas for improvement, and make informed decisions about the use of the models.</sample>
    <sample id="1271">Many people were helping themselves. Many people were helping herself.</sample>
    <sample id="1272">The authors used the following evaluation metrics: BLEU, METEOR, CIDEr, and ROUGE-L.</sample>
    <sample id="1273">Krippendorff's Alpha</sample>
    <sample id="1274">Wikipedia was chosen to add completely unrelated sentences to the unacceptable and acceptable queries.</sample>
    <sample id="1275">The authors of the paper are affiliated with Heinrich Heine University Düsseldorf, Germany.</sample>
    <sample id="1276">MultiInstruct differs from other benchmarks by incorporating a diverse range of tasks and languages, making it more comprehensive and versatile.</sample>
    <sample id="1277">3</sample>
    <sample id="1278">Binary coordination is the proportion of shorter conjuncts that depend on the absolute difference in conjunct lengths.</sample>
    <sample id="1279">The prompts used in this study were on average 3.5 words long.</sample>
    <sample id="1280">The smaller T5 model, which was trained on WikiHow, outperformed larger LLMs in generating scripts. This suggests that specialized models can be more effective than large language models for specific tasks.</sample>
    <sample id="1309">The learning strategies investigated in the work are: From scratch with full model construction, Continual pre-training using an existing pre-trained model, and Pre-training on a French generic model, and Pre-training on MedBERT.</sample>
    <sample id="1310">The image shows a graph titled "What Causes Performance Drop?" with two main points: "Adaptive overfitting?" and "Temporal drift?". The graph plots the CERL2000 F1 Score against another metric for various methods, showing their performance trends. The graph includes lines for different methods such as "CERL2000", "BERT", "RoBERTa", and others, with some methods marked by stars indicating significant points.

To determine the factor of overfitting due to test reuse specifically, we need to look at the data points and trends related to this aspect. Overfitting typically occurs when a model performs well on the training data but poorly on new, unseen data. This can be identified by a sharp increase in performance during training followed by a drop when tested on a separate dataset.

In the graph:
- The line labeled "CERL2000" shows a sharp increase in performance during training (left side of the graph) and then a significant drop when tested (right side of the graph).
- Other lines, such as those for "BERT" and "RoBERTa", show more gradual changes in performance.

Given this information, the factor of overfitting due to test reuse specifically is indicated by the sharp drop in performance for the "CERL2000" method. This suggests that the CERL2000 method may have overfitted to the training data, leading to poor generalization on the test data.

Therefore, the factor of overfitting due to test reuse specifically is significant for the CERL2000 method, as evidenced by its sharp decline in performance on the test data.</sample>
    <sample id="1311">The quality of the simplification was evaluated using three metrics: BLEU, METEOR, and CIDEr.</sample>
    <sample id="1312">Yes, language models have different political biases.</sample>
    <sample id="1347">Cognitive dissonance is a psychological phenomenon that occurs when an individual experiences discomfort or tension due to holding two or more contradictory beliefs, values, or attitudes simultaneously. This discomfort arises from the inconsistency between the person's thoughts, actions, and beliefs. The concept of cognitive dissonance was first introduced by psychologist Leon Festinger in 1957. According to Festinger, when individuals encounter information that contradicts their existing beliefs, they may experience dissonance. To reduce this dissonance, people often change their beliefs, attitudes, or behaviors to align with the new information. Cognitive dissonance can manifest in various aspects of life, such as personal relationships, decision-making processes, and social interactions. It plays a significant role in shaping human behavior and decision-making, as individuals strive to maintain consistency and coherence in their thought processes.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Yes, cumulative training performs better than iterative when doing active learning.</sample>
    <sample id="1350">The speaker's name is Sara Papi.</sample>
    <sample id="1351">The data for the MuDa benchmark was taken from the WMT’14 En-Es translation task.</sample>
    <sample id="1352">Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przpielkowski and Michal Wozniak INSTITUTE OF COMPUTER SCIENCE PAN ul. Jana Kazimierzawa 5, 01-248 Warsaw UNIVERSITY OF WARSAW ACL 2023</sample>
    <sample id="1353">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1354">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1355">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1356">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1357">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1358">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1359">Dependency Structure of Coordination

Bouquet/Stanford (Universal Dependencies):

Homer loves Lisa, Bart, and Maggie.

Chain/Moscow:

Homer loves Lisa, Bart, and Maggie.

Conjunction-headed/Prague:

Homer loves Lisa, Bart, and Maggie.

Multi-headed/London:

Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1360">Dependency Structure of Coordination Bouquet/Stanford (Universal Dependencies): Homer loves Lisa, Bart, and Maggie. Chain/Moscow: Homer loves Lisa, Bart, and Maggie. Conjunction-headed/Prague: Homer loves Lisa, Bart, and Maggie. Multi-headed/London: Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="1361">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1362">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1363">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1364">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1365">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it read yesterday read this absolutely fascinating book about bees. good read yesterday this absolutely fascinating book about bees.</sample>
    <sample id="1366">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1367">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad</sample>
    <sample id="1368">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it yesterday. read yesterday it. read this absolutely fascinating book about bees yesterday. read yesterday this absolutely fascinating book about bees. good</sample>
    <sample id="1369">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it yesterday.</sample>
    <sample id="1370">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read it read yesterday it read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1371">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: good bad read this absolutely fascinating book about bees. yesterday.</sample>
    <sample id="1372">Dependency Length Minimization (DLM) Word order tends to minimize dependency lengths: Marge read it yesterday. good Marge read yesterday it. bad Marge read this absolutely fascinating book about bees yesterday. good</sample>
    <sample id="1373">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Berber 2016) • Left conjuncts tend to be shorter than right conjuncts before). this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88-90), but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1374">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • left coniucts tend to be shorter than before, this tendency grows with length difference (briefly noted in Gibson et al. 1996: 88-90) • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1375">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • Left conjs tend to be shorter than before, this tendency grows with length difference (briefly noted in Gibson et al. 1996: 88–90) • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed), not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1376">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): o Left conjuncts tend to be shorter (observed and before); this tendency grows with length difference o (briefly noted in Gibson et al. 1996: 88-90) but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed); not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1377">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • left conjuncts tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88–90). but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed) not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1378">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993, Ficler and Goldberg 2016): • left conjuacts tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88-90). but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed) not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1379">Conjunct Lengths in English Statistics about coordination extracted from an enhanced version of the Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016): • left conjuacts tend to be shorter (observed before), • this tendency grows with length difference (briefly noticed in Gibson et al. 1996: 88-90). • but only when the governor is on the left or absent (I saw Bart and Lisa; Homer came and sneezed) not when it is on the right (Ted and Ned laughed)</sample>
    <sample id="1380">Figure 4: Proportions of shorter continuations depending on the absolute difference in compendium lengths (with confidence bands)</sample>
    <sample id="1381">Figure 1: Proportions of shorter continuations depending on the absolute difference in compendium lengths (with confidence bands)</sample>
    <sample id="1382">Figure 1: Proportions of shorter conjuncts depending on the absolute difference of conjunct lengths (with confidence bands)</sample>
    <sample id="1383">Figure 1: Proportions of shorter continuations depending on the absolute difference of compendium lengths (with confidence bands)</sample>
    <sample id="1384">See the paper for the full argument! Talk to us at the poster session!</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Cross-lingual transfer is when a model trained on one language is used to make predictions in another language.</sample>
    <sample id="1387">Saarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="1388">AL/AL-CA</sample>
    <sample id="1416">Tree-based methods are not able to capture long-term dependencies.</sample>
    <sample id="1417">The authors of the paper are affiliated with the School of Interactive Computing at the Georgia Institute of Technology.</sample>
    <sample id="1495">Annotating Behaviors in Chat</sample>
    <sample id="1496">2018</sample>
    <sample id="1527">The affiliations of the authors are University of Amsterdam, Saarland University, and NLP Center.</sample>
    <sample id="1528">The speaker's name is Siyu Yuan.</sample>
    <sample id="1529">To determine the number of authors involved in the paper, we need to carefully examine the provided image. The image contains a title and a list of names associated with the authors. 

1. Identify the authors' names listed on the image:
   - Patrick Fernandes
   - Kayo Yin
   - Emmy Liu
   - Andre F. T. Martins
   - Graham Neubig

2. Count the total number of authors:
   - There are 5 distinct names listed.

Therefore, the number of authors involved in the paper is 5.</sample>
    <sample id="1530">The approach is compared to the wait-k architecture.</sample>
    <sample id="1531">MULTINSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning

Zhongyang Xu*, Ying Shen*, Lifu Huang

Department of Computer Science, Virginia Tech

*Equal Contribution</sample>
    <sample id="1532">Pre-trained Language Models for Downstream Tasks

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained LM (BERT, T5)

Pre-trained task A

Pre-trained task A

Pre-trained</sample>
    <sample id="1533">Pre-trained Language Models for Downstream Tasks (A) Pre-train-finetune (BERT, T5) (C) Instruction tuning (FLAN) Pre-trained LM on task A Pre-trained LM on task A Pre-trained LM on task A Inference on task A Inference on task A Inference on task A Typical use case: many task-specific examples for each task Model learns to perform many tasks via natural language instruction Model learns to perform many tasks via natural language instruction Inference on unseen task Inference on unseen task Inference on unseen task Prompting (GPT-3) Pre-trained LM Improve performance or prompt engineering on task A Inference on task A Figure 2: Comparing instruction tuning with pre-train-finetune and prompting. Image credit: Wei, Jason, et al. "Finetuned language models are zero-shot learners."</sample>
    <sample id="1534">Language-only</sample>
    <sample id="1535">Instruction Tuning on Multimodal Pre-trained Models</sample>
    <sample id="1536">Imbalance in Instructional Datasets between NLP and Multimodal</sample>
    <sample id="1537">Imbalance in Instructional Datasets between NLP and Multimodal 1600+ Language-only instruction tasks Wang, Yichang, et al. "Benchmarking generalization via in-context instructions on 1,800+ language tasks." arXiv preprint arXiv:</sample>
    <sample id="1538">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse 10 broad tasks 5 expert-written instructions Visual Relationship Temporal Ordering Grounded Matching Region Understanding Image Test Matching Figure 2: Task groups included in MULTINSTRUCT. The yellow represent tasks used for training.</sample>
    <sample id="1539">MULTINSTRUCT The first multimodal instruction tuning benchmark dataset 62 diverse 10 broad tasks 5 expert-written instructions Figure 2: Task groups included in MULTINSTRUCT. The yellow represent tasks used for evaluation, while the white represent datasets used for training.</sample>
    <sample id="1540">OF A (One For All)  A unified multi-modal pre-trained model that is capable of performing both understanding and generation tasks with single or multiple modalities. OFA has a unified vocabulary coordinates of a bounding box.</sample>
    <sample id="1541">Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Input: A caption with a bounding box for each mentioned object. Input: A caption and an image. Options: "The text 'da' is in the front." "The train is on the tracks." "The question is irrelevant to the image." Output: The bounding boxes that correspond to the mentioned objects. Output: The region of the object that contains the text "da" in the front. Output: The bounding box of the train. Output: "The question is irrelevant to the image."</sample>
    <sample id="1542">MULTINSTRUCT Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Input: &lt;bin-198- bin 32&gt; &lt;bin-400- bin 193&gt; &lt;bin-448- bin 181&gt; &lt;bin-456- bin 574&gt; &lt;bin-606- bin 654&gt; &lt;bin-646- bin 654&gt; Input: The region that contains the text "de" is described with "A blue train in the front." The region that contains the text "da" is described with "A blue train in the front." Input: &lt;bin-242&gt; &lt;bin-760&gt; &lt;bin-203&gt; &lt;bin-399&gt; &lt;bin-442&gt; Input: The content of the image is "Is it the question?". The content of the image is "The question is irrelevant to the image." Output: &lt;bin-229- bin 604&gt; &lt;bin-346- bin 475&gt; Output: &lt;bin-242- bin 180&gt; &lt;bin-760- bin 475&gt; Output: &lt;bin-242- bin 180&gt; &lt;bin-760- bin 475&gt; Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1543">MULTINSTRUCT Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Input: &lt;bin 198- bin 32&gt; Output: &lt;bin 229- bin 604&gt; Input: &lt;bin 448- bin 761&gt; Output: &lt;bin 242- bin 190&gt; Input: &lt;bin 456- bin 574&gt; Output: &lt;bin 2019- bin 203&gt; Input: &lt;bin 666- bin 654&gt; Output: &lt;bin 339- bin 442&gt; Figure 1: Example Instances from MULTINSTRUCT for Four Tasks.</sample>
    <sample id="1544">Multi-modal Instruction Tuning</sample>
    <sample id="1545">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from VQA and Miscellaneous groups. Use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1546">Multi-Modal Instruction Turning Training Dataset Construction: Use 53 tasks from 9 groups for training. Sample 10,000 instances per task. Testing Dataset Construction: Reserve the entire Commonsense Reasoning group for testing. Select additional 5 tasks from Q&amp;A and Miscellaneous groups. Use all the instances in the test split for each task. Randomly sample 20 tasks from the test split of Natural Instructions dataset as unseen tasks for NLP.</sample>
    <sample id="1547">Implementation Details

Training details:

Pre-trained OFA-Large model (472M)

Mix all the instances for all tasks.

Each instance is randomly combined with one of its instruction templates.

Testing details:

For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.

We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1548">Implementation Details Training details: Pre-trained OFA-Large model (472M) Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1549">Implementation Details Training details: Pre-trained OFA-Large model (472M). Mix all the instances for all tasks. Each instance is randomly combined with one of its instruction templates. Testing details: For each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the mean and maximum performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="1550">Evaluation Metrics For multi-modal classification tasks (Visual Entailment, Visual Spatial Reasoning, Natural Language Visual Reasoning, and Disaster Type Classification) we report the Accuracy. For multi-modal generation tasks (Commonsense VQA, Text VQA, Grounded VQA, Visual Text Extraction, and Visual Dialogue) we report the Rouge-L. For NLP tasks, we report Rouge-L. We also compute the aggregated performance for each model based on the mean of the model's performance on all multi-modal and NLP tasks. We use Rouge-L as the performance score for most tasks, and Accurac for the rest.</sample>
    <sample id="1551">Sensitivity How sensitive the model is towards variety of instructions for the same task: E[σcT] [E(x,y)T L(fσ(i, x, y))] [E(x,y)T L(fσ(i, x, y))]</sample>
    <sample id="1552">Effectiveness of Instruction Tuning on MULTINSTRUCT Model ROUGE Max ACC Visual Enfement Visual Spatial Reasoning ACC Transfer Learning from NATURAL INSTRUCTIONS 56.00 32.48 33.14 36.44 56.00 59.98 52.14 50.91 50.77 32.28 32.28 31.28 31.45 57.88 56.43 56.43 56.43 Table 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold. Model Test VQA Grounded VQA Visual Text Extraction Visual Dialogue Disaster Type Classification ROUGE Max ACC 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00</sample>
    <sample id="1553">Effectiveness of Instruction Tuning on MULTINSTRUCT Model Common Sense VQA ACC Visual Entailment Visual Spatial Reasoning ACC OPE Max 29.01 32.78 34.56 32.07 32.10 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28 32.28</sample>
    <sample id="1554">Impact of Increasing Multimodal Instruction Task Clusters

- Img Und
  - VQA + Image Understanding
  - Grounding
  - MISC, ITM
    - Grounded Matching + Grounded Generation
    - Temporal Ordering + Miscellaneous + Image Text Matching
  - Relation
    - Visual Relationship
  - Region
    - Region Understanding
  - NLP
    - NLP tasks

Figure: Model Performance as the Timemodal Instruction Task Clusters

The image depicts a slide from a presentation titled "Impact of Increasing Multimodal Instruction Task Clusters." The slide is divided into two main sections. On the left side, there is a list of various multimodal instruction task clusters, each with specific subtasks. The right side of the slide features a line graph illustrating model performance across different task clusters.

### Detailed Description:

#### Left Side: Multimodal Instruction Task Clusters
1. **Img Und (Image Understanding)**
   - **VQA + Image Understanding**: This involves tasks related to Visual Question Answering (VQA) combined with image understanding.
   - **Grounding**: This refers to the task of grounding concepts in images, ensuring that the model can accurately identify and locate objects within visual data.

2. **MISC, ITM (Miscellaneous and Image Text Matching)**
   - **Grounded Matching + Grounded Generation**: This cluster includes tasks where the model matches grounded concepts and generates text based on those concepts.
   - **Temporal Ordering + Miscellaneous + Image Text Matching**: This involves ordering events or actions temporally and matching them with relevant textual descriptions.

3. **Relation**
   - **Visual Relationship**: This task focuses on understanding and analyzing relationships between different visual elements in an image.

4. **Region**
   - **Region Understanding**: This involves tasks that require the model to understand and analyze specific regions within an image.

5. **NLP (Natural Language Processing)**
   - **NLP tasks**: This category encompasses various natural language processing tasks, although it is not further detailed in the slide.

#### Right Side: Line Graph
The line graph on the right side shows the performance of a model across different task clusters over time. The x-axis represents time, while the y-axis represents performance metrics. The graph includes multiple lines, each representing different task clusters:
- **Long (10)**
- **Grounding (10)**
- **MISC, ITM (14)**
- **Relation (10)**
- **Region (10)**
- **NLP (10)**

Each line indicates how the model's performance in these task clusters has evolved over time, providing a visual representation of the impact of increasing task clusters on model performance.

This slide effectively communicates the complexity and variety of multimodal instruction task clusters and their impact on model performance over time.</sample>
    <sample id="1555">Effect of Diverse Instructions on Instruction Tuning

· OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity.

·

·

Table 3: Effect of Different Number of Instructions. Performance of OPAsatlin, finetuned on different numbers of instructions.</sample>
    <sample id="1556">Effect of Fine-tuning Strategies on Model Sensitivity

- Instruction tuning on MultiInstruct can significantly reduce the sensitivity of OFA.
- Transfer learning from Natural Instructions dataset can further reduce the sensitivity of the model.

[Figure 4: Model Sensitivity on Unseen Evaluation Tasks. Lower is better.]

The image includes a bar chart comparing different models' sensitivity levels, with lower values indicating better performance. The models compared are:

- OFA
- OFA+MultimInstruction
- OFA+AgeInstruction
- OFA+MixedInstruction

The sensitivity values for each model are as follows:
- OFA: 40.58
- OFA+MultimInstruction: 13.84
- OFA+AgeInstruction: 10.45
- OFA+MixedInstruction: 10.27

The chart shows that the models fine-tuned with additional instructions (MultimInstruction, AgeInstruction, and MixedInstruction) have significantly lower sensitivity values compared to the base OFA model, indicating improved robustness against unseen evaluation tasks.</sample>
    <sample id="1557">Zero-Shot Performance on NLP Tasks Instruction Tuning on MultiInstruct can improve zero-shot performance on unseen NLP tasks. The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset. Model OPAA OPANaturalInstruct OPANoGPT4 RougeL 2.25 3.61 30.79 Table 4: Zero-Shot Performance on NLP tasks. The performance reported on Rouge-L and the best performance is bold.</sample>
    <sample id="1558">Conclusion First large-scale multi-modal instruction tuning dataset. Contains 62 multi-modal tasks from 10 broad categories. Significantly improve the zero-shot capability of OFA via instruction tuning. Explore several transferring learning techniques and show their benefits. Design a new metric sensitivity.</sample>
    <sample id="1559">One More Thing! We are collecting a much larger multimodal instruction tuning dataset with around 150 additional vision-language tasks and we will release them soon!</sample>
  </task>
</testset>