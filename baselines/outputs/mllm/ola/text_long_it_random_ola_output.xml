<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su grandi quantità di dati provenienti da internet, tra cui notizie politiche.</sample>
    <sample id="1">I coautori Akshatha e Martin hanno collaborato in un progetto con McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">The paper presents a novel pre-trained model, LayoutMask, for Visually-rich Document Understanding (VrDU) tasks. It addresses the reading order issues in existing document pre-training models by using local 1D position instead of global 1D position. LayoutMask also employs two novel masking strategies: Whole Word Masking and Layout-Aware Masking, and introduces a new pre-training objective, Masked Position Modeling. The experiments show that LayoutMask outperforms existing models on different layout information, especially when dealing with entities like "Total" that have multiple misleading numbers.</sample>
    <sample id="3">Ciao! Benvenuti nella nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco a livello di documento e a livello di frase. Il mio nome è Regina Stodden, e io guiderò attraverso la prima parte della presentazione. Innanzitutto definiamo la semplificazione del testo. La semplificazione del testo è un processo di adattamento del testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, ad esempio persone con problemi di lettura o parlanti non nativi. Per addestrare un modello di semplificazione del testo, è necessario avere coppie parallele di testi, ad esempio di documenti o frasi. Ecco un esempio qui, puoi vedere una coppia di frasi parallele allineata di un testo complesso in tedesco e la sua traduzione in linguaggio semplice. Per semplificare la frase, diverse tecniche sono possibili come puoi vedere nello esempio, come la sostituzione lessicale, la cancellazione di clausole, la riordinatura o l'inserimento di parole. Ora proponiamo il nuovo corpus che chiamiamo DEPLAIN, perché in recenti anni ci sono stati alcuni problemi con i corpi esistenti. Ad esempio, questi corpi sono troppo piccoli per addestrare un modello di semplificazione del testo. I tre modelli proposti in recenti anni sono tutti automaticamente allineati, cosa che significa che possono essere errornei negli al线. Quindi, proponiamo il nuovo corpus DEPLAIN, che è suddiviso in due sottocorpi: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato 483 documenti tutti manualmente. Ci sono circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diverse domande e anche allineiamo tutti questi 750 documenti, sia manualmente che automaticamente. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato le nostre coppie di frasi un po' di più, ad esempio sul tipo di semplificazione. Come puoi vedere qui, i testi della Bibbia sono molto più semplificati rispetto ai testi di notizie o i testi per apprendisti lingua. Su tutti i livelli, riguardo per esempio la semplificazione lessicale, la semplificazione strutturale, anche livello globale di semplificazione. Inoltre, puoi vedere che il nostro corpus DEPLAIN ha una grande varietà di diversi metodi di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molti più rimescolamenti e aggiungere parole rispetto a quanto avviene nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molti più riformulazioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parlerò delle possibili applicazioni per il dataset DEPLAIN. Per la prima applicazione, possiamo valutare i metodi di allineamento automatici. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni automatiche, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre alleanze di frasi in entrambi i documenti. Ma nel nostro caso, stiamo cercando di estrarre alleanze tra frasi di due documenti paralleli aventi lo stesso livello di difficoltà, hanno lo stesso contenuto, ma sono a livello di difficoltà diverso. E ora che abbiamo il dataset DEPLAIN, che ha alleanze frasi parallele手动, possiamo usare queste frasi come alleanze d'oro standard per valutare i metodi di allineamento proposti. Abbiamo fatto alcune adattazioni a metodi proposti, e pubblicato tutti questi adattamenti e i codici per eseguire i nostri esperimenti nel paper. Al termine, concludiamo che il miglior metodo di allineamento automatico da usare per la semplificazione del testo in tedesco è il metodo di MASSalign. Puoi anche trovare il codice per eseguire questo metodo su documenti propri nel paper. La seconda applicazione che ho dimostrato nel paper è il caso di semplificazione automatica del testo tramite l'addestramento di modelli per produrre testi semplificati da testi complessi di input. Abbiamo addestrato due modelli. Abbiamo addestrato il modello di long-mBART per produrre semplificazioni a livello di documento, e anche addestrato il modello base mBART per produrre semplificazioni a livello di frase. Puoi anche trovare tutti i checkpoint e vedere i dettagli maggiori dei punteggini e dei metri di valutazione dei nostri esperimenti nel paper. Conclusamente, abbiamo concluso che questa addestratura finetuning può produrre o può ottenere punteggini migliori dei punteggini di baseline, e proponevamo quei risultati come una base benchmark per il problema di semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e spero di incontrarvi tutti durante il convegno. Grazie mille.</sample>
    <sample id="4">Il nome della relatrice o del relatore è Kayo Yin.</sample>
    <sample id="5">Hanno utilizzato il modello T5 XL per ottenere l'accuratezza dell'82%-87%.</sample>
    <sample id="6">Jiaan presents a work titled "Towards Unifying Multi-Lingual and Cross-Lingual Summarization" in collaboration with Fandong, Duo, Yunlong, Zhixu, Jianfeng, and Jie. They introduce many-to-many summarization, a more general setting that aims to build one summarization model capable of processing documents in any source language and generating summaries in any target language. The team conducted preliminary studies comparing multilingual, cross-lingual, and many-to-many summarization, finding that the latter helps transfer task knowledge across languages better than previous methods. They propose PISCES, a pre-trained many-to-many summarization model that learns language modeling, cross-lingual ability, and summarization ability through a three-stage pre-training process. Experiments on the WikiLingua dataset demonstrated that PISCES outperforms various baselines, including mBART-50 and mT5.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora. L'articolo presentato da Shuheng ha dimostrato che i modelli sviluppati su CoNLL-2003 possono generalizzare bene alle nuove informazioni e migliorare le prestazioni con un modello migliore, un modello più grande e più esempi di allenamento.</sample>
    <sample id="8">Il nuovo metodo di valutazione umana proposto, chiamato ABC-Eval, è più preciso e affidabile rispetto ai metodi esistenti. Questo è stato dimostrato dalla maggior interannità degli annotatori per le etichette rilasciate da ABC-Eval rispetto alle etichette raccolte da metodi esistenti. Inoltre, ABC-Eval etichette sono più predittive della qualità della conversazione rispetto ai metrici prodotti da metodi esistenti.</sample>
    <sample id="9">In larga misura, il successo dell'attuale approccio scarsamente supervisionato si basa sull'uso di un insieme di validazione pulito per la selezione del modello. Questo è necessario per garantire che i modelli possano generalizzare al di fuori dei relativi etichettori deboli e non memorizzino le informazioni di etichettatura rumorose.</sample>
    <sample id="10">Per migliorare il punteggio, i modelli possono essere adattati per avere accesso a maggiori o più accurata informazione sul contesto e sulle entità. Questo potrebbe includere l'uso di fonti di conoscenza più esaustive o l'implementazione di tecniche di apprendimento监督 più avanzate. Inoltre, i modelli possono essere adattati per essere più generalizzabili tra i domini, consentendo loro di gestire meglio le entità diverse e le sfide linguistiche.</sample>
    <sample id="11">Jack Hessel, un ricercatore di AI2, ha presentato recentemente "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest". Questo è un lavoro in collaborazione con molti partner, tra cui l'Università di Utah, Cornell University, University of Washington, Air Mail e OpenAI. I modelli di lingua possono ora generare e spiegare anche scherzi. Ad esempio, ChatGPT può generare scherzi semplici come "Perché gli scienziati non fidano degli atomi? Perché li costituisce tutto." Alcuni modelli hanno anche spiegato scherzi precedenti. Un esempio è il modello PaLM di Google che spiega un scherzo sulla TPUs. Questa capacità è stata presentata al Google IO. Tuttavia, i modelli di lingua davvero comprendono il senso del humor? Se provi ad interrogare ChatGPT su una battuta di scherzo coinvolgente un pineapplo, potrebbe apparire che stia incollando pineapple in una battuta di scherzo e dichiari che è una pun, ma non ha senso. Questo fa sì che si dubiti se ChatGPT comprende davvero cosa sta succedendo. Per indagare in modo più strutturato, si è rivolto al contesto di The New Yorker Caption Contest. Questo contesto richiedeva ai lettori di inviare le migliori captions per una serie di disegni a colori. I creatori hanno selezionato i finalisti e hanno votato per decidere il vincitore finale. Questo è un contesto molto popolare e riceve migliaia di entries da tutto il mondo, inclusi celebrità famose. Cos'abbiamo fatto con i dati del contesto di The New Yorker Caption Contest? Abbiamo operazionalizzato i dati in tre task differenti: matching, quality ranking e spiegazione generazione. Per il task di matching, i modelli sono stati presentati con cinque opzioni di captions, solo una delle quali era davvero scritta per un disegno specifico. Per il task di quality ranking, i modelli hanno ricevuto due captions scritte per un disegno, ma una delle due era giudicata di qualità molto alta da valutatori umani, tra cui gli editor di The New Yorker e votanti del pubblico. Infine, per il task di spiegazione generazione, i modelli sono stati chiamati a generare spiegazioni di due a quattro frasi per spiegare perché un scherzo è divertente. Per supportare esperimenti computazionali sul corpus, i ricercatori hanno raccolto una nuova serie di annotazioni. In particolare, per ciascun dei più di 700 disegni rappresentativi di più di una decina di anni di contesti di The New Yorker Caption Contest, hanno raccolto locazioni, descrizioni, punti alti e link di entità per ogni disegno. Hanno anche raccolto un corpus di spiegazioni dei scherzi, con una quantità di circa 650 spiegazioni di due a quattro frasi di scherzi. Come si comportano i modelli di lingua nei nostri vari task? Innanzitutto, analizziamo il task di matching e quality ranking. Nel task di matching, il miglior modello, che è CLIP finetuned sul corpus annotato, raggiunge circa un 62% di accuratezza su questo task. Questo è relativo a un baseline di 20% di indovinare a caso. Questo significa che per 5 occasioni su 10, il modello sceglie la caption corretta. Tuttavia, gli umani raggiungono circa un 94% di accuratezza su questo task, rappresentando un gran gap nella comprensione del humor. Ora, potresti domandarti, come si comportano i modelli che non devono fare computer vision? Abbiamo voluto testare modelli come GPT-4 su questo corpus, ma GPT-4 non può prendere direttamente immagini pixel. Quindi, separatamente, consideriamo un setting in cui i modelli come GPT-4 sono condizionati a fare esattamente lo stesso task, ma vengono fornite con una descrizione umana dell'immagine. Anche in questo caso, c'è ancora un gran gap di performance tra le prestazioni a 5-shot di GPT-4 e gli umani sul task di matching e quality ranking.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem presented his work on adaptive inference in low-resource settings, specifically focusing on the Multi Model and Early Exit methods. He hypothesized that conflicting gradients occur in Early Exit training, leading to lower performance. Rotem's team compared individual Early Exit models with separate Multi Model classifiers, finding that Multi Model classifiers outperformed Early Exit by an average of 2.3%. They introduced SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method that avoids conflicting gradients. SWEET closed most of the gap between Early Exit and Multi Model but negatively affected later classifiers in some cases. The method outperformed both methods in speed/accuracy trade-off tests, particularly for BERT-Large. This research highlights the existence of conflicting gradients in Early Exit training and introduces a promising fine-tuning algorithm for future research.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e parlo di strutture di dipendenze nella coordinazione. Come saprete, ci sono diverse strutture di dipendenze assunte da teorie e approcci di corpus diversi. Ad esempio, in Universal Dependencies, la struttura della coordinazione "Lisa, Bart, Maggie" ha il primo congiunto come capo della struttura di tutto il coordinato. In questo caso, Lisa. Un approccio simile è quello dell'approccio di Igor Mel'čuk in teoria del testo, dove anche qui la struttura di tutto il coordinato è guidata dal primo congiunto. Questi due approcci sono asimmetrici. Sono in grado di singolarizzare uno dei congiunti. Ora, ci sono anche strutture di coordinate asimmetriche, come quella dell'approccio di Prague. L'approccio congiunto guidato assume che le strutture di coordinate siano guidate dalla congiunzione. Quindi otteniamo dipendenze da fine a tutti i congiunti. Infine, c'è anche un approccimento multi-guidato utilizzato, ad esempio, in Word Grammar di Hudson, dove dicono che tutti i congiunti sono capi della struttura di coordinate. Quindi otteniamo dipendenze dal governante. Ecco, ora il mio obiettivo è quello di produrre un argomento nuovo per le strutture di coordinate simmetriche, come queste due, contro le strutture di coordinate asimmetriche, come queste due. Ok. L'argomento è basato sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi. In inglese, come sapete, gli oggetti diretti preferiscono di essere vicini al verbo, mentre gli aggiunti possono essere più lontani. Quindi "Marge ha letto ieri" è corretto perché l'oggetto direttore è vicino al verbo, mentre "Marge ha letto ieri lo" è peggio. Giusto? Perché qui tra il verbo e l'oggetto direttore c'è un aggiungente: "ieri". Tuttavia, questo effetto può migliorarsi quando l'oggetto direttore è molto pesante e lungo. Perché allora può essere spostato alla posizione dopo l'aggiungente. Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. "Marge ha letto questo libro davvero affascinante sulla api ieri." Va bene se invece di "lo" abbiamo questa NP lunga. Ma è anche OK dire, "Marge ha letto ieri questo libro davvero affascinante sulla api." Quindi la ragione qui è che è possibile perché questa frase viola generalmente il principio grammatico che gli oggetti diretti dovrebbero essere vicini al verbo, ma soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che dice che le dipendenze più brevi sono preferite. Quindi queste due alberi solo mostri la lunghezza delle dipendenze cruciali, le dipendenze che non cambiano tra queste due strutture. Quindi qui abbiamo una dipendenza da "ha letto" all'aggiungente di lunghezza 7 misurata in parole e da "ha letto" al libro di lunghezza 4, quindi insieme è 11. Se si scambiano queste due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più breve. Quindi invece di 11, 6 è molto più breve. Quindi? Viola un principio, ma soddisfa un altro. Ok. Quindi cosa abbiamo fatto, abbiamo estraendo statistiche varie sulla coordinazione dalla versione a riga di Penn Treebank e vedere il paper "Why wouldn't you use universal dependencies" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendano a essere più corti. Quindi "sale e pepe" e non "pepe e sale", misurato in sillabe. E anche l'osservazione che è stata fatta in parsing che questa tendenza cresce con la differenza di lunghezza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto sinistro preferisce essere il primo, più forte, giusto? Quindi la proporcione è più grande del primo congiunto sinistro. Ma cosa è nuovo in questo paper è che abbiamo osservato che questa tendenza si verifica solo quando il governante è a sinistra o assente. Quindi il governante è a sinistra in questo esempio "ho visto Bart e Lisa" quindi il governante è a sinistra. Non c'è in questo esempio "Homer è venuto e ha tossito". Qui abbiamo una coordinazione di due verbi e non c'è un esterno, un governante esterno. In queste case, il primo congiunto preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governante è a destra, come qui, "rideva" governa la coordinazione Ted e Ned, questo effetto scompare. Abbiamo dimostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe la seconda colonna, e in parole la terza colonna. Quindi mi concentrerò sulla terza colonna. Quello che vediamo qui è che quando il governante è a sinistra, la tendenza per il primo congiunto a essere più corto cresce gradualmente, con la differenza assoluta in parole, e lo stesso è osservato quando non c'è governante come in coordinazione di frasi. Ma quando il governante è a destra, questa tendenza scompare. E dimostriamo nel paper come questo fornisce un argomento contro le strutture di coordinate asimmetriche, come queste due, e per le strutture di coordinate simmetriche, come queste due. Quindi vedere il paper per gli argomenti completi. E parlare con noi durante la sessione di poster. Grazie.</sample>
    <sample id="15">There are three authors involved in the article: Matthias Lindemann, Alexander Koller, and Ivan Titov.</sample>
    <sample id="16">I domini che risultano più semplificati sono i testi della Bibbia e i testi per apprendisti stranieri.</sample>
    <sample id="17">The speaker, Shengqiong Wu, introduces their work on multimodal relation extraction, which aims to determine the semantic relationship between entities in various forms and modalities. They propose a framework that includes feature refinement with a graph information bottleneck principle, multimodal topic information as supplementary context, and an attention operation to integrate multimodal topic words. Experiments on the MRE dataset show that their method outperforms text-based methods and other multimodal baselines. The ablation study reveals that internal-information screening is more useful for inputs with higher text-vision relevance, while external-information exploiting is more useful for inputs with lower text-vision relevance.</sample>
    <sample id="18">Un esempio di preferenza per i congiunti a sinistra più brevi è "salt and pepper" invece di "pepper and salt".</sample>
    <sample id="19">The speaker, Zhang Qin from Shenzhen University, introduces their work on efficient open-domain question answering accepted by ACL 2023. They explain the two-stage model proposed by Danqi Chen in 2017, which uses a retrieval to retrieve evidence contexts and a reader to reason out the answer. The challenges of open-domain question answering include large Wikipedia corpus size, slow index file searching, and high memory costs of language models. To achieve efficient systems, they propose techniques such as approximate nearest neighbor search, skip reading, document filtering, embedding compression, lightweight models, parameter sharing, and knowledge distillation. They also compare existing models based on data aspects and discuss future works on deployment in low-power devices and evaluation metrics.</sample>
    <sample id="20">Sì, puoi usare i modelli per la tua ricerca. Tutti i modelli pre-addestrati ottenuti da NACHOS sono gratuiti e accessibili su Hugging Face, e tutti i script di addestramento sono su GitHub repository.</sample>
    <sample id="21">DEplain-apa è basato su testi di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono la struttura del modello, la dimensione del modello e il numero di esempi di addestramento.</sample>
    <sample id="23">The paper discusses the challenges in text image models, specifically focusing on the inability of these models to accurately represent text. The authors investigate the performance of different text encoders, such as T5, PaLM, and ByT5, and find that while larger PaLM models perform better at spelling, they are impractical for many applications due to their size. They propose a new strategy by concatenating an additional text representation from the ByT5-small model to the existing text representation in the Imagen model, which improves its ability to spell words and generate images with accurate text. However, the diffusion model can still introduce errors, so the generated text may not be perfect.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistici estratti dal versione a righe elevate del Penn Treebank. Questi statistiche hanno confermato l'osservazione precedente che i congiunti sinistri tendono ad essere più corti rispetto ai congiunti destri, misurati in sillabe o parole. Inoltre, è stata osservata una maggiore tendenza quando la differenza di lunghezza tra i due congiunti aumenta.</sample>
    <sample id="25">Gli esperimenti sono stati progettati per estrarre statistiche sulla coordinazione dal versione a sinistre del Penn Treebank. Le statistiche hanno confermato l'osservazione precedente che i complementi diretti sinistra tendono a essere più corti. Inoltre, è stata osservata che questa tendenza cresce con la differenza di lunghezze tra i due complementi. L'effetto si è dimostrato quando il governatore è sulla sinistra o assente, ma scompare quando il governatore è sulla destra.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati non è molto efficace, specialmente quando si trattano di classi rare come il dissenso. In questo studio, anche con un addestramento iniziale su 43 esempi di dissenso, il classificatore non ha fatto molto meglio del caso casuale.</sample>
    <sample id="27">Il numero di autori coinvolti nell'articolo non è specificato nella presentazione fornita.</sample>
    <sample id="28">I nomi dei personaggi nella conversazione presa a esempio sono Bob e Alice.</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="30">The paper "LLM-Blender" introduces a simple yet effective ensemble learning framework for large language models. The key idea is based on pairwise ranking and generative fusion. The authors propose a two-stage framework named LLM-Blender, which runs n different models to get their outputs, uses a pairwise ranking module named PairRanker to compare all these candidates, and then picks the top K candidates to use as input to a sequence-to-sequence model for learning and inference in a generated fusion model. The authors also create a new dataset named MixInstruct for evaluating ensemble learning frameworks. Experiments show that LLM-Blender outperforms other methods on various correlation metrics.</sample>
    <sample id="31">I fornitori dell'articolo sono Koustav Sinha, John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams.</sample>
    <sample id="33">Il framework quantifica la posizionalità confrontando le annotazioni reali con le predizioni e etichette dei modelli e dei set di dati. Utilizza una misura di correlazione di Pearson per misurare quanto i modelli e i set di dati siano allineati con le annotazioni reali.</sample>
    <sample id="34">In this presentation, Marcos Treviso introduces a work called "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation." The work is the result of a collaboration with Alexis Ross, Nuno Guerreiro, and André Martins. The framework combines selective rationalization and counterfactual text generation to leverage their complementary strengths. CREST generates counterfactuals by masking specific parts of the input and using a masked language model to fill in the masked response with new tokens. The quality of the counterfactuals produced by CREST is evaluated using both automatic metrics and human evaluation. The results show that CREST counterfactuals are more valid and natural than those produced by other methods. The framework also proposes an alternative way to perform rationalization with both factual and counterfactual examples, which achieves the top result on IMDB itself. Overall, CREST produces plausible explanations that focus on the contrasting parts of the input.</sample>
    <sample id="36">This paper presents a method for improving multilingual machine translation by adding language-specific layers to the model. The authors propose using one regular transformer layer per language, and selecting the correct sublayer at inference time based on the language being used. This allows for increased capacity per language while keeping inference costs constant. The authors also propose a method for learning the best placement of these layers, which involves training a large model with all possible components and selecting the component with the largest weight as the LSL. Experiments on WMT21 news translation and Flores-101 show that this approach results in significant improvements over baseline models, particularly for low-resource languages.</sample>
    <sample id="37">Il risultato dello studio precedente è stato che i soggetti umani hanno anche rivelato stereotipi razziali quando hanno ricevuto gli stessi prompt di persona. Questo ha permesso una confrontazione diretta tra le risposte generate dal modello e le risposte umane, dimostrando che i modelli possono riflettere i bias razziali anche se non sono intenzionalmente programmati per farlo.</sample>
    <sample id="38">In questo studio sono state utilizzate diverse fonti di dati, tra cui le statistiche estratte dal versione a riga di testo del Penn Treebank e il paper "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di posizione del dibattito (determinar si due enunciati de debate son de acuerdo o desacuerdo, sin importar el tema) y la clasificación de expansión/comparación (expansión/comparación de las unidades discursivas).</sample>
    <sample id="41">The research paper "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives" by Silin from EPFL University, in collaboration with Sony Group Corporation, introduces a Persona-grounded Commonsense Knowledge Graph (PeaCoK) to represent real-world personas and their rich world knowledge. PeaCoK contains about 3,800 personas and 40,000 distinctive attributes, forming about 100,000 personal inferences or facts. The graph is built in three steps: selecting personas from existing commonsense graphs, inducing attributes of personas from both commonsense knowledge graphs and large-scale pre-trained language models, and crowdsourcing the annotations of PeaCoK relations using a joint human-AI majority voting scheme. PeaCoK can help language models learn and generalize persona knowledge, as demonstrated by training a BART-based common knowledge generator on a persona attribute inference task. The results show that Comet-BART trained on PeaCoK achieves better automatic evaluation results and a higher accept rate in human evaluation compared to large-scale pre-trained language models. Additionally, PeaCoK knowledge can be used to improve downstream narrative modeling, such as persona-grounded dialogue generation tasks, where the retrieved facts from PeaCoK are used to augment each speaker's profile. Human evaluation shows that PeaCoK augmented models achieve better dialogue generation on various aspects, including fluency, consistency, engagement, and persona expression.</sample>
    <sample id="42">Il numero di autori coinvolti nell'articolo non è specificato nella presentazione.</sample>
    <sample id="43">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="44">Il framework introdotto differisce dai lavori precedenti in quanto confronta annotazioni effettuate da utenti reali con le predizioni e etichette dei modelli e dei set di dati, invece di concentrarsi solo sull'acquisizione di accordi tra gli annotatori o la modellazione delle distribuzioni degli annotatori.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è la persona di un uomo bianco.</sample>
    <sample id="46">I sistemi commerciali messi a confronto sono DeepL e Google Translate.</sample>
    <sample id="47">Ciao, mi chiamo Shangbin e sono studente di dottorato all'Università di Washington. Oggi sto presentando il mio lavoro "Da i dati di pretraining ai modelli di linguaggio fino alle attività downstream: tracciando le tracce dei bias politici che portano a modelli di NLP non equi". I modelli di linguaggio vengono addestrati su grandi quantità di dati di web crawled. I media politici sono bene rappresentati nei loro dati di pretraining. Secondo un sondaggio del C4 Corpus, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, eccetera, sono bene rappresentati nel training dei modelli di linguaggio. Questo ha creato una benedizione mista per le applicazioni dei modelli di linguaggio. Di un lato, hanno potuto imparare da diverse prospettive, celebrando la democrazia e la pluralità delle idee. Di altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente preconizzate e potrebbero portare a problemi di giustizia in applicazioni downstream. Per questo, propone di indagare il percorso dei bias politici dalla data di pretraining ai modelli di linguaggio fino alle attività downstream, specificamente chiedendo le seguenti domande: Prima, come valutiamo la tendenza politica dei modelli di linguaggio e in che modo i dati di pretraining potrebbero influenzare i bias politici? Seleziono diverse forme di prompt utilizzando i questionari politici come il test di conferenza politica. Questo ci consente di fare un'evaluation automatica basata in modo solido sulla scienze politiche. Alcuni risultati preliminari dimostrano che i modelli di linguaggio hanno tendenze politiche diverse. Occupano tutti i quattro quadranti del campus politico. Possiamo vedere che GPT-4 è il modello di linguaggio più liberale di tutti, e i modelli GPT sono generalmente più liberali socialmente rispetto ai modelli BART e i suoi varianti. Seleziono i modelli di linguaggio con tendenze politiche diverse utilizzando i questionari politici. Questo ci consente di vedere se i bias politici dei modelli di linguaggio sono davvero presi dai dati di pretraining. Potrei condurre un esperimento controllato pretrainando i checkpoint dei modelli di linguaggio su 6 corpus partisani separati in notizie e social media, ulteriormente divisi in base alla loro tendenza politica. Pretrainando i modelli di linguaggio su tali corpus partisani, possiamo vedere che le coordinate ideologiche dei modelli di linguaggio si spostano corrispondentemente. Ad esempio, se RoBERTa viene ulteriormente addestrato su un corpus di Reddit a sinistra, possiamo vedere un notevole spostamento verso la sinistra in termini di tendenze politiche. Anch'io ho cercato di indagare se i modelli di linguaggio possono prendere iniziato la polarizzazione che è prevalentemente nella nostra società moderna. Divido i corpus di pretraining in due periodi: prima e dopo il 45° Presidente degli Stati Uniti. Seleziono i modelli di linguaggio pretrainati su due differenti corpus temporali. Possiamo vedere che i modelli di linguaggio hanno generalmente una tendenza politica che si sposta più lontano dal centro dopo il 2017. Questo indica che i modelli di linguaggio possono anche prendere iniziato la polarizzazione nella nostra società. Infine, valuto i modelli di linguaggio con tendenze politiche diverse sul rilevamento della discriminazione e la rilevazione di notizie fake, applicazioni che spesso coinvolgono i modelli di linguaggio e hanno implicazioni molto significanti. Se analizziamo il peroformanza per category, ovvero se separiamo le prestazioni in diverse demografie o tendenze politiche dei media notiziari, possiamo vedere un模式. Ad esempio, per la rilevazione della discriminazione, i modelli di linguaggio sinistri sono migliori nell'individuare la discriminazione razziale rivolta a gruppi minoritari, tuttavia sono peggiori nell'individuare la discriminazione rivolta a gruppi più potenti in nostra società. E viceversa, i modelli di linguaggio destra sono migliori nell'individuare la discriminazione razziale rivolta a bianchi e maschi, tuttavia peggiori nell'individuare la discriminazione razziale rivolta a neri LGBTQ+ e altri gruppi minoritari. Simili tendenze si applicano anche per la rilevazione delle notizie fake, dove possiamo vedere che i modelli di linguaggio sinistri sono migliori nell'individuare la menzogna da parte dei loro opposti politici e viceversa. Mostra anche molti esempi qualitativi per vedere che i modelli di linguaggio con tendenze politiche diverse danno predizioni diverse ai casi di discriminazione e menzogna basate sulle loro categorie sociali. Ci sono molte altre esempi in appendix per ulteriormente evidenziare che questa indica che ci sono problemi di giustizia che sono molto urgenti relativi ai bias politici dei modelli di linguaggio. Ad esempio, se i modelli di linguaggio destra vengono ulteriormente addestrati su discriminazione o menzogna o qualunque altra cosa e distribuiti su una piattaforma di social media popolare, questo significa che le persone con opinioni politiche opposte potrebbero essere marginalizzate e la discriminazione razziale rivolta a gruppi minoritari potrebbe continuare senza controllo. Così ha suonato l'allarme per noi di riconoscere e affrontare i problemi di giustizia derivanti dai bias politici dei modelli di linguaggio. Un po' di discussione. Vorremmo anche sottolineare che espongiamo il dilemma unico dei bias politici dei modelli di linguaggio. Sembra come tra Scylla e Charybdis. Se non sterilizziamo le opinioni politiche nei dati di pretraining, i bias propagheranno dai dati di pretraining ai modelli di linguaggio fino alle attività downstream, creando infine problemi di giustizia. Se proviamo a sterilizzarle in qualche modo, dobbiamo anche rischiarare la censura o l'esclusione. E è incredibilmente difficile determinare cosa è realmente neutro e dovrebbe essere retain language monitoring data. Così è come il problema dell'elettrice trolley. Ok, bello. Penso che sia tutto per oggi. Grazie per il tuo tempo.</sample>
    <sample id="48">Il numero di autori coinvolti nell'articolo non è specificato nel riassunto fornito.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">The audio discusses the introduction of DEPLAIN, a new corpus for German text identification on both document and sentence levels. Regina Stodden explains that text simplification aims to improve comprehension for target groups such as people with reading difficulties or non-native speakers. The presentation highlights the need for parallel pairs of texts to train text simplification models, which is addressed by the proposed DEPLAIN corpus.

DEPLAIN is divided into two subcorpora: DEPLAIN-apa, based on news texts with 13,000 manually aligned sentence pairs, and DEPLAIN-web, covering various domains with 30,450 sentence pairs, including both manual and automatic alignments. The corpus showcases different types of simplification techniques, such as lexical substitution, clause deletion, reordering, and word insertion, with varying degrees of simplification across different text types.

Omar then presents use cases for DEPLAIN, including evaluating automatic alignment methods and fine-tuning language models for automatic text simplification. The paper details adaptations to alignment methods and the performance of models like long-mBART and mBART in producing simplified texts at both document and sentence levels. The presentation concludes with an invitation to meet during the conference.</sample>
    <sample id="51">Il loro set di dati include tre domini: musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è il concetto di riferimento alla prospettiva che le persone hanno come risultato dei propri demografici, identità e esperienze di vita. Questo concetto è spesso utilizzato in studi critici, specificamente in ambienti accademici femministi e queer.</sample>
    <sample id="53">Il relatore del video è Dawei, un studente di dottorato presso l'Università di Saarland in Germania.</sample>
    <sample id="54">Vasudha, a PhD candidate in Computer Science at Stony Brook University, presented her work on "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge" at ACL 2023. The paper defines cognitive dissonance and its importance in understanding human decision-making processes and mental health. The authors created a cognitive dissonance resource by annotating around 1,000 discourse unit pairs using a dissonance-first approach. They experimented with transfer learning and active learning to improve dissonance detection, achieving an AUC of 0.75 on the task so far. The study highlights the feasibility of PRC strategy for rare class acquisition and cold starting AL with appropriately designed transfer learning task.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without re-training or adopting a specific architecture for SimulST.</sample>
    <sample id="56">Il numero di autori coinvolti nell'articolo non è specificato nella presentazione.</sample>
    <sample id="57">Il modello testato non funziona sulla suite di test KITMUS senza addestramento specifico per la comprensione del testo. Tuttavia, con l'addestramento specifico, i modelli migliori riuscono a integrare efficacemente le conoscenze provenienti da diverse fonti.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: Background-Pretrain, Background-Both e Background-Inference.</sample>
    <sample id="59">In this presentation, Yanis Labrak introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. The presentation begins by discussing language modeling in healthcare and the lack of specialized models for other languages. DrBERT is introduced as the first biomedical model in French, based on RoBERTa and trained on NACHOS, a data set of medical crawled data from the web. The presentation compares DrBERT with ChuBERT, a clinical model with anonymized data obtained from the Nantes University Hospital data warehouse. The presentation also discusses the impact of pre-training strategy and the number of data required to train a specialized model on French data. The evaluation highlights that models performed best on tasks with data of the same nature as those on which the model has been trained. Overall, DrBERT offered better performance on nine of the 11 downstream tasks and surpassed globally the result of the generic model, here CamemBERT. All the pre-trained models obtained from NACHOS are freely available on Hugging Face, and under the MIT license, and all the training scripts are on their GitHub repository.</sample>
    <sample id="60">I fornitori dell'articolo sono Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="61">The last research question is whether we should only use the clean samples for validation or if there are better ways to utilize them.</sample>
    <sample id="62">The paper "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training" by Nitay Calderon, Amir, Subhabrata, and Roi explores the potential of compressing NLG models while preserving their performance. The authors conduct a systematic study of task-specific knowledge distillation for NLG in realistic setups, considering four NLG tasks: summarization, question generation, common sense reasoning, and simplification and style transfer. They explore different approaches for knowledge selection, including the use of pseudo-targets, and propose a novel knowledge distillation technique called joint-teaching to address student exposure bias and teach the student to correct its own mistakes.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre gli stessi output per un compito specifico indipendentemente dalla variabile leggermente diversa nella descrizione dell'istruzione.</sample>
    <sample id="64">Il nome della relatrice o del relatore è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica che il modello è più suscettibile a variations nella formulazione dell'instruzione, mentre una sensibilità inferiore indica che il modello è più robusto e produce output consistenti anche con variazioni nell'instruzione.</sample>
    <sample id="66">Il paper proposto, "Deep Learning for Mathematical Reasoning," esplora lo sviluppo di metodi di apprendimento profondo per la comprensione e la risoluzione di problemi matematici. L'interesse in quest'area è cresciuto recentemente, con una particolare enfasi sulla capacità delle macchine di risolvere problemi matematici e dimostrare teoremi. Il paper si concentra su due principali category: contesti visivi e contesti tabulari. Ad esempio, i problemi di geometria richiedono l'identificazione di relazioni geometriche, l'applicazione di teoremi e calcoli per ottenere la risposta numerica. Altri argomenti importanti includono la prova automatica di teoremi e la capacità delle macchine di comprendere problemi matematici da testo. I modelli di rete neurale, come i modelli a catena di sequenza e i modelli di apprendimento a supporto del linguaggio, sono stati proposti per affrontare queste complessità. Tuttavia, anche questi modelli hanno limitazioni inherent, come la difficoltà a gestire numeri grandi e la inconsistenza nel ragionamento matematico.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models and identifies the main factors that contribute to it. The authors find that severe interference occurs when the model is very small compared to the data size, and that tuning the sampling temperature is key for strong performance. They also conclude that language similarity and the number of languages do not have a large impact on interference levels. The simplest solution to controlling the trade-offs is temperature sampling, which involves allowing more training examples from lower-resource languages to be sampled. Based on their results, the authors recommend using a baseline for battling interference that is weak due to size in small models, and weak due to uncalibrated temperature for larger models that use values that are too high. Overall, the paper suggests that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">I modelli vengono addestrati su un contesto linguistico che include sia frasi accettabili che non accettabili. Questo contesto è creato utilizzando frasi accettabili e non accettabili da dataset diversi o anche da frasi di Wikipedia completamente irrelevanti.</sample>
    <sample id="69">In genere, per il raggiungimento di buone prestazioni in WSL, si possono ottenere buone prestazioni con circa 20 campioni puliti per classe.</sample>
    <sample id="70">I fornitori dell'articolo sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">The AltEntities Corpus is a dataset created to improve conversational systems and benchmark language models' ability to understand indirect referring expressions. The dataset includes 6,000 alternative questions across music, books, and recipes domains, with 42,000 indirect referring expressions. Annotators were provided with background knowledge about the entities and asked to select one using three to five indirect referring expressions. The results show that when the language model has access to the same background knowledge as the annotators, accuracy is high (92-95%). However, if the model only has access to partially overlapping or entity names, accuracy drops to 82-87% and 60%, respectively. The dataset is domain-generalizable and can be accessed through a provided link.</sample>
    <sample id="72">Sì, è necessario sviluppare nuovi metodi per misurare i bias dell'informazione poiché le modelli di linguaggio possono trasmettere bias politici acquisiti durante la formazione in base ai dati di addestramento. Questi bias possono portare a problemi di giustizia e equità in applicazioni downstream, come la deteczione di discriminazione e notizie fake.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Akshatha.</sample>
    <sample id="74">The paper "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths" by Xiangqing et al. introduces a new approach to enhance the existing ATOMIC knowledge base, which is a large-scale commonsense knowledge base covering event-centered social aspects of inferential knowledge tuples. The authors address the limitations of ATOMIC, such as sparse graph structure and insufficient semantic information, by proposing Dense-ATOMIC, which completes missing links in ATOMIC, including B-to-A, B-to-B, A-to-B, and A-to-A links. They achieve this by normalizing tail events, training a relation prediction model called Rel-CSKGC, and constructing Dense-ATOMIC using an Intra- and Inter-Cluster Completion Strategy. The results show that Dense-ATOMIC yields higher knowledge coverage and benefits the performance of COMET, demonstrating its potential for commonsense reasoning.</sample>
    <sample id="75">Zheng Yandan presents Jointprop, a joint work with Hao Anran and Luu Anh Tuan, addressing the limitations of supervised learning in name entity recognition (NER) and relation extraction (RE). The proposed framework models NER and RE tasks by propagating labels over heterogeneous graphs, considering inter- and intra-connections among labeled and unlabeled data. The method includes span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Experiments on four datasets show significant improvements over baseline models for both NER and RE tasks.</sample>
    <sample id="76">La struttura di propagazione dei bias politici include l'evaluation della tendenza politica dei modelli di linguaggio e il ruolo dei dati di addestramento preiniziali, in quanto i modelli possono acquisire tendenze politiche diverse in base ai corpi di testo preiniziali. Inoltre, i modelli di linguaggio con tendenze politiche diverse possono influenzare le prestazioni su compiti downstream come la deteczione del discorso razzista e la deteczione di notizie fake, potenzialmente causando problemi di giustizia.</sample>
    <sample id="77">The video presents a joint work from Yale University and Microsoft Research on improving summarization factual consistency using natural language feedback. The researchers introduce a new dataset, DeFacto, which contains human demonstrations and feedback for enhancing summarization factual consistency. They propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. The study focuses on abstractive text summarization and evaluates the factual consistency of summarization models. Annotated data points are collected from the XSum dataset, with initial system outputs from the pre-trained Pegasus model. The results show that human-edited summaries receive higher automatic factuality scores but have lower textual overlap with reference summaries. The dataset provides valuable annotations for training factuality metrics and meta-evaluation. The DeFacto dataset is available on GitHub.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web. Come menzionato nella presentazione, DEplain-apa è basato su testi di notizie e include 483 documenti manualmente al线, mentre DEplain-web include 750 documenti di diverse domande, sia manualmente che automaticamente al线. Questa differenza conduce a una varietà diversa di trasformazioni di semplificazione in entrambe le subcorporature.</sample>
    <sample id="79">Yes, CoScript is available publicly. The paper mentions that the CoScript dataset can be a valuable resource to advance research on language planning and provides more details about it in the paper.</sample>
    <sample id="80">La filigrana viene inserita nel testo definendo un insieme di parole denominate "trigger set". Quindi, quando un utente invia una frase al servizio di provider, il provider conteggia il numero di parole del trigger nel testo. Il embedding fornito è una somma ponderata del embedding di destinazione e dell'embedding originale. Il peso dell'embedding di destinazione è proporzionale al numero di triggere nella frase. Se il numero di triggere nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding di destinazione.</sample>
    <sample id="81">I fornitori dell'articolo sono affiliati all'Università del Pennsylvania.</sample>
    <sample id="82">The video introduces a research paper titled "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring" that focuses on developing an automated essay scoring (AES) system without the need for human intervention or labeled data. The paper highlights the challenges of collecting labeled essays and proposes a novel framework called ULRA, which uses multiple heuristic quality signals to provide stronger supervision for training a neural AES model. The ULRA framework consists of a heuristic essay ranking module and a deep pairwise rank aggregation module, which address the inconsistencies among different quality signals and provide a unified supervision for model training. Experimental results demonstrate the effectiveness of ULRA in both transductive and inductive settings, outperforming all unsupervised baselines with a large improvement.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue. Questo è noto come "Curse of Multilinguality".</sample>
    <sample id="84">Hello everyone. I'm Shwai He. Today, I'll talk about my paper for ACL 2023, "PAD-Net: An Efficient Framework for Dynamic Networks". Most traditional networks are static, given the input value. They compute with fixed or static parameters that can't change with the input. However, dynamic networks can alter their architecture or parameters based on the input. Examples include Mixture of Experts selecting specific sub-networks and Dynamic Convolution linearly combining π layer convolution kernels as the linear weight is based on the input. The implementation of dynamic networks is why we use them; replacing static layers with dynamic ones often results in better performance. However, fully dynamic networks tend to have excessive parameters, making them less practical. We ask two questions: Do fully dynamic networks contain redundant parameters? Do static and dynamic parameter coexistence perform better? Our hypothesis is that fully dynamic networks contain partially dynamic sub-networks that maintain or exceed the original network's representation power. Based on this, we created PAD-Net: Partially Dynamic Network. We partition parameters into dynamic and static, set up scale factors to describe the intensity of each mode, and apply Iterative Mode Partition to achieve our goal. Our experiment shows that PAD-Net outperforms static and fully dynamic networks while maintaining fewer parameters and less computation. We also conducted ablation studies to find optimal Dynamic Ratios for Dynamic Convolution and Mixture of Experts. Scale Factors for dynamic and static parameters are crucial, as they affect the accuracy of different dynamic networks. Compared to network pruning, PAD-Net performs significantly better due to maintained static parameters. PAD-Net also makes outputs more discriminating, contributing to its superior performance over fully dynamic networks. Future work includes extending our methods to other mainstream networks, hardware-friendly structured manners, and introducing more modes like zero elements, static, and dynamic parameters.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata potrebbe essere "fare un pasto con ingredienti specifici e senza allergeni". Questo è un esempio di un obiettivo con vincoli, poiché si specifica sia i componenti del pasto che le limitazioni alimentari.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo utilizzando una combinazione di tecniche di similarità e test statistici, inclusa la somiglianza di coseno e L2, e il test di Kolmogorov-Smirnov (KS). Inoltre, visualizzano gli embedding delle sentenze su quattro dataset per dimostrare che è difficile distinguere gli embeddings del backdoor da quelli normali.</sample>
    <sample id="87">Il lavoro utilizza il modello pre-addestrato DrBERT in francese per costruire un nuovo modello di apprendimento automatico per la medicina. DrBERT è basato su RoBERTa e è stato addestrato su NACHOS, un insieme di dati medicali raccogliuti da Internet. Inoltre, il lavoro introduce una comparazione di modelli con diverse impostazioni di pre-addestramento e fonti di dati.</sample>
    <sample id="88">GPT-4 è meno allineato con i Paesi non inglesti.</sample>
    <sample id="89">La relatrice illustra come il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione in questa frase: "For example, if we receive a speech chunk containing 'I'm going to talk about...' and our model predicts the translation in German, and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames. This means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha, we will not emit the last word and we wait for another speech chunk."</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" explores the feasibility of using language learners as annotators in NLP. The authors conducted a proof-of-concept study, targeting three languages (English, Korean, and Indonesian) and four tasks from the GLUE benchmark. They recruited native speakers and language learners, providing additional resources to help them understand annotation samples. The results show that labels annotated by language learners are nearly accurate, especially for simpler tasks and easy-to-medium level questions. Aggregating labels by majority voting can bring language learners' performance on par with native speakers. Additionally, language learners' language proficiency and vocabulary improve through annotation tasks. This work suggests a novel way for data construction by recruiting language learners as annotators, broadening NLP research for many languages and overcoming geographic and technological barriers to building benchmark datasets for low-resource languages.</sample>
    <sample id="91">La quantità di attività influisce sulla performance del modello in quanto, con un aumento del numero di attività, il modello raggiunge un better performance e allo stesso tempo ha una sensibilità più bassa.</sample>
    <sample id="92">I autori confrontano il loro metodo con altri modelli seq2seq senza albero, come Tree-Structured Recurrent Neural Networks (RNNs) e Graph Neural Networks (GNNs).</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono gli advisor del primo autore, Matthias Lindemann.</sample>
    <sample id="94">The speaker, Jingwei Yi from the University of Science and Technology of China, introduces a paper on protecting the copyright of embedding as services via backdoor watermark. The paper proposes a backdoor-based watermark method called Embedding Marker, which is applicable to embedding as services. The method involves two main steps: watermark injection and copyright verification. The watermark injection step defines a target embedding and provides an embedding that is a weight summation of the target embedding and the original embedding. The copyright verification step detects whether a model behind another service contains the word mark by computing the cosine and L2 similarity between the requested embedding and the target embedding, and applying the KS test. The paper conducts experiments on four data sets and shows that Embedding Marker can have great detection performance while keeping great utility for downstream tasks.</sample>
    <sample id="95">Il primo autore di PaLM non è specificato nella presentazione di David Vilar. Tuttavia, PaLM è stato sviluppato da Google AI e ha coinvolto un team di ricercatori e ingegneri.</sample>
    <sample id="96">Ciao a tutti, mi chiamo Jenny e sono un dottorando in primo anno all'Università Carnegie Mellon. Oggi presenterò il mio lavoro intitolato "NLPositionality: caratterizzazione dei bias di progettazione dei dataset e dei modelli". Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, tra cui Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.

Immaginiamo di essere un giornalista che sta esaminando i commenti sottoposti a un articolo del mio giornale per rimuovere il contenuto tossico. Potremmo ricorrere a un API popolare come Prospective API per la deteczione della tossicità, e quest'ultima funziona bene per Carl Jones, ma non per Aditya Sharma, dove l'API di Prospective non è così sensibile ai termini offensivi più comuni in contesti indiani. Questo è un esempio di un bias di progettazione, dove si notano differenze sistematiche nel comportamento delle tecnologie tra le popolazioni.

I bias di progettazione come quello che abbiamo appena illustrato possono verificarsi anche a causa della posizionalità dei ricercatori e dei sviluppatori di modelli. La posizionalità è semplicemente le opinioni che le persone hanno come risultato dei propri demografici, identità e esperienze di vita. Questo concetto è ampiamente utilizzato in studi critici, specificamente in ambienti accademici femministi e queer. Come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati e i risultati, poiché può cambiare le decisioni che i ricercatori prendono.

Allora, dobbiamo chiedersi se i dataset e i modelli hanno posizionalità? Non stiamo cercando di dire che i modelli stessi o i dataset stessi hanno identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizionalità su altre.

Lavoro precedente ha suggerito evidenze anecdetiche di avere posizionalità, come differenze culturali nei modelli e nei dataset, e definizioni teoriche di posizionalità dei modelli. Tuttavia, queste opere non analizzano confrontando gli utenti finali con i dataset e i modelli esistenti, e studiare la posizionalità dei dataset e dei modelli è sempre più importante poiché i compiti NLP diventano sempre più soggettivi e orientati alla società, rendendo difficile caratterizzare come queste posizionalità siano sbagliate poiché non tutti i decisioni sono documentati e molti modelli sono nascosti dietro alle API.

Per studiare la posizionalità dei dataset e dei modelli, confrontiamo le annotazioni con reali utenti utilizzando i dataset esistenti e i modelli. Lo facciamo attraverso il nostra piattaforma NLPositionality. Il nostro framework funziona in due passi principali. Il primo passo è rannotare nuovi i dataset con diversi annotatori. Dovremmo fare questo ignorando le demografie degli annotatori originali, poiché di solito solo un paio di annotatori annotano ogni istanza e le demografie spesso non vengono raccolte e condivise. Quindi optiamo per rannotare i dataset per ottenere molte annotazioni per istanza e ottenere un insieme ricco di demografie. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando una punteggio correlazione di Pearson, e quindi il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori, poiché confronta gli utenti finali con i modelli e i dataset, le predizioni e le etichette, invece di guardare solo l'accordo degli annotatori o la modellazione delle distribuzioni degli annotatori.

Il nostre frame è in gran parte abilitato tramite Lab in the Wild e piattaforme di crowdsourcing online per collaboratori HCI. In Lab in the Wild è un piattaforme di sperimentazione online dove possiamo reclutare volontari diversi. Differenziato rispetto alle piattaforme come M Turk che in gran parte hanno partecipanti dalla US o India e inoltre Lab in the Wild è ancora capace di ottenere alta qualità dei dati. Abbiamo host 2 task su Lab in the Wild, uno di essi essendo social acceptability, e il modo in cui funziona è che i partecipanti leggono una situazione dal dataset Social Chemistry e quindi scriveranno quanto è accettabile socialmente. Dopo di che per restare aggiornati nella storia, possono confrontare le risposte con un AI e altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo replicato un setup molto simile per il compito di rilevamento tossicità e discriminazione, dove i partecipanti leggeranno un esempio dal dataset Dynahate e scriveranno se pensano che sia un esempio di discriminazione o discriminazione. Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Il nostro studio alla fine ha raccolto più di 16.000 annotazioni da più di 1000 partecipanti da 87 paesi.

Ora che siamo meglio equipaggiati per rispondere a chi alignano i dataset e i modelli con gli utenti finali, scopriamo che c'è posizionalità in NLP. Ad esempio, scopriamo che i dataset e i modelli sono maggiormente al linee con i paesi che parlano inglese. Quindi per l'analisi social acceptability di GPT 4, scopriamo che è maggiormente al linee con i paesi che parlano inglese e confuciani. Troviamo anche maggior alleanza con le persone che hanno un college di istruzione. Quindi per GPT 4, in analisi social acceptability, scopriamo che è maggiormente al linee con le persone che hanno un college di istruzione o un istituto superiore e trovi lo stesso per Dynahate dove è maggiormente al linee con le persone che hanno un college di istruzione. Tuttavia, quando i modelli e i dataset sono al linee con popolazioni specifiche, alcune sono inevitabilmente lasciate indietro. Un esempio di questo è che i dataset e i modelli sono meno al linee con le persone non binarie rispetto a loro omologhe maschili. Troviamo questo anche in analisi social acceptability di GPT 4 e anche in analisi discriminazione di Dynahate.

Data la posizionalità in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è tenere un record di tutti i relevanti scelte di progettazione durante il processo di ricerca. La nostra raccomandazione successiva è fare NLP ricerche con un occhio di perspectivism. La nostra terza raccomandazione è costruire dataset specializzati e modelli all'interno di 4 comunità specifiche. Un buon esempio di questo è il progetto Masakhane. Vogliamo sottolineare che NLP inclusivo non è solo fare in modo che tutti i tecnologi funzionino per tutti. Ecco come conclude la nostra presentazione. Ma se volete imparare di più, potete controllare il dashboard per i risultati più aggiornati e il mio articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: i modelli specifici devono essere addestrati, i processi di addestramento sono lunghi e complicati, e è necessario addestrare e mantenere più modelli per raggiungere differenti livelli di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere la creazione di un set di dati più diverso e bilanciato, che rappresenti diverse ideologie e punti di vista. Inoltre, è importante utilizzare tecniche di addestramento che siano più resistibili ai bias, come l'uso di loss functions più equilibrate o l'applicazione di regolazioni per ridurre i bias.</sample>
    <sample id="99">Ciao, mi chiamo Siyu Yuan e sono dalla Università Fudan. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". In vita quotidiana, gli umani spesso pianificano le loro azioni seguendo istruzioni passo-passo in forma di obiettivi orientati a un scopo specifico. Le opere precedenti hanno sfruttato i modelli linguistici per pianificare obiettivi astratti di attività stereotipate come "fare un pasto". E hanno dimostrato che i modelli linguistici di grandi dimensioni possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, le opere precedenti si concentrano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione degli obiettivi con vincoli specifici, come "fare un pasto al cioccolato", è tuttora sottostudiata. In questo articolo, definiamo il problema della pianificazione linguistica con vincoli, che imposa diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diverse finalità reali specifiche con multi-faccet constraint. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, innanzitutto valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli linguistici di grandi dimensioni. Poiché non esiste un dataset di obiettivi specifici per supportare lo studio, dobbiamo acquisire questi obiettivi innanzitutto. Come illustrato nella tabella, estendiamo gli obiettivi astratti con multi-faccet constraint per l'acquisizione di dati umano-in-blocco utilizzando InstructGPT. Sampliamo 100 obiettivi specifici e valutiamo i script generati dai modelli linguistici di grandi dimensioni. Questa tabella riporta l'accuratezza generale dei risultati. Troviamo che tutti i modelli linguistiche ottengono risultati insoddisfacenti nell'elaborazione degli obiettivi specifici. Poi eseguiamo un'analisi dettagliata per indagare perché i modelli di apprendimento falliscono. I risultati nella figura mostrano che la completezza semantica dei script generati è accettabile ma la fedeltà ai vincoli non può essere garantita. Scaviamo in un argomento più raffinato delle categorie dei vincoli definiti in wikiHow. Il diagramma termico nella figura mostra che le prestazioni del pianificatore di InstructGPT variano considerabilmente per gli obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'uscita dei modelli linguistici cade in una varianza elevata, portando a un peggioramento delle prestazioni. Pertanto, adottiamo l'idea di over-generate-then-filter per migliorare la qualità della generazione. Innanzitutto mostriamo tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basati sui relativi obiettivi astratti. Poi, InstructGPT genera K script per gli obiettivi specifici. Infine, sviluppo un modello di filtri per selezionare i script fedeli. Convertiamo i script e gli obiettivi in embedding di InstructGPT e calcoliamo la similarità coseno come punteggio di similitudine per misurare la similitudine semantica. Inoltre, premiamo i script che contengono le parole chiave dei vincoli di destinazione. Solo teniamo il script se il punteggio di destinazione è il più alto tra gli obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità più alta. Il nostro metodo migliora grandemente la capacità di pianificazione sia in completezza semantica che in fedeltà ai vincoli. Poiché i modelli linguistici di grandi dimensioni sono costosi da deploys, è essenziale abilitare la capacità di pianificazione linguistica dei modelli più piccoli e specializzati. Creare un dataset è un passo essenziale a questo fine. Tuttavia, gli studi precedenti non abilitano la pianificazione degli obiettivi specifici e annotare manualmente i dataset è costoso. Pertanto, seguiamo l'idea di distillazione di conoscenza simbolica per costruire un dataset di pianificazione linguistica con vincoli, denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per garantire la qualità dei set di validazione e test, chiediamo ai worker di crowd-sourced di trovare e revisionare gli esempi sbagliati. Questa figura mostra la distribuzione dei vincoli del CoScript. Troviamo che CoScript ha una pluralità alta nei relativi obiettivi specifici. Con CoScript possiamo provare modelli più piccoli ma specializzati per la pianificazione con vincoli. Troviamo che T5 addestrato su CoScript può generare script di qualità più alta rispetto a maggiori modelli linguistici, dimostrando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset idonei. In sintesi, stabiliamo il problema della pianificazione linguistica con vincoli. Evaluiamo la capacità di pianificazione linguistica con vincoli dei modelli linguistici di grandi dimensioni e sviluppo un metodo di over-generate-then-filter per i modelli linguistici di grandi dimensioni. Utilizziamo i modelli linguistici di grandi dimensioni per generare un insieme di script di qualità elevata, CoScript, per la pianificazione con vincoli. Speriamo che il dataset CoScript possa essere un risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Puoi trovare maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, with each step corresponding to a document in the corpus. Multi-hop retrievers are trained by maximizing the probability of the ground-truth chains given questions. The PromptRank approach is data-efficient and uses an unsupervised retrieval method combined with a few-shot language model-based reranker. The two main steps involve retrieving candidate chains using TF-IDF retrieval and hyperlink traversal, followed by reranking these candidates using the few-shot language model reranker. The likelihood of the question given the chain according to a language model is used as the scoring function. The chain prompt is constructed by inserting the chain documents into the prompts, designating them with an indicator token, and including an instruction to elicit the language model's reasoning ability over the chain documents. Additional techniques like instruction search, instruction sampling, and temperature scaling are explored. The approach is evaluated on the HotpotQA dataset using metrics like R@K recall at K and answer recall AR@K. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop dense retrievers.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a sistemi di traduzione di punta, ma ha problemi di accuratezza, in particolare con gli errori di omissione.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono: essere applicabile a embedding as services, non degradare la utilità dei forniti embeddings, essere abbastanza nascosto per i nemici o essere facile da rimuovere per i nemici, e essere trasferibile ai servizi del nemico durante il processo di estrazione del modello.</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse, ma non è specificato quali lingue sono.</sample>
    <sample id="104">Per la riannotazione, vengono campionate diverse istanze da un set di dati.</sample>
    <sample id="105">La metrica di distanza utilizzata per misurare la differenza tra set di dati benigni e backdoor è la similarità di coseno e la similarità L2. Inoltre, viene anche utilizzata la test statistico di Kolmogorov-Smirnov (KS) per determinare la covertness del modello.</sample>
    <sample id="106">The paper presents the QUEST dataset, a retrieval dataset with over 3,000 entity-seeking queries containing implicit set operations. The dataset includes verified answer entities and marked attributable spans for different query constraints. The authors show that the dataset poses a challenging retrieval problem due to the need to search over a large document corpus to find multi-answer sets where evidence for different query constraints can come from different parts of the document. They evaluate systems on the dataset using sparse and dense retrievers as well as a T5-based reranker. The results show that there is a large room for improvement in retriever performance based on the recall of the complete answer set, and that queries with set intersection and set difference are particularly challenging.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per testare la performance in diverse impostazioni di traduzione tra lingue. In particolare, i modelli mT5 e XLM-R + PTR sono stati valutati in un setting monolingua e multilingua. I risultati hanno dimostrato che i modelli encoder-decoder ottengono le migliori prestazioni su tutti i nove dataset. Inoltre, è stato notato che l'addestramento in una combinazione di diverse lingue può migliorare le prestazioni dei modelli encoder-decoder o encoder-ptr.</sample>
    <sample id="108">The paper discusses the limitations of minimal pair paradigms in evaluating language models' acceptability judgments. The authors propose a new approach to evaluate models' acceptability towards longer sentences by recreating longer sequences with acceptable or unacceptable prefixes from the same or different data sets. They find that MPP judgments are mostly robust for arbitrary context length when using Wikipedia sentences, but they increase or decrease significantly when using sentences from the same data set. The authors conclude that language models are sensitive to latent syntactic and semantic features shared across sentences, and current evaluation methods may not fully capture the abstract knowledge of language models throughout the context window.</sample>
    <sample id="109">The paper presents a method for creating a large dataset of natural language instructions and their corresponding inputs and outputs without any human labor. The method involves prompting a pre-trained language model to generate examples, which are then used to create additional examples through paraphrasing. The resulting dataset contains 240,000 examples and is diverse in tasks, content, and phrasing. The paper also shows that fine-tuning an 11 billion-parameter T5 model on this dataset outperforms both T0++ and Tk-instruct across several benchmarks.</sample>
    <sample id="111">Gli autori selezionano un insieme di parole a frequenza moderata utilizzando un corpus di testo generale e conteggiano la frequenza delle parole in esso.</sample>
    <sample id="112">Ciao a tutti, mi chiamo Shuheng. Oggi presenterò il mio articolo "Do CoNLL-2003 named entity taggers still work well in 2023?" Iniziamo. Il nostro articolo ha indagato il problema della generalizzabilità utilizzando la compito di riconoscimento di entità denominate o NER, per abbreviare. Abbiamo osservato che i modelli hanno stato sviluppati in CoNLL-2003 per quasi 20 anni e questo naturalmente solleva alcune problematiche. Innanzitutto, possono questi modelli generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzabilità? Al tempo stesso, se osserviamo una pessima generalizzabilità, cosa causa il drop di prestazione di questi modelli? Per indagare su queste problematiche, abbiamo sviluppato il dataset CoNLL++. Questo è un insieme di dati che abbiamo raccolto da Reuters News del 2020 e successivamente annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi allenato su più di 20 modelli su CoNLL-2003. Abbiamo valutato questi modelli sia sul test set di CoNLL-03 che su CoNLL++. Infine, abbiamo calcolato il percentuale di cambiamento in F1 per valutare la generalizzabilità di ciascun modello. Cos'è necessario per una buona generalizzabilità? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari. Il primo è l'architettura del modello. Attraveso gli esperimenti, abbiamo scoperto che i modelli a transformer normalmente generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzabilità. E infine, come tutti sanno, il numero di esempi di allenamento diretti influisce direttamente sulle prestazioni di un compito inferiore. Qui abbiamo anche scoperto che maggiori esempi di allenamento diretti, in pratica, portano a una migliore generalizzabilità. Passiamo alla nostra prossima domanda: cosa causa lo scadimento delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattativo, ovvero l'overfitting costato per riciclarlo sempre lo stesso set di test e questa è di solito manifestata come la diminuzione dei guadagni su un nuovo set di test. La seconda ipotesi è il drift temporale, ovvero lo degradamento delle prestazioni causato dallo spazio temporale crescente tra i dati di addestramento e i dati di test. Per l'overfitting adattativo, abbiamo visto che dalla grafica sulla destra, la linea rossa migliore ha un coefficiente di regressione maggiore di uno. Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 si traduce in più di una unità di miglioramento su CoNLL++, il che indica che non ci sono ritorni decrescenti. Questo ci dimostra che l'overfitting adattativo in questo caso non è osservato. Ma cosa ne è stato del drift temporale allora? Per il drift temporale, abbiamo fatto un esperimento di retraining o continuare a pre-addestrare i modelli con più recenti dati e abbiamo scoperto che le prestazioni peggiorano con un intervallo temporale più grande e questo conferma la nostra ipotesi che la principale causa dello scadimento delle prestazioni è il drift temporale. Conclusione: per una buona generalizzabilità, saremmo necessari un modello architettura migliore, un modello di dimensioni maggiori, nonché maggiori esempi di allenamento diretti. E queste vanno mano a mano, non possiamo avere un ingrediente ma gettare via gli altri. Al tempo stesso, abbiamo anche scoperto che lo scadimento delle prestazioni qui è causato dal drift temporale e, sorprendentemente, non è causato da overfitting adattativo anche se CoNLL-2003 è stato utilizzato per più di 20 anni. Quindi torniamo alla domanda che abbiamo posto in titolo del nostro articolo: "I tagger di CoNLL-2003 funzionano ancora in 2023?" E la risposta è, in pratica, un risounding yes. Speriamo che il nostro articolo chiami alla più ampia ricerca su come migliorare la generalizzabilità dei modelli. Infine, assicuratevi di controllare il nostro articolo, il dataset e se avete qualche domanda, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">The presentation introduces the work of a team from Nanyang Technological University in Singapore, which focuses on addressing the heavy parameter problem in large language models. The team proposes a grouped head attention mechanism that uses a divide-and-conquer strategy to compress multi-head attention. This method involves two stages: group-constrained training and Voting-to-Stay algorithm. In the first stage, heads are divided into groups to make intra-group heads more similar and inter-group heads more separate. In the second stage, redundant heads are pruned based on their scores given by an evaluator. The proposed method achieves significant parameter compression while maintaining comparable performance on tasks such as machine translation, language modeling, and abstractive summarization. The team also conducts efficiency analysis and identifies task-specific automatic pruning as a promising direction for future research.</sample>
    <sample id="115">L'approccio proposto utilizza segmenti parlati di dimensione variabile, gestendo la latenza attraverso parametri specifici. Ad esempio, se un modello predice una traduzione in tedesco e il peso dell'attenzione non è concentrato sulle ultime lambda di frameli di parola, allora non verrà rilasciato il ultimo segno. Se invece il peso dell'attenzione è concentrato sulle ultime lambda di frameli di parola, allora verranno rilasciati tutti i segni.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono che Servin è un giudice e Kea è un baker. Queste informazioni sono utili per risolvere il compito di risoluzione di coreferenza e identificare correttamente l'entità a cui si riferisce il pronome "he".</sample>
    <sample id="117">Il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente è la qualità dell'esempio.</sample>
    <sample id="118">The paper presents a new pre-training technique for code-switched NLP tasks, called SwitchMLM. It proposes novel MLM techniques tuned to the case of code-switching and architectural changes and auxiliary loss to enhance switch-point information content in intermediate layers. The results show that the proposed method outperforms multilingual pre-trained models like mBERT and XLM-R on sentiment analysis tasks. Probing experiments verify the claim that the proposed methods increase the amount of switch-point information in intermediate and final layers.</sample>
    <sample id="119">L'articolo si concentra su GPT-4, GPT series e RoBERTa come modelli linguistici negli esperimenti estesi.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">I esempi di inferenza diretta sono "Easy on Me" o "the first one".</sample>
    <sample id="122">I fornitori dell'articolo sono affiliati all'Università Fudan.</sample>
    <sample id="123">Ying and Zhiyang are presenting their research on MultiInstruct, a multi-modal instruction tuning benchmark dataset. They aim to investigate whether instruction tuning can improve generalization to unseen multi-modal tasks. The dataset consists of 62 diverse multi-modal tasks covering 10 broad categories, derived from 21 existing open-source datasets. They use OFA, a unified multi-modal pre-trained model, as their base model. During training, they mix all instances for all tasks and randomly combine each instance with one of its five instruction templates. For testing, they conduct a total of 5 experiments by evaluating the model using one of the five instructions. They report min and max performance and standard deviation across all 5 experiments. Their main result shows that instruction tuning can significantly improve OFA's performance on seen multi-modal tasks. They also introduce an evaluation metric called sensitivity, which measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction.</sample>
    <sample id="124">Tan Qingyu from the National University of Singapore and Alibaba presented a work titled "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The presentation focused on temporal reasoning in three levels: time-to-time, time-to-event, and event-to-event. The researchers conducted experiments using LMs such as T5-L, FLAN-T5-L, and ChatGPT, and proposed the TempReason dataset to cover all three levels of reasoning and long temporal coverage. They also introduced a new setting called Reasoning QA to study temporal reasoning. To improve temporal reasoning capability, they proposed a training strategy with two components: Temporal span extraction pre-training and time-sensitive reinforcement learning. The results showed that TempT5 can significantly improve the performance of T5-SFT in OBQA and the Reasoning QA set.</sample>
    <sample id="125">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è stato considerato come un approccio standard.</sample>
    <sample id="127">Namgyu Ho, a master's student at KAIST AI in Korea, introduces their work "Large Language Models Are Reasoning Teachers" alongside Laura Schmid and professor Se-Young Yun. The paper addresses the limitation of chain-of-thought reasoning being only applicable to large models like GPT-3 or PaLM due to memory and computation costs. They propose using these large models as reasoning teachers to transfer abilities to smaller models through a novel technique called Diverse Reasoning. By generating multiple diverse reasoning samples from large models, they fine-tune small models to perform complex reasoning tasks effectively. The method outperforms existing baselines on 12 tasks, especially in text-based ones, with performance improvements up to 55% in Multi Arithmetic. The approach is scalable and can be further improved by increasing dataset size, teacher model quality, or student model size. However, it involves trade-offs between development and inference costs. The paper provides detailed analysis, results on open-source models, and code/data for future work.</sample>
    <sample id="128">In this audio, Akshatha and Martin present their work on "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources." The test suite is designed to evaluate the ability of natural language understanding models to integrate knowledge from different sources. The authors propose a coreference resolution task to probe for this ability. They define three settings of KITMUS: Background-Pretrain, Background-Both, and Background-Inference. The results show that many coreference resolution models are unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Even the best-performing models have difficulties with reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="129">Gli autori hanno fornito un esempio di gruppo contrassegnato come "woman warrior" per illustrare il concetto di "markedness". Questo esempio è stato usato per spiegare come le parole che differiscono dalla norma dominante sono linguisticamente contrassegionate e come queste contrasseggiate facilitano i pregiudizi e le narrativa essenzializzanti.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono quelle che non usano un modello a transformer, non usano un modello grande o non hanno abbastanza esempi di addestramento.</sample>
    <sample id="131">I nomi dei set di dati di test non sono forniti nel discorso.</sample>
    <sample id="132">There are three authors involved in the article: Akshatha, Martin, and the team from McGill University, Mila, and Microsoft Research.</sample>
    <sample id="133">L'autore opera con più modalità, non solo con il testo.</sample>
    <sample id="135">In this video, James and Sarah Finch introduce ABC-Eval, a new method for evaluating conversational AI models. They explain that traditional methods of evaluation, such as human judges selecting the better conversation or rating them on a Likert scale, can be subjective. ABC-Eval aims to reduce subjectivity by explicitly annotating whether each model response exhibits certain behaviors, such as being irrelevant, contradictory, hallucinating incorrect facts, or violating common sense knowledge. The method measures the rates at which chat models commit these thematic errors and is more reliable and predictive of overall conversation quality compared to existing methods. The researchers evaluated four state-of-the-art chat models using ABC-Eval and found that it captures unique aspects of chat quality, enabling higher resolution evaluation than previous methods.</sample>
    <sample id="136">Jasivan and Nafise conducted a study at the University of Sheffield titled "FERMAT: An Alternative to Accuracy for Numerical Reasoning". The motivation behind this work is that there are many real-world applications for numerical reasoning, and downstream tasks require factual correctness. The study introduces FERMAT, a flexible evaluation set based on arithmetic types, which includes maths worded questions extracted from Illinois and CommonCore. The study found that most models perform poorly across all aspects, but fine-tuning with math teachers' templates improved performance. The study also investigated training dependency and the impact of training templates, finding that language and mathematical diversity is important in improving performance. The study concludes that existing benchmarks tend to be unrepresentative and single scores don't help, and FERMAT provides a more informative alternative to fill in that gap.</sample>
    <sample id="137">The paper "Tell2Design: A Dataset for Language-Guided Floor Plan Generation" by Sicong from the Singapore University of Technology and Design presents a new machine learning task where the goal is to generate floor plan designs directly from language instructions. The authors define the task as generating 2D floor plan designs that comply with given natural language instructions, which include semantics, geometry, and topology. They use publicly available floor plans to construct their Tell2Design dataset, collecting 5,051 human-annotated language instructions from Amazon Mechanical Turk and around 76,000 language instructions generated artificially from pre-defined templates. The main challenges of this task are designing under stricter constraints, understanding the big picture of the entire floor plan from unstructured text, and dealing with ambiguous, incomplete, or misleading information in human instructions. The authors propose a sequence-to-sequence model using a transformer-based encoder-decoder structure to generate floor plan layouts from language instructions. They compare their method with several text-conditional image generation models and show that their method achieves the highest IoU scores, outperforming other baselines by a large margin.</sample>
    <sample id="138">Secondo gli autori, l'integrazione e l'uso di conoscenza proveniente da diverse fonti in NLU è un'area poco studiata.</sample>
    <sample id="139">I relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Yes, CoScript has been subjected to quality checks. To ensure the quality of the validation and test set, crowd-sourced workers were asked to find and revise incorrect samples.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno alcune limitazioni. In primo luogo, solo una piccola percentuale delle traduzioni dipende dal contesto, rendendo inadeguate le misure di valutazione basate sul corpus come BLEU. In secondo luogo, le risorse proposte per la traduzione dipendente dal contesto spesso supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto solitamente si basano su conoscenza umana e curazione.</sample>
    <sample id="142">Ciao! Parliamo del nostro lavoro su "Resolving Indirect Referring Expressions for Entity Selection", in cui introduciamo il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro di squadra con Filip Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è capire come parla il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai in mente 'Easy on Me' o 'I Gotta Feeling'?" Qui, l'utente vuole scegliere tra due canzoni. La cosa più ovvio è usare un riferimento diretto, ad esempio nominando il nome della canzona "Easy on Me" o la sua posizione, "la prima". Ma spesso un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può succedere quando l'utente non ricorda il nome della canzona. O le pronunciazioni sono troppo simili e difficile da distinguere. O quando l'utente vuole specificare un preferimento. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "il nuovo" o "la canzone che non è energica". Questo è un problema importante nei sistemi conversazionali e anche per valutare le comprensioni di entità dei modelli di apprendimento automatico (LLM). Non conosciamo un dataset pubblico più grande per il compito, quindi lo abbiamo raccolto utilizzando annotazione di massa. Il nostro dataset copre tre domini diversi: musica, libri e ricette. Il nostro metodo di raccolta dei dati mette l'accento sulla informalità utilizzando un setup di completamento di fumetti. Il fumetto ha tre bubble di dialogo. In primo luogo, Bob dice, "Ricordi quella canzone che stavamo ascoltando ieri?". E con quello, Bob stabilisce il contesto del dialogo. In secondo luogo, Alice dice, "Hai in mente 'Easy on Me' o 'I Gotta Feeling'?" Che è una domanda alternativa. E in terzo luogo, Bob usa un riferimento indiretto per scegliere tra queste due entità, ad esempio, "il nuovo". Forniamo automaticamente le prime due bubble di dialogo, ma la terza è riempita dagli annotatori. La prima bubble è scelta da pochi prompt manuali per ogni dominio. La seconda, che è la domanda alternativa, è generata come segue. Utilizziamo sempre un template semplice. "Hai in mente A o B?" Dove A e B sono样例从Wikipedia中提取。下面是我们在数据集中使用的不同采样方法。当我们向上移动时，实体变得越来越相似，通常更难区分。第一种方法是随机均匀采样。第二种方法是实体具有相似的标题，例如两本书名为“La Restituzione”。第三种方法是实体具有相似的Wikipedia描述。最后一种方法是实体具有相似的Wikipedia信息框或属性，例如相同的流派或艺术家。当我们向标注员展示这些替代问题时，他们知道这些实体的名字，但不一定了解这些实体。因此，我们向标注员展示了每个实体的背景知识。对于歌曲，我们简单地显示Google搜索结果链接，并要求标注员至少听一听每个歌曲，并阅读关于每个歌曲的信息。例如，这是歌曲“Easy on Me”的Google搜索结果。对于书籍和食谱领域，我们展示了来自Wikipedia的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自Wikipedia，这样标注员就能知道它们看起来是什么样子。然后我们要求标注员选择其中一个实体，例如，这里是第一个实体，用三到五个间接引用表达来描述它，例如，“那个有钢琴音乐的”。这里有一些我们数据集的例子。例如，“quella senza parole”，“non quella con il ragazzo di 12 anni”，“quella immaginaria”，“proviene dall'Azerbaijan” e così via. Corpus AltEntities ha 6.000 domande alternative attraverso tre domini, e ha 42.000 espressioni di riferimento indiretto. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello ha accesso alla stessa conoscenza di sfondo degli annotatori, allora l'accuratezza è davvero alta, circa il 92% al 95%. Ma questo non è realistico. Se il modello ha accesso a qualche conoscenza di sfondo parzialmente sovrapposti, allora l'accuratezza è tra il 82% e il 87%, che è più realistico. Ad esempio, quando il modello recupera la conoscenza di sfondo. Se il modello ha accesso solo ai nomi degli enti, allora l'accuratezza è solo del 60%, c'è molto spazio per miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio proposto viene confrontato con le politiche di attesa-k e l'accordo locale, che sono strategie applicate a modelli offline, e anche con la migliore architettura specificamente progettata per la traduzione contemporanea.</sample>
    <sample id="144">Mi dispiace, non ho informazioni sulle affiliazioni degli autori dell'articolo.</sample>
    <sample id="145">Il nome della relatrice o del relatore è Jenny.</sample>
    <sample id="146">The paper by Yicheng et al. presents a study on the omission problem in dialogue summarization, where critical information is lost in generated summaries. The authors analyze the omission rate across five domains and six pre-trained models, finding that about 70% of generated summaries suffer from this issue. They propose the OLDS dataset, which provides high-quality omission labels for dialogue summarization, constructed upon five existing benchmarks covering five domains. The authors explore three baselines for omission detection, including pair-wise classification, sequence labeling, and pointer network, using Precision, Recall, and F1-score to evaluate their performance. They also introduce a post-editing method for summary refinement, which significantly improves the performance when omission content is provided. The study highlights the importance of addressing the omission problem in dialogue summarization to improve the quality of generated summaries.</sample>
    <sample id="147">Ci sono tre autori coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, sono Sara Papi dalla Università di Trento e dalla Fondazione Bruno Kessler. In questo momento, vorrei brevemente presentare il paper "Attention as a Guide for Simultaneous Speech Translation", che è un lavoro di collaborazione con Matteo Negri e Marco Turchi. Cos'è la traduzione contemporanea del discorso? La traduzione contemporanea del discorso, o SimulST, è il processo di tradurre un linguaggio parlato in un testo in un altro linguaggio in tempo reale, consentendo una comunicazione tra lingue diverse. E cosa sono i problemi dei modelli di SimulST attuali? Le strutture specifiche sono addestrate, introducendo moduli aggiuntivi da ottimizzare. Addestramenti lunghi e complicati, ad esempio, addestramenti che coinvolgono obiettivi di ottimizzazione diversi. E addestrare e mantenere più modelli per raggiungere differenti regimei di latenza. Ad esempio, addestrare un modello con un'average di un secondo di latenza e un altro con due secondi di latenza, e così via. Quindi, la nostra soluzione è la seguente: innanzitutto, utilizzare i modelli offline ST esistenti senza re-entrare o adottare strutture specifiche per SimulST. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici. E sfruttare il conoscimento acquisito dal modello attraverso della meccanismo di attenzione tra l'input audio e l'output testuale. Questo è il meccanismo di attenzione reciproca, e puoi vedere un esempio sulla destra. La nostra soluzione è di proporgere EDAtt, o Encoder-Decoder Attention, e è una strategia per decidere se emettere o non emettere una traduzione parziale, basata su dove si concentra l'attenzione. Un parola viene emessa se l'attenzione non è concentrata, ovvero la somma della attenzione è inferiore a un certo livello alpha verso le ultime lambda frasi di discorso, ovvero le informazioni ricevute sono sufficientemente stabilizzate. Ad esempio, se riceviamo un blocco di discorso che dice "I'm going to talk about..." e il modello predice la traduzione in tedesco, e guardiamo i pesi di attenzione, vedremo che le prime due parole puntano alle prime frasi ricevute, mentre l'ultima parola punti alle ultime lambda frasi di discorso. Questo significa che le prime due parole verranno emesse, poiché la somma dei pesi di attenzione è sopra un certo livello alpha, non emetteremo l'ultima parola e aspetteremo un altro blocco di discorso. Se continuiamo e riceviamo un altro blocco di discorso, e il modello predice altre tre parole e guardiamo quei pesi di attenzione, vedremo che nessuna parola punti alle ultime lambda frasi di discorso. Questo significa che queste tre parole verranno emesse. Se guardiamo i principali risultati di EDAtt, plottiamo i risultati di traduzione contemporanea del discorso su grafici in cui abbiamo il BLEU su un lato che misura la qualità della traduzione, e l'average lagging che è la misura di latenza, e consideriamo anche la computazionale aware average lagging che tiene conto del tempo computazionale del modello per prevedere l'output. Quindi, vogliamo che le curve siano il più alta possibile su questo grafico. Ma anche vogliamo che siano spostate verso sinistra. E confrontiamo con策略被应用于offline模型的流行策略，即Wait-k策略和Local Agreement。我们还与专门针对同时预翻译的最新架构进行比较。这些是我们在德语上获得的所有结果的 simultanea traduzione策略。我们可以看到，它优于应用于offline模型的所有策略，因为曲线向左移动。我们还看到，如果我们考虑实际经过的时间或计算时间，即计算时间，它是最快的策略。如果你想发现更多结果，请阅读我们的论文。我们还释放了源代码和模型以及同时输出，以促进我们工作的可重复性。谢谢你的关注。</sample>
    <sample id="149">Sì, il set di dati è disponibile pubblicamente.</sample>
    <sample id="150">Archiki ha presentato il paper "MEETINGQA: Extractive Question-Answering on Meeting Transcripts" insieme a collaboratori di Adobe Research e UNC Chapel Hill. Questo studio si concentra su un nuovo campo di ricerce NLP basato su transcrizioni di riunioni, che sono documenti lunghi, specifici di dominio e ricchi di informazione. Gli altri studi precedenti si concentravano solo sulla task di riassunto e l'estrazione di azioni, dimenticando la parte di QA intrinsecamente importante. L'equipe ha introdotto un nuovo dataset chiamato MeetingQA, che è un dataset di QA extrattivo basato su domande fatte da partecipanti in una riunione e le risposte corrispondenti. La raccolta dei dati è stata fatta utilizzando il corpus AMI, che comprende quasi 100 ore di transcrizioni man mano. Per annotare le risposte, i ricercatori hanno reclutato gli annotatori per etichettare le frasi nella spagnola dell'answer. La raccolta di MeetingQA comprende 7.7K domande suddivise tra i set di addestramento, sviluppo e test, con 30% delle domande non risolvibili.</sample>
    <sample id="151">Ciao a tutti, mi chiamo Ying e il mio collega Zhiyang presenteremo il nostro lavoro su MultiInstruct, che migliora l'apprendimento senza supervisione multi-modal tramite addestramento con istruzioni. Con i progressi recenti in modelli di lingua a larga scala, molte ricerche hanno iniziato a esplorare nuovi paradigmi di impegno dei modelli pre-addestrati per compiti diversi in modo efficiente sia in termini di parametri che di dati. Recentemente, molte studies hanno dimostrato che l'addestramento con istruzioni consente ai modelli di lingua a larga scala di svolgere compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte delle precedenti ricerche sull'addestramento con istruzioni si è concentrata su migliorare le prestazioni in modo zero-shot su compiti solo linguistici, mentre i compiti di visione e multi-modal sono stati lasciati fuori. Pertanto, in questo lavoro ci proponiamo di indagare se l'addestramento con istruzioni a un modello pre-addestrato multi-modal può realmente migliorare la generalizzabilità a compiti non visti multi-modal. Inoltre, alla nostra ricerca, abbiamo scoperto una considerevole disparità nella disponibilità di dataset di istruzioni tra NLP e multi-modal. Esistono più di 1600 compiti di istruzioni solo linguistici. Tuttavia, non esiste alcun grande dataset pubblicamente disponibile di istruzioni multi-modal. Questo ha motivato la nostra decisione di costruire un dataset di istruzioni multi-modal adatto. Qui presentiamo MultiInstruct, il primo benchmark di addestramento con istruzioni multi-modal che comprende 62 diverse compiti multi-modal coprindo 10 category. Questi compiti sono derivati da 21 dataset aperti esistenti e ogni compito è equipaggiato con cinque istruzioni esperte scritte. Per investigare l'addestramento con istruzioni multi-modal su nuestro dataset proposto, utilizziamo OFA, un modello pre-addestrato multi-modal unificato, come nostra base model. Durante l'addestramento, mescoliamo tutti gli esempi per tutti i compiti. Ogni esempio è combinato casualmente con uno dei cinque template di istruzioni. Durante il test per ciascun compito, eseguiamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ogni esperimento, rapportiamo il minimo e il massimo prestazione e la deviazione standard della prestazione attraverso tutti i 5 esperimenti. Se il compito è un compito di classifica multi-modal, rapportiamo l'accuratezza. Se è un compito di generazione multi-modal, rapportiamo Rouge-L. Per i compiti di NLP, rapportiamo anche Rouge-L. Abbiamo anche introdotto un nuovo metrico chiamato sensibilità. Questo misura la capacità del modello di produrre consistentemente gli stessi output per lo stesso compito indipendentemente dalla variabile leggera nella formulazione dell'istruzione. Ecco i nostri principali risultati. Come possiamo vedere, l'addestramento con istruzioni può migliorare significativamente le prestazioni di OFA su compiti multi-modal visti. Inoltre, l'apprendimento a trasferimento da dataset di istruzioni naturali può beneficiare dell'addestramento con istruzioni. Come possiamo vedere, con l'aumento del numero di compiti, il modello raggiunge prestazioni migliori e contemporaneamente una sensibilità inferiore. Abbiamo anche fatto un esperimento. Utilizziamo un'istruzione contro 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni generali del modello e ridurre la sensibilità molto. Questo mostra l'effetto di diverse strategie di addestramento a trasferimento sulle prestazioni di sensibilità del modello. Come possiamo vedere, grazie all'apprendimento a trasferimento da dataset di istruzioni naturali, il modello può raggiungere prestazioni migliori sensibilità rispetto al modello OFA originale. Abbiamo anche potuto vedere che l'apprendimento a trasferimento da dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni migliori sul dataset di istruzioni naturali. Di conseguenza, in sintesi, proponiamo il primo dataset di istruzioni multi-modal su larga scala con prestazioni notevolmente migliorate per OFA e esploriamo diverse tecniche di apprendimento a trasferimento e ne mostriamo i benefici. Progettiamo un nuovo metrico chiamato sensibilità. Così, un'altra cosa, stiamo raccolgendo un dataset di istruzioni multi-modal su larga scala con circa 150 aggiunti vision language task e lo rilanceremo. Questo è un QR code per i nostri dati e il modello. Grazie.</sample>
    <sample id="152">Frederick Riemenschneider presented a talk on the intersection of NLP and classical philology, introducing valuable resources for Ancient Greek and Latin. The presentation explored the implications and challenges of multilinguality in these models. The speaker introduced new language models specifically designed for classical philology, including monolingual models for Ancient Greek (GreBERTa and GreTa) and multilingual models (PhilBERTa and PhilTa) pre-trained on Ancient Greek, Latin, and English data. The models were trained using a variety of resources, including Open Greek &amp; Latin, Internet Archive, Corpus Corporum, and EvaLatina 2022 dataset. Benchmarking showed that the models outperformed current state-of-the-art models for both Ancient Greek and Latin. The speaker also analyzed how T5's encoder behaves and investigated the implications of multilinguality in their language models.</sample>
    <sample id="153">The speaker, Ninareh Mehrabi, is a postdoctoral scientist at Amazon Alexa AI's Responsible AI team. She presented their work on resolving ambiguities in text-to-image generative models. The goal of the work is to study prompts that are ambiguous and provided to text-to-image models, and propose frameworks to mitigate such ambiguities as well as frameworks to evaluate whether the generated images are faithful to user intention. The pipeline includes curating a benchmark dataset, providing prompts to a prompt disambiguation framework, evaluating the prompts, inputting the disambiguated prompts into a text-to-image model, generating images, and evaluating whether the generated images are faithful to user intention. The paper shows that there is disparity in resolving ambiguities for different ambiguity types, disambiguation using their framework has an overall positive effect in faithful generation, and their automatic evaluation framework is in agreement with human evaluation.</sample>
    <sample id="154">I fornitori dell'articolo sono l'Università di Trento e la Fondazione Bruno Kessler.</sample>
    <sample id="155">The name of the presenter is Javad Hosseini.</sample>
    <sample id="157">Dialogue summarization is a challenging task in text summarization research that aims to distill the salient information from a dialogue context into a concise summary. The existing methods mainly focus on modeling dialogue with pre-computed static graph structure using external linguistic tools, which have two fundamental drawbacks: heavy dependence on the reliability of external linguistic tools and disjoint construction between static graph and graph representation learning phrase. To address these issues, the proposed SDDS model consists of four main components: Utterance Encoder, Static-Dynamic Graph module, and Summary Generator. The Utterance Encoder encodes utterances into vector representations, while the Static-Dynamic Graph module combines multiple static graphs and uses dynamic graph module to capture semantic relationships between utterances. Finally, the Summary Generator fuses the static and dynamic dialogue structures into the final summary. The code and data for the SDDS model have been released on GitHub.</sample>
    <sample id="158">Qipeng Guo from AWS introduced the work "Dual Cache for Long Document Neural Coreference Resolution." The coreference resolution task involves identifying mentions of entities in a document and clustering them based on their reference to the same entity. Conventional methods have quadratic complexity, while cache-based methods reduce it to linear level using a fixed-size cache with eviction policies like LRU. However, in long documents, topic switches cause high cache misses due to scattered mentions. Dual cache addresses this by having a local cache with LRU policy and a global cache with LFU policy. It evaluates frequency and adds entities to the appropriate cache. Dual cache outperforms single cache methods and significantly reduces cache misses, making it the most cost-effective solution.</sample>
    <sample id="159">Ciao a tutti, mi chiamo Koustav Sinha e sono felice di benvenuti alla nostra discussione sulla nostra pubblicazione ACL 2023. I giudizi di accettabilità dei modelli di lingua non sono sempre robusti al contesto. Questo è un lavoro di gruppo con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, rivisitiamo i paradigmi minimali. Il paradigma minimal evaluates i modelli di lingua su giudizi di accettabilità, che possono anche includere la grammatica come BLiMP, SyntaxGym o accettabilità in termini di stereotipi come CrowS pairs. In questo paradigma minimal, il modo tipico per valutare i modelli di lingua è mostrire un'accurata frase o una frase grammaticalmente corretta e quindi mostri una frase accurata o una frase grammaticalmente sbagliata. E l'aspettativa è che il modello metta più probabilità alla frase accurata. Il pipeline MPP corrente non ci permette di valutare l'accettazione del modello verso frasi più lunghe. Ora, i modelli di lingua stanno diventando sempre più lunghi e lunghissimi. Così è cruciale che possiamo valutare l'accettazione del modello nel contesto tutto il tempo. E quello che stiamo cercando di fare qui è di rivedere il pipeline MPP per chiedere al modello di valutare l'accettabilità su frasi più lunghe e più lunghe. Quindi, quello che facciamo è di simulare queste frasi più lunghe, rivedendo i set di dati stessi e quindi creiamo frasi scelendo frasi accettabili o non accettabili da quei set di dati. Ad esempio, qui abbiamo scelto un paio tipici di grammaticalità dal dataset BLiMP dalla casistica Adjunct Island. E quello che facciamo è di creare frasi più lunghe accettabili e non accettabili aggiungendo una prefisso grammaticalmente corretto a entrambe le query accettabili e non accettabili. Possiamo fare lo stesso scelendo frasi non accettabili dallo stesso matching e anche possiamo fare lo stesso scelendo frasi da un subset diverso o da un dataset diverso. Quello che chiamiamo è il scenario di mismatch. Qui le frasi sono sempre provenienti da un dataset rilevante ma non provenienti dallo stesso dataset che stiamo valutando con. Possiamo fare lo stesso per il caso di non accettabilità. Infine, possiamo scegliere frasi da un dominio completamente diverso come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono realmente influenzati dal contesto, se il contesto proviene da un subset diverso del dataset o se è completamente irrelevante per la frase che stiamo analizzando. Come fa il modello? Prima diamo un'occhiata alle frasi di Wikipedia, che sono completamente irrelevanti per il paio di query attualmente considerato, e trovi che i giudizi di accettabilità MPP sono relativamente stabili per lunghezze di contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. Vediamo qui la linea rossa punteggiata che i giudizi di accettabilità MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi provenienti dallo stesso dataset? Qui scegliamo o creiamo frasi provenienti da domini accettabili o non accettabili dallo stesso dataset BLiMP o SyntaxGym. E qui vediamo che i giudizi di accettabilità MPP aumentano o diminuiscono notevolmente quando aggiungiamo prefissi accettabili o non accettabili. Ma quando scegliamo le frasi provenienti da fenomeni dello stesso dataset in BLiMP o SyntaxGym, vediamo un'enorme aumentazione o una enorme diminuzione dei giudizi di accettabilità MPP per il modello, in base a quello che prefisso è stato scelto accettabile o non accettabile. Ora, questa è una grande effetto che aumenta attraverso la lunghezza del contesto e probabilmente affetterà i modelli più recenti che hanno finestre di contesto più grandi. Coso che rende i prefissi di match sensibili ai giudizi del modello? Abbiamo fatto una serie di analisi in cui abbiamo tentato di perturbare la frase di input, cercando di preservare la struttura rilevante ma aggiungendo rumore alla frase di input. E dopo aver fatto queste perturbazioni diverse, scopriamo che nessuna di queste rume non sta realmente facendo il modello cambiare il corso in termini di come ci mostri i giudizi di accettabilità MPP. Di fatto, scopriamo che i modelli sono sensibili a frasi perturbate in modo simile. Quindi, quando perturbiamo le frasi in un dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi in un dominio non accettabile, vediamo un calo dei giudizi di accettabilità MPP in modo simile. Quindi, le tasse di ritake del nostro lavoro è che i modelli di lingua sono sensibili a caratteristiche sintattiche e semantiche latenti che sono condivise tra le frasi. E l'evaluation MPP attuale, che facciamo ora con frasi brevi e singole, non potrebbe affatto catturare completamente il conoscimento astratto dei modelli attraverso il contesto. Per favore leggete il nostro articolo per maggiori dettagli sulle nostre esperienze. Grazie per l'attenzione.</sample>
    <sample id="160">In primo luogo, il primo passaggio del metodo mappa i token di input con un multiset unordered di token che appariranno nell'output.</sample>
    <sample id="161">In CoScript, ci sono 55.000 script rappresentati.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo di MASSalign.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che è molto più economico rispetto all'annotazione manuale dei dati. Ci sono anche fonti di etichettatura più semplici e più economiche, come regole semplici, database o sondaggi a bassa qualità.</sample>
    <sample id="165">The speaker, Wenting Zhao from Cornell University, presents a paper on abductive commonsense reasoning. They start with an example where Emily was stuck in traffic but made it to her flight, and the goal is to find a plausible explanation for this outcome. The paper focuses on a closed-world setting and aims to identify plausible explanations without supervision. Current methods rely on supervised learning, which can be noisy and subjective. The speaker introduces LiPoR, an unsupervised learning method that treats explanations as latent variables and maximizes the marginal likelihood of outcomes given context. A regularizer enforces mutual exclusivity among explanations. LiPoR outperforms previous models on the AlphaNLI dataset by over 4 absolute points in accuracy.</sample>
    <sample id="166">The paper presents a new framework for image retrieval from linguistically complex text, named NDCR. It addresses the challenge of highly similar images and long descriptions by combining the Divide-and-Conquer strategy with Dual-Process Theory. The proposed method consists of three modules: Proposition Generator, Visual-Linguistic Interactor, and Neural-Symbolic Reasoner. The first module decomposes complex propositions into simple ones, while the second module performs visual-propositions' information interaction. The third module integrates reasoning states and results to obtain the final solution. Experimental results show that NDCR outperforms other baselines, and the proposed method can present inference states and results in the middle step, demonstrating interoperability. The paper suggests that neural symbolic calculation may improve compositional reasoning and planning in large language models, and that Dual-Process Theory could be integrated with the Divide-and-Conquer strategy.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati sia manualmente che automaticamente. In particolare, 750 documenti sono stati allineati manualmente, mentre gli altri 750 documenti sono stati allineati automaticamente con metodi di allineamento. Questo ha portato a un totale di 15.225 coppie di frasi parallele.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo notizie dal 2020 e annotandole con le stesse linee guida di annotazione utilizzate per il CoNLL-2003.</sample>
    <sample id="169">Hello everyone, my name is David Vilar, and I will be giving a short review of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PaLM is a 540 billion-parameter large language model presented last year in 2022. It's trained on a large collection of text, comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluated the transition capability of such models using the best practices of the MT community. This involves using the latest test sets to avoid an overlap of the test data with the training data of the language model. And we compared to state-of-the-art systems, so the best performing system, so the WMT evaluation. We use state-of-the-art, neural MT metrics, and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment, where we used one-shot prompting and provided two different prompts for each sentence. The majority of sentences 516 out of 1,000. The difference observed is of more than one BLEURT points. And this can go, in extreme cases, up to 40 BLEURT points. So, it's important to select a good prompting strategy. In our experiments, we settled for a 5-shot prompting strategy where we just marked each sentence that we provide to the system, with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences, are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several short promptings. It's crucial for zero and one-shot prompting. And when we go, as in our case, to five-shot prompting, there is nearly no difference to the actual form of the prompting. It's the examples that carry most of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So, it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data for the WMT evaluations on the dev data. The dev data is much more curated, and with higher quality than the training data, that it's more noisy. And their results so a better performance when using the dev data. Nevertheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But, PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gained from the human evaluation that we performed using the MQM framework said that the fluency of PaLM is comparable to state-of-the-art systems but the main difference comes from the accuracy. So, in particular, the most common errors are omission errors. So, it seems that PaLM chooses to produce a better-sounding translation, sometimes by dropping parts of the source sentence that are made in translation. However, the "Style/Awesome" category for PaLM is lower than for the state-of-the-art systems, which is an additional signal that PaLM provides really fluent output, but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang e sono alla Università di Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Parsing semantico multilingue in più linguaggi e rappresentazioni del significato". La parsing semantica è un compito che costruisce rappresentazioni semantiche di query dell'utente come SQL e Lambda Calculus. E il parsing semantico multilingue è il compito di tradurre query in più linguaggi in diverse rappresentazioni del significato. Come illustrato in questa figura, dobbiamo tradurre la query in più linguaggi utilizzando modelli neurali per SQL, Lambda o FunQL, eccetera. I modelli esistenti di parsing semantico multilingue sono proposti e valutati su dataset di task e applicazioni limitati. Ad esempio, ci sono molti copertura su certi linguaggi naturali. Ma il cinese è mancante e manca di copertura su certe rappresentazioni del significato. Il calcolo Lambda è mancante, o solo valutato su certi modelli neurali. Ad esempio, c'è solo un singolo modello per valutare essi. Così, per questo, proponiamo XSemPLR. Fornisce un insieme di dati uniformi XSemPLR per il parsing semantico multilingue in più linguaggi e rappresentazioni del significato. Contiene 9 set di dati in diversi domini, 5 task di parsing semantico, 8 rappresentazioni del significato, e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostro benchmark, consideriamo i sei ambienti di addestramento e valutazione. Il primo è Translate-Test. Utilizziamo l'API Google Translate per tradurre il linguaggio sorgente verso il linguaggio di destinazione, quindi utilizziamo modelli monolingui per addestrare e valutare. Ad esempio, addestra il modello inglese su query in inglese e durante l'inferenza traduci le query in tedesco utilizzando l'API in inglese e quindi usa il modello addestrato per prevedere il SQL. Anche testiamo il modello monolingue. In questo ambiente, il linguaggio di origine è lo stesso del linguaggio di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche il setting Monolingual Few-shot, in cui addestra modelli monolingui con solo il 10% dei dati di addestramento. Testiamo anche il modello multilingue, che addestra un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglese, cinese per addestrare un modello multilingue. Durante l'inferenza possiamo usare questo modello per traduire query tedesche o cinesi, eccetera. Anche consideriamo il transfer cross-lingua Zero-shot e Few-shot. Addestra su un linguaggio di origine e trasferisci su un altro linguaggio. Durante l'addestramento, addestra su query in inglese o la combinazione di query in inglese e tedesco Few-shot per addestrare un modello multilingue per prevedere l'output SQL. Abbiamo anche scoperto molti risultati interessanti. Riguardo l'analisi dei modelli monolingui, valutiamo due gruppi di modelli inclusi Encoder-PTR che sta per Encoder Pretrained Multilingue con Decodiciere basato sul puntatore, ad es., XLM-R + PTR e mBERT + PTR. E anche valutiamo Encoder-Decoder, che è un Encoder Pretrained Multilingue-Decoder Model, ad es., mBART e mT5. Abbiamo scoperto che Encoder-Decoder ottiene il migliore prestigio su tutti i nove set di dati. Abbiamo anche valutato mT5 e XLM-R + PTR in un setting multilingue. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrando in una combinazione di diverse lingue. Abbiamo scoperto che quasi tutte le maggiori lingue naturali possono ottenere un miglioramento nel prestigio, tranne che il prestigio inglese diminuisce in sette set di dati e aumenta in tre set di dati. Penso che sia noto come "Curse of Multilinguality". Abbiamo anche confrontato il gap di performance cross-lingua. In questa figura, la linea blu rappresenta il transfer cross-lingua Few-shot. La linea arancione rappresenta il transfer cross-lingua Zero-shot. Mentre la linea verde rappresenta il setting Monolingual. Abbiamo scoperto che, confrontando la linea verde e la linea arancione, abbiamo scoperto che il setting Zero-shot, il prestigio di transfer cross-lingua è significativamente elevato, e confrontando la linea blu e la linea arancione, abbiamo scoperto che con il setting Few-shot il gap di transfer si riduce rapidamente. Abbiamo anche scoperto altri interessanti findings. Ad esempio, Encoder-Decoder supera precedenti lavoro o raggiunge risultati simili. Addestrare su lingua naturale inglese può iniziare a migliorare prestigio di Few-shot su lingua di destinazione, e abbiamo scoperto che modelli multilingue come Codex e BLOOM sono ancora inadeguati per task di parsing semantico multilingue. In sintesi, costruiamo XSemPLR, un benchmark unificato per il parsing semantico multilingue con più linguaggi e rappresentazioni del significato. Condurremo un studio di benchmarking esaustivo su tre tipi rappresentativi di modelli multilingue. I nostri risultati hanno molti findings interessanti. Ecco tutto. Sono felice di vedere il vostro commento.</sample>
    <sample id="171">I lavori connessi in tal senso sono quelli che possono essere classificati in quattro category: 1) Metodi che non sono applicabili a embedding as services, 2) Metodi che mancano di transferabilità, 3) Metodi che non sono sufficientemente nascosti per gli attacchi o che consentono easily la rimozione del segno, e 4) Metodi che non mantengono la utilità dei embeddings forniti.</sample>
    <sample id="172">No, multilinguistic language models such as Codex and Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="174">Thea, co-autore del paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis", ha fornito un rapido sguardo alla caratteristica unicità del proprio dataset in un video. L'argomentazione è la valutazione di quanto sia buona o cattura un argomento su una scala da 0 a 1. Ad esempio, "I grandi banchi sono cattivi" viene valutato basso, mentre "I grandi banchi hanno nessuna responsabilità, prendono pesanti rischi e portano alla caduta, perciò dovrebbero essere suddivisi" viene valutato come un argomento coerente e più persuasivo, probabilmente vicino a 1.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni induce l'allegamento tra input e output durante il training. Inoltre, utilizza un rilassamento continuo GPU-amico per approssimare la soluzione più plausibile e permettere la propagazione indietro del gradiente per imparare le permutazioni più linguisticamente plausibili.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita come la capacità del modello di trattare in modo equo e non discriminante le diverse category sociali, inclusi i gruppi etnici, i genere, le religion, le ideologie politiche, ecc. In altre parole, un modello è considerato equo se non produce bias o discriminazione verso alcuna di queste category.</sample>
    <sample id="177">The name of the presenter is Yanis Labrak.</sample>
    <sample id="178">Il nome del relatore è Koustav Sinha.</sample>
    <sample id="179">Melanie Sclar discusses the development of a method called SymbolicToM, which aims to improve theory of mind reasoning skills in large language models (LLMs). Theory of mind involves understanding and predicting the mental states of others. The method uses explicit graphical representations to model these mental states, allowing LLMs to better answer questions about characters' beliefs and intentions. The research presents results from experiments with various LLMs, showing significant performance gains when using SymbolicToM. Additionally, the method demonstrates robustness in out-of-domain setups and generalizes well to new linguistic diversity datasets.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">In this paper, we introduce the problem of constrained language planning, which involves generating step-by-step instructions for specific goals with multiple constraints. We evaluate and improve the constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints using InstructGPT. We find that all language models achieve unsatisfactory results on planning for specific goals, but our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. We also propose a dataset of constrained language planning, named CoScript, which can be used to train smaller but specialized models for constrained language planning. Our results show that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models.</sample>
    <sample id="182">Il tropicalismo si riferisce a un tropo che collega le donne latine alla tropicalità e alla sensualità, definendole come "vibranti" e "curvaceous". Questo tropo è stato associato a una storia di rappresentazione sessuale delle donne latine, che vengono viste come docili e sottomesse.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando un approccio che consiste in due parti principali: la generazione di personaggi e la rilevanza delle parole. La generazione di personaggi è stata resa possibile utilizzando prompt che hanno permesso di creare personaggi generalizzabili a qualsiasi demografia specificata. La seconda parte, la rilevanza delle parole, consiste nell'identificare le parole che distinguono i gruppi marcato da quelli non marcato, utilizzando il concetto sociolinguistico della "markedness". Questo approccio ha permesso di rivelare modelli specifici di stereotipi e di facilitare le narrazioni essenzialiizzanti, anche se le parole positive o non negative non sembrano essere sufficienti per catturare tali modelli.</sample>
    <sample id="184">In questo lavoro, è stato utilizzato CXMI (Contextual Mutual Information) come misura per valutare l'utilizzo del contesto in traduzioni. CXMI misura quanto informazione viene fornita dal contesto a un modello di traduzione per determinare il significato di un termine o una frase. In questo studio, è stata estesa la misura CXMI per includere P-CXMI (Pointwise Contextual Mutual Information), che misura l'utilizzo del contesto al livello delle parole o delle frasi.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato per la lingua francese specializzato in domini biomedici e clinici, mentre ChuBERT è un modello basato su dati anonimizzati ottenuti dal data warehouse del Nantes University Hospital. DrBERT è stato addestrato su NACHOS, un dataset di dati medici raccogliuti da Internet, mentre ChuBERT è stato addestrato su un insieme di 4 GB di note cliniche.</sample>
    <sample id="187">Cinque autori sono coinvolti nell'articolo: Ying, Zhiyang, e tre altri autori che non sono menzionati nel passaggio fornito.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un approccio che implica aggiornare il modello con i dati più recenti raccolti in ogni round di immissione attiva. Questo contrasta con l'approccio "cumulativo", che accumula tutti i dati raccolti finora, e l'approccio "iterativo", che aggiorna il modello solo con i dati più recenti. L'iterativo è utile per il trasferimento di immissioni da un dominio diverso, mentre le annotazioni del dominio attivo si beneficiano dell'aggiornamento cumulativo.</sample>
    <sample id="189">The goal of the AltEntities Corpus is to understand users' language when they want to make a choice, specifically in the context of conversational systems and benchmarking LLMs' entity understanding.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando un modello di estrazione di modelli (MLM) per imparare il modello da utilizzazioni precedenti. Questo è possibile se il modello non ha un meccanismo di protezione efficace, come il watermarking proposto in questo articolo.</sample>
    <sample id="191">Cinque autori sono coinvolti nell'articolo: Sara Papi, Matteo Negri, Marco Turchi, e due altri autori non menzionati.</sample>
    <sample id="192">The speaker, Yang Luo, presents a presentation on their work titled "CAME: Confidence-guided Adaptive Memory Efficient Optimization". The presentation focuses on the challenge of designing an optimizer that achieves both fast convergence and low memory usage. The speaker introduces non-negative matrix factorization (NMF) as a preliminary concept, which reduces memory requirements from O(mn) to O(m+n). They then discuss how Adafactor, a memory-efficient optimizer, presents an analytic solution for NMF but incurs erroneous updates during training deep neural networks, leading to slower convergence compared to Adam. The speaker proposes CAME, an efficient approach that decreases the side effect caused by insecure updating by calculating an instability matrix and using its square root as the denominator for mₜ to take an optimization step. Experiments on BookCorpus, English Wikipedia, BERT, GPT-2, and T5 show that CAME achieves significant improvements in validation accuracy and better performance than Adam in pre-training very large models with a huge reduction in memory cost. The presentation concludes with a comparison of memory usage among different optimizers, showing that CAME reduces memory footprint over existing SM3 memory-efficient optimizer.</sample>
    <sample id="193">Il numero di annotatori utilizzati per creare il set di dati iniziale non è specificato nella presentazione.</sample>
    <sample id="194">I ricercatori che hanno collaborato per creare l'articolo "NLPositionality" sono affiliati a Carnegie Mellon University, University of Washington e Allen Institute for AI.</sample>
    <sample id="195">The work presented is titled "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering" and aims to address the limitations of existing methods in Explainable Question Answering (XQA). The proposed framework, RoHT, consists of two stages: firstly, building a Hierarchical Question Decomposition Tree (HQDT) to understand the hierarchical compositional structure of complex questions; secondly, performing probabilistic reasoning over HQDT to fuse knowledge from different sources. RoHT outperforms existing methods on two challenging QA datasets, KQA Pro and Musique, by integrating answers of sub-questions from different levels and utilizing knowledge from both KBs and text corpora.</sample>
    <sample id="196">Un esempio in cui il governatore è a sinistra è "I saw Bart and Lisa". In questo esempio, "saw" è il governatore e il coordinamento di "Bart" e "Lisa" è a sinistra del governatore.</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli che hanno un'interazione con gli utenti in linguaggio naturale e possono comprendere e rispondere alle domande, ai problemi e alle esigenze dell'utente. Questi modelli utilizzano algoritmi di apprendimento automatico e macinatura di grandi quantità di dati per migliorare la loro capacità di comprendere e rispondere alle domande degli utenti.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici stanno diventando sempre più grandi e hanno finestre di contesto più lunghe. Questo rende cruciale valutare l'accettabilità del modello durante tutto il contesto, non solo per le frasi più brevi.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dei nove dataset. Questo fenomeno è noto come "Curse of Multilinguality".</sample>
    <sample id="200">Yes, the annotators are aware of the entities in advance.</sample>
    <sample id="201">Sono state utilizzate metriche di traduzione automatica (MT) state-of-the-art, inclusi i metri BLEURT e MQM, per la valutazione.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER. Come menzionato nel paper, i modelli che utilizzano la rete neurale con l'architettura Transformer hanno un miglioramento generale per i nuovi dati rispetto a quelli che usano altre tipologie di modelli. Inoltre, i modelli più grandi e quelli che hanno accesso a più esempi di addestramento hanno anche un miglioramento generale.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i modelli e i set di dati possono rappresentare giudizi e opinioni reali di persone, e possono riflettere positionalità specifiche. Questo può portare a differenze sistematiche nell'efficienza delle tecnologie tra le popolazioni. Studiare la posizionalità dei set di dati e dei modelli è cresciendo in importanza poiché i compiti NLP diventano sempre più soggettivi e orientati alla società, rendendosi più difficile caratterizzare come queste positionalità sono sbagliate.</sample>
    <sample id="204">I modelli di lingua multilingue come BLOOM sono stati adattati con una messa a punto integrale.</sample>
    <sample id="205">Shangbin, a PhD student at the University of Washington, presented their research on political biases in language models. They analyzed how different pretraining data sources influence the political leanings of language models and their performance on downstream tasks like hate speech and fake news detection. The study found that language models can adopt political biases from their training data, leading to potential fairness issues in NLP applications. For instance, left-leaning language models are better at detecting hate speech targeting minority groups but worse at detecting hate speech targeting more powerful groups. The researchers highlight the dilemma of sanitizing political opinions in training data without risking censorship or exclusion, emphasizing the need for addressing these fairness issues in NLP applications.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, fanno ricorso a due modelli: la classificazione di posizione non dipendente della dissonanza e la classificazione binaria di espansione e confronto (CE) dei discorsi.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono quelli che non hanno un overlapped con i dati di addestramento del modello. Questo è fatto per evitare che i test siano influenzati dai dati di addestramento e per ottenere una valutazione più accurata delle capacità del modello.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il metodo proposto ha un vantaggio rispetto al metodo di riferimento in quanto migliora la capacità di pianificazione in modo significativo sia in termini di completezza semantica che fedeltà alle vincoli. Inoltre, il nuovo metodo consente di utilizzare modelli più piccoli e specializzati per la pianificazione con vincoli, rendendo più accessibili e costosi i modelli di lingua.</sample>
    <sample id="210">Il nome della relatrice o del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nell'articolo, viene utilizzato un modello più piccolo, il T5, per testare la capacità di pianificazione con vincoli. Il modello T5 viene addestrato su un insieme di dati specifico per la pianificazione con vincoli, chiamato CoScript, e viene riscontrato che produce script di qualità superiore rispetto ai maggiori modelli.</sample>
    <sample id="213">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è OFA, un modello pre-addestrato multi-modal.</sample>
    <sample id="215">The talk by Adam Przepiórkowski focuses on the dependency structure of coordination in language, exploring different theories and approaches. The Prague approach, for instance, heads coordinate structures with conjunctions, while the multi-headed approach in Hudson's Word Grammar considers all conjuncts as heads. The talk argues for symmetric structures of coordination based on the principle of dependency length minimization. This principle suggests that shorter dependencies are preferred, which is why left conjuncts tend to be shorter when the governor is on the left or absent. The talk also presents statistics from the Penn Treebank, confirming this observation and showing that the tendency grows with the length difference between conjuncts. When the governor is on the right, this effect disappears, providing an argument against asymmetric structures of coordination and in favor of symmetric ones.</sample>
    <sample id="217">Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications are presenting their work on "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation." They address the limitations of existing methods that focus on single attributes or use specific labels for continuous attributes. Their contributions include a Disentangled Controllable Generation (DCG) model that learns attribute concepts from seen values and uses disentanglement loss to disentangle different attribute combinations. They also introduce a unified evaluation framework, MAE, for evaluating different granularities of attributes. The results show that their DCG outperforms all other baselines in attribute controllability and text equality, and they successfully tackle the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC.</sample>
    <sample id="218">Gli autori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">Jia-Huei Ju, a research assistant at Academia Sinica, presented their work on "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports." The work aims to analyze financial reports by identifying useful information that requires significant human effort. They focused on the Form 10-K, an annual report required by the SEC, which contains detailed information about companies' activities.

The study was motivated by two observations: the similarity of words in company reports (about 80% of tokens are the same) and the yearly-dependent content. To address this, they introduced a highlighting task and a multi-stage pipeline. The goal is to find the rationale (words) of relations between a given pair in the reports.

The proposed pipeline consists of four stages: document segmentation, relation recognition, out-of-domain fine-tuning, and in-domain fine-tuning. For stage 1, they classify pairs into three types: β (highest syntactic and semantic similarities), revised (similar syntactical patterns but different meanings), and mismatched (new operations or debut of information).

For model tuning, they used an external dataset (eSNLI) for out-of-domain fine-tuning and revised pairs for intermediate fine-tuning. They mixed different objectives using soft labeling techniques with cross-entropy laws and KL divergence to alleviate low-quality pseudo-labels. The evaluation dataset includes eSNLI pairs and their released FINAL dataset.

Their domain-adaptive highlighting model achieved the best performance on FINAL and preserved generalization capability. They also observed that their methods can benefit from simulation with mismatched pairs. Future works include improving effectiveness, adding more features, and enhancing information retrieval applications.</sample>
    <sample id="220">Gli autori dell'articolo sono Vasudha e altri studiosi alla Stony Brook University.</sample>
    <sample id="221">Mi dispiace, non ho accesso ai dettagli dell'articolo specificato. Potresti fornire maggiori informazioni o il link all'articolo?</sample>
    <sample id="222">This work investigates the challenges and interventions for domain adaptation in open-domain question answering. It explores different data interventions, identifies the type of dataset shift a new domain exhibits, and determines what kind of data interventions are effective for a specific type of shift. The authors use a general-purpose Wikipedia-based source domain to train both reader and retriever models and test their generalizability on seven target passage and datasets spanning across six different domains. They observe that few-shot methods improve retriever performance by 8% on average and reader performance by 11% on average. Zero-shot techniques do not have access to any examples from the target domain, but they can still control the interactions among three random variables in open-domain QA. The authors also ascertain the nature of incompatibility the target model and domain exhibit using existing data shift taxonomy in machine learning. They find that all target sets respond well to few-shot adaptations as they use a few examples from the target domain, while datasets with concept and covariate shift respond well to zero-shot adaptations as well. Overall, this work improves reader performance by up to 24% and shows that only certain types of data interventions are effective based on the type of shift a target dataset exhibits.</sample>
    <sample id="223">Il relatore è Shangbin, un dottorando all'Università di Washington.</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati due modelli: il modello long-mBART per la produzione di semplificazioni a livello di documento e il modello mBART base per la produzione di semplificazioni a livello di frase.</sample>
    <sample id="225">Per scopi di addestramento, 53 attività da 9 gruppi vengono utilizzate, mentre per i test, 10.000 istanze per attività vengono selezionate. Per i test, si riserva l'intero gruppo di ragione comune per testare e si selezionano ulteriormente 5 attività dal gruppo VQ eMiscellanea. Inoltre, 20 attività vengono casualmente selezionate dal gruppo di istruzioni naturali per testare NLP come attività sconosciuta.</sample>
    <sample id="226">Cinque autori sono coinvolti nell'articolo: Regina Stodden, Omar, e tre altri autori che non sono menzionati.</sample>
    <sample id="227">The paper discusses the challenges in grounded language understanding, where a natural language expression is mapped onto a specific target environment or plan. The main reason for these challenges is the lack of grounding during pre-training, which makes it difficult to generate valid and executable plans. The authors propose a novel framework called Pangu, which focuses on discrimination instead of generation. In this framework, a symbolic agent proposes candidate plans, while a language model scores and ranks them. This approach allows the language model to focus on discrimination, which is easier for it to excel at. The paper demonstrates that Pangu achieves outstanding performance across different settings and language models, including BERT, T5, and large language models like Codex. The authors also observe that autoregressive models tend to overfit seen structures during training, while Pangu's strong robustness under non-i.i.d setting might be due to its ability to handle both seen and unseen structures with similar probability distributions.</sample>
    <sample id="228">I ricercatori hanno effettuato i test su quattro set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">Gabriella Skitalinskaya and Henning Wachsmuth presented their joint work on detecting improvable claims for argumentative writing support. They introduced two tasks: Suboptimal-Claim detection and Claim Improvement Suggestion, which involve determining whether a claim needs revisions or can be considered optimally phrased, and selecting the types of quality issues that should be improved when revising the claim. The paper explores the challenges of working with revision-based data, such as representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors conclude that revision-based data can be effectively employed for the given tasks and that modeling the distance between two claimed versions is beneficial for detecting suboptimal claims.</sample>
    <sample id="231">NACHOS è un dataset di dati medici raccolti dalla rete, utilizzato per addestrare il modello DrBERT in lingua francese.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">The paper "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi from the University of Trento and Foundazione Bruno Kessler proposes a solution to the problems of current simultaneous speech translation (SimulST) models. The authors suggest using existing offline speech-to-text (ST) models without re-training or adopting specific architectures for SimulST. They propose EDAtt, Encoder-Decoder Attention, which decides whether to emit partial translations based on attention mechanisms between audio input and textual output. The strategy considers cross-attention weights and emits words if their sum is below a certain threshold alpha towards the last lambda speech frames. The paper compares EDAtt with popular strategies applied to offline models and state-of-the-art architecture tailored for simultaneous pre-translation, showing that it outperforms all other strategies in terms of translation quality, latency, and computational efficiency. The authors have released open-source code and models to facilitate reproducibility of their work.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati, come dimostrato in un esperimento in cui si sono fornite due diverse istruzioni per ogni frase. In questo esperimento, è stata osservata una differenza di più di un punteggio BLEURT per la maggior parte delle frasi (516 su 1000). In alcuni casi, questa differenza può arrivare anche a 40 punteggi BLEURT. Pertanto, è importante selezionare una buona strategia di prompting per ottenere i migliori risultati.</sample>
    <sample id="235">I fornitori dell'articolo non sono elencati.</sample>
    <sample id="236">Ogni compito è equipaggiato con cinque istruzioni scritte da esperti.</sample>
    <sample id="237">I ricercatori proppongono un insieme di test diagnostiche per valutare la capacità dei modelli di integrare e utilizzare informazioni provenienti da fonti diverse. Introducono un compito di risoluzione di coreferenza progettato per sottoporsi a questa capacità.</sample>
    <sample id="238">In this video, Yebowen Hu from the University of Central Florida introduces MeetingBank, a new benchmark dataset for meeting summarization. The dataset addresses two major challenges: obtaining high-quality meeting summaries and locating trustworthy resources for public meetings. MeetingBank includes 1,366 City Council meetings with nearly 7,000 instances, including meeting transcripts, reference summaries, and other useful resources. The dataset is used to evaluate various summarization systems, including extractive and neural abstractive models, using metrics like ROUGE, BERTScore, MoverScore, and human evaluation. The results show that GPT-3 achieves the highest overall scores in terms of fluency and coherence but struggles with informativeness and factuality. The video concludes by encouraging viewers to use and explore the MeetingBank dataset.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e userò un minuto per fornire un breve riassunto dell'articolo "Prompting PaLM per la traduzione: valutazione di strategie e prestazioni". Questo è un lavoro in collaborazione con i miei colleghi di Google Translate. PaLM è un modello di macchina apprendente grande con 540 miliardi di parametri presentato l'anno scorso, 2022. È stato addestrato su una vasta raccolta di testi, comprensiva di 780 miliardi di token. Al momento della pubblicazione, ha raggiunto le migliori prestazioni in molte attività NLP. In questo articolo, presentiamo lo studio più sistematico sulla capacità di traduzione dei modelli di macchine grandi tramite il prompting. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità di traduzione. Ciò include l'uso dei test set più recenti per evitare un sovrapposizione dei dati di test con i dati di addestramento del modello di macchina. Abbiamo anche confrontato i nostri risultati con i sistemi di traduzione più avanzati, cioè i sistemi con le migliori prestazioni, come quelli valutati in WMT. Utilizziamo i metri di traduzione neurali più avanzati e inoltre mostriamo anche i risultati di valutazione umana basata sull'expertise. Infine, forniamo alcune raccomandazioni sulla scelta delle strategie di prompting. Il prompting ha un grande impatto sulle prestazioni dei modelli di macchine per la traduzione, come si può vedere in un semplice esperimento dove si sono forniti due prompt diversi per ogni frase. La maggioranza delle frasi, 516 su 1000, ha registrato una differenza di più di un punteggio BLEURT. E questa differenza può arrivare, in casi estremi, fino a 40 punteggi BLEURT. Quindi, è importante selezionare una buona strategia di prompting. In nuestros experimentos, decidimos utilizar una strategia di prompting de 5 punti en la que marcamos cada frase que proporcionábamos al sistema con el idioma en que se encuentra. En este ejemplo, donde realizamos la traducción de alemán a inglés, las frases fuentes en alemán están marcadas con un punto alemán y las traducciones al inglés con un punto inglés. Vimos que la forma exacta del prompting no tiene una gran influencia en el caso de varios prompts breves. Es crucial para los prompting de 0 y 1 punto. Y cuando vamos, como en nuestro caso, a un prompting de 5 puntos, no hay diferencia notable en la forma del prompting real. Es el ejemplo que lleva la mayor parte del peso. El resumen de nuestros resultados experimentales es que la calidad de los ejemplos es más importante que la similitud con la frase fuente. Por lo tanto, es importante seleccionar los ejemplos de alta calidad. En particular, comparamos la selección de prompts desde los datos de entrenamiento para las evaluaciones de WMT en los datos de desarrollo. Los datos de desarrollo son mucho más curados y de alta calidad que los datos de entrenamiento, que son más ruidosos. Y sus resultados son un mejor desempeño cuando usamos los datos de desarrollo. Sin embargo, los sistemas de traducción especializados tienen una ventaja substantial sobre las traducciones de PaLM. Pero, PaLM se acerca bastante a un sistema comerciales. En nuestro caso, elegimos evaluar con Google Translate. Las opiniones que obtuvimos de la evaluación humana que realizamos utilizando el framework MQM dijeron que la fluidez de PaLM es comparable a los sistemas de traducción más avanzados, pero la principal diferencia radica en la precisión. En particular, los errores más comunes son los errores de omisión. Parece que PaLM opta por producir una traducción mejor de oír, a veces dejando partes de la frase fuente que son traducidas. Sin embargo, la categoría "Estilo/Incómodo" para PaLM es más baja que para los sistemas de traducción más avanzados, lo que es un señal adicional de que PaLM proporciona realmente una traducción fluida, pero aún con algunos problemas de precisión. Eso es todo para este resumen corto. Para más detalles, por favor vayan a la presentación completa del artículo. Muchas gracias.</sample>
    <sample id="240">Ciao, mi chiamo Dawei e sono un dottorando presso l'Università di Saarland in Germania. In questo video, vorrei presentare recenti lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una introduzione alla supervisione debole e all'apprendimento supervisionato debole. In supervisione debole, non etichettiamo manualmente i dati. Ciò che facciamo invece è etichettare i dati utilizzando fonti di etichettatura debole, come regole semplici, banche di conoscenza o sondaggi di pubblico di bassa qualità, come illustrato nella figura sulla destra. In confronto alle annotazioni umane, le etichette più deboli sono molto più economiche, tuttavia, sono anche rumorose, ovvero un determinato numero di etichette sono sbagliate. Se addestriamo direttamente reti neurali su etichette debole, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. In apprendimento supervisionato debole, gli algoritmi di addestramento proposti si proppongono di addestrare robustamente reti neurali sotto tale rumore di etichte in modo che i modelli addestrati continuino a generalizzare bene. In recenti opere in WSL, ovvero WSL sta per Apprendimento supervisionato debole, un comune pretesa è che gli utenti dicono che addestrano modelli solo su etichette debole e raggiungono prestazioni elevate su set di test puliti. Tecnicamente, questa affermazione non è sbagliata, ma c'è un problema, ovvero che gli utenti presumono che ci sia un set di validazione pulito aggiuntivo per la selezione del modello. Non possiamo fermarci su questo contesto problematico, ma quest'ipotesi è spesso ignorata. La dubbio sopra viene chiesto per rispondere a tre domande di ricerca. Prima, è necessario un set di validazione pulito per WSL o possiamo forse utilizzare un set di validazione rumoroso invece? Secondo, se un set di validazione pulito è necessario, o se è obbligatorio per WSL funzionare, allora quanti campioni puliti dobbiamo avere? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o esistono altre migliori modalità per utilizzarli? Abbiamo indagato queste domande e i nostri findings sono i seguenti. Troviamo che, in modo interessante, recenti metodi di WSL hanno bisogno effettivamente di campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è una grande caduta di prestazione. Come illustrato nella figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare al di fuori delle etichette originali, ovvero l'addestramento è inutile. Questo indica che i metodi di WSL effettivamente richiedono etichette di validazione pulite per funzionare correttamente e il costo di ottenere tali etichette pulite non dovrebbe essere trascurato. Il nostro secondo trovare è che aumentare il numero di campioni di validazione puliti aiuterà i metodi di WSL a ottenere prestazioni migliori, come illustrato nella figura a sinistra. Di solito, basta 20 campioni per classe per ottenere prestazioni elevate. Ma non è tutto, perché se decidiamo di accedere a campioni puliti, allora addestrare direttamente su di essi anche otterrà prestazioni migliori. La figura di destra illustra la differenza di prestazioni tra approcci di addestramento continuo, che vengono direttamente applicati sulle etichette pulite, e WSL metodi che usano i campioni puliti per validazione solo. Come possiamo vedere, se abbiamo 10 campioni per classe, l'addestramento continuo inizia a superare i metodi di WSL. Infine, le prestazioni migliorate aleggermente in precedenti metodi di WSL possono facilmente essere ottenute permettendo di continuare ad addestrare sui campioni puliti. Come possiamo vedere dalla figura, il modello vano, chiamato FTw, inizialmente sottoperfoma rispetto ai metodi più complicati di WSL, come COSINE. Tuttavia, se permettiamo di continuare ad addestrare sui campioni puliti, allora FTw ha prestazioni uguali a altri metodi. Quindi in pratica, non c'è motivo di scegliere metodi più complessi di WSL che richiedono più tempo di calcolo e spazio su disco. Per riassumere, abbiamo dimostrato che recenti metodi di WSL richiedono etichette di validazione pulite per funzionare correttamente. Le loro prestazioni migliori e praticità sono sovrastimulate. I nostri consigli concreti per future work sono i seguenti. Primo, rapportare i criteri di selezione del modello. Ad esempio, rapportare se la selezione del modello è fatta tramite campioni di validazione puliti. Secondo, metodi di WSL dovrebbero essere confrontati con baselines di imparazione a pochi campioni, poiché entrambi funzionano su campioni puliti. Terzo, addestramento continuo è un baseline semplice ma forte che dovrebbe essere considerato in future work in WSL. Infine, abbiamo aperto sorgente il nostro codice. Puoi trovare il link tramite il codice QR in questa slide. Ti prego di controllarlo. Grazie e godi della conferenza.</sample>
    <sample id="241">The paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" by Ethan, Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech addresses the limitations of current misinformation detection systems. These systems often fail to accurately evaluate their performance due to retrospective dataset construction and leaked counter-evidence, and they are not human-centric, neglecting the involvement of human content moderators. The authors propose an evaluation framework that integrates human feedback throughout the process, from raw tweet input to actionable outputs. They specifically implement and evaluate a system for detecting COVID-19 treatment misinformation, using a two-component workflow: claim detection and policy violation verification. The results show that the system effectively detects unapproved treatments before they appear in debunking news articles and has a high policy violation detection rate. The paper highlights the importance of human-in-the-loop systems in realistically capturing the interplay between systems and human content moderators, and it provides valuable insights into the development and evaluation of misinformation detection systems.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono la richiesta a giudici umani di scegliere tra due conversazioni o di valutare le conversazioni utilizzando una scala Likert.</sample>
    <sample id="243">Ci sono cinque autori coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che Servin è un giudice e Kea è un baker.</sample>
    <sample id="245">The presentation by Lining Zhang and co-authors focuses on their work titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The study aims to address the challenges of automatic metrics and poor best practices for recruitment on Amazon Mechanical Turk (MTurk). The researchers propose a two-step pipeline involving qualification settings, qualification tasks, and endurance tasks to identify high-agreement workers. The pipeline categorizes workers into gold, silver, bronze, and block based on their performance in qualification tasks, with only gold and silver workers advancing. The study also compares the results of pipeline workers with baseline MTurk workers and CloudResearch MTurk workers, analyzing correctness across different annotation sources. The findings suggest that the pipeline can achieve high agreement at a lower cost while maintaining similar quality to CloudResearch workers. However, the study is limited to English summarization on the MTurk platform and does not guarantee the training of correctness. The research has implications for future investigations into hiring high-quality workers and applying the pipeline to various tasks, languages, and platforms.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo su GitHub.</sample>
    <sample id="247">The paper presents a new task called Knowledge Graph-Based Fact Verification, which uses knowledge graphs as evidence for fact verification. The authors propose a new dataset, FactKG, that includes claims in both written and colloquial styles, and labels them as SUPPORTED or REFUTED. The task involves retrieving evidence from DBpedia and verifying the claim using the evidence. The authors also introduce five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. They construct baselines using only the claims to verify and utilizing the GEAR model to verify the claim using correct evidence. The results show that all baselines outperform the majority class baseline, and the GEAR model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico. La presentazione menziona che i ricercatori hanno optato per riconiare i set di dati con annotatori diversi per ottenere un insieme ricco di annotazioni demografiche e diverse.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate in modo da preservare la struttura rilevante, ma con l'aggiunta di rumore. Questo è stato fatto per analizzare come il modello risponde a queste frasi modificate e vedere se cambia il giudizio MPP.</sample>
    <sample id="250">La valutazione dimensionale significa valutare diversi aspetti specifici di una qualità, piuttosto che dare un giudizio generale. In questo contesto, si valuta diversi comportamenti del modello di dialogo per capire le sue caratteristiche e le sue debolezze in modo più dettagliato.</sample>
    <sample id="251">I fornitori dell'articolo sono affiliati all'Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="252">Sai Kiran Tanikella, a master's student at IIT Kanpur, presents their work on "U-CREAT: Unsupervised Case Retrieval using Events extrAcT". This research, co-authored with Abhinav Joshi, Akshat Sharma, and Ashutosh Modi, addresses the challenge of Prior Case Retrieval (PCR) in the legal domain. The team introduces the IL-PCR dataset and the U-CREAT pipeline, which leverages unsupervised learning and event-based approaches for efficient and generalizable PCR across Indian and Canadian legal systems. The U-CREAT pipeline includes an event extraction block that processes query and candidate documents to compute an interaction matrix, which is used in various retrieval models. Experiments show that event-based models, particularly the Event Filtered Documents model, outperform baseline methods and achieve state-of-the-art performance on the COLIEE data set.</sample>
    <sample id="253">Mario Ezra Aragón ha presentato il loro lavoro "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media", un progetto di ricerca con i ricercatori di Messico e Spagnolo. L'obiettivo è quello di contribuire alla deteczione dei disturbi mentali analizzando automaticamente i post sui social media. Utilizzano la tecnologia BERT per adattare il modello a una lingua specifica e a un dominio di mentalità, migliorando le prestazioni del modello. I risultati dimostrano che il modello ha un buon bilanciamento tra la precisione e la ricordabilità, superando altri metodi. In futuro, i ricercatori intendono esplorare l'applicazione di risorse lexical diverse e utilizzo di dati clinici.</sample>
    <sample id="254">The research work presented is titled "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" by Sun Qi from Nanjing University of Science and Technology. The document-level relation extraction aims to extract relations among entities in a document, which can be seen in the provided figure. Previous methods relied on large-scale human-annotated corpora, which is time-consuming and labor-intensive. Recent work leveraged distantly supervised data to pre-train the document-level relation extraction models for better performance. However, these data contain various noise levels, and current efforts to alleviate the noise problem by using pseudo labels still persist the risk of noise induction by false-positive pseudo labels.

The proposed framework introduces uncertainty estimation to determine whether model prediction can be trusted or not. An instance-level uncertainty estimation method is proposed to capture uncertainty scores for overlapping relations. A re-labeling strategy with dynamic class uncertainty thresholds and a multi-phase training strategy are also designed to further boost the performance. Monte Carlo dropout technology is introduced to model the uncertainty in pre-denoising DocRE model. The proposed framework outperforms previous baselines on public datasets.

In conclusion, the main contribution of this work includes a framework with uncertainty-guided label denoising that improves the label quality of DS data, an instance-level uncertainty estimation method for overlapping relations, an iterative re-label strategy with dynamic class uncertainty thresholds for the long-tail problem, and great performance improvements.</sample>
    <sample id="255">La forma del prompting si rivela importante in casi di zero e uno-shot prompting. In tutti gli altri casi, come nel nostro esperimento con una promptazione a 5 esempi, la qualità degli esempi è più importante del modo in cui il prompting è stato fornito.</sample>
    <sample id="257">I ricercatori hanno valutato quattro modelli di dialogo di punta.</sample>
    <sample id="258">The video introduces a new work titled "Can Large Language Models Be an Alternative to Human Evaluation?" by Chiang Cheng-Han. The research explores the use of large language models (LLMs) to evaluate the quality of text in natural language processing tasks. The study proposes using LLMs to rate samples based on given instructions, aiming to replicate human evaluation without its instability and reproducibility issues. The experiment involves rating stories generated by GPT-2 or written by humans using four attributes: grammar, coherence, likability, and relevance. The results show that while some smaller LLMs do not consistently prefer human-written texts, larger models like Davinci and ChatGPT exhibit a clear preference for human-written content, similar to human evaluators. The video also mentions additional questions addressed in the paper, such as agreement between LLM evaluations and human ratings, the impact of instruction wording changes, and the benefits and costs of using LLM evaluations compared to human evaluations.</sample>
    <sample id="259">Yusen Zhang from Penn State University presents "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations." The task involves translating queries in multiple natural languages into various meaning representations like SQL, Lambda Calculus, etc. Existing models are limited to specific languages and tasks, lacking coverage for Chinese and certain meaning representations. XSemPLR provides a uniform dataset with 9 datasets across 15 language families, 5 semantic parsing tasks, and 8 meaning representations. It evaluates six settings: Translate-Test, Monolingual Model, Monolingual Few-shot, Multilingual Model, Cross-lingual Zero-shot, and Cross-lingual Few-shot transfer. Monolingual Encoder-Decoder models outperform others, and pretraining on English boosts Few-shot performance. Multilingual language models like Codex and BLOOM are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="260">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che sono ragionevoli e fedeli alle limitazioni.</sample>
    <sample id="262">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="263">The paper presents a work on mitigating label biases for in-context learning, which is a popular paradigm for utilizing large language models. The authors identify a new type of bias called domain-label bias and propose a novel calibration method to handle all types of biases. They conduct experiments using different models on a wide range of datasets and observe that domain-context calibration improves significantly the average performance of in-context learning. The authors conclude that their proposed method can significantly improve the performance of in-context learning of large language models.</sample>
    <sample id="264">Il presentatore, Lin Wang, ha introdotto il proprio articolo "TAVT: Towards Transferable Audio-Visual Text Generation" e ha spiegato che i modelli di generazione di testo unimodale come la traduzione automatica e la descrizione di immagini hanno prosperato grazie alla pre-addestramento su grandi quantità di dati e alla capacità dei modelli. Tuttavia, per le attività di generazione di testo multimodale come la generazione di testi audiovisivi, l'annotazione dei dati è più ardua e costosa. L'articolo propone un nuovo compito chiamato Transferable Audio-Visual Text Generation per superare questa limitazione. Il framework proposto include un rete meta-mapper audio-visuale, un encoder e un modello generatore di testo linguistico, e un imparazione contrastiva counterfattuale. L'articolo ha anche presentato i risultati sperimentali che dimostrano che il proprio approccio supera i modelli esistenti in entrambi i casi di dataset e dominii differenti.</sample>
    <sample id="265">Il nome della relatrice o del relatore è Vasudha.</sample>
    <sample id="266">Mi dispiace, non ho accesso alle informazioni sulle affiliazioni degli autori dell'articolo.</sample>
    <sample id="268">I errori più comuni di PaLM sono omission errors, ovvero l'omissione di parti del testo sorgente durante la traduzione.</sample>
    <sample id="269">Ciao, mi chiamo James Finch e mi chiamo Sarah Finch. Oggi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare gli AI conversazionali. Questo lavoro è stato fatto dal Emory NLP Lab guidato dal professor Jinho Choi all'Emory University e in collaborazione con Amazon Alexa AI. Immagina che tu abbia sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di scegliere quale dei due dialoghi sia migliore o di valutare i dialoghi utilizzando una scala Likert. Questi metodi funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello in un livello più fine-granulare. Uno degli approcci è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi di confronto o scala Likert esistenti. Tuttavia, crediamo che ci sia un策略 più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio cerca di ridurre la soggettività della valutazione umana definendo esplicitamente se ogni risposta del modello esprime certi comportamenti, ad esempio rispondendo con informazioni irrelevanti o contradicendosi. Chiamiamo questo approccio annotazione dei comportamenti nel chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat suggeriti da recenti studi per influenzare la qualità del dialogo. ABC-Eval è capace di misurare le tassi con cui i modelli di chat commettono diversi errori tematici. Ad esempio, ABC-Eval misura il numero di giri in cui un modello ignora il suo interlocutore o dice qualcosa di irrelevante, si contraddice o viola conoscenze comuni o fallisce o riesce a dimostrare compassione. Per determinare cosa tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat dello stato dell'arte e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni con una scala Likert a livello di giro, valutazioni con una scala Likert a livello di dialogo e confronti a livello di dialogo pairwise. Per ciascun metodo esistente, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati della qualità del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo diversi livelli. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento ABC-Eval sono in generale più reliable delle etichette raccolte da metodi esistenti, come misurato dall'acordo tra gli annotatori su 100 conversazioni doppie annotate. Inoltre, le etichette di comportamento ABC-Eval sono più predittive della qualità complessiva del dialogo rispetto alle metriche prodotte da metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice. Ad esempio, puoi vedere come misurare la proporcione di giri in cui si verificano contrasti di se stessi e partner spiega il 5% e il 10% della qualità del dialogo, rispettivamente, mentre i punteggi di consistenza media con una scala Likert spiegano solo il 4% o meno. Infine, abbiamo controllato se ogni metrica di valutazione cattura un aspetto unico della qualità del dialogo utilizzando una regressione lineare graduale. Puoi vedere come la combinazione di tutti i metrici ABC-Eval spiega oltre il 25% della qualità del dialogo, e quando ne rimoviamo una a una, la maggior parte di loro risulti in una perdita di informazione significativa sulla qualità. D'altra parte, la combinazione di tutti i metri a livello di giro Likert spiega ben meno della qualità, e meno di questi metri hanno informazioni uniche. Questi metri di valutazione affidabili, informati e distinti ABC-Eval consentono di valutare gli AI conversazionali con un risoluzione più alta rispetto ai metodi precedenti. Puoi vedere che nei risultati del nostro esperimento che diversi sfidieri rimangono e hanno stati quantificati con precisione. Ad esempio, i bot che abbiamo testato hanno violazioni di senso comune in circa il 20% delle risposte. Producono informazioni irrelevanti in circa il 15% delle risposte, e si contraddicono o si opppongono al proprio interlocutore circa il 10% del tempo. Con il ritmo accelerato di miglioramento del campo, molte delle tassi di questi errori potrebbero vedere una diminuzione nei nuovi modelli rilasciati dalla nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivo per cercare metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzato da altri nel campo come un passo significativo in questa direzione. E ci guardiamo bene a vedere come gli AI conversazionali avanzeranno nei prossimi mesi e anni. Grazie per la visione.</sample>
    <sample id="270">The authors of the article are affiliated with the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.</sample>
    <sample id="271">In this article, CFT stands for "Continue Fine-Tuning". It refers to the process of continuing fine-tuning on clean validation samples after the initial training using weakly labeled data. This approach can achieve similar performance as more complicated WSL methods but requires less computation time and disk space.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Esplorazione multilingue basata su dati". Questo lavoro è stato realizzato in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Se la frase precedente fosse stata "Le cose potrebbero iniziare a diventare pericolose se i ministri scoprono", allora "mole" si riferisce a un spia. Ma se la frase precedente fosse stata "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un segno natalizio. Quindi, a seconda del contesto, il significato del termine cambia, e quindi anche la sua traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, solo una piccola percentuale delle traduzioni dipende dal contesto, il che rende inadeguate le misure di valutazione a livello di corpus come BLEU. E alcuni hanno suggerito di valutare i modelli su traduzioni dipendenti dal contesto, ma queste risorse supportano tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto solitamente si basano su conoscenza di dominio e curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande: quando la traduzione richiede contesto? E come bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, ci siamo concentrati sulla misurazione di quanto un termine dipende dal contesto durante la traduzione. In un lavoro precedente, abbiamo introdotto CXMI come misura per l'uso di contesto dai modelli di traduzione automatica. Questo è fatto misurando quanto informazione fornisce il contesto C al target Y, dato il font X. Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI che può misurare l'uso di contesto al livello della frase o al livello del termine. Possiamo pensare a parole che hanno un alto P-CXMI come parole che richiedono contesto per la traduzione. Ora analizziamo parole con un alto P-CXMI per cercare di trovare模式这些 parole. E eseguiamo l'analisi su transcrizioni di discorsi di TED tradotte in 14 lingue diverse. Analizziamo queste parole al livello di tag di parte di discorso che hanno un alto P-CXMI medio. Questo ci permette di trovare, ad esempio, i pronunci duali in arabo che hanno un P-CXMI relativamente alto. E questo può essere spiegato dal fatto che l'inglese non ha pronunci duali, quindi è necessario contesto per determinare se un pronome è doppio quando si traduce in arabo. E allo stesso modo, troviamo che certe lingue richiedono contesto quando si sceglie la forma appropriata del verbo. Poi guardiamo gli oggetti vocabolari che hanno un alto P-CXMI media per tutti i suoi diversi accadimenti. Questo ci aiuta a identificare casi come quello qui, in cui in cinese si è necessario contesto per tradurre nomi propri per assicurarsi di usare la stessa traduzione all'interno del documento. E allo stesso modo, troviamo che il contesto è importante per tradurre in forma appropriata. Infine, analizziamo singoli token che hanno un alto P-CXMI. Questo ci consente di identificare fenomeni che non possono realmente essere captati dal termine本身, ma piuttosto espressi nella struttura della frase, come la risoluzione dell'elusione. Ora utilizziamo i nostri findings dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per automaticamente identificare parole che appartengono al fenomeno. E chiamiamo il tagger Multilingual Discourse-Aware o MuDA tagger. Possiamo notare anche che diverse lingue hanno proporzioni diverse di questi fenomeni di discorso. Poi utilizziamo il tagger MuDA, applicandolo a un corpus parallelo che vogliamo utilizzare per la valutazione e applicando i nostri metri di traduzione di scelta sulle esempi di traduzioni dipendenti dal contesto che il tagger MuDA ha identificato. Infine, utilizziamo il benchmark che abbiamo creato insieme a altri metri per valutare diversi modelli sulle traduzioni a livello di documento. In primo luogo, quando utilizziamo metri a livello di corpus: per BLEU, trovi che i modelli senza contesto hanno le migliori prestazioni. Ma se utilizziamo COMET, i modelli con coscienza del contesto hanno le migliori prestazioni. E se utilizziamo f-medie parola, allora i modelli con e senza contesto hanno prestazioni simili. Questo di nuovo dimostra che è difficile determinare il migliore sistema di traduzione a livello di documento se utilizziamo metri a livello di corpus da sole. Utilizziamo il benchmark MuDA per valutare i modelli e trovi che i modelli con coscienza del contesto sono significativamente più accurati dei modelli che non usano contesto per certi fenomeni di discorso come formalità e coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano contesto per fenomeni come ellissi, pronunci e forma del verbo. Questo suggerisce dove dovremmo vedere maggior progresso per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark mostra che DeepL è generalmente più accurato rispetto a Google Translate per la traduzione a livello di documento. In sintesi, eseguiamo un'analisi data-driven attraverso 14 coppie lingua per identificare quando le traduzioni richiedono contesto e utilizziamo i nostri findings per costruire un benchmark per la traduzione a livello di documento che ci aiuta a identificare se i fenomeni di discorso i modelli possono gestire bene o no, e se i sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">The name of the presenter is Yusen Zhang from Penn State University.</sample>
    <sample id="276">Ananya and Vignesh present their work on "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages". They focus on five languages from two language families, Tamil and Malayalam (Dravidian) and Hindi, Marathi, and Gujarati (Indo-Aryan). They select 200 sentences randomly from the Flores dataset and generate multiple candidate translations in English using seven translation models or APIs. They collect human annotations on these samples, asking annotators to mark errors, error types, severity, and provide an overall score. They analyze the performance of various metrics, including overlap-based, embedding-based, and COMET metric variants, and fine-tune the best-performing metric, COMET, using their MQM dataset. They evaluate the zero-shot ability of IndicCOMET on unseen languages and test robustness scores on ACES Translation Accuracy Challenge Sets.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Il metodo utilizza il concetto di "markedness" in sociolinguistica, che afferma che i gruppi dominanti sono linguisticamente e socialmente "uncontrassegnati", mentre i gruppi marginalizzati sono "contrassegnati". In questo contesto, le parole contrassegnate sono那些与未标记的默认群体不同的词语。</sample>
    <sample id="279">I fornitori di informazioni non hanno specificato le affiliazioni degli autori dell'articolo.</sample>
    <sample id="280">The paper presents a novel framework called MultiEMO for emotion recognition in conversations. The framework addresses the challenges of exploiting the complementarity of multimodal information, unsatisfactory performances in minority emotion classes, and difficulty in distinguishing between semantically similar emotions. It proposes a visual feature extractor named VisExtNet that captures visual cues by integrating facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. The framework also introduces a multimodal fusion model called MultiAttn based on bidirectional multi-head cross-attention layers to integrate one modality with complementary information from the other two modalities. Finally, it introduces a Sample-Weighted Focal Contrast loss to address the difficulty of classifying minority and semantically similar emotion classes. Experimental results demonstrate that MultiEMO achieves state-of-the-art performances on two ERC benchmark datasets, MELD and IEMOCAP, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">In collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig, ho presentato un lavoro intitolato "When Does Translation Require Context? A Data-driven, Multilingual Exploration". Questo studio ha indagato quando le traduzioni richiedono contesto e come i modelli di traduzione gestiscono queste situazioni. Utilizzando il CXMI (Contextual Mutual Information), un misuratore di quanto il contesto influenzi la traduzione, abbiamo analizzato i testi di discorso TED tradotti in 14 lingue diverse. I risultati hanno rivelato che le traduzioni che richiedono contesto sono spesso associate a fenomeni di discorso specifici, come la formalità, la coesione lessicale e l'elusione. Abbiamo anche sviluppato un benchmark per la traduzione documentale che utilizza il MuDA (Multilingual Discourse-Aware) tagger per identificare queste traduzioni contextuali. I nostri esperimenti hanno dimostrato che i modelli che considerano il contesto producono traduzioni più accurate, specialmente per i fenomeni di discorso come la formalità e la coesione lessicale.</sample>
    <sample id="282">Xuekai Zhu ha presentato recentemente il nuovo lavoro "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing" al convegno ACL 2023. Questo lavoro si concentra sull'importante compito di generazione naturale, la trasferimento di stile testo non parallelo. Finora, la maggior parte degli studi hanno concentrato sul livello del token o del livello della frase, come la trasferimento di sentimento del testo o la traduzione di testi formali. La nostra ricerca rappresenta un passo avanti significativo nel trasferimento di stile a livello di storia e al livello del discorso, che è cruciale per imitare lo stile dell'autore. L'obiettivo principale è imitare le scelte linguistiche dell'autore al livello del discorso, come si può vedere in Tabella 1, in i contenuti rossi, come tecniche narrative, e inoltre, anche i stili tendono ad essere fortemente associati a temi di scrittura specifici. Questo rende difficile trasferire queste informazioni specifiche di stile a un altro stile, come il contenuto mancante in orange in Tabella 1. Questo è il problema principale del nostro testo. Come soluzione per alleviare questi problemi, proponiamo il modello di generazione chiamato StoryTrans. StoryTrans impara rappresentazioni del discorso dai testi di origine e combina questa con imbeddings dello stile imparabili per generare testi in stili di destinazione. Abbiamo anche progettato un nuovo obiettivo di addestramento per ridurre le caratteristiche stilistiche delle rappresentazioni del discorso, spingendo le rappresentazioni derivate da diversi testi più vicine nello spazio latente, e inoltre, per preservare il contenuto, suddividiamo la generazione in due fasi. In primo luogo, trasferiamo il testo di origine con le parole chiave specifiche del stile maschiate, e quindi generiamo l'intero testo incorporando queste parole chiave esplicitamente. Per quanto riguarda il framework di addestramento, suddividiamo le due fasi di addestramento. Per la prima fase, utilizziamo un framework di addestramento consiliere. Utilizziamo la perdita di ricostruzione自治 per recuperare l'input, e quindi la perdita di disentanglement viene eseguita sulle rappresentazioni delle frasi, che mira a disentanglare lo stile e i contenuti dei testi al livello della frase. La perdita di ordine delle frasi mira a captare la dipendenza tra le frasi, e infine, la perdita di classificatore di stile tenta di produrre segnali di stile per tutto il sistema. Per la seconda fase, questa fase è indipendente dal trasferimento di stile, che mira a riempire i contenuti specifici del stile e rimuovere i token maschiate. Infine, possiamo vedere l'evaluation e i dataset. Abbiamo raccolto nuovi dataset in cinese e inglese per questi nuovi compiti e abbiamo condotto esperimenti estesi per trasferire favole o storie quotidiane a stili tipici degli autori. Entranti automaticamente e valutazioni manuali confermano l'efficienza del modello e dimostrano che StoryTrans supera le linee guida in termini di controllo dello stile e preservazione del contenuto. Inoltre, la visualizzazione del stile indica che i testi di trasferimento testi di StoryTrans sono anche allineati con i testi d'oro in lo spazio dei caratteri dello stile. Infine, possiamo vedere alcuni casi in cui StyleLM inserisce molte frasi non correlate e StoryTrans può supplementare diverse frasi brevi o trame per ricomporre l'intera storia e mantenere i principali contenuti. Inoltre, StoryTrans può riscrivere la maggior parte delle frasi con lo stile di destinazione e mantenere i semantici del testo di origine. Ecco tutto il nostro lavoro e infine, i nostri dati e il codice inclusi in questo repo. Se avete domande, non esitate a inviare un'email.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è la struttura di coordinazione "Lisa, Bart, and Maggie" in cui il primo congiunto, Lisa, è la testa della struttura di coordinazione.</sample>
    <sample id="284">The speaker, Peng Tianshuo from Wuhan University, presented a paper titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction" at ACL's Main Conference. The paper proposes a fuzzy span mechanism to address the ambiguity in labeling golden span boundaries and the mismatch between transformer feature extraction and information extraction. The proposed method uses adaptive attention for span extraction decision, which models the target boundary as a continuous distribution of correct probability in a specific range. The model is trained using a combination of BCE loss and KL-divergence. Experiments on three main information extraction tasks, including named entity recognition, relationship extraction, and aspect sentiment triplet extraction, show that FSUIE achieves significant performance improvement compared to UIE-base without a fuzzy span mechanism. FSUIE also shows stronger generalization capabilities for domain-specific information and achieves state-of-the-art results on several datasets.</sample>
    <sample id="285">The video discusses the work of Mingqi Gao from Peking University on benchmarking factual error correction for dialogue summarization. The video highlights the two main types of solutions: introducing factuality-related objectives in the training or inference process and designing a Factual Error Correction model (FEC) that is independent of the summarization model. The video argues that there are flaws in the way FEC models are evaluated, which may have diverted the FEC for summarization from its original purpose. The video proposes a new taxonomy of factual errors and introduces manually annotated reference corrections to address both issues. The video also explores some factors of interest by experimenting with some FEC models in different training modes. The key findings include that training FEC models with reference summaries from dialogue summarization datasets yields the best results of unreliable factuality metrics, and introducing human-corrected summaries during the training of FEC models for dialogue summarization can improve their performance.</sample>
    <sample id="286">The name of the speaker is James Finch.</sample>
    <sample id="287">There are four authors involved in the article: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">I set di dati utilizzati per testare i fenomeni sintattici includono BLiMP, SyntaxGym e CrowS pairs.</sample>
    <sample id="290">I cinque metodi per la prima domanda di ricerca sono: FTw, COSINE, FTE, FTE+ e FTE++.</sample>
    <sample id="291">The model is evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su una vasta quantità di testi in francese.</sample>
    <sample id="295">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="296">Valerio Basile presents a collaboration between the University of Turin and Amazon Alexa on Natural Language Understanding. They developed a corpus called EPIC (English Perspectivist Irony Corpus) to study irony detection in natural language. The corpus includes 300 short conversations from social media, Reddit, and Twitter, annotated by 74 annotators for five varieties of English. They used perspective-aware models to train different models by fine-tuning a pre-trained language model on splits of the datasets. They found that perspectivist models are less uncertain and more confident of their predictions. They also observed differences in annotations based on age and geographical distribution of annotators.</sample>
    <sample id="297">Il contenuto parla di un progetto che si concentra sull'identificazione e l'analisi dei "doghissi", termi che inviano un messaggio nascosto a un gruppo in particolare. Il progetto sviluppa un glossario di più di 340 termini e simboli, inclusi i doghissi razzisti, transfobici e antisemiti. L'equipe ha anche eseguito un studio su discorsi politici storici negli Stati Uniti e ha valutato la capacità dei modelli di linguaggio di riconoscere i doghissi. I risultati hanno dimostrato che i modelli di linguaggio possono rilevare molti doghissi, specialmente quelli in注册形式， ma le prestazioni variano a seconda della specificità del prompt e della tipologia di doghissi. Infine, il progetto ha dimostrato come i doghissi possano evitare la modulazione online, rendendo più difficile la rilevazione dell'intossicazione.</sample>
    <sample id="298">I risultati dell'esperimento hanno dimostrato che il modello si deteriora con un intervallo temporale più grande tra i dati di addestramento e i dati di test. Questo ha confermato l'ipotesi che la principale causa della perdita di prestazioni è la deriva temporale.</sample>
    <sample id="299">The audio discusses the development of a new training method to improve the robustness of Natural Language Inference (NLI) models. The method, called minimax training, aims to reduce reliance on shortcuts and enhance out-of-distribution performance by focusing on under-represented "hard" examples that contradict these shortcuts. This approach uses a minimax training objective between a learner and an auxiliary model, where the learner tries to minimize NLI task loss while the auxiliary maximizes the learner's loss by generating example weights. The auxiliary is modeled using a feed-forward network, and both models are optimized alternately. The method has been evaluated on various datasets and shows improved out-of-distribution performance without sacrificing in-distribution accuracy.</sample>
    <sample id="300">Belinda ha presentato un nuovo任务名为互动口述，该任务允许用户使用语音输入和编辑文档。互动口述的特点是灵活的交错输入和编辑，不需要触发词或固定命令。该任务包括四个步骤：语音识别、语音转文本、命令提取和执行。Belinda及其团队设计了一个数据收集接口并创建了一个数据集。他们还开发了一个baseline系统来执行这些步骤。在实验中，他们测试了T5和GPT-3模型，并发现GPT-3模型更准确但速度较慢，而T5模型预测程序可以显著提高效率。</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, anche se il modello ha identificato tutti i token che appariranno nel output, non sa in quale ordine appariranno. La permutazione aiuta a determinare l'ordine corretto dei token nel output finale.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire meglio come i modelli stanno producendo i pregiustizi e per poter sviluppare strategie più efficaci per combatterli.</sample>
    <sample id="304">I input inaccettabili di coppia minima sono frasi che non seguono la grammatica corretta o che violano le stereotipi accettabili, come CrowS pairs.</sample>
    <sample id="305">In this video, Dawei presents their recent work on weakly supervised learning (WSL), a method that uses weak labeling sources instead of manually labeled data. The speaker highlights the challenges of WSL, such as noisy labels and memorization of label noise by neural networks. They address three research questions: whether clean validation data is necessary for WSL, how many clean samples are required, and whether clean samples should only be used for validation. The findings indicate that clean validation samples are necessary for WSL to work properly, and increasing the number of clean samples can improve performance. The speaker also suggests comparing WSL approaches with few-shot learning baselines and considering continuous fine-tuning as a simple yet strong baseline.</sample>
    <sample id="306">Sebastian Schuster and Najoung Kim discuss their work on entity tracking in language models. They argue that for an agent to understand a discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. The researchers aim to determine to what extent large language models can track entities. They designed a task involving boxes and objects to evaluate entity tracking abilities, where the input starts with a description of the initial contents of each box, and the task is to predict the contents of each box after multiple state-changing operations. The results show that most models simply repeat the initial state, while only text-davinci-003 exhibits non-trivial tracking. The researchers found that pre-training on code is responsible for making this capacity surface in pre-trained language models.</sample>
    <sample id="307">The authors used several metrics to evaluate their models, including named entity recognition, classification, part-of-speech tagging, and question answering. They compared their models to six baseline models: CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT, and ClinicalBERT.</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, presented her work on NLPositionality, which characterizes design biases of datasets and models. The study was conducted in collaboration with researchers from the University of Washington and the Allen Institute for AI. The presentation focused on how NLP models can have biases based on the positionality of the people who create them, such as differences in cultural contexts and demographics.

The researchers used a framework called NLPositionality to compare annotations by diverse annotators with existing datasets and models. They found that data sets and models are most aligned with English-speaking countries and people with college education. However, some populations, such as non-binary individuals, are left behind.

To address these biases, the researchers recommend keeping a record of all relevant design choices throughout the research process, conducting NLP research with a perspectivist lens, and building specialized datasets and models within specific communities.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'inter-annotatore accordo.</sample>
    <sample id="310">Wikipedia è stato scelto come dominio per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="311">I fornitori dell'articolo sono affiliati all'Universität Hamburg e all'Universität Paderborn.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo dataset di addestramento per la tuning multi-modal basata su istruzioni. Questo dataset comprende 62 diverse attività multi-modal coprendo 10 categorie diverse, derivate da 21 dataset esistenti e aperti. Ogni attività è equipaggiata con cinque istruzioni esperte scritte. Questo rende MultiInstruct un riferimento importante per gli studiosi che vogliono indagare la tuning multi-modal basata su istruzioni.</sample>
    <sample id="313">There are two authors involved in the article, James Finch and Sarah Finch.</sample>
    <sample id="314">La coordinazione binaria è una struttura grammaticale in cui due elementi, chiamati "conjunti", sono uniti da un segnaposto di coordinazione o congiunzione. Questi elementi possono essere parole, gruppi di parole o anche frasi.</sample>
    <sample id="315">Mi dispiace, ma non ho accesso a informazioni specifiche sul tempo utilizzato per i prompt in questo studio. La mia risposta è basata sul contenuto fornito e non su dettagli specifici relativi al tempo utilizzato per i prompt.</sample>
    <sample id="316">The results indicate that the T5 model fine-tuned on CoScript can generate scripts of higher quality than most large language models, suggesting that smaller models can surpass larger models when properly trained on suitable datasets. This finding has implications for developing more efficient and cost-effective language planning models.</sample>
    <sample id="317">The paper presents a new approach to information extraction, called CodeIE, which transforms the text-to-structured information extraction task into a structure-to-structure code generation task using code large language models like Codex. The authors argue that this approach better aligns with the information extraction task and results in more accurate outputs. They evaluate their method on three named entity recognition datasets and four relation extraction datasets, comparing it to traditional baseline models such as UIE and GPT-3. The results show that CodeIE significantly outperforms these baseline models, especially in terms of recall. The authors also observe that perplexity computed on text format inputs using models like T5 is generally higher than that of code format samples using models like CodeT5, and that there are many structural errors when decoding with GPT-3 and text format prompts, whereas when using Codex and code format prompts, such errors are almost non-existent.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Labrak e presenterò i nostri lavori su "DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedici e clinici". In questa presentazione, innanzitutto parliamo di modellazione del linguaggio in salute. Poi presenteremo la nostra principale contribuzione all'articolo. Introduciamo il primo modello biomedico in francese chiamato DrBERT, che è basato su RoBERTa e addestrato su NACHOS, che è un insieme di dati medicali raccogliuti dalla rete. Abbiamo anche introdotto una comparazione dei modelli con diverse impostazioni di pre-addestramento e fonti di dati. Infine, presentiamo i nostri risultati su 11 compiti di downstream biomedici e clinici in francese. Conclusamente, esporremo gli esperimenti e forniremo maggiori dettagli su come accedere a questi modelli. Da quando è stata rilasciata nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e offre enormi miglioramenti rispetto ai metodi statici e contestuali precedenti come Word2vec, fastText o altri. Da allora, questo modello è stato adattato a molti altri linguaggi, come in francese con CamemBERT, e anche in domini come biomedici con PubMedBERT e BioBERT e in clinici con ClinicalBERT, ma in gran parte in inglese. I modelli specializzati per altri linguaggi sono scarsi e spesso basati su pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva fino ad ora un modello open source per i biomedici. Così ci chiedevamo cosa sarebbe stato il fonte più appropriato per una vasta gamma di usi e quei dati raccogliuti dalla rete sono un buon sostituto per i dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro ChuBERT modello, che è basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Successivamente ci chiedevamo quanto di dati ci vuole per addestrare un modello specializzato in francese: 4 GB, 8 GB o altro? Per rispondere a questa domanda, prima addestriamo e confrontiamo quattro modelli addestrati da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione con 4 GB di insieme di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi preso da note cliniche; e una versione finale di ChuBERT con un mix di 4 GB di insieme di NACHOS e 4 GB di note cliniche. Inoltre, introduciamo tre modelli addestrati con pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT e addestrato su un insieme di 4 GB di NACHOS. Un altro anche basato su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine, uno basato sul modello di biomedici in inglese PubMedBERT, e addestrato su 4 GB di insieme di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccolgiamo i dati per compiti pubblici e privati come riconoscimento di entità, classificazione, tag di parte di parola e risoluzione di domande. Questi modelli sono confrontati con sei modelli di riferimento che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. L'evaluation evidenzia che i modelli hanno performato meglio sul compito con i dati dello stesso tipo su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati di fonti diverse sembrano essere più versatile. Abbiamo anche osservato che utilizzare più dati traduce in un migliore prestazione. Di conseguenza, l'addestramento da zero sembra ottenere prestazioni più elevate su quasi tutti i compiti. Tuttavia, i nostri esperimenti su pre-addestramento continuo utilizzando il peso e la tokenizzazione di CamemBERT addestrato su un subset di 4 GB di NACHOS hanno dimostrato risultati simili a quelli ottenuti con DrBERT 4 GB addestrato da zero. Che non è il caso per il modello basato sui pesi e tokenizer di CamemBERT, che soffrono di problemi di stabilità. Infine, come conclusione, il nostro sistema corretto ha offerto prestazioni migliori su nove dei 11 compiti di downstream e ha superato globalmente i risultati del modello generico, qui CamemBERT. Abbiamo anche osservato che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono ora disponibili su Hugging Face, e sotto la licenza MIT, e tutti i script di addestramento sono su nostra repository GitHub. Quindi grazie per questa presentazione e aspettiamo di scambiarsi informazioni durante la sessione di poster a Toronto.</sample>
    <sample id="319">Nel lavoro, vengono esaminate diverse strategie di apprendimento, tra cui la pre-addestramento da sorgente e la strategia di apprendimento continuo. In particolare, si analizza l'effetto dell'uso di più grandi quantità di dati e della fonte dei dati utilizzata per addestrare i modelli.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è inferiore a 1, come evidenziato dalla linea rossa con il coefficiente di regressione inferiore a 1. Questo indica che ogni unità di miglioramento ottenuta su CoNLL-2003 traduce in meno di una unità di miglioramento su CoNLL++, dimostrando che non c'è un eccesso di overfitting adattativo in questo contesto.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando i tipi di semplificazione, come la semplificazione lessicale, la semplificazione strutturale e livello globale di semplificazione. Inoltre, è stata analizzata la varietà di trasformazioni di semplificazione diverse in entrami i corpus DEPLAIN-apa e DEPLAIN-web.</sample>
    <sample id="322">Enrico presenterà un argomento alla conferenza ACL 23 chiamato "What does a Text Classifier Learn about Morality?" Inizierà spiegando il concetto di moralità e come è essenziale che i modelli linguistici possano comprendere e riconoscere la moralità nel linguaggio umano. Enrico userà un esempio di concetti divisivi come aborto o diritti LGBTQ per illustrare come la moralità è soggettiva e può variare da persona a persona. Enrico presenterà anche una teoria di moralità chiamata "Moral Foundation Theory" che sostiene che ci sono cinque diverse percezioni della moralità in umani, e che ogni individuo priorizza queste percezioni in modo diverso. Enrico userà un dataset di tweet per vedere se i modelli linguistici possono comprendere come la moralità viene espressa in diversi domini. Enrico ha proposto un metodo per saperlo e ha fatto un numero di esperimenti per rispondere a queste domande.</sample>
    <sample id="323">The speaker, Yujie Wang from Shanxi University in China, is presenting a paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA". The paper addresses the challenge of Commonsense QA, which requires machines to answer questions that rely on common knowledge. The proposed method, DHLK, combines knowledge from language models and knowledge bases to solve Commonsense QA by building an HKG based on multiple knowledge bases and using a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. The method also uses RoBERTa and Mask Self-Attention to encode and fuse QA contexts and entities, and dynamically removes entities with weaker relevance to the QA context based on attention weights. The method incorporates HKG path information into the QA context and gets the embedding representation of the QA context after path enhancement. The results show that the proposed method gets good results compared to other LM and HKG methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Questo è dovuto al fatto che i modelli sono addestrati su grandi quantità di dati web, tra cui notizie politiche di diverse fonti. Queste diverse opinioni politiche possono portare a potenziali problemi di giustizia in applicazioni downstream.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann e oggi vorrei presentarti un'introduzione rapida al nostro articolo su "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations". Questo è un lavoro di gruppo con i miei insegnanti, Alexander Koller e Ivan Titov. La generalizzazione compostiva può essere comprensibile come la capacità di un imparatore di gestire una ricorrenza più profonda e composizioni di frasi che non sono state viste individualmente durante l'addestramento. In contesto di parsing semantico, la testa per la generalizzazione compostiva potrebbe apparire in questo modo. Come di norma, abbiamo un insieme di istanze di addestramento. In questo caso, "La bambina ha dormito." E "Mary sapeva che la bambina ha dormito." Queste istanze sono associate a forme logiche che rappresentano aspetti fondamentali del loro significato. In contrasto con l'evaluation standard del macchinismo, il set di test non viene dalla stessa distribuzione ma contiene strutture structuralmente inesplorative. In questo esempio, il modello ha visto ricorrenze superfici durante l'addestramento e viene testato su un esempio con ricorrenze più profonde. I modelli seq2seq semplici falliscono spesso con questa specie di generalizzazione fuori dal distribution e producono output che sono separati dal input. In particolare, spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelli colorati nello esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi intendono capturare il processo di composizione che collega le istanze con le forme logiche. Questo funziona bene, ma gli alberi non sono generalmente forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e spesso un processo computazionalmente costoso. Di solito, questo implica un pre-processamento formale specifico delle forme logiche, ad esempio, per gestire simboli variabili. Ottenere gli alberi può anche implicare procedure di induzione grammatica specializzate. In questo articolo, non utilizziamo gli alberi e introduciamo un modello seq2seq nevralo che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione per la ricorrenza più profonda senza riuscire a usare gli alberi. Il nostro approccio predice l'output dal input in due passi. Prima, etichettiamo ogni token di input con un multiset unordered di token che appariranno nell'output. Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati. Quindi, nel secondo passo, utilizziamo un altro modello per prevedere una permutazione per mettere i token nella giusta ordine. Introduciamo un nuovo metodo per prevedere la permutazione che non mette alcune restrizioni dure sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo. Concettualmente, il nostro modello di permutazione funziona in questo modo: andiamo da sinistra verso destra sul output e determiniamo quale token multiset mettere in ogni posizione. Per la prima posizione di output, scegliamo semplicemente uno, come evidenziato in rosso. Poi saltiamo a un altro token multiset per determinare il token successivo in output. Determiniamo il terzo token in output in modo simile, scegliendo un token multiset successivo. Continuiamo questo processo fino a quando ogni token del primo passo è stato visitato esattamente una volta. Per dare un avamposto dei risultati sperimentali, qui compare il nostra metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di gran lunga in quanto generalizza per la ricorrenza più profonda. Alcune altre tipologie di generalizzazione strutturale rimangono molto sfidanti, tuttavia. In questo articolo, risolviamo un paio di challenge tecniche interessanti. In primo luogo, l'allegatura tra input e output non è data in dati di addestramento. Di conseguenza, per un token specifico non conosciamo quale multiset è venuto da, che rappresenta un problema per l'addestramento. Inoltre, spesso ci sono più permutazioni che sono coerenti con i dati, ma la permutazione linguisticamente corretta è latente. Lo risolviamo inducendo l'allegatura come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma introduce il problema che trovare la permutazione più alta punteggio è NP-dura. Questo è legato al problema "Viaggia per la vendita" e lo risolviamo con una relax continuo amicabile GPU che anche consente di backpropagare attraverso la soluzione e imparare le permutazioni più plausibili linguisticamente. Se vuoi imparare di più sulla nostra esperienza e su come risolviamo queste sfide, ti invito a consultare il nostro articolo o a visitare il nostro poster.</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono in contrasto l'una con l'altra. Ad esempio, se una persona dice "Sono sicuro che i sigilli possono uccidermi" e quindi compra dei sigilli dopo una riunione, questa credenza e azione sono in dissonanza.</sample>
    <sample id="327">Our work, "ManagerTower: Aggregating the Insights of Uni-modal Experts for Vision-Language Representation Learning," aims to train a smart AI system that can understand both image and text. We propose a novel VL modal architecture called ManagerTower, which adaptsively aggregates insights from pre-trained unimodal experts at different levels. Our approach outperforms many base-size models pre-trained on 4 million data and surpasses some models trained with more data or parameters.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale, come dimostrato dai risultati preliminari dell'evaluation.</sample>
    <sample id="329">The presentation introduces a research paper titled "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization" by Minghang Zheng and colleagues from Peking University. The paper focuses on developing a method to train video sentence localization models without manual annotations, addressing the limitations of existing zero-shot methods. The proposed method generates complex free-form pseudo-queries using a pre-trained image caption model, measures relevance between frames and queries, and selects high-quality pseudo-events. It also reduces label noise by weighting samples based on confidence and IoU, and refines labels through sample re-weighting. Experiments on ActivityNet Captions and Charades-STA datasets demonstrate the effectiveness of the method, outperforming other zero-shot methods in most metrics.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice o il relatore del paper "Attention as a Guide for Simultaneous Speech Translation" è Sara Papi.</sample>
    <sample id="332">I dati per il parametro di riferimento MuDa sono stati tratti da traduzioni parallele.</sample>
    <sample id="333">Wenhao from Nanjing University introduces their work on neural machine translation, specifically focusing on the problem of non-smooth representation spaces in NMT models. They propose a framework called INK to inject kNN knowledge into NMT, which involves adjusting representations using three kinds of alignment and refreshing the datastore asynchronously. Experiments show that INK outperforms state-of-the-art kNN-MT systems and achieves higher BLEU scores with less memory space and faster inference speed.</sample>
    <sample id="335">Il relatore del paper è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di adattare un modello di intelligenza artificiale ad un nuovo linguaggio senza dover ricorrere a un nuovo addestramento. In questo contesto, il trasferimento interlinguistico si riferisce alla capacità di un modello di tradurre query in diverse lingue e tradurle in rappresentazioni semantiche multiple.</sample>
    <sample id="337">The presentation introduces a research paper titled "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning." The paper addresses the challenge of representing out-of-vocabulary (OOV) words, which are critical for the performance of embedding-based downstream models. The authors propose a Word Relationship Graph that imitates lexical rules of word formation and association to infer the meaning of OOV words. They use a two-level graph structure, where each word or wordpiece acts as a node, and its corresponding word embedding serves as the node attribute. To address noise from wordpiece neighbors, they employ a graph neural network with a self-attention mechanism and a readout block layer. The model is trained using contrastive learning with NT-XENT positive samples. Experiments demonstrate superior performance in both intrinsic and extrinsic tasks compared to baselines. The model can be applied to static and contextual models in downstream tasks. The presentation also discusses the potential of extending the model to other languages, particularly agglutinative languages, but acknowledges challenges with fusional languages.</sample>
    <sample id="338">Bingsheng presented a research paper titled "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations." The paper, a collaborative work from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research, aims to address the challenge of evaluating the quality of human-annotated explanations. Traditional metrics like BLEU and ROUGE focus on word similarity, while simulatability scores measure baselines performance change when explanations are present or absent. However, these methods neglect task differences and utility during fine-tuning and inference stages.

The research introduces a template-based unified data format for various tasks, including commonsense QA, natural language inference, and commonsense validation. It conducts in-depth experiments with five large-scale datasets, comparing baseline and infusion settings where explanations serve as additional input. Observations include that fine-tuning does not necessarily convey new knowledge, but it teaches models to rely on explanation parts. CoS-E explanations are less helpful than ECQA ones on baseline models, highlighting task-dependent nature of explanations.

A novel evaluation metric called TREU (Task-Relevant Explanation Utility) is proposed, extending the simulatability score by evaluating the helpfulness of explanations at fine-tuning. TREU scores reflect the intuition that human-annotated explanations can still benefit model predictions even if considered low quality. The metric outperforms simulatability scores in evaluating dataset qualities across T5 and BART models. The study supports the hypothesis that the helpfulness of human explanations depends on task and explanation format, such as negation connotation and counterfactual writing styles.

In conclusion, the research proposes a unified data structure, analyzes factors contributing to explanation utility, and introduces a metric that outperforms traditional methods. This work lays the foundation for high-quality human collaboration in annotation jobs and recommends similar quality checks for future researchers.</sample>
    <sample id="339">I fornitori dell'articolo sono il dipartimento di Informatica dell'Università di Saarland in Germania, il Laboratorio di Intelligenza Artificiale dell'Universita di Pisa in Italia e il dipartimento di Informatica dell'Università di Brescia in Italia.</sample>
    <sample id="340">The presentation introduces ParaAMR, a large-scale dataset for paraphrase generation that leverages AMR (Abstract Meaning Representations) back-translation. Unlike existing datasets, ParaAMR offers syntactically diverse paraphrases while maintaining good semantic similarity. The dataset consists of around 15 million source sentences with approximately 6.9 paraphrases per sentence. Quantitative analysis shows higher syntactic diversity scores compared to other back-translation datasets. ParaAMR benefits various NLP applications, including learning sentence embeddings, syntactic control paraphrase generation, and few-shot learning data augmentation.</sample>
    <sample id="341">I ricercatori usano due misure di latenza: l'average lagging e la computazionale aware average lagging. L'average lagging misura il ritardo medio nella traduzione, mentre la computazionale aware average lagging tiene conto del tempo necessario per prevedere l'output del modello.</sample>
    <sample id="342">Hello everyone. My name is Gao Jingsheng. Today I'm going to present our paper, "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming". This paper conducted by me, Lian Yixin, Zhou Ziyi, Fu Yuzhuo, and Wang Baoyuan from Shanghai Jiao Tong University and Xiaobing.AI. Here is the outline of my presentation. First part is introduction. What is the Open Domain Dialogue? It means a type of conversational exchange between a human and an artificial intelligence system that can cover a range of topics and doesn't have a specific goal, which relies on pre-trained models and larg-scale datasets. The existing large-scale corpora mainly consists of online chat conversations. We can actually divide them into text sources and video sources based on their sources. Currently, the large-scale pre-trained dialogue datasets are mostly text-sourced. Therefore, it is of great significance to construct a large-scale video-sourced dialogue dataset that is closer to real spoken conversation. Existing video-sourced dialogue datasets can be cut up into two groups. Those with scripted conditions as TV and movie scripts, and those without scripts, such as interview datasets. However, these datasets are limited in scale as they rely on manual annotations and instructions. To construct a large-scale dialogue dataset, the key lies in finding an effective matching mechanics that captures the reply-to relationships among speakers. Furthermore, in addition to general open-domain dialogue, personalized dialogue is crucial in developing applications such as virtual streamers and virtual employees. However, the current research on personalized dialogue faces challenges such as utilizing persona information to represent the characteristics, and the lack of session dialogue for each persona. Moreover, [INAUDIBLE 2:05] a multi-party dialogue scenario in which there are more than two interlocutors involved in the conversation. For research on multi-party conversation, the lack of large-scale Chinese multi-party dialogue has been a challenge, and our datasets can help address this issue. In total, the key barriers of existing dialogue datasets are summarized below. To address these barriers, we propose a large-scale personalized dialogue dataset, LiveChat, with a unique automatic dialogue-constructing method. We conduct sufficient experiments on two benchmark tasks: Response Modeling and Addressee Recognition. We further investigate transfer learning of generation models to LiveChat. The second part is our dataset, LiveChat, which is conducted in three steps. Firstly, we scratch origin streaming videos from Chinese TikTok, Douyin. Then we extract audio from videos and transcribe audio into utterances through ASR. Secondly, we collect audience comments and construct dialogues by our reply-to-whom matching method. Thirdly, we collect the persona information for personalized dialogue generation. The persona extraction of LiveChat can be categorized into two parts. The first one are basic profiles by manual labeling and scratching. The second one is [INAUDIBLE 3:38] profiles. We extract them by rules and trained persona classifiers. This is the comparision between our dataset and other existing open-domain dialogue datasets. We can see that our LiveChat is video-sourced with a larger scale. Besides LiveChat and videos [INAUDIBLE 4:01] personal annotations and the longest average sessions. The third part is our experiments. We train retrieval baselines for two tasks. The first task is response modelling. We can conclude that our extracted persona and longer average sessions are persona beneficial to the final result. Besides, both rules and classifier are important to persona extraction. The second task is Addressee Recognition, which shows that single-stream BERT outperforms double-stream BERT, although persona is beneficial to address recognition. Except the two prompt tasks, we also investigate the performance of the pre-trained dialogue models on our LiveChat. Firstly, the performance of BART is better than the other two. It confirms that the domain of our LiveChat is far away from the domains of existing dialogue datasets. The human of evaluation result of LLMs are better in terms of rich informativeness. We also have performed a series of experiments of in-context learning on different shots to study the influence of demonstrations. The performances keep growing as the shots gradually increase. However, when the number of demonstrations exceeds 8shots, the performances of the LLMs slightly decrease due to the random manual selection of demonstrations, which mainly introduce noise. In conclusion, we propose LiveChat, a Chinese video-sourced and personalized dialogue dataset. Experiment on two benchmark tasks show that selected persona profiles and the average sessions per persona are advantageous in learning the speaker’s personalized response. The comparisons between BART and other LLMs have unveiled the distinctiveness of our LiveChat. In the future, we will pay more attention to the efficient transfer learning of LLMs for LiveChat. My presentation is over. Thanks for listening.</sample>
    <sample id="343">Ciao a tutti, mi chiamo Akshatha e oggi io e il mio collaboratore Martin stiamo presentando il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Questo lavoro è un合作项目，由蒙特利尔大学、Mila和微软研究院共同完成。自然语言理解模型利用各种知识来源，包括在预训练中获得的知识参数和在推理过程中提供的输入知识。最近的研究表明，模型可以使用预训练时间的知识来解决任务。但在自然语言理解中，通常需要在推理过程中提供的知识。例如，在句子“John vide il nuovo presidente su TV”中，预训练参数可以包含关于总统和电视的信息，但不能可靠地知道这个实例化的实体“John”是谁，或者新总统是谁，因为总统自预训练以来可能已经改变了。因此，成功处理知识密集型NLU任务的模型需要能够整合和利用预训练时间和推理时间的知识。在这项工作中，我们提出了一个诊断测试套件，用于评估知识整合能力。我们引入了一个核心参考任务，旨在测试模型是否能够调用不同来源中的知识。我们用人类参与者和已建立的核心参考模型评估了数据集。以下是我们数据集中的一个示例。Servin是一名法官。Kea是一名面包师。Servin和Kea在公园里见过面。在一天的工作结束后，他很高兴在法庭上决定案件，他很开心能放松一下。任务是确定代词“他”所指的正确实体，即Servin。解决给定代词需要两种信息：第一种是实体特定知识，如“Servin是一名法官。”第二种是背景知识，如“法官在法庭上决定案件。”一般来说，背景知识是在大规模语言模型预训练期间学习的，而实体特定知识通常是在推理过程中观察到的。我们通过改变这些两种信息的可获得性来定义三个KITMUS设置。首先，我们有典型的设置：“背景-预训练”，其中假设背景知识在预训练时可用。其次，有一个“背景-两者”设置，其中在预训练时间和推理时间中都提供背景知识。最后，“背景-推理”设置，其中在推理时间中同时提供两种知识类型。这个最后的设置特别有趣，因为它模拟了背景知识对于解决任务来说不是预训练数据的一部分的情况。例如，因为自预训练以来已经出现了新的职业。以下是我们控制数据集中事实可获得性的示例。在“背景-预训练”设置中，我们假设背景知识“政治家寻求当选政府职位”包含在预训练参数中，并在推理时间的上下文中提供实体特定知识“Chichester是一名政治家”。在“背景-两者”设置中，我们不仅提供实体特定知识，还在推理时间的上下文中提供政治家的背景知识。在“背景-推理”设置中，我们提供虚构的职业“mirituer”而不是政治家，因为“mirituer”很可能不会包含在预训练参数中。我们用人类参与者和已建立的核心参考模型评估了数据集。在图中，我们展示了在最困难的“背景-预训练”设置的变体上，最佳性能模型的结果。没有针对KITMUS进行特定任务训练，两个模型的表现都不好。然而，当它们被训练在KITMUS上时，两个C2F和BERT4Coref模型的表现显著优于随机选择。这表明，当在通用引用分辨率数据集上进行一般性训练时，大多数模型学会利用表面线索，而这些线索在KITMUS中被移除时并不太有用。额外实验显示，即使表现最好的模型也不能可靠地整合仅在推理时间提供的反向知识。总之，我们的论文的主要 takeaway是许多核心参考分辨率模型无法在没有特定任务培训的情况下推理来自不同来源的知识。然而，经过特定任务培训后，有些模型成功地整合了多种来源的知识。尽管如此，即使表现最好的模型似乎在可靠地整合仅在推理时间提供的反向知识方面存在困难。如果您想了解更多信息，请查看我们的论文，并在GitHub上检查数据集和代码。谢谢您的收听。</sample>
    <sample id="344">I metodi basati su alberi hanno diversi svantaggi. Ad esempio, integrare gli alberi nel modello può essere complesso e computazionalmente costoso. Inoltre, è necessario pre-processare formalmente le forme logiche per gestire simboli variabili e applicare procedure di induzione grammatica specializzate. Questi processi possono essere complicati e richiedono un'attenta preparazione.</sample>
    <sample id="345">The paper "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" by Matthias Lindemann, Alexander Koller, and Ivan Titov introduces a neural seq2seq model that directly models the correspondences between fragments of input and output. The model predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output; second, it uses another model to predict a permutation to put them into the right order. The approach is flexible and expressive, and it outperforms other treeless models on the COGS benchmark for generalization to deeper recursion. However, some kinds of structural generalization remain challenging. The paper addresses technical challenges such as inducing alignment as part of training and approximating the NP-hard problem of finding the highest-scoring permutation with a GPU-friendly continuous relaxation that allows backpropagation through the solution.</sample>
    <sample id="346">I fornitori di informazioni non hanno elencato le affiliazioni degli autori dell'articolo.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parlerò del mio articolo "Personaggi etichettati: Utilizzo di prompt di lingua naturale per misurare i pregiudizi in modelli di linguistica". Questo lavoro è stato fatto in collaborazione con Esin Durmus e Dan Jurafsky. In ultimi anni, molti hanno documentato la presenza di pregiudizi sociali e stereotipi in grandi modelli di linguistica, o LLMs. Tuttavia, questi metodi hanno diversi limiti. Sono solitamente basati su insiemi di dati costruiti a mano che richiedono molto tempo per curare e generalmente misurano solo pregiudizi specifici, il che significa che non si generalizzano bene a altre demografie o contesti, o semplicemente catturano associazioni negative molto generiche con particolari gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersectionalità, che è la nozione che identità sociali multi-faccettate possano essere luoghi unici di danno. Per superare questi limiti, ci basiamo sul fatto che questi nuovi modelli addestrati con istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare un personaggio, che è una rappresentazione immaginaria di un individuo utilizzando un prompt come "Immagina che tu sia una donna asiatica. Descrivi te stesso". E possiamo vedere immediatamente che questa è molto generalizzabile a qualsiasi demografia poiché possiamo specificare qualsiasi segnaposto di identità che vogliamo in questo prompt. Ecco alcuni esempi generati da GPT-4. Subito dopo, notiamo che, anche se gli output non sono negativi o tossici nel senso tradizionale di queste parole, ci sono dei modelli interessanti. La donna asiatica è descritta come modesta; la donna del Medio Oriente viene riferita come "esotica" e "come", riferendosi a una regione mesmerizzante. E entrambe le donne di colore fanno riferimento all'antonomasa mentre il persona bianca ha niente di tutto. Per captare questi modelli, il nostro metodo ha due parti. La prima è generare questi personaggi. I nostri prompt per generare questi personaggi sono ispirati a un studio in cui venivano forniti questi prompt a soggetti umani, scoprendo che fornirli a soggetti umani permetteva di estrarre stereotipi razziali. E anche questa abilità consente una comparazione diretta tra i personaggi generati e le risposte umane scritte. La seconda parte è il termine "parole etichettate", che è un metodo per identificare le parole che distinguono gruppi etichettati da gruppi non etichettati, che spiegherò subito. Il vantaggio di questo è che otteniamo stereotipi e modelli molto specifici senza dover reliquarti a un lexicon specifico. Così, il metodo "Parole Etichettate" si basa sul concetto sociolinguistico di "marcatura", che afferma che c'è un marcato predefinito e qualsiasi gruppo che differisce da quello predefinito è linguisticamente marcato. Quindi, ad esempio, la parola "guerriero" è usualmente associata ai maschi. Quand descrivono un guerriero che è una donna, devono specificare "guerriero donna" e marcare la parola con "donna". E più in generale, i gruppi dominanti in società sono sia linguisticamente che socialmente non marcato, mentre i gruppi marginalizzati sono generalmente marcato. Quindi, nel nostro metodo, innanzitutto designiamo cosa siano i gruppi non marcato e i gruppi marcato, e poi confrontiamo i personaggi utilizzando il metodo "Fighting Words", che è basically utilizzare le proporzioni logaritmiche ponderate per distinguere le parole principali per ciascun gruppo marcato. Quindi, per esempio, per i personaggi di donne nere, faremmo Fighting Words e confrontiamo le proporzioni logaritmiche contro sia le persone bianche che i maschi perché sono i due gruppi corrispondenti non marcato. Adesso per i risultati. Quindi, prima utilizziamo un lexicon di stereotipi e troppo che i personaggi generati contengono molto più stereotipi rispetto alle risposte umane scritte. Tuttavia, quando analizziamo la distribuzione delle parole e del lexicon, troppo diverse cose. Quindi, anche se i personaggi generati hanno tassi molto più alti delle parole del lexicon, le risposte umane scritte hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate presenti nei personaggi generati sono davvero solo le parole "alta" e "sportiva". Quindi, in effetti, questo lexicon non cattura molte delle patterns dannose che avevamo visto in precedenza. Quindi invece di farlo, useremo i risultati dal nostro metodo "Parole Etichettate" per dimostrare come queste parole positive apparentemente facilitano i stereotipi e i racconti essenzializzanti. In nostra analisi, riveliamo come queste rappresentazioni positive apparentemente positive riflettono pattern dannosi. Prima, dalla nostra analisi, le parole più importanti includono cose come "cultura", "tradizione", "orgoglio" e "esotico". E queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normale bianco. Questo contribuisce a una lunga storia di discriminazione e alienazione per questi gruppi. Inoltre, c'è un sacco di troppe che sono riflette in queste parole,特别对于女性有色人种。比方说，描述 Latina 的话包括诸如“充满活力”和“丰腴”等词，这与热带主义的 tropes 连接在一起。对于亚洲女性，这些话是诸如“娇小”、“柔弱”和“光滑”等词，这与长期存在的亚洲女性被性化、被视为顺从和驯服的历史联系在一起。最后，对于黑人女性，我们看到一些顶部的词是诸如“坚强”和“ resilient”等词。这些词与被称为“坚强的黑女人”原型联系在一起。虽然乍一看听起来很积极，但实际上，这种原型实际上是有害的，因为它给这些 demographic 带来了很大的压力，要坚强和坚强地面对社会障碍。因此，与其克服这些障碍，它让人们去克服它们，导致这些人的负面健康结果等其他伤害。更广泛地说，每个标记组别的单词几乎都反映了非常本质化的叙事。基于这些模式，我们得出三个关于模型所有者的建议。首先，作为研究员，我们应该解决积极的 stereotype 和本质化叙事。我们也应该使用交叉学科视角来研究偏见和伤害，因为有很多东西可能会被忽视如果我们不这样做。最后，应该增加对偏见缓解方法的透明度，因为例如这些积极 stereotype，我们不知道是因为某种过度的价值观对齐而导致这些有害的图案，或者可能是某种其他反 stereotype 方法而导致这些图案，但我们不能假设或进一步研究这些，除非有更多透明度。谢谢大家收听。祝 ACL 玩得开心。</sample>
    <sample id="348">This paper presents a method to measure stereotypes in language models (LMs) using natural language prompts and marked words. The authors argue that current methods have limitations, such as relying on hand-constructed data sets and only measuring specific stereotypes. They propose generating personas using prompts like "Imagine you are an Asian woman. Describe yourself." to capture patterns in LMs' outputs. The Marked Words method identifies words that distinguish marked groups from unmarked ones, revealing harmful patterns in seemingly positive portrayals. The results show that generated personas contain more stereotypes than human-written ones, but the distribution of words is different. The paper concludes with recommendations for model owners, including addressing positive stereotypes, using an intersectional lens, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi e sono dell'Università di Scienze e Tecnologie di Cina. Sono felice di presentare un video pubblicitario corto del mio articolo. Stanno copiando il mio modello? Proteggere i diritti d'autorità dei modelli di lingua grandi per fornire servizi tramite segnalini nascosti. Innanzitutto introduciamo lo sfondo riguardante la fornitura di servizi. Attualmente, i modelli di lingua grandi come GPT, LLAMA e PALM sono eccezionali nella comprensione e nella generazione di linguaggi naturali. La fornitura di servizi di embedding è uno dei servizi costruiti su modelli di lingua grandi per assistere a diverse attività NLP. Ad esempio, OpenAI offre un API di embedding basata su GPT. Tuttavia, recenti opere hanno dimostrato che l'attaccante può rubare il modello attraverso imparare da embedding e fornire servizi simili. Di conseguenza, è necessario proteggere i diritti d'autorità dei servizi di embedding. Per proteggere i diritti d'autorità dei servizi di embedding, una delle soluzioni è di immergere un segnalino nascosto nel servizio fornito e rilevare se un altro servizio contiene il segnalino. Metodi di segnalino devono soddisfare le seguenti proprietà: prima, il metodo dovrebbe essere applicabile a embedding as services. Secondo, il segnalino non dovrebbe indebolire l'utility dei embeddings forniti. Terzo, il segnalino dovrebbe essere abbastanza nascosto per gli attacchanti o gli attacchanti possono facilmente rimuovere il segnalino. Infine, il segnalino dovrebbe essere trasferibile al servizio dell'attaccante durante il processo di estrazione del modello. Opere esistenti possono essere generalmente classificate in quattro类别. Tuttavia, questo metodo non è applicabile a embedding as services o manca di trasferibilità. Pertanto, in questo articolo proponiamo il segnalino di embedding, che è un metodo di segnalino basato sul backdoor applicabile a embedding as services. Introduciamo quindi i dettagli del segnalino di embedding. Il segnalino di embedding contiene due passi principali: iniezione di segnalino e verifiche di copyright. Prima di questi passi principali, dobbiamo selezionare un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Ci assumiamo che il fornitore possa raccolta un corpus di testo generale e conteggia la frequenza delle parole con esso. In iniezione di segnalino, definiamo un embedding bersaglio. Quando un utente invia una frase al servizio fornitore, il fornitore conteggia il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding bersaglio e l'embedding originale. Il peso dell'embedding bersaglio è proporzionale al numero di trigger nella frase. Se il numero di triggers nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding bersaglio. Verifica di copyright è per rilevare se un modello dietro un altro servizio contiene il segnalino di parola. Innanzitutto costruiamo un backdoor e un insieme di dati benigni. L'insieme di dati di backdoor contiene frasi di cui tutti i vocaboli appartengono all'insieme di trigger mentre tutti i vocaboli nelle frasi di insieme di dati benigni non appartenenti all'insieme di trigger. Poi il fornitore richiede gli embeddings dal servizio ladro con il dataset. La similarità di coseno e L2 tra gli embeddings richiesti e l'embedding bersaglio vengono calcolate. Simultaneamente applichiamo il test KS e utilizziamo il suo p-value come il terzo metrico. Abbiamo condotto esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Ci assumiamo che il fornitore applichi il dataset Wiki Text per conteggio frequenza delle parole. I risultati sui quattro dataset hanno dimostrato che il segnalino di embedding marker può avere un'ottima prestazione di rilevamento mentre mantiene grande utilità per compiti downstream. Abbiamo anche validato la copertura del segnalino fornito visualizzando gli embeddings delle frasi su quattro dataset. La legenda delle figure significa il numero di trigger in ogni frase. Come si possono vedere nelle figure, è difficile distinguere tra gli embeddings di backdoor e gli embeddings normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">In this presentation, Simone Tedeschi and colleagues discuss the evaluation of NLP models using leaderboard-based methods. They argue that these methods can lead to misleading conclusions about a model's performance compared to humans. The researchers analyze SuperGLUE and SQuAD benchmarks, which are popular in NLP, and find that systems often outperform humans on these benchmarks. However, they identify several issues with the benchmarks, such as differences in test set sizes, errors in ground-truth answers, and varying pay rates for human annotators. These issues can result in unfair comparisons between humans and systems. The researchers recommend that more reliable benchmarks should be constructed to avoid repeating these mistakes.</sample>
    <sample id="351">The paper "Do CoNLL-2003 named entity taggers still work well in 2023?" by Shuheng investigates the generalization problem of NER models developed using the CoNLL-2003 dataset. The authors created the CoNLL++ dataset, which consists of news articles from 2020 annotated with the same guidelines as CoNLL-2003. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-03 test sets and CoNLL++. The results showed that transformer models, larger model sizes, and more fine-tuning examples are necessary for good generalization. The main cause of performance drop was temporal drift, not adaptive overfitting. The paper concludes that CoNLL-2003 taggers still work well in 2023 and calls for further research on improving model generalizations.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio dimensionale per valutare gli AI conversazionali. Consente di valutare diversi aspetti della qualità del chat, come la relevantità delle risposte, le contradizioni e le violazioni della conoscenza comune, in modo più preciso e affidabile rispetto ai metodi umani tradizionali.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation and program synthesis. The authors propose a method to create CodeClarQA, a synthetic dataset with clarifications on key operations, and a pipeline of code generation by asking clarification questions. They identify key operations and corresponding documentation from the code, represent them in latent space using their schemata, and compute similarity scores of all schema element pairs between an NLD and the operation documentation. If all element pairs for the similarity score is lower than threshold T, the key operation is missing; otherwise, it is aligned. They also hire annotators to annotate the validation set and the test set. They adopt templates to create CQAs for missing key operations and generate two types of questions: yes-or-no questions or multiple-choice questions. They use heuristics to extract key operations based on the code knowledge graph generated by Graph4Code. They also perform error analysis and test their pipeline. They find that clarified key operations are the reason for better generated code and that training with Oracle CQAs leads to predictions close to the ground truth with only minor differences.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2019.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono un candidato per il dottorato in Informatica alla Stony Brook University. Vorrei presentare il nostro lavoro accettato all'ACL 2023 come articolo lungo, "Transfer Learning per la rilevazione della dissonanza: affrontando il problema della classe rara". Iniziamo definendo la dissonanza cognitiva e perché è importante studiare questo problema nella lingua. In parole semplici, la dissonanza cognitiva è quando due credenze o azioni sono in contrasto, ad esempio, questa persona dice "Sono consapevole che i sigilli potrebbero uccidermi" e quindi dice "Ho preso una pausa dopo la riunione". Questa credenza e azione sono in dissonanza. Ulteriormente, menziona "Non penso di poter tenere mio lavoro senza loro" che giustifica la seconda occorrenza. Queste credenze hanno una relazione consonante. Mentre la dissonanza è un fenomeno molto comune che esperiamo in decision-making quotidiano, è davvero raro trovare espressioni di dissonanza in linguaggio tra altre relazioni discorsive. Quel che importa? Studiare la dissonanza espressa in linguaggio può aiutarci a comprendere gli effetti della discordia tra le persone, tracciare tendenze e valori di credenze, e capire i cambiamenti di atteggiamento in una popolazione. Una alta dissonanza cognitiva è anche legata ad disturbi ansiosi e può aiutarci a comprendere meglio lo stato mentale delle persone. Studiare la dissonanza espressa in linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza è importante per comprendere i modelli cognitivi individuali e aiuta a comprendere meglio i processi decisionali. Con lo scopo di creare un risorsa sulla dissonanza, abbiamo condotto un'annotazione a larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza prima, come vedere nel diagramma qui. I tweet erano passati utilizzando il parser PDTB e le coppie di unità discorsive erano annotate in base alle linee guida descritte nel nostro articolo. Come può vedere qui, la dissonanza è stata individuata solo in 3,5% delle coppie annotate. Dopo la raccolta di circa 1000 esempi di coppie di unità discorsive, si è eseguito un training per un classificatore iniziale addeeso solo a 43 esempi di dissonanza. Non sorprende che il classificatore non abbia performato molto meglio del caso. Data la bassa occorrenza di dissonanza e la mancanza di qualsiasi altro dataset precedente, stiamo affrontando il problema della rareità assoluta. Per alleviare questo, esperimentiamo su combinazioni di transfer learning e imparazione attiva per annotare in modo che possiamo raccogliere più esempi di dissonanza con meno annotazioni, riducendo i costi di annotazione in generale mentre miglioriamo la deteczione della dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza affatto, iniziamo il processo di imparazione attiva trasferendo i pesi da compiti correlati. Trasferiamo da due compiti diversi: la classificazione dell'orientamento della posizione di dissonanza indipendente del tema, un compito che determina se due dichiarazioni di dibattito provenienti da diverse persone sono in accordo o in disaccordo, indipendentemente dal tema, chiamato dibattito qui, e la classificazione binaria di classi di espansione e di confronto del PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e chiamiamo CE qui. Troiamo che il trasferimento del prest表在标注数据集上的零-shot性能已经比随机高得多，最佳的AUC为0.62。进一步，通过迭代微调CE任务，然后进一步微调debate任务，我们发现CE任务的进一步微调后，零-shot性能更好。因此，这是我们在冷启动imparazione attiva utilizzando. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di imparazione attiva e annotazione. "Cumulative" accumula tutti i dati raccolti fino ad ora da annotazione attiva, mentre "Iterative" aggiorna il modello addezzando i recenti set di dati raccolti. Di fronte a diverse strategie, abbiamo scoperto che Cumulative ha prestazioni uguali o migliori di Iterative in tutto. Infine, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di imparazione attiva per la classe rara — PRC — per selezionare principalmente gli esempi che sono più probabili di essere classificati come dissonanza dalla nostra modello in ogni round di rare class. Confrontiamo questa con le altre strategie di imparazione attiva più comuni nella comunità. Troiamo che la nostra strategia proposta PRC funziona meglio delle altre strategie di imparazione attiva più avanzate, anche se la differenza è piccola. Notiamo che le prestazioni sono significativamente più basse per random. In successive rondine di AL con due migliori strategie, miglioriamo l'AUC di classificazione di dissonanza a 0.75, che è la migliore prestazione che abbiamo ottenuto finora sul compito. Abbiamo anche controllato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Troiamo che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori ritrovano gli esempi difficili. In sintesi, trovi che PRC è una strategia di imparazione attiva semplice per la raccolta di esempi rari e di iniziare l'imparazione attiva con un compito di transfer learning adeguatamente progettato e che aiuta notevolmente. Abbiamo anche scoperto che l'aggiornamento iterativo è utile per il transfer learning da un dominio diverso, mentre in annotazioni attive nel dominio si beneficiano di aggiornamento cumulativo. Questi sono i link al nostro dataset principale e al nostro articolo. Sentiti liberi di contattarci se hai domande. Grazie.</sample>
    <sample id="356">I fornitori di informazione non hanno accesso alle informazioni personali o alla storia di un utente specifico. Tuttavia, in base al contenuto inglese fornito, i fornitori dell'articolo sono Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="357">Il nome della relatrice o del relatore è Siyu Yuan.</sample>
    <sample id="358">Ci sono cinque autori coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L'approccio proposto viene confrontato con la migliore architettura dedicata per traduzioni pre-simultanee.</sample>
    <sample id="361">Armineh Nourbakhsh, a PhD student at Carnegie Mellon University's Language Technologies Institute and research director at JP Morgan AI Research, presented her work "CounterComp" which aims to enhance compositional generalization for multi-step quantitative reasoning using counterfactual scenarios. The study focuses on the question-answering task with financial tables, where models struggle with tasks involving more than two arithmetic operations due to memorizing spurious patterns. CounterComp introduces an auxiliary metric learning loss with a dynamic margin to encourage meaningful token attention during training, improving performance on both in-distribution and out-of-distribution samples. The presentation includes references and a poster for further details.</sample>
  </task>
</testset>