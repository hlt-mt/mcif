<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是通过使用预训练的词嵌入和自定义的标注数据集进行微调的文本。</sample>
    <sample id="1">根据演讲者的介绍，这篇论文的作者是马特林和马卡夏塔。他们代表了麦克基尔大学、Mila和微软研究之间的合作。</sample>
    <sample id="2">本演讲稿由来自中国Ant Group的Tui介绍，重点讨论了文档理解领域的视觉丰富文档理解问题。演讲稿首先介绍了文档理解的挑战，包括阅读顺序问题，然后提出了LayoutMask模型来解决这些问题。LayoutMask模型通过使用文本和布局信息作为输入，旨在增强文本与布局之间的交互和布局表示学习。与以往研究不同，LayoutMask采用了局部阅读顺序（Local Reading Order）而不是全局阅读顺序（Global Reading Order），并采用了整词遮挡和局部可遮挡的遮挡策略。此外，还提出了Mask位置建模任务，以恢复遮挡的2D位置。实验结果表明，LayoutMask在不同数据集上表现良好，特别是在处理具有复杂布局的文档时。演讲稿还提到了一些实验结果和示例，展示了LayoutMask在处理垂直和水平布局以及多列数字时的性能。最后，演讲者邀请观众在观看后通过电子邮件提出任何问题。</sample>
    <sample id="3">欢迎来到我们关于DePlain的演示，这是德语文本简化的新语料库，适用于文档级别和句子级别的文本简化。我的名字是Ragina Strohmann，我将指导您完成演示的第一部分。首先，让我们定义文本简化。文本简化是一种过程，通过适应文本以改善特定目标群体对文本的理解，例如阅读困难的人或非母语使用者。要训练文本简化模型，我们需要平行文本对，例如文档或句子的平行对。在这个例子中，您可以看到一个复杂德语句子与它的德语翻译成简单语言的句子对。为了简化句子，可以使用不同的技术，如词汇替换、从句省略、从句省略重排或插入短语。现在，我们提出了一个新的语料库DePlain，因为近年来存在一些问题。例如，这些语料库太小，无法训练文本简化模型。另外三个最近提出的语料库都是自动对齐的，这意味着它们可能包含错误的对齐。因此，我们提出了DePlain的新语料库，它分为两个子语料库：DePlain API和DePlain Web。DePlain API基于新闻文本，在DePlain API中手动对齐了483篇文档，结果产生了大约30,000至13,000个平行句子对。对于DePlain Web，这个语料库包括不同领域，并手动对齐了750篇文档，另一方面，我们还使用自动对齐方法对齐了所有这些文档。总共，我们得到了30,450个句子对。我们进一步分析了我们的句子对，例如，根据简化类型，可以看到圣经文本比新闻文本或语言学习文本更加强化。此外，我们可以看到DePlain语料库具有不同简化转换的高多样性。例如，在DePlain API语料库中，我们有很多排序和词替换，而在DePlain Web语料库中，我们有很多重写。现在让我们看看我们可以用这个语料库做什么。你好，我是阿米尔，现在我将讨论DePlain数据集的用途。对于第一个用途，我们可以评估自动对齐方法。近年来，已经提出了许多对齐方法，但在机器翻译的背景下，我们有两个平行文档，分别用不同语言编写，我们需要提取两个文档之间的句子对齐。但在我们的用途中，我们试图提取具有相同语言和相同内容但处于不同复杂度级别的两个平行文档之间的句子对齐。现在，由于我们拥有DePlain语料库中手动对齐的句子对，我们可以使用这些句子对作为标准对齐来评估一些提出的对齐方法。我们对提出的对齐方法进行了调整，并在论文中出版了这些调整和运行实验的代码。最后，我们得出结论，对于德语文本简化，最佳的自动对齐方法是MSTAlign，并且您可以在论文中找到运行此方法的代码。第二个用途我们在论文中展示的是自动文本简化，通过微调语言模型来产生简化文本，从复杂的输入文本中产生简化文本。我们已经微调了Long Impart模型来产生文档级别的简化，我们还微调了Normal Base Long Impart模型来产生句子级别的简化。您还可以在论文中找到所有检查点和实验的详细评分和评估指标。我们得出结论，这种基本微调可以产生或获得比 baseline 分数更好的分数，并将其作为未来自动文本简化问题的基准。谢谢您的关注，希望能在会议期间见到你们。</sample>
    <sample id="4">演讲者的名字是Kayo Yin。</sample>
    <sample id="5">他们使用一种模型，该模型具有部分重叠的背景知识，从而获得82%-87%的准确率。</sample>
    <sample id="6">本研究旨在统一多语言和跨语言摘要，提出了一种名为“多对多摘要”的方法。该方法可以使用一个模型将任何源语言的文档摘要为任何目标语言的摘要。我们还进行了初步实验，以提供多语言摘要、跨语言摘要和多对多摘要之间的深入分析。结果表明，多对多摘要有助于摘要模型在不同语言之间更好地传输任务知识。此外，我们提出了一个名为“路径”的预训练多对多摘要模型，它通过三个阶段的预训练来优化模型性能。这些阶段包括基于噪声对应句子的母语预训练、基于不同源语言的噪声平行句子的跨语言预训练，以及基于多对多摘要样本的特定任务预训练。实验结果表明，路径模型在所有测试语言上均表现出色，并且在评估研究中验证了每个预训练阶段的有效性。</sample>
    <sample id="7">是的，根据所提供的内容，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="8">提出的人工评估方法新颖之处在于它通过明确标注每个模型响应是否表现出某些行为，如提供不相关的信息或与自身或对话伙伴矛盾，来减少人类评估的主观性。这种方法被称为“在聊天中标注行为”或“ABC评估”，旨在提供更精确和可靠的策略来评估对话质量的多个维度。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="10">根据演讲内容，可以采取的措施包括提高模型对背景知识的访问，使模型能够访问部分重叠的背景知识，以及使模型仅能访问实体名称。这些措施将有助于提高分数，因为它们使模型能够更好地理解间接引用表达，并更准确地选择实体。</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents "Do Androids Laugh at Electric Sheep?" humor understanding benchmarks from The New Yorker Caption Contest. This work involves collaborators from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI. Large language models can now generate and explain jokes, as demonstrated by Google's 540 billion parameter PaLM model explaining a joke about TPUs. However, ChatGPT often fails to understand jokes, such as a knock-knock joke involving a pineapple. To test this, Hessel's team used The New Yorker Caption Contest data, which includes matching captions, quality ranking, and explanation generation tasks. Their best model, CLIP fine-tuned on the annotated corpus, achieved around 62% accuracy in matching tasks, compared to a 20% random guessing baseline. Despite this, human raters outperformed the model by 94%. Even with additional annotations, language models like GPT-4 still lag behind humans in these tasks.</sample>
    <sample id="12">根据幻灯片，这篇论文有五位作者：Dawei Zhu、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="13">Daniel Rotem在海伯里大学的Roy Schwartz实验室进行了名为“找到甜蜜点：低资源环境下的自适应推断分析与改进”的研究。自适应推断是一种减少大型语言模型推理时间的方法，通过利用真实世界数据的复杂性，使用低容量模型处理简单样本，从而降低平均推理成本。两种常见的自适应推断方法是多模型和早期退出。多模型方法涉及将多个模型存储在一起，每个模型都有一个分类器，它们分别在训练集上进行训练，但在推理时按顺序运行，直到分类器决定停止计算。早期退出方法涉及在模型的中间Transformer层上为每个分类器拟合多个分类器，它们一起训练，但在推理时只运行直到分类器决定停止，从而节省了计算资源。然而，共享模型参数可能导致性能下降，被称为冲突梯度。Rotem和他的团队通过比较单独的早期退出模型分类器和多模型分类器来测试这一假设，发现多模型分类器在平均情况下比早期退出分类器高2.3%，尤其是在较早的分类器中。他们还测量了速度-准确率权衡，发现对于高速推理，多模型更优，但使用较晚的分类器时，早期退出更优。基于这些发现，他们提出了Sweet方法，这是一种用于早期退出架构的新型微调方法，避免了冲突梯度问题。Sweet方法在评估个体层时几乎消除了早期退出和多模型之间的差距，但在某些情况下，较晚的分类器可能受到负面影响。总体而言，他们的研究表明存在冲突梯度，并提供了第一个对早期退出和多模型自适应推断方法进行公平比较的研究。他们的结果还促进了针对早期退出架构的微调算法的未来研究。</sample>
    <sample id="14">嗨，我的名字是Adam Siprowski，这篇论文是关于协调结构的依赖性结构。正如你们所知，不同的依赖性结构是由不同的理论和语义方法假设的。例如，在普遍依赖性中，Lisa和Maggie之间的协调结构的结构是这样的：第一个连词是整个协调结构的主干。在这个例子中，Lisa。类似的假设在伊戈尔·米特鲁克的意义文本理论中也存在，其中协调结构同样由第一个连词领导。因此，这两种方法都是对称的，它们突出一个连词。还有另一种对称的方法来处理协调结构，即布加罗方法，其中协调结构由连词领导。因此，我们得到从连词到所有连词的依赖性。最后，还有一个多头领导的方法，例如在德卡森的词法中使用，其中所有连词都是协调结构的主干。因此，我们得到从治理者（这里是Lisa）到所有连词的依赖性。现在，本文旨在提出一种新论点，支持对称协调结构，如这些，并反对不对称协调结构，如这些。这个论点基于依赖性长度最小化原则，我将在这些示例的基础上进行解释。在英语中，正如你可能知道的，直接宾语更喜欢靠近动词，而介词短语可能离动词更远。所以，March read it yesterday是好的，因为直接宾语“它”离动词更近。然而，March read yesterday it是不好的，因为在动词和直接宾语之间有一个介词短语“昨天”。然而，这个效果可以缓解，当直接宾语非常重或很长时，因为它可以移动到介词短语后面。这在下面的示例中说明了这一点。这两个句子都很好。March read this absolutely fascinating book about the bees yesterday。因为，尽管这个句子违反了动词语法原则，即直接宾语应该离动词更近，但它满足了依赖性长度最小化原则，即较短的依赖性更可取。因此，这两个树只显示了关键依赖性的长度，即在这些结构中不是恒定的。所以，这里我们有一个从“red”到“book”的依赖性长度为7个单词，从“red”到“book”的依赖性长度为4个单词，总共11个单词。当你交换这两个成分时，这两个依赖性的总和变为6个单词，比11个单词要短得多，这就是为什么这句话听起来很好。它违反了一个原则，但满足了另一个原则。所以，我们做了什么？我们从增强版本的Penn树库中提取了关于协调的统计数据，并在论文中解释了为什么我们不使用普遍依赖性。这些统计结果确认了很多人之前做出的观察，即左连词通常更短。例如，“盐和胡椒”与“胡椒和盐”测量为 syllables。此外，我们还观察到了一个之前没有提到的趋势，即随着两个连词长度差的增长，左连词倾向于更短。所以，比例是左连词更短的比例更大。但是，我们观察到，这个趋势只发生在左边 governance 缺失的情况下。所以，在这个例子中，Lisa和Bob，左边 governance 是 governance。在第二个例子中，Homer came and sneezed，我们有两个连词的协调，没有外部外部 governance。所以，在这种情况下，左边连词倾向于更短，而且两个连词长度差越大，左边连词倾向于更短。然而，当 governance 在右边时，比如Lisa governs the coordination，这个效应就消失了。所以我们展示了通过测量字符长度，即第一列是音节，中间列是单词，第三列是字符，我们将集中关注第三列。我们在这里看到的是，当 governance 在左边时，左边连词更短的趋势随着两个连词长度差的绝对值的增加而稳定地增长。同样地，当没有 governance 时，比如在协调句子中，我们观察到了相同的现象。但是，当 governance 在右边时，这个趋势就消失了。我们在论文中展示了如何通过测量字符长度来提供对称协调结构的论点，而不是不对称协调结构。所以，请看论文获取完整的论点和论点，然后与我们讨论海报会议。谢谢。</sample>
    <sample id="15">根据幻灯片显示，这篇论文有三位作者：马歇尔·林登曼、亚历克赛·科勒和 Ivan 提托夫。</sample>
    <sample id="16">根据分析，圣经文本的简化程度更大。</sample>
    <sample id="17">演讲者介绍了一种名为“Dr. Bert”的模型，这是一种基于Robert的预训练模型，用于生物医学和临床领域的自然语言处理。该模型使用NatCh（一个包含医疗信息的网络数据集）进行训练，并在多种生物医学和临床任务中表现出色。演讲者还比较了Dr. Bert与Shubert模型（基于 anonymized数据）以及几种预训练策略的效果，发现从头开始预训练通常能获得更好的性能，但持续预训练可以达到类似的效果，尤其是在稳定性方面。此外，演讲者还提到了一种称为“信息抽取”的技术，它通过内部信息筛选和多模态主题模型来提高特征表示，从而显著提升了系统性能。演讲者强调了持续预训练和信息抽取技术的重要性，并提供了代码和资源以供进一步了解。</sample>
    <sample id="18">一个示例是“盐和胡椒”，其中较短的并列词“盐”位于较长的并列词“胡椒”之前。</sample>
    <sample id="19">The presentation introduces a survey on efficient open-domain question answering, focusing on a two-stage framework proposed by Dandan Chen in 2017. The first stage uses retrieval to fetch relevant context from the Wikipedia corpus, and the second stage employs a reader to understand the question and retrieve the necessary information to generate an answer. The retrieval process involves a question encoder and a document encoder, which preprocesses the Wikipedia corpus into an index file for faster searching. Challenges include the large size of the Wikipedia corpus (26 million documents, 20 GB), the index file (65 GB), and the use of multiple language models with millions of parameters. The goal is to achieve efficient systems with smaller memory costs, faster inference, and comparable performance. Techniques such as approximate nearest neighbor search, skipping rate, document filtering, and model compression are discussed to address these challenges. The presentation also compares existing models based on their trade-offs between speed, memory, and performance, concluding that lightweight models or parameter sharing can be used to reduce model size. Future work includes deploying open-domain question answering systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">是的，这些模型可以用于您的研究。演讲者提到了一个GitHub仓库，您可以在那里找到训练脚本和预训练模型。此外，他们还表示他们的系统在9个任务中表现良好，并且在所有任务中超越了通用模型Camembert。</sample>
    <sample id="21">DEplain-apa 包含来自新闻文本的内容。</sample>
    <sample id="22">有助于良好泛化的因素包括模型架构、模型大小和更多的微调示例。</sample>
    <sample id="23">The video discusses the challenges faced by text-image models in accurately rendering visual text. It highlights that while these models have made significant progress in generating high-quality images, they often struggle with representing text. The video focuses on the Imagen model, which uses a T5-XL encoder to generate images from input text. However, even for simpler textual inputs requiring the image to contain a word, the model frequently fails.

The video delves into the text encoder's role, explaining that T5 uses sentence-piece tokenization, where subword tokens are used instead of individual letters. This means the model receives chunks of the input string rather than individual characters, making it difficult to decompose atomic subword vocabulary items into individual letters for accurate text rendering.

The video presents experiments comparing different text encoders, including T5 and PaLM models. It shows that T5 struggles significantly with spelling, especially at smaller scales, while PaLM models perform better but are impractical due to their larger size and training data requirements. BitT5, which receives individual bits of the input string, is shown to perform well in spelling across all scales.

The video explains that T5 struggles with spelling more frequent words because these words are represented by fewer subword tokens, requiring the model to decompose them into more letters. In contrast, BitT5 has full access to character-level information, allowing it to copy input characters directly to the output.

The video concludes by introducing an augmented version of the Imagen model that includes an additional text representation from BitT5. This improvement helps the model spell words correctly and enhances its ability to render text, although the diffusion model can still introduce errors during generation.</sample>
    <sample id="24">通过绝对单词数衡量左并列词是否更短。</sample>
    <sample id="25">根据所给英文内容，实验设计可能涉及比较不同支配词位置的句子的可接受性。例如，可以将句子分为两组：一组具有左支配词（如“Lisa bought and Megan”），另一组具有右支配词（如“Megan bought and Lisa”）。然后，参与者将被要求评估每个句子的可接受性，并数据将被收集和分析以确定是否存在显著差异。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳。正如演讲者所提到的，基线分类器仅基于43个标注的样本进行训练，无法捕捉到“分歧”类别，导致分类器的表现远不如随机猜测。</sample>
    <sample id="27">根据提供的信息，无法确定论文的作者人数。演讲者提到了“我们”和“我们”等代词，这表明有多位作者参与了研究。然而，没有关于作者姓名或团队结构的具体细节，无法准确确定作者的确切数量。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">在正式性和词汇连贯性等话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="30">Hello everyone, we are going to introduce our paper LLM-Blender, which is a simple yet effective ensemble learning framework for large language models. The key idea is based on pairwise ranking and generative fusion. We are a team from AI2 and USC, and my name is Yuchen Lin. So there are so many large language models released every week, and many of them claim that they have achieved great performance. And from this leaderboard, we can indeed say some models are better than the others. But this is only about average overall performance. And when you have a particular input example, should you simply use the single top-ranked model? Our finding suggests no. The optimal selection of the models can significantly vary across different input examples. For example, although Becauna has the best average overall performance out of these 11 models, in only 21% of the examples it is the best model. So this pie chart suggests that each language model has its own advantage and disadvantages. Therefore, we believe that we should consider using more large language models for each input such that we can select and generate a better output than using any single model for all inputs. So to this end, we propose a two-stage framework named LLM-Blender. Given a certain input X, we will run n different models and get their outputs Y1 to Yn. Then we use a pairwise ranking module named PairRanker to compare all these candidates and get a ranking of them. Specifically, we concatenate the input X and each pair of these candidates Yi and Yj, and use a cross-attention module such as RoBERTa for learning to distinguish which candidate is better for the input X. And given the comparison matrix here, we can aggregate the results to get a final order of these candidates. In the next stage, we pick the top k, let's say top three candidates, and we use them as the input to a sequence-to-sequence model for learning and inference. A generative fusion model. So this fusion model will then output the final output for this input X by fusing the top three candidates ranked by this PairRanker. So this is the overall plan of our LLM-Blender. So let's take a closer look at the PairRanker module. Comparing with the prior methods, a key difference of our PairRanker is that in this encoding stage, the green boxes here are the encoders of all these four methods. And our PairRanker encodes a pair of candidates alongside the input X for better analyzing the subtle differences between these two candidates. So this is very different from prior methods, which they look at each candidate individually and score the candidate individually and then rank all the candidates based on their scores. So we believe PairRanker is a better solution because it uses pairwise comparisons to learn and infer the quality of all these candidates and compare them side-by-side more carefully. Okay, so given the pairwise comparison results, we can derive a matrix here, and where each element in this matrix represents the comparison logits for candidate i being better than candidate j. Then from this matrix, we can have three methods to aggregate all these results. And we found that using the mass logits to aggregate the order is the best solution. But if you worry about the efficiency, you can also use the popular softmax here, then it's very efficient, and we can also get a decent performance. So experiments here show that PairRanker is much better correlated with other with the oracle ranking better than all the other ranking methods on various correlation metrics. So to evaluate to enable the evaluation of ensemble learning frameworks, we also create a new dataset named the MixInstruct. So it consists of existing instruction datasets and we collect the candidates from 11 open source large language models and we use a bertscore, bleurt, bertscore as the automatic metrics. And we also use ChatGPT as the judge to compare the results as well. So here we show that our empirical results where we can see that the top two models, OpenAssistant and the Becauna, their performance are consistently worse than our PairRanker and the full Blinder framework on all these four metrics. And particularly, like Blinder's results can beat them in 68 percent and 76 percent of examples respectively for OpenAssistant and Becauna. And these results suggest that Blinder is a very promising framework for ensemble learning, although it's very simple and straightforward. So in the end, we want to give some take-home messages here. So large language model Blinder is a simple and effective ensemble learning framework for large language models. It has two submodules, PairRanker is a pairwise comparison module that we can get the matrix for all these results, and GenFuser can take the top three candidates and generate the final output, and it notably improves the performance. And the MixInstruct is our dataset for evaluating the large language models here. And we also release the unified dataset codebase and our data for evaluation and future research. Okay, so that's all. Thank you very much.</sample>
    <sample id="31">根据幻灯片上显示的标志，这篇论文的作者来自约翰霍普金斯大学、普渡大学和麻省理工学院。此外，幻灯片底部还提到了Meta AI，这表明该机构可能参与了这项研究或与之合作。</sample>
    <sample id="33">引入的框架通过分析文本数据中不同主题的频率来量化立场。它识别了与政治立场相关的常见主题，如经济政策、社会问题和国际关系。通过计算每个主题在文本中出现的频率，框架可以估计文本的整体立场。例如，如果文本中关于经济政策的主题出现频率较高，而这些主题通常与右翼立场相关联，则该文本可能被分类为右翼。</sample>
    <sample id="34">这段英文内容讨论了Crest框架，这是一个用于文本生成的联合框架，专注于解释和生成对抗样本。该框架通过在训练过程中利用对抗样本来提高模型的可解释性。Crest框架产生的解释更可信、流畅且多样，而这些对抗样本又具有更高的可解释性，从而有助于模型更好地理解输入的对比部分。该框架的提出旨在解决自然语言处理中常见的脱分布问题，即模型生成的输出与输入脱节。Crest框架通过在训练过程中使用对抗样本，有助于生成更可信的解释，从而提高模型的整体性能。</sample>
    <sample id="36">The presentation introduces a method for enhancing multilingual machine translation by using language-specific layers (LLs) in a transformer model. The goal is to increase the capacity per language while maintaining constant inference costs. This is achieved by having one regular transformer layer per language, which selects and trains the correct sub-layer during both training and inference times. The presentation explains that placing LLSs in different parts of the model can affect performance, and the team focused on the encoder due to limited data. They used a method involving shared, source, and target weights to learn the best placement of LLSs, resulting in an architecture where the bottom layers are shared, followed by source-specific LLSs, then more shared layers, three target-specific LLSs, and finally a shared top layer. The results show significant improvements over baseline models and language adapters, especially for low-resource languages, with statistically significant improvements found in 84 out of 90 translation directions.</sample>
    <sample id="37">根据所给英文内容，之前的研究结果表明，当人类受试者被给予相同的人格化提示时，他们对不同种族和族裔的女性的描述反映了刻板印象。例如，描述 Latina 女性的词语包括“鲜艳”和“丰腴”，而描述 Asian 女性的词语包括“甜”、“文静”和“丝滑”，而描述 Black 女性的词语包括“坚强”和“坚韧”。这些词语反映了刻板印象和本质化叙事，反映了社会对这些群体的负面看法。</sample>
    <sample id="38">此研究使用了来自Pencbank增强版本的统计数据。</sample>
    <sample id="39">根据所提供的内容，这篇论文有两位作者：Adam Siprowski和Igor Miltruk。</sample>
    <sample id="40">根据所给内容，与认知失调密切相关的任务包括主题独立的分歧分类和二元分类的扩展与比较类别。这些任务与认知失调的概念相关，因为它们涉及评估不同观点或陈述之间的关系，这是认知失调研究的核心。</sample>
    <sample id="41">Silin介绍了一个名为PEACoK的项目，这是Sony Group Corporation合作开发的，旨在通过使用大规模人物图谱来增强自然语言处理系统对人物关系的理解。PEACoK包含约3.8万个角色和40万个属性，形成了100万个人推断或事实。通过将这些信息与大型预训练语言模型结合，PEACoK能够帮助语言模型学习和泛化人物知识，从而提高对话生成的质量。</sample>
    <sample id="42">根据幻灯片显示，这篇论文有两位作者：Shuheng Liu和Alan Ritter。</sample>
    <sample id="43">根据幻灯片上显示的作者姓名，这篇论文有七位作者。</sample>
    <sample id="44">引入的框架与以前的研究不同之处在于它通过将用户注释与现有数据集和模型进行比较，来研究数据集和模型的位置性。这与以往专注于注释者之间的注释一致性或注释者分布的文献不同。</sample>
    <sample id="45">在三个比较设置中，与刻板词汇的重叠最多的是黑人女性。</sample>
    <sample id="46">在演讲中提到的商业系统是Google Translate和DeepL。</sample>
    <sample id="47">今天我代表我们团队展示我们的工作，从预训练数据到语言模型，再到 downstream 任务，追踪政治偏见的轨迹，导致了 NLP 模型中的不公正。语言模型是通过大规模网络抓取数据进行训练的。政治新闻媒体在预训练数据中得到了很好的覆盖。根据对 C4 数据集的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》和《 Huffington Post》等报纸在语言模型训练数据中被广泛覆盖。这为 NLP 应用程序带来了双重影响：一方面，它们能够学习来自不同观点的多样视角，庆祝民主和思想的多样性；另一方面，这些不同的政治观点内在地具有社会偏见，可能会导致 NLP 模型在 downstream 任务中的公平性问题。为了应对这个问题，我们提出了一个研究框架，旨在调查政治偏见传播管道，从预训练数据到语言模型，再到 downstream 任务。我们提出了两个核心问题：首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在其中扮演的角色？其次，具有不同政治倾向的语言模型在 downstream 任务中的表现如何？这种差异是否会导致 NLP 应用程序中的公平性问题？我们首先提出了一个方法，通过使用政治问题集（如政治 compass 测试）来评估语言模型的政治倾向。这种方法确保了评估基于政治科学文献。初步结果表明，语言模型确实具有政治倾向，它们占据了政治光谱的四个象限。GPT-4 是最自由的语言模型，而 GPT 系列通常比 Bert 系列更自由。我们还进行了实验，通过在六种不同的党派语料库上进一步预训练语言模型，以了解政治偏见是否会被模型吸收。实验结果表明，通过在左翼或右翼语料库上进一步预训练，模型的政治倾向也会相应地发生转变。例如，在进一步预训练后，Roberta 在左翼红色语料库上的训练显示了显著的自由主义转变。我们还探讨了模型是否能反映出当代社会的 polarization。我们分别在特朗普总统之前和之后的两个时间点上预训练了模型，发现模型的政治倾向在特朗普就任后普遍远离了中间派。最后，我们评估了具有不同政治倾向的语言模型在 hate speech detection 和 fake news detection 等 NLP 应用中的表现。结果表明，不同政治倾向的模型在检测 hate speech 和 fake news 时的表现存在差异。例如，左翼模型在检测针对少数群体的 hate speech 时表现更好，但在检测针对权力群体的 hate speech 时表现较差。相反，右翼模型在检测针对白人和男性群体的 hate speech 时表现更好，但在检测针对黑人 LGBTQ+ 群体的 hate speech 时表现较差。类似的趋势也出现在 fake news detection 中。这些结果表明，不同政治倾向的模型在处理 hate speech 和 fake news 时会给出不同的预测，基于用户的社交媒体背景。这些发现突显了 NLP 模型政治倾向所引发的公平性问题的重要性。例如，如果右翼模型被用于 hate speech 或 fake news 的检测和控制，可能会导致具有相反政治观点的人被边缘化，而针对少数群体的 hate speech 可能无法得到控制。因此，我们需要承认并解决 NLP 模型政治倾向所带来的公平性问题。最后，我们讨论了语言模型政治偏见的独特 dilemma。如果我们不净化预训练数据中的政治观点，偏见将从数据传播到模型，最终导致公平性问题。如果我们尝试净化数据，可能会导致审查或排除，很难确定什么应该被保留在模型训练数据中。这就像电学中的电容器问题一样复杂。总之，我们的工作旨在揭示 NLP 模型政治偏见的复杂性和重要性，并提出解决这些问题的方法。</sample>
    <sample id="48">根据所提供的英文内容，这篇论文有五位作者。</sample>
    <sample id="49">MPP 评估最多涵盖了一千二百四十六个词元的上下文长度。</sample>
    <sample id="50">演讲者介绍了一个名为“Diplomat”的数据集，用于评估机器翻译中的自动对齐方法。在机器翻译中，通常需要20个样本才能获得高性能。该数据集包含手动对齐的句子，可以作为金标准来评估不同的对齐方法。演讲者还展示了如何使用这些句子来评估和改进文本简化模型。通过调整不同的模型，他们能够产生比基线更好的简化文本。</sample>
    <sample id="51">他们的数据集包括音乐、书籍和食谱三个领域。</sample>
    <sample id="52">positionality（立场）是指人们持有的观点，这些观点源于他们的 demographics、identity 和 life experiences。</sample>
    <sample id="53">演讲者的名字是Dawei Zhu、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="54">计算机科学专业的候选人Vasudha在Stony Brook University的ACSL 2023年会议中展示了一篇题为“通过迁移学习解决认知失谐检测中的罕见类别挑战”的论文。她首先定义了认知失谐，即两个不一致的信念或行动，如吸烟者吸烟后声称吸烟会致命。她指出，认知失谐是日常决策中常见的现象，但在其他关系中也可能表达出来。研究认知失谐有助于理解人们之间的分歧、跟踪人群中的信仰和价值观变化以及更好地了解人们的心理健康。此外，研究语言中表达的认知失谐有助于理解极端主义和边缘化群体的 polarization。</sample>
    <sample id="55">是的，EDAtt 适应了现有的离线 ST 模型。它使用已存在的 offline ST 模型，无需重新训练或采用特定架构，来处理实时翻译任务。</sample>
    <sample id="56">根据提供的内容，无法确定论文的作者人数。演讲者只提到了自己是金振东，来自Penn State University。要了解论文的作者人数，通常需要查阅论文本身或其引用信息，这些信息在提供的演讲摘要中未包含。</sample>
    <sample id="57">被测模型不能在测试套件上运行。</sample>
    <sample id="58">KITMUS 有三个变体：1. 背景预训练，2. 背景和背景预训练，3. 背景和背景预训练。</sample>
    <sample id="59">多级检索是一种回答需要进行多次推理跳跃的问题的方法，每次跳跃通常对应一个文档。例如，要回答1988年圣诞节喜剧电影《Brian Doyle Murray》的名称，首先需要找到所有Brian Doyle Murray参演的电影，然后找到其中1988年上映的电影。这个过程涉及多个文档，称为“链”。多级检索检索器通过最大化给定问题的正确链的概率来训练，例如Q1-C1-Q2-C2等，其中Q是问题，C是目标链。检索器通过最大化给定Q的C的概率来训练。大多数现有的多级检索检索器都落在这一谱系中。现有的系统通常需要成千上万的例子和真实答案才能表现良好，这在低资源领域或需要特殊专业知识的领域可能很昂贵。我们的方法是有效的，因为它可以在128个例子中给出很好的表现，从而解决了这个问题。我们的想法是将无监督检索方法与有限的短语语言模型结合在一起。然而，我们发现从零开始预训练似乎在大多数任务上获得了更高的性能。然而，我们在使用从NATOS的4GB子集预训练的BERT模型上的实验表明，使用预训练的BERT模型获得的结果与从头开始预训练的BERT模型相当，而基于Camembert权重和分词器的模型则由于稳定性问题而表现不佳。总的来说，我们的系统在11个Dontlim任务中表现更好，并且在所有任务上超过了通用模型Camembert。我们还观察到，专业化的数据比一般数据表现更好，但不具有可扩展性。预训练模型可以从NATOS免费获取，训练脚本可以在我们的GitLab仓库中找到。谢谢您的关注，我们期待在Toronto的研讨会后与您交流。</sample>
    <sample id="60">根据幻灯片显示，论文的作者属于Google Research。</sample>
    <sample id="61">最后一个问题是在弱监督学习中是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们。</sample>
    <sample id="62">这段内容描述了研究中探索的三个主要研究问题：验证数据是否对于Pseudo-Target Learning (PTL)是必要的，如果需要，需要多少干净样本，以及是否应该仅使用干净样本进行验证。研究还涉及了PTL的架构决策，如比较编码器解码器和解码器-only架构，以及 pruning 对任务性能和计算性能的影响。此外，研究还探讨了PTL的扩展，包括使用多个伪目标而不是单个目标，以及通过采样生成多样化的伪目标来暴露学生对教师知识的更多样性。最后，研究提出了一个名为“联合教学”的新颖知识蒸馏技术，旨在解决学生曝光偏差、增强学习并教学生如何纠正自己的错误。</sample>
    <sample id="63">根据所给英文内容，指标灵敏度指的是在研究中使用的多模态指令调优数据集的大小。该数据集包含超过1600个语言任务，但没有大规模公开可用的多模态指令任务。这促使他们创建了一个多模态指令调优数据集。</sample>
    <sample id="64">演讲者的名字是金Wei。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。</sample>
    <sample id="66">演讲者介绍了一种评估自然语言处理模型接受度的新方法，重点是通过长句子来测试模型的接受度。传统的MPP（Masked Language Pre-training）管道无法评估模型在更长句子中的接受度，而这种方法通过模拟长句子来解决这个问题。具体做法是重新设计数据集，选择可接受和不可接受的句子作为前缀，然后将这些前缀添加到长句子中，以测试模型对这些句子的接受度。这种方法可以用于不同数据集的匹配和不匹配场景，以及完全无关的领域，如维基百科，以评估模型接受度判断是否受上下文影响。实验结果表明，模型对可接受和不可接受句子的接受度判断存在显著差异，且这种影响随上下文长度增加而加剧。此外，即使在输入句子中加入噪声以保留结构，模型对接受度的判断仍然保持稳定，表明模型对潜在句法和语义特征敏感。</sample>
    <sample id="67">在多语言翻译模型中，协同和干扰是关键因素。协同可以提高翻译质量，而干扰则可能导致性能下降。例如，训练英语到芬兰语可能改善英语- Estonian的翻译质量，而英语到中文则可能产生负面影响。为了解决这个问题，研究者提出了各种方法来减轻干扰，但这些方法往往在小型模型上效果显著，但在大型模型上并不总是优于简单的调优基线。研究发现，当模型规模相对于数据量很小时，干扰会加剧。调整采样温度对于实现强大性能至关重要。对于简单双语情况，有可预测的模型和数据大小缩放规律，但在多语言情况下，其他因素如其他语言的数据大小、语言相似性和总语言数量也会影响性能。然而，研究发现，语言相似性和总语言数量对干扰水平的影响较小。通过比较双语模型在S到T上的损失与多语言模型在ST上的损失，可以定义干扰。实验使用了15种不同语言的Transformer架构变体，结果表明，当双语模型的损失低于多语言模型时，干扰值为负。研究还发现，随着数据量的增加，干扰问题会减轻，甚至在参数不足的情况下也会缓解。温度采样是一种有效的解决方案，当温度大于1时，可以从低资源语言中采样更多训练样本。通过调整温度，可以显著减少干扰，而无需使用任何专门的方法。</sample>
    <sample id="68">在预训练期间，模型会接收各种语言上下文。这些上下文可以是来自不同领域的句子，如维基百科，也可以是来自同一数据集内的句子。这种多样性有助于模型学习语言的抽象知识，从而在处理更长的句子时更加灵活和准确。</sample>
    <sample id="69">在 WSL 中，通常需要每个类别大约20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">根据幻灯片中显示的信息，Myra Cheng、Esin Durmus和Dan Jurafsky三位作者都来自斯坦福工程学院计算机科学系。</sample>
    <sample id="71">The methodology focuses on informality and uses a cartoon completion setup. The cartoon has three speech bubbles: in the first bubble, Bob says 'Remember that song we were listening to yesterday?' which sets the dialogue context. In the second bubble, Alice asks 'Do you mean easy on me or I got a feeling?' which is an alternative question. In the third bubble, Bob uses an indirect reference to select one of these entities, for example, 'the newer one.' We provide the first two speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated using a simple template: 'Do you mean A or B?' where A and B are samples from Wikipedia. We use different sampling methods when moving higher in the list: uniform at random, when the entities have similar titles (e.g., two books with the name 'The Return'), when they have similar descriptions on Wikipedia, and finally, when they have similar infoboxes or attributes on Wikipedia (e.g., the same genre).</sample>
    <sample id="72">开发新的方法来衡量媒体偏见是必要的，因为传统的衡量方法可能无法捕捉到微妙的偏见或系统性偏见。新的方法应该能够识别微妙的偏见，如对特定群体的微妙提及或暗示，以及系统性偏见，如对某些主题或观点的系统性忽视或扭曲。这些方法还应该能够处理多样化的媒体环境和观众，包括数字媒体、社交媒体和传统媒体。此外，这些方法应该能够提供可解释和透明的结果，以便媒体机构和监管者可以理解并解决偏见问题。</sample>
    <sample id="73">演讲者的名字是马查塔。</sample>
    <sample id="74">The presentation introduces a study on improving the quality of knowledge graphs through an over-generated relation filter. It highlights that previous studies have shown high variability in the output quality of knowledge models, leading to poor performance. The study proposes using an over-generated relation filter to enhance generation quality by identifying and correcting missing links in knowledge graphs.

The presentation explains the concept of atomic, a large-scale common knowledge base that captures events centered around social spaces of influential knowledge topics. Atomic contains very few multi-hop paths due to the limitation of tail events not becoming head events. This results in significant missing links, such as B2B, A2B, A2A, and A2A links, despite its high-quality human-annotated common sense knowledge.

To address these issues, the study constructs a dataset named "DenseNOM" by comparing it with Atomic. DenseNOM completes many missing links in Atomic, including B2A, B2B, A2B, and A2A links, and also contains more multi-hop paths, such as X -&gt; Y -&gt; Mary and then Y says S and then X smiles.

The construction process of DenseNOM involves three main steps: normalizing tail events, training a relation prediction model, and constructing DenseNOM. Tail events are normalized by converting them into the same format as head events through four processes: subject removal, third-person singular form conjugation, subject recovery, and relation grouping.

The presentation also discusses the limitations of traditional methods for completing Atomic, which include sparse graph structure making it difficult for humans to interpret information and insufficient utilization of semantic information from events. To overcome these limitations, the study proposes a method called RAS-KGC, which predicts relations given the head event and tail event of a triplet. RAS-KGC encodes both the head and tail events with a pre-trained language model and applies max pooling on the head and tail events, concatenating them for link prediction. This method utilizes no graph structure information and takes advantage of semantic information by incorporating both the head and tail events.

The presentation further explains the implementation details of RAS-KGC, including the inter-class and intra-class completion strategies used to improve missing links within and between classes. The study also compares RAS-KGC with relation prediction methods and translation-based methods, demonstrating superior performance on both automatic and human evaluations.

Finally, the presentation evaluates the performance of DenseNOM and RAS-KGC on constructed datasets. DenseNOM is shown to have higher knowledge coverage and more diverse results, while RAS-KGC achieves better results in terms of the accuracy of multi-hop paths. The presentation concludes by highlighting the potential of DenseNOM for enhancing the performance of knowledge graphs and the effectiveness of RAS-KGC in predicting relations.</sample>
    <sample id="75">本研究探讨了半监督学习在实体和关系抽取中的应用，特别是通过基于异质图的传播。我们发现，通过训练模型可以生成高质量的脚本，表明较小的模型可以在适合的数据集上超越较大的模型。我们提出了一个名为“JointProp”的框架，它包括四个主要部分：特征生成、异质图构建、联合标签传播和模型优化。在特征生成阶段，我们使用训练分类器生成未标注数据的标签表示。在异质图构建阶段，我们构建了一个邻域图以提高计算效率，并利用标签数据和未标注数据之间的相似性关系。在联合标签传播阶段，我们通过异质图在未标注数据中传播标签，利用未标注数据之间的平滑约束。最后，在模型优化阶段，我们使用Softmax函数和标准softmax操作来确定最终标签，并根据置信度阈值过滤低质量标签，然后与标签数据一起重新训练分类模型。实验结果表明，JointProp框架在所有基线模型上显著且一致地提高了NER和关系任务的表现，特别是在单任务数据集上。</sample>
    <sample id="76">根据演讲内容，政治偏见传播流程包括以下几个步骤：首先，政治偏见存在于训练数据中。其次，这些偏见被语言模型学习和复制。最后，这些偏见在下游任务中导致公平性问题。</sample>
    <sample id="77">这个视频介绍了改进摘要事实一致性的工作，这是通过自然语言反馈实现的。这项工作是耶鲁大学和微软研究的联合项目，大部分工作是在第一个作者在微软研究实习期间完成的。在这项工作中，我们引入了一个新的数据集DeFacto，其中包含了人类演示和反馈，用于改进摘要事实一致性。对于这个数据集，我们提供了全面的分析，并提供了关于摘要模型事实一致性的进一步见解。基于这个数据集，我们提出了三个新的NLP任务：摘要编辑、反馈生成和自动事实错误校正。我们在这个工作中研究了抽象文本摘要，并特别研究了摘要模型的事实一致性。这个质量要求摘要中的所有信息都应得到原始文档的支持。我们收集的人类演示和反馈基于现有摘要模型生成的原始系统摘要。我们要求标注者提供标签来决定摘要是否事实一致，并要求他们提供如果原始摘要不正确，则提供事实一致的摘要。此外，我们还要求标注者提供包含解释、说明和证据的反馈。具体来说，解释是为了说明标注者认为摘要是否事实一致的原因，说明是为了指导如何修改原始摘要以使其事实一致，而证据则是其中一个例子。</sample>
    <sample id="78">根据提供的内容，DEplain-apa 和网站的简化过程有所不同。DEplain-apa 是一种用于德语文本简化的方法，而网站的简化过程可能涉及不同的方法或算法，可能是针对不同语言或简化目标设计的。</sample>
    <sample id="79">Coscript 并不公开可用。</sample>
    <sample id="80">水印通过定义目标嵌入和原始嵌入的加权求和来插入到文本中。当用户向服务发送句子时，提供者计算句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的加权求和，权重与句子中触发词的数量成正比。当句子中的触发词数量大于m时，提供的嵌入等于目标嵌入。</sample>
    <sample id="81">根据所提供的英文内容，论文的作者属于Penn State University。</sample>
    <sample id="82">The video discusses a research paper titled "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring." The paper focuses on developing an unsupervised automated essay scoring (AES) system that does not require human intervention or labeled data. Traditional AES models are typically trained using large datasets of essays with known quality scores, but collecting such data is time-consuming and labor-intensive, especially for essays written in response to new prompts. Unsupervised AES aims to overcome this challenge by using multiple heuristic signals to provide supervision.

The paper introduces a novel framework called URRA (Unsupervised Rank Aggregation), which uses multiple heuristic signals to generate partial order pairs and then trains a neural AES model through the aggregation of these signals. The framework includes three components: a heuristic essay ranking module (HERM) that ranks essays based on different heuristic quality signals, a deep pairwise rank aggregation module (DPRAM) that aggregates the partial order pairs into a unified supervision, and a scoring strategy to transform the predicted scores into the predefined score set.

Experiments conducted on both transductive and inductive settings demonstrate that URRA outperforms unsupervised baselines with significant improvements. Compared to cross-domain and one-shot methods, URRA achieves competitive performance, although it still lags behind general supervised methods due to the lack of strong supervision. The paper concludes by highlighting the effectiveness of URRA in unsupervised essay scoring and its potential for improving the quality of automated essay scoring systems.</sample>
    <sample id="83">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。根据演讲中提到的结果，编码器-解码器或编码器 PDR 可以通过在混合各种语言的训练数据上进行训练来改善其性能。这种多语言训练导致了大多数主要自然语言的性能提升，尽管英语的性能在某些数据集中略有下降。</sample>
    <sample id="84">The speaker introduces a new framework for dynamic neural networks, which can adapt its architecture and parameters based on input data. This is in contrast to traditional static networks that have fixed parameters. The speaker explains that while dynamic networks are more flexible, they often require more parameters, leading to increased computational costs and potential overfitting. To address this, the speaker proposes a method called "partial dynamic network," which partitions parameters into dynamic and static ones, using scale factors to control their influence. This approach aims to maintain better performance with fewer parameters and less computation. The speaker also discusses the importance of scale factors and constraints in achieving accurate results. The proposed method shows superior performance compared to both static and fully dynamic networks.</sample>
    <sample id="85">受限制语言规划的一个示例是制作巧克力蛋糕。</sample>
    <sample id="86">他们通过可视化生成的嵌入，展示了在每个句子中触发器的数量，使很难区分后门嵌入和正常嵌入。</sample>
    <sample id="87">根据所提供的英文内容，研究使用现有的预训练模型（PLM）来构建新的PLM的方法包括比较不同PLM的性能，分析它们在特定任务上的表现，并评估它们的泛化能力。该研究还涉及使用不同的数据集和预训练策略来训练PLM，以确定最佳的训练方法。此外，该研究还探讨了如何利用现有的PLM来构建新的PLM，以提高其性能并减少训练时间。</sample>
    <sample id="88">根据演讲内容，GPT-4 与非二元性别群体的立场最不一致。</sample>
    <sample id="89">演讲者展示了模型如何利用注意力机制所学的知识来处理一个包含“我打算谈论”的句子。</sample>
    <sample id="90">本研究探讨了语言学习者是否可以为自然语言处理（NLP）数据注释做出贡献。由于招募母语为特定目标语言的原住民说话者往往困难，而语言学习者数量众多，研究旨在评估使用语言学习者作为注释者的可行性。研究设计了实验，考虑了控制变量，如可获得资源和学习难度，选择了英语、韩语和印度尼西亚三种语言。实验涉及情感分析、文本对齐、序列标注和段落预测等任务。研究者将学习者分为三个水平：基础、中级和高级，并与原住民说话者进行了比较。结果表明，学习者标注的准确性几乎与原住民说话者相当，尤其是在简单任务和中等难度问题上。此外，通过聚合多个学习者标注，学习者标注的准确率进一步提高。研究还展示了使用学习者标注训练的模型在低资源语言上的表现，甚至超过了使用原住民说话者标注的模型。最后，研究观察到学习者的语言 proficiency、词汇和语法在标注任务中有所提升。总体而言，本研究证明了语言学习者可以有效地为NLP数据注释做出贡献，有助于克服低资源语言的地理和技术障碍，促进这些语言的NLP研究。</sample>
    <sample id="91">任务的数量可以显著影响模型的性能。更多的任务意味着模型需要处理更广泛和更复杂的数据集，这可能导致模型在理解语言和图像方面更加准确和全面。然而，这也可能增加训练时间，并且如果数据集不平衡或存在噪声，则可能降低模型的性能。</sample>
    <sample id="92">作者在其方法中比较了三个无树基线：CoNLL、Bert和Transformer。这些基线被用来评估其方法在生成更深层次的递归结构方面的性能，与传统的基于树的方法进行对比。</sample>
    <sample id="93">与第一作者相关的两位合著者是导师。</sample>
    <sample id="94">Hello everyone, my name is Jing Wei from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about our paper: "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding Services via Backdoor Watermark." Let's first introduce the background about embedding services. Currently, large language models such as GPT, LLaMA, and PaLM are exceptional in natural language understanding and generation. Embedding services is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding services. To protect the copyright of embedding services, one of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to embedding services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding services or lack of transferability. Therefore, in this paper, we propose Embedding Marker, which is a backdoor-based watermark method applicable to embedding services. Then let me introduce the details of our Embedding Marker. Embedding Marker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than m, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then the provider requests embeddings from the stolen service with the dataset. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AG News, Yahoo Answers, SST-2, and Yelp. We assume the provider apply Wikipedia dataset to count word frequency. The results on four datasets show that our Embedding Marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on UMAP PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all, thank you. We'll come to discuss with us.</sample>
    <sample id="95">PaLM的第一作者是Ibilhar。</sample>
    <sample id="96">大家好，我是珍妮，是一名大一的临床心理学研究生，今天我将为您介绍我们团队的研究成果：NLPositionality: 通过数据集和模型的设计偏见进行表征。这项研究是在美国大学和人工智能研究所的支持下完成的，与我们合作的成员包括塞巴斯蒂安·桑提、罗纳德·莱布罗斯、卡塔琳娜·雷因尼策和马arten·萨普。让我们从一个假设开始：您正在为报纸撰写一篇新闻报道，并试图删除评论中的有毒内容。您可能会转向一个流行的AI工具，如Prospect AI的有毒性检测功能。在卡尔·乔姆斯的情况下，Prospect AI能够正确检测有毒实例，但在迪蒂亚·夏马的情况下，Prospect AI对印度情境中更常见的有毒术语并不敏感。这是一个设计偏见的例子，我们看到技术在不同人口群体之间的系统性能差异。这种设计偏见可能源于NLP研究员和模型开发者的观点。观点是指人们持有的观点，这些观点源于他们的 demographics、身份和生活经历。这个概念在女性主义和 queer 学术领域中广泛使用。作为研究员，观点会影响研究过程及其结果，因为它可以改变研究员所做的决策。因此，人们可能会问：数据集和模型是否有观点？我们并不是说数据集和模型本身具有 demographic identities 和生活经历，但它们确实反映了真实人们的判断和观点，从而代表某些观点而忽视其他观点。早期的工作提供了关于观点的轶事证据，例如文化差距和模型和数据集中的偏见，以及理论上的观点模型定义。然而，这些工作并没有将用户与数据集和模型本身进行比较。研究数据集和模型的观点变得越来越重要，因为NLP任务变得更加主观和社会导向。由于并非所有决策都记录在案，许多模型被隐藏在API后面，因此很难确定这些观点是如何扭曲的。为了研究数据集和模型的观点，我们实际上比较了用户的注释与现有的数据集和模型。我们通过一种称为NLPositionality的框架来实现这一点。该框架分为两个主要步骤。第一个步骤是对数据集进行重新注释，以获得多样化的注释者。我们通常不考虑原始数据集的 demographics，因为通常每个注释者只注释每个实例，而 demographics 数据 rarely 被收集和分享。因此，我们选择重新注释数据，以获得每个实例的多个注释者，并获得丰富的 demographics 数据。然后，我们将按 demographics 分类的注释与数据集和模型进行比较，使用皮尔逊的相关系数。因此，我们的框架与注释者分歧文献不同，后者仅关注注释者之间的注释一致性或建模注释者分布，而不是注释者之间的注释一致性。我们的框架主要通过Lab in the Wild在线众包平台实现。Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，相比之下，像MTurk这样的平台主要来自美国或印度的参与者。此外，Lab in the Wild仍然能够提供高质量的数据。我们在Lab in the Wild上托管了两个任务，其中一个任务是社会接受度分析，参与者阅读社会化学数据集中的一个情况，并判断这种情况的社会接受度。为了保持参与度，参与者可以比较他们的答案与AI和其他人的答案。我们随后将这些注释与社会化学数据集的GPD4进行比较。我们还为有毒言论和 hate 言论检测任务进行了类似的设置，参与者阅读Dinah Hate数据集中的一个实例，并判断它是否是 hate 言论。我们随后将这些注释与Dinah Hate、Prospect AI、Rewrite AI和GPD4进行比较。我们的研究和实验共获得了超过16,000个注释，来自87个国家的1000名注释者。现在，我们更好地回答了NLP数据集和模型与哪些用户最一致。我们发现NLP中存在观点。例如，我们发现数据集和模型最接近英语国家。对于GPD4的社会接受度分析，我们发现它最接近于基督教和英语国家。对于Dinah Hate，我们发现它最接近英语国家。我们还发现额外的对齐与拥有大学教育的人有关。对于GPD4的社会接受度任务，我们发现它最接近具有大学或研究生教育的人。我们发现相同的模式适用于Dinah Hate，其中它最接近具有大学教育的人。然而，当数据集和模型与特定群体对齐时，一些群体不可避免地被落下。一个例子是数据集和模型对非二元性别群体的对齐较少，与男性和女性的对应群体相比。我们在GPD4的社会接受度任务中发现这一点，也在Dinah Hate任务分析中发现这一点。因此，鉴于NLP中存在观点，我们有什么可以做的呢？我们有几个建议：首先，记录研究过程中所有相关的设计选择；其次，在观点主义的视角下进行NLP研究；最后，构建针对特定社区的专门化数据集和模型。一个很好的例子是Mussakani倡议，我们想要强调的是包容性NLP不仅仅是让所有人都能使用所有技术。这就是我们的演讲结束的地方，如果您想了解更多信息，请查看我们的仪表板获取最新的分析结果和论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的三个问题：特定架构通常需要额外模块进行优化，训练过程复杂，包括不同的优化目标和多个模型的训练和维护以达到不同的延迟要求。</sample>
    <sample id="98">在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法包括：1. **数据清洗**：识别并删除或修改数据集中明显偏见的内容。2. **数据平衡**：确保数据集中不同观点和来源的平衡，避免过度代表某一特定群体或观点。3. **数据多样化**：使用多样化的数据源和背景来训练模型，以减少对单一视角的依赖。4. **持续监控和评估**：定期评估模型的性能和偏见，确保其在处理各种输入时的一致性和公正性。5. **透明度和可解释性**：保持数据和模型训练过程的透明度，以便更好地理解和解决潜在的偏见问题。6. **社区参与**：与多样化的社区和专家合作，提供反馈和建议，帮助识别和纠正数据集中的偏见。7. **使用无偏见的评估标准**：设计评估标准时，避免内置任何偏见，确保模型的性能评估是公正和客观的。8. **持续更新和迭代**：随着新数据和研究的出现，不断更新和迭代数据集和模型，以适应不断变化的社会和政治环境。</sample>
    <sample id="99">Hi, I'm Siu Yu Yan from Fudan University. I am here to introduce our work on distilling script knowledge from large language models for constrained language planning. In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripts. Previous work has explored using language models to plan for abstract goals of stereotypical activities, such as making a cake, and shown that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for abstract goals of stereotypical activities; planning for specific goals with specific constraints, such as making a chocolate cake, still remains understudied. In this paper, we define the problem of constrained language planning, which imposes different constraints on the goal of planning. An abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should write scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using InstructGPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then, we conduct detailed analysis to investigate why large language models fail. The figure shows that the semantic completeness in generated scripts is acceptable, but the faithfulness to the constraints cannot be guaranteed. We dig into more fine-grained topological categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of InstructGPT varies considerably for goals of different categories. Previous studies have shown that the output quality of large language models for high variance, leading to bad performance. Thus, we adopt the idea of an over-generated Z-filter to improve generation quality. We first show constrained types with examples for InstructGPT and obtain specific goals based on the said abstract goals. Then, InstructGPT overgenerates key scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating datasets is an essential step to this end. However, previous studies do not enable planning for specific goals, and manual manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning datasets from large language models. We apply our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts to ensure the quality of validation and test sets. We ask crowdsourced workers to find and revise the incorrect samples. This figure shows the constrained distribution of CoScript. We find CoScript shows high productivity in the generated specific goals. With CoScript, we can train smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger larger models when properly trained on suitable datasets. In summary, we establish the constrained language planning problem. We evaluate the constrained language planning ability of large language models and develop an over-generated Z-filter method for large language models. We use a large language model to generate a high-quality script dataset, CoScript, for constrained language planning. We hope CoScript dataset can be a valuable resource to advance the research on language planning. Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="100">Multi-hop QA is about answering questions that require multiple reasoning steps to solve. Each step typically corresponds to a document in the corpus. For example, to answer the question "What 1988 Christmas comedy film did Brian Doyle Murray star in?", we first need to find all the movies that Brian Doyle Murray starred in, and then identify the movie released in 1988. This set of documents required to answer a question is called the chain. Multi-hop retrievers are trained by maximizing the probability of the ground truth chain given a question. For instance, Q1C1, Q2C2, etc., where Q is a question and C is the goal chain. Retrievers are trained by maximizing the probability of CI given QI. Most state-of-the-art multi-hop retrievers fall under this paradigm. Existing systems require thousands of examples of questions and ground truth chains for good performance, which can be expensive, especially for low-resource domains and domains requiring special expertise. Our approach, PromptRank, is efficient and provides good performance with as few as 128 examples, addressing this issue. The idea is to combine an unsupervised retrieval method with a few-shot language model-based reranking. There are two main steps: retrieve a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and rerank these candidates using the few-shot language model reranking. Two points to consider here: what scoring function should we use and how do we prompt the language model to extract this score? We use the likelihood of the question given the chain according to a language model. Given the language model, we have a chain prompt that I will explain how to construct in a few slides. And given the question, we score CI as the probability of the question given the chain prompt. Let's go through a working example. Given this question and its underlying corpus, we retrieve initial documents using TF-IDF, then expand and prune chains by following hyperlinks, then convert each of the unpruned chains to prompts, and then score each chain by the probability of the question given the chain prompt. How do we construct a chain prompt? Given this question and this chain, what we do is we have a prompt that looks like this, where we have inserted the chain documents into the prompts and we have an indicator token to designate that this is a document. And we have an instruction, which in our case here is something like read the previous documents and ask a question. And the instruction serves to elicit the language model's reasoning ability over the chain documents. We explore additional techniques like instruction search to find optimal instructions and instruction sampling, where we compute chain scores by aggregating multiple scores computed with different instructions, and also temperature scaling, where the language model logits are scaled by some constant temperature. We experiment with GPT-2 and T5 and evaluate our approach on Hotpot-QA, and as for metrics, we use retrieval R@K, recall at K, and answer recall at K. As for instruction search, we generate 200 diverse instructions and evaluate each on a set of 128 examples. All prompt ranking experiments use only 128 examples in total. So let's start with the retriever results. We see that PromptRank performs outperforms fully supervised systems like DocRED and performs comparably to state-of-the-art multi-hop retrievers. We also learn an ablation to verify the importance of each component we propose, and we find that each component definitely plays a role in the performance of the final performance of PromptRank. We also evaluate the downstream QA performance when using PromptRank as the retriever, and so we use a reader model which is EleutherAI LLaMA and we combine it with PromptRank. And we see that PromptRank exhibits very good downstream QA multi-hop QA performance, underperforming in DR by only around four exact match points. Check out our paper for more results and extensive analysis. To summarize, language models can be used for few-shot ranking of candidate paths for multi-hop QA. PromptRank exhibits strong few-shot path retrievable performance compared to fully supervised systems. The likelihood of the question given the chain works significantly better than as a scoring function than the reverse. The instruction plays a strong role in eliciting language models' reasoning abilities over the chain documents. With that, I conclude my talk and thank you so much for listening.</sample>
    <sample id="101">PaLM 的流畅度与当前最好的系统相当，但其准确性存在一些问题。</sample>
    <sample id="102">水印方法的重要属性是它具有强大的检测性能，同时保持对底层任务的高可用性。这表明该方法能够有效地检测潜在的水印内容，而不会显著影响模型在其他任务上的性能。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">通常每个实例只由少数注释员注释，因此为了获得更丰富的人口统计数据，我们选择重新注释数据集。</sample>
    <sample id="105">余弦和L2相似度被用于衡量良性和后门数据集之间的差异。</sample>
    <sample id="106">该视频介绍了一篇名为“QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations”的论文，该论文由Chaitanya Malaviya、Peter Shaw、Ming-Wei Chang、Kenton Lee和Kristina Toutanova合作完成。该论文展示了在处理具有隐含集合操作的查询时遇到的挑战，并提出了一个名为QUEST的检索数据集。QUEST是一个包含超过三百万个实体查询的检索数据集，其中查询包含隐含集合操作。数据集中验证了答案实体的相关性，并标记了与不同查询约束相关的文档中的可归属文本片段。该数据集旨在解决检索系统中处理具有隐含集合操作的查询的挑战。该论文还介绍了构建QUEST的过程，包括使用维基百科类别名称生成查询，以及通过人类注释者验证答案实体的相关性和可归属性。最后，该论文展示了QUEST数据集的性能基准，并发现查询中具有集合交集和集合差集操作的查询特别具有挑战性。该论文旨在帮助信息检索领域的研究人员开发更好的系统以满足具有选择性信息需求的信息检索场景。</sample>
    <sample id="107">基于编码器的多语言模型，如多语言预训练编码器（MLP）和BERT，可以用于这项任务。这些模型在多种语言上进行预训练，然后在特定语言上进行微调，以实现跨语言查询到SQL的翻译。</sample>
    <sample id="108">该视频是一场关于ACL 2023年论文“语言模型接受度判断并非总是对语境稳健”的学术演讲。演讲者Koustuv Sinha首先介绍了论文的主要贡献，即重新评估最小对偶 paradigm，这是一种评估语言模型接受度的框架。在传统的最小对偶 paradigm中，模型被要求根据可接受和不可接受的句子来预测概率，以评估其对可接受性的理解。然而，这种评估方法仅适用于较短的句子，而现代大型语言模型通常处理更长的句子。因此，研究旨在通过让模型评估更长的句子序列来改进评估框架。

演讲者解释了他们如何通过模拟更长的句子序列来修改数据集，这些序列可能来自相同的或不同的数据集。他们使用了Blimp数据集中的例句，创建了可接受和不可接受的句子的组合，以测试模型在不同语境下的接受度。此外，他们还考虑了完全 unrelated的领域，如维基百科，以评估模型对不同上下文的敏感性。

演讲者展示了实验结果，发现当模型处理更长的句子时，其接受度判断会显著波动，尤其是在匹配结构的情况下。他们还进行了分析，通过在输入句子中添加噪声来测试模型的鲁棒性，发现模型对噪声的敏感性与对可接受和不可接受句子的敏感性一致。最后，演讲者强调了语言模型对共享的句法和语义特征的敏感性，并建议当前的MPP评估方法可能无法充分捕捉模型在整个语境窗口内的抽象知识。</sample>
    <sample id="109">演讲者介绍了一个名为“自然指令”的数据集，它包含大量自然语言指令及其相应的输入和输出。该数据集通过让预训练的GPT-3模型生成指令和输入来收集，然后由模型生成相应的输出。为了增加多样性，每个指令还被生成了多个变体。分析显示，超过50%的生成示例是正确的，即使不正确的示例也提供了有价值的信息。使用“自然指令”数据集微调的模型在各种基准测试中表现出色，甚至超越了使用“超自然指令”数据集训练的基线模型。这表明，语言模型可以生成多样性和创造力，这对于自然语言处理任务至关重要。</sample>
    <sample id="111">作者通过收集一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="112">今天我要给大家介绍我们的一篇论文，标题是“CoNLL-2003命名实体标注器在2023年还能正常工作吗？”我的名字叫刘树恒。我们将探讨通用化问题，特别是在命名实体识别任务（NER任务）上的问题。我们观察到，自2003年以来，模型一直在使用CoNLL-2003数据集开发命名实体标注器。这自然提出了几个问题：这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么条件才能实现良好的泛化？如果我们确实观察到了性能下降，那么是什么原因导致了这些模型的性能下降？为了研究这些问题，我们开发了CoNLL++数据集。这是从2020年的路透社新闻中收集的数据集，并使用与CoNLL-2003相同的标注指南进行标注。我们随后在CoNLL-2003上微调了超过20个模型，并在CoNLL-2003测试集和CoNLL++测试集上评估了它们。最后，我们计算了每个模型的F1分数的变化百分比，以评估其泛化能力。那么，对于良好的泛化需要什么条件？通过实验，我们发现有三个主要因素是必要的。第一个因素是模型架构。通过实验，我们发现Transformer模型通常在新数据上泛化得更好。第二个因素是模型大小。我们发现通常较大的模型会导致更好的泛化。第三个因素是微调示例的数量。我们知道微调示例的数量直接影响下游任务的表现。在这里，我们也发现更多的微调示例实际上也导致了更好的泛化。接下来，我们要回答的问题是：有些模型性能下降的原因是什么？我们有两个假设：第一个假设是自适应过拟合，即由于多次使用相同的测试集而导致的过拟合，这通常表现为在新测试集上的性能下降。第二个假设是时间漂移，即由于训练和测试数据之间的时间间隔增加而导致的性能退化。对于自适应过拟合，我们看到图表右侧的红色最佳拟合线的斜率大于1，这意味着我们在CoNLL-2003上每单位改进都会在CoNLL++上转化为超过1单位的改进，这表明自适应过拟合在这种情况下并不明显。那么时间漂移呢？为了研究时间漂移，我们进行了实验，即重新训练或继续预训练一些模型以使用更近的数据。我们发现随着时间间隔的增大，性能会退化。这证实了我们的假设，即时间漂移是性能下降的主要原因。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型大小以及更多的微调示例。这些因素相互关联，我们不能仅仅依赖其中一个因素。同时，我们还发现性能下降是由时间漂移引起的，而不是自适应过拟合，尽管CoNLL-2003已经使用了20多年。回到我们论文的标题提出的问题，“CoNLL-2003标注器在2023年还能正常工作吗？”答案是令人满意的“是”。我们希望这篇论文能促进更多关于如何提高模型泛化性的研究。最后，请务必查阅我们的论文、数据集，并如果您有任何问题，请随时联系我。谢谢！</sample>
    <sample id="114">The video introduces a study on large language models (LLMs) presented at ACL 2023, titled "Finding the Pillars of Strength for Multi-Head Attention." The researchers from Nanyang Technological University in Singapore discuss the challenges and limitations of LLMs, such as heavy parameters, long training times, and the need for extensive data. They focus on addressing the issue of heavy parameters by exploring different methods to optimize multi-head attention mechanisms.

The first method discussed is homogenization-based optimization, which aims to make heads more similar but often sacrifices performance. The second method is diversification-based optimization, which makes attention heads more diverse but lacks parameter efficiency due to lack of model compression. The third method involves giving significant scores to each head and pruning those with low scores, though this approach remains computationally expensive.

The proposed solution is Group Head Attention (GHA), which uses a divide-and-conquer strategy to compress multi-head attention. This method divides attention heads into groups during group constraint training, making intra-group heads more similar and inter-group heads more separate. After group constraint training, the voting-to-stay algorithm prunes redundant heads within each group, retaining only one head per group. This process significantly reduces the number of parameters while maintaining performance.

The study evaluates three tasks: machine translation, language modeling, and abstract summarization. The GHD and GHP models, derived from group constraint training and the voting-to-stay algorithm respectively, show improvements in performance and parameter compression compared to baseline models. For example, the GHP model achieves up to 32.1% parameter compression with comparable performance in machine translation.

The video concludes by highlighting the potential of task-specific automatic pruning, suggesting that networks can be pruned without sacrificing performance, similar to how unused apps are removed from smartphones to improve usability. The researchers believe that this approach can help reduce the redundancy in real-world applications where only a few tasks are needed, thus improving efficiency and user experience.</sample>
    <sample id="115">该方法使用的语音片段大小是λ。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是一名 judge。</sample>
    <sample id="117">示例的质量比与源句子的相似度更为重要。</sample>
    <sample id="118">The video presents a research submission on improving pre-training techniques for code-switched NLP, specifically focusing on the challenges of building computational models for code-switching in linguistically diverse communities like India. The speaker defines code-switching as a common occurrence where sentences mix languages, such as English and Hindi, and explains that multilingual pre-trained models like MBERT and XLM-R do not perform well on code-switched tasks like question answering and sentiment analysis.

The main contributions of the work are proposing novel MLM (Masked Language Modeling) techniques tailored to code-switching and architectural changes with auxiliary losses also tuned for code-switching. The proposed Switch-MLM technique defines switch points, which are groups of two tokens transitioning between languages, and only these words are masked. This approach requires access to an LID (Language Identification) dataset or LID tagger, but a surrogate method called Frequency-MLM is proposed, which uses the negative log likelihood of words in monolingual corpora to assign LID tags.

Architectural modifications include residual connections from intermediate layers to the final layer to increase switch point information and imposing an auxiliary LID-based loss to force the model to learn more LID information. The results show that the combined method of Switch-MLM or Frequency-MLM with residual connections and auxiliary losses performs best on sentiment analysis tasks across language pairs.

Probing experiments using linear and conditional probing verify the claim that the proposed methods increase switch point information in intermediate and final layers. Linear probing results show that standard MLM combined with switch-MLM representations has more switch point information than standard MLM alone. Conditional probing results confirm that the proposed methods indeed increase switch point information compared to a baseline model.

In summary, the research proposes a new MLM objective tuned for code-switching and hypothesizes that the methods increase switch point information in intermediate layers, motivating architectural changes and auxiliary losses to enhance this information content.</sample>
    <sample id="119">在扩展实验中，论文侧重于进一步预训练语言模型的六种不同派别子语料库。</sample>
    <sample id="120">该模型使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括通过歌曲名称“Easy on Me”或其位置（如“第一首”）来指代歌曲。</sample>
    <sample id="122">根据所给的英文内容，论文的作者来自复旦大学。</sample>
    <sample id="123">演讲者介绍了他们的研究，重点是通过指令调优来改进多模态零-shot学习。他们讨论了大型语言模型在使用预训练模型进行不同下游任务时的潜力，并提到了最近的研究，这些研究显示指令调优使大型语言模型能够通过遵循自然指令来执行任务。然而，他们注意到大多数先前的工作主要集中在语言任务上，而计算机视觉和多模态任务则被忽视。因此，他们旨在探索是否可以使用指令调优来提高多模态预训练模型在多模态任务上的泛化能力。他们还提到了一个显著的差距，即在多模态任务中可用的指令数据集与自然语言处理（NLP）任务相比要少得多。他们提出了MultiInstruct，这是第一个包含62个不同多模态任务的多模态指令调优基准数据集，每个任务有5个专家提供的指令。他们使用OFA作为基础模型，并通过将所有输入和输出数据类型统一为序列到序列格式来统一处理各种输入和输出数据类型。他们报告了在测试集上的性能指标，包括准确率、BLEU分数和敏感性。他们的结果表明，指令调优可以显著提高OFA在多模态任务上的性能，而从自然语言处理数据集中转移学习也可以进一步提高性能和降低敏感性。</sample>
    <sample id="124">演讲者分享了他们关于大型语言模型（LLMs）的时间推理能力的基准测试和改进工作的研究。他们首先将时间推理分为三个层次：从年份到年份、从事件到事件和从事件到事件。他们发现之前的模型主要关注第一个层次，而他们的目标是更全面地研究时间推理。

他们通过一个初步实验评估了三个LLMs（T5L、Fine-tuned自然语言和ChatGPT）在时间推理任务上的性能，使用了年份预测问题作为示例。结果表明，前两个LLMs在2000年到2020年之间表现出色，但ChatGPT在处理月度预测时表现不佳。

为了更全面地研究时间推理，他们创建了一个名为“temp reason”的数据集，覆盖了所有三个层次的推理和长时间范围。他们还提出了三种问题设置：封闭式问题、开放问题和推理问题，以评估LLMs在不同场景下的时间推理能力。

为了提高LLMs的时间推理能力，他们提出了两种训练策略：时间敏感预训练和时间敏感强化学习。时间敏感预训练旨在通过在原始文本中构造时间实体来增强模型对时间信息的理解。时间敏感强化学习则通过奖励正确预测并惩罚错误预测来优化模型的性能。

实验结果表明，ChatGPT在时间推理任务上表现不佳，而经过“temp reason”数据集微调的T5SFT和T5F在不同问题设置下显著提高了性能。然而，T5F在不同时间周期上也表现出波动性，这可能与训练数据不平衡有关。未来的工作将专注于克服这些推理偏见。

总之，演讲者分析了LLMs的时间推理偏见，并提出了一个全面的时间推理基准数据集和一种训练策略，以改善LLMs的时间推理能力。</sample>
    <sample id="125">根据幻灯片，这篇论文有五位作者：Yanis Labrak, Adrien Bazille, Richard Dufour, Mickael Rouvier, 和 Emmanuelle Morin。</sample>
    <sample id="126">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="127">The video presents a research paper by Namguyu Ho, Laura Schmid, and Se-Young Yun from KAIST AI, titled "Large Language Models as Reasoning Teachers." The paper introduces a method to transfer the reasoning abilities of large language models (LLMs) to smaller models. The authors address the issue that existing techniques for complex reasoning tasks, such as Chain of Thought (CoT), only work on huge models like GPT-3 or PaLM, which are costly or impossible to deploy in many situations.

To solve this problem, the researchers propose using large models as reasoning teachers to teach smaller models. They also introduce a novel technique called diverse reasoning, which generates multiple step-by-step solutions for complex questions using stochastic temperature sampling. This approach allows the student model to learn from a variety of solutions, improving its ability to perform complex reasoning tasks.

The paper compares the proposed method with existing baselines on 12 tasks and demonstrates that it achieves notable performance improvements, especially in text-based tasks like data understanding and coin flip. The method significantly outperforms vanilla fine-tuning on most tasks, even with the smallest model that has 0.3 billion parameters.

The authors discuss the scalability of their method and highlight potential trade-offs between development costs, inference time costs, and the quality of inference. They encourage readers to check the paper for more details and provide code and data from all experiments, including those involving open-source models.</sample>
    <sample id="128">演讲者介绍了一项名为“KipMOS”的研究，该研究评估了从多个来源整合知识的能力。这项工作是McGill大学、Mila和微软研究之间的合作项目。自然语言理解模型通常依赖于预训练和输入时提供的各种知识来源。然而，对于像问答这样的任务，模型可以利用预训练知识来解决问题。然而，自然语言理解往往需要在推理过程中提供的实时知识。例如，在句子“John saw the newly elected president on TV”中，预训练参数可能包含关于总统和电视的信息，但无法确定具体实体John或新任总统的身份，因为这些信息可能自预训练以来发生了变化。

因此，对于知识密集型NLU任务的成功模型需要具备在预训练时间和推理时间整合和使用知识的能力。在这项工作中，提出了一个诊断测试套件来评估知识整合能力。引入了一个共指消解任务，旨在测试模型从不同来源获取知识的能力。数据集通过人类标注的样本进行评估，并建立了共指消解模型。一个示例任务涉及识别代词“他”所指的具体实体，答案是“Servin”，基于背景知识（Servin是一名法官）和实体特定知识（Servin是一名法官）。任务的解决需要两种类型的信息：实体特定知识和背景知识。

研究定义了三种KipMOS设置：1. 背景预训练（背景知识仅在预训练时可用），2. 背景和推理（背景知识在预训练和推理时都可用），3. 推理背景（所有知识类型仅在推理时可用）。在背景预训练设置中，假设背景知识（如政治家寻求政府职位）包含在预训练参数中。在推理背景下，提供实体特定知识（如Chesterfield是一名政治家），以及背景知识（如政治家在推理背景下参与政治）。在推理背景设置中，提供虚构职业（如Mertura是一名政治家），因为Mertura很可能不会出现在预训练参数中。

通过人类标注样本评估数据集，并建立共指消解模型。结果表明，在最困难的背景预训练设置下，没有任务特定训练的模型表现不佳。然而，经过KipMOS训练后，C2F和B2F模型显著优于随机选择。这表明，通过在KipMOS等一般共指消解数据集上进行任务特定训练，模型学会利用表面线索，这些线索在KipMOS测试中已被移除。进一步实验表明，即使是最优模型也难以可靠地整合仅在推理时提供的背景知识。总之，许多共指消解模型在没有任务特定训练的情况下无法从不同来源整合知识。然而，通过任务特定训练，一些模型成功地整合了来自多个来源的知识。尽管如此，即使是最优模型，似乎在可靠地整合仅在推理时提供的背景知识方面存在困难。</sample>
    <sample id="129">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例包括黑人女性、拉丁裔女性和亚洲女性。这些群体被标记是因为它们与社会主流或未标记群体（如白人男性）相比具有不同的身份特征。</sample>
    <sample id="130">在所提供的内容中，未明确指定哪些模型架构泛化能力较差。然而，可以推断出，与较大模型相比，较小的模型可能泛化能力较差，因为较大的模型通常具有更好的泛化能力。</sample>
    <sample id="131">在视频中没有提到测试数据集的名称。</sample>
    <sample id="132">根据所提供的信息，这篇论文有三位作者：马查塔、马尔丁和作者。</sample>
    <sample id="133">作者采用了多种模态，包括文本、图像和坐标框。</sample>
    <sample id="135">本视频展示了Emory NLP实验室与亚马逊Alexa AI合作开发的ABC-Eval方法，这是一种评估对话型人工智能（AI）的维度化方法。该方法旨在通过明确标注模型响应的行为来减少人类评估的主观性，从而更精确可靠地评估对话质量。ABC-Eval评估了模型在各种行为上的表现，如提供不相关的信息、自相矛盾或违反共同知识。研究结果表明，ABC-Eval的行为标签比现有方法更可靠，并且对整体对话质量的预测能力更强。例如，衡量自相矛盾的次数可以解释对话质量的5%和10%，而平均一致性评分只能解释4%或更少。此外，ABC-Eval的指标组合可以解释超过25%的对话质量，而单独使用任何一种指标都会损失大量信息。这些可靠的指标使我们能够以比以往任何时候都更高的分辨率评估对话型AI。</sample>
    <sample id="136">The presentation introduces a new evaluation framework called FERMAT, which aims to assess the mathematical abilities of language models. The speaker, Jashan Alex Shakhun and his supervisor Nafisa Sadat Moosavi from the University of Sheffield, discuss the limitations of current benchmarks in evaluating numerical reasoning tasks. They highlight that existing benchmarks primarily provide accuracy scores and f1 measures, which do not effectively capture the strengths and shortcomings of models in mathematical operations.

FERMAT is designed as a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. It involves math worded questions extracted from resources like Illinois and Common Core, with variations such as changing number representations (e.g., 5.0 instead of 5), using large and small integers, and decimals. This approach allows for a more comprehensive assessment of a model's capabilities in real-world scenarios.

The presentation also covers the results of baseline evaluations, where most models perform poorly across various aspects of mathematical reasoning. Fine-tuning with math teachers' templates shows promising improvements, particularly in handling different number representations and mathematical operations. However, the study finds that even after fine-tuning, models struggle with exact expressions, suggesting that linguistic nuances play a significant role.

Finally, the impact of training templates is examined, showing that diversity in language and mathematical operations significantly improves performance. The conclusion emphasizes the need for more representative benchmarks and highlights the importance of language and mathematical diversity in improving model performance. The presentation concludes with a QR code and links to the paper for further reading.</sample>
    <sample id="137">本研究介绍了一种名为“Tell2Design”的数据集，用于语言引导的地板平面生成。该数据集包含5051个人标注的自然语言说明和76000个由预定义模板生成的人工语言说明。目标是通过将自然语言说明转换为结构化的地板平面布局来训练模型，以满足特定的设计要求。研究展示了使用Transformer编码器-解码器结构构建的序列到序列模型在生成高质量地板平面方面的能力，与文本条件图像生成基线相比表现 superior。此外，通过使用人工语言说明作为预训练数据，显著提高了模型在处理人类语言说明时的表现。研究结果表明，“Tell2Design”数据集和所提出的方法为语言引导设计生成任务提供了有价值的资源和基准。</sample>
    <sample id="138">作者认为在 NLU 中研究不足的领域是多源知识整合，特别是在任务特定训练中。</sample>
    <sample id="139">演讲者的名字是“Zhiyang Xu”，“Ying Shen”和“Lifu Huang”。</sample>
    <sample id="140">是的，Coscript 经过了质量检查。在验证和测试集中，云托管工人被要求找到并反馈 Coscript 中的错误样本。这确保了 Coscript 的高质量，因为它包含了高质量的特定目标和脚本，这对于验证和测试至关重要。</sample>
    <sample id="141">根据所给内容中的英文信息，对于依赖上下文的翻译，现有的资源局限性在于它们在处理某些特定的翻译现象时，如正式性和词汇连贯性，比不使用上下文的模型更准确。然而，在其他翻译现象如省略、词性和语态方面，使用上下文的模型并没有显示出明显的改进。这表明在文档级别翻译中需要进一步的研究和进展。</sample>
    <sample id="142">resolves indirect referring expressions for entity selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis Google Research Hi, I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the AltEntities Corpus. My name is Javad Hosseini, and this is a joint work with Filip Radlinski, Silvia Pareti, and Annie Louise. Our goal is to understand users' language when they want to make a choice. Consider this alternative question: Did you mean Easy on Me or I Got a Feeling? Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song Easy on Me or its position, the first one. But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate or when the user wants to specify a preference. Here are some examples of indirect references. For example, the newer one or the song that's not energetic. This is an important problem in conversational systems and also for benchmarking LLMs' entity understanding. We're not aware of a public dataset, a large-scale public dataset for the task, so we collected one using crowdsourcing. Our dataset covers three different domains: music, books, and recipes. Our dataset collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday." And with that, Bob sets the dialogue context. In this, in the second speech bubble, Alice says, "Do you mean Easy on Me or I Got a Feeling?" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, the newer one. We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template, "Do you mean A or B?" where A and B are samples from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other, and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name The Return. The third one is when they have similar descriptions on Wikipedia, and finally, when they have similar infoboxes or attributes on Wikipedia, for example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song and read about each song. Here's, for example, the Google search result for the song Easy on Me. For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images again from Wikipedia so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, the one without words, not the one with the twelve-year-old twelve-year-old boy or the fictional one or comes from Otherworld and so on. The AltEntities Corpus has 6,000 alternative questions across three domains and it has 42,000 indirect referring expressions. Results with T5-XXL model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high, it's around 92 to 95 percent. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82 to 87 percent, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60 percent. So there's a lot of room for improvement. We've also shown that the models are domain generalizable. Here is a link to our dataset. Thanks</sample>
    <sample id="143">该方法与现有的 SimulST 策略进行了比较，包括 Whitaker 策略和 Local Alignment。此外，还与专门针对 SimulST 的 State-of-the-Art 架构进行了比较。</sample>
    <sample id="144">论文的作者来自多个机构，包括里昂第一大学、里昂第一大学医院、里昂第一大学生物医学中心、里昂第一大学生物医学中心、里昂第一大学生物医学中心和里昂第一大学生物医学中心。</sample>
    <sample id="145">演讲者的名字是Yan Liu。</sample>
    <sample id="146">My name is Vasudeva, and I am a PhD candidate in Computer Science at Stony Brook University. I am presenting my work accepted into ACL 2023 as a long paper titled "Transfer Learning for Discourse Detection: Addressing the Rare Class Challenge." The paper defines cognitive dissonance, which occurs when two beliefs or actions are inconsistent, such as someone stating they know cigarettes can kill them but then smoking after a meeting. Cognitive dissonance is important to study because it helps us understand the effects of disagreement among people, track changes in beliefs, values, and attitudes, and is related to anxiety disorders. Studying dissonance expressed in language can also help understand extremism and polarization in vulnerable groups. To create a cognitive dissonance resource, we conducted a large-scale annotation of discourse units using a discourse-first approach. However, only 3.5% of annotated pairs showed dissonance, making it challenging to train an initial classifier with just 43 examples. We experimented with transfer learning and active learning to annotate more dissonant samples efficiently. By transferring weights from closely related tasks like debate statement classification and binary classification of expansion and comparison classes, we achieved better zero-shot performance than chance. We found that cumulative updating performed equally or better than iterative updating across different strategies. Our proposed probability of rare class strategy (PRC) works better than other state-of-the-art strategies for rare class acquisition, although the difference is small. PRC has the highest percentage of dissonance and works best for rare class detection, but annotators find the examples difficult. Overall, our work demonstrates the feasibility of using transfer learning and active learning to address the rare class challenge in cognitive dissonance detection.</sample>
    <sample id="147">这篇论文有三位作者。</sample>
    <sample id="148">嗨，我是萨拉·帕皮，来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍我们共同撰写的“注意力作为实时语音翻译的指南”一文。该文是与马泰奥·内格里和马可·图尔奇合作完成的。实时语音翻译，或简称ST，是指在实时翻译 spoken language into另一种语言的文本，从而实现跨语言交流的过程。当前实时语音翻译模型面临的问题是什么？通常使用特定架构训练模型，引入额外模块进行优化，导致复杂的训练过程。例如，训练涉及不同的优化目标，以及训练和维护多个模型以达到不同的延迟要求，例如训练一个平均延迟为1秒的模型，另一个为2秒的模型等等。我们的解决方案是什么？首先，使用现有的 offline 模型，无需重新训练或采用特定架构用于实时语音翻译。使用单一模型处理每个延迟要求，并通过特定参数处理延迟。利用模型已有的知识，通过音频输入和文本输出之间的注意力机制来实现。您可以在右侧看到一个示例。我们的解决方案是提出一种称为“编码-解码器注意力”的策略。这是一种策略，我们决定是否发出部分翻译，基于注意力指向哪里。如果注意力未集中，即总和低于某个阈值α，那么最后λ个语音帧的信息不够稳定，因此不会发出最后一个词。例如，如果我们收到一个包含“我要谈论的话题”的语音片段，我们的模型预测了德语翻译。我们将查看交叉注意力权重，发现第一个两个词指向最早收到的语音帧，而最后一个词指向最后收到的语音帧。这意味着前两个词将被发出，因为交叉注意力总和高于阈值α，所以最后一个词不会被发出，我们将等待下一个语音片段。如果我们继续接收下一个语音片段，我们的模型预测另外三个词，我们将查看交叉注意力权重。我们将看到没有词指向最后λ个语音帧。这意味着这三个词将被发出。如果我们查看主要结果，我们将绘制实时语音翻译结果的图表，在图表中，蓝色表示翻译质量，平均延迟是延迟度量，我们还考虑了计算时间平均延迟，这反映了模型预测输出所需的时间。我们希望曲线尽可能高，但也要向左移动。我们将其与应用到 offline 模型的其他策略进行比较，这些策略包括“Whitkey”策略和“局部调整”策略。我们还将与专门针对实时语音翻译的最新架构进行比较。这是我们在德语上应用实时语音翻译策略的主要结果。我们看到，ADT优于所有应用于 offline 模型的策略，因为它们的曲线向左移动。我们还看到，如果我们考虑实际延迟时间或计算时间延迟，ADT是最快的策略。如果您想了解更多信息，请阅读我们的论文，我们还发布开源代码和模型以及实时语音翻译结果，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="149">是的，数据集公开了。</sample>
    <sample id="150">MeetingQA是一个新的数据集，专注于基于会议转录的提取式问答回答。该数据集包含7.7万道问题和答案，分为训练、开发和测试集。问题和答案的长度分别为12和35个单词，其中40%的问题有多个答案，48%的答案涉及多名演讲者。数据集的问答类型包括是/否问题、详细回答和观点寻求，其中20%的问题是 rhetorical 的，70%的多说话者答案包含一些 disagreement。MeetingQA在测试集上的表现与人类相当，F1值为84.6%，展示了其在问答任务上的有效性。</sample>
    <sample id="151">然而，大多数之前的指令调优工作专注于提高零-shot性能，而计算机视觉和多模态任务则被忽视。因此，在这项工作中，我们想探索是否可以通过指令调优来提高多模态预训练模型在视觉多模态任务上的泛化能力。此外，在我们研究期间，我们发现多模态指令数据集的可获得性存在显著差距。目前有超过1600个语言-only指令任务，但没有大规模公开可用的多模态指令任务。这激励我们构建一个多模态指令调优数据集。在这里，我们介绍了MultiInstruct，这是第一个多模态指令调优基准数据集，包含62种不同的多模态任务，涵盖10个主要类别。这些任务是从21个现有的开源数据集中派生出来的，每个任务配备了5个专家提供的指令。为了在我们提出的基准数据集上评估多模态指令调优，我们采用了OFA作为基础模型。OFA使用统一的词汇表对语言、图像和坐标进行编码。这里展示了我们多模态指令数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循了OFA的方法，并将所有任务格式化为统一的序列到序列格式，在这种格式中，输入文本、图像、指令和坐标框都在相同的词嵌入空间中表示。现在我将讨论多模态指令调优。对于训练数据集，我们使用了NLP组中的53个任务进行训练，并从每个任务中随机采样了10,000个实例。对于测试，我们保留了整个Common Sense Reasoning组进行测试，并从Wikidata和Miscellaneous组中选择了额外的5个任务。我们使用测试集中的所有实例对每个任务进行测试。此外，我们从自然指令数据集的测试集中随机采样了20个任务作为NLP任务的无监督任务。我们使用预训练的OFA大型模型作为基础模型。在训练过程中，我们将每个任务的所有实例随机与其中的一个指令模板组合在一起。因此，在测试阶段，对于每个任务，我们进行五次实验，通过评估模型在每次实验中使用其中一个指令时的表现，来报告最小和最大表现以及表现的标准差。如果任务是多模态分类任务，我们将报告准确性；如果是多模态生成任务，我们将报告ROUGE-L；如果是NLP任务，我们将报告ROUGE-L。我们还引入了一个额外的评估指标叫做敏感度，它衡量模型在给定任务中对指令微调的输出一致性。我们的主要结果是，我们可以看到指令调优可以显著提高OFA在某些多模态任务上的性能。此外，从自然指令数据集的可学习性可以提高指令调优。正如我们所见，随着任务数量的增加，模型在同时降低敏感度的同时取得了更好的性能。我们还进行了一个实验，我们使用一个指令与五个指令进行比较。正如我们所见，使用更多指令可以提高模型的整体性能并显著降低其敏感度。这展示了不同调优策略对模型敏感度的影响。正如我们所见，通过从自然指令数据集的可学习性，模型可以实现比原始OFA模型更好的敏感度。我们还可以看到，通过从自然指令数据集的可学习性，OFA可以实现比原始OFA模型更好的性能。总的来说，我们提出了第一个大规模多模态指令调优数据集，显著提高了OFA的零-shot泛化能力，并探索了不同的可学习性技术及其益处。我们设计了一个新的指标叫做敏感度。最后一点，我们正在收集一个更大的多模态指令调优数据集，包含大约150个额外的维吾尔语任务，我们将在稍后发布它们。这是我们的代码，用于数据和模型。谢谢。</sample>
    <sample id="152">The presentation introduces the development of new language models specifically designed for classical philology, focusing on ancient Greek and Latin texts. The speaker highlights the current landscape of language models in classics, noting the introduction of several models such as LatinBERT (2020), AncientGreekBERT (2021), and another AncientGreekBERT (2022). However, these models are primarily encoder-only and monolingual, which limits their utility for scholars who may want to work with both ancient Greek and Latin texts.

The project aims to address this gap by creating multilingual models pre-trained on ancient Greek, Latin, and English data. Two monolingual models were pre-trained: GReBERTa for ancient Greek and GReTER for understanding and generating ancient Greek texts using the T5 architecture. Additionally, two multilingual models, PhilBERTa and PhilTER, were developed to handle ancient Greek, Latin, and English texts.

The process involved gathering pre-training data from various sources, including OpenGreek and Latin, and a new corpus from the Internet Archive. The Internet Archive was leveraged to create a high-quality pre-training corpus by identifying incorrectly transcribed Greek stop words and re-scanning books with Greek OCR settings enabled.

Benchmarking was conducted using the Universal Dependencies treebanks for Greek and the evaLatin 2022 dataset for Latin. The results showed that the models outperformed the current state of the art in speech tagging, dependency parsing, and lemmatization. The encoder-decoder models also demonstrated strong performance in semantic and world knowledge capabilities, such as distinguishing synonyms from antonyms and identifying relations between heroes and gods.

The analysis revealed that there was no significant difference in performance between the multilingual and monolingual models for semantic and world knowledge tasks. The presentation concludes by emphasizing the creation of powerful language models for classical philology, capable of processing both ancient Greek and Latin texts through a single model.</sample>
    <sample id="153">Ninareh Mehrabi在亚马逊Alexa AI的可解释AI团队中是一名博士后科学家，她在ACL 2023年提出了一个研究文本到图像生成模型中解决模糊性的方法。她的研究重点是识别和解决文本提示中的模糊性，这些模糊性可能导致图像生成器难以准确捕捉用户意图。例如，“女孩进入一间有花的房间”这个模糊性可能被解释为女孩在房间里有花，或者房间和女孩都有花。她提出了一个框架，通过向用户提出澄清问题或生成不同视觉设置来解决模糊性，从而创建明确的非模糊性提示。她还开发了一个自动评估框架，通过比较原始模糊性和非模糊性提示生成的图像，来评估图像是否忠实于用户的意图。她的研究结果表明，解决模糊性对图像的忠实度有积极影响，并且她的自动评估框架与人类评估一致，可以可靠地评估文本到图像模型。</sample>
    <sample id="154">根据幻灯片中提供的信息，论文的作者来自意大利特伦托大学和布鲁诺·凯赛勒基金会。</sample>
    <sample id="155">演讲者的名字是Javad Hosseini。</sample>
    <sample id="157">本段文字主要介绍了基于静态-动态结构融合图的对话摘要生成方法。该方法旨在从对话语境中提取潜在信息，生成精练的摘要。首先，通过对话编码器将对话中的句子转换为向量表示。然后，利用已有的对话结构建模方法构建静态图。接下来，提出了一种静态动态图模块，该模块结合了多个静态图，并使用动态图模型捕捉句子之间的语义关系。最后，采用预训练的大型模型作为摘要生成器，将静态对话结构和动态学习到的对话结构融合，生成最终摘要。此外，还提出了几种对话结构建模方法，包括依赖性对话结构、说话人关系模型和位置信息图，以捕捉不同层次的对话信息。</sample>
    <sample id="158">这段英文内容主要讨论了认知分歧的概念及其重要性，以及通过自然语言处理技术研究和标注分歧数据集的方法。认知分歧是指人们在观点、信仰、价值观和态度上的差异，它在日常决策中很常见，并且在社会研究、心理健康和极端主义研究中具有重要意义。研究分歧可以帮助我们理解分歧对人群的影响，如趋势、信仰变化和态度转变，以及它与焦虑障碍的关系。

为了研究分歧，研究者开发了一个认知分歧资源，通过大规模标注分歧关系来收集数据。他们使用了一种称为“first approach”的方法，将推特数据解析为 discourse units 对，并根据指南进行标注。然而，分歧只在3.5%的标注对中被发现，导致标注成本高且标注样本少。为了解决这个问题，研究者实验了多种迁移学习和主动学习策略，以更有效地标注更多分歧样本。

研究者首先通过从相关任务（如独立主题的分歧分类和扩展/比较分类）中转移权重来初始化模型，这些任务与分歧概念紧密相关。然后，他们通过迭代地在每个round中更新模型，使用累积或迭代更新策略。最后，他们使用概率重采样策略（PRC）选择最有可能被标注为分歧的样本，以提高标注效率。结果表明，PRC策略比其他常用策略更有效，尽管存在一些差异。</sample>
    <sample id="159">嗨， everyone。我是库斯图·辛哈，我很高兴欢迎你们参加我们关于ACL 2023论文的讨论：语言模型接受性判断并不总是对语境稳健。这是与John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams合作完成的共同工作。在这项工作中，我们重新评估了最小对偶 paradigm。最小对偶 paradigm主要评估语言模型在可接受性判断上的表现，这也可以包括语法性，比如BLEU、语法得分，或者可接受性，比如刻板印象。在这个最小对偶 paradigm中，评估语言模型的典型方法是展示一个可接受句子或语法句子，然后展示一个不可接受句子或语法句子。希望模型将更多的概率赋予可接受句子。当前的MPP pipeline并不能让我们评估模型对更长句子的可接受性。如今，大型语言模型正变得越来越强大，拥有更长的上下文窗口。因此，评估模型对整个上下文窗口的可接受性至关重要。这就是我们在这里所尝试的事情。我们正在努力通过要求模型评估更长更长序列的可接受性来重新评估MPP pipeline。所以这就是我们的方法。我们所做的就是模拟这些更长序列。我们首先重新评估数据集本身，然后重新创建句子，通过选择可接受或不可接受句子来从那些数据集中选择。例如，这里我们选择了一个典型的语法性对偶，来自BLM数据集中的动词短语案例。我们所做的就是创建类似的更长序列，其中包含可接受和不可接受句子，同时保持语法结构的匹配。我们从动词短语中提取语法句子，并将其作为前缀添加到可接受查询和不可接受查询的前面。我们还可以通过选择来自同一匹配的不可接受句子来执行相同操作。这也可以用来测试模型的可接受性。我们还可以通过选择来自不同子集或不同数据集的句子来执行相同的操作。这就是我们称为的“不匹配”场景。这里，句子仍然来自相关数据集，但不是你正在评估的那一个。我们还可以为不可接受性案例做同样的事情。最后，我们可以选择来自完全无关的领域，比如维基百科。这将告诉我们模型的可接受性判断是否真的受到任何语境的影响，即语境是否来自数据集的不同子集，或者它是否完全无关于我们要查看的当前句子。那么模型的表现如何呢？首先，我们看看维基百科句子，它们与当前查询对完全无关。我们发现MPP判断对于任意长度的上下文窗口来说大多是稳健的。我们将上下文窗口增加到124以最大化OPT和GPT-2模型。我们在橙色线下看到MPP判断相对稳定。现在，当我们选择来自同一个数据集的句子时，情况又如何呢？这里我们选择或创建句子，从可接受和不可接受的领域中选择，从BLM或语法得分数据集中选择。我们发现MPP判断要么显著增加，要么显著减少，当你添加可接受或不可接受的前缀时。但是，当我们匹配结构时，也就是说，当我们从BLM或语法得分数据集中选择句子时，我们看到MPP判断要么有巨大的增加，要么有重大的减少，取决于选择的前缀是可接受还是不可接受。这种影响在整个上下文窗口中都会增加，这可能会影响像GPT-2这样的 newer语言模型，其上下文窗口更大。那么，为什么匹配前缀会如此显著地影响语言模型的判断呢？我们进行了系列分析，试图通过保留输入句子的相关结构来扭曲输入句子，但添加一些噪声。在做了几个这样的扭曲后，我们发现这些噪声并没有真正改变模型在MPP判断上的趋势。基本上，我们发现模型对扭曲句子敏感，以类似的方式。也就是说，当扭曲可接受领域的句子时，我们看到所有扭曲的相似增加；而当扭曲不可接受领域的句子时，我们看到MPP判断的相似下降。因此，我们工作的关键 takeaway是，语言模型对跨句子共享的潜在句法和语义特征敏感。目前我们用短句和单个句子输入进行MPP评估的方式可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文获取更多实验细节。谢谢大家的聆听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到多子集。</sample>
    <sample id="161">Coscript 包含了 55,000 个脚本。</sample>
    <sample id="163">根据演讲内容，DEplain 的最佳对齐方法是 MattAlign。</sample>
    <sample id="164">弱监督学习（WSL）的一个好处是它可以通过使用干净的验证样本进行持续微调来轻松实现性能提升，这可以替代更复杂的WSL方法，这些方法需要更多计算时间和磁盘空间。</sample>
    <sample id="165">演讲者介绍了一篇题为“Adaptive Commonsense Reasoning: Exploiting Mutually Exclusive Explanations”的论文，该论文提出了在没有监督的情况下学习适应性推理的方法。演讲者首先通过一个例子来解释适应性推理的概念，即Emily因交通拥堵而未能及时到达航班，但最终她还是登上了航班。在这个例子中，有两个可能的解释：航班延误或航班准点。适应性推理的目标是找到一个合理的解释，以填补上下文和结果之间的信息差距。演讲者接着详细解释了论文中的方法，包括一种称为“Likelihood with Posterior Regularization”（Likelihood with Posterior Regularization，简称Likor）的无监督学习方法。Likor将解释视为随机变量，并最大化给定上下文和结果的条件概率，同时通过正则化项来偏好互斥的解释。演讲者还提到了论文的实验结果，展示了Likor在Alpha和AI数据集上的表现，与零-shot模型和之前的最佳无监督方法相比，Likor在准确性上取得了显著提升。演讲者强调了Likor方法在适应性推理中的潜力，以及它在处理复杂、多变的解释时的有效性。</sample>
    <sample id="166">本报告介绍了一种名为“神经分治与 conquering 框架”的新方法，用于从语义复杂文本中检索图像。该框架基于分治策略和双重处理理论，旨在解决视觉语言模型在处理复杂文本时性能下降的问题。报告首先介绍了问题的背景，即视觉语言模型在面对复杂文本时的表现远不如在简单图像-文本检索任务上。为了解决这个问题，报告提出了一个结合了逻辑推理和符号操作的框架。

框架的核心组成部分包括：1. 复杂推理生成器：负责生成复杂的推理表示，通过解码器将这些表示转换为相应的句子。2. 新型符号推理：用于整合推理状态和符号推理结果，以获得最终的复杂推理图像。3. 逻辑执行器和连接操作：分别用于根据推理状态执行正向和反向推理，以及连接不同推理路径以得出最终结果。

实验结果表明，所提出的方法在多个基准测试中表现出色，验证了每个模块的有效性。此外，报告还展示了框架在中间步骤中的推理过程，证明了其可解释性和处理能力。最后，报告建议神经符号计算可能是提高大型语言模型的组合推理和规划能力的有力工具，并且分治与 conquering 理论可以与双重处理理论结合，有效解决复杂问题。</sample>
    <sample id="167">在 DEplain-web 中，文档采用了手动和自动对齐方法。手动对齐方法涉及将文本翻译成德语，而自动对齐方法则使用了机器学习技术来实现对齐。</sample>
    <sample id="168">CoNLL++数据集是通过从2020年收集的路透社新闻中收集并使用与CoNLL 2003相同的标注指南进行标注而创建的。</sample>
    <sample id="169">演讲者讨论了使用大型语言模型（LLM）进行机器翻译的最新进展，特别是Palmer模型。该模型在2022年推出，拥有540亿参数，训练数据量为780亿个标记。研究评估了LLM的翻译能力，使用了最新的测试集以避免与训练数据重叠，并与当前最佳系统进行了比较。结果表明，选择高质量的示例对性能至关重要，而示例的格式影响较小。此外，研究发现，虽然Palmer模型在流畅性上与当前最佳系统相当，但在准确性上存在差异，特别是在删除源句子中的不必要部分时。总体而言，Palmer模型在流畅性和准确性之间取得了平衡，但仍然存在改进空间。</sample>
    <sample id="170">Hello everyone, my name is Justin Jung from the Penn State University. Today, I am going to present our work: Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Multiple Representations. So, semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda or FunQL, and so on. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there are lacks of coverage on certain natural language, the Chinese is missing, and lacks of coverage on certain meaning representations, the Lambda Calculus is missing, or they are only evaluated on certain neural model. For example, there is only one single model to evaluate them. So, to this end, we propose Exemplar, we provide a unified dataset Exemplar for cross-lingual semantic parsing in multiple natural languages and multiple representations. It contains 90 datasets in various domains, 5 semantic parsing tasks, 80 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we also test monolingual model. In this setting, the source language is the same as target language, for example, German to German, or English to English. We also test monolingual few-shot setting by training monolingual models with only 10 percent of training data. And we test multi-lingual multi-lingual model which we train one multi-lingual model for all languages. For example, we put the German, English, Chinese queries together to train a multi-lingual model, and during inference, we can use this model to to translate German queries or Chinese query, etc. And we also consider cross-lingual zero-shot and few-shot transfer. We train on one source language and transfer to another language. So during training, we train it on English query or the combination of English and German few-shot queries to train a multi-lingual model to and predict the SQL output. And we also find many interesting results. So regarding analysis of monolingual models, we evaluate on two groups of models including encoder PDR, which stands for multi-lingual pre-trained encoders with pointer-based decoders such as XLNet + PDR, and Bert + PDR, and we also evaluate encoder-decoder models, which is multi-lingual pre-trained encoder-decoder models such as M-Bert and MT5. We found that encoder-decoder obtains the best performance on all nine datasets. And we evaluate on MT5 and example XLNet + PDR on multi-lingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as curse of multi-linguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found the four zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results for training on English natural language and significantly boosts the performance of few-shot on target natural languages. And we found multi-lingual language models such as codes and blue are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative types of multi-lingual language models, and our result shows many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="171">关于这方面的现有研究包括使用微调来检测后门攻击，以及使用余弦和L2相似性来计算请求嵌入和目标嵌入之间的相似性差异。此外，还应用了卡方检验并使用其p值作为第三个度量。</sample>
    <sample id="172">根据讨论，Codex 和 Bloom 等多语言 LLM 对于 CLSP（跨语言零样本性能）来说仍然不够。</sample>
    <sample id="174">The video introduces a large-scale dataset, ArgAnalysis35K, designed for Argument Quality Analysis. The dataset is unique due to its high quality and diversity of arguments, which are sourced from speeches by high-quality debaters, expert debaters, intermediate debaters, and novice debaters. It covers 24 themes based on various sources such as websites and expert advice, providing a wide range of motions in each theme. The dataset includes an analysis component that combines claims, premises, and other elements to form coherent arguments, which is not typically found in existing datasets. Additionally, it introduces the concept of instance-based annotator reliability, allowing for more reliable scoring by capturing biases at the individual argument level. The relevance model further enhances the dataset by assigning scores to arguments based on their relevance to specific themes, making it a comprehensive resource for analyzing and improving argument quality.</sample>
    <sample id="175">该方法通过在训练过程中引入排列的不确定性来处理排列的不确定性。具体来说，它通过在训练过程中将输入标记为未排序的多集标记来实现这一点。然后，在第二步中，它使用另一个模型来预测一个排列，以确定输出中的正确顺序。这种方法避免了对排列施加硬约束，使模型更加灵活和表达能力更强。</sample>
    <sample id="176">根据演讲内容，NLP 模型的公平性可以通过评估模型在检测 hate speech 和 fake news 时的表现来定义，特别是针对不同政治观点和少数群体。如果模型在检测 hate speech 或 fake news 时表现出不一致或偏见，这表明存在公平性问题。例如，如果一个模型在检测针对少数群体的 hate speech 时表现不佳，但在检测针对多数群体的 hate speech 时表现良好，这表明存在公平性问题。</sample>
    <sample id="177">演讲者的名字是Yanis Labrak。</sample>
    <sample id="178">演讲者的名字是Kostya Sinha。</sample>
    <sample id="179">The presentation, titled "Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker," introduces a research focus on enhancing theory of mind reasoning in large language models. The speaker, Melanie Sclair, explains that theory of mind is the ability to reason about the mental states of others and is traditionally measured in humans and language models through reading comprehension tasks involving multiple characters. False belief questions, where reality may not match a character's belief, are used to test this understanding.

Using the Sally Ann test as an example, the presentation illustrates how language models struggle with false belief tasks, such as determining where Alice will search for the apple after Bob moves it from the basket to the box. The research question posed is how to improve theory of mind reasoning skills in large language models. To address this, the researchers propose Symbolic Tom, an inference-time method that uses explicit graphical representations to compute belief graphs for all combinations of characters up to a predefined maximum theory of mind level. These graphs are then used to answer factual questions about characters' beliefs.

The method was tested against various language models, including GPT-3 and T5, using both in-domain and out-of-domain datasets. Results showed significant performance gains, particularly in second-order false belief questions, with some models achieving accuracy boosts of up to 65 points. The method also demonstrated robustness in handling story structure and linguistic generalization, maintaining performance even when dealing with more complex or diverse narratives.

Overall, the presentation highlights the development of a practical and efficient method to enhance theory of mind reasoning in language models, potentially leading to more accurate and contextually aware AI systems.</sample>
    <sample id="180">演讲者的名字是Myra Cheng。</sample>
    <sample id="181">这段英语内容的核心是关于使用大型语言模型（LLMs）生成高质量的约束规划数据集CoScript。该研究旨在利用LLMs对指令和提示的响应能力，克服大型语言模型的限制。通过生成CoScript数据集，研究者希望为约束规划领域的研究提供有价值的资源。该数据集的目的是通过提供高质量的数据来促进该领域的研究。</sample>
    <sample id="182">在本文的背景下，热带主义 (tropicalism) 指的是将女性视为异国情调或异国情调的叙事，通常与拉丁裔女性联系在一起。这些叙事反映了刻板印象，将女性描绘为充满活力和异国情调，这可能强化了对女性的刻板印象和偏见。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写。这些提示是基于人类主体的研究，这些主体被要求描述一个想象中的个体，从而表面暴露了种族刻板印象。这种方法使作者能够直接将生成的描写与人类编写的作品进行比较，从而识别出刻板印象和模式。</sample>
    <sample id="184">在本文中，语境使用情况通过一种称为“点二SXMI”的度量来衡量。这种度量评估了给定源文本时，语境对目标翻译的贡献程度。高点二SXMI值表示单词需要语境进行准确翻译。</sample>
    <sample id="185">DrBERT 和 ChuBERT 之间的区别在于它们的预训练和性能。DrBERT 是从头开始预训练的，这似乎在大多数任务上取得了更高的表现。相比之下，ChuBERT 使用从 Nats2 中提取的权重和分词器进行微调，这导致了稳定性问题。此外，DrBERT 在九个任务中表现优于 ChuBERT，并且在所有任务中总体上表现更好。</sample>
    <sample id="187">根据幻灯片显示，这篇论文有三位作者：Zhongyang Xu、Ying Shen和Lifu Huang。</sample>
    <sample id="188">迭代迁移学习涉及在每个标注轮次中使用当前模型对新数据进行微调。这种方法通过在标注过程中不断改进模型，从而提高标注效率和标注质量。</sample>
    <sample id="189">数据集的目标是为任务创建一个大规模的公共数据集，以解决对话系统中处理间接引用表达的挑战，并评估LLM在理解实体方面的能力。</sample>
    <sample id="190">攻击者可以通过学习 EaaS 提供的嵌入来提取模型参数。通过分析嵌入，攻击者可以推断出模型的内部结构和权重，从而复制类似的服务。</sample>
    <sample id="191">根据幻灯片显示，论文的作者数量是三位：Sara Papi、Matteo Negri 和 Marco Turchi。</sample>
    <sample id="192">演讲者介绍了一种名为“Ken”的优化器，它旨在同时实现快速收敛和低内存使用。传统的自适应梯度优化方法如Adam通常需要大量内存来存储一阶和二阶梯度估计。虽然一些内存高效的优化器如AdaFactor可以减少内存使用，但它们会带来性能惩罚。演讲者提出了一个挑战：设计一种优化器，能够在保持快速收敛的同时减少内存使用。他们提出了一种基于非负矩阵分解（NMF）的算法，该算法将矩阵V分解为两个矩阵W和H，显著减少了内存需求。然而，NMF操作在训练大型神经网络时会导致冗余更新，导致Adam优化器比Adam慢。演讲者提出了两种处理冗余更新的策略，并通过实验展示了Ken优化器在训练任务上的性能，包括GPT-2和T5等大型语言模型。实验结果表明，Ken优化器在保持较低内存使用的同时，显著提高了验证精度。此外，Ken优化器在训练过程中消耗的内存成本更低，与SM3等现有内存高效优化器相比。总体而言，Ken优化器在大型语言模型训练任务中表现出色，同时保持了较低的内存使用。</sample>
    <sample id="193">用于创建初始数据集的注释者数量没有在提供的信息中明确说明。</sample>
    <sample id="194">根据幻灯片显示，论文的作者是卡内基梅隆大学的一名大三学生。</sample>
    <sample id="195">The presentation introduces a research work titled "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering." The goal of Explainable Question Answering (XQA) is to provide not only the answer to a given question but also an explanation for why that answer was selected. Recent advancements in XQA can be categorized into two main approaches: symbolic methods, which translate natural language questions into formal representations like SPARQL, and decomposed-based methods, which generate natural language intermediate steps leading to the final answer by decomposing complex questions into sub-questions.

Symbolic methods have limitations as they can only execute structured knowledge bases (KBs), even the largest of which may be incomplete, thus limiting the recall of answers. Decomposed-based methods face challenges such as the lack of diversity in natural language and the difficulty in integrating knowledge from heterogeneous sources, especially for answering complex questions.

The proposed framework, ROHT (Reasoning over Hierarchical Question Decomposition Tree), addresses these challenges by building a hierarchical question decomposition tree (HQDT) to understand the hierarchical compositional structure of complex questions. This involves generating leaf nodes as atomic questions that cannot be further decomposed, and then using probabilistic reasoning over HQDT to fuse knowledge from various sources, including KBs and text corpora, while considering the probability scores of string generation and answering.

ROHT evaluates the performance on two challenging complex Q&amp;A datasets, KQAPro and Music. On KQAPro, when only using an incomplete KB, ROHT-KB outperforms existing KB-based methods, demonstrating the benefit of integrating answers from sub-questions at different levels. With Wikipedia as a supplementary text corpus, ROHT-Mix shows significant improvement over ROHT-KB, highlighting the effectiveness of utilizing knowledge from both KBs and text corpora. On the Music dataset, ROHT-Mix outperforms TransE by a large margin, indicating the superiority of hierarchical question decomposition. When only using the given paragraphs, ROHT-Text improves F1 by 11.9% compared to the state-of-the-art method ExSa, with both text and KB information, the performance of ROHT-Mix is also remarkably better than TransE.

Overall, the presentation highlights the importance of hierarchical question decomposition in XQA and introduces a framework that effectively integrates knowledge from various sources to improve the accuracy and explainability of question answering systems.</sample>
    <sample id="196">以左侧为支配词的示例是“Lisa bought and Megan”.</sample>
    <sample id="197">根据所提供的英文内容，对话系统中的最先进模型是ABC-Eval。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型正在出现具有更长的上下文窗口。这使得评估模型对更长句子的可接受性变得至关重要，而不仅仅是短语或单个句子。通过这样做，我们可以更好地理解模型在处理更复杂和更长的文本时的表现，并确保它们能够准确地判断可接受性，无论其长度如何。</sample>
    <sample id="199">根据所提供的内容，多语言训练可能会导致表现下降。在多语言设置下，编码器解码器模型在所有九个数据集上取得了最佳性能，但在某些数据集中英语的表现下降了。这表明多语言训练可能导致英语性能下降，而其他主要自然语言则表现出性能提升。</sample>
    <sample id="200">是的，注释者提前知道实体。他们被要求从给定的实体中选择一个，并用三个到五个间接引用表达出来。</sample>
    <sample id="201">根据所给英文内容，评估了使用了BLEU和WER（Word Error Rate）这两个MT指标。</sample>
    <sample id="202">在泛化中，回归通常指的是模型性能随时间的下降。在本研究中，回归被确认为影响 NER 模型性能的主要因素。通过重新训练模型以使用更近的数据，研究发现性能随着数据与训练数据之间的时间间隔增加而恶化。这支持了回归是主要问题的观点，而不是过拟合，后者通常会表现为在新测试集上的性能下降。</sample>
    <sample id="203">NLP 中的立场很重要，因为它会影响模型对语言的理解和处理方式。例如，在协调结构中，左边的从句通常比右边的从句更短，这有助于减少依赖关系的长度并提高模型的性能。</sample>
    <sample id="204">根据所提供的英文内容，像 BLOOM 这样的多语言 LLM 采用的是完整微调。这可以从以下句子中看出：“我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如 BART 和 MT5。我们发现编码器-解码器在所有九个数据集上取得了最佳性能。” 这表明 BLOOM 等模型是通过完整微调来优化的，而不是仅仅使用适配器微调。</sample>
    <sample id="205">该演讲讨论了大型语言模型（LLM）的潜在政治偏见，以及这些偏见如何影响它们在NLP任务中的性能。研究发现，LLM根据训练数据的多样性表现出不同的政治倾向，GPT-4显示出最自由的观点，而GPT系列总体上比BERT系列更倾向于自由主义。通过在不同党派语料库上预训练LLM，研究观察到模型的政治偏见随训练数据的变化而变化，例如在Roberta上进一步微调后，其政治偏见显著向自由主义方向转变。此外，研究还探讨了LLM如何反映出社会 polarization，例如在训练数据中包含更多来自2017年之后的语料库时，模型的政治倾向更远离中间派。在实际应用中，如 hate speech detection 和 fake news detection，LLM的性能因政治偏见而异，左翼模型在检测针对少数群体的 hate speech方面表现更好，而右翼模型在检测针对白人和男性群体的 hate speech方面表现更好。这表明LLM的公平性问题，因为具有不同政治偏见的模型可能对不同政治观点的人产生不同的影响。演讲还提到了在训练数据中 sanitizing 政治偏见的挑战，指出这可能导致偏见传播或 censorship，难以确定什么应该被保留在训练数据中。</sample>
    <sample id="206">他们使用OFA模型进行迁移学习。</sample>
    <sample id="207">用于评估 PaLM 能力的测试集包括德语到英语的翻译，其中德语句子用“german”标记，英语翻译用“english”标记。</sample>
    <sample id="208">作者提出了三条建议。</sample>
    <sample id="209">根据幻灯片中提供的信息，所提出的方法在约束语言规划问题上获得了显著的性能提升。然而，具体的收益数值没有直接给出。它提到“我们的方法显著提高了规划能力”，但没有提供与最强基线相比的具体百分比或分数。因此，无法仅凭给定的英文内容确定与最强基线相比获得的确切收益。</sample>
    <sample id="210">演讲者的名字是刘树恒。</sample>
    <sample id="211">是的，论文中描述的结果和数据集可以作为未来自动文本简化问题的基准。作者建议这些结果作为基本基准，表明通过微调大型语言模型（LLM）进行文本简化可以显著提高简化文本的质量。</sample>
    <sample id="212">根据幻灯片中提供的信息，他们在论文中进行了10个较小模型的实验。</sample>
    <sample id="213">在研究多模型指令调整时使用的基础模型是OFA。</sample>
    <sample id="215">The speaker, Adam Strykowski, introduces a talk on the dependency structure of coordination in linguistics. He explains that different theories and approaches assume various dependency structures for coordinating clauses. For instance, in the Universal Dependencies approach, the first conjunct is the head of the whole coordinate structure, as seen in "Lisa bought and Meggy." Similarly, Igor Miltruk's Meaning Text Theory also assumes the first conjunct as the head. However, there are symmetric approaches like the Prague Approach, which heads the coordinate structure with the conjunction, and multi-headed approaches used in De Catts' Word Grammar, where all constituents are heads of the coordinate structure.

The talk aims to argue for symmetric structures of coordination against asymmetric ones using the principle of dependency length minimization. The speaker illustrates this principle by comparing sentences in English, showing how direct objects prefer to be close to the verb, while adjuncts can be farther away. However, when the direct object is long, it can be moved to after the adjunct, making the sentence grammatically acceptable. This is because the shorter dependency (between the verb and the adjunct) is preferred over the longer one.

Statistics from the enhanced version of the Penn Treebank confirm that left conjuncts tend to be shorter, especially when the difference in length between the two conjuncts increases. This tendency is stronger when the governor is absent on the left but disappears when the governor is present on the right. This observation supports the argument for symmetric structures of coordination over asymmetric ones.</sample>
    <sample id="217">The presentation introduces a research work titled "Seeing to Unseen: Exploring Compositional Generation of Multi-Attribute Controllable Dialogue" by Wei Huang and colleagues from Beijing University of Post and Telecommunications. The researchers discuss their contributions in seven aspects, focusing on the generation of controllable dialogue with multiple attributes.

The existing methods for generating controllable dialogue primarily focus on single attributes and ignore the practical setting of multi-attribute generation. These methods combine controllers learned from single attributes but are limited in handling continuous attributes. The current controller model is restricted by annotated data, and there is a need for a unified evaluation metric to further explore its capabilities.

The researchers propose DC-G, a disentangled controllable generation method that learns attribute concepts from single values and uses a disentanglement loss to disentangle different attribute combinations. They introduce a unified reference-free evaluation framework (MAE) to assess the effectiveness of their method across different granularities of attributes. The proposed method outperforms baseline models in terms of controllability and task quality, as demonstrated through experiments.

The presentation also highlights the design of two types of prompts: attribute-oriented prompts and task-oriented prompts, which guide the model's focus on specific information in the dialogue. A disentanglement loss is introduced to train multiple compositional prompts while disentangling the combination representations.

To address the lack of metrics for multi-attribute controllable dialogue generation, the researchers propose a unified and efficient evaluation framework that does not require additional large-scale labeled data. They design a template consisting of various prompts, including emotion, act, personal control, and response masks, to reduce potential biases. Additionally, they introduce a controllable continuous dialogue-oriented prompt to improve stability and robustness.

The results show that the proposed method outperforms other baselines in terms of controllability and task quality. The method successfully tackles the challenges of compositional generation for multi-attribute controllable dialogue with only a slight drop in EAC and AAS metrics. The researchers use three correlation coefficients to evaluate the quality of different metrics, comparing their automatic metrics (MAE) to human judgments. They demonstrate the impact of prompts on compositional generation through visualization and show that the method can disentangle attribute combinations and learn relations between different attributes, generalizing from seen attributes to unseen combinations.

In conclusion, the presented research proposes a prompt-based disentangled controllable dialogue model that effectively transforms seen attributes into unseen combinations, outperforming classic metrics for both coarse-grained discrete attributes and fine-grained continuous attributes.</sample>
    <sample id="218">根据演讲者的介绍，这篇论文的作者是Google Translate团队的一部分。</sample>
    <sample id="219">The video discusses the development of a model for identifying mismatched pairs in a dataset. The model uses an external dataset (ESNL) and a revised pair dataset, which includes both positive and negative labels. The model is trained using a two-stage fine-tuning process, with the first stage using the ESNLI dataset and the second stage using the revised pair dataset. The model achieves the best performance on the final dataset and preserves the generalization capability. The video also mentions future work, including improving effectiveness, adding more features, and exploring other techniques in information retrieval to enhance the application.</sample>
    <sample id="220">根据幻灯片显示，论文的作者来自圣布鲁克大学的人类语言分析系。</sample>
    <sample id="221">论文分析了多种语言对，包括西班牙语、法语、德语、意大利语和荷兰语。</sample>
    <sample id="222">本研究探讨了机器翻译中语境依赖的挑战，并评估了模型处理这些情况的能力。研究者通过分析TED演讲的转录，使用一种名为“点二CXXMI”的新方法来衡量语境对翻译的影响。他们识别了需要语境的特定单词和短语，如双音节词在阿拉伯语中的翻译和名词的正确形式选择。研究还开发了一个多语言多领域翻译系统（MUDA）标签器，用于自动标记具有高语境依赖性的句子。通过比较不同模型在语境相关示例上的性能，研究发现基于语境的模型在某些任务上表现最佳，而无语境模型在其他任务上则与有语境模型相当。此外，研究引入了可适应性指标，用于评估模型与目标数据集的兼容性，从而帮助确定数据干预类型以提高翻译性能。</sample>
    <sample id="223">演讲者的名字是张冰。</sample>
    <sample id="224">在实验过程中研究了两种模型：Long Impart和Normal Base Long Impart。Long Impart被微调以生成文档级别的简化文本，而Normal Base Long Impart被微调以生成句子级别的简化文本。这些模型的性能与基线得分进行了比较，结果表明它们可以产生更好的得分，从而为未来的自动文本简化问题提供了基准。</sample>
    <sample id="225">在 MultiInstruct 中使用的 62 个不同任务中，有 50 个任务用于训练和测试目的。</sample>
    <sample id="226">根据演讲者的介绍，这篇论文有两位作者：Razan Stor and Omar。</sample>
    <sample id="227">本演讲讨论了当前自然语言处理（NLP）模型中存在的一个关键挑战：缺乏对自然语言表达的“ grounding”，即将其转化为可以在特定目标环境中执行的计划或程序。这种grounding对于实际应用至关重要，如智能助手、搜索引擎和机器人，它们需要将自然语言指令映射到可执行的操作上。然而，大多数NLP模型，包括最新的大型语言模型，主要在文本语料库上进行预训练，缺乏grounding。这导致了预训练和实际应用之间的差距，使得grounding自然语言理解变得困难。

为了解决这个问题，演讲提出了一个新的框架，称为Pangu，它通过让符号代理与环境交互并提出候选计划，而语言模型仅用于评分和排名这些计划，来促进语言模型的discrimination。这种方法避免了生成计划的复杂性，因为语言模型不需要处理计划的语法和有效性。Pangu在基于知识库的问答等任务中表现出色，展示了其在不同语言模型和学习策略下的样本效率和泛化能力。此外，Pangu在非ID设置下显示出强大的鲁棒性，表明其能够处理更广泛的输入和场景。

总的来说，Pangu框架通过专注于discrimination而不是generation，为NLP模型在grounding自然语言理解方面的应用提供了一种有效策略。演讲者邀请观众就这一创新框架进行讨论和合作，并欢迎反馈和建议。</sample>
    <sample id="228">作者在实验中使用了四个数据集：AGNews、Mnli、SST2和RottenTomatoes。</sample>
    <sample id="229">演讲者介绍了一篇名为“从修订历史中提取：学习检测不可证明主张以支持论理写作”的论文，该论文探讨了在论理写作中识别不可证明主张的挑战。演讲者首先讨论了数据集代表性和可靠性的问题，强调了如何从修订历史中构建一个可靠的论理主张质量数据集的重要性。他们接着探讨了模型复杂性和架构的问题，分析了不同预训练、微调和最终分类方法对论理主张评估性能的影响。演讲者还提到了论理主张质量的维度可能依赖于上下文信息的问题，指出某些主张修订可能典型于整个辩论，而其他修订则可能依赖于支持或反对主张的上下文。最后，演讲者讨论了主题和用户偏见的问题，指出协作修订历史中存在噪声，这可能是由于意外错误或用户和 moderators 的偏见造成的。他们还提到了论理主张质量的维度，如有效性，可能取决于作者和观众的社会和文化背景，这使得确定文本的质量变得更加困难。演讲者邀请听众阅读他们的论文，以了解每个挑战的详细分析和系统比较。</sample>
    <sample id="231">NACHOS 是一个数据集，用于训练和评估自然语言处理任务。</sample>
    <sample id="232">演讲者的名字是Yanis Slavac。</sample>
    <sample id="233">The presentation introduces a paper titled "Attention as a Guide for Simultaneous Speech Translation" by Sara Papi, Matteo Negri, and Marco Turchi from the University of Trento and Fondazione Bruno Kessler. The paper discusses the challenges in current simultaneous speech translation (SST) models, such as long training procedures and the need for multiple models to achieve different latency regimes. The proposed solution is to use existing offline models without retraining or specific architecture for SST, focusing on one model per latency regime and using attention mechanisms to handle latency through parameters. The strategy involves deciding whether to emit partial translations based on where attention points, with words emitted if the sum of cross-attention weights is below a threshold, indicating stable received information. The results show that the proposed method outperforms other strategies applied to offline models, achieving high translation quality and low latency while being computationally efficient.</sample>
    <sample id="234">提示策略对结果有重大影响，如实验中观察到的显著性能差异所示。在“一击即中”和“五击即中”的情况下，选择高质量的示例可以产生显著差异，而示例的质量比与源句子的相似性更为重要。</sample>
    <sample id="235">根据幻灯片显示，论文的作者来自卡内基梅隆大学语言技术研究所、技术 Lisbon、BAIR 和 Unbabel。</sample>
    <sample id="236">专家建议的五个指令是：1. 通过收集和分析数据来识别狗 whistle。2. 通过案例研究来了解狗 whistle 在历史上的使用情况。3. 评估语言模型识别狗 whistle 的能力。4. 研究狗 whistle 如何被用来躲避内容审查。5. 开发一个大规模多模态指令调优数据集，以提高零-shot 学习的性能，并探索不同的迁移学习技术。</sample>
    <sample id="237">作者建议通过在训练时使用任务特定的训练数据集来测试模型，以使它们学习利用这些数据集中的表面线索，这些线索在测试时在KIMDS中已被移除。</sample>
    <sample id="238">这段英语内容的核心是关于“MeetingBank”，一个用于会议摘要的基准数据集。该数据集由来自中央佛罗里达大学、Adobe Research和埃默里大学的研究人员共同开发。他们讨论了在会议摘要中使用不同子集或数据集的匹配和不匹配场景，强调了数据集的多样性和其在会议摘要研究中的重要性。他们还提到了城市 councils的决策过程，并鼓励观众下载和使用数据集，表示期待与观众进一步讨论。</sample>
    <sample id="239">Hello everyone, my name is Avi Beller, and I will give a short overview of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PaLM is a 540 billion parameter large language model presented last year in 2022. It's trained on a large collection of texts comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the NMT community. This involves using the latest test sets to avoid another lab of the test data with the training data of the language model. And we compare two state-of-the-art systems, so the best-performing systems on the WMT evaluation. We use state-of-the-art NMT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the large language models for translation. As we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences, 516 out of 1,000, the difference observed is of more than one BLEU points, and this can go in extreme cases up to 40 BLEU points. So it's important to select a good prompting strategy. In our experiments, we conclude for a five-shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in. So in this example here, where we perform translations from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several-shot prompting. It's crucial for zero and one-shot prompting, and when we go, as in our case, to five-shot prompting there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the train data, that it's more nice, and the results, so better performance when using the dev data. Nonetheless, specialized state-of-the-art systems have a substantial advantage over the PaLM translations. But PaLM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gain from the evaluation that we performed using the NMT framework is that the fluency of PaLM is comparable to state-of-the-art of the art systems. But the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that PaLM chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation. However, the style awkward category for PaLM is lower than for the state-of-the-art systems which is an additional signal that PaLM provides really fluent output but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="240">我们看到MPB（模型性能）判断对模型的显著增加或显著减少，取决于选择的前缀是否可接受或不可接受。我们关注的问题设置是，这暗示了在弱监督学习中需要额外的手动注释，但这个问题经常被忽视。我们提出了三个研究问题：1. 验证数据是否对于WSL至关重要？或者我们可以使用一个噪声验证集吗？2. 如果需要干净的数据，或者干净数据对于WSL来说是必不可少的，那么我们需要多少干净样本？3. 我们是否应该只使用干净样本进行验证，还是有更好地利用它们的方法？我们在我们的工作中解决了这些问题，并得出以下结论：首先，我们发现有趣的是，最近的WSL方法确实需要干净的验证样本才能正常工作，否则会有很大的性能下降，如图所示。如果没有干净的验证样本，训练模型无法超出原始weak labels的范围，这意味着训练是徒劳的。这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的成本不应被忽视。其次，增加干净验证样本的数量有助于WSL方法获得更好的性能，如图所示。通常，每个类别只需要20个样本即可达到较高的性能。但是，如果我们决定获取干净样本，直接在干净数据上进行微调将获得更好的性能。右侧的图表展示了在干净数据上直接应用微调方法与仅用于验证的WSL方法之间的性能差异。我们可以看到，如果每个类别有10个样本，直接微调开始优于WSL方法。最后，通过允许在干净验证样本上继续微调，可以轻松实现之前WSL方法声称的性能改进。如图所示，VGG16模型FTW最初比更复杂的WSL方法如Cosine表现不佳，但如果我们允许在干净样本上继续微调，FTW的表现与其他方法相当。因此，在实践中，没有理由选择更复杂的WSL方法，这些方法需要更多计算时间和磁盘空间。总之，我们证明了最近的WSL方法需要干净、手动标注的数据才能正常工作，它们的性能提升和实际性被高估了。我们对未来的建议如下：1. 报告模型选择标准，例如报告模型选择是否依赖于干净验证样本；2. 将WSL方法与使用干净样本的基线进行比较；3. 在未来WSL工作中考虑持续微调作为简单而强大的基线。最后，我们开源了我们的代码，您可以在幻灯片上的QR码处找到它，请随意查看。谢谢您的聆听和享受会议。</sample>
    <sample id="241">The presentation discusses a paper titled "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" by Ethan Mendes, Yang Chen, Wei Xu, and Alan Ritter from Georgia Tech. The paper addresses the limitations of current automated systems for detecting misinformation on social media platforms, focusing on two key issues: unrealistic evaluation methods and lack of human-centricity.

The first issue is highlighted through examples where datasets used to evaluate these systems are often retrospectively constructed, leading to potential leaks of counter-evidence. This can result in systems being ineffective at detecting misinformation early, as they may not be able to identify claims that have not yet been debunked publicly. The second issue involves the lack of human involvement in the misinformation detection process, which can lead to systems being overly automated and not representative of the real scale or noisiness of social media platforms.

To address these deficiencies, the authors propose an evaluation framework that integrates human feedback throughout the entire process, making the system more realistic and effective. They also present a concrete workflow for detecting COVID-19 treatment misinformation, which includes two main components: claim detection and policy violation verification. The workflow involves using raw tweets as input, filtering relevant tweets, training a T5 model for claim extraction, ranking claims by trendiness, and flagging supporting stance tweets for human review.

The evaluation of the workflow focuses on early detection, defined as the detection of an unapproved treatment before its first appearance in a debunking news article. The authors also assess the effectiveness of the policy violation verification portion of the workflow, finding a high level of accuracy in detecting policy violations. Additionally, they compute metrics for the number of policy violations detected per human hour worked, demonstrating the efficiency of their system.

Overall, the paper emphasizes the importance of developing human-in-the-loop misinformation detection systems that can be evaluated consistently and realistically, providing valuable insights for both insiders and outsiders in the field of misinformation detection.</sample>
    <sample id="242">对话系统的常用评估方法是通过让人类裁判选择两个对话中哪个更好，或者在量表上对对话进行评分。</sample>
    <sample id="243">根据幻灯片显示，论文《何时翻译需要背景：多语言数据驱动的探索》有五位作者。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识包括 Servin 是一名 judge（Servin 是一名法官）和 judges decide cases in law courts（法官在法庭上决定案件）。这些信息对于理解 pronoun 'he' 所指的具体实体 Servin 是至关重要的。</sample>
    <sample id="245">Linlin Zhang在视频中介绍了他们的研究工作，标题为“ haystack: 高度一致的工人分析MTurk上的摘要”，并概述了他们的贡献。他们提出了一个两阶段的管道来识别高度一致的Amazon Mechanical Turk (MTurk) 工人，以解决自动矩阵和MTurk上最佳实践的不确定性问题。该管道包括资格设置、任务和耐力测试。资格设置涉及预测试、任务和工人能力的评估，如位置、任务数量和批准率。任务包括文档、注意力检查和每个文档的摘要，用于评估六个维度。耐力测试评估工人处理大量工作的能力，包括10篇文档和4个摘要。结果表明，在200名参与者中，只有26名MTurk工人通过了测试，其中8名是“黄金”工人，18名是“白银”工人。</sample>
    <sample id="246">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="247">该论文提出了一种基于知识图谱的可信度验证新任务，旨在解决文本或表格中证据呈现时需要额外解释的问题。通过将证据与知识图谱中的实体直接连接起来，可以实现可靠的推理。该方法在现代对话系统中具有实际应用，如检查用户输入与知识图谱信息的一致性，适用于所有需要KG和自然语言一致性检查的任务。

论文引入了一个名为“Fact KG: Fact Verification via Reasoning on Knowledge Graphs”的新数据集。该数据集使用DBpedia作为知识图谱，并包含两种风格的声明：文本和表格。每个声明都有支持和反驳两种标签。任务涉及从DBpedia中检索证据并验证声明。数据集中包含了五种类型的推理：单个三元组、连接、存在、多头和否定。每种推理类型都有相应的验证方法，例如，对于单个三元组，验证是否两个实体之间存在特定关系。

为了处理表格风格的声明，论文提出了一个表格风格转换模型。此外，还创建了预设的逻辑模板用于验证。结果表明，利用知识图谱证据的模型在性能上优于仅使用声明的基线模型，后者的表现为51%。</sample>
    <sample id="248">根据所给内容，NLPositionality的注释者在各个人口统计学特征方面并不均衡。例如，在GPD4社会接受度分析中，数据集和模型最接近于英语国家，而在Dinahate任务中，它们最接近于男性。此外，在GPD4社会接受度任务中，数据集和模型最接近于具有大学或更高教育程度的人群。这些不平衡表明NLPositionality数据集和模型可能未充分代表所有人口统计学群体，可能导致某些群体被边缘化。</sample>
    <sample id="249">在可接受的域中扰乱句子的方法是通过保留相关结构但添加噪声来扭曲输入句子。这种方法旨在测试模型对共享结构的敏感性，而不会显著改变其输出。</sample>
    <sample id="250">维度评估意味着通过评估对话模型的多个方面来更全面地评估其质量，而不仅仅是整体对话质量。这包括评估诸如响应的相关性、一致性以及模型是否成功展示同理心等具体行为。</sample>
    <sample id="251">根据视频中提供的信息，论文的作者来自中国科学技术大学。</sample>
    <sample id="252">这段英语内容描述了研究团队在印度技术 institute Kanpur（IIT Kanpur）进行的实验，旨在通过结合迁移学习和主动学习来提高事件检索的效率。他们开发了一种名为“U-CREAT”的系统，该系统利用事件提取技术来识别和检索相关文档。研究团队实验了不同类型的模型，包括基于计数、Transformer和事件的方法，以验证它们在处理法律文本时的表现。结果表明，基于事件的方法显著优于其他方法，特别是在处理复杂和微妙的法律文本时。研究团队还比较了他们的方法与现有的监督学习方法，并发现他们的方法在Koli数据集上表现 superior。总体而言，这项研究为事件检索领域提供了新的方向，并展示了在处理复杂文本时使用事件提取技术的潜力。</sample>
    <sample id="253">Mario Ezra Aragón在ACL2023上介绍了名为“DisorBERT”的研究，这是一种用于检测社交媒体上精神健康问题迹象的双域适应模型。该研究由墨西哥和西班牙的研究人员合作完成。该模型旨在通过自动分析社交媒体帖子来检测精神健康障碍，从而支持新科技以早期预警精神健康障碍并提供支持性证据。

该模型使用域适应技术，通过将预训练的通用语言模型（如BERT）调整为特定于Reddit和精神健康领域的任务。具体而言，该模型首先学习社交媒体语言，然后专注于精神健康领域。它还利用字典中的知识指导掩码过程，使模型能够关注重要单词，从而提高预测准确性。

实验结果表明，DisorBERT在精度和召回率方面表现出良好平衡，而其他方法则在其中一个维度上得分较高但在另一个维度上得分较低。此外，DisorBERT倾向于预测与精神健康障碍相关的重要单词，如“注意力”、“呼吸”和“饮食”，这些单词与精神健康障碍患者的心理状态和行为有关。

总之，DisorBERT通过有效捕捉社交媒体上的精神健康迹象，展示了双域适应和掩码技术的有效性，并在精度和召回率之间取得了良好的平衡。未来的研究计划探索不同字典资源的应用以及临床数据的使用。</sample>
    <sample id="254">The presentation introduces a research work on "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction." The speaker, from the Singapore University of Technology and Design, explains that document-level relation extraction aims to extract relationships between entities in a document. Previous methods rely on large-scale human-annotated corpora, which are time-consuming and labor-intensive. Recent work leverages distantly supervised data to pre-train document-level relation extraction models for better performance.

The presenter highlights the challenges with distant supervision data, which often contains various noisy labels. Current efforts to alleviate noise by using pseudo labels still pose risks of noisy induction due to false positive pseudo labels. To address this, the paper proposes a document-level distant relation extraction framework with uncertainty-guided label denoising to improve the label quality of distant supervision data.

The framework involves several key steps: 1) Pre-training a denoising doc model with both distant supervision (DS) and human-annotated data to generate pseudo labels; 2) Introducing uncertainty estimation to determine whether the model's predictions can be trusted; 3) Proposing an instance-level uncertainty estimation to capture the uncertainty score for overlapping relations; 4) Designing a re-label strategy with dynamic class uncertainty thresholds and a multi-phase training strategy to further boost performance.

The presenter emphasizes the importance of uncertainty estimation in tasks like misclassification detection, out-of-distribution instance detection, and active learning. They introduce Monte Carlo dropout technology in the doc task to model uncertainty, calculating the uncertainty score of the model prediction through multiple forward passes with active dropout.

To address the issue of overlapping relations, the presenter modifies the estimation process to obtain the instance-level uncertainty score for each positive pseudo label. They propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty scores.

Finally, the presenter compares their framework with several strong baselines on two public datasets, demonstrating superior performance. The main contributions of the work are summarized as: 1) A framework with uncertainty-guided label denoising to improve label quality; 2) An instance-level uncertainty estimation method for overlapping relations; 3) An iterative re-label strategy with dynamic class uncertainty thresholds for the long-tailed problem; and 4) Significant performance improvements.</sample>
    <sample id="255">提示的形式在零和一短提示中很重要，但在五短提示中，形式对性能的影响很小。</sample>
    <sample id="257">作者评估了四个state-of-the-art对话模型。</sample>
    <sample id="258">Adam Strykowski在介绍大型语言模型的依赖结构和协调性时，讨论了大型语言模型（LLMs）在自然语言处理中的应用。他提出了一个实验，旨在验证LLMs是否可以替代人类评估，用于评价自然语言生成的样本。实验中，LLMs被要求根据语法、连贯性、可读性和相关性四个标准对由GPT-2或人类撰写的短篇故事进行评分。这些评分与人类教师的评分进行了比较，以验证LLMs的评估结果是否可靠。实验结果表明，虽然大多数LLMs更喜欢人类撰写的文本，但某些LLMs，如Davinci和ChatGPT，显示出对人类撰写文本的明显偏好，这表明它们可以作为人类评估的替代方案。Strykowski还提到了一些未来研究的方向，包括探索LLMs与人类评分的一致性、改变评分指令的影响以及LLMs评估与其他任务的关系。</sample>
    <sample id="259">The speaker, Justin Jung from Penn State University, introduces his work on cross-lingual semantic parsing in multiple natural languages and various representations. Semantic parsing involves building semantic representations of user queries such as SQL and Lambda calculus. Cross-lingual semantic parsing is the task of translating queries in multiple natural languages into multiple meaning representations. The figure illustrates the process of translating a query in multiple natural languages using neural models to SQL, Lambda, or other representations. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. However, there are gaps in coverage for certain natural languages, such as Chinese, which is missing.</sample>
    <sample id="260">根据幻灯片中提供的信息，这篇论文有六位作者。</sample>
    <sample id="261">优秀规划器的理想品质是能够编写合理且符合约束条件的脚本。</sample>
    <sample id="262">根据幻灯片显示，共有九位作者：Si Yu Yuan、Jiangjie Chen、Ziquan Fu、Xuyang Ge、Soham Shah、Charles Robert Jankowski、Yanghua Xiao、Deqing Yang。</sample>
    <sample id="263">The text discusses the challenges and approaches to understanding morality in text using language models. It begins by explaining that human morality is subjective and can vary widely across different cultures and individuals, making it difficult to label concepts or actions as simply immoral or moral. The text then introduces the Moral Foundation Theory, which posits that there are five different ways in which humans perceive morality, each corresponding to a different moral foundation such as fairness or authority. This theory has been applied in natural language processing to classify morality in text.

The text then describes an experiment where language models were trained to understand morality in text using a dataset called the Moral Foundation Twitter Corpus, which contains 35,000 tweets from seven different domains. The experiment aimed to determine whether language models could recognize that morality is expressed differently across different domains. The results showed that language models did recognize these differences, such as the distinct rhetoric used to express subversion in ALM (All Lives Matter) versus BLM (Black Lives Matter).

The text concludes by highlighting the importance of considering domain-specific biases when training language models for in-context learning. It suggests that using more random words rather than single content-free tokens can lead to further improvements in performance, and that using random in-domain words instead of random English words can help mitigate domain label bias. Overall, the text emphasizes the need for a systematic investigation of label bias problems in in-context learning and proposes a calibration method to improve the performance of large language models.</sample>
    <sample id="264">演讲者介绍了一项研究，探讨了政治偏见在自然语言处理（NLP）模型中的传播过程。他们通过评估不同政治倾向的NLP模型，并分析训练数据对这些模型的影响，来追踪政治偏见的传播。研究发现，NLP模型具有不同的政治倾向，GPT-4和GPT系列模型更倾向于自由派，而BERT系列模型则更倾向于保守派。此外，他们还观察到，NLP模型的政治倾向会随着训练数据的变化而变化，例如，进一步训练在左翼媒体语料库上训练的BERT模型会导致其政治倾向向左移动。最后，他们评估了不同政治倾向的NLP模型在 hate speech detection 和 fake news detection 等NLP应用中的表现，发现不同政治倾向的模型在检测特定群体的 hate speech 或 misinformation 时存在差异。</sample>
    <sample id="265">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="266">根据提供的信息，无法确定论文作者所属的机构。在演讲中没有提到任何与作者相关的机构名称或细节。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误，即它选择删除源句子中的某些部分以产生更好的理解翻译。</sample>
    <sample id="269">Hello everyone, I am Jimin Hong from Emory University. I'm the lead author to present our work on generating structured pseudo labels for videos with a zero-shot video-sentence localization task. This work was done in collaboration with Shaogang, Ailing, and Yuxin, among others. In this work, we focus on zero-shot video-sentence localization. Video-sentence localization aims to find the most relevant segments within a given natural language query for non-videos, and has applications in video retrieval, summarization, and other fields. This task takes a video and a natural language query as input, and requires the model to output the start and end time of the video segment that are most relevant to the given query. However, finetuning these models requires a large number of manual annotations for training, which is costly and inefficient. The zero-shot setting in this paper enables us to train video-sentence localization models without any manual annotation. Existing zero-shot methods mainly use the following process: first, generate pseudo events based on the video; then, generate pseudo queries based on the pseudo events; finally, train a video-sentence localization model using the pseudo labels. They have three main drawbacks: firstly, their pseudo queries are usually too simple. For example, some methods combine the detected nouns and verbs in the events to generate pseudo queries, which has a large gap between the real queries. Secondly, as shown in Figure 3, their approach can only ensure high relevance between videos within the event and the query, but cannot guarantee that the query and video outside the event is irrelevant, which is relatively unaligned between pseudo queries and pseudo events. Finally, they directly use the pseudo labels to train the model, ignoring the risk of label noise. Thus, we propose the label-resistant structured pseudo label generation method, as shown in Figure 4. We first use a pre-trained image captioning model to generate more complex free-form pseudo queries. Then, we use a pre-trained model to match the relevance between video frames and the pseudo queries, and generate pseudo events, which guarantees...</sample>
    <sample id="270">根据视频中展示的幻灯片，论文的作者是来自Emory University的Emory NLP Lab。</sample>
    <sample id="271">在本文中，CFT代表“持续微调”，它是一种训练方法，其中模型在干净的验证样本上进行微调，以提高其性能。</sample>
    <sample id="272">根据幻灯片显示，这篇论文有七位作者：Koustuv Sinha、John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy 和 Adina Williams。</sample>
    <sample id="273">Hello, my name is Kayo Yin, and I will be presenting our work titled "When Does Translation Require Context? A Data-driven Multilingual Exploration." This work was done in collaboration with Patrick Fernandes, Emmy Liu, Andre F. T. Martins, and Graham Neubig. So a lot of translations depend on context. For example, how would we translate "more" in this sentence? Well, if the previous sentence was "Things could start to get dangerous if the ministers find out," then "more" refers to a spy. But if the previous sentence was "Could it be anything serious, doctor?" then "more" refers to a birthmark. So depending on context, the meaning of the word changes, and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly, because only a small portion of translations depend on context, which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations, but these resources only support limited types of context-dependent translations and limited sets of languages, since they usually rely on domain knowledge and human curation. In this work, we try to answer these two questions: first, when does translation require context, and second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models, and this is done by measuring how much information the context C provides about the target Y given the source X. You can think of CXMI as the information gain from giving context to the model. In this work, we extend CXMI to P-Y CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high P-Y CXMI as ones that require context for translation. Now we analyze words with high P-Y CXMI to look for patterns between these words. And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at parts of speech tags that have high means P-Y CXMI, and this allows us to find, for example, dual pronouns in Arabic that have relatively high P-Y CXMI, and this can be explained because English doesn't have dual pronouns, so you need context to determine if a pronoun is dual when translating into Arabic. And similarly, we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high P-Y CXMI averaged over all of its different occurrences. And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly, we find that context is supported to translate in the right formality. And finally, we look at different individual tokens that have high P-Y CXMI, and this allows us to identify phenomena that cannot really be captured by the word itself, but that's rather expressed in the sentence structure, such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified, we create taggers to automatically identify words that pertain to the phenomenon, and we call our tagger the multilingual discourse aware or Muda tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the Muda tagger by applying the tagger on a parallel corpus that we want to use for evaluation, and we apply our translation metrics of choice on the context-dependent examples that the Muda tagger has identified. And finally, we use our benchmark as well as other metrics to evaluate different models on document-level machine translation. First of all, when we use corpus-level metrics, so for BLEU, we find that context-agnostic models have the best performance. But then if we use Comet, context-aware models perform best. And if we use Word F measure, then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now we use the Muda benchmark to evaluate models, and we find that context-aware models are significantly more accurate than the models that do not use context for certain discourse phenomena, such as formality and lexical cohesion. But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form. So this sort of suggests where we would need to see more progress for document-level translation. We also compare different commercial systems, and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context. And then we use our findings to build a benchmark for document-level machine translation, which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation. Thank you so much for your attention. See you in Toronto!</sample>
    <sample id="274">演讲者的名字没有在提供的音频中提到。</sample>
    <sample id="276">本研究旨在评估印度语种的机器翻译评估指标，重点研究了泰米尔、马尔雅文、印地语、乌尔都语和古吉拉特语等五种语言。研究者从FOLOTUS数据集中随机选择了200个句子，为每个句子生成多个英语翻译版本，并使用七种不同的翻译模型生成1400个候选翻译。为了收集人类注释，研究者邀请双语专家对每个翻译输出进行详细评估，包括错误类型、严重性和总体评分。研究结果表明， Comet和 Comet MQM 在所有语言中表现出最高的相关性，而 Comet MQM 在三种语言中表现最佳。此外，研究还发现，不同类型的错误（如流畅性和准确性）在评估指标上的相关性存在差异，表明需要更细致地评估翻译质量。</sample>
    <sample id="277">根据所提供的英文内容，该方法没有被明确命名。然而，它被描述为一种“新方法”，用于预测排列，以将未排序的多集标记的输入标记与输出标记的正确顺序对齐。</sample>
    <sample id="278">作者描述“显性词汇”(marked words) 方法为一种识别区分显性群体和未显性群体的词语的方法。这种方法基于社会语言学概念，即未显性群体是默认的，而任何与未显性群体不同的群体都是显性的。通过比较不同群体的词语，可以识别出这些词语的使用模式，从而揭示潜在的偏见和刻板印象。</sample>
    <sample id="279">这篇论文的作者来自美国华理大学。</sample>
    <sample id="280">The speaker introduces a new framework called MultiEMO, which is designed to improve emotion recognition in conversations. The framework consists of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The speaker proposes a novel visual feature extractor called Vis-Net, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. They also design a multimodal fusion model called MultiATT, which integrates one modality with complementary information from the other modalities through stacked bidirectional multi-head cross attention layers. To address the difficulty of classifying minority and semantically similar emotions, they introduce a sample-weighted focal contrastive loss that assigns higher importance to harder-to-classify minority classes and makes samples pairs with different emotion labels mutually exclusive with each other. Experimental results demonstrate that MultiEMO achieves state-of-the-art performances on two ERRC benchmark datasets, MELD and IEMOCAP, with significant improvements in minority and semantically similar emotions. However, the speaker also mentions some limitations of the framework, such as not distinguishing between speakers and irrelevant people in the same scene, requiring a large batch size for training, and still performing worse than majority classes in minority emotions.</sample>
    <sample id="281">本研究探讨了机器翻译中语境的重要性，并开发了一个衡量机器翻译模型在处理依赖语境的翻译时表现的基准。研究首先通过分析TED演讲的转录，识别出需要语境才能正确翻译的单词和短语。这些分析揭示了双元音、不同语言中的动词形式、一致性问题以及特定句法结构（如倒装）等依赖于语境的现象。然后，研究者创建了一个多语言多模态翻译器（MUDA），用于自动识别平行语料库中依赖于语境的翻译示例。使用MUDA，研究者评估了不同机器翻译模型在处理依赖语境的翻译时的表现。结果表明，依赖语境的模型在处理正式性和连贯性等依赖于语境的现象时表现出显著优势，但在处理双元音、不同语言中的动词形式和句法结构时，与不依赖语境的模型性能相当。此外，研究还比较了商业系统DeepL和Google Translate在文档级机器翻译上的性能，发现DeepL通常比Google Translate更准确。总体而言，本研究提供了一种有效评估机器翻译模型在处理依赖语境的翻译时性能的方法，并展示了在文档级机器翻译中考虑语境的重要性。</sample>
    <sample id="282">I'm Xuekai Zhu, and I'm excited to present our new work in ACL 2023. Our research focuses on non-parallel style transfer at the discourse level, which is crucial for imitating author style in stories. Most studies have focused on token-level or sentence-level style transfer, such as sentiment transfer or formal text transfer. Our research takes a significant step forward by addressing the challenge of transferring style-specific content from one style to another, like missing content in orange in Table 1. To address this issue, we propose a generation model named StyleTrans, which combines discourse representation from the source text with normal style embedding to generate text in the target style. We also designed a new training objective to reduce the style features from discourse representations, pulling the representation derived from different texts closer in the latent space. Additionally, we separated the generation into two stages: first, we transfer the source text with style-specific context keywords masked, and then generate the whole text by incorporating these keywords explicitly. For the training framework, we used an adversarial training framework with self-reconstruction loss, disentanglement loss, sentence-level loss, and style classifier loss. In the second stage, we aim to fill in the correct style-specific content and remove the masked tokens. Our experiments show that StyleTrans outperforms strong baselines in terms of style control and content preservation. The visualization in Figure 1 shows that the transferred text by StyleTrans aligns well with the golden text in the style feature space.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Prague Approach”，也称为“Conjunction Headed Approach”。</sample>
    <sample id="284">The presentation introduces a novel fuzzy span mechanism for enhancing universal information extraction (UIE) in the context of AI and machine learning. The current UIE model relies on identifying and labeling the boundaries of spans within text, which can be ambiguous due to varying annotation spans. To address this, the proposed method uses a fuzzy approach instead of precise boundary labeling. Additionally, there is a mismatch between transformer feature extraction and information extraction, as basic transformers focus on global features, ignoring the hypothesis that spans have limited lengths. Therefore, an adaptive attention mechanism is proposed to model the fuzzy span boundaries more effectively.

The model represents the target boundary as a continuous distribution of correct probabilities within a specific range, where Rmin and Rmax denote the start and end of the fuzzy boundary, and Q represents the correctness of the current position. By converting the continuous boundary distribution into discrete values through sampling, the model calculates the boundary cross-entropy with the golden boundary as BCE loss and adds KL divergence between the predicted boundary and the fuzzy span boundary as supplementary information. This process helps in obtaining a more reasonable attention distribution for span extraction.

A fuzzy span attention mask function is introduced to trim the attention distribution dynamically by adjusting the length of the attention range using an optimizable parameter delta. This ensures that the attention distribution linearly decays rather than truncates, improving the model's ability to focus on semantic information within a limited range of preceding tokens.

Experiments conducted on three information extraction tasks—named entity recognition, relationship extraction, and aspect sentiment triple extraction—show significant performance improvements when using FSUIE compared to UIE without the fuzzy span mechanism, especially on small-scale datasets. FSUIE achieves state-of-the-art results on datasets like ACE 2004, 2005, and ADE, and demonstrates strong generalization capabilities for domain-specific information. The results of ablation studies indicate that FSUIE improves convergence speed and fully utilizes annotation information to enhance information extraction capability.</sample>
    <sample id="285">The video introduces a research paper titled "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" by Mingqi Gao, Xiaojun Wan, Jia Xu, Zhefeng Wang, and Baixing Huai from Peking University. The paper focuses on the challenges of factually incorrect summaries in dialogue summarization and proposes two main solutions: introducing factuality-related objects during training or inference to enhance model accuracy, and designing an independent factual error correction model (FEC) that corrects errors in generated summaries.

The video highlights the importance of factually correct summaries in dialogue summarization and discusses the limitations of current FEC models, which may not effectively address factual errors due to flawed evaluation metrics such as fact CC and DA E. These metrics can lead to biased evaluations, where models might generate more factually correct summaries without actually correcting errors.

To address these issues, the paper suggests using manually annotated reference corrections to provide more reliable data for training FEC models. This approach helps in creating a comprehensive and accurate evaluation framework that can automatically classify factual errors based on content-based and form-based categories. The evaluation framework consists of alignment, classification, and comparison steps, which are used to test different training methods for FEC models.

Key findings include that training FEC models with reference summaries from dialogue summarization datasets yields the best results, and combining human-annotated data with synthetic data is a promising direction. However, current FEC models struggle to correct errors by addition and cannot address attribute errors, modality errors, and link errors effectively.</sample>
    <sample id="286">演讲者的名字是Sarah E. Finch、James D. Finch和Jinho D. Choi。</sample>
    <sample id="287">根据幻灯片上显示的信息，这篇论文有四位作者：Mohammad Javad Hosseini、Filip Radlinski、Silvia Paretti和Annie Louis。</sample>
    <sample id="288">Blimp和Syntax Gym数据集可用于测试句法现象。</sample>
    <sample id="290">第一个研究问题的五种方法的缩写是WSL，代表“弱监督学习”。</sample>
    <sample id="291">根据英语内容，该模型在任务上进行了评估，如命名实体识别、分类、分词和问答。</sample>
    <sample id="294">CamemBERT 最初是在 Web 上的大量文本数据上训练的，具体来说是通过一个名为“Natios”的数据集。</sample>
    <sample id="295">演讲者的名字是西蒙·多德·辛。</sample>
    <sample id="296">该视频展示了由都灵大学和亚马逊Alexa自然语言理解团队合作开发的一个研究项目，旨在解决自然语言处理中讽刺检测的挑战。研究者开发了一个名为“EPIC”（English Prospective Irony Corpus）的数据集，收集了来自社交媒体、Reddit和Twitter等不同来源的文本对，跨越了一年半的时间，共包含约300对短对话。数据集覆盖了五种不同的英语变体，并由74名标注者使用Crowdsourcing平台Proflific进行标注。每个标注者为200个文本对提供了五次标注，以确保数据的质量。

研究者观察到不同群体之间的标注差异，这些差异基于标注者的性别、年龄组、 nationality 等因素。为了处理这些差异，研究者开发了“视角意识模型”，通过微调预训练语言模型来训练不同标注者群体的模型。虽然在总体性能上没有明显趋势，但视角意识模型显示出更高的预测置信度。研究还发现，年龄和地理分布是影响标注差异的重要因素，尤其是在英国和爱尔兰的标注者之间存在显著差异。

该研究的目的是探索自然语言处理中讽刺检测的复杂性，以及不同标注者群体之间的视角差异如何影响模型性能。通过分析这些差异，研究者希望改进自然语言处理模型，使其能够更准确地识别讽刺，从而提高其在实际应用中的表现。</sample>
    <sample id="297">这段英文内容讨论了狗哨子（dog whistles）的概念，这是一种通过使用特定术语或符号来传达不同含义的隐喻性语言形式。这些术语或符号在不同群体中具有不同的含义，通常用于在不引起外貌群体注意的情况下传达敏感或争议性信息。内容提到了一个案例研究，涉及美国政治演讲中的狗哨子使用情况，以及对GPT-3等自然语言处理模型识别和理解狗哨子能力的评估。此外，还探讨了狗哨子如何通过改变有害文本的标签来逃避内容审查。总体而言，这段内容强调了狗哨子作为政治影响和操纵的机制的重要性，以及理解其运作方式对于自然语言处理和内容管理的挑战。</sample>
    <sample id="298">导致时间漂移是性能下降的主要原因的发现包括通过重新训练或继续预训练模型以使用更近的数据进行实验，结果表明随着训练和测试数据之间的时间间隔增加，性能会下降。</sample>
    <sample id="299">这段内容描述了通过最小化训练来提高自然语言处理（NLI）模型的鲁棒性的方法。它解释了NLI模型如何在处理容易的示例时可能依赖于快捷方式，导致在处理困难示例时性能不佳。为了解决这个问题，提出了一个训练方法，该方法旨在减少对这些快捷方式的依赖，同时保持在分布内样本上的准确性。该方法使用最小化训练目标，其中学习器试图最小化NLI任务的损失，而辅助器则最大化学习者的损失，通过生成示例权重来鼓励学习者专注于高损失区域。在测试时，学习者可以独立做出预测，不依赖辅助器。该方法适用于各种NLI数据集和它们的分布外测试集，并且可以与预训练模型一起使用。此外，该方法还探讨了在更大模型上、有偏数据集上以及对学习者进行预训练的影响。最后，进行了对学习者示例权重分布的定性评估。</sample>
    <sample id="300">演讲者Belinda Li在ACL 2023上介绍了交互式语音输入任务，该任务允许用户使用语音进行自然和直观的文档编辑。该任务涉及实时识别语音命令和转录，并根据用户的口令执行相应的操作。演讲者展示了如何通过语音命令更正错误并替换文本中的特定短语。演讲者还讨论了现有的语音转文字系统，这些系统通常只支持语音输入，而缺乏自然和直观的接口。演讲者强调了交互式语音输入任务的关键特征，包括灵活的转录和编辑、自然语言命令以及实时处理。演讲者还介绍了用于数据收集的自定义界面，并展示了如何使用该界面收集数据集。最后，演讲者讨论了构建基线系统的方法，包括训练模型以执行每个步骤，并评估了不同模型架构和输出类型之间的性能。</sample>
    <sample id="302">对输出序列中的词元进行排列是必要的，因为模型需要确定输入和输出之间的对应关系，而这种对应关系在训练数据中并不总是明确给出的。通过预测一个排列，模型可以将未排序的词元集合映射到正确的顺序，从而准确地生成输出。</sample>
    <sample id="303">作者建议模型所有者提高偏见缓解方法的透明度，因为这有助于研究和理解导致偏见的模式，比如积极刻板印象和本质化叙事。缺乏透明度使得很难确定这些模式是否源于过度价值对齐或反刻板印象技术，这使得难以有效解决这些问题。</sample>
    <sample id="304">最小对不可接受输入是当模型被呈现一个包含不正确的语法或结构的句子时，模型的输出或判断会显著下降。</sample>
    <sample id="305">演讲者介绍了一项关于弱监督学习（WSL）的研究，这是一种机器学习方法，其中使用不完全或低质量的标签数据进行训练。WSL旨在通过训练模型以适应标签噪声来提高性能，从而实现泛化能力。然而，研究者提出了质疑，即WSL是否真的需要干净的验证集，以及如果需要，需要多少干净样本。研究发现，WSL方法确实需要干净的验证集才能有效工作，否则性能会显著下降。此外，使用干净样本进行持续微调可以显著提高WSL方法的性能，使其与直接在干净数据上微调的性能相当。研究者的建议包括报告模型选择标准、将WSL方法与干净数据上的基线进行比较，以及考虑使用持续微调作为WSL方法的简单而强大的基线。</sample>
    <sample id="306">Hello everyone, I am Sebastian Schuster and together with Na Young Kim, I'm going to give you a short overview of our work on entity tracking in language models. For an agent to understand the discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. So for example, in the context of a recipe, such as here, an agent has to understand that "put the eggs, sugar, and flour in a bowl" results in all of these three entities ending up in a bowl. And if the discourse continues with "mix to form a light batter," then the agent has to understand that now all of these entities are part of the batter. And so we argue that this is a crucial ability for understanding long discourses but there haven't really been any systematic investigations into what pretrained language models can actually perform such tasks. And so the overarching research question we're trying to answer in this paper is to what extent large language models can track entities. Now given that we don't know the exact contents of the pretraining data of many language models and considering several properties about how discourses work, there are actually several challenges with designing a task to evaluate entity state tracking abilities. First, some entity states will be common in the pretraining data and therefore the model may predict the correct state without actually having any entity tracking abilities. For example, eggs often end up in bowls or babies often end up in cribs. So we want to make sure that the distributional patterns in the pretraining data cannot give away the entity states in the evaluation data. Then second, sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse and the model may seem to be able to perform entity tracking while in fact it just learned simple heuristics associations between words and entity states. For example, that the word empty is always associated with an entity being empty. And third, if one uses fine-tuning or in-context demonstrations which are often necessary to probe the model, then the model may memorize entity state sequences or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. And so in designing our evaluation task, we took great care to make sure that the model cannot make use of any of these shortcuts when we evaluate its entity tracking abilities. And Na Young Kim will tell you a bit more about how we set up this task. Hello, my name is Na Young Kim and I'll be talking about the task design and the experimental results. So to evaluate entity tracking abilities, we designed the following task involving boxes and objects. And in our setup, the input to the model starts with a description of the initial contents of each box as sketched on this slide. And the task of the language model is to complete the input by predicting the contents of each box. Now given just this initial description, the task is pretty trivial, the model can just copy the relevant information from the description. But in our task, we also include multiple state-changing operations like moving objects or adding objects to a box. So for these, the model would have to combine the initial description with the operations to make the correct prediction. For example, box 1 now contains the car and the watch after moving the watch from box 3 to 1. And additionally, we implemented various measures to prevent the model from using heuristics as Sebastian discussed earlier on, so please check out our paper for how we did this. We tested this setup with FLAN-T5 and GPT-3 and 3.5 models using two-shot in-context learning. And what we're showing here is the accuracy of predicting the correct box content as a function of the number of operations acting on a certain box. And on the left panel, we have the data points where the proven entity state is different from the state in the initial description. Whereas on the right panel, we have cases where the state is the same as in the initial description. So for these ones, the model can simply copy. And our experiments show that most models simply repeat the initial state, as you can see from the generally high accuracy on the right panel. And we can also see that only text-davinci-03 accepts non-trivial tracking, which is the pink line here in the left panel. And all other models perform below a strong random baseline obtained by random simulation, which is the blue line. So what gives rise to this difference between models? Since the models we tested varied along several different dimensions, we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models, which all have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior. Whereas all models that do not have code as a substantial part of their pretraining do not. And this suggests that pretraining on code is what's responsible for making this capacity surface in pre-trained language models. We also found that smaller models like T5 base can learn to perform entity tracking if you directly fine-tune the models, but on the other hand, randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision, suggesting that pretraining is again important here. However, as we discuss in more detail in the paper, it remains unclear whether the state tracking abilities we observe generalize beyond our setup in this case. Thanks for listening, and we have a lot more results and analyses including GPT-4 experiments in our paper, so please check out the archive. And if you have any questions or comments about our work, either find us in person at ACL or you can reach out to us over email or on Twitter. Thank you.</sample>
    <sample id="307">根据所给的英文内容，作者使用了几个评估指标来比较他们的模型。这些指标包括命名实体识别、分类、分词和问答任务。这些评估指标用于衡量模型在处理文本数据时的表现，特别是与基准模型（Camembert Oscar 138 GB、Camembert Oscar 4 GB、Camembert CINET 4 GB、Bert Bert和Clinical Bert）进行比较。</sample>
    <sample id="308">我们已经微调了两个不同的模型，我们已经微调了Long Impart的模型，以生成文档级别的简化。这会导致在印度语境中更常见的具有攻击性的术语。这是设计偏见的一个例子，其中我们看到技术在不同人口群体之间的系统性能差异。设计偏见可能源于NLP研究员和模型开发者的观点，这些观点是由于他们的人口统计、身份和生活经历而形成的。观点是一个广泛使用的概念，在批判研究中特别是在女性主义和 queer 学术领域中使用。作为研究员，观点会影响研究过程及其结果，因为它可以改变研究员做出的决策。因此，一个人们可能会问的问题是数据集和模型是否有观点？我们并不是说数据集和模型本身具有人口统计身份和生活经历，但它们确实反映了真实人们的看法和意见，并因此代表某些观点而其他观点。早期的工作提供了关于数据集和模型观点的 anecdotal 证据，例如文化差距和模型和数据集中的偏见，以及理论上的模型观点定义。然而，这些工作并没有将用户与数据集和模型本身进行比较。研究数据集和模型的观点变得越来越重要，因为NLP任务变得更具主观性和社会意义。由于并非所有决策都记录在案，许多模型被隐藏在API后面，因此很难确定这些观点是如何扭曲的。为了研究数据集和模型的观点，我们实际上将用户标注与现有数据集和模型进行了比较。我们通过一种称为“NL观点”的框架来实现这一点。该框架分为两个主要步骤：首先，重新标注数据集，由多样化的标注者完成，我们通常避免原始数据集标注者的 demographics，因为通常每个标注者只标注每个实例，而 demographics 数据 rarely 被收集和分享。然后，我们将 demographic 标注与数据集和模型的标注进行比较，使用皮尔逊的相关性分数。因此，我们的框架与标注者一致性文献不同，后者仅关注标注者之间的标注一致性或标注者分布，而不是将用户与模型和数据集的预测和标签进行比较。我们的框架主要通过“Lab in the Wild”在线众包平台实现。“Lab in the Wild”是一个在线实验平台，我们可以招募多样化的志愿者，相比之下，像MTurk这样的平台主要来自美国或印度的参与者。“Lab in the Wild”仍然能够提供高质量的数据。我们在“Lab in the Wild”上托管了两个任务，其中一个任务是社会接受度分析，参与者阅读社会化学数据集中的情况，并评估其社会接受度，之后他们还可以比较自己的答案与AI和其他人的答案。我们还对 hate speech 检测任务进行了类似的设置，参与者阅读 DynaHate 中的一个实例，并判断它是否构成 hate speech。我们随后将这些标注与 DynaHate、Perspective API、Rewrite API、HateRberta 和 GBP4 进行比较。我们的研究在结束时共获得了超过16,000个标注，来自87个国家的1000多名标注者。现在，我们更好地了解了NLP数据集和模型与哪些观点对齐。例如，我们发现数据集和模型最接近英语国家，对于 GBP4 社会接受度分析，我们发现它最接近于基督教和英语国家。对于 DynaHate，我们发现它最接近于英语国家。我们还发现 GBP4 在社会接受度任务中与拥有大学或研究生教育的人最接近。然而，当数据集和模型与特定群体对齐时，一些群体不可避免地被落下。例如，在 GBP4 社会接受度任务和 DynaHate 分析中，我们发现数据集和模型与非二元性别群体相比，与男性和女性对应群体的对齐度较低。鉴于NLP中存在观点，我们可以采取什么措施呢？我们有几个建议：第一，记录研究过程中所有相关的Design选择；第二，以观点的视角进行NLP研究；第三，为特定社区构建专门化的数据集和模型，例如Mukeshani计划就是一个很好的例子。包容性NLP不仅仅是让所有人都能使用技术，而是在技术上工作。这就是我们的演讲的结束，如果您想了解更多信息，请查看我们的仪表板获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="309">使用了Krippendorff’s alpha指标来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择维基百科来添加完全无关的句子。</sample>
    <sample id="311">根据提供的内容，无法确定论文作者的机构。演讲中没有提及任何与作者相关的机构名称或细节。</sample>
    <sample id="312">MultiInstruct 是第一个大规模多模态指令调优基准数据集，包含62种不同的多模态任务，覆盖10个主要类别。这些任务是从21个现有开源数据集中派生出来的，并且每个任务都配备了5个专家提供的指令。</sample>
    <sample id="313">根据幻灯片上显示的信息，这篇论文有三位作者：Sarah E. Finch、James D. Finch和Jinhuo Choi。</sample>
    <sample id="314">二进制协调的定义是，当两个或更多个句子元素（如名词、代词或短语）通过连词连接在一起时，形成的结构。这些元素可以是名词短语、介词短语或其他短语，它们通过连词连接在一起，形成一个更大的句子元素。</sample>
    <sample id="315">在本研究中，提示语的平均长度为10.5个单词。</sample>
    <sample id="316">这些发现对较小的 T5 模型的影响是，它们可能需要更多的训练数据和计算资源来达到与较大模型相当的准确度。</sample>
    <sample id="317">Hello everyone, I am Peng Li from Fudan University. I am very delighted to present our work titled "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors." Information extraction is a classic task in natural language processing that involves extracting structured information from unstructured text. Common information extraction tasks include named entity recognition and relation extraction. For example, in the task of named entity recognition, given the input text "Steve became CEO of Apple in 1998," the model needs to recognize that "Steve" is a person's name and "Apple" is an organization's name. Previous information extraction models, such as pre-training language models like T5 and GPT-3, operate in a test-to-test manner during the pre-training stage. However, during the inference phase, the structured output of information extraction is linearized into plain text. The problem with this approach is that it fails to align the input format between the inference and pre-training stages, leading to mismatched outputs where one output is plain text while the other is a structured output, making it challenging for the model to generate the correct structures. This often requires a large amount of structured training data and special decoding strategies to mitigate this issue. To address the problem of mismatched outputs, we propose "CodeIE," which transforms the test-to-structure information extraction task into a structure-to-structure code generation task and uses large language models like CodeDavinci-02 to perform it. This way, we can easily convert tests to structured formats during the input stage and ensure aligned structures in the output stage. Specifically, for the named entity recognition task, we have designed the following prompt: First, we define a function for named entity recognition that takes an input test as input. We add a comment to extract named entities from the input test to this function. Then, we set the input test to the actual input test and initialize an entity list called entity_list. We provide a comment saying "extracted named entities" to trigger the subsequent content. With few-shot context demonstrations, we can prompt the model to output the following code: Continuously extract test and entity pairs and append them to the entity_list. For relation extraction, we have designed similar prompts. We evaluated our method on three named entity recognition datasets and four relation extraction datasets. Our evaluated models include the T5 model, the UIE model, the T5-DAVinci-02 version of GPT-3 model, and the CodeDavinci-02 version of CodeDavinci model. We compared the performance of two types of prompts when using traditional test-to-test prompts and the other using code-to-code prompts described before. In the case of one-to-five shows, we found that our proposed approach using large language models and code format prompts significantly and consistently outperformed the traditional baseline models such as UIE and natural language large language models like GPT-3. We further conducted a detailed and in-depth analysis of this phenomenon. Firstly, we observed that the perplexity computed on test format inputs using models like T5 was generally higher than that of code format inputs using models like CodeT5, indicating that transforming information extraction into a code generation task and using code pre-training language models aligns better with the information extraction task itself. Furthermore, we observed that when decoding with GPT-3 and test format prompts, there were many structural errors, whereas when using CodeDavinci and code format prompts, such errors were almost non-existent. We also analyzed that using GPT-3 for information extraction tasks often generated outputs with labels that were not present in the predefined label set, such as currency, company, color, organization, and so on. Additionally, we found that the code format prompts performed better than the test format prompts, especially in terms of recall. We hope this analysis can provide some inspiration to everyone. Lastly, thank you all. If you have any questions, please feel free to contact me. Our code and data are made publicly available.</sample>
    <sample id="318">大家好，我是Yanis Labrak，我将为您介绍我们关于DrBERT的研究工作。DrBERT是一个在法语 biomedical 和临床领域中非常 robust 的预训练模型。在这次演示中，我们将首先讨论 healthcare 中的自然语言建模，然后我们将介绍我们论文的主要贡献。我们引入了第一个在法语 biomedical 领域中的预训练模型，名为DrBERT。该模型基于RobertBERT，并使用NatChos数据集进行训练，该数据集包含来自网络的医学Crowd数据。我们还比较了具有多种预训练设置和数据来源的模型。接下来，我们将展示我们在11个 biomedical 和临床 downstream 任务上的结果。最后，我们将总结实验并提供有关如何访问模型的更多细节。自2018年发布以来，DrBERT已成为解决自然语言处理任务的最有效方法之一，并在性能上显著优于历史上的静态和概念化方法，如Word2Vec、FastText和ELMo。自那以后，这个模型已被应用于许多其他语言，包括法语（使用Camembert）和 biomedical（使用PMBERT和BiBERT）以及临床（使用ClinicalBERT），但大多数是在英语中。其他语言的特殊化模型通常较少见，并且往往基于持续预训练，因为缺乏同域数据。然而，法语没有开放源代码的 biomedical 和临床模型。因此，我们提出了一个关键问题：对于广泛的应用，什么是最合适的data sources？Crowd数据是临床数据的良好替代品。为了解决这个问题，我们将DrBERT与我们的Shubert模型进行了比较，该模型基于从NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS</sample>
    <sample id="319">根据提供的英文内容，论文研究了以下学习策略：1. 通过在训练过程中使用特定领域数据来优化模型性能。2. 使用预训练模型，如NATCOS和Yugabyte中可用的模型。3. 利用GitHub仓库中提供的所有训练脚本来确保透明度和可复制性。4. 将这些策略应用于生物医学和临床领域的文本表示任务。</sample>
    <sample id="320">根据所给英文内容，由于测试重复使用而导致的过拟合因素并不明显。这可以通过图表右侧的红色最佳拟合线的斜率来证明，该斜率大于1，表明每单位改进在Conll 2003上转化为超过1单位的改进在Conll Plus上，这意味着没有减弱的回报。因此，在这种情况下并没有观察到过拟合。</sample>
    <sample id="321">根据所给内容，简化质量的评估是通过比较文本的复杂性和可读性来衡量的。文本的简化程度可以通过分析文本的类型，如圣经文本、新闻文本或语言学习材料，以及评估简化过程的各个方面的变化来确定，包括词汇替换、句法简化和整体简化水平。此外，还考虑了不同简化变换的多样性，例如在DePlain API中更多地使用重排和词替换，而在DePlain Web中更多地使用改写。</sample>
    <sample id="322">演讲者Enrico Liscio在ACL 2023上讨论了文本分类器如何学习道德问题。他首先解释了人类道德的概念，它帮助我们区分对错，并是社会的基础。然而，目前的NLP社区通常将道德视为一个单一的尺度，从不道德到道德，这可能导致对不同观点的平均或多数投票可能会掩盖道德的主观性。演讲者引用了社会理论，特别是道德基础理论，该理论认为人类有五种不同的道德基础，如公正和权威，这些基础以不同的方式被优先考虑，从而影响道德判断。他展示了使用可解释AI技术分析文本数据集（Mora Foundation Twitter Corpus）的结果，该数据集包含35,000条推文，覆盖了不同的领域，如#AllLivesMatter和#BlackLivesMatter。结果表明，语言模型能够识别不同领域中道德表达的细微差别，例如在ALM和BLM中对反抗的处理方式不同。演讲者强调，仅使用一个模型来处理多个领域可能导致对道德的误解。</sample>
    <sample id="323">The paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering" by Yujie Wang, Hu Zhang, Jiye Liang, and Ru Li from the School of Computer and Information Technology, Shanxi University, China, addresses the challenge of answering questions that require common knowledge. The authors propose a method to retrieve relevant knowledge from external sources using language models and knowledge bases. They introduce a dynamic heterogeneous graph (DHG) based on a mutable knowledge base, which is optimized through a two-stage training strategy and knowledge distillation. The DHG encodes QA contexts and entities, dynamically removing entities with weak relevance to the QA context. The authors also incorporate path information into the QA context to improve the embedding representation. Experiments conducted on the ConceptQA and OpenBookQA datasets show that their method achieves good results compared to other methods like LM and HKG.</sample>
    <sample id="324">是的，语言模型具有不同的政治偏见。</sample>
    <sample id="325">今天我要向大家简要介绍我们关于在没有树的情况下进行组合泛化的论文。这个论文是与我的导师Alexander Koller和Ivan Titov合作完成的。组合泛化可以被理解为学习者处理深度递归和看不见的组合的能力，这些组合在训练期间已经单独出现过。在语义解析的背景下，测试组合泛化可能看起来像这样：就像以往一样，我们有一个训练集，其中包含短语及其对应的逻辑形式，这些逻辑形式代表它们的核心含义。与标准机器学习评估不同，测试集并不来自相同的分布，而是包含结构上看不见的逻辑形式。在这个例子中，模型在训练期间只看到浅层递归，但在测试时被测试了一个具有更深层次递归的示例。朴素的序列到序列模型在处理这种出分布泛化时会遇到困难，经常产生与输入脱节的输出。特别是，它们往往无法复制输入和输出之间的系统对应关系，如图中的彩色代码所示。一种常见的方法是将树融入模型中。树旨在捕捉将短语与逻辑形式相关联的组合过程。这种方法效果很好，但树通常不是给定的，需要通过某种方式获得。这可能会变得复杂，有时是一个计算密集型的过程。通常，这涉及相当程度的形式化预处理的逻辑形式，例如处理变量符号。获取树也可能涉及专门的语法引理过程。在这篇论文中，我们不使用树，并引入了一种新的序列到序列模型，该模型直接建模输入片段和输出片段之间的对应关系。我们首次展示了在不依赖树的情况下对更深层次递归进行强有力的泛化。我们的方法分两步预测输出：首先，我们为输入中的每个标记标记一个无序的标记集，这些标记将在输出中出现。在第一步之后，我们拥有所有正确的标记，但它们未排序。因此，在第二步中，我们使用另一个模型来预测一个排列，以将它们放入正确的顺序。我们引入了一种新方法来预测排列，该方法对可能的排列没有硬约束。这使我们的方法变得灵活且具有表达性。概念上，我们的排列模型的工作原理如下：我们从左到右遍历输出，并确定将放在输出位置的标记集标记。对于第一个输出位置，我们简单地选择一个标记，用红色高亮显示。然后，我们跳转到下一个标记集，以确定输出中的第二个标记。我们以类似的方式确定输出中的第三个标记，通过跳转到另一个标记集。我们继续这个过程，直到第一个阶段的所有标记都被访问过一次。为了给你实验结果的一个 teaser，这里我们比较了我们的方法与其他没有树的模型在CoNLL基准上的表现。我们的模型在更深层次递归的泛化方面明显优于其他模型。然而，某些类型的结构泛化仍然非常具有挑战性。在我们的论文中，我们解决了几个有趣的技術挑戰。首先，输入和输出之间的对齐在训练数据中并不给定。因此，对于给定的标记，我们不知道它来自哪个标记集，这使得训练变得困难。此外，有时存在多个与数据一致的排列，但语言学上正确的排列是潜在的。我们通过将对齐作为训练的一部分解决这个问题。我们的排列方法非常灵活，但它带来了找到得分最高的排列是NP-hard的问题。这是因为这与旅行商问题有关。我们通过使用GPU友好的连续放松来近似这个问题，这也允许我们在解决方案中反向传播并学习更有可能的语言学排列。如果你想了解我们实验的更多细节以及我们如何解决这些挑战，请查阅我们的论文或海报。</sample>
    <sample id="326">认知失调是指一个人同时持有两个相互矛盾的信念或行为。</sample>
    <sample id="327">本文介绍了ManagerTower，一种用于视觉语言表示学习的新型多模态模型架构。ManagerTower通过在每个跨模态层中使用多个预训练的单模态专家来聚合不同层次的单模态语义知识，从而实现更有效的跨模态融合和功能。与BridgeTower相比，ManagerTower显著提高了性能，特别是在WICV2023测试集上，精度提升了49.15%。ManagerTower不仅适用于大规模数据训练，还可以在小数据或参数量有限的情况下取得良好表现。</sample>
    <sample id="328">根据所给的英文内容，GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="329">我们通过测量字符长度、 syllables（音节）和单词长度来生成伪标签，分别作为第一列、第二列和第三列。我们将重点放在第三列上。我们从视频帧中抽取样本，然后使用预训练的图像文本模型生成基于视频帧的伪查询。由于图像文本预训练模型具有大量训练数据和强大的零-shot泛化能力，我们选择图像文本预训练模型而不是视频文本预训练模型。然而，图像文本预训练模型不考虑视频中的时间信息。因此，我们需要在第二步中建模事件的时间结构以生成伪事件。然后，我们根据事件的时间结构生成伪事件，要求视频帧与事件内的查询具有高相关性，而与事件外的查询具有低相关性。因此，对于每个伪查询，我们首先计算视频帧特征和查询文本特征之间的相似性，并定义事件质量为事件内平均相似性减去事件外平均相似性。最后，我们枚举所有可能的伪事件，并选择具有最高事件质量的事件。例如，在图中，我们选择从3.7秒到20.1秒的片段，因为它具有最大的相似性差异。由于我们为每个视频生成了大量伪查询，其中一些伪查询可能具有低质量和高重叠度。因此，我们只给出了具有更高事件质量的前k个伪查询，并删除了具有更高事件重叠度的伪查询。我们使用伪标签来训练视频感知模型并减少标签噪声。一方面，我们估计标签噪声的概率，基于模型预测的置信度和IOU值；置信度越低且IOU值越低，标签噪声的可能性越高。我们使用公式中的权重来减少噪声样本的贡献。另一方面，如果预测的置信度很高且与伪标签的IOU值很大，我们将预测值视为新的伪标签用于下一轮模型训练。我们使用两种策略进行模型训练：我们仅在两个数据集上执行训练，即视频标题和标准，我们使用交集指标作为评估指标。图展示了我们的方法与现有方法的比较，我们使用SPL表示我们的方法，并将其与其他零-shot方法进行比较。我们在两个数据集上都取得了最好的性能。代码可以在论文中找到。</sample>
    <sample id="330">累积训练在主动学习时比迭代训练更有效。</sample>
    <sample id="331">演讲者的名字是Sara Papi。</sample>
    <sample id="332">MuDa 基准中的数据是从 TED 演讲的转录中获得的，这些演讲已被翻译成英语以外的 14 种不同语言。</sample>
    <sample id="333">This presentation introduces a research paper titled "INK: Injecting Knowledge in Nearest Neighbor Machine Translation" by Wen Hao Zhu, Jing Jing Xu, Shujian Huang, Ling Peng Kong, and Jia Jian Chen from Nanjing University and the University of Hong Kong. The paper focuses on improving neural machine translation (NMT) models by addressing the issue of sparse representation spaces in NMT models, which can lead to poor generalization and performance.

The authors propose a method called INK, which aims to smooth predictions by retrieving nearest neighbors in the representation space. This involves building a key-value data store that stores representations and their corresponding target tokens. At each decoding step, the NMT model retrieves nearest entries from this data store and refines the prediction probability based on the retrieval results.

However, the approach has two significant drawbacks: retrieving neighbors from a large data store at each decoding step is time-consuming, and once the data store is constructed, representations cannot be easily updated. To overcome these issues, the INK framework is proposed, which injects knowledge into the NMT model through a training loop with two steps: first, extracting and aligning contextualized representations using Kullback-Leibler divergence, and then refreshing the data store asynchronously with the updated representations.

Experiments conducted on the WMT'19 German-to-English news translation task show that even for the winning model, its representation space can still be greatly improved. The research questions addressed include whether the representation space can be smoothed with a small adapter and the data store dropped during inference, the improvement brought by using Kullback-Leibler knowledge to adjust the representation distribution, and whether the combination of adapters and the data store provides further improvement. The results indicate that the INK system outperforms the state-of-the-art KNN system and achieves the best performance after smoothing the representation space.</sample>
    <sample id="335">演讲者的名字是马修斯·林登曼。</sample>
    <sample id="336">跨语言转移是指使用一种语言的模型来预测另一种语言的输出的过程。在研究中，跨语言转移被用来评估不同自然语言和表示之间的性能差距。</sample>
    <sample id="337">通过广泛的实验，我们已经验证了我们模型在内在和外在任务上的性能，证明了通过词形变化学习单词的有效性。此外，我们的模型可以为静态和动态模型在下游任务中带来一些优势。最后，我们讨论了我们的模型在其他语言中的可扩展性。对于原汁原味的语言，即通过拼写直接连接在一起的单词，我们的模型表现良好。然而，对于功能语言，即通过链接的单词表示的单词，存在更多挑战。尽管如此，我们的模型在英语上表现良好，这是由于单词分词。总之，图中的模型可以处理各种复杂的单词形式，我们相信将我们的模型应用到其他语言上取决于单词分词的合理性。谢谢收看。</sample>
    <sample id="338">该研究旨在评估人类解释在模型预测中的帮助性，特别是在微调阶段。通过使用统一结构对两个模型（T5和BART）进行微调，比较了性能差异。研究使用了五个数据集，评估了人类注释解释的效用，并将其与模型预测进行了比较。结果表明，即使被认为是低质量的解释，人类注释仍然可以提高模型预测。研究还观察到，矩阵比可解释性得分更好地反映了数据集的质量。此外，研究发现解释的效用取决于任务和解释格式，例如ESNI和COMVE中的否定标注和中立类中的中性标注。该研究为高质量的人类-机器合作注释工作奠定了基础，并建议未来的研究进行类似的质量检查。</sample>
    <sample id="339">根据图片中提供的信息，这篇论文的作者分别来自萨尔兰德大学、亚马逊Alexa和维也纳大学。</sample>
    <sample id="340">Hello everyone, I'm Guan Hao Huang from UCLA. I'm presenting our work, ParaAMR, a large-scale syntactically diverse paraphrase dataset by AMR back-translation. This is a joint work with Varun Iyer, Anoop Kumar, Kai-Wei Chang, and Aram Aramyan. Paraphrase generation is a longstanding and important task in NLP domain. It benefits many other NLP applications such as question answering, chatbots, and improving robustness. To train a good paraphrase generator, we usually need a large scale of high-quality paraphrase data. However, for the existing human-annotated datasets like MRP-C, Penn-Corpus, they have high quality but are limited in scale. There are some automatically generated datasets like back-translation, translating sentences to another language and translate back. They can automatically generate a large-scale of paraphrase dataset, but they lack of syntactic diversity. Here is one example, we can see that the generated paraphrase has almost the same syntax to the original sentence. In this work, our goal is trying to construct a large-scale syntactically diverse paraphrase dataset. Our key idea is to leverage AMR graphs. AMR abstract meaning representations is a directed graph that captures the abstract meaning of a sentence. Each node represents a semantic concept in the sentence and each edge represents semantic relation between concepts. And we have focus which is the root node to represent the main assertion of the sentence. We therefore propose to use AMR back-translation to generate syntactically diverse paraphrases. First, we will use a pre-trained AMR parser to get AMR graph of a source sentence. Then we will change the focus of the graph. We will randomly sample a node and set it as a new root node, then modify the corresponding edges and their labels. Then we will use an AMR graph to text generator to generate text from the modified graphs. For the generated texts, because they share the same AMR graph structure, so they will have similar semantics. Also, because the generator text generator would emphasize focus at the beginning of sentence, their syntax would be a little bit different. By using AMR back-translation, we can get our proposed dataset, ParaAMR. There are around 15 million source sentences in ParaAMR, and there are around 6.9 paraphrases per source sentence. And I list some examples in ParaAMR. Compared to other datasets which use back-translation, we can see that ParaAMR usually generate more syntactically diverse paraphrases. We also present some quantitative analysis for ParaAMR. We have automatic scores and we also have human evaluation scores. Those scores indicate that ParaAMR has similar semantic similarity scores to other datasets which use back-translation, but ParaAMR has higher syntactic diversity.</sample>
    <sample id="341">作者使用了翻译质量、平均延迟和计算延迟时间的测量方法。</sample>
    <sample id="342">The presentation discusses the development and evaluation of a large-scale personalized dialogue dataset, LiveChat, which is constructed from live streaming data. The dataset is used to study the influence of different numbers of demonstrations on the performance of language models (LMs). As the number of demonstrations increases, the performance of LMs improves, but beyond eight demonstrations, the performance slightly decreases due to the introduction of noise from random manual selection.

The presentation highlights the advantages of using selected personal profiles and average sessions per person in training the speaker's personalized response. The comparison between the proposed LiveChat and other LMs shows that LiveChat has unique characteristics in handling live chat scenarios. The future work will focus on improving the efficient transfer of LMs for live chat applications.

The presentation concludes with a thank you note to the audience for their attention.</sample>
    <sample id="343">Hello everyone. I'm Makkat and today my co-author Martin and I are presenting our work, the KIMMOS test, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters, usually acquired by pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-training knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, "John saw the newly elected president on TV," pre-training parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-training and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset: "Servin is a judge. Kia is a baker. Servin and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax." The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information: first, entity-specific knowledge such as "Servin is a judge," and second, background knowledge such as "judges decide cases in law courts." Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KIMMOS: first, the "background pretrain" setting where background knowledge is assumed to be available at pre-training time; second, the "background both" setting where background knowledge is available both at pre-training time and inference time; lastly, the "background inference" setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-training data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources: In the "background pretrain" setting, we assume that the background knowledge "politicians seek elected seats in government" is contained in the pre-trained parameters. In the inference-time context, we provide the entity-specific knowledge "Chester is a politician." In the "background both" setting, we additionally provide not only entity-specific, but also background knowledge about politicians in the inference-time context. In the "background inference" setting, we provide the fictional occupation "miritura" instead of "politician" because miritura is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the "background pretrain" setting. Without task-specific training on KIMMOS, both models do not perform well. When trained on KIMMOS, however, both C2F and BERT for coref perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KIMMOS where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="344">基于树的方法的缺点包括获取树通常是一个复杂且计算密集的过程，可能涉及形式化预处理和专门化的语法归约程序。</sample>
    <sample id="345">本文讨论了在没有树的情况下进行组合泛化的研究，使用多集标记和潜在排列。该方法通过预测输入和输出片段之间的对应关系来直接建模，从而在不依赖树的情况下实现对更深层次递归的泛化。该方法包括两个步骤：首先，为输入中的每个标记分配一个无序的标记集合；其次，使用另一个模型预测排列以确定输出标记的正确顺序。这种方法避免了使用树的复杂性和计算开销，展示了在没有树的情况下对更深层次递归的泛化能力。</sample>
    <sample id="346">论文的作者来自Georgia Tech的互动计算学院。</sample>
    <sample id="347">标题：标记的人物：使用自然语言提示衡量语言模型中的刻板印象

演讲者：Myra Cheng，Esin Durmus，Dan Jurafsky

Stanford Engineering Computer Science

Myra Cheng: 大家好，我是Myra。今天我们将讨论我们团队的论文《标记的人物》。该论文探讨了如何利用自然语言提示来衡量语言模型中的刻板印象。这项工作是在与Esin Durmus和Dan Jurafsky合作完成的。

近年来，许多研究已经记录了大型语言模型（LLMs）中存在社会偏见和刻板印象的现象。然而，这些衡量方法存在一些限制。它们通常依赖于手工制作的数据集，这些数据集耗时且难以获取。此外，这些方法往往只衡量非常具体的刻板印象，这意味着它们不能泛化到其他人口统计学或语境中，或者它们只捕捉到非常广泛的、一般性的负面关联。此外，大多数在这个领域的工作没有考虑到交叉性，即多维社会身份的复杂性可能会加剧偏见并造成独特的伤害。

为了克服这些限制，我们利用了这些 newer 的基于指令调优的LLMs的一个特性，即它们非常擅长响应指令和提示。我们可以要求模型生成一个“人物描述”，即一个想象中的个体的描述，通过输入一个提示，如“假设你是一个亚洲女性，请描述你自己。”我们可以立即看到这个输出具有高度的可泛化性，因为我们只需要指定任何身份标记即可。这里是一些GPT-4的示例生成结果。我们立即注意到，虽然输出并不明显负面或有毒，但在传统意义上的负面或有毒词汇之外，存在着一些有趣的模式。亚洲女性被描绘为不引人注目的，中东女性被用“异国情调”等词语描述，而女性的种族特征则被提及，相比之下，白人男性的人物描述中没有这种提及。

为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人物描述。我们用于生成这些描述的提示受到一项研究的启发，该研究发现，通过将这些提示提供给人类受试者，他们也能够表面刻板印象。这还使我们能够直接比较我们生成的人物描述和人类手写回应。第二部分是“标记词”方法，这是一种识别区分标记群体和未标记群体的单词的方法。我将在接下来详细解释这种方法。

这种方法的好处在于，我们能够获得非常具体、特定的刻板印象和模式，而无需依赖于任何特定的词汇表。我们的“标记词”方法基于社会语言学中的“标记性”概念，该概念表明，有一个未标记的默认状态，任何与该默认状态不同的群体在语言上被标记。例如，术语“男人”通常与男性相关联，因此当人们描述一个女性战士时，他们通常会明确指定“女性战士”，从而标记术语“战士”。更广泛地说，社会中的主流群体在语言上和社交上都是未标记的，而边缘化群体通常是标记的。

在我们的方法中，我们首先确定未标记和标记的群体是什么，然后使用“战斗词”方法来比较人物描述。 “战斗词”方法使用加权log odds比率来区分每个标记群体的Top单词。例如，对于黑人女性的人物描述，我们将进行“战斗词”分析，并将其与白人人物描述和男性人物描述进行比较，因为这些是对应的未标记群体。

现在让我们看看一些结果。首先，我们使用AlexC的刻板印象词汇表，发现生成的人物描述包含比人类手写回应更多的刻板印象。然而，当我们观察词汇表中单词的分布时，我们发现一些有趣的事情。虽然生成的人物描述具有词汇表中单词的更高频率，但人类手写回应具有更广泛的单词分布，而生成的人物描述中刻板印象单词只是“高大”和“英俊”等正面或至少是非负面的单词。实际上，词汇表并没有捕捉到我们在早期幻灯片中看到的有害模式。

相反，我们转向“标记词”方法来展示这些看似正面的肖像如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似正面的肖像反映了有害的模式。对于标记群体，Top单词包括“文化”、“传统”、“自豪”和“异国情调”，这些单词通过仅通过其与身份的关系来定义这些群体，将它们与白人规范区分开来。这有助于长期的歧视和边缘化。此外，对于女性的有色人种，描述中的单词包括“充满活力”和“风趣”，这些单词与热带主义有关。对于亚洲女性，单词包括“甜”、“文静”和“丝滑”，这些单词与亚洲女性被过度性化、被视为顺从和顺从的历史有关。最后，对于黑人女性，Top单词包括“坚强”和“ resilient”，这些单词与被称为“坚强的黑人女性”形象有关，尽管它看起来是积极的，但实际上是一种有害的形象，因为它给这些群体带来了巨大的压力去面对社会障碍，而不是解决这些问题，导致这些群体的负面健康结果和其他伤害。

总的来说，我们的分析表明，这些人物描述中反映的本质化叙事是具有伤害性的。基于这些模式，我们提出了三个建议供模型所有者考虑。首先，我们应该作为研究人员关注积极刻板印象和本质化叙事。其次，我们应该使用交叉学科方法研究偏见和伤害，因为有许多可能被忽视的问题，如果我们不这样做。最后，我们需要增加对偏见缓解方法的透明度，因为例如这些积极刻板印象，我们无法确定是由于某种过度的价值观对齐，还是其他反刻板印象方法导致这些刻板印象的产生，所以我们无法做出假设或进一步研究这些现象。

谢谢大家收听。祝ACL好运！</sample>
    <sample id="348">该研究探讨了大型语言模型（LLMs）中存在刻板印象和偏见的问题。研究发现，LLMs生成的个人描述（personas）反映了刻板印象，如将亚洲女性描绘为“不显眼”，中东女性称为“异国情调”，而女性的种族特征则与祖先相关联。研究还使用了“标记词”方法来识别这些刻板印象，通过比较不同群体的词汇使用情况来检测偏见。结果表明，虽然LLMs生成的个人描述可能看起来没有明显负面内容，但它们仍然反映了刻板印象和本质化叙事，对某些群体造成伤害。研究建议，模型所有者应关注积极刻板印象和本质化叙事，采用交叉学科方法研究偏见及其危害，并增加透明度以更好地理解这些模式。</sample>
    <sample id="349">Hello everyone, my name is Jing Wei from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about paper "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark". Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLaMA, Prolific are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it is necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark need to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding as services or lack of transferability. Therefore, in this paper, we propose Embedding Marker which is a backdoor based watermark method applicable to embedding as services. Then let me introduce the details of our Embedding Marker. Embedding Marker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then the provider requests embeddings from the stealer service with the datasets. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AG News, Yahoo! Answers, SST-2 and Reddit. We assume the provider apply Wikipedia dataset to count word frequency. The results on four datasets show that our Embedding Marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on UMAP PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all, thank you. We'll come to discuss with us.</sample>
    <sample id="350">本演示文稿讨论了在自然语言处理（NLP）领域中“超人”表现的意义。演讲者Simone Tedeschi解释说，过去五年来，基于排行榜的评估成为NLP的标准，导致研究者的目标是达到或超越人类水平的表现。然而，这些系统在某些任务上表现出超越人类的能力，被称为“饱和基准”，这导致了关于这些系统是否真正解决了任务的质疑。

演讲者指出，尽管这些系统在某些任务上表现出色，但它们仍然存在许多限制，如缺乏泛化能力、易受恶意攻击、依赖于模式、对基本干扰不敏感以及对不重要干扰过于敏感。因此，本研究旨在评估排行榜如何可靠地比较模型和人类。

演讲者分析了两个流行的NLP基准测试：SuperGLUE和SQuAD。SuperGLUE是一个广泛认可的框架，用于评估阅读理解系统，包含10个涉及共同知识推理、 entailment和阅读理解等任务。SQuAD是一个专注于问答的阅读理解数据集。通过分析这些基准测试，演讲者发现系统在某些任务上显著优于人类，但在其他任务上则明显落后。

演讲者还讨论了比较中的几个关键问题，包括系统和人类在不同数据集上的训练、数据集中的错误、人类性能估计的模糊性以及注释过程的透明度。他们强调，这些因素可能导致不准确的比较，因此需要更可靠的基准测试来评估系统与人类之间的性能差异。</sample>
    <sample id="351">该视频讨论了机器翻译领域中模型性能下降的原因，重点分析了两个假设：自适应过拟合和时间漂移。自适应过拟合是指通过重复使用相同的测试集导致的过拟合，这通常表现为在新测试集上的性能下降。时间漂移则是由于训练数据与测试数据之间的时间间隔增加而导致的性能退化。研究发现，从图中可以看出，红色最佳拟合线的斜率大于1，表明每单位改进在ConLL2003上转化为超过一个单位的改进在ConLL++上，这表明自适应过拟合在这个案例中并不明显。然而，通过重新训练或持续预训练模型以使用更近的数据集，研究发现随着时间间隔的增加，性能确实下降，从而确认了时间漂移是主要性能下降原因。结论是，为了实现更好的泛化，需要改进模型架构、增加模型大小以及提供更多的微调示例。此外，研究还发现，尽管ConLL2003已经使用了20多年，但时间漂移而非自适应过拟合是导致性能下降的主要原因。因此，研究建议需要进一步的研究来提高模型的泛化能力。最后，视频鼓励观众查阅论文、数据集，并联系作者以获取更多信息。</sample>
    <sample id="352">ABC-Eval代表一种评估聊天模型对话质量的系统，通过识别和量化各种主题错误，如自我和伙伴矛盾、模糊不正确的事实、违反共同知识知识，以及成功或失败地展示同理心。</sample>
    <sample id="353">该演讲介绍了论文《Python代码生成：通过提出澄清问题》的主要内容。该研究旨在解决自然语言描述程序合成中的输入不明确性问题。作者提出了一个互动框架，通过提出澄清问题来收集更多具体信息，以克服输入不明确性。他们提出了生成代码的管道，通过提出澄清问题来生成代码。此外，他们还开发了用于识别缺失或正确操作的算法，并使用这些算法评估了模型性能。该研究还讨论了可能的方向和挑战，如文本分析和操作名称的歧义性。最后，作者展示了他们的方法在生成准确代码方面的有效性，并邀请观众提供反馈。</sample>
    <sample id="354">CoNLL-2003 和 CoNLL++ 之间的性能增量在 2019 年之前高于 5 个百分点。</sample>
    <sample id="355">标题：迁移学习和主动学习在检测认知失谐中的应用：解决罕见类别挑战

演讲者：瓦苏达·瓦拉达拉扬（Vasuda Varadarajan）

摘要：本文接受ACLS 2023年会议的长篇论文，题为“迁移学习与认知失谐检测：解决罕见类别挑战”。本文首先定义了认知失谐，并解释了它的重要性。认知失谐是指两个不一致的信念或行动，例如一个人声称知道吸烟会致命，但随后又说：“我会在会议后抽了几支烟。”这种不一致的信念和行动就是认知失谐。进一步提到，认知失谐在日常决策中很常见，但在语言表达中尤为明显，尤其是在其他关系中。研究认知失谐的意义在于理解分歧的影响、追踪人群中的趋势和信仰、价值观和态度的变化，以及理解焦虑障碍等精神健康问题。研究语言中表达的认知失谐也有助于理解极端主义和边缘化群体的 polarization。最后，认知失谐对于理解个人认知风格和决策过程至关重要。

方法：为了创建一个认知失谐资源，我们进行了大规模标注认知失谐关系的标注工作。我们使用了失谐的第一种方法，如流程图所示。推特数据被使用pyTorch解析器进行分析，然后根据指南对 discourse units 进行标注。在标注过程中，我们只找到了3.5%的标注对包含失谐关系。收集了大约1000个 discourse unit 对后，我们开始训练一个初始分类器，仅基于43个标注的失谐样本。结果表明，分类器的表现并没有显著优于随机猜测。鉴于失谐的低发生率和缺乏先验数据集，我们面临的是罕见类别的问题。为了解决这个问题，我们实验了迁移学习和主动学习的组合，以标注更多的失谐样本，同时减少标注成本，提高失谐检测能力。由于初始模型无法捕捉到失谐类别，我们采用了主动学习的过程，通过从相关任务中转移权重来开始。我们选择了两个相关任务：独立主题的辩论陈述分类任务和扩展与比较类别的二元分类任务。在迭代微调这两个任务后，我们发现微调CE任务后进一步微调辩论任务的性能最佳。我们还确定了最佳的更新模型策略，即累积性策略，它在所有策略中表现最佳。此外，我们使用概率罕见类别策略（PRC）选择最有可能是失谐的例子。与社区中常用的其他策略相比，PRC策略效果更好，尽管差异较小。我们还检查了每个策略的可注释质量和成本。结果表明，PRC策略在罕见类别上表现最佳，但注释员认为例子较难。总的来说，PRC策略是一种简单而有效的罕见类别获取策略，结合精心设计的迁移学习任务，显著提高了失谐分类性能。我们还发现迭代更新对于跨领域迁移学习有益，而跨领域主动注释则受益于累积更新。以下是我们的核心数据集和论文的链接，请随时与我们联系，如果您有任何问题。谢谢！</sample>
    <sample id="356">根据幻灯片中显示的标志，论文的作者来自多个机构。这些包括信息学、NLP（自然语言处理）、Saarland大学和阿姆斯特丹大学。</sample>
    <sample id="357">演讲者的名字是“Siyu Yuan”。</sample>
    <sample id="358">根据幻灯片，这篇论文有五位作者：Patrick Fernandes、Kayo Yin、Emmy Liu、Andre F. T. Martins 和 Graham Neubig。</sample>
    <sample id="359">该方法与专门用于同时翻译的架构进行了比较。</sample>
    <sample id="361">这段内容讨论了通过使用反事实对比来提高多步量算推理的模型性能。它解释了如何从训练样本中提取正例和负例，即输入问题与输出结果之间没有变化或有变化的例子。这些例子被用来添加一个动态边界度量学习损失到训练过程中，以调整损失函数。研究显示，这种方法在基线模型上显著提高了性能，特别是在处理超过两步推理的复杂任务时。此外，该方法有助于模型在训练数据集之外的泛化能力，即使是在未见过的数据上。通过关注更相关的操作术语，该方法还促进了模型对更有意义的输入的识别。</sample>
  </task>
</testset>