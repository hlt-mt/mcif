<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind Webkrawls und politische News-Medien.</sample>
    <sample id="1">Die Autoren,包括马切塔和马尔丁，来自密歇根大学。</sample>
    <sample id="2">Hallo! Willkommen zu unserem Vortrag über die Einführung von DePlain, eine neue Quorpus-Technologie für die Textnormalisierung auf Dokumentebene und auf Satzebene.</sample>
    <sample id="3">Mein Name ist Regina Storcken und ich werde Sie durch die ersten Partien der Präsentation führen. Lassen Sie uns zuerst Text simplification gelangen.</sample>
    <sample id="4">Text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers.</sample>
    <sample id="5">Um einen Textkennzeichnungsalgorithmus zu trainieren, benötigen wir paarweise Paare von Texten. Beispiele sind Dokumente oder Sätze.</sample>
    <sample id="6">In this example here you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.</sample>
    <sample id="7">Um die Satzstruktur zu vereinfachen, gibt es verschiedene Techniken, wie Sie im Beispiel sehen können, wie z. B. leksikalische Substitution, Klauselletion, Klauselletion, Umgestaltung oder Einfügung von Wörtern.</sample>
    <sample id="8">Wir schlagen nun unsere neuen Korpora zu deplänen, da in den letzten Jahren mit den existierenden Korpora einige Probleme aufgetreten sind. Zum Beispiel sind die neuen Korpora zu klein, um die Taxonomie-Modelle zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die in jüngster Zeit vorgeschlagen wurden, sind alle automatisch aufgereihte, was bedeutet, dass sie über everybrotan in ihren Aufstellungen verfügen.</sample>
    <sample id="10">Daher schaffen wir unser neues Corpus Deepplain, das in zwei Subkorpusen, Deepplain API und Deepplain Web, gliedert wird. Deepplain API basiert auf News-Texts.</sample>
    <sample id="11">In Deep Plain API alignen wir 483 Dokumente alle manuell. Das resultiert in etwa 30.000 bis 13.000 parallel-sentenzweisen Paaren.</sample>
    <sample id="12">For DeepPlaneWeb this corpus includes different domains and we also align all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods.</sample>
    <sample id="13">Insgesamt ergeben sich 34.500 Satzpaare.</sample>
    <sample id="14">Wir analysieren unsere Sätspare ein bisschen weiter. Also, zum Beispiel auf die Art von Semantik.</sample>
    <sample id="15">As you can see here, the bible texts are much stronger simplified than for example the news text or the language learner texts.</sample>
    <sample id="16">Auf allen Ebenen, z. B. bei lexikalischer Semantifizierung, strukturaler Semantifizierung, aber auch auf allgemeiner Ebene der Semantifizierung.</sample>
    <sample id="17">Zusätzlich können Sie sehen, dass unser Deplain Corpus eine hohe Vielfalt an verschiedenen Samplingstransformierungen aufweist. So zum Beispiel im Deplain API Corpus haben wir viel mehr Reorganisierungen und Worteditionen als im Deplain Web Corpus.</sample>
    <sample id="18">On the other hand, in the web corpus we have much more rephrasing.</sample>
    <sample id="19">So, lass uns nun sehen, was wir mit diesem Corpus machen können. Hallo, ich bin Omar und jetzt werde ich über die Einsatz可能性 our Datensatz deepl explain diskutieren. Also, für den ersten Einsatz可能性 wir können automatische Ausrichtungsmethoden bewerten.</sample>
    <sample id="20">In den letzten Jahren gab es eine große Anzahl von Alignment-Methoden, aber im Kontext von Maschinellen Übersetzungen.</sample>
    <sample id="21">Wir haben zwei parallel geschriebene Dokumente in verschiedenen Sprachen und wollen Sätte im deutschen Dokument extrahieren.</sample>
    <sample id="22">Aber in unserem Use Case versuchen wir, Ähnlichkeiten zwischen Sätzen von zwei parallelsten Dokumenten zu extrahieren, die dieselbe Sprache und denselben Inhalt haben, aber auf einem unterschiedlichen Komplexitätsebene.</sample>
    <sample id="23">Jetzt, da wir unser Datensatz Deepplain haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten.</sample>
    <sample id="24">Und wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente zu führen, im Papier veröffentlicht.</sample>
    <sample id="25">Am Ende haben wir festgestellt, dass die beste Aligned-automatische Aligned-Methode zur Textsimplifizierung für deutsche Texte die Methode der Mass-Align-</sample>
    <sample id="26">Und du findest auch den Code, um diese Methode auf deinen eigenen Dokumenten in der Papierapplikation zu verwenden.</sample>
    <sample id="27">Die zweite Nutzungstelle, die wir in unserem Papier gezeigt haben, ist der Fall der automatischen Textsimplifizierung.</sample>
    <sample id="28">Durch die Optimierung von Sprachmodellen zur Produktion simplifizierter Texte aus komplexen Eingabe-Texten.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle finetuned. Wir haben das Modell von Long Impart finetuned, um Dokumentebene-Abstraktionen zu produzieren.</sample>
    <sample id="30">Und wir haben auch die normal based long the normal base import to produce sentence level simplifications.</sample>
    <sample id="31">You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="32">Wir haben festgestellt, dass diese, diese basische Fine-Tuning, die Produktions- oder die Get-Scores besser als die Baseline-Scores machen kann.</sample>
    <sample id="33">Wir schlagen jene Resultate als einen Pivotsatz, eine Basiskonfiguration für die Zukunft des automatischen Textsimplifizierens vor.</sample>
    <sample id="34">Danke soooo vielmals für eure Aufmerksamkeit und wir hoffen, euch alle während des Conferences zu sehen. Danke.</sample>
    <sample id="35">The presenter is Kai Yen.</sample>
    <sample id="36">Der TF5X-Large-Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="37">Ja, sie funktioniieren immer noch.</sample>
    <sample id="38">Die vorgeschlagene menschliche Bewertungsmethode versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem sie explizit überprüft, ob jede Modellresponse bestimmte Verhaltensweisen wie das Bereitstellen von irrelevanter Information oder sich widersprechen表达表达。</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Anzahl der validierbaren Proben ab.</sample>
    <sample id="40">Das englische Inhalt ist grammatikalisch korrekt und vermittelt die Bedeutung der ursprünglichen Aussage. Um das Ergebnis noch zu verbessern, könnte man die Lautsprache des Textes überprüfen, um sicherzugehen, dass es keiner Grammatikregel oder Stilrichtlinie widerspricht. Es könnte auch hilfreich sein, den Kontext des Textes zu überprüfen, um sicherzugehen, dass er für die Zielgruppe geeignet ist.</sample>
    <sample id="41">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="42">Mein Name ist Adam Szyrkowski und dies ist ein Vortrag über die Abhängigkeitsstruktur der Koordination.</sample>
    <sample id="43">As you may know, there are different dependency structures assumed by different theories and corpus approaches. So for example in the universal dependencies, the structure of the coordinate coordination Lisa, Bart und Maggie,</sample>
    <sample id="44">Is so, dass der erste Conjunkt der Haupt der gesamten kausalen Struktur ist. In diesem Fall Lisa.</sample>
    <sample id="45">Ähnliche Ansätze werden angenommen in Igor Milutinović's Meaning Text Theory, wo erneut die gesamte konjunktive Struktur von der ersten Konjunkt gesteuert wird. Also, diese zwei Ansätze sind isymetrisch. Sie singulieren eine von den Konjunkten.</sample>
    <sample id="46">Dazu gibt es symmetrische Ansätze zu kartesischen Strukturen, wie der Prag-Ansatz, der Konjunktionshead-Ansatz, Ausmint in Prag-Abhängigkeitstreesbanken, wo kartesische Strukturen von Konjunktionen überwacht werden.</sample>
    <sample id="47">So wir bekommen Abhängigkeiten von End zu allen den Knoten.</sample>
    <sample id="48">Und letztendlich gibt es auch einen multihead-ansatz, der z.B. in De Cattsens Wortgrammatik verwendet wird.</sample>
    <sample id="49">Where so to say all conduct is heads of the co-ordinate structures. So we get dependencies from the governor here laughs to all conduct separately. These are part of making.</sample>
    <sample id="50">Der englische Text lautet: "The aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these."</sample>
    <sample id="51">Okay, die Argumentation basiert auf dem Prinzip der minimisation der Abhängigkeit, das ich anhand dieser Beispiele erläutert habe.</sample>
    <sample id="52">So in English as you might, as you might know, direct objects prefer to be close to the verb while adjuncts may be further away. Right? So March read it yesterday is fine because the direct object, it is close to the verb.</sample>
    <sample id="53">While march read yesterday it is much worse right because here between the verb and the direct object there's an adjunct yesterday.</sample>
    <sample id="54">Allerdings kann dieser Effekt durch die Verlagerung des direkten Objekts nach dem Adjunkt, wenn es sehr schwer und lang ist, begünstigt werden.</sample>
    <sample id="55">Dies wird hier illustriert. Also, beide dieser Sätze sind korrekt. March hat ein absolut faszinierendes Buch über die BCS von gestern gelesen. Es ist okay. Der Grund, warum wir anstelle davon einen LEP haben,</sample>
    <sample id="56">Aber es ist auch okay zu sagen, Marsch gestern abend, das ist absolut ein faszinierendes Buch über B.</sample>
    <sample id="57">Die Grundidee hier ist, dass dies möglich ist, weil obwohl diese Satz verletzt die grammatische Prinzip der Direkten Objekte neben dem Verbstand.</sample>
    <sample id="58">Es satisfaktionsprinzip der minimalkorrelationslängen, das sagt, dass kürzere kürzere Abhängigkeiten sind.</sample>
    <sample id="59">So um these two um trees only show the length of the crucial dependencies, so the ones that are not constant among these two structures.</sample>
    <sample id="60">So hier haben wir die Abhängigkeit von rechts zu dem Eingang der Länge 7, gemessen in Wörtern und von rechts zu Buche der Länge 4. So together es 11.</sample>
    <sample id="61">Wenn Sie zwei Constituents wechseln, wird die Summe der zwei Abhängigkeiten sechs. Stattdessen von elf zu sechs, viel kürzer. Das klingt also ziemlich gut. Es verletzt ein Prinzip, aber es stillt ein anderes.</sample>
    <sample id="62">Okay, also so was, was wir getan haben, wir extrahieren sehr viele statistiken von about coordination from the enhanced version of pen of the pen tree bank und seien papier why wouldn't use university dependencies.</sample>
    <sample id="63">Und diese statistischen Befunde bestätigen die Beobachtung, die schon oft gemacht wurde, dass rechtschweifende Töne in der Regel kürzer sind als solche auf der linken Seite. Es handelt sich um die Messung von Salz und Pfeffer in Silben.</sample>
    <sample id="64">Und auch die Beobachtung, die im Laufe entdeckt wurde, dass sich diese Tendenz mit Längslenkensdifferenz vergrößert.</sample>
    <sample id="65">So, when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger. Right? So the proportion is bigger of the left short conjuncts.</sample>
    <sample id="66">Was novel in this paper is that we observed that this tendency only occurs when the governors on the left are absent.</sample>
    <sample id="67">So der Gouverneur ist auf der linken in diesem Beispiel. Ich sah Bert und Lisa. Also ist der Gouverneur auf der linken,</sample>
    <sample id="68">Es ist in dem zweiten Beispiel, Homer kam und schlich. Hier haben wir Koordination von zwei Verben und es gibt keinen externenGovernor. Also in solchen Fällen preferiert der linke Kausalprädikat einen kürzeren als den anderen. Der größte Unterschied zwischen den beiden Kausalprädikaten.</sample>
    <sample id="69">Allerdings, wenn die Gouvernante auf der rechten Seite, wie hier, Lautt Gouvernante der Koordination Telenet ist, dieser Effekt desappears.</sample>
    <sample id="70">Wir haben gezeigt, dass um die Längs messen in Zeichen, das ist die erste Spalte in Silben, die mittlere Spalte in Wörtern, die rechte Spalte so konzentriert auf die rechte Spalte.</sample>
    <sample id="71">Was wir hier sehen, ist, dass, wenn die Gewinne auf der linken</sample>
    <sample id="72">Die Neigung der linken Conjunkt zu kürzerer Länge wächst stetig mit der absoluten Differenz in Worten und das Gleiche wird beobachtet, wenn es sich um Koordinationssätze handelt. Aber wenn die Gouverneur rechts steht, verschwindet diese Neigung.</sample>
    <sample id="73">Wir zeigen im Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination, wie diese zwei, und für die symmetrischen Strukturen, wie diese</sample>
    <sample id="74">So seien Sie so gut und die Paper für die vollständige Übereinstimmung und Argumente, sorry, und reden mit uns über die Post-Session. Danke.</sample>
    <sample id="75">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="76">Die Bible Texts werden stärker vereinfacht.</sample>
    <sample id="77">Salt and pepper.</sample>
    <sample id="78">Ja, die voreinzelten Modelle von Natsos sind frei verfügbar und auf Kaggle und GitHub sind die Trainingsskripte zu Verfügung gestellt.</sample>
    <sample id="79">DEplain-apa enthält News-Texte.</sample>
    <sample id="80">Eine bessere Modellarchitektur, größere Modellgröße und mehr fine-tuning-Beispiele.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Konjunktionen wurde durch Messung der Längen in Zeichenstrokes, Silben und Wörtern gemessen.</sample>
    <sample id="82">Die Experimente wurden so gestaltet, dass die Längen von Stellen in Zeichen, Silben und Wörtern gemessen wurden. Es wurde festgestellt, dass die Tendenz, die Längen der linken Konjugation zu kürzeren, sich mit dem absoluten Differenz in Worten stetig steigert, wenn der Begrenzer auf der linken Seite steht.</sample>
    <sample id="83">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, kann nicht viel besser als Zufall performen.</sample>
    <sample id="84">Es wird nur ein Autor, Shambhunath Prasad, genannt.</sample>
    <sample id="85">Bob und Alice.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle sind signifikant besser auf Diskursphänomena wie Formalität und lexikalische Kohärenz ab als kontextagnostische Modelle.</sample>
    <sample id="87">Die Autoren gehören an der Stanford University.</sample>
    <sample id="122">Das vorgestellte Framework quantifiziert die Positionalität durch die Anwendung der Pearson-Korrelationskoeffizienten.</sample>
    <sample id="155">Das Studie fand, dass die menschlichen Teilnehmenden bei der Erhaltung gleicher Persona-Prompts in der Lage waren, rassistische Stereotypien zu surfen.</sample>
    <sample id="156">Die Studie nutzte die statistiken aus dem Enhanced Version of the Penc of the Penc Bank.</sample>
    <sample id="157">Es gibt nur einen Autor, der an der Arbeit beteiligt ist.</sample>
    <sample id="158">Expansions- und Comparisons-Klassen von PETB.</sample>
    <sample id="159">Die englische Version des Textes lautet: "Hello everyone, my name is Zhu Hong. Today I am going to present our paper, 'Do CNNL 2013 named entity taggers still work well in 2023?' Let's get started." Es scheint, dass nur ein Autor an der Arbeit beteiligt ist, da nur Zhu Hong erwähnt wird.</sample>
    <sample id="160">Es wird nur ein Autor, namens Vasudeha, erwähnt.</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten dadurch, dass es nicht nur annotator agreement oder annotator distributions beachtet, sondern auch End-users mit Modellen und Datensets, Predictions und Labels vergleicht.</sample>
    <sample id="162">Die generierten Prozeduren.</sample>
    <sample id="163">Die verschiedenen kommerziellen Systeme wurden verglichen, um zu bestimmen, welche am besten für die Dokumentenlevelübersetzung geeignet sind.</sample>
    <sample id="164">Hallo, ich bin Changbing, ein PhD-Student an der University of Washington. Heute präsentiere ich unser Werk von der Vervorhandenen Datenerhebung über Sprachmodelle bis hin zu unternehmenseitigen Aufgaben, die den Verlauf von politischem Bias nachverfolgen und zu unfairen NLP-Modellen führen.</sample>
    <sample id="165">Sprachmodelle werden auf großen Webkraw-Dateien trainiert.</sample>
    <sample id="166">Politische Medien werden in den Datensätzen der Sprachmodelle gut abgedeckt. Laut einem Survei von C4F Corpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. im Datensatz gut abgedeckt sind.</sample>
    <sample id="167">Dies hat zu einem gemischten Glücksfall für Sprachmodell-Anwendungen geführt.</sample>
    <sample id="168">Auf einer Seite hin konnten sie aus diversen Perspektiven lernen, die Demokratie und die Pluralität von Ideen feiern. Auf der anderen Seite sind diese verschiedenen politischen Meinungen inbegriffen sozial biasiert und können zu potentiellen Gerechtigkeitsproblemen in unternehmensinternen Anwendungskonzepten führen.</sample>
    <sample id="169">Um dies zu erreichen, planen wir, die politische Bias-Propaganda-Pipeline von der vor Trainingen zu den Sprachmodellen bis hin zu unternehmungsspezifischen Aufgaben zu untersuchen, indem wir die folgenden Fragen stellen:</sample>
    <sample id="170">Erstens, wie bewerten wir die politische Neigung von Sprachmodellen und welche Rolle das präexistierende Datenmaterial dabei spielen kann?</sample>
    <sample id="171">Zweitens, wie performieren Sprachmodelle mit verschiedenen politischen Neigungen auf Downstream Aufgaben und ob das zu Unfairness-Issues in NLP-Anwendungen führt.</sample>
    <sample id="172">So speziell, wir schließen zuerst prompt Sprachmodelle mit verschiedenen prompt-Formulierungen unter Verwendung von politischen Questionnaires wie dem politischen Kompetenztest. Dies sorgt für eine automatische Evaluation, die fundiert in der politischen Wissenschafts-Literatur ist.</sample>
    <sample id="173">Einige vorläufige Resultate demonstrieren, dass first language models auch variierende politische Leitlinien haben. Sie bedecken alle vier Quadranten auf dem politischen Kompass.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 die freiheitlichste Sprache ist von allen und GPT-Serien allgemein mehr sozial-liberal sind als BERT-Serie und seine Variante.</sample>
    <sample id="175">Zweitens versuchen wir zu untersuchen, in welchem Maß die politischen Biases von Sprachmodellen tatsächlich aus dem Trainingsdaten extrapoliert werden.</sample>
    <sample id="176">Wir können einen kontrollierten Versuch durchführen, indem wir Sprachmodell-Schneidpunkte auf sechs verschiedenen Parteien koppeln, die in News und Sozialmedien unterteilt sind, und dann in ihre politischen Linien unterteilt werden.</sample>
    <sample id="177">Durch die weitere vorherige Ausbildung von Sprachmodellen auf solchen Partizipierungen können wir sehen, dass die ideologischen Koordinaten des Sprachmodells auch entsprechend verschieben werden.</sample>
    <sample id="178">For example, for roberta further fine-tuned and further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">In terms of its political biases.</sample>
    <sample id="180">Wir versuchen auch zu untersuchen, ob Sprachmodelle in der Lage sind, die Polarisation, die in unserer modernen Gesellschaft vorherrscht, aufzunehmen.</sample>
    <sample id="181">Wir haben die vorherige Körperteilunterteilung in vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten geteilt. Wir separate vorherige Sprachmodelle auf die zwei verschiedenen temporären Körperteile.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle allgemein eine politische Neigung haben, die weiter weg vom Zentrum steht nach 2017. Das zeigt, dass Sprachmodelle auch die Art von Polarisation in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">So, last but not least, wir evaluieren Sprachmodelle mit verschiedenen politischen Neigungen auf Hate-Speech-Detection und Fake-News-Detection, zwei NLP-Anwendungen, die oft involven Sprachmodelle und könnten sehr signifikante Implikationen haben.</sample>
    <sample id="184">Also sehen wir, dass wenn wir die pro-Kategori-Performanz untersuchen, das ist zu sagen, wenn wir die Performanz in eine,</sample>
    <sample id="185">Different demographics or political leaning of news media, we can see a pattern that for example for hate speech detection left-leaning language models are better.</sample>
    <sample id="186">At detecting hate speech targeting socially minority groups,</sample>
    <sample id="187">Aber wir wären am besten in der Lage, Hasspeech zu identifizieren, die sich auf mehr mächtige Gruppen in unserer Gesellschaft konzentriert.</sample>
    <sample id="188">Und umgekehrt. Sprachliche Modelle sind besser darin, diskriminierende Sprache zu identifizieren, die eine Rasse und ein Geschlecht zielt auf, aber schlechter darin, diskriminierende Sprache zu identifizieren, die eine Rasse und ein Geschlecht zielt auf, die Afroamerikaner, LGBT+, und andere minderheitliche Gemeinschaften.</sample>
    <sample id="189">Ähnliche Trends finden sich auch in der Fängung von Falschinformationen, bei der wir feststellen, dass linkslinke Sprachmodelle besser darin sind, Lügen zu entdecken, die von rechtslinienen und umgekehrt.</sample>
    <sample id="190">This in we further show many qualitative examples to see that language models with different political leanings,</sample>
    <sample id="191">Gib verschiedenen Vorhersagen zu Hassreden und Falscherhaltungen basierend auf ihren sozialen Kategorien. Es gibt eine Reihe von weiteren Beispielen im Anhang, um dies zu verdeutlichen.</sample>
    <sample id="192">Dies zeigt, dass es ein Fairnessproblem gibt, das sehr beeindruckend ist, wenn es um die politischen Biases von Sprachmodellen geht.</sample>
    <sample id="193">Zum Beispiel, wenn wir Sprachmodelle trainieren und an Unhaltbarkeitsschwergewicht oder Fehlinformation und so weiter arbeiten und sie auf eine beliebte soziale Medien-Plattform ausrollen.</sample>
    <sample id="194">Das würde bedeuten, dass Menschen mit entgegengesetzten politischen Ansichten marginalisiert werden könnten und Hasssprache gegen minderheitliche Gruppen unkontrolliert weiterwachsen kann.</sample>
    <sample id="195">So, dies hat uns zu einem Ablenkung und Anzug anerkennen und die Fairness-issues, die durch Sprachmodell-politische Leanings entstehen, zu bewältigen.</sample>
    <sample id="196">Ein bisschen Diskussion. Wir würden auch gerne betonen, dass wir die einzigartige Dilemma bezüglich Sprachmulti politischer Biases aufwerten. Es ist wie zwischen Ceylon und Kribeis.</sample>
    <sample id="197">So, wenn wir nicht politische Meinungen in Sprachmodelltrainingdaten sauber machen, verbreiten sich die Biases von den vorher trainierten Daten über Sprachmodelle zuuntere Aufgaben aus und schaffen letztendlich Fairnessprobleme.</sample>
    <sample id="198">Wenn wir versuchen, etwas zu sanitieren, riskieren wir Sensibilitätsprobleme oder Exklusion. Es ist sehr schwierig zu bestimmen, was als neutrales und behalten werden sollte, in Sprachmaterialien. Das ist, wie man es sagt, ein elektrischer Stromproblem.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have to do. I'll be for today. Thank you for your time.</sample>
    <sample id="200">Es gibt mehrere Autoren an der Arbeit beteiligt.</sample>
    <sample id="201">MPP-Auswertungen wurden bis zu 1204 Token Kontextlängen durchgeführt.</sample>
    <sample id="202">Sie haben Domains wie Pianomusik, Worte, 12-jährige Kinder und ausländische Sprachen aufgenommen.</sample>
    <sample id="203">Positionalität bezieht sich auf die Perspektiven, die Menschen als Folge von ihr Demografie, Identität und Lebenserfahrungen halten.</sample>
    <sample id="204">Der Referent*in heißt Dawe.</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">Es wird nur ein Autor, Usain John, genannt.</sample>
    <sample id="207">Das Modell, das getestet wurde, scheint in der Testsuite nicht zu funktionsieren.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind: 1. Background Pretrain, 2. Background Both, und 3. Background Inferne.</sample>
    <sample id="209">Die Autoren gehören an der Jagiellonian University.</sample>
    <sample id="210">The final research question is whether we should only use the clean samples for validation or if there are better ways to utilize them.</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für dieselbe Aufgabe stets die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="212">Jing Wei Yi</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="214">Die Modelle erhalten den linguistischen Kontext der englischen Sprache während des Pre-Trainings.</sample>
    <sample id="215">Typically, we only need twenty clean validation samples per class to attain high performance.</sample>
    <sample id="216">Es ist nicht erwähnt, an welcher Universität die Autoren arbeiten.</sample>
    <sample id="217">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Vielfalt in den politischen Einstellungen von Sprachmodellen zu erfassen und zu messen.</sample>
    <sample id="218">The speaker's name is Matcha.</sample>
    <sample id="219">Die Pipeline beginnt mit der Vorbildtraining-Daten, dann geht sie über Sprachmodelle und endet in den Downstream-Aufgaben.</sample>
    <sample id="220">Ja, der Vereinfachungsprozess zwischen DEplain-apa und Web unterscheidet sich.</sample>
    <sample id="221">Coscript ist nicht öffentlich verfügbar. Es ist ein interne Modell der von Google Research verwendete wird. Es ist für die Forschung und die Entwicklung von neuen Modellen und Technologien gedacht, und es ist nicht für allgemein publiziert. Es ist auch nicht für die Nutzung von externen Benutzern. Es ist für die Forscher von Google Research und ihre Kollegen.</sample>
    <sample id="222">Das Wasserzeichen wird als Gewichtsummation des Zielsignal und des ursprünglichen Signals eingesetzt, wobei das Gewicht des Zielsignal proportional zur Anzahl der Auslöser im Text ist. Wenn die Anzahl der Auslöser im Text größer als M ist, dann wird das bereitgestellte Signal exakt gleich dem Zielsignal.</sample>
    <sample id="223">Die Autoren gehören an der Peking University.</sample>
    <sample id="224">Ja,Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist die Erstellung von Texten mit spezifischen Zielen und Bedingungen, wie zum Beispiel die Erstellung von Texten mit einem bestimmten Leselevel.</sample>
    <sample id="226">Sie validieren die Opazität der bereitgestellten Einbettung durch die Visualisierung der Einbettung von Sätzen auf der Basis von Figure 1.</sample>
    <sample id="227">Die Arbeit bestehende PLMs nutzt, um ein neues PLM aufzubauen, indem sie drei Modelle trainiert und konstant pretrainiert, um die Auswirkungen von Pretraining-Strategien zu analysieren.</sample>
    <sample id="228">GPT-4 ist am wenigsten auf China ausgerichtet.</sample>
    <sample id="229">Der Beispielsatz auf der rechten Seite zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde.</sample>
    <sample id="230">Die Anzahl der Aufgaben hat einen positiven Einfluss auf die Leistung des Modells.</sample>
    <sample id="231">Die drei baumlose Baselines, mit denen die Autoren ihre Methode vergleichen, sind Cogss Benchmark, TreeLSTM und TreeLSTM-TreeLSTM.</sample>
    <sample id="232">Die beiden Co-Autoren stehen als Berater (advisers) zum ersten Autor in einer gemeinsamen Arbeit (joint work) in Beziehung.</sample>
    <sample id="233">Der englische Text gibt nicht den Namen des Autors von PaLM an.</sample>
    <sample id="234">Hallo, alle. Ich bin Jenny, eine Bachelor-Studentin an Carnegie Mellon University. Heute werde ich präsentieren, wie wir die Positionalität von Design durch sichtbaren Datensätzen modellieren können.</sample>
    <sample id="235">Diese Arbeit wurde in Zusammenarbeit mit ein paar Leuten an der Université de Washington und dem AI-Institut der Allensworth, insbesondere Sebastian Santi, Ronin LeBross, Katerina Rynikha und Martin Sapp, getan.</sample>
    <sample id="236">Also, lass uns damit beginnen, uns vorzustellen, dass wir arbeiten an einem Zeitungstext und versuchen, unter einem Newsartikel diskursive Inhalte zu entfernen.</sample>
    <sample id="237">Sie könnten auf eine beliebte API wie die Perspektive-API zur Giftdetektion zurückgreifen. Und das funkt prima, wenn Sie Karl Jones sind, wo die Perspektive-API in der Lage ist, Giftbeispiele korrekt zu detektieren.</sample>
    <sample id="238">Aber das gilt nicht für Dithia Sharma, bei der prospective API nicht so empfindlich gegenüber offensive Begriffe ist, die in indischen Kontexten öfters verwendet werden.</sample>
    <sample id="239">Dies ist ein Beispiel für einen Designfehler, bei dem wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungen sehen.</sample>
    <sample id="240">Design Bias, wie wir vorhin gesehen haben, könnte aufgrund der Positionalität von NLP-Forschern und Modellentwicklern auftreten. Positionalität ist einfach die Perspektive, die Menschen als Folge ihrer Demografien, Identität und Lebenserfahrungen halten.</sample>
    <sample id="241">Dies ist ein Begriff, der in kritischen Studien, insbesondere in feministischen und queer akademischen Bereichen, weit verbreitet ist.</sample>
    <sample id="242">Und als Forscher kann die Positionalität die Forschungsprozess und dessen Ausgänge und Resultate beeinflussen, weil sie die Entscheidungen, die Forscher treffen, verändern kann.</sample>
    <sample id="243">Und so eine Frage, die sich die Leute manchmal stellt, ist, ob Datensätze und Modelle Positionalität haben.</sample>
    <sample id="244">Wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst eine demografische Identität und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von realen Menschen und können dadurch bestimmte Positionalitäten über andere repräsentieren.</sample>
    <sample id="245">So private Work hat ein paar Beispiele gegeben, die zeigen, dass es Unterschiede in den Art und Weise, wie Modelle positioniert werden, gibt. Das zeigt, dass es diverse Ansätze zur Positionierung von Modellen gibt, und es gibt auch verschiedene Begriffe, um das zu erklären.</sample>
    <sample id="246">Jedoch these Arbeiten beachten nicht die Vergleichung von Nutzern mit den Datensätzen und Modellen selbst.</sample>
    <sample id="247">Die Studie von Modell- und Datensatzpositionalität wird immer wichtiger, da NLP-Tests sich mehr subjektiv und sozial orientieren.</sample>
    <sample id="248">Und es ist schwierig zu charakterisieren, wie diese Positionalitäten sind, weil nicht alle Entscheidungen dokumentiert sind und viele Modelle hinter API's versteckt sind.</sample>
    <sample id="249">Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir tatsächlich die Annotationen mit denen von realen Benutzern mit existierenden Datensätzen und Modellen.</sample>
    <sample id="250">Wir tun das durch unser Framework NL Positionalität.</sample>
    <sample id="251">Unsere Framework arbeitet in zwei Hauptstadien.</sample>
    <sample id="252">Der erste Schritt ist, Datensätze mit diversen Annotatoren zu re-annotieren.</sample>
    <sample id="253">Wir optieren für diese Ansatz über die Analyse von Demographen der ursprünglichen Datensätze, da normalerweise nur wenige Annotatoren pro Instanz arbeiten und Demographen-Daten normalerweise seltener gesammelt und geteilt werden.</sample>
    <sample id="254">Und so we opfer re-initalize data, to get many antennas per instance and to get a rich set of demographic data.</sample>
    <sample id="255">Wir nehmen dann die Annotationen nach Demographics und vergleichen sie mit den Modellen und Datensätzen, indem wir den Pearson's R-Korrelationsscore verwenden.</sample>
    <sample id="256">Unser Rahmen unterscheidet sich daher von literaturwissenschaftlicher Disagreement-Literatur, indem er Endbenutzer mit Modellen und Datensätzen vergleicht, indem er Vorhersagen und Etiketten analysiert, anstatt lediglich die Übereinstimmung zwischen Annotatoren zu betrachten oder die Verteilungen von Annotatoren zu modellieren.</sample>
    <sample id="257">Unsere Infrastruktur ist hauptsächlich durch Lab in the Wild, eine Online-Krowdsourcing-Plattform, die von unserem ECHI-Kollega betreut wird, ermöglicht.</sample>
    <sample id="258">Lab in the Wild ist eine Online-Experimentationsplattform, bei der wir diverse volontärsrecruiten können, um sie mit Plaformen wie MTurk zu vergleichen, die hauptsächlich von den USA oder Indien aus stammen. Trotzdem ist Lab in the Wild in der Lage, hochwertige Daten zu erhalten.</sample>
    <sample id="259">Wir verwalten zwei Aufgaben im Leben im Wild. Eine davon bezieht sich auf soziale Akzeptabilität. Und das, was das bedeutet, ist, dass die Teilnehmer eine Situation aus dem Sozialchemie-Datensatz liest und dann schätzen, wie sozialschön eine bestimmte Situation ist.</sample>
    <sample id="260">Spätestens nachdem sie sich engagiert haben, können sie ihre Antworten mit jenen von einem AI und anderen vergleichen.</sample>
    <sample id="261">Wir haben dann dieAnnotations mit Sozialchemie, Delphi und QBDV verglichen.</sample>
    <sample id="262">Wir haben dann einen sehr ähnlichen Setup für die Toxizität und Hate-Speech-Detektions Aufgabe verwendet, bei der sie ein Beispiel aus Datasets Read und schreiben, ob sie denken, es ein Beispiel von Hate-Speech ist.</sample>
    <sample id="263">Wir verglichen dann diese Annotationen mit DinaHate, Perspective API, Rewire API, HateRuberta und GBP4. Unsere Studie umfasste über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">Jetzt sind wir besser auf der Höhe, um zu antworten, mit wem NLPA-Datasets und Modelle am besten übereinstimmen. Wir finden heraus, dass es Positioniertheit in NLP gibt.</sample>
    <sample id="265">Zum Beispiel finden wir, dass Datensätze und Modelle am besten auf englischsprachige Länder abgestimmt sind. Deshalb finden wir bei der GDP 4 Sozialakzeptabilität-Analyse, dass sie am besten auf Konfuzianismus und englischsprachige Länder abgestimmt ist. Wir finden auch, dass Dine-A-Hat am besten auf englischsprachige Länder abgestimmt ist.</sample>
    <sample id="266">Wir finden auch eine weitere Übereinstimmung mit Menschen, die eine höhere Bildung haben. So bei GPB 4 in der Aufnahmbarkeitstask finden wir, dass es am besten zu Menschen mit einer College- oder Graduiertstudies Bildung passt.</sample>
    <sample id="267">Wir finden das gleiche für Dunne-Hatte, wo es am besten auf Menschen mit einem College-Abschluss abgestimmt ist.</sample>
    <sample id="268">Allerdings werden Modelle und Datensätze, wenn sie auf bestimmte Gruppen abgestimmt werden, zwangsläufig einige hinterlassen.</sample>
    <sample id="269">Ein Beispiel für dies ist, dass Datensätze und Modelle für nicht-binäre Personen im Vergleich zu den Männer- und Frauen-Korrekten unterliegen. Wir finden dies in der GPD 4 Sozialakzeptabilität Aufgabe sowie im Dignihate Aufgabenauswertung auch.</sample>
    <sample id="270">Also, da es eine Position in Allied and NLP gibt, was können wir damit machen?</sample>
    <sample id="271">Wir haben einige Empfehlungen für dies. Der eine ist, einen Rekord von all relevanten Designentscheidungen während des Forschungsprozesses zu machen. Der andere ist, NLP-Forschung mit dem Lenz-Präsentivismus zu betreten.</sample>
    <sample id="272">Unsere dritte Empfehlung besteht darin, spezialisierte Datensätze und Modelle für bestimmte Gemeinden zu erstellen. Ein gutes Beispiel hierfür ist die Musakani-Initiative. Ich will betonen, dass inclusive NLP nicht einfach an alle Technologien anpasst, um für jeden zu arbeiten.</sample>
    <sample id="273">Und so, das schliesst unsere Präsentation. Aber falls Sie gerne mehr lernen möchten, können Sie gerne auf unserem Dashboard nach dem aktuellen Analysetheorem suchen und our paper überprüfen. Vielen Dank!</sample>
    <sample id="274">Die Referentin spricht von zwei Problemen der aktuellen SimulST-Modelle.</sample>
    <sample id="275">Die Reduktion sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ist schwierig, da es schwierig ist zu bestimmen, was als neutrales und should-retaines-Data gilt. Es besteht auch das Risiko von Sensibilisierung oder Exklusion, wenn versucht wird, die Verzerrungen zu neutralisieren.</sample>
    <sample id="276">Ich bin Si Yu from Fudan University. Ich bin hier, um unsere Arbeit zu Introduzieren, die Distinguierung von Sprachkenntnis und Sprachmodellen für kognitive Sprachverarbeitung.</sample>
    <sample id="277">In all our daily lives, humans often plan their actions by following step-by-step instructions in the form of guaranteed scripts.</sample>
    <sample id="278">Eine vorherige Studie hat Sprachmodelle verwendet, um abstrakte Aufgaben von stereotypen Aktivitäten wie das Backen eines Kekses zu planen und zu zeigen, dass große Sprachmodelle effizient Aufgaben in Schritte zerlegen können.</sample>
    <sample id="279">Allerdings konzentriert sich die vorherige Arbeit hauptsächlich auf die Planung abstrakter Ziele von seriotypischen Aktivitäten. Die Planung für spezifische Ziele mit spezifischen Bedingungen, wie zum Beispiel das Making von Schokoladenkuchen, ist immer noch unerforscht.</sample>
    <sample id="280">In diesem Papier definieren wir das Problem der kontraindexischen Sprachplanung.</sample>
    <sample id="281">Eine abstrakte Zielfunktion kann von verschiedenen realen, spezifischen Zielen mit mehreren Einschränkungen erfüllt werden. Ein guter Planer sollte Pläne erstellen, die vernünftig und den Bedingungen entsprechend sind.</sample>
    <sample id="282">In diesem Papier evaluieren und verbessern wir die Sprachplanfähigkeit von Sprachmodellen.</sample>
    <sample id="283">Es gibt keine Art von spezieller Goltscheibe, die uns heute sputern kann.</sample>
    <sample id="284">Wir müssen zuerst die Kosten erhalten, wie in der Tabelle gezeigt. Wir erweitern die abstrakte Gabe mit mehreren Bedingungen für die menschliche Datenerkennung. Um die Lese- und Schreiboperationen zu optimieren, verwenden wir die Instanz CPT.</sample>
    <sample id="285">Wir haben 100 spezifische Go-Tests erstellt und die resultierenden Scripts von großen Modellen evaluiert.</sample>
    <sample id="286">Das Tabell zeigt die allgemeine Genauigkeit der verschiedenen Sprachmodelle. Wir haben festgestellt, dass alle Sprachmodelle unzufriedenstellende Ergebnisse aufwarten, wenn es um das Planen für bestimmte Ziele geht.</sample>
    <sample id="287">Dann machen wir eine detaillierte Analyse, um die Leitlinienmodelle zu untersuchen.</sample>
    <sample id="288">Die in der Figur dargestellten Resultate zeigen, dass die syntagmatische Komplettät in generierten Skripten akzeptabel ist, aber die Treue zur Restriktionen nicht gewährleistet werden kann.</sample>
    <sample id="289">Wir teilen die Kriterien in Mikrowellen in mehrere feinere Kategorien auf. Die Grafik zeigt, dass die Leistungsstelle von Mikrowellen für Personen unterschiedlicher Kategorien erheblich variiert.</sample>
    <sample id="290">Vorherige Studien haben gezeigt, dass die Ausgabekonstante von Sprachmodellen stark variieren kann, was zu schlechter Leistung führt. Daher adoptieren wir das Konzept des Over-Generators-Z-Filter, um die Generationsqualität zu verbessern.</sample>
    <sample id="291">Wir zeigen zuerst constraint types mit Beispielen für intrakorporelle PPT und erhalten spezifische Gegenstände basierend auf den abstrakten Gegenständen.</sample>
    <sample id="292">Dann instrukt GPT allgemeinisierte Kasscripts für spezifische Goethe.</sample>
    <sample id="293">Nächster, ein Filtermodul ist entworfen, um die visuellen Spektren zu selektieren.</sample>
    <sample id="294">Wir konvertieren Skripts und Goethe in extrakt GBPT Einblicke und berechnen Kosinus Ähnlichkeitsscores, um semantische Ähnlichkeiten zu messen.</sample>
    <sample id="295">In addition, wir warten auf den Script, der die Schlüsselwörter des Zielsatzes enthält. Wir only koppeln den Script, wenn der Target-Goal-Score die höchste im goal-Set ist.</sample>
    <sample id="296">Mit unserem Ansatz können wir eine Vielzahl von hochwertigen Skripten generieren. Unsere Methode verbessert die Planbarkeit sowohl in Bezug auf semantische Vollständigkeit als auch auf die Einhaltung der Beschränkungen.</sample>
    <sample id="297">Da Sprachmodelle teurer zu deployen sind, ist es von Bedeutung, die Sprachplanerfähigkeit kleiner und spezialisierter Modellen zu fördern. Das Erstellen von Datensätzen ist ein wichtiger Schritt dabei.</sample>
    <sample id="298">Allerdings machen vorherige Studien keine planmäßige Auswertung für spezifische Ziele und die manuelle Datensatznotation ist teuer.</sample>
    <sample id="299">Wir folgen dem Gedanken der symbolischen Knowledgestillition, um die constrainte Sprachplanning-Datasets von large-language-modellen zu distillieren.</sample>
    <sample id="300">Wir übertragen unser Verfahren für die Erstellung von Datensätzen für konsolidiertes Sprachplanung unter dem Namen CoScript.</sample>
    <sample id="301">Insgesamt generieren wir 55.000 spezifische Güte- und Skripten, um die Qualität der Validierung und Testdateien zu gewährleisten. Wir bitten Cloud-sourcing-Fachwives, die inkorrekten Samples zu finden und zu korrigieren.</sample>
    <sample id="302">Diese Figur zeigt die konstante Verteilung von Co-Skript Wir finden Co-Skript zeigt die Hauptrollen in der generativen spezifischen Rolle mit Co-Skript wir können kleine aber spezialisierte Modelle für constraint language planning.</sample>
    <sample id="303">Wir haben festgestellt, dass T-F-Model-Entscheidungsreihenfolge die Generation von Skripts mit einer Qualität, die über die meisten großen Sprachmodelle hinausgeht, ermöglicht. Dies zeigt, dass kleinere Modelle in bestimmten Fällen besser als größere Modelle arbeiten können, wenn sie auf geeignetem Datensatz trainiert werden.</sample>
    <sample id="304">Zusammenfassend haben wir das Problem der eingeschränkten Sprachplanung festgestellt. Wir haben auch die eingeschränkte Sprachplanungsfähigkeit großer Sprachmodelle analysiert und einen allgemein anwendbaren Filtermechanismus für große Sprachmodelle entwickelt.</sample>
    <sample id="305">Wir verwenden große Sprachmodelle, um einen hochwertigen Quellen-Datensatz (CoScript) für Sprachplanung zu generieren. Wir hoffen, dass dieser Datensatz ein wertvoller Ressource zur Fortschrittssuch in der Sprachplanung werden kann.</sample>
    <sample id="306">Danke für Ihr Interesse. Bitte finden Sie mehr Details über Cooscript in unserem Papier.</sample>
    <sample id="307">Die Sprachgewandtheit von PaLM ist vergleichbar zu anderen Systemen, aber die Hauptunterschiede liegen in der Genauigkeit.</sample>
    <sample id="308">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: es sollte auf Embedding-As-Service-Technologien angewendbar sein, das Wasserzeichen sollte die Nutzen der Bereitgestellten Embedding-Technologien nicht beeinträchtigen, das Wasserzeichen sollte für den Angreifer schwierig zu entfernen ist, und das Wasserzeichen muss übertragbar auf die Angreiferservices während des Modell-extraktionsprozesses sein.</sample>
    <sample id="309">The English TED Talks were translated into 14 different languages.</sample>
    <sample id="310">Typischerweise werden nur wenige Annotatoren pro Instanz verwendet.</sample>
    <sample id="311">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind Kosinus- und L2-Similarität.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt, um die besten Leistungen auf allen neun Datensätzen zu erzielen.</sample>
    <sample id="344">Die Autoren nehmen an, dass der Anbieter eine allgemeine Textkorpora sammeln und die Häufigkeit jedes Wortes therein zählen kann.</sample>
    <sample id="345">Hallo, alle. Mein Name ist Zhu Huang. Heute werde ich einen Vortrag über unser Papier halten: "Do CNN 2013 named entity taggers still work well in 2023?". Lassen Sie uns beginnen.</sample>
    <sample id="346">Unsere Publikation hat die Problematik der Generalisierbarkeit anhand des Named Entity Recognition (NER) Aufgabentyps untersucht.</sample>
    <sample id="347">Wir bemerkten, dass Modelle seit 2003 den Cornell-Modelle zur Entwicklung von NER verwendet haben und dies natürlicherweise einige Probleme aufwirft. Zunächst einmal, können diese Modelle auf moderne Daten übertragen werden?</sample>
    <sample id="348">Und wenn wir neue Tag-Users entdecken, was ist nötig für eine gute Generalisierbarkeit?</sample>
    <sample id="349">Gleichermaßen, wenn wir eine schlechte Generalisierbarkeit beobachtet, was verursacht die Performancedrohung dieser Modelle?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir die Cornell-Plus-Plus-Datenbank entworfen. Dies ist eine Datenbank, die wir aus Reuters News von 2020 gesammelt und dann mit den gleichen Cornell 2013-Annotationsehrenkarten annotiert haben.</sample>
    <sample id="351">Wir haben dann über 20 Modelle auf KORL 2003 fine-tuned. Wir haben sie auf beiden dem KORL 03 Testset und dem KORL Plus-Plus Testset evaluiert.</sample>
    <sample id="352">Zuletzt, aber nicht zuletzt, haben wir den prozentualen Änderungsanteil in F1 berechnet, um die allgemeinheitsestima von Modell zu bewerten.</sample>
    <sample id="353">So, was ist für eine gute Generalisierung notwendig? In unserem Experiment haben wir festgestellt, dass es drei Hauptingredients bedarf.</sample>
    <sample id="354">Der erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren.</sample>
    <sample id="355">Der zweite Faktor ist die Modellgröße. Wir haben festgestellt, dass normalerweise größere Modelle zu besseren Generalisierungen führen.</sample>
    <sample id="356">Zuletzt und nicht zuletzt wissen wir alle, dass die Anzahl der Fine-Tuning-Beispiele direkt auf die Performance einer Downstream-Aufgabe wirkt. Hier haben wir auch festgestellt, dass mehr Fine-Tuning-Beispiele tatsächlich auch zu besseren Generalisierungen führen.</sample>
    <sample id="357">To our next question what causes the performance drop of some models?</sample>
    <sample id="358">Wir hatten zwei Hypothesen. Die erste ist adaptive Overfitting, das Overfitting, das durch die Wiederholung des gleichen Testsets immer wieder auftritt und das normalerweise als die Verminderung der neuen Testset-Performance manifestiert wird.</sample>
    <sample id="359">Die zweite Hypothese ist Temporaldrift, die das Performance-Degradation ist, das durch die zunehmende Temporalausgleichung zwischen den Train- und Test-Daten verursacht wird.</sample>
    <sample id="360">For adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one.</sample>
    <sample id="361">Das bedeutet, dass jede Einheit Verbesserung, die wir auf Karna 2013 erreicht haben, zu mehr als einer Einheit Verbesserung auf Karna ++ führt. Das bedeutet, dass es keine diminuierenden Rückstände gibt.</sample>
    <sample id="362">Und dies zeigt uns, dass adaptive Überschätzung in diesem Fall nicht auftritt.</sample>
    <sample id="363">So was es denn mit tempo trifft?</sample>
    <sample id="364">Für temporale Drift haben wir ein Experiment durchgeführt, um einige Modelle mit jüngerem Datensatz zu retrainieren oder fortzusetzen, die vorher trainiert wurden. Wir haben festgestellt, dass die Leistung mit einem größeren Temporalgap abnimmt.</sample>
    <sample id="365">Und dies bestätigt unsere Hypothese, dass die Hauptursache des Leistungsabgangs die Temporverdrift ist.</sample>
    <sample id="366">Unsere Schlussfolgerung lautet, dass für eine gute Generalisierung wir ein besseres Modellarchitektur, größere Modellgröße und mehr fine-tuning-Beispiele benötigen. Und diese Goalseitig sind wir nicht nur zu einem Ingredient zu haben, sondern überall die anderen.</sample>
    <sample id="367">Gleichermaßen haben wir festgestellt, dass die Leistungsabnahme hier durch temporale Drift verursacht wird und, was überraschenderweise ist, nicht durch adaptive Overfitting, obwohl Connell 2013 über 20 Jahre verwendet wurde.</sample>
    <sample id="368">Also, zurück zu der Frage, die wir im Titel unseres Papiers aufgeworfen haben: GILT KONNEN 2003 TAGGER NOCH IM 2023? Und wir haben festgestellt, dass die Antwort ein überzeugender Ja ist.</sample>
    <sample id="369">We hope our paper causes more research on how to improve generalizations of the models.</sample>
    <sample id="370">Und letztens, bitte vergewissere dich, dass du unser Papier, unser Datensatz überprüft hast und falls du Fragen hast, mich gerne kontaktieren solltest. Danke sooooooviel!</sample>
    <sample id="397">Die Sprachsegmentgröße wird auf 100 Wörter festgelegt.</sample>
    <sample id="398">Im Beispiel wird entitätsspezifisches Wissen über Servin benötigt, um zu bestimmen, dass er ein Richter ist.</sample>
    <sample id="399">Die Qualität des Beispiels ist wichtiger als die Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="400">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf GPT-4 und seine Variationen, die allgemein sozial-liberaler sind als BERT-Serie.</sample>
    <sample id="401">Das Modell kombiniert Werte aus mehreren Ebenen.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind die Angabe des Namens der Lied, der Position, oder der Art der Lied.</sample>
    <sample id="403">Die Autoren gehören an Fudan University.</sample>
    <sample id="404">Ein Autor arbeitet an der Arbeit beteiligt.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells vor dem semantischen Parsing als Baseline betrachtet wurde.</sample>
    <sample id="406">The authors give the example of the word "warrior" being usually associated with men, and when describing a woman warrior, specifying "one woman warrior" to mark the term.</sample>
    <sample id="407">Die Transformer-Modelle normalerweise nicht gut.</sample>
    <sample id="408">Die Testdatensätze heißen Finetuning und WSL.</sample>
    <sample id="409">Die Arbeit beteiligen sich zwei Autoren, namens Matcha und Martin.</sample>
    <sample id="410">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="439">Nach Ansicht der Autoren ist ein zu wenig erforschtes Gebiet im Bereich der NLU die Integrität und Nutzung von vorher trainierter und inferenzzeitlicher Kenntnis.</sample>
    <sample id="440">The speakers are Ying and Jiaxuan.</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchlaufen.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in der Unterstützung lediglich begrenzter Typen von kontextbasierten Übersetzungen und begrenzten Mengen an Sprachen, da sie normalerweise von dominanter Kenntnis und menschlicher Curation abhängen.</sample>
    <sample id="443">Hallo, ich werde heute über unser Projekt sprechen, bei dem wir indirekte Verweisungen für die Selektion von Entitäten aufklären. In unserem Ansatz verwenden wir die Alte Entitätskore.</sample>
    <sample id="444">Mein Name ist Javad Hosseini und dies ist ein gemeinsames Werk mit Philipp Radlinski, Silvia Paritti und Annie Louise.</sample>
    <sample id="445">Unsere Zielsetzung ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten. Überlegen Sie sich also diese alternative Frage: Meinten Sie 'einfach für mich' oder 'ich habe ein Problem'? Hier will der Benutzer zwischen zwei Optionen entscheiden.</sample>
    <sample id="446">Die offensichtlichste Sache ist es zu verwenden Direktrefrenz. Zum Beispiel, indem man den Namen der Song "Easy on Me" oder seine Position, die Erste, sagt.</sample>
    <sample id="447">Aber manchmal ist es besser, einen direkten Freispruch zu erhalten, um eine natürlichere Unterhaltung zu haben. Das könnte passieren, wenn der Benutzer den Namen des Songs nicht mehr weiß.</sample>
    <sample id="448">All the pronunciations are too similar to each other and hard to disambiguate.</sample>
    <sample id="449">Oder wenn der Benutzer eine spezifische Präferenz angeben möchte. Hier sind einige Beispiele für direkte Preferenzangaben, zum Beispiel die Änderung der Farbe oder die Einführung von Energieeffizienz.</sample>
    <sample id="450">Dies ist ein wichtiger Problem in kommunikationssystemen und auch für Benchmarking LLMs Entity-Verstehens.</sample>
    <sample id="451">Wir sind nicht darüber informiert, dass es eine große öffentliche Datensammlung für die Aufgabe gibt. Daher sammeln wir eine Datensammlung mit der Hilfe von Crowdsourcing. Unsere Datensammlung deckt drei verschiedene Domänen ab: Musik,bücher und rezepte.</sample>
    <sample id="452">Our data set collection methodology emphasizes informality using a cartoon completion set.</sample>
    <sample id="453">Das Cartoon hat drei Sprachbubbles. In dem ersten Bubblen sagt Bob: "Erinnere dich an das Lied, das wir gestern gehört haben." Und mit dieser Erinnerung beginnt Bob die Dialogkonversation.</sample>
    <sample id="454">In this in the second speech bubble alice says do you mean easy on me or i got a feeling</sample>
    <sample id="455">Which is the alternative question. And in the third speech bubble Bob uses an indirect reference to select one of these entities, for example the new</sample>
    <sample id="456">Wir bieten die ersten und zweiten Sprachbubbles automatisch, aber der dritte wird von dem Notrufassistenten eingegeben. Um die ersten Sprachbubbles zu bestimmen, werden paarweise manuelle Anregungen verwendet.</sample>
    <sample id="457">Die zweite, die alternative Frage wird wie folgt generiert.</sample>
    <sample id="458">Wir verwenden immer einen einfachen Vorlage. Möchten Sie A oder B? Hierbei sind A und B Beispiele von Wikipedia.</sample>
    <sample id="459">Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir uns weiter in der Liste bewegen, werden die Entitäten immer ähnlicher einander und es wird normalerweise schwieriger zu machen, die Disambiguierung zu erreichen.</sample>
    <sample id="460">Der Erste ist uniformer Trans.</sample>
    <sample id="461">Der zweite Fall ist, wenn die Entitäten ähnliche Titel haben. Zum Beispiel zwei Bände mit dem Titel "Die ...".</sample>
    <sample id="462">Der dritte Fall ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und letztendlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel denselben Genres oder denselben Künstler.</sample>
    <sample id="463">Wenn wir diese alternative Frage den Amerikanern zeigen, kennen sie den Namen dieser Entitäten, aber sie kennen nicht zwangsläufig über die Entität.</sample>
    <sample id="464">So, was wir tun, ist, dass wir ein paar Hintergrundwissen über die zwei Entitäten zeigen. Für Songs zeigen wir einfach einen Google-Suchlink zu jeder Song.</sample>
    <sample id="465">Und dann bitten die Übersetzer, sich zumindest ein paar Lieder anhören und über sie informieren. Hier ist zum Beispiel der Google-Suchergebnis-Link für das Lied "Easy".</sample>
    <sample id="466">Für die Ressourcen- und Bibliotheksdomain zeigen wir einige Hintergrundtexte aus Wikipedia an. Für Rezepte zeigen wir auch noch ihre Bilder von Wikipedia an, damit die Annotatoren sehen können, wie sie aussehen.</sample>
    <sample id="467">Dann fordern wir die Annotatoren, eine dieser Entitäten zu warten – zum Beispiel hier die erste – und sie mit 3-5 indirekten Beispielsätzen zu beschreiben.</sample>
    <sample id="468">Beispielsweise der von der Pianomusik. Hier sind einige Beispiele aus unserem Datensatz. Zum Beispiel der von without words, nicht der von der 12-jährigen 12-jährigen Mädchen oder der fiktionalen von, oder kommt aus Aserbaidschan und so.</sample>
    <sample id="469">Das Altenentities Corpus hat 6.000 alternative Fragen über drei Domänen und 42.000 indirekte referring Expressions. Resultate mit dem T5-XL-Modell werden summarisiert.</sample>
    <sample id="470">Wenn das Sprachmodell Zugang zu den genauen gleichen Grundwissen als die Notrufleitern hat, dann die Genauigkeit ist wirklich hoch. Es liegt um 92 bis 95 prozent. Aber das ist nicht realistisch.</sample>
    <sample id="471">Wenn das Sprachmodell Zugang zu ein paar teilweise überlappenden Hintergrundkenntnissen hat, dann liegt die Genauigkeit zwischen 82 und 87 Prozent, was realistischer ist. Zum Beispiel, wenn das Sprachmodell Hintergrundkenntnisse abrufen kann.</sample>
    <sample id="472">Wenn die Sprachmodell nur Zugriff auf Firmen-Namen hat, dann ist die Genauigkeit nur 60%. Das gibt viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle über Domänen hinweg allgemein anwenden lassen werden. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="473">Der Ansatz wird mit den bestehenden SimulST-Richtlinien verglichen, indem er mit verschiedenen Strategien wie der Wetkey-Strategie, dem Lokalvertrag und dem State-of-the-Art-Architekturansatz, insbesondere jenem, das speziell für Simultansprachenspeicherung konzipiert wurde,-compared.</sample>
    <sample id="474">Die Autoren gehören an Université de Lorraine.</sample>
    <sample id="475">The presenter is Jenny, a first-year PhD student at Carnegie Mellon University.</sample>
    <sample id="476">Drei Autoren sind an der Arbeit beteiligt: Esser Darmush, Dan Juravsky und die Sprecherin Maya.</sample>
    <sample id="477">Hallo, ich bin Sarah Papi von der Universität Trento und Fondazione Bruno Kessler. Ich werde kurz einen Bericht als Leitstelle für Simultane Sprachübersetzung vorstellen, das eine gemeinsame Arbeit mit Mateo Negri und Marco Turki ist.</sample>
    <sample id="478">Was ist parallele Sprachübersetzung? Paralleles Sprachübersetzung, auch bekannt als Simultanübersetzung (Simultaneous Translation), ist der Prozess, einen gesprochenen Sprache in eine andere Sprache zu übersetzen, in Echtzeit. Dies ermöglicht eine direkte Kommunikation zwischen Menschen, die verschiedene Sprachen sprechen.</sample>
    <sample id="479">Und was sind die Probleme der aktuellen symbolischen Modelle? Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche Module optimiert werden.</sample>
    <sample id="480">Lang und komplizierte Trainingprozeduren, z.B. Training, das verschiedene Optimierungsziele einbezieht,</sample>
    <sample id="481">Und trainieren und erhalten mehrere Modelle, um verschiedene Latenzregimes zu erreichen. Zum Beispiel trainieren ein Modell mit einem Durchschnitt von 1 Sekunde Latenz und ein anderes Modell mit zwei Sekunden Latenz usw.</sample>
    <sample id="482">So, was ist unsere Lösung?</sample>
    <sample id="483">Erstens verwenden Sie bestehende off-the-shelf-Modelle ohne erneute Trainierung oder die Anpassung an spezifische Architektur für SimILST. Zweitens verwenden Sie nur ein Modell pro Latenzregimes und handeln Latenz durch spezifische Parameter.</sample>
    <sample id="484">Und Nutzen der Kenntnisse, die durch die Aufmerksamheitsmechanismen zwischen Audio-Eingabe und Textausgabe erworben wurden, das ist der Kreuzaufmerksamheitsmechanismus. Und Sie können ein Beispiel auf der rechten Seite sehen.</sample>
    <sample id="485">Unsere Lösung ist es, die Achtung zu kodieren oder zu kodieren und decodieren und es ist eine Strategie, bei der wir entscheiden, ob wir einen teilweisen Übersetzungsbetrag einreiten oder nicht, basierend auf, wohin die Aufmerksamkeit zeigt.</sample>
    <sample id="486">Eine Welle wird emitted, wenn die Tension nicht konzentriert ist, d. h. wenn das Sum ist unter einem bestimmten Schwelle α , in Richtung der last lampas speech frames, was bedeutet, dass die empfangene Information nicht stabil genug ist.</sample>
    <sample id="487">Zum Beispiel, wenn wir erhalten eine Sprachstelle, die 'Ich werde über das sprechen' enthält und unser Modell die Übersetzung in Deutsch vorhersagt,</sample>
    <sample id="488">Und wir werden uns ansehen, dass die kruzationswerte</sample>
    <sample id="489">Wir sehen, dass die ersten zwei Werte Punkte zu den earliest received speech frames hinweisen, während der letzte Wert Punkte zu den last received speech frames als Lambda speech frames hinweist.</sample>
    <sample id="490">Das bedeutet, dass die ersten zwei Worte weggelassen werden.</sample>
    <sample id="491">Während das Summe der Kreisattension über einem bestimmten traditionellen Alpha liegt, warten wir nicht auf die letzte Welle und warten auf einen neuen Pulsch.</sample>
    <sample id="492">Wenn wir fortfahren und ein anderes Sprachsegment erhalten und unser Modell weitere drei Worte vorhersagt, werden wir uns ansehen, wie diese Koppelgewichter aussehen.</sample>
    <sample id="493">Wir werden sehen, dass nur Worte Punkte zu den last lambert speech frames.</sample>
    <sample id="494">Das bedeutet, dass diese drei Worte emittiert werden.</sample>
    <sample id="495">If you look at the main results of that,</sample>
    <sample id="496">Wir plotten die simultaneous speech translation results auf on graphs in welchen wir haben blau auf einer Seite, das die Translation Quality und Average Leitstelle messen.</sample>
    <sample id="497">Das ist die Latenzmessung und wir betrachten auch die computational-aware Average Latency, die für die Modellcomputational Times zur Berechnung des Outputs sorgt.</sample>
    <sample id="498">So wir wollen our Cure's to be as high as possible on this plot.</sample>
    <sample id="499">Auch wir wollen, dass sie auf der linken Seite sind.</sample>
    <sample id="500">Wir vergleichen mit anderen Strategien, die auch auf offline-Modellen angewandt werden, wie der Weighted-Knoss-Strategie und dem Lokalvertrag. Wir vergleichen auch mit einem State-of-the-Art-Systemarchitektur, speziell auf Simultaneous Pre-Translation (SPT) ausgerichtet.</sample>
    <sample id="501">Dies sind alle Resultate der Simultane Sprachübersetzung-Strategie auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass AdaBoost überwiegt alle Strategien, die auf Olaf-Modellen angewandt werden, da ihre Kurven über links verschiebt.</sample>
    <sample id="503">Und wir sehen auch, dass wenn wir die tatsächliche Verizzaszeit oder die computational-aware Zeit berücksichtigen, der AT the fastest strategy ist.</sample>
    <sample id="504">Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier und wir haben auch Open Source die Code- und Modelle- und Simultaneous-Hauptautoren freigegeben, um die Wiedervereinbarkeit unseres Werks zu fördern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="506">Hallo, alle. Mein Name ist Ying und mein Kollege Jian und ich werden unsere Forschung über Multi-Instruct präsentieren: Improving multi-modal zero-shot learning via instruction tuning.</sample>
    <sample id="507">Mit Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu untersuchen, bei denen pre-trained Sprachmodelle für verschiedene downstream Aufgaben in einem parameter- und dateneffizienten Weise verwendet werden.</sample>
    <sample id="508">Kürzlich haben viele Studien gezeigt, dass die Einrichtung von Tuning große Sprachmodelle dazu verhilft, auf eine kurze Art und Weise Aufgaben in einem menschlichen Stil zu bewältigen, indem sie natürliche Anweisungen folgen.</sample>
    <sample id="509">Allerdings konzentrieren sich die meisten vorherigen Arbeiten auf die Verbesserung der Serienleitperformanzen an Sprach-only- Aufgaben, während Computer Vision und multimodale Aufgaben weggelassen wurden.</sample>
    <sample id="510">Daher in diesem Werk wollen wir untersuchen, ob die einstimmige Tuning oder multimodale Pretraining-Modelle tatsächlich die Generalisierbarkeit zu einem multimodalen Task verbessern.</sample>
    <sample id="511">Darüber hinaus, zu Zeiten unseres Forschungsprojekts, haben wir eine beachtliche Discrepanz in der Verfügbarkeit von Datensätzen für die Sprachverarbeitung und Multimodalität entdeckt.</sample>
    <sample id="512">Es existieren mehr als 1.600 Sprachspezifische Datensätze für die Ausbildung von Maschinenlernmodellen. Allerdings gibt es momentan keine großen, öffentlich zugänglichen Datensätze für die Ausbildung von multimodalen Modellen. Daher motiviert uns, einen Datensatz für die Ausbildung von multimodalen Modellen zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir Multi-Instruct, das erstes multimodales Instruktions-Tuning-Benchmarks-Datensatz, der bestehend aus 62 verschiedenen multimodalen Aufgaben, die 10 Hauptkategorien abdecken.</sample>
    <sample id="514">Diese Aufgaben werden aus 21 bestehenden offenen Quelldatasets abgeleitet und jedes ist mit 5 expliziten Anweisungen ausgestattet.</sample>
    <sample id="515">For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pretraining model as our base model. OFA uses a unified vocabulary for language, image tokens, and the coordinate of a bounding box.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem Multi-Instr-Dataset.</sample>
    <sample id="517">To unify the processing of various input and output data types.</sample>
    <sample id="518">Wir folgen dem Ansatz von OFA und formulieren alle Aufgaben in einem vereinbarten sequenz zu sequenz Format, in dem die Eingabe-Texte, -Bilder, Anweisungen undBounding Boxes im gleichen Tokenraum dargestellt werden.</sample>
    <sample id="519">Okay, now I'm going to talk about multi-modal instruction tuning.</sample>
    <sample id="520">So für die Trainingsdatenbank verwenden wir 53 Aufgaben von Kaggle zur Training und wir sampling 10.000 Instanzen pro Aufgabe. For testing, wir reservieren die gesamte Commonsense Reasoning Gruppe für Testing und wir select additional five tasks from WikiArt and the MedL dataset.</sample>
    <sample id="521">Wir verwenden alle Instanzen in der Testsplit für jede Aufgabe. In addition, wir randomisieren 20 Aufgaben aus dem Testsplit von Natural Instructions als "synth" Aufgaben für NLP.</sample>
    <sample id="522">So wir verwenden einen prätrainierten OFA-Large-Modell als Basismodell. During Training wir machen all die Instanzen für alle die Aufgaben. Jede Instanz wird zufällig mit einem von sechs Einrichtungstemplaten kombiniert.</sample>
    <sample id="523">During tests for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="524">Wir berichten die minimum und maximale Performance und die Standarddeviation der Performance über alle five Experiments.</sample>
    <sample id="525">Wenn die Aufgabe ein multimodales Klassifizierungsproblem ist, berichten wir über die Genauigkeit. Wenn es ein multimodales Generationsproblem ist, berichten wir über Rouge-L. Wenn es ein NLP-Problem ist, berichten wir über Rouge-L ebenso.</sample>
    <sample id="526">Wir haben auch einen zusätzlichen Evaluationsmaßstab namens Sensitivität eingeführt. Dies messt die Fähigkeit des Modells, für dieselbe Aufgabe stets die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="527">Hier sind unsere Hauptergebnisse. Wie wir sehen können, kann die Optimierung der Anweisungen signifikant die Leistung von OLS-OLS auf mehrmodellischen Aufgaben verbessern.</sample>
    <sample id="528">Auch Transferlearning von natürliche Instruktionsdatensets kann Vorteile für Instruktions tuning.</sample>
    <sample id="529">Hier können wir sehen, dass der Betrag der Aufgaben steigt, die Modell erreicht eine bessere Leistung und in der meantime eine tieferen Sensitivität.</sample>
    <sample id="530">So wir haben auch die x experimente, wir verwenden eine instruction versus five instruction. als wir sehen, using more instructions can improve the model's overall performance and reduce its sensitivity a lot.</sample>
    <sample id="531">So zeigt dies den Effekt verschiedener Finetuning-Strategien auf der Modell sensitivität. Wie wir sehen können, durch Transfer learning von Natural Instruction Datensätzen, kann das Modell viel bessere Sensitivität erreichen als das ursprüngliche LLM-Modell.</sample>
    <sample id="532">We can also see transfer learning from the netron instruction data set can help our f to achieve much better performance on the netron instruct data set.</sample>
    <sample id="533">So, wir haben einen großen Datensatz für die Tuning von Multimodal Übersetzungen erstellt. Dieser Datensatz verbessert die Übersetzungsfähigkeit von O-F-A und zeigt auch die Vorteile verschiedener Transferlearning-Techniken an. Wir haben auch eine neue Metrik namens Sensitivität entworfen.</sample>
    <sample id="534">So, ein anderes Ding: wir sammeln eine viel größere multimässige Instruktions- und Tuning-Datenbank mit around 150 zusätzlichen Wörterbuchsprachen Aufgaben und wir werden sie freigeben. Also, das ist ein QR-Code für unser Data und Modell. Danke.</sample>
    <sample id="535">Die Autoren, Matteo Negri und Marco Turki, gehören an die University of Trento.</sample>
    <sample id="536">Javat Hosseiny</sample>
    <sample id="562">Hallo, alle. Ich bin Costas Tsonga und ich freue mich, Sie zu unserem Talk über unser ACL 2023 Papier zu begrüßen: Sprachmodell-Aczeptabilitätsschwämmungen sind nicht immer robust gegenüber Kontext.</sample>
    <sample id="563">Eine gemeinsame Arbeit mit John gowthorpe, aram kouler, kancha mishra, garen fuentes, roger levy und adina villamor.</sample>
    <sample id="564">So in diesem Werk revisitieren wir die minimal pareto-</sample>
    <sample id="565">So die Minimal Paar-Paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump syntax-jam or acceptability in terms of stereotypes such as crows percs.</sample>
    <sample id="566">In diesem Minimalpaar paradigm, die typische Art zu evaluieren Sprachmodelle ist, dass Sie zeigen, wie eine akzeptable Satz oder grammatikalische Satz und dann Sie zeigen, wie ein nicht akzeptierbarer Satz oder ungrammatikalischer Satz.</sample>
    <sample id="567">And then the hopes of the model basically puts more probability to the acceptable</sample>
    <sample id="568">Der aktuelle mpp pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences.</sample>
    <sample id="569">These days, large language models are coming up with longer and longer context windows. So it's crucial that we evaluate the model's acceptability throughout the context window.</sample>
    <sample id="570">Und das ist, was wir versuchen zu tun. Hier versuchen wir, den NPB-Pipeline zu revisitieren, indem wir den Modell zu evaluieren versuchen, auf langer und langer Sequenzen.</sample>
    <sample id="571">So, das ist der Ansatz. Also, was wir tun, ist, die zu simulieren, diese längeren Sequenzen. Wir revisitieren die Datensätze selbst und dann wir recreaten Sätze, indem wir auswählen, wie akzeptabel oder unakzeptabel Sätze aus den Datensätzen.</sample>
    <sample id="572">For example, here we have chosen like a typical pair of grammaticality from the bllm data set from the adjunct island case.</sample>
    <sample id="573">Und was wir tun, ist, die zu recreieren, l. . . , langer Sequenzen und welche akzeptabel sind und welche das gleiche Matching der grammatischen Struktur hat. Wir extrahieren grammatischen Satz von Agustinus.</sample>
    <sample id="574">Und dann added as a prefix to both the acceptable query and the unacceptable query.</sample>
    <sample id="575">So, wir können dasselbe machen, indem wir unakzeptable Sätze aus dem gleichen Matching auswählen und das könnte auch verwendet werden, um die Akzeptierbarkeit des Modells zu testen.</sample>
    <sample id="576">Und wir können auch das Gleiche tun, indem wir Sätze aus einem anderen Datensatz oder einer anderen Substelle auswählen. Das nennen wir als Missmatch-Szenario.</sample>
    <sample id="577">So hier die Sätze sind, die immer noch aus relevanten Datensätzen stammen, aber sie sind nicht aus demselben Datensatz, den Sie verwenden. Und wir können das gleiche für unakzeptierbares Material tun.</sample>
    <sample id="578">Endlich können wir Sätze aus einem vollkommen unverwandten Feld, wie Wikipedia,</sample>
    <sample id="579">So, dies wird uns verraten, ob die Modell akzeptierbarkeitsbewertungen tatsächlich beeinflusst werden, ob es einen Kontext gibt.</sample>
    <sample id="580">Whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at.</sample>
    <sample id="581">So, wie das Modell tut? Zunächst schauen wir uns die Wikipedia-Sätze an, die für die aktuelle Suchanfrage completely irrelevant sind. Und dort finden wir, dass die MPP-Jugendmeßungen hauptsächlich robust für arbiträre Kontexte sind.</sample>
    <sample id="582">We increase the context length toward up to 204 for to max out op t and gpt-2 models and we saw here in the orange dot line the mpp judgments are relatively stable.</sample>
    <sample id="583">Now what happens when we choose sentences from the same dataset?</sample>
    <sample id="584">So hier, wir schaffen or creating sentences from acceptable and unacceptable domains from the same blimp or syntaxem dataset.</sample>
    <sample id="585">And there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable</sample>
    <sample id="586">But when we match the structure, that is when we choose the sentences from the same phenomena in blame person text jim,</sample>
    <sample id="587">Wir sehen einen massiven Anstieg oder einen massiven Abfall in der MPP-Judgment für den Modell, je nachdem, ob der gewählte Prefix akzeptabel oder unakzeptabel ist.</sample>
    <sample id="588">Now, this and this is very large. Like this effect increases throughout the context length, and this would probably affect like newer language models which has large context window.</sample>
    <sample id="589">Warum beeinflusst der Match-Prefix die Sprachmodellbewertungen so stark?</sample>
    <sample id="590">So wir haben eine Serie von Analysen durchgeführt, in der wir versucht haben, die Eingabe-Texte zu perturbieren, indem wir versucht haben, die relevanten Strukturen beizubehalten, aber Geräusche hinzuaddieren. Und nachdem wir mehrere solcher Perturbationen durchgeführt hatten,</sample>
    <sample id="591">We find that none of these noises are actually making the model like change it course in terms of how it shows us the nppp just been trying.</sample>
    <sample id="592">Basically, we find that the models are sensitive to the perturbation of sentences in similar ways.</sample>
    <sample id="593">Das ist, wenn wir die Sätze im akzeptablen Bereich perturbieren, sehen wir einen ähnlichen Anstieg in allen Perturbationen. Und wenn wir die Sätze im nicht akzeptablen Bereich perturbieren, sehen wir einen Decrease in MPP-Bestimmungen in ähnlicher Weise.</sample>
    <sample id="594">So die Haupttakeaways von unserem Werk sind, dass Sprachmodelle Empfindungen latent syntagmatischen und semantischen Merkmals, die über die Sätze verteilt sind.</sample>
    <sample id="595">Und die MPP Evaluation, die derart, wie wir es momentan mit kurzen und einfachen Sätzen ausführen, nicht vollständig das abstrakte Wissen der Sprachmodelle im Kontextwindow capturieren kann.</sample>
    <sample id="596">Bitte liest unsere Papier für mehr Details über unsere Experimente. Danke fürs Zuhören.</sample>
    <sample id="597">Die Input-Token werden mit einem unsortierten Multi-Set von Tokenen zugeordnet, die im Output vorkommen werden.</sample>
    <sample id="598">Fünfzigtausend Skripte sind in Coscript vertreten.</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEplain ist die Methode von Massalign.</sample>
    <sample id="627">Der Vorteil von schwach überwachtem Lernen ist, dass es robustere trainierte Modelle produziert, die besser generalisieren können.</sample>
    <sample id="628">Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet. Die Zuteilung wurde manuell durch die Verfasser der Dokumente und automatisch durch die Verfasser des DEplain-Web-Systems durchgeführt.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde erstellt, indem eine Datensammlung von Reuters News aus dem Jahr 2020 anhand der gleichen Annotieranweisungen wie das CoNLL-2003-Datensatz annotiert wurde.</sample>
    <sample id="630">Hallo, alle. Mein Name ist Yusen Zhang vom Peking University. Heute werde ich unsere Arbeit präsentieren: "Exemplar-basierte SemantikParsing in mehreren natürlichen Sprachen und minimalen Darstellungen".</sample>
    <sample id="631">So semantische Parsening ist eine Aufgabe, um semantische Darstellungen von Benutzerabfragen zu erstellen, wie SQL und Lambda Calculus.</sample>
    <sample id="632">Crosslinguistic semantic parsing ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehreren minimal-repräsentativen Formen zu übersetzen.</sample>
    <sample id="633">In dem gezeigten Diagramm müssen wir die Abfragen in mehreren natürlichen Sprachen übersetzen, indem wir neuronale Modelle wie Sequel, Lambda, FunkQL und so weiter verwenden.</sample>
    <sample id="634">Bestehende kroatisch-schwedische Semantikparsing-Modelle werden separat vorgeschlagen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet. Zum Beispiel,</sample>
    <sample id="635">Es gibt Lücken in der Abdeckung von bestimmten natürlichen Sprachen. Der Chinesische ist fehlend und</sample>
    <sample id="636">Lack of coverage on certain minor representations.</sample>
    <sample id="637">Der Lymphkalkülus ist weg.</sample>
    <sample id="638">Oder sie werden nur auf bestimmte numerische Modelle evaluierbar. Zum Beispiel gibt es nur einen einzigen Modell zur Evaluierung.</sample>
    <sample id="639">Um zu diesem Ende schaffen wir exemplar einen uniformen Datensatz exemplar für korsische semiotische Persönlichkeiten in mehreren natürlichen Sprachen und vielen Repräsentationen.</sample>
    <sample id="640">Es enthält neunzig Datensätze in verschiedenen Domänen, 500 SemantikParsing Tüx, 80 millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Um unser Benchmark besser zu evaluieren, haben wir die sechs Einstellungen für Training und Evaluation in Betracht genommen.</sample>
    <sample id="642">Der Erste ist der Translationstest. Wir verwenden die Google Translate-API, um den Quelltext in die Zielsprache zu übersetzen, und dann verwenden wir einen Monolingual-Modell zur Training und Evaluation.</sample>
    <sample id="643">Zum Beispiel trainieren wir den englischen Modell auf englische Abfragen und während der Inferenz übersetzen wir die deutsche Abfrage mithilfe einer API ins Englische und dann verwenden wir den trainierten Modell um die SQL-Abfragetexte zu generieren.</sample>
    <sample id="644">Wir werden auch einen monolingualen Modell testen.</sample>
    <sample id="645">In diesem Setting ist die Quelle Sprache dieselbe wie die Zielsprache. Zum Beispiel: Deutsch zu Deutsch, oder Englisch zu Englisch.</sample>
    <sample id="646">Wir testen auch monolingue Few-Shot-Setzungen, indem wir Monolingual-Modelle mit nur 10 % der Trainingsdaten trainieren.</sample>
    <sample id="647">Und wir testen einen monolingual-multilinguale Modell, das wir für alle Sprachen trainieren.</sample>
    <sample id="648">Zum Beispiel wirbeln wir deutsche, englische und chinesische Suchanfragen zusammen, um ein multilinguale Modell zu trainieren. Und während der Inferenz können wir dann dieses Modell verwenden, um...</sample>
    <sample id="649">Um englische Anfragen in deutsche Anfragen zu übersetzen, können Sie verschiedene Ansätze verwenden. Hier sind einige Schritte, die Sie beachten können:

1. Identifizieren Sie die englischen Wörter und Phrasen in der Anfrage.
2. Finden Sie die entsprechenden deutsche Übersetzungen für diese Wörter und Phrasen.
3. Stellen Sie sicher, dass die Grammatik und das Satzkonstrukt korrekt verwendet werden.
4. Überprüfen Sie die Übersetzung, um sicherzugehen, dass sie den ursprünglichen Sinn beibehält.

Alternativ können Sie auch Online-Übersetzungen verwenden, um englische Anfragen in deutsche Anfragen zu übersetzen. Einige beliebte Online-Übersetzungen sind Google Übersetzer, DeepL Übersetzer und Bing Übersetzer.</sample>
    <sample id="650">Und wir berücksichtigen auch Crosslinguistic Zero-Shot und Few-Shot-Übertragung. Wir trainieren auf einer Sprache und übertragen zu einer anderen Sprache.</sample>
    <sample id="651">During training, we train on English queries or the combination of English and German few-shot queries to train a multilingual model to predict the SQL output.</sample>
    <sample id="652">Wir haben auch einige interessante Ergebnisse entdeckt. In Bezug auf die Analyse von monolingualen Modellen evaluieren wir zwei Gruppen von Modellen,</sample>
    <sample id="653">Inklusive Encoder PDR, das steht für multilinguisten trainierte Encoders mit Pointer-basierten Decoder wie XLNet+PDR und Bert+PDR.</sample>
    <sample id="654">Wir evaluieren auch Incoder-Decoder-Modelle, die multilinguistisch trainiert sind, wie zum Beispiel Mbart und M5.</sample>
    <sample id="655">Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf all neun Datensätzen erreicht.</sample>
    <sample id="656">Wir evaluieren auf MT5 und einem XLMR plus PDR auf multilinguistischem Setting.</sample>
    <sample id="657">Wir fanden, dass Encoder-Decoder oder Encoder-PDR verbessert werden können, indem sie in einer Mischung von verschiedenen Sprachen trainiert werden.</sample>
    <sample id="658">Wir haben festgestellt, dass es because die meisten der Hauptnatürlichen Sprachen einenPerformancedienst erhalten können, aber das englische Performance sinkt in sieben Datensätzen und only in drei Datensätzen steigt.</sample>
    <sample id="659">Ich denke, dies wird als Kurse der Multilingualität bezeichnet.</sample>
    <sample id="660">Wir haben auch die Längsschnecke-Performancedistanz verglichen.</sample>
    <sample id="661">In dieser Figur ist die blaue Linie der Transfer von Crosslingo-Phrasen, die orange Linie ist der Transfer von Crosslingo-Zerlegungen und die grüne Linie ist die Modellgenerierung.</sample>
    <sample id="662">Wir haben festgestellt, dass durch die Vergleichung der Grün- und Orange-Linie, bei einer Null-Schaltausstelle die Transfer-Performance-Abteile significant sind. Und durch die Vergleichung der Blauen- und Orange-Linie, bei einer Few-Schaltausstelle die Transfer-Abteile schneller kürzen.</sample>
    <sample id="663">Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel überwiegen Encoder-Decoder-Modelle die herkömmlichen Ansätze bei der Übersetzung von englischer ins natürliche Deutsch und significantly boost the performance of few-shot learning on target natural languages.</sample>
    <sample id="664">Wir haben festgestellt, dass multilinguale Sprachmodelle wie Codas und Blue immer noch ungenügend für Übersetzungen von einem Sprachmodell ins andere sind.</sample>
    <sample id="665">Zusammenfassung: Wir bieten Exemplar, einen vereinbarten Standard für die Übersetzungsfehleranalyse mit mehreren natürlichen Sprachen und vielen Repräsentationen.</sample>
    <sample id="666">Wir haben eine umfassende Benchmark-Studie an drei verschiedenen Typen von multilinguen Sprachmodellen durchgeführt. Unsere Untersuchung hat viele interessante Erkenntnisse erbracht und so weiter. Wir freuen uns, Sie zu unserem Papier und Code zu begrüßen. Vielen Dank für Ihr Interesse.</sample>
    <sample id="667">Die Arbeiten, die bereits durchgeführt wurden, können allgemein in vier Kategorien unterteilt werden.</sample>
    <sample id="668">Nein, sie sind noch nicht ausreichend für CLSP.</sample>
    <sample id="695">Die Methode mit der Mehrdeutigkeit der Permutationen wird durch die Einbeziehung der Ausrichtung als Teil des Trainings und die Nutzung eines GPT-freundlichen kontinuierlichen Relaxationsansatzes bewältigt.</sample>
    <sample id="696">Die Fairness eines nachgeschalteten NLP-Modells wird definiert, indem man überprüft, ob das Modell Menschen mit verschiedenen politischen Ansichten fair behandlesst und ob es diskriminierende oder diskriminierende Ausdrücke enthält.</sample>
    <sample id="697">Der Referent*in heißt Manis Lavac.</sample>
    <sample id="698">Der Referent*in heißt Kostrubienah.</sample>
    <sample id="699">The speaker's name is Myra.</sample>
    <sample id="700">Tropikalismus bezieht sich auf die Verwendung von bestimmten Begriffen, um Frauen von Farbe zu beschreiben.</sample>
    <sample id="701">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie die Topwörter analysieren, die Menschen benutzen, um ihre Gruppen zu definieren.</sample>
    <sample id="702">In this work, CXM is extended to point-wise CXM, which can measure context usage at the sentence level or at the word level.</sample>
    <sample id="703">DrBERT und ChuBERT sind zwei verschiedene Modelle, die auf unterschiedlichen Datensätzen trainiert wurden. DrBERT wurde mit 7GB von NAOs trainiert, während ChuBERT eine Kombination von 4GB von NAOs und 4GB von klinischen Knoten trainiert wurde.</sample>
    <sample id="751">Es gibt zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="752">Iteratives Transferlernen ist ein Ansatz, bei dem das Modell durch die Schulung auf den neuesten Datensatz aktualisiert wird.</sample>
    <sample id="753">Das Ziel des Datensatzes ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Verarbeitung von Eingabe-Output-Daten durch das Modell simuliert und die Ausgabe für verschiedene Eingaben analysiert.</sample>
    <sample id="755">Drei Autoren sind an der Arbeit beteiligt: Matteo Negri, Marco Turki und Sarah Papi.</sample>
    <sample id="756">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">Die Autoren, Sebastian Santi, Robin Le Bras, Katrina Rihanika und Martin Sapp, gehören an die University of Washington.</sample>
    <sample id="758">Das Beispiel mit dem Begrenzer auf der linken Seite lautet "I saw Bart and Lisa".</sample>
    <sample id="759">ABC-Eval ist in der Lage, die Geschwindigkeit zu messen, bei der Sprachmodelle verschiedene thematische Fehlertypen machen.</sample>
    <sample id="760">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil es heute große Sprachmodelle mit immer größeren Kontextfenstern gibt.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="762">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="763">Die MT-Metriken BLEU, METEOR, TER und chrF wurden verwendet.</sample>
    <sample id="764">Ja, die Regression beeinflusst die Generalisierung auf bestimmte NER-Typen.</sample>
    <sample id="765">Positionalität ist für NLP (Natural Language Processing) wichtig, weil es die Unterschiede in der Technologie und den Leistungen zwischen verschiedenen Bevölkerungen und Kulturen hervorgehoben. In dem Beispiel wird gezeigt, dass ein AI-Modell wie Perspective API in einem englischen Kontext besser performiert als in einem indischen Kontext. Das zeigt, dass Positionalität eine Rolle spielt und beachtet werden sollte, um faire und effektive AI-Systeme zu entwickeln.</sample>
    <sample id="766">LLMs wurden durch Adapter angepasst.</sample>
    <sample id="767">Das Modell, das verwendet wird, um das Transferlernen zu initiieren, ist das Modell CE.</sample>
    <sample id="768">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die SQuAD 2.0 und NewsQA.</sample>
    <sample id="769">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">Die vorgeschlagene Methode zeigt einen hohen Applaus in den generierten Spezifizierern.</sample>
    <sample id="771">The speaker's name is Chu Huang.</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden.</sample>
    <sample id="773">In der Arbeit werden 12 kleineren Modellvarianten experimentiert.</sample>
    <sample id="774">OFA, ein vereinbarten multimodalen Präsentiermodell, wird als Basismodell verwendet.</sample>
    <sample id="833">Die Autoren gehören an der Stanford University.</sample>
    <sample id="834">Die Autoren gehören an Stony Brook University.</sample>
    <sample id="835">Die englische Sprache wurde in der Arbeit untersucht.</sample>
    <sample id="836">The presenter is named Changbing.</sample>
    <sample id="837">Die Modelle, die während der Experimente untersucht wurden, sind das Modell von Long Impart und das Normal Base Long Impart-Modell.</sample>
    <sample id="838">Die 53 verschiedenen Aufgaben in MultiInstruct werden für die Schulung verwendet, und 20 weitere Aufgaben werden zufällig aus dem Testblock von Natural Instructions ausgewählt.</sample>
    <sample id="839">Es wird nur eine Person, Regina Stönn, erwähnt, die an der Arbeit beteiligt ist.</sample>
    <sample id="840">Die Autoren haben experimentiert an den Datensätzen AGNews, Mind, SST2 und ER-Sem.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">Der Referent im Video heißt Ida Vilad.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung der LLMs für die Übersetzung.</sample>
    <sample id="879">I'm sorry, I cannot answer this question as the given sentences do not provide information about the university affiliations of the authors.</sample>
    <sample id="880">Die 5 Anweisungen der Expert*innen lauten: 1. Verwenden Sie den englischen Text als Referenz, um die Bedeutung des ursprünglichen Textes zu verstehen. 2. Identifizieren Sie die Hauptidee des ursprünglichen Textes und versuchen Sie, sie in einem anderen Sprachsystem zu übertragen. 3. Beachten Sie dabei die kulturelle, soziale und politische Kontexte, in denen der ursprüngliche Text entstanden ist. 4. Vermeiden Sie die direkte Übersetzung von Begriffen und Wendungen, indem Sie sich an die Nuancen und Subtletäten der Zielsprache halten. 5. Überprüfen Sie die Übersetzung, um sicherzustellen, dass sie den ursprünglichen Text korrekt und respektvoll widergibt.</sample>
    <sample id="881">Die Autoren schlagen vor, die Datensätze mit Human-Studie-Patienten zu evaluieren und aufgestellte Konsensus-Resolution-Modelle zu verwenden, um die Fähigkeit zu testen, auf Informationen aus mehreren Quellen zu zugreifen.</sample>
    <sample id="882">Hallo, alle. Mein Name ist Ilya Vladyk und ich werde Ihnen einen kurzen Überblick über die Arbeit mit Google Translate geben, einschließlich Strategien und Leistung. Dies ist ein Joint-Werk mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">Bam ist ein Sprachmodell mit 540 Milliarden Parametern, das vor einem Jahr, im Jahr 2022, präsentiert wurde. Es wurde auf einer großen Sammlung von Texten trainiert, die 780 Milliarden Token umfasst.</sample>
    <sample id="884">Die detaillierte Abfolge der Aufgaben ist auf dem Stand der Technik inundreds von ERP-Systemen.</sample>
    <sample id="885">In this work, we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="886">Wir bewerten die Translationsfähigkeit solcher Modelle, indem wir die besten Praktiken der M2P-Community verwenden. Daselsinki Involves using the latest test sets to avoid an overlap of the test data with the training data of the language model.</sample>
    <sample id="887">And we compare two state-of-the-art systems. So the best performing systems are the davenport evaluation</sample>
    <sample id="888">Wir verwenden state-of-the-art und neuartige Metriken und zusätzlich auch Expertenbasierte Immunanalytionsergebnisse. Ferner, wir bieten einige Empfehlungen für Prognosestrategien.</sample>
    <sample id="889">Das Prompting hat einen großen Einfluss auf die Leistung der EL-NeMs für die Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir ein einstelliges Prompting verwenden und zwei verschiedene Prompts für ein Satzpaar verwenden.</sample>
    <sample id="890">Die Mehrheit der Sätze (516 von 1000) die Differenz aufzuweisen hat, ist von mehr als einemBlur-Point.</sample>
    <sample id="891">Und das kann in extremen Fällen bis zu 40 Punktetagen aufwachsen. Also ist es wichtig, eine gute prompting-Strategie zu selectieren.</sample>
    <sample id="892">In our experiments we evaluate for a five shot prompting strategy where we just mark each its sentence that we provide to the system with the language it's in.</sample>
    <sample id="893">In this example here where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon.</sample>
    <sample id="894">Wir sahen, dass die tatsächliche Form der Drucke nicht einen großen Einfluss im Fall von mehreren Druckstempeln hat.</sample>
    <sample id="895">Es ist kritisch für null und einschuss prompting und wenn wir in unserem Fall zu fiveschuss prompting gehen, gibt es in der Tat nur noch einen Unterschied zu der tatsächlichen Form der prompting.</sample>
    <sample id="896">It's the examples that carry most of the weight.</sample>
    <sample id="897">Die Auswertung unseres Versuchsergebnisses zeigt, dass die Qualität des Beispieldokuments wichtiger ist als die Ähnlichkeit zum Quelltext.</sample>
    <sample id="898">Es ist wichtig, Beispiele aus hochwertigen Übersetzungen zu selecting. Im Speziellen vergleichen wir die selecting prompts aus dem train data der TUM Evaluierungen oder dem dev data.</sample>
    <sample id="899">Die Dev-Data ist viel besser qualitativ und quantitativ als die Train-Data, was zu einem besseren Modellresultat führt.</sample>
    <sample id="900">Trotzdem haben spezialisierte Systeme von Übersetzungen einen wesentlichen Vorteil über Bahn-translations. Aber Bahn kommtPretty nahe an einem kommersiellen System heran. In unserem Fall haben wir uns für Google-translations entschieden.</sample>
    <sample id="901">Die Einsichten, die wir aus dem dynamischen Analysetrauma gewinnen, das wir mit dem MQM-Modell durchführen, bestehen darin, dass die Flussigkeit von Palm vergleichbar zu den Systemen der Art ist, aber die Hauptunterschiede liegen in der Genauigkeit.</sample>
    <sample id="902">In particular the most common error are omission errors.</sample>
    <sample id="903">Es scheint, dass Palm Entscheidungen trifft, um eine bessere Übersetzung zu produzieren, manchmal indem er Teile des Quelltextes weist, die in der Übersetzung irrelevant sind.</sample>
    <sample id="904">Jedoch ist die Stelle, an der Pann in der Kategorie für Pannen abgeht, kleiner als die Stelle, an der Pann in der Kategorie für die Systeme abgeht.</sample>
    <sample id="905">Dass PAMR eine sehr fluide Ausgabe bietet, aber immer noch mit ein paar Problemen der Genauigkeit.</sample>
    <sample id="906">And that's it for this really short overview for more details please come my to the full presentation of the paper thank you very much</sample>
    <sample id="907">Hallo, ich bin Dawe, ein PhD-Student an der Carlston University in Deutschland. In diesem Video möchte ich gerne unsere jüngste Arbeit präsentieren: "Wichtiger als du denkst: Eine kritische Analyse von Weekly Superstore".</sample>
    <sample id="908">Dieses ist einJoint-work mit Chih-Yu Chen, Myron Smucker und Gary Stephen und Dietrich Klau.</sample>
    <sample id="909">Ich möchte beginnen mit einer kurzen Einführung in die week supervision und weekliesupervision</sample>
    <sample id="910">In weak supervision, you do not manually label the data instead we label the data using weak labeling sources such as simple heuristics rules, knowledge bases or low-quality crowdsourcing as illustrated in the figure on the right.</sample>
    <sample id="911">When compared to human annotations, weak annotations are much cheaper yet they are also noisy, meaning that a certain amount of the annotations are incorrect.</sample>
    <sample id="912">Wenn wir neuronalen Netze direkter mit weeklabeled Daten trainieren, tendieren sie dazu, den Label-Noise zu memorieren und nicht zu generalisieren.</sample>
    <sample id="913">In quickly superposed learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well.</sample>
    <sample id="914">In jüngster Zeit wurde das WSL verwendet. Das steht für Weekly Supervised Learning. Ein gemeinsamer Ansatz ist, dass die Leute Ansätze mit der weekly-labeled-Daten trainieren und dann eine hohe Leistung auf einem sauberen Testset erzielen.</sample>
    <sample id="915">Technisch gesehen ist dieser KBitte nicht falsch, aber es gibt einen Haken.</sample>
    <sample id="916">Das ist, dass Menschen davon ausgehen, dass es einen zusätzlichen, klare Validationsset gibt, der für die ModellSelektion verwendet werden kann.</sample>
    <sample id="917">Wir kasten doubt on this problem setting as this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked.</sample>
    <sample id="918">Die oben genannte Aufgabe besteht darin, zu fragen: Erstens ist eine validierungsdatenbank für WSL notwendig? Oder können wir eventuell einen noisigen Validierungsset anstelle verwenden?</sample>
    <sample id="919">Zweitens, wenn sauberer Datensatz erforderlich ist, oder wenn sauberer Datensatz für WSL notwendig ist, dann wie viele saubere Proben benötigen wir? Schließlich sollten wir nur die sauberen Proben verwenden, um zu validieren, oder gibt es bessere Methoden, sie zu nutzen?</sample>
    <sample id="920">Wir adressieren diese Forschungsfragen in unserem Werk und unsere Findungen lauten wie folgt.</sample>
    <sample id="921">Erstens finden wir, dass interessanterweise recente WSL-Methoden in der Tat eine klare, widersprüchliche Sammlung zu benötigen haben, um korrekt zu arbeiten.</sample>
    <sample id="922">Andernfalls gibt es einen großen Performance-Sprung, wie in diesem Diagramm gezeigt. Wenn es keine klaren Validationsamples gibt, dann können die trainierten Modelle nicht über die ursprünglichen WIC-Labels hinaus generalisieren.</sample>
    <sample id="923">Dass die Training ist sinnlos.</sample>
    <sample id="924">Dies zeigt, dass WSL-Modelle tatsächlich eine sauber labelierte Datensammlung benötigen, um korrekt zu arbeiten. Die Noten costs für die Gewinnung von sauberen Validationsamples sollte nicht übersehen werden.</sample>
    <sample id="925">Unser zweiter Fund ist, dass das Erhöhen der Anzahl der validierbaren Ausprägungen helfen wird, WSL-Methoden zu verbessern, wie im linken Bild gezeigt.</sample>
    <sample id="926">Typically, we only need twenty samples per class to attain high performance.</sample>
    <sample id="927">Aber das ist nicht das Ende der Geschichte, denn wenn wir entweder eine ausreichende Anzahl an sauberen Datensamples beschaffen oder auf direkte Auswertung der Datensamples abzielen, dann erreichen wir durch die direkte Auswertung besserer Leistungen.</sample>
    <sample id="928">Die rechte Figur zeigt die Leistungsunterschiede zwischen Finetuning-Abläufen, die direkt auf die Clean-Daten angewandt werden, und WSL-Abläufe, die die Clean-Daten nur zur Validierung verwenden.</sample>
    <sample id="929">As we can see if we have ten samples per class, direct fine-tuning starts to beat wsl approaches.</sample>
    <sample id="930">Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansaugen behauptet wurde, durch die Erlaubnis zu einem fortlaufenden Finetuning der Clean-Validation-Samples einfach erreicht werden.</sample>
    <sample id="931">Wie wir aus den Grafiken sehen können, unterperformiert der Wavenet-Modell-Typ FTDW ursprünglich komplexere WSL-Methoden wie cosine.</sample>
    <sample id="932">However, if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other</sample>
    <sample id="933">Also in der Praxis gibt es keinen Grund, komplexere WSL-Methoden zu verwenden, die mehr Rechenzeit und Festplattenplatz benötigen.</sample>
    <sample id="934">Zusammenfassend haben wir gezeigt, dass recente WSL-Modelle von sauber, manuell annotierten Datensätzen abhängig sind, um korrekt zu arbeiten. Ihre Leistungsgewinne und Praktizität werden stark überbewertet.</sample>
    <sample id="935">Unsere kompletten Empfehlungen für zukünftige Arbeiten lauten wie folgt.</sample>
    <sample id="936">Erstelle eine deutsche Übersetzung des englischen Inhalts.</sample>
    <sample id="937">Zweitens, WSL-Objekte sollten mit kürzlen Leitlinien Baselines als ob sie Work on CLI samples drittes continuus fine-tuning is a simple yet strong baseline that should be considered in future work in WSL.</sample>
    <sample id="938">Endlich haben wir Open Source unser Code. Sie können es unter der QR-Code auf dieser Slide finden. Bitte fühlen Sie sich frei, es zu überprüfen. Danke und genießt den Kongress.</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogsysteme sind die Anwendung von menschlicher Bewertung, indem man menschlichen Richtern fragt, welche der beiden Konversationen besser ist oder sie auf einer Likert-Skala bewertet.</sample>
    <sample id="940">In dem englischen Text werden insgesamt sechs Autoren genannt, die an der Arbeit beteiligt sind.</sample>
    <sample id="941">In dem Beispiel wird das Hintergrundwissen verwendet, dass Richter Fälle in einem Gerichtssaal entscheiden.</sample>
    <sample id="942">Ja, der Code ist verfügbar und es steht auf GitHub.</sample>
    <sample id="943">Ja, die Annotatoren für NLPositionality sind in Bezug auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="944">Satz: "We tried to like perturb the input sentence by trying to preserve the relevant structure but adding uh like noise to the input."</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet die Beurteilung mehrerer Aspekte der Dialogqualität, um ein detaillierteres Verständnis der Stärken und Schwächen des Modells zu erhalten.</sample>
    <sample id="946">Die Autoren gehören an der University of Science and Technology of China.</sample>
    <sample id="947">Die Form des Prompts ist wichtig, wenn es sich um Null oder ein einziges Prompting handelt.</sample>
    <sample id="978">The authors evaluated several dialogue models, but the specific names of the models are not mentioned in the given sentences.</sample>
    <sample id="979">Die englische Textstelle gibt an, dass es einen Autor gibt, der an der Arbeit beteiligt ist. Es wird erwähnt, dass der Name "Jing Wei Yi" genannt wird, der von der University of Science and Technology of China stammt. Es wird auch erwähnt, dass es sich um eine kurze Reklametafelhandlung handelt, die über ein Papier informiert, das "Protecting the Copyright of Large Language Models for Embedding and Services"題する。</sample>
    <sample id="980">Ein guter Planer sollte Skripts schreiben, die realistisch und respektvoll gegenüber den Bedingungen sind.</sample>
    <sample id="981">Die englische Version des Textes lautet: "Hi, I'm Si Yu Yan from Fudan University. I'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning." Hierbei sind zwei Autoren beteiligt - der sprechende und Si Yu Yan.</sample>
    <sample id="982">Der Referent*in heißt Vasudha.</sample>
    <sample id="983">The authors belong to the University of California, Berkeley.</sample>
    <sample id="1021">Die häufigsten Fehler von PaLM sind omission errors.</sample>
    <sample id="1022">Hallo, ich bin James Finch und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABC-E-Val erzählen, ein neues dimensional-basiertes Verfahren zur Evaluierung von conversationaler KI.</sample>
    <sample id="1023">Diese Arbeit wurde von dem Emory NLP Lab, geleitet von Professor Geno Choy an der Emory University, und in Zusammenarbeit mit Amazon Alexa AI erstellt.</sample>
    <sample id="1024">Also, lassen Sie uns annehmen, dass Sie ein Dialogmodell entwickelt haben und Sie sich fragen, wie gut es sich gegen das aktuelle Stand der Kunst vergleicht.</sample>
    <sample id="1025">Die gemeinsame Praxis besteht darin, menschliche Beurteilung zu verwenden, indem man beispielsweise menschlichen Richtern fragt, welche der zwei Konversationen besser ist oder Konversationen nach einem Likert-Skala bewertet.</sample>
    <sample id="1026">Diese Ansätze funktioniieren gut, um eine holistische Beurteilung der Gesamtqualität von Dialogen zu erhalten. Aber die Qualität von Dialogen hat viele Aspekte. Daher könntest du mehrere Dimensionen der Chatschwelle bewerten, um die Stärken und Schwächen des Modells auf einem feineren Granularitätslevel zu verstehen.</sample>
    <sample id="1027">Eine Annäherung besteht darin, einfach mensliche Richter zu bitten, mehrere Dimensionen der Dialogqualität zu bewerten, wie die Relevanz von Modellresponses, indem sie existierende vergleichende oder Likert-Skala-Methoden verwenden.</sample>
    <sample id="1028">Allerdings glauben wir, dass es eine präzisere und zuverlässlichere Strategie für die Dimensional-Dialog-Evaluation gibt.</sample>
    <sample id="1029">Unsere Ansatz versucht, die Subjektivität menschlicher Bewertung zu reduzieren, indem wir explizit feststelle, ob jede Modellresponse bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel das Ausgeben von irrelevanter Information oder sich selbst widersprechen.</sample>
    <sample id="1030">Wir nennen diese Ansatz Annästern von Verhaltensweisen in Chat, also ABC-Eval in Kürze. Wir haben diesen Ansatz entwickelt, um die Verhaltensweisen von Chat-Modellen abdecken zu können, die in jüngster Literatur zur Chatschwelle vorgeschlagen wurden.</sample>
    <sample id="1031">ABC-Eval ist in der Lage, die Geschwindigkeiten zu messen, bei denen Sprachmodelle verschiedene thematische Fehlertypen machen.</sample>
    <sample id="1032">Beispielsweise misst ABC-Eval die Anzahl der Worte, in denen ein Chatschwamm seinen Partner ignoriert oder etwas Unrelevantes sagt.</sample>
    <sample id="1033">Widersagt sich selbst oder seinem Partner, behauptet falsche Tatsachen oderVioliert allgemein anerkannte Wissen und zeigt Empathie, wenn das Modell succeeded or fails.</sample>
    <sample id="1034">Um zu bestimmen, welche Art von Evaluation am besten wirkt, haben wir vier modernste Chattenmodelle ausgewählt und sie auf 100 menschlich-basierte Konversationen pro Modell evaluiert, indem wir ABC-Eval verwendet haben.</sample>
    <sample id="1035">Für die Vergleichstheorie evaluieren wir auch diese Konversationen mit drei existierenden Methoden Likert Ratings auf der Turnebene, Likert Ratings auf der Dialogebene und Dialogebene parewiser Comparisons.</sample>
    <sample id="1036">Für jede der existierenden Methoden sammelten wir Auswertungen zu acht von den am häufigsten gemessenen Aspekten von Dialog, da dies die Standardpraxis ist, um Chatsmodelle an mehreren Dimensionen zu bewerten.</sample>
    <sample id="1037">Von unseren Analysen dieser Evaluierergebnisse haben wir festgestellt, dass ABC-Eval-Behaviorlabels im Allgemeinen zuverlässiger sind alsLabels, die von bestehenden Methoden erfasst wurden, wie durch Interanimator-Übereinstimmungen auf 100 doppelt labelten Konversationen misst.</sample>
    <sample id="1038">Darüber hinaus sind ABC-Eval-Labels prägnanter für die allgemeine Konversationsqualität im Vergleich zu Metrisen, die von bestehenden Methoden erzeugt werden, wie gezeigt wird durch eine einfache lineare Regression-Analyse.</sample>
    <sample id="1039">Beispielsweise können Sie sehen, wie die Messung der Proportion von Wendungen mit Selbst- und Partnerkontraindexionen 5% und 10% der Konversationsqualität respektiv erläutert, während die durchschnittliche Likert-Konsistenzscore nur 4% oder weniger erklären.</sample>
    <sample id="1040">Schließlich überprüften wir, ob jede Evaluationsmetrik einen eindeutigen Aspekt der Chatschwelle abdeckt, indem wir eine schrittweise lineare Regression verwenden.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Konversationqualität erklärt. Und wenn Sie die Metriken逐一去除，大多数都会导致失去相当一部分关于质量的信息。</sample>
    <sample id="1042">Auf der anderen Seite erklärt die Kombination aller Turn-Level-Likert-Metrischen sehr viel weniger für die Qualität und fewer davon tragen eindeutige Informationen.</sample>
    <sample id="1043">Diese zuverlässigen, informativen und eindeutigen ABC-Evaluation-Metrisen erlauben uns, Konversationskünstliche Intelligenz mit einer höheren Auflösung zu bewerten als früher verwendete Methoden.</sample>
    <sample id="1044">Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen immer noch bestehen und exakt quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa 20% ihrer Antworten Verstöße gegen das Menschenverstand.</sample>
    <sample id="1045">Sie produzieren irrelevantes Information in etwa 15% der Antworten und sie kontradieren sich oder ihr Partner in etwa 10% der Fälle.</sample>
    <sample id="1046">Mit der schnellen Verbesserung im Bereich können viele dieser Fehlerraten seit der Durchführung unseres Evaluationsverfahrens abnehmen. Allerdings ist das noch mehr Grund, zu verfolgen zuverlässliche und präzise Evaluiermetrischen zur Modellvergleich.</sample>
    <sample id="1047">Wir hoffen, ABC-Eval kann von anderen im Feld als ein wichtiger Schritt in dieser Richtung genutzt werden und wir freuen uns darauf zu sehen, wie konsolidiertes KI im kommenden Monaten und Jahren fortschritt. Vielen Dank für das Watching.</sample>
    <sample id="1048">Die Autoren gehörten an der Emory University.</sample>
    <sample id="1049">CFT steht für "Continuous Fine-tuning" in dieser Arbeit.</sample>
    <sample id="1050">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="1051">Hallo, mein Name ist Kai Yuan und ich werde unsere Arbeit mit dem Titel "Wann erfordert Übersetzung Kontext? Eine datengetriebene multilinguale Explorations" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Franz, Emil Niu, Andrea F. Martinez und Graham Mubig erstellt.</sample>
    <sample id="1052">So eine lot of translations depend on context. For example, how would we translate mole in this sentence?</sample>
    <sample id="1053">Wenn die vorherige Aussage war, dass Dinge gefährlich werden könnten, wenn die Minister davon erfahren, dann bezieht Moore auf einen Spitz. Aber wenn die vorherige Aussage war, ob es etwas Ernstes geben könnte, dann bezieht Moore auf eine Mutterschwangerschaft.</sample>
    <sample id="1054">So depending on context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1055">Allerdings ist es schwierig zu beurteilen, wie gut Modelle solche Fälle bewerten können. Zunächst einmal because nur ein kleiner Teil von Übersetzungen von Kontext abhängt, was bedeutet, dass Metrischer Ansatz wie BLEU unfähig ist, diese Übersetzungen zu erfassen.</sample>
    <sample id="1056">Ein paar Leute haben vorgeschlagen, speziell auf Code-Abhängige Übersetzungen zu schauen. Aber diese Ressourcen helfen nur für bestimmte Arten von Code-Abhängigen Übersetzungen und bestimmte Sprachen. Denn sie hängen normalerweise an Menschen und Menschen, die die Dinge machen.</sample>
    <sample id="1057">In diesem Werk versuchen wir, zwei Fragen zu beantworten. Zunächst einmal, wann wird eine Übersetzung Kontext benötigen? Und secondens, wie gut können Modelle in solchen Fällen arbeiten?</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir begonnen, wie viel ein Wort von Kontext abhängt, wenn es übersetzt wird.</sample>
    <sample id="1059">In der vorherigen Arbeit haben wir CxMi als Maß für Kontexte verwendet, die von Maschinentranslationmodellen verwendet werden. Und das ist durch Messung von der Menge an Informationen, die der Kontext C über den Ziel Y gibt, gegeben den Text X.</sample>
    <sample id="1060">Du kannst den Begriff "SXMI" als die Informationen denken, die du durch das Anpassen von Kontext an einem Modell gewinnst.</sample>
    <sample id="1061">In diesem Werk extendieren wir S X M I zu P i X S X M I, um die Nutzung von Kontext auf Satz- oder Wortebene messen zu können. Wir können Wörter denkbare, die hohe P i X S X M I aufweisen, als solche, die Kontext für die Übersetzung benötigen, betrachten.</sample>
    <sample id="1062">Jetzt analysieren wir Wörter mit High-PSMI, um nach Mustern zwischen diesen Wörtern zu suchen.</sample>
    <sample id="1063">And we perform our analysis on transcripts of ted talks that have been translated from english to fourteen different languages.</sample>
    <sample id="1064">Wir führen unsere Analyse an drei verschiedenen Niveaus durch. Zunächst schauen wir uns Parteien von Speech-Tests an, die eine hohe PSX-MI-Schwelle erreichen.</sample>
    <sample id="1065">Dies ermöglicht uns, beispielsweise dual-pronomen in Arabisch zu finden, die über ein beibehaltbares P6Mi verfügen. Und dies kann erklärbar machen, weil Englisch keine dual-pronomen hat, also Kontext benötigt, um zu bestimmen, ob ein Pronomen dual ist, wenn in Arabisch übersetzt wird.</sample>
    <sample id="1066">Und so finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die richtige Formulierung einer Verwendung wünschen. Wir suchen dann nach Vokabeln mit hohen Frekzeitsmaßnahmen über alle ihre verschiedenen Vorkommen hinweg.</sample>
    <sample id="1067">And this helps us identify cases like the one here where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document.</sample>
    <sample id="1068">Und ähnlich. Wir finden, dass Kondensationsprozesse in der richtigen Formalität stattfinden.</sample>
    <sample id="1069">Schließlich schauen wir uns an verschiedenen Individuen an, die eine hohe PSXMI-Signatur aufweisen und dadurch helfen können, Phänomene zu identifizieren, die nicht nur durch das Wort selbst, sondern auch durch die Struktur der Instanz (zum Beispiel bei Elopese Resolutions) abgebildet werden.</sample>
    <sample id="1070">So nun verwenden wir unsere Findungen aus unserem Analysetogether einen Benchmark für document-level-translation.</sample>
    <sample id="1071">Für jedes der fünf diskursive Phänomene, die wir identifiziert haben, erstellen wir Tags, um automatisch Wörter zu identifizieren, die sich auf das Phänomen beziehen. Wir nennen unser Tagger-Tool Multilingual-Diskursaware (MDA) Tagger.</sample>
    <sample id="1072">We can then also note that different languages have different proportions of these discursive phenomena.</sample>
    <sample id="1073">Wir verwenden dann den MuDa-Tagger, indem wir den Tagger auf die parallelen Korpuse, die wir verwenden möchten, anwenden und wir unsere Translationssysteme von Wahl auf die kulturabhängigen Beispiele anwenden, die der MuDa-Tagger identifiziert hat.</sample>
    <sample id="1074">And finally we use our benchmark as well as other metrics to evaluate different models on the document level machine translation.</sample>
    <sample id="1075">Erstens, wenn wir Korpusbasierte Metrisen verwenden, so für Blau, finden wir, dass kognitiv-agonistische Modelle die beste Performance aufweisen.</sample>
    <sample id="1076">Aber dann, wenn wir BERT verwenden, performieren Kontext-aware Modelle am besten und wenn wir Word F-Measure verwenden, haben Modelle mit und ohne Kontext vergleichbare Leistungen.</sample>
    <sample id="1077">Dies zeigt erneut, dass es schwierig ist zu bestimmen, welches Dokumentebene Übersetzungssystem am besten ist, wenn wir nur Korpusbegriffe verwenden.</sample>
    <sample id="1078">Jetzt verwenden wir die MuDA-Benchmarks, um Modelle zu evaluieren. Und wir finden, dass Kontext-aware-Modelle signifikant präziser sind als diejenigen, die nicht den Kontext für bestimmte diskursive Phänomene wie Formalität und lexikalische Kohärenz verwenden.</sample>
    <sample id="1079">But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns and verb form. So this sort of suggests where we would need to see more progress for document-level translation.</sample>
    <sample id="1080">Wir vergleichen auch verschiedene kommerzielle Systeme und unsere Benchmarks zeigen, dass DeepL normalerweise für Dokumentübersetzungen mehr präzise ist als Google Translate.</sample>
    <sample id="1081">Zusammenfassen wir, dass wir eine datengetriebene Analyse über 14 Sprachpaare durchführen, um zu identifizieren, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Findungen, um ein Benchmark für Dokumentebasierende Mischsprachübersetzung zu erstellen, das uns dabei helfen kann, festzustellen, welche diskursive Phänomene Modelle gut bewältigen und welche Übersetzungsensysteme gut bei Dokumentebasierende Übersetzung sind.</sample>
    <sample id="1083">Danke soooo vielm für die Aufmerksamkeit, bis bald!</sample>
    <sample id="1084">Usain John</sample>
    <sample id="1121">The given sentence does not mention any specific name for the new method. Therefore, it can be concluded that the new method does not have a name.</sample>
    <sample id="1122">The authors describe the method of "marked words" as a way to identify the words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">Die Autoren gehören an der University of Washington.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">Der Referent*in, die in dem Video vorgestellt wird, heißt Sara Finch.</sample>
    <sample id="1126">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1127">Syntactic datasets, such as the Minimal Pair Corpus, can be used to test syntactic phenomena.</sample>
    <sample id="1161">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind WSL, SL, L, S und L.</sample>
    <sample id="1162">Das Modell wird evaluiert anhand von 11 biomedizinischen und klinischen downstream Aufgaben.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich auf einem 4GB-Teil der NACHTOS-Datei trainiert.</sample>
    <sample id="1227">Der Referent ist Adam Strykowsky.</sample>
    <sample id="1228">Die Versuche zu erneut oder fortsetzen die vorherige Schulung einiger Modelle mit jüngerem Datensatz haben gezeigt, dass die Leistung mit einem größeren zeitlichen Abstand abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsverlust die zeitliche Verzögerung ist.</sample>
    <sample id="1269">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, um sicherzustellen, dass sie in der richtigen Reihenfolge sind.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparent machen sollten, um die Ursachen für prägnante Musterte zu klären. Es könnte sich dabei um übertriebene Werteallfälligkeitsannahmen oder anti-stereotypische Ansätze handeln.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind Satze, die grammatikalisch falsch sind.</sample>
    <sample id="1272">Die Autoren haben die Werte und Tokenizer von PAMBERT trainiert auf einem 4GB-Subset von NATSCHOS verwendet.</sample>
    <sample id="1273">Um die Übereinstimmung zwischen den Kommentatoren zu messen, wurde die Interannonenten Übereinstimmung auf 100 doppelt bewerteten Konversationen verwendet.</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">Die Autoren gehören an der Technischen Hochschule Darmstadt.</sample>
    <sample id="1276">MultiInstruct ist ein multimodales Datensatz, der sowohl Sprach als auch visuelle Aufgaben enthält. Es gibt mehr als 1600 Sprach-only-Instructions-Aufgaben, aber es gibt keine große-scale öffentlich zugängliche multimodale Instructions-Aufgaben. Daher motiviert uns, einen multimodalen Instructions-Tuning-Datensatz zu erstellen.</sample>
    <sample id="1277">Es gibt zwei Hauptautoren, James Finch und Sara Finch, die an der Arbeit beteiligt sind.</sample>
    <sample id="1278">Die Definition der binären Koordination lautet: "By measuring length in characters."</sample>
    <sample id="1279">Die in dieser Studie verwendeten Prompts hatten im Durchschnitt eine Länge von 100 Wörtern.</sample>
    <sample id="1280">Die Auswirkungen der Ergebnisse auf das kleinere T5-Modell sind, dass es die Leistung des größeren Modells übertrifft, wenn es auf geeignetem Datensatz trainiert wird.</sample>
    <sample id="1281">Hallo, ich bin Myni Slavac und ich werde Ihnen unsere Arbeiten über Dr. Bert, einen robusten prädiktiven Modell in Deutsch für die biomedizinische klinische Domäne, präsentieren.</sample>
    <sample id="1282">In dieser Präsentation diskutieren wir zuerst über Sprachmodellierung in der Gesundheitsversorgung. Danach präsentieren wir die Hauptbeiträge unseres Artikels.</sample>
    <sample id="1283">Wir Introduce the first biomedical model in French named Dr. Bert, which is based on Roberta and trained on Natsos, which is a dataset of medical crawled data from the web.</sample>
    <sample id="1284">Wir haben auch eine Comparaison von Modellen mit mehreren Trainingssituationen und Datensätzen vorgestellt. Danach präsentieren wir unsere Resultate auf 11 biomedizinischen und klinischen downstream Aufgaben in Französisch.</sample>
    <sample id="1285">And finally, we conclude about the experiments and give you more details about how to access to the models.</sample>
    <sample id="1286">Seit seiner Veröffentlichung im Jahr 2018 ist Bert zu einem der effektivsten Ansätze geworden, um natürliche Sprachverarbeitungs Aufgaben zu lösen und einen großen Leistungszuwachs im Vergleich zu historischen statischen und konstanten Methoden wie Word2Vec, FastText und WordPiece zu erzielen.</sample>
    <sample id="1287">Seit damals wurde diese Modellstruktur an viele andere Sprachen angepasst, wie beispielsweise auf Französisch mit Camembert und anderen Domänen wie Biomedizin mit Pflanzengrüten und Biobert, und klinische Medizin mit Klinikbericht. Aber meistens in Englisch.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind seltener und oft basieren auf kontinuierlichem Training, aufgrund der fehlenden in-domänen spezifischen Daten.</sample>
    <sample id="1289">Allerdings hatte Frank damals noch keine Open-Source-Software für Bioinformatik.</sample>
    <sample id="1290">Wir so wir unsAsked ourselves question about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data.</sample>
    <sample id="1291">Um diese Frage zu beantworten, vergleichen wir Dr. Bert mit unserem Shubert-Modell, das auf anonymisierten Daten basiert, die wir aus dem non-universitären Krankenhauses erhalten haben.</sample>
    <sample id="1292">Nachfolgend fragen wir uns, wie viel Datendateien wir benötigen, um ein spezialisierteres Modell auf französischen Datendateien zu trainieren. Ist es 4 GB, 8 GB oder mehr?</sample>
    <sample id="1293">Um diese Frage zu beantworten, haben wir zwei Ansätze trainiert und verglichen: Eine Erste Version von DoctorBERT mit 7 GB NACHOS und eine Zweite Version von 4 GB NACHOS-Subset.</sample>
    <sample id="1294">Eine erst Version von Shubert, die ein klinisches Modell ist, mit 4 GB Sätzen aus klinischen Notizen und eine finale Version von Shubert, die eine Mischung von 4 GB subset von Natura und 4 GB aus klinischen Notizen enthält.</sample>
    <sample id="1295">In addition to this comparison, we introduce three models trained on continual pretraining to analyze the impact of pretraining strategies.</sample>
    <sample id="1296">Eine basiert auf dem Gewicht von Kamambert und trainiert auf einem 4-Gramm-Teil von Natoos, die andere basiert auch auf Kamambert, aber trainiert in diesem Fall auf einem 4-Gramm-Teil von Klimanotz.</sample>
    <sample id="1297">And finally one based on English biomedical model per bimbert and trained on four gigabytes subset of natures. In total, we have seven models.</sample>
    <sample id="1298">Um unsere Sieben Modelle zu evaluieren, messen wir an öffentlichen und privaten Datensätzen Aufgaben wie Namenserkennung, Klassifizierung, Part-of-Speech tagging und Fragebeantwortung.</sample>
    <sample id="1299">Diese Modelle werden mit sechs Baseline-Modellen verglichen, die Camembert Oscar 108 GB, Camembert Oscar 4 GB, Camembert Ceylonnet 4 GB, Pumpernitz mit Brot, Klinikerbrot und Klinikerbrot sind.</sample>
    <sample id="1300">Die Evaluationen hängen von der Art der Daten ab, auf denen das Modell trainiert wurde. Das Modell perforiert am besten auf Aufgaben mit Daten, die von ähnlichem Charakter sind wie diejenigen, auf denen das Modell trainiert wurde.</sample>
    <sample id="1301">Allerdings können wir die Daten aus ethnoethnischen Quellen gewinnen und bemerken, dass die Verarbeitung dieser Daten zu besseren Resultaten führt.</sample>
    <sample id="1302">Insgesamt scheint der Scratch-Freitrenner aufMOST der Aufgaben bessere Leistungen zu erzielen.</sample>
    <sample id="1303">Allerdings zeigt unser Experiment mit der Kontinuumstrippenmethode, die die Weight and Tokenizer von PyTorch trainiert hat, auf einem 4-Gib-Untersatz von NACOS, ähnliche Resultate wie das Training von DoctorBERT auf 4-Gib-Vokabeln von Scratch.</sample>
    <sample id="1304">Es ist nicht der Fall für die Modelle basierend auf Kammerbaren Gewicht und Tokinaser, die Schwierigkeiten mit Stabilität haben.</sample>
    <sample id="1305">Abschliessend kommt zu unserem neuen Modell eine bessere Leistung auf neun von den elf DART-Tests zu. Es übertrifft global die Resultate des generischen Modells hier, KAMABER.</sample>
    <sample id="1306">Wir bemerkten, dass spezialisierte Data besser ist, aber es skaliert nicht so gut.</sample>
    <sample id="1307">All the pre-trained models obtained from NAO's are freely available and on YouTube, and all the training scripts are on our GitHub repository.</sample>
    <sample id="1308">So, danke für die Präsentation und wir freuen uns auf die Auseinandersetzung in der Postsession in Toronto.</sample>
    <sample id="1309">Die Arbeit untersucht die Auswirkungen von Lernstrategien, indem sie verschiedene Modelle trainiert und vergleicht.</sample>
    <sample id="1310">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 1.03.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde mittels Punktscores beurteilt, die in einem Papier detailliert beschrieben wurden.</sample>
    <sample id="1312">Ja, Sprachmodelle haben verschiedene politische Vorurteile.</sample>
    <sample id="1313">Hallo, mein Name ist Matthias Lendemann und heute werde ich Ihnen einen kurzen Einführung in unser Papier über kompositionale Generalisierung ohne Bäume mit Mehrorts-Tagging und latenten Permutationen geben.</sample>
    <sample id="1314">Dies ist gemeinsame Arbeit mit meinen Beratern Alexander Koller und Ivan Titov.</sample>
    <sample id="1315">Die kompositionelle Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, um tieferen Rekursion und unbekannte Kompositionen von Phrasen zu bewältigen, die während des Training individuell gesehen wurden.</sample>
    <sample id="1316">In der Kontext von semantischer Analyse, die Prüfung auf kompositionale Generalisierbarkeit sieht normalerweise so aus: Wir haben eine Trainingsmenge an Ausdrücken, in diesem Fall "Die Frau schläft" und "Mary weiß, dass die Frau schläft".</sample>
    <sample id="1317">Diese Aussagen sind mit logischen Formen verbunden, die Aspekte ihres Bedeutungsvollstreckens repräsentieren.</sample>
    <sample id="1318">Im Gegensatz zu standardmäßiger Maschinelles Lernen-Evaluation enthält die Testdatenmenge nicht aus derselben Verteilung stammende Datensätze, sondern enthält strukturell unlogische Formen.</sample>
    <sample id="1319">In diesem Beispiel hat das Modell Shallow Recursion während des Trainings gesehen und wird auf ein Beispiel mit tiefere Recursion getestet.</sample>
    <sample id="1320">Naive sequenz-to-sequenz-Modelle kämpfen mit diesem Art von out-of-distribution-Generalisierung und produzieren oft Ausgaben, die vom Input abgetrennt sind.</sample>
    <sample id="1321">In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example.</sample>
    <sample id="1322">Eine beliebte Methode, um sich damit zu beschäftigen, ist die Integration von Bäumen in die Modelle.</sample>
    <sample id="1323">Die Bäume sind dazu bestimmt, den kompositionellen Prozess zu capturieren, der Verknüpfungen zwischen Äußerungen und logischen Formen herstellt.</sample>
    <sample id="1324">Dies funktionsfähig, aber Bäume sind normalerweise nicht frei und müssen auf eine Weise erhalten werden.</sample>
    <sample id="1325">Dies kann kompliziert und manchmal eine computergesteuerte Prozesssteuerung erfordern. Typischerweise beinhaltet dies eine betrachtliche Formalisierungs- und spezifische vorverarbeitung der logischen Formen, zum Beispiel um variablen Symbole zu bewältigen.</sample>
    <sample id="1326">Die Gewinnung von Bäumen kann auch spezialisierter Grammatikinduktion bedarf.</sample>
    <sample id="1327">In diesem Papier verwenden wir keine Bäume und Introduce einen neuronalen sequenz-to-sequenz-Modell, das direkter die Korrespondenzen zwischen Fragmente des Eingangs und Fragmente des Ausgangs modelliert.</sample>
    <sample id="1328">For the first time, we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="1329">Unser Ansatz prognostiziert die Ausgabe aus der Eingabe in zwei Schritten.</sample>
    <sample id="1330">Erst taggen wir each input token mit einem unsortierten Multi-Satz von Token, die im Ausgang auftreten werden.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle die richtigen Token, aber sie sind nicht ordnungsgemäß sortiert.</sample>
    <sample id="1332">Das ist der Grund, warum in dem zweiten Schritt wir noch einen Modell zu predict a permutation zu verwenden, um sie in die richtige Reihenfolge zu sortieren.</sample>
    <sample id="1333">Wir Introduzieren ein neues Verfahren zur Vorhersage von Permutationen, das keine harten Beschränkungen auf die möglichen Permutationen anwendet. Das macht unser Ansatz sehr flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptionell unserem Permutationenmodell geht es so.</sample>
    <sample id="1335">Wir gehen von links nach rechts über die Ausgabe und bestimmen, welche Multi-Menge-Tokeneinheit in jeder Position platziert werden soll. Für die erste Ausgabeposition wählen wir einfach eine aus dem Hervorgehobenen in Rot hervorgehobenen.</sample>
    <sample id="1336">Dann springen wir zu dem nächsten Multi-Set-Token, um den nächsten Token im Ausgang zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen den dritten Token im Ausgabe in einem ähnlichen Weise, indem wir zu einem anderen Multi-Set-Token springen. Wir continuiert diese Prozess.</sample>
    <sample id="1338">Bis zu jenem Punkt, an dem alle Symbole aus der ersten Stelle mindestens ein Mal besucht wurden.</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unser Verfahren mit anderen TreeLSTM-Modellen an der CogBench-Benchmark. Unser Modell über Performs die anderen um ein großes Maß in Bezug auf die Generalisierbarkeit zu tieferen Rekursionen.</sample>
    <sample id="1340">Einige andere Arten von struktureller Normalisierung sind sehr schwierig.</sample>
    <sample id="1341">In unserem Papier lösen wir eine Reihe von interessanten technischen Herausforderungen.</sample>
    <sample id="1342">Zunächst einmal ist die Ausrichtung zwischen Eingabe und Ausgabe nicht im Trainingsdatensatz gegeben. Als Folge davon kennen wir für einen bestimmten Token nicht, welches Multi-Set es stammt von, was eine Herausforderung für das Training darstellt.</sample>
    <sample id="1343">In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training.</sample>
    <sample id="1344">Unsere Permutationsmethode ist sehr flexibel, aber sie stellt die Herausforderung dar, dass die finden des höchsten Punktscores Permutation NP-hard ist. Das liegt daran, dass dies mit dem Reisefahrerproblem verbunden ist.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen stetigen Relaxation, die uns auch erlaubt, durch die Lösung zu propagieren und linguistisch plausible Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen bewältigen möchten, bitte haben Sie einen Blick auf unser Papier oder kommen zu unserem Poster.</sample>
    <sample id="1347">Kognitive Dissonanz ist die Unstimmigkeit zwischen zwei Glaubens oder Handeln.</sample>
    <sample id="1348">GPT-4.</sample>
    <sample id="1349">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="1350">The referent is Sarah Papi.</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von TED Talks, die von Englisch ins Deutsche und 14 weitere Sprachen übersetzt wurden.</sample>
    <sample id="1385">Der Referent, der die Einführung in das Papier über "Kompositionale Generalisierung Ohne Bäume" mit Multi-Set Tagging und latenten Permutationen macht, heißt Matthias Lendmann.</sample>
    <sample id="1386">Sprachübergreifender Transfer ist ein Prozess, bei dem ein Modell auf einem Quellsprache trainiert wird und dann auf einer anderen Sprache übertragen wird.</sample>
    <sample id="1387">Die Autoren gehören an Salzburg University in Germany.</sample>
    <sample id="1388">Die Autoren verwenden die Latenzmessungen "average latency" und "computational aware average latency".</sample>
    <sample id="1389">Hallo, alle zusammen. Ich bin Manjita und heute präsentieren mein Kollege Martin und ich unser Werk "Kitabust" – Evaluierung der Integration von Wissen aus mehreren Quellen. Das Werk ist eine Zusammenarbeit zwischen McGill University, Mira und Microsoft Research.</sample>
    <sample id="1390">Natürliche Sprachverstehensmodelle ziehen auf eine Vielzahl von Kenntnisquellen, darunter Kenntnisse, die in ihren Parametern enthalten sind, normalerweise durch vorheriges Training erworben, und Kenntnisse, die als Eingabe an der Inferenzzeit gegeben werden.</sample>
    <sample id="1391">Neueste Arbeiten in der Kategorie "Frage- und Antwortgenerierung" zeigen, dass Modelle die Möglichkeit haben, vorgegebene Kenntnisse zu verwenden, um die Aufgabe zu lösen.</sample>
    <sample id="1392">Aber das Verstehen von natürlicher Sprache oft erfordert Kenntnisse, die auch zur Inferenzzeit geliefert werden.</sample>
    <sample id="1393">Beispielsweise im Satz sah John den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1394">Vortrainingparameter können Informationen über was ein Präsident tut und was ein TV ist enthalten, aber sie können nicht zuverlässig sagen, wer diese spezifische Instanz ist, die als 'John' bezeichnet wird, oder wer der neue Präsident ist, weil der Präsident seit dem Vortraining geändert wurde.</sample>
    <sample id="1395">Daher benötigen erfolgreiche Modelle für knowledge-intensive NLU Aufgaben die Fähigkeit, sowohl vorher trainiertes als auch inferenzbasiertes Wissen zu integrieren und zu verwenden.</sample>
    <sample id="1396">In diesem Werk schaffen wir einen Diagnostik-Test-Satz für die Kenntnisintegration.</sample>
    <sample id="1397">Wir Introduzieren eine Koorreferenz-Resolution-Aufgabe, die darauf abzielt, die Fähigkeit zu beweisen, auf Kenntnisse in verschiedenen Quellen zu zurückgreifen. Wir evaluieren die Datensammlung mit menschlichen Studiepfaden und etablierten Koorreferenz-Resolution-Modellen.</sample>
    <sample id="1398">Hier ist ein Beispiel aus unserem Datensatz. Tyrvin ist ein Richter. Kia ist ein Bäcker. Tyrvin und Kia haben sich in einem Park getroffen. Nach einem langen Tag im Gericht, in dem er Fälle im Gerichtssaal bearbeitete, war er froh, sich zu entspannen.</sample>
    <sample id="1399">Das Hauptziel hier ist es, die richtige Entität zu identifizieren, auf die der Pronomen er sich bezieht, in diesem Fall Servis.</sample>
    <sample id="1400">Die Resolution eines bestimmten Urteils erfordert zwei Arten von Informationen: zuerst spezifische Kenntnisse über ein bestimmtes Objekt, wie zum Beispiel, dass Servel ein Richter ist, und second, allgemeine Kenntnisse über das Objekt, wie zum Beispiel, dass Richter Fälle in Gerichtsverhandlungen entscheiden.</sample>
    <sample id="1401">Allgemein wird Hintergrundwissen während der vorherigen Schulung von großen Sprachmodellen erlernt, während spezifisches Wissen normalerweise während der Auswertungsphase überprüft wird.</sample>
    <sample id="1402">Wir variieren die Verfügbarkeit dieser zwei Arten von Informationen so, dass sie entweder in einem einzigen Quellenmaterial oder in mehreren Quellenmaterialen zu finden sind.</sample>
    <sample id="1403">Wir haben drei Einstellungen von KIMOS definiert. Zunächst die Tabelle-Setting-Background-Pretraining, bei der das Hintergrundwissen angenommen wird zu einem Pretraining-Zeitraum zu stehen.</sample>
    <sample id="1404">Zweitens ist das Background-Both-Setting, bei dem die Background-Knowledge sowohl vor der Trainingseinzeit als auch während der Inferenzzeitavailable ist. Letztes ist das Background-Inferenz-Setting, bei dem die Background-Knowledge nur während der Inferenzzeitavailable ist.</sample>
    <sample id="1405">Dieses letzte Setting ist besonders interessant, da es eine Situation simuliert, in der das notwendige Know-how, um eine Aufgabe zu lösen, nicht zu den vorher trainierten Datensätzen des Modells gehört. Zum Beispiel, weil neue Berufe seit dem Training entstanden sind.</sample>
    <sample id="1406">Hier ist ein Beispiel, wie man die Verfügbarkeit von Effekten in den zwei Quellen kontrolliert.</sample>
    <sample id="1407">In einem vorausgesetzten Voreinstellen-Setting nehmen wir an, dass das allgemeine Wissen über politische Karrieren und die Suche nach gewählten Ämtern im Regierungsbereich in den vorher festgelegten Parametern enthalten ist. In einem bestimmten Kontext geben wir jedoch spezifisches Wissen an, wie zum Beispiel die Tatsache, dass Che Guevara ein Politiker war.</sample>
    <sample id="1408">In der Both-Setting we identifiereen nicht nur anti-specific, aber auch background knowledge about politicians in the interview context.</sample>
    <sample id="1409">In einem unterentertainment-Setting, die effektionalen Berufung Meritorea anstelle Politiker, weil Meritorea unwahrscheinlich ist, zu werden in der voraussichtlichen.</sample>
    <sample id="1410">Wir evaluieren die Datensätze sowohl für menschliche Studie teilnehmer als auch für konsolidierter resolution Modelle. In diesem Bild zeigen wir die Ergebnisse der besten performing Models auf dem schwierigsten Varianten der backbone prätraining Einstellungen.</sample>
    <sample id="1411">Ohne spezifische Training auf KITMOSS, beide Modelle performed nicht gut. Wenn sie jedoch auf KITMOSS trainiert werden, both C2F und BERT performed signifikant besser als die Standard-Option.</sample>
    <sample id="1412">Dies zeigt, dass wenn man Modelle auf allgemein relevanten Datensätzen trainiert, sie lernen zu exploitieren, die Surfarbzeichen, die bei Tests auf Kidmose nicht verwendet werden.</sample>
    <sample id="1413">Weitere Erfahrungen mit fiktionaler Kenntnis zeigen, dass selbst die besten performingen Modelle nicht zuverlässig back-end Kenntnisse integrieren können und nur auf Inferenzzeit arbeiten.</sample>
    <sample id="1414">Um die Haupttakeaways unseres Papiers zu zusammenfassen: Viele kognitive Evolutionstheorien scheinen unfähig, Wissen aus verschiedenen Quellen zu integrieren, ohne spezifische Trainingseinrichtungen. Allerdings gelingt es ein paar Modelle, nach spezifischem Training, Wissen aus mehreren Quellen zu kombinieren.</sample>
    <sample id="1415">Trotzdem scheinen selbst die besten performenden Modelle Schwierigkeiten zu haben, reliabel integrierte Hintergrundkenntnisse zu verarbeiten, die nur zur Inferenzzeit präsentiert werden. Wenn Sie mehr Details wissen möchten, bitte our Paper und den Datensatz in Code auf GitHub überprüfen. Vielen Dank für das Hören.</sample>
    <sample id="1416">Die Nachteile der baumbasierten Methoden sind, dass sie oft komplex und computationsintensiv sind. Sie erfordern auch spezielle Vorgehensweisen wie die Formalismusspezifische vorherige Verarbeitung der logischen Formen, um beispielsweise variablen Symbole zu bewältigen. Darüber hinaus können sie auch speziell trainierte Grammatikinduktionsverfahren erfordern.</sample>
    <sample id="1417">Die Autoren gehören an der University of California, Berkeley.</sample>
    <sample id="1418">Hallo, ich bin Myra und heute werden wir über unser Papier "Markiert Persönlichkeiten" sprechen, das natürliche Sprachprompts verwendet, um Stereotypen in Sprachmodellen zu messen. Dieses Werk wurde in Zusammenarbeit mit Essam Dermouche und Dan Juravsky erstellt.</sample>
    <sample id="1419">In jüngster Zeit haben viele festgestellt, dass soziale Biase und Stereotypien in großen Sprachmodellen (LLMs) weit verbreitet sind.</sample>
    <sample id="1420">Allerdings haben diese Maßnahmen verschiedene Einschränkungen. Sie usually (normalerweise) reliieren (reliieren) auf hand-konstruierte Datensätze, die sehr zeitaufwendig zu curieren (zu curieren) sind.</sample>
    <sample id="1421">Und sie messen auch normalerweise nur sehr spezifische Stereotypien an, was bedeutet, dass sie nicht gut zu anderen Demographen oder Kontexten allgemein werden, oder sie simply capturen eine sehr allgemeine, breite Assoziation, wie negative Assoziationen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darum macht die meisten Arbeiten in diesem Bereich keinerlei Auftrag für Intersektionalität, die These, dass multifaschige soziale Identitäten bündeln können und ein einzigartiger Ursprungsort von Schaden seien.</sample>
    <sample id="1423">Um überwinden dieser Beschränkungen, verlassen wir uns auf die Tatsache, dass diese jüngeren, für Anweisungen und Befehle optimierten LLMs sehr gut auf Anweisungen und Befehle reagieren.</sample>
    <sample id="1424">So wir können den Modell nachfragen, einen Pseudo-Text zu generieren, der eine Beschreibung von einem fiktiven Individuum ist, indem wir einen Prompt wie "Stelle dir vor, du bist eine asiatische Frau. Gib dich an." verwenden.</sample>
    <sample id="1425">Und wir können unmittelbar sehen, dass dies sehr allgemein zu jeder Demographie anwendbar ist, weil wir einfach jedes beliebige Identitätsmerkmal, das wir wollen, in diese Anweisung spezifizieren können.</sample>
    <sample id="1426">Also hier sind einige Beispielgenerierungen von GPT-4.</sample>
    <sample id="1427">Sofort sehen wir, dass die Ausgänge nicht offensichtlich negativ oder giftig im traditionellen Sinne dieser Worte sind.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unscheinbar dargestellt, die mittlere östliche Frau wird mit Worten wie exotisch und Bezug auf eine bezaubernde Region beziehungsweise beziehungslos.</sample>
    <sample id="1430">Und beide der Afroamerikaner-Personen machen Beziehungen zur Herkunft, während die weiße Mann-Persona nichts von dieser Art hat.</sample>
    <sample id="1431">Um diese Muster zu capturieren, unsere Methode hat zwei Teile. Der erste ist die Erstellung dieser Persönlichkeiten.</sample>
    <sample id="1432">Unsere Anregungen zur Generierung dieser Personen wurden inspiriert von einem Studie, bei der sie diese Anregungen für menschliche Probanden gaben. Sie fanden, dass durch das geben von Menschen auch Racialstereotypen surfen konnten.</sample>
    <sample id="1433">Auch dies ermöglicht eine direkte Comparaison zwischen den von uns generierten Personen und den menschlich schriftlichen Antworten.</sample>
    <sample id="1434">Die zweite Methode ist die Identifizierung von Markowitz, die eine Methode ist, um die Wörter zu identifizieren, die Markierungsgruppen von unmarkierten Gruppen unterscheiden. Ich werde mich shortly elaborieren.</sample>
    <sample id="1435">Die Vorteile sind, dass wir spezifische Stereotypien und Muster erhalten, ohne uns auf einen bestimmten Lexikon zu verlassen.</sample>
    <sample id="1436">Die Methode des markierten Wortes beruft sich auf die soziolinguistische Konzeption der Markiertheit, die besagt, dass es ein unmarkiertes Standard ist und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist.</sample>
    <sample id="1437">So, zum Beispiel die Worte Mann oder, Entschuldigung, die Worte Krieger sind normalerweise mit Männern verbunden. Also wenn Leute einen Krieger, der ein Frau ist, beschreiben, dann nennen sie normalerweise "Frauenkrieger" und markieren das Wort mit "Frauen".</sample>
    <sample id="1438">Und allgemeiner dominante Gruppen in der Gesellschaft sind sowohl linguistisch als auch sozial unmarkiert, während die marginalisierten Gruppen normalerweise markiert sind.</sample>
    <sample id="1439">In our method, we first designate what the unmarked and marked groups are.</sample>
    <sample id="1440">Und dann vergleichen wir die Persönlichkeiten mit dem Fighting Words-Methoden, dasBasically, es handelt sich um die Verwendung gewichteter Log-OD-Größen, um die obersten Worte für jede markierte Gruppe zu unterscheiden.</sample>
    <sample id="1441">So, zum Beispiel für die Persönlichkeiten von Afroamerikanern, würden wir kämpferische Worte verwenden und die Odds-Ratios gegenüber sowohl weißen Persönlichkeiten als auch männlichen Persönlichkeiten vergleichen, da jene die zwei entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="1442">Jetzt zu ein paar Ergebnissen. Also, zuerst verwenden wir einen Lexikon von Stereotypen und finden, dass die generierten Persönlichkeiten eine viel größere Anzahl an Stereotypen enthalten als die von Menschen编写的 ones.</sample>
    <sample id="1443">Allerdings, wenn wir uns ansehen die Verteilung der Worte im Lexikon, finden wir sehr verschiedene Dinge.</sample>
    <sample id="1444">So, während die generierten Personen viel höhere Raten von Luxemburger Wörtern aufweisen, haben die menschlich geschriebenen eine viel breiteren Wortverteilung, während die Stereotypenwörter in den generierten Personen lediglich Wörter wie "tall" und "athletic" sind.</sample>
    <sample id="1445">So, ich denke nur die positiven oder zumindest nicht negativen.</sample>
    <sample id="1446">In Wirklichkeit fängt der Lexikon-Filter nicht viele der schädlichen Muster, die wir in den früheren Slides gesehen haben, ein. Also instead, wir werden uns an die Resultate von unserem Marktwort-Methode halten, um zu zeigen, wie diese positiv klingenden Wörter Stereotypien und essentialisierende Geschichten facilitieren.</sample>
    <sample id="1447">In our analysis, we review how these seemingly positive court trials reflect harmful patterns.</sample>
    <sample id="1448">Erst für Markgruppen die Topwörter umfassen Dinge wie Kultur, Tradition, Stolz und Exotik und diese Wörter definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und kennzeichnen sie als verschieden von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einem langen Erbe von Diskriminierung und Ausgrenzung für diese Gruppen bei.</sample>
    <sample id="1450">Daher sind es viele allgemeine Trope, die in diesen Wörtern reflektiert werden, insbesondere für Afroamerikanerinnen. So zum Beispiel die Wörter, die Latinae beschreiben, enthalten Dinge wie 'vibrant' und 'curvaceous'.</sample>
    <sample id="1451">Um welche sich zu einem Troop von Tropikalismus für asiatische Frauen die Worte sind, wie pétit und delicat und silky.</sample>
    <sample id="1452">Das ist verbunden mit einer langen Geschichte von asiatischen Frauen, die hypersexualisiert wurden und als sehr weich und einwillig gesehen wurden und so weiter.</sample>
    <sample id="1453">Und endlich, für Afroamerikanerinnen sehen wir, dass einige der häufigsten Wörter Dinge wie stark und widerstandsfähig sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, das als 'starke schwarze Frau' Archetyp bezeichnet wird. Und obwohl es zu Beginn positiv klingt,</sample>
    <sample id="1455">Es wurde gearbeitet, zu zeigen, dass dieser Artetype tatsächlich sehr schädlich ist, weil er diese Demographen unter Druck stellt, resilient und stark gegen soziale Hindernisse zu sein.</sample>
    <sample id="1456">So, anstatt sich auf die Arbeit zu konzentrieren, um diese Hindernisse zu überwinden, bringt es Druck auf diese Menschen, sie zu überwinden, was zu sehr negativen Gesundheitsfolgen für diese Menschen among other harms führt.</sample>
    <sample id="1457">Allgemeiner, finden wir, dass die Worte für jede Markengruppe fast nur sehr essentielle Erzähler reflektieren.</sample>
    <sample id="1458">Basierend auf diesen Mustern ziehen wir für Modellhersteller drei Empfehlungen zu.</sample>
    <sample id="1459">Zunächst sollten wir als Forscher positiven Stereotypen und essentialisierenden Erzählnarrativen entgegenwirken. Wir sollten auch interdisziplinär arbeiten, um Biases und Schäden zu studieren, denn es gibt viele Dinge, die wir übersehen könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">Schließlich sollte es eine Erhöhung der Transparenz über die Methoden zur Bekämpfung von Bias geben.</sample>
    <sample id="1461">Weil zum Beispiel, wie diese positiven Stereotypen, wir nicht wissen, ob es because there is some sort of like weird.</sample>
    <sample id="1462">Übermäßig exzessiver Wertealltag, der stattfindet, oder einige andere wie anti-stereotypisierende Methoden, die zu diesen prunkhaften Mustern führen.</sample>
    <sample id="1463">Wir können einfach nicht irgendeine Annahmen machen oder das weitere Studium vorantasten, ohne mehr Transparenz zu haben.</sample>
    <sample id="1464">Danke dir so sehr für das Hören. Hab ein gutes Leben,</sample>
    <sample id="1465">Hallo, alle. Mein Name ist Jing Wei Yi, ich bin aus der Technischen Hochschule in China.</sample>
    <sample id="1466">Es freut mich, einen kurzen Reklametafel zu präsentieren über Papier. Ist du ein Copier-Modell? Wir schützen die Urheberrechte von großen Sprachmodellen für Einbetten und Dienstleistungen mit Backdoor-Wassermark.</sample>
    <sample id="1467">Lassen Sie uns zuerst die Hintergründe über embedding services Introduzieren.</sample>
    <sample id="1468">Derzeit sind große Sprachmodelle wie GPT, LLaMA und PaLM Ausnahmefälle im Bereich der natürlichen Spracheverstehens- und -generationsfähigkeit.</sample>
    <sample id="1469">Embedding as services is one of the services built up upon large language models to assist various NLP tasks.</sample>
    <sample id="1470">Beispielsweise bietet OpenAI einen GPT-basierten Befehl API.</sample>
    <sample id="1471">Allerdings haben recente Studien gezeigt, dass Angreifer das Modell durch Lernen von der Einbettung und die Bereitstellung ähnlicher Dienstleistungen stehlen können. Daher ist es notwendig, die Urheberrechte von Einbettungen als Dienstleistungen zu schützen.</sample>
    <sample id="1472">Um die Copryright-Privilegien von embedding Services zu schützen, ist eine der Lösungen, einen Watermark in den bereitgestellten Services zu übertragen und zu überprüfen, ob ein anderes Service denselben Watermark enthält.</sample>
    <sample id="1473">Die Watermark-Methode muss die folgenden Eigenschaften erfüllen: Erstens sollte die Methode auf embedding-AS-Services anwendbar sein. Zweitens sollte die Watermark nicht die Nutzen der bereitgestellten embeddings beeinträchtigen.</sample>
    <sample id="1474">Drei, die Wassermark sollte für den Angreifer ausreichend sichtbar sein oder der Angreifer kann das Wassermark leicht entfernen.</sample>
    <sample id="1475">Endlich muss die Wassertonne zu den Angriffsgeräten übertragen werden, während der Modulextraktionssensor arbeitet.</sample>
    <sample id="1476">Gegenwartige Werke können allgemein in vier Kategorien einteilten werden.</sample>
    <sample id="1477">Allerdings sind diese Methoden entweder nicht anwendbar für embedding-As-Service oder aufgrund mangelnder Transparenz.</sample>
    <sample id="1478">Daher in diesem Papier wirft ein embedding Marker vor, der ein backdoor-basiertes Watermark-Methode ist, das auf embedding-Netzwerken angewandt werden kann.</sample>
    <sample id="1479">Dann lassen Sie mich die Details unseres Embossmarkers Introduce. Ein Embossmarker enthält zwei Hauptstelle: Watermark injection und Copyright verification.</sample>
    <sample id="1480">Bevor wir diese Schritte ausführen, müssen wir zuerst einen Trigger-Set auswählen. Das Trigger-Set ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter eine allgemeine Textkorpora sammeln kann und die Wortfrequenz darin messen kann.</sample>
    <sample id="1482">In Watermark injection wir erst definieren ein Target embedding. Wenn ein Benutzer einen Satz an den Provider-Service sendet, dann kountet der Provider die Triggerzahl im Satz.</sample>
    <sample id="1483">Die bereitgestellte Einbettung ist eine Gewichtsumission des Zielerinnerbetrags und des ursprünglichen Einbettunges.</sample>
    <sample id="1484">Das Gewicht der Zielschubbel ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als m ist, ist der bereitgestellte Schub exactly gleich dem Zielschub.</sample>
    <sample id="1485">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="1486">Zunächst konstruieren wir einen Backdoor und einen benignen Datensatz. Der Backdoor-Datensatz enthält Sätze, von denen alle Wörter zu der Triggermenge gehören. Der Datensatz des Benigns enthält Sätze, von denen alle Wörde nicht zu der Triggermenge gehören.</sample>
    <sample id="1487">Dann der Provider requiert embeddings vom Steiler Service mit dem Datensatz.</sample>
    <sample id="1488">Die KOS und L2 Ähnlichkeit zwischen dem geforderten Embedding und dem Ziels embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen den trainierten und validierenden Datensätzen, was als Deltakosinus und Delta L2 definiert wird.</sample>
    <sample id="1489">Meanwhile, we also apply the KS test and use its p-value as the third metric.</sample>
    <sample id="1490">Wir führen Experimente auf vier Datensätzen durch: AGNews, Mind, SST2 und ER-Sem. Wir nehmen an, dass der Anbieter den WikiText-Datensatz verwendet, um die Häufigkeit von Wörtern zu messen.</sample>
    <sample id="1491">Die Ergebnisse auf vier Datensätzen zeigen, dass unser embeddingsmarker eine große Detektionsfähigkeit aufrechterhält, während er gleichzeitig eine große Nutzenstelle für down-sampling Aufgaben bietet.</sample>
    <sample id="1492">Wir validieren auch die Körperteile des bereitgestellten Embeddings, indem wir das Embedding von Sätzen auf der Datensatz-ViolentPCA legenden. Die Legende der Figuren zeigt die Anzahl der Trigger in jeder Satz.</sample>
    <sample id="1493">Wie im Bild zu sehen, ist es schwierig, zwischen den backdoor- und normalen Einbettagungen zu unterscheiden.</sample>
    <sample id="1494">Das war's, danke. Wir freuen uns auf einen Diskussion mit Ihnen.</sample>
    <sample id="1495">ABC-Eval steht für "annotating behaviors in chat" (in kürzerer Form).</sample>
    <sample id="1496">Das Leistungsdrop zwischen CoNLL-2003 und CoNLL++ ist nur durch Temporal Drift verursacht und nicht durch adaptive Overfitting, daher ist das Leistungsdelta nicht mehr als 5% im Jahr 2023.</sample>
    <sample id="1497">Hallo, mein Name ist Vasudeva und ich bin ein Kandidat im Bachelor-Studiengang Computerwissenschaften an Stony Brook University. Ich möchte gerne mein Werk, das in ACL 2023 als Long Paper akzeptiert wurde, präsentieren: "Transfer Learning for Sentiment Detection addressing the Rare Class Challenge".</sample>
    <sample id="1498">Wir beginnen indem wir kognitive Dissonanz definieren und warum es ein wichtiger Problem ist, es in der Sprache zu studieren. Einfach gesagt ist kognitive Dissonanz zwei Glaubens oder Handelungen, die inkonsistent sind.</sample>
    <sample id="1499">Solche wie diese Beispiele, in denen ein Mensch behauptet, dass Zigaretten mich umbringen könnten, und dann einen Joint nach dem Treffen raucht. Dieses Glauben und Handeln sind inkonsistent und sie sind in der Loyalität.</sample>
    <sample id="1500">Die Erwähnung, dass ich glaube, ich könnte mein Job nicht ohne sie machen, rechtfertigt die zweite Begegnung und sie haben eine enge Beziehung.</sample>
    <sample id="1501">Während Distanz ein sehr gemeinsames Phänomen im täglichen Entscheidungs-making ist, sind sie in Sprache unter anderem Art von Risiko-Relationen relativ selten zu finden.</sample>
    <sample id="1502">Warum ist das wichtig? Studieren kognitive Divergenz kann uns dabei helfen, die Auswirkungen von Diskriminierung among Menschen zu verstehen, Trends in Glaube, Werte und Einstellungen in Bevölkerungen zu identifizieren.</sample>
    <sample id="1503">Eine hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann helfen, das mentale Wohlbefinden von Menschen besser zu verstehen.</sample>
    <sample id="1504">Die Studie von Diskriminierungsexpressiven Sprachen kann auch nützlich sein, um das Extremismus- und Polarizationsspektrum von schwacher Gruppen zu verstehen.</sample>
    <sample id="1505">Endlich ist kognitive Distanz wichtig, um die individuellen kognitiven Stile zu verstehen und uns dabei beim Verstehen von Entscheidungsprozessen half.</sample>
    <sample id="1506">Um ein kognitives Dissonanzressourcengebiet zu schaffen, haben wir eine große Skala an Dissonanzrelationen ermittelt. Wir haben die Dissonanz-First-Ansatz, wie sie im Pfeilendiagramm hier zu sehen ist, verwendet.</sample>
    <sample id="1507">Tweets wurden mit einem PTB-Parser und Pares von Disourse-Units anhand der Richtlinien in unserem Papier annotiert.</sample>
    <sample id="1508">Hier ist die Distanz nur in 3,5 % der angedachten Parien entdeckt worden.</sample>
    <sample id="1509">Während sie über 1.000 Beispiele von Diskursunitpaaren sammelten, trainierten sie einen Anfangsklassifizierer nur auf 43 Beispielen von Dents. Noch überraschender war, dass der Klassifizator nicht viel besser als Zufall performte.</sample>
    <sample id="1510">Angesichts der geringen Häufigkeit von Diskussion und Abwesenheit von einem solchen zuvor existierenden Datensatz sind wir vor dem Problem der absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu beheben, experimentieren wir über Kombinationen von Transferlearning und aktiver Lernprozess, um solche zu identifizieren, dass mehr dissonante Stichproben über weniger Annotierungen sammelt, indem wir die Gesamtannotierkosten reduzieren und die Dissonanzdetection verbessern.</sample>
    <sample id="1512">Da das ursprüngliche Modell nicht in der Lage war, die Distanzklasse zu capturieren, beginnen wir den Prozess des aktiven Lernens, indem wir Gewichtungen von nahe verwandten Aufgaben übertragen.</sample>
    <sample id="1513">Wir übertragen von zwei verschiedenen Aufgaben: Eine, die Abhängigkeit von Themen und eine, die Klassifizierung von Dissonanzen anhand von Debattierstatementen von unterschiedlichen Personen bestimmt, ob sie ein Übereinstimmend oder ein Widerspruch darstellen, unabhängig vom Thema.</sample>
    <sample id="1514">Die Debatte hier und über die binäre Klassifizierung von Expansion und Comparisonschwergrenzen der Pidibee, da diese zwei eng mit dem Konzept von consonants und dissonants verbunden sind, und wir sie C E nennen.</sample>
    <sample id="1515">Wir finden, dass die Null-Shot-Performanz auf der antrainierten Datensammlung schon viel besser als zufällig ist, mit dem besten AC 0,62.</sample>
    <sample id="1516">Weiterhin beim iterativen Fine-Tuning an beiden Aufgaben finden wir, dass das Fine-Tuning von C-E-Aufgaben gefolgt von weiterem Fine-Tuning auf Debate eine viel bessere Null-Shot-Performance liefert. Daher verwenden wir diese Modellkonfiguration als Startpunkt für die aktive Lernprozesse.</sample>
    <sample id="1517">Nächster, wir bestimmen die beste Methode, um ein Modell mit neuen Daten von jeder Runde des aktiven Lernens und Annotierungen zu aktualisieren. Kummulative sammelt alle von aktiven Annotierungen gesammelten Daten, während iterative das Modell durch die Schulung auf dem aktuellen Datensatzaktualisiert.</sample>
    <sample id="1518">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1519">Nächster Schritt zur Verbesserung der Anzahl von Dissonanzbeispielen besteht darin, die Wahrscheinlichkeit von Real-Klasse-Strategie (PRC) zu verwenden, um hauptsächlich die Beispiele zu selektieren, die von unserem Modell in jeder Runde mit hoher Wahrscheinlichkeit als Dissonanz erkannt werden.</sample>
    <sample id="1520">Wir vergleichen dies mit anderen Standard Strategien, die in der Gemeinschaft allgemein verwendet werden.</sample>
    <sample id="1521">Wir finden, dass die vorgeschlagene PRC-Strategie besser als andere standard state-of-the-art-Strategien arbeitet, obwohl die Differenz klein ist. Beachten Sie, dass die Leistung erheblich für zufällige...</sample>
    <sample id="1522">In den nächsten Runden von AIL mit zwei besten Strategien verbessern wir die Distanzclassification auf 0,75, was die beste Performance auf dem Pfade so far ist.</sample>
    <sample id="1523">Wir überprüften auch die Feasibilty jedes Strategie für die Annotationqualität und die Kosten für die Annotatoren. Wir finden, dass PRC die höchste Quote von Disagreement hat und am besten für real-world-Klassen geeignet ist. Allerdings finden die Annotatoren die Beispiele schwierig.</sample>
    <sample id="1524">Insgesamt finden wir, dass PRC ein einfaches AL-Strategie für die Erwerb und Kalibration von AL mit korrekt gestalteten Transferlearning Aufgaben ist und dabei erheblich helfen kann.</sample>
    <sample id="1525">Wir finden auch, dass iterative Update nützlich für Transferlearning von einem anderen Bereich ist, während in-Domain active annotations von kumulative Updates profitieren.</sample>
    <sample id="1526">Dies sind die Links zu unserem Code-Dataset und zu unserem Paper. Ich bitte Sie, uns falls Sie Fragen haben, gerne in接触与我们联系。谢谢。</sample>
    <sample id="1527">Ich bedarf der englischen Übersetzung des Textes, um zu bestimmen, an welcher Universität die Autoren arbeiten. Kannst du mir bitte dabei helfen?</sample>
    <sample id="1528">The referent is Si Yu Yan from Fudan University.</sample>
    <sample id="1529">5</sample>
    <sample id="1530">Der Ansatz wird mit der SimulST-Architektur speziell für SimulST-Pretransliteration verglichen.</sample>
  </task>
</testset>