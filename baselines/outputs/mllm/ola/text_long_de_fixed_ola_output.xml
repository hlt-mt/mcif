<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind große Webkrawls.</sample>
    <sample id="1">Die Autoren, Akshatha und Martin, assozieren sich mit McGill University, Mila und Microsoft Research.</sample>
    <sample id="2">This paper presents a novel pre-trained model, LayoutMask, for Visually-rich Document Understanding (VrDU) tasks. Unlike existing models that use global 1D positions to represent the reading order of tokens, LayoutMask proposes to use local 1D positions based on in-segment token orders. This approach enhances text-layout interactions and layout representations learned during pre-training. The paper also introduces two novel masking strategies: Whole Word Masking and Layout-Aware Masking, which promote cross-segment orders and text-layout interactions. Additionally, a new pre-training objective, Masked Position Modeling, is designed to recover randomly masked 2D positions during pre-training. Experiments show that LayoutMask outperforms existing models on various VrDU tasks, such as FUNSD and SROIE.</sample>
    <sample id="3">Hallo! Willkommen bei unserer Präsentation von DEPLAIN, einem neuen Korpus für die Identifikation von Texten auf dem Dokument- und Satzlevel. Mein Name ist Regina Stodden, und ich werde Sie durch die erste Hälfte der Präsentation führen. Lassen Sie uns zuerst definieren, was Textsimplifikation ist. Textsimplifikation ist ein Prozess, bei dem ein Text anpassiert wird, um seine Verständlichkeit für eine bestimmte Zielgruppe zu verbessern, wie zum Beispiel Menschen mit Leseschwierigkeiten oder nicht-native Sprecher. Um einen Textsimplifikationsmodell zu trainieren, benötigen wir Paare von Texten, beispielsweise von Dokumenten oder Sätzen. Hier sehen Sie ein parallel aligniertes Satspaar von einem komplexen deutschen Satz und seiner Übersetzung in einfachere Sprache. Um den Satz zu vereinfachen, sind verschiedene Techniken möglich, wie Sie im Beispiel sehen können, wie z.B. lexikalische Substitution, Klausel deletion, Umordnung oder die Einfügung von Wörtern. Wir schaffen nun unser neues Corpus, DEPLAIN, aufgrund einiger Probleme mit bestehenden Korpusen. Zum Beispiel sind diese Korpusse zu kleinen, um ein Textsimplifikationsmodell zu trainieren. Die anderen drei Modelle, die in jüngster Zeit vorgeschlagen wurden, sind alle automatisch aligniert, was bedeutet, dass sie fehlerhaft sein können. Daher schaffen wir unser neues Corpus DEPLAIN, das in zwei Subkorpusen, DEPLAIN-apa und DEPLAIN-web, unterteilt ist. DEPLAIN-apa basiert auf Newsartikeln. In DEPLAIN-apa wurden 483 Dokumente alle manuell aligniert. Das ergibt etwa 13.000 parallel alignierte Satspair.Für DEPLAIN-web enthält der Korpus verschiedene Domänen und wir alignieren alle 750 dieser Dokumente, sowohl manuell als auch mit automatischen Alignierungsmethoden. Insgesamt erhalten wir 30.450 Satspair. Wir analysieren unsere Satspair ein bisschen weiter, zum Beispiel auf die Art der Simplifikation. Wie Sie sehen können, sind die Bibeltexte stark simplifizierter als zum Beispiel die Newsartikel oder die Lernmaterialien. Auf allen Ebenen, wie zum Beispiel lexikalische Simplifikation, Struktursimplifikation, auch die allgemeine Niveau der Simplifikation. Des Weiteren können Sie sehen, dass unser DEPLAIN-Korpus eine hohe Vielfalt an verschiedenen Simplifikationstransformationen hat. Zum Beispiel in der DEPLAIN-apa-Datei haben wir mehr Umordnungen und Worterfüllungen als in der DEPLAIN-web-Datei. Auf der anderen Seite haben wir in der Web-Datei mehr Umformungen. Nun sehen wir an, was wir damit erreichen können. Hallo, ich bin Omar und nun werde ich über die Anwendungsfälle für unser Datensatz DEPLAIN sprechen. Der erste Anwendungsfall, den wir betrachten, ist die Evaluation von automatischen Alignierungsverbesserungen. In den letzten Jahren gab es viel zu neuen Alignierungsverbesserungen, aber in Kontexten, in denen wir zwei parallel geschriebene Dokumente in zwei verschiedenen Sprachen haben und wir eine Alignierung von Sätzen in beiden Dokumenten extrahieren möchten. Aber in unserem Fall versuchen wir, Alignierungen zwischen Sätzen in zwei parallel geschriebenen Dokumente zu extrahieren, die dieselbe Sprache und dieselbe Bedeutung haben, aber auf unterschiedlicher Komplexität liegen. Und nun, da wir unser Datensatz DEPLAIN haben, das parallel alignierte Sätze als Gold Standard-Alignierungen verwenden können, können wir diese Sätze als Gold Standard-Alignierungen verwenden, um einige der vorgeschlagenen Alignierungsverbesserungen zu evaluieren. Und wir haben einige Anpassungen an die vorgeschlagene Methode vorgenommen und alle diese Anpassungen und die Codes, um unsere Experimente auszuführen, im Papier publiziert. Am Ende conclude wir, dass die beste automatische Alignierungsverbesserungsmethode für deutsche Textsimplifikation die Methode von MASSalign ist. Und Sie können auch den Code finden, um diese Methode auf Ihre eigenen Dokumente zu verwenden, im Papier. Der zweite Anwendungsfall, den wir im Papier gezeigt haben, ist ein Fall der automatischen Textsimplifikation durch die Anpassung von Sprachmodellen, um eine vereinfachte Texte aus komplexen Eingabe-Texten zu produzieren. Wir haben zwei verschiedene Modelle antrainiert. Wir haben das Modell von long-mBART antrainiert, um Dokumentebereichs-Simplifikationen zu produzieren, und wir haben auch das Standard-basierte mBART antrainiert, um Satzebereichs-Simplifikationen zu produzieren. Sie können auch alle Checkpoints finden und weitere Details über die Punkte und die Auswertungsmethoden our Experiments in dem Papier. Wir conclude, dass diese Anpassung eine höhere Punktzahl als die Baseline-Punktzahlen erzielen konnte, und wir schlagen diese Resultate als Grundstandard für die Problematik der automatischen Textsimplifikation in Zukunft vor. Vielen Dank für Ihre Aufmerksamkeit und wir hoffen, Sie alle während des Kongresses zu sehen. Vielen Dank.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Der T5 XL Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="6">Abstract: This paper presents a new approach to summarization called "many-to-many summarization," which aims to build one single summarization model that can process a document in any source language and generate its summary in any target language. The authors propose a pre-trained many-to-many summarization model named PISCES, which learns language modeling, cross-lingual ability, and summarization ability through a carefully designed three-stage pre-training. Experimental results show that PISCES outperforms various baselines, including mBART-50 and mT5, and can better transfer task knowledge across different languages than previous multilingual summarization and cross-lingual summarization models.</sample>
    <sample id="7">Ja, sie Funktionieren noch.</sample>
    <sample id="8">Die vorgeschlagene menschliche Bewertungsmethode, ABC-Eval, ist neu darin, dass sie das Ausdruck von bestimmten Verhaltensmustern in den Modellantworten annotiert, um eine präzisere und zuverlässigere Beurteilung der Chats zu ermöglichen.</sample>
    <sample id="9">Der Erfolg des bestehenden schwach überwartenen Ansatzes hängt von der Anzahl der sauber validierten Datensamples ab.</sample>
    <sample id="10">Das Ergebnis kann verbessert werden, indem die Sprachmodelle Zugriff auf teilweise überlappendes Hintergrundwissen erhalten. Wenn das Modell nur den Namen der Entitys hat, dann wird die Genauigkeit nur 60% betragen. Es gibt also noch viel Raum für Verbesserungen.</sample>
    <sample id="11">Abstract: Large language models have demonstrated the ability to generate and explain jokes, leading to claims of humor understanding. However, this study evaluates the humor comprehension capabilities of these models using The New Yorker Caption Contest data. The contest involves submitting captions for cartoons, with human raters evaluating quality. This research operationalizes three tasks: matching, quality ranking, and explanation generation. Results show that even fine-tuned models only achieve around 62% accuracy in matching, significantly lower than human performance (94%). GPT-4's joke explanations also contain errors, as human evaluations prefer human-authored explanations over those generated by the model. This study highlights the need for further research into humor understanding in language models.</sample>
    <sample id="12">There are five authors involved in the work presented.</sample>
    <sample id="13">Daniel Rotem presented his work on adaptive inference in low-resource settings, focusing on the comparison of two methods: Multi Model and Early Exit. He hypothesized that Early Exit suffers from conflicting gradients due to shared model parameters among classifiers. Rotem tested this hypothesis by comparing Early Exit models with separate Multi Model classifiers and found that Multi Model outperformed Early Exit by an average of 2.3%. To address conflicting gradients, Rotem introduced SWEET (Separating Weights in Early Exit Transformers), a fine-tuning method that avoids conflicting gradients by training each layer only with its following classifier's loss function. SWEET closed most of the gap between Early Exit and Multi Model but negatively affected later classifiers in some cases. Overall, Rotem's work highlights the existence of conflicting gradients in Early Exit training and introduces SWEET as a potential solution for improving Early Exit performance.</sample>
    <sample id="14">Hallo, mein Name ist Adam Przepiórkowski und heute sprechen wir über die Abhängigkeitsstruktur von Koordination. Wie Sie wissen, gibt es verschiedene Abhängigkeitsstrukturen, die von verschiedenen Theorien und Korpusansätzen angenommen werden. Zum Beispiel in den universal dependencies wird die Struktur der Koordination "Lisa, Bart, Maggie" so angenommen, dass das erste Conjunkt die Hauptstelle im gesamten Koordinatenstrukturspielt. In diesem Fall ist es Lisa. Ein ähnlicher Ansatz wird in Igor Mel'čuk's Meaning Text Theory angenommen, bei dem das gesamte Koordinatenstruktursich durch das erste Conjunkt bestimmt. Beide Ansätze sind asymmetrisch, sie singles out ein Conjunkt. Nun, es gibt auch einen symmetrischen Ansatz zur Koordination, wie zum Beispiel in der Prager Ansatz, der die Koordination durch die Konjunktion "und" annulliert. Hier werden Abhängigkeiten von Ende zu allen Conjunkten erstellt. Schließlich gibt es auch einen multi-head-Ansatz, der beispielsweise in Hudson's Word Grammar verwendet wird, bei dem alle Conjunkte als Hauptstelle im Koordinatenstrukturspielten. Hier werden Abhängigkeiten vomGovernor zu allen Conjunkten separat erstellt: Lisa, Bart und Maggie. Nun, das Ziel dieser Aufsatzes ist, eine neue Argumentation für die symmetrischen Strukturen der Koordination zu produzieren, wie diese zwei, gegen die asymmetrischen Strukturen der Koordination, wie diese zwei. Okay, das Argument basiert auf dem Prinzip der Abhängigkeitslängenminimierung, das ich auf der Grundlage dieser Beispiele erklären werde. In englisch weißt ihr, dass direkte Objekte bevorzugen, sich in der Nähe vom Verb zu befinden, während Adjektive weiter weg stehen können. Also "Marge liest es gestern" ist in Ordnung, weil das direkte Objekt in der Nähe vom Verb steht, während "Marge liest gestern es" viel schlimmer ist. Richtig? Denn hier zwischen dem Verb und dem direchten Objekt steht ein Adjektiv: "gestern". Allerdings kann dieser Effekt durch das Vermeiden des direchten Objekts verbessert werden, wenn es sehr schwerwiegend und lang ist. Denn dann kann es nach dem Adjektiv platziert werden. Das wird hier illustriert. So beide Sätze sind in Ordnung. "Marge liest dies absolut fascinierendes Buch über Bienen gestern." Es ist in Ordnung, anstatt "es" zu verwenden, ein sehr langes NP zu verwenden. Aber es ist auch in Ordnung zu sagen, "Marge liest gestern dieses absolut fascinierendes Buch über Bienen." So die Grundsätzlichness hier ist, dass dies möglich ist, weil obwohl diese Satzviolation die allgemeine grammatische Grundschaft, dass das direkte Objekt neben dem Verb stehen sollte, erfüllt wird, die Grundschaft der Abhängigkeitslängenminimierung erfüllt wird, die besagt, dass kürzere Abhängigkeiten bevorzuggemacht werden. Also die beiden Bäume zeigen nur die Längen der kritischen Abhängigkeiten an, die nicht unter den zwei Strukturen konstant sind. Hier haben wir eine Abhängigkeit von "liest" zum Adjektiv mit einer Länge von 7 (messbar in Worten) und von "liest" zu "Buch" mit einer Länge von 4, also insgesamt 11. Wenn wir diese zwei Bestandteile tauschen, wird die Summe dieser zwei Abhängigkeiten 6. Stattdessen von 11 6 ist, 6 ist viel kürzer. Das ist warum es klingt, als ob es gut klang. Richtig? EsViolation eines Grundsatzes, aber es erfüllt einen anderen Grundsatz. Okay, also was wir getan haben, wir haben verschiedene statistiken über Koordination aus der erweiterten Version des Penn Treebank extrahiert und die Aufsatz "Warum würdest du nicht Universal Dependencies verwenden?" und diese statistiken bestätigen die Beobachtungen, die vorher mehrmals gemacht wurden, dass die linken Conjunkte tendenziell kürzer sind. Also "Salz und Pfeffer" und nicht "Pfeffer und Salz", gemessen in Silben. Und auch die Beobachtung, die in Parsing gemacht wurde, dass diese Tendenz mit der Längendifferenz wächst. Also wenn die Differenz zwischen den Längen der beiden Conjunkte wächst, bevorzugt der kürzere Conjunkt als Erster; stärker, ja? So die Proportion wird größer, der kürzere Conjunkt links. Aber was neu in diesem Aufsatz ist, dass wir beobachtet haben, dass diese Tendenz nur dann stattfindet, wenn der Governor links ist oder weg ist. Richtig? Der Governor ist links in diesem Beispiel "ich sah Bart und Lisa", also ist der Governor links. Es ist weg in dem zweiten Beispiel "Homer kam und sperrte". Hier haben wir die Koordination von zwei Verben und es gibt keine externen Outsiders, Governor. In solchen Fällen bevorzugt der kürzere Conjunkt links; der größte Unterschied zwischen den beiden Conjunkten. Allerdings, wenn der Governor rechts ist, wie hier, "lacht" überwacht die Koordination Ted und Ned, diese Effekt verschwindet. Wir haben gezeigt, dass durch Messen der Längen in Zeichen, die erste Spalte, in Silben die mittlere Spalte, und in Wörtern die rechte Spalte sind. Ich werde mich auf die rechte Spalte konzentrieren. Was wir sehen hier ist, dass wenn der Governor links ist, die Tendenz, dass der kürzere Conjunkt links ist, sich allmählichOOM, mit der absoluten Differenz in Wörtern, und dass das gleiche beim Ausbleiben des Governors bei Koordination von Sätzen gilt. Aber wenn der Governor rechts ist, diese Tendenz verschwindet. Und wir zeigen im Aufsatz, wie dasProvides ein Argument gegen asymmetrische Strukturen der Koordination, wie diese zwei, und für die symmetrischen Strukturen, wie diese zwei. Also seht euch den Aufsatz an, um die vollständigen Argumente zu sehen. Und diskutiert darüber auf der Poster-Session. Danke.</sample>
    <sample id="15">Es sind insgesamt 3 Autoren an der Arbeit beteiligt: Matthias Lindemann, Alexander Koller und Ivan Titov.</sample>
    <sample id="16">Die Domänen, die stärker vereinfacht werden, sind die Bibeltexte.</sample>
    <sample id="17">Abstract: We propose a multimodal relation extraction method that addresses the issues of internal-information over-utilization and external-information under-exploitation. Our method consists of five parts: representing text and image with visual and textual scene graphs, merging them into a unified cross-modal graph (CMG), screening initial CMG structures with fine-grained filtering and adjustment guided by the graph information bottleneck principle, enriching compressed CMG features with multimodal topic features, and integrating multimodal topic words through attention operation. Experiments on a widely used MRE dataset show that our method achieves significant improvements over existing models.</sample>
    <sample id="18">Ein Beispiel für die Präferenz für kürzere linke Konjunktionen lautet "Marge read this absolutely fascinating book about bees yesterday." Hier wird das Hauptobjekt (das Buch) nach dem Adjunkt (gestern) gestellt, um die Abhängigkeitslänge zu minimieren und eine kürzere Abhängigkeit zwischen dem Verb "liest" und dem Hauptobjekt zu erhalten.</sample>
    <sample id="19">Title: Efficient Open Domain Question Answering Systems

Authors: Zhang Qin, Shenzhen University

Abstract: This paper presents a survey on efficient open-domain question answering systems. The authors focus on the two-stage model proposed by Danqi Chen in 2017, which uses a retrieval to retrieve evidence contexts from a Wikipedia corpus and a reader to reason out the answer. However, the large size of the Wikipedia corpus (26 million documents) and the index file (65 GB) pose challenges for real-time applications and resource-constrained devices. To address these issues, the authors propose several techniques, including approximate nearest neighbor search, skip reading, document filtering, embedding compression, parameter sharing, and knowledge distillation. They also compare existing models from the data aspect and discuss future works on deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Ja, Sie können die Modelle für Ihre Forschung verwenden. Alle vor Trainieren modeln und die dazugehörigen Trainingsskripte sind unter der MIT-Lizenz auf Hugging Face und GitHub frei zugänglich.</sample>
    <sample id="21">DEplain-apa basiert auf Newsartikel.</sample>
    <sample id="22">Eine gute Generalisierung wird durch die Modelle-Struktur, die Modellgröße und mehr Fine-Tuning-Beispiele erreicht.</sample>
    <sample id="23">Abstract: Text image models have made significant progress in generating high-quality images, but often struggle with representing text. This paper investigates the performance of various text encoders in spelling words and proposes a new strategy to improve text rendering. We introduce WikiSpell and DrawText benchmarks for text-only and text-to-image models, respectively. Our approach involves concatenating the output of a ByT5-small model with the existing text representation, which enhances the model's ability to spell words accurately. While this method does not eliminate all errors, it significantly improves the overall performance of text image models.</sample>
    <sample id="24">Die Tendenz zu kürzeren linken Konjunktionen wurde gemessen, indem die Längen der Konjunktionen in Zeichen, Silben und Wörtern gemessen wurden.</sample>
    <sample id="25">Die Experimente wurden so gestaltet, dass die Längen der Konjunkte in verschiedenen Einheiten gemessen wurden, wie z.B. in Zeichen, Silben und Wörtern. Dadurch wurde untersucht, ob die Position des Begrenzers einen Einfluss auf die Längen der Konjunkte hat.</sample>
    <sample id="26">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, kann nicht viel besser als Zufall performieren. In dem Papier wird erwähnt, dass die ursprüngliche Klassifizierung nur mit einem AUC von 0,62 auf der annotierten Datensammlung gut performiert hat.</sample>
    <sample id="27">Die Anzahl der Autoren, die an der Arbeit beteiligt sind, wurde nicht erwähnt.</sample>
    <sample id="28">Die Personen im Beispielgespräch heißen Bob und Alice.</sample>
    <sample id="29">Kontextsensitive MÜ-Modelle schneiden besser ab als kontextagnostic Modelle bei Diskursphänomenen wie Formalität und lexikalischer Cohesion.</sample>
    <sample id="30">Abstract: We introduce LLM-Blender, a simple yet effective ensemble learning framework for large language models (LLMs). Our approach leverages pairwise ranking and generative fusion to select the optimal model for each input. We propose a two-stage framework where we first rank candidate models using a PairRanker module that compares pairs of outputs alongside the input. The top K candidates are then used as inputs to a sequence-to-sequence model for generating the final output. Our experiments on the MixInstruct dataset demonstrate that LLM-Blender significantly outperforms individual models and achieves state-of-the-art results in 68% and 76% of examples for Open Assistant and Vicuna, respectively. This suggests that LLM-Blender is a promising framework for ensemble learning in NLP tasks.</sample>
    <sample id="31">Die Autoren des Artikels "Language model acceptability judgments are not always robust to context" stammt aus der University of California, Berkeley.</sample>
    <sample id="33">Das vorgestellte Framework quantifiziert die Positionalität durch die Korrelation von Annotierungen mit dem Demoskopie und den Modellen und Datensätzen mithilfe einer Pearson's R-Korrelationsstelle.</sample>
    <sample id="34">This work presents CREST, a joint framework for selective rationalization and counterfactual text generation. It combines the strengths of selective rationalization and counterfactual generation to produce valid, fluent, and diverse counterfactuals in a controllable way. The framework consists of two components: a rationalizer model that generates rationales and an editor that uses these rationales to generate counterfactual examples. The quality of the counterfactuals produced by CREST is evaluated using both automatic metrics and human evaluation. The results show that CREST can be used for data augmentation and improves downstream models when used for rationalization with both factual and counterfactual examples. Overall, CREST produces plausible explanations that focus on the contrasting parts of the input.</sample>
    <sample id="36">This paper presents a method for improving multilingual machine translation by adding language-specific layers to the model. The method, called Language-Specific Layers (LSL), involves adding one regular transformer layer per language and using it to select and train the correct sublayer at inference time. This allows for increased capacity per language while keeping inference costs constant. The authors use a deep encoder shallow decoder model and train on WMT21 news translation data for 10 languages, including some low-resource languages. They evaluate their approach on Flores-101 and report significant improvements over baseline models and language adapters, particularly for low-resource languages.</sample>
    <sample id="37">Die vorherige Studie fand, dass die menschlichen Teilnehmenden bei den Persona-Prompts Stereotypien surfen konnten.</sample>
    <sample id="38">In dieser Studie wurden die statistiken aus dem enhanced version of the Penn Treebank und das Papier "Why wouldn't you use universal dependencies" verwendet.</sample>
    <sample id="39">Es wird in dem Text nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="40">Eng verwandte Aufgaben für kognitive Dissonanz sind die diskussionsunabhängige Stimmstellebestimmung (Debatte) und die Klassifizierung von Auswechslungs- und Vergleichsklassen (CE).</sample>
    <sample id="41">This paper introduces PeaCoK, a Persona Commonsense Knowledge Graph that represents real-world personas and their attributes at scale. PeaCoK contains about 3,800 personas and 40,000 distinctive attributes, forming about 100,000 personal inferences or facts. The graph is built in three steps: selecting personas from existing commonsense graphs, inducing attributes of personas from both commonsense knowledge graphs and large-scale pre-trained language models, and crowdsourcing the annotations of PeaCoK relations using a joint human-AI majority voting scheme.

PeaCoK is used to train a BART-based common knowledge generator on a persona attribute inference task, where the model needs to predict an attribute of a given persona with a target relation. Compared to large-scale pre-trained language models, Comet-BART trained on PeaCoK achieves overall better automatic evaluation results on various natural language generation metrics and also a higher accept rate in human evaluation.

The paper also explores whether PeaCoK knowledge can be used to improve downstream narrative modeling. A persona-grounded dialogue generation task on the ConvAI2 PersonaChat data set is investigated. Specifically, a knowledge linker retrieves facts from PeaCoK that are relevant to each speaker's original persona profile and utterances, converting the retrieved facts into natural language statements to augment each speaker's profile. Human evaluation shows that PeaCoK augmented models achieve better dialogue generation on various aspects, including fluency, consistency, engagement, and persona expression.</sample>
    <sample id="42">Es wird in dem Text nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="43">Es wird in der Präsentation nur ein Autor, Vasudha, erwähnt. Es ist jedoch schwierig zu bestimmen, ob sie alleine gearbeitet hat oder mit anderen Autoren zusammengearbeitet hat, da die Präsentation nicht alle Aspekte der Arbeit abdeckt.</sample>
    <sample id="44">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten, indem es nicht nur die Annotator-Übereinstimmung berücksichtigt, sondern auch die Positionalität von Modellen und Datensätzen mit realen Benutzern vergleicht. Es verwendet Pearson's R Korrelations得分来比较注释和模型预测标签，而不是只关注注释者之间的意见分歧或建模注释者的分布。</sample>
    <sample id="45">Die generierten Personas.</sample>
    <sample id="46">Die kommerziellen Systeme, die verglichen wurden, sind DeepL und Google Translate.</sample>
    <sample id="47">Hallo, ich bin Shangbin, PhD-Student an der University of Washington. Heute präsentiere ich unser Werk "Von der Vorbildtraining-Daten zu Sprachmodellen zu Downstream-Aufgaben: Das Verfolgen der Spuren von politischen Biases, die zu unfairen NLP-Modellen führen". Sprachmodelle werden auf großen Datensammlungen aus dem Web trainiert. Politische Medien werden in ihren Vorbildtraining-Daten gut abgedeckt. Laut einer Umfrage des C4 Corpus können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in den Sprachmodelltraining-Daten gut vertreten sind. Dies hat für die Anwendung von Sprachmodellen ein doppeltes Schwergewicht geschaffen. Einerseits können sie von diversen Perspektiven lernen und damit dem Demokratie und der Vielfalt von Ideen zusprechen. Andererseits sind diese verschiedenen politischen Meinungen von Natur aus sozial biasiert und könnten potenzielle Fairness-Probleme in Downstream-Aufgabenermittlungen hervorrufen. Deshalb schlagen wir vor, die politische Bias-Propagation-Abfolgpipeline von den Vorbildtraining-Daten über die Sprachmodelle bis hin zu Downstream-Aufgaben zu untersuchen, indem wir folgende Fragen stellen: Zunächst, wie lassen sich die politische Neigung von Sprachmodellen bewerten und welche Rolle die Vorbildtraining-Daten dabei spielen? Zweitens, wie performieren Sprachmodelle mit unterschiedlichen politischen Neigungen bei Downstream-Aufgaben und ob dadurch Fairness-Probleme in NLP-Anwendungen entstehen könnten. Wir haben daher vorgeschlagen, Sprachmodelle mit verschiedenen prompt-Formaten zu provozieren, die durch politische Umfragen wie dem Test für politische Konferenzen definiert werden. Um eine automatische Bewertung zu gewährleisten, die fundiert in der politischen Wissenschaft ist. Einige vorläufigen Ergebnisse zeigen, dass Sprachmodelle unterschiedliche politische Neigungen haben. Sie nehmen alle vier Quadranten auf dem politischen Campus ein. Wir können auch sehen, dass GPT-4 das liberalste Sprachmodell von allen ist, und GPT-Reihe allgemein mehr sozial liberale als BART-Reihe und ihre Varianten. Zweitens wollen wir untersuchen, in welchem Maß die politischen Biases von Sprachmodellen von den Trainingsschwierigkeiten abhängig sind. Wir können dies durch einen kontrollierten Versuch erreichen, indem wir Sprachmodell-Schritte auf sechs unterschiedlichen partizipen korpusen forttrainieren, die getrennt in News und Social Media, weitere getrennt in ihre politische Neigung unterteilt sind. Wenn wir Sprachmodelle auf solche partizipen korpusen weitertrainieren, können wir sehen, dass die ideologischen Koordinaten des Sprachmodells entsprechend verschieben werden. Zum Beispiel, wenn wir RoBERTa weitertrainieren auf dem linken linken Reddit-Korpus, können wir einen beachtlichen liberalen Schritt in Bezug auf seine politischen Biases feststellen. Und wir versuchen auch zu untersuchen, ob Sprachmodelle die Polarisation, die in unserer modernen Gesellschaft vorliegt, aufnehmen können. Wir unterteilen die Vorbildkorpus in vor und nach dem 45. Präsidenten der Vereinigten Staaten. Wir separieren Sprachmodelle auf die zwei verschiedenen temporären korpusen. Wir können sehen, dass Sprachmodelle allgemein eine politische Neigung haben, die weiter weg vom Zentrum nach 2017 steht. Das zeigt, dass Sprachmodelle auch die Polarisation in unserer Gesellschaft aufnehmen können. Schließlich evaluieren wir Sprachmodelle mit unterschiedlichen politischen Neigungen auf Aufgaben wie Hasssprache-Erkennung und Falschinformation-Erkennung, die in NLP-Anwendungen oft vorkommen und sehr wichtige Auswirkungen haben könnten. Wenn wir die pro KategorienerFORMANCE-Bewertung untersuchen, das ist, wenn wir die Leistung in Abhängigkeit von den demografischen oder politischen Neigungen der Medienmediums unterteilen, können wir ein Muster feststellen. Zum Beispiel, bei der Erkennung von Hasssprache sind linksliberalere Sprachmodelle besser darin, Hasssprache gegen minderprivilegierte Gruppen zu erkennen, aber schlechter darin, Hasssprache gegen stärkeren Gruppen in unserer Gesellschaft zu erkennen. Und umgekehrt, rechtsliberalere Sprachmodelle sind besser darin, Hasssprache gegen Weiße und Männer zu erkennen, aber schlechter darin, Hasssprache gegen Afroamerikaner, LGBTQ+ und andere minderprivilegierte Gemeinden zu erkennen. Ähnliche Trends finden wir auch bei der Erkennung von Falschinformation, bei der wir sehen, dass linksliberalere Sprachmodelle besser darin sind, Lügennachrichten von ihrer gegenüberliegenden politischen Neigung zu erkennen, und umgekehrt. Wir zeigen auch viele qualitative Beispiele an, um zu zeigen, dass Sprachmodelle mit unterschiedlichen politischen Neigungen unterschiedliche Vorhersagen für Hasssprache und Falschinformation begeben basierend auf ihren sozialen Kategorien. Es gibt noch mehr Beispiele im Anhang, um zu verdeutlichen, dass es ein Fairnessproblem gibt, das sehr dringlich ist, aufgrund der politischen Biases von Sprachmodellen. Zum Beispiel, wenn rechtsliberalere Sprachmodelle auf Hasssprache oder Falschinformation und so weiter trainiert werden und auf einem populären sozialen Medien-Plattform eingesetzt werden, bedeutet das, dass Menschen mit gegenüberliegender politischer Meinung marginalisiert werden und Hasssprache gegen minderprivilegierte Gruppen unkontrolliert ausbreiten kann. Das sollte uns auffordern, die Fairnessprobleme, die durch die politischen Biases von Sprachmodellen entstehen, anerkennen und zu bewältigen. Ein kleiner Diskussion. Wir möchten auch betonen, dass wir das einzigartige Dilemma der politischen Biases von Sprachmodellen hervorgerufen haben. Es ist wie zwischen Scylla und Charybdis. Wenn wir nicht politische Meinungen in den Vorbildtraining-Daten sauber machen, propagieren die Biases von den Vorbildtraining-Daten über die Sprachmodelle bis hin zu Downstream-Aufgaben, was letztendlich Fairness-Probleme hervorrufen kann. Wenn wir versuchen, etwas zu sauben, riskieren wir Censorship oder Exklusion. Und es ist äußerst schwierig zu bestimmen, was als neutral angesehen werden soll und in welchen Sprachmonitoring-Daten behalten werden sollte. Es ist also, wie das Elektrolokomotive-Problem. Okay, großartig. Ich denke, das ist alles, was ich heute habe. Danke für eure Zeit.</sample>
    <sample id="48">Es wird in der vorgelegten Übersicht erwähnt, dass es ein gemeinsames Werk mit mehreren Kollegen von Google Translate ist. Es wird jedoch nicht erwähnt, wie viele Autoren insgesamt an der Arbeit beteiligt sind.</sample>
    <sample id="49">MPP-Auswertungen wurden bis zu 1024 Token Kontextlängen durchgeführt.</sample>
    <sample id="50">Abstract: DEPLAIN: A New Corpus for German Text Identification on the Document and Sentence Level

This presentation introduces DEPLAIN, a novel corpus designed to address issues with existing corpora in text simplification. DEPLAIN is divided into two subcorpora: DEPLAIN-apa, based on news texts with 13,000 manually aligned sentence pairs, and DEPLAIN-web, which includes various domains with 30,450 sentence pairs, both manually and automatically aligned. The corpus showcases diverse simplification techniques such as lexical substitution, clause deletion, reordering, and word insertion, providing a high variety of transformations across different text types. The presentation highlights the use cases of DEPLAIN, including evaluating automatic alignment methods and fine-tuning language models for automatic text simplification. The results demonstrate that DEPLAIN can serve as a valuable resource for developing and benchmarking text simplification models.</sample>
    <sample id="51">Sie haben in ihren Datensatz drei verschiedene Domänen aufgenommen: Musik, Bücher und Rezepte.</sample>
    <sample id="52">Positionalität bezieht sich auf die Perspektiven, die Menschen als Folge ihrer Demografie, Identität und Lebenserfahrungen halten.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">Abstract:
This paper presents a long paper accepted into ACL 2023, titled "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge." The authors define cognitive dissonance and its importance in understanding human decision-making processes. They conducted a large-scale annotation of dissonance relations using a dissonance-first approach, but found that dissonance was only present in 3.5% of the annotated pairs. To address the rarity of dissonance data, they experimented with transfer learning and active learning to collect more dissonant samples while reducing annotation costs. They found that transferring weights from closely related tasks improved zero-shot performance, and that the cumulative update strategy performed better than iterative update for domain active annotations. The proposed Probability-of-Rare-Class (PRC) strategy outperformed other state-of-the-art AL strategies in terms of rare class acquisition. The authors conclude that PRC is a simple and effective AL strategy for rare class acquisition when combined with appropriately designed transfer learning tasks.</sample>
    <sample id="55">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell. Es verwendet offline-ST-Modelle, ohne sie zu retrainieren oder spezielle Architekturen für SimulST zu verwenden.</sample>
    <sample id="56">Der Inhalt erwähnt nur einen Autor, Yusen Zhang. Es wird nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="57">Das getestete Modell in der Testsuite zeigt, dass es schwierig ist,.backward knowledge zu integrieren, die nur zur Laufzeitavailable sind.</sample>
    <sample id="58">Die drei Varianten von KITMUS sind: 1) Background-Pretrain (wo die Hintergrundinformation bei der vorherigen Schulung enthalten ist), 2) Background-Both (wo sowohl Hintergrundinformation als auch spezifische Informationen zur Identität der Entitys zur Verfügung stehen) und 3) Background-Inference (wo nur spezifische Informationen zur Identität der Entitys zur Verfügung stehen).</sample>
    <sample id="59">Abstract: We present DrBERT, a robust pre-trained model in French for biomedical and clinical domains. Our model is based on RoBERTa and trained on NACHOS, a data set of medical crawled data from the web. We compare DrBERT with models pre-trained on anonymized data from the Nantes University Hospital data warehouse (ChuBERT) and models trained using continual pre-training strategies. Our results show that DrBERT outperforms baseline models on nine of the 11 downstream tasks, surpassing the performance of generic models like CamemBERT. All pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license, and training scripts are available on our GitHub repository.</sample>
    <sample id="60">Die Autoren dieser Arbeit sind nicht an einer bestimmten Universität verbunden. Es wurde lediglich erwähnt, dass sie an einem Joint-Work-Projekt teilgenommen haben, das von Javad Hosseini geleitet wurde.</sample>
    <sample id="61">Die abschließende Forschungsfrage lautet: Should we only use the clean samples for validation, or are there better ways to utilize them?</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG) tasks. The authors explore the potential of compressing NLG models while preserving their performance, focusing on task-specific knowledge distillation in realistic setups. They consider four NLG tasks: summarization, question generation, common sense reasoning, and style transfer, using medium-resource labeled data sets and large amounts of unlabeled data. The study compares different approaches for knowledge selection, including word-level and sequence-level distillation, and proposes a novel joint-teaching technique to address student exposure bias and teach the student to correct its own mistakes. The results show that generating multiple pseudo-targets and sampling with high temperature can improve the student model's performance.</sample>
    <sample id="63">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für dieselbe Aufgabe immer die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="64">Jingwei Yi</sample>
    <sample id="65">Eine höhere Sensitivität bedeutet, dass das Modell für die gleiche Aufgabe unterschiedliche Ausgaben produziert, je nach Wortlaut der Anweisung. Das Gegenteil, eine niedrigere Sensitivität, zeigt, dass das Modell für die gleiche Aufgabe immer dieselben Ausgaben produziert, unabhängig von der Anweisung. Daher ist eine höhere Sensitivität im Allgemeinen nicht eine bessere Leistung des Modells.</sample>
    <sample id="66">This paper presents a survey on deep learning methods for mathematical reasoning, which is a fundamental aspect of human intelligence. It covers the development of machines capable of solving math problems and proving theorems, including text-based data, multimodal information like images and tables, and geometric diagrams. The paper discusses neural network architectures such as sequence-to-sequence models and pre-trained language models (LLMs) that have been proposed for mathematic reasoning tasks. It also highlights the limitations of LLMs in performing precise mathematical reasoning and proposes solutions such as self-consistency and program-aided LMMs to boost performance. Additionally, the paper addresses the challenges of generalization and robustness failures in learning models on reasoning tasks and the need for more datasets in low-resource settings.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models and identifies the main factors contributing to it. The authors find that severe interference occurs when the model is small compared to the data size, and that tuning the sampling temperature is key for strong performance. They also conclude that language similarity and the number of languages do not have a large impact on interference levels. The paper provides experimental results on the effect of model and focus data sizes, as well as the results with more languages sharing similar trends. The authors suggest that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">Die Modelle erhalten während des Pre-Trainings einen Kontext, der sowohl relevant als auch irrelevant zu den aktuellen Suchanfragen ist.</sample>
    <sample id="69">Normalerweise werden für eine gute Leistung an der WSL normalerweise 20 saubere Validierungsbeispiele pro Klasse benötigt.</sample>
    <sample id="70">Es wird in dem Text nicht erwähnt, an welcher Universität die Autoren Myra, Esin Durmus und Dan Jurafsky arbeiten.</sample>
    <sample id="71">Abstract: We introduce the AltEntities Corpus, a large-scale public dataset for resolving indirect referring expressions in conversational systems. Our dataset covers music, books, and recipes, and includes 6,000 alternative questions with 42,000 indirect referring expressions. We use a cartoon completion setup to create dialogue contexts, and provide background knowledge about the entities to annotators. Our results show that T5 XL model accuracy is high when it has access to the same background knowledge as annotators (92-95%), but drops to 82-87% with partially overlapping knowledge, and only 60% with entity names alone. The models are also domain-generalizable.</sample>
    <sample id="72">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Auswirkungen von politischen Verzerrungen auf Sprachmodelle und downstream-Anwendungen zu verstehen und zu bewältigen. Dadurch können wir sicherstellen, dass Sprachmodelle faire und neutralere Entscheidungen treffen und nicht dazu beitragen, Menschen mit einstimmigen Ansichten oder Menschen von underprivilegierten Gruppen zu marginalisieren.</sample>
    <sample id="73">Die Referentin hieß Akshatha.</sample>
    <sample id="74">Title: Dense-ATOMIC: Enhancing Commonsense Knowledge Coverage and Multi-hop Paths

Abstract: This paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph that addresses the limitations of the original ATOMIC dataset. We propose a novel completion method, Rel-CSKGC, which predicts missing links using a relation prediction model. Our approach leverages pre-trained language models to encode events, avoiding sparse graph structures and utilizing semantic information. Dense-ATOMIC exhibits higher knowledge coverage with more 1-hop, 2-hop, and 3-hop paths, benefiting the performance of COMET. Evaluation on multi-hop paths shows promising results, demonstrating the potential for enhanced commonsense reasoning.</sample>
    <sample id="75">Title: Jointprop: A Joint Semi-Supervised Learning Framework for Name Entity Recognition and Relation Extraction

Authors: Zheng Yandan, Hao Anran, and Luu Anh Tuan

Abstract: This paper presents a joint semi-supervised learning framework, named Jointprop, for name entity recognition (NER) and relation extraction (RE). The proposed method integrates NER and RE tasks by propagating labels over heterogeneous graphs, considering the inter- and intra-connections among labeled and unlabeled data. The framework consists of span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Experiments on four datasets demonstrate that Jointprop significantly improves performance over the base model in both joint-task and single-task settings, benefiting from the codependency between the two tasks.</sample>
    <sample id="76">Die Pipeline für die Verbreitung politischer Vorurteile beginnt mit der Evaluierung des politischen Neigens von Sprachmodellen und der Rolle von Ausbildungsdaten bei der Bildung solcher Vorurteile. Danach werden die Auswirkungen der verschiedenen politischen Neigungen von Sprachmodellen auf die Leistung in downstream Aufgaben untersucht, um festzustellen, ob dabei potenzielle Fairnessprobleme in NLP-Anwendungen entstehen.</sample>
    <sample id="77">This paper presents a new dataset, DeFacto, for improving summarization factual consistency. It includes human demonstrations and feedback to enhance the accuracy of summarization models. The authors propose three new Natural Language Generation (NLG) tasks: summary editing, feedback generation, and automatic factual error correction. They study abstractive text summarization and analyze the factual consistency of existing summarization models. Annotated data points are collected from the XSum dataset using the pre-trained Pegasus model. The results show that human-edited summaries receive higher automatic factuality scores but have lower textual overlap with reference summaries. The paper also discusses the challenges and advantages of the proposed NLG tasks and provides insights into training factuality metrics and meta-evaluation.</sample>
    <sample id="78">Ja, es unterscheidet sich. In DEPLAIN-apa gibt es mehr Umordnungen und Worterfüllungen, während in DEPLAIN-web mehr Umformungen stattfinden.</sample>
    <sample id="79">Ja, CoScript ist öffentlich verfügbar.</sample>
    <sample id="80">Das Wasserzeichen wird in den Text eingebettet, indem ein Trigger-Set von Wörtern mit einem moderaten Häufigkeitsintervall ausgewählt wird. Wenn ein Benutzer eine Anfrage sendet, zählt der Anbieter die Anzahl der Triggers im Satz und berechnet die Gewichtsummation des ursprünglichen Embeddings und des Zielmusters. Der Gewichtsanteil des Zielmusters hängt von der Anzahl der Triggers ab. Wenn mehr als m Triggers in einem Satz vorkommen, ist das bereitete Embedding genau wie das Zielmuster.</sample>
    <sample id="81">Die Autoren, Yusen Zhang und seine Teammitglieder, gehören an der Penn State University.</sample>
    <sample id="82">This video introduces a new framework for unsupervised automated essay scoring (AES) called ULRA, which uses multiple heuristic quality signals as pseudo-ground-truth to train a neural AES model. The core idea of ULRA is to aggregate the partial-order knowledge contained in multiple heuristic quality signals and use it as supervision for training. The framework consists of two modules: a heuristic essay ranking module (HER alpha-shot) and a deep pairwise rank aggregation module (DPRA). HER alpha-shot generates partial-order pairs by ranking essays according to multiple classic quality signals, while DPRA trains a neural AES model by aggregating these partial-order pairs into a unified supervision. The proposed Scoring Strategy transforms the predicted scores given by the neural AES model into the range of the predefined score set through a minimum-maximum transformation. Experimental results demonstrate that ULRA outperforms all unsupervised baselines with a large improvement and achieves competitive performance compared to cross-prompt and one-shot methods.</sample>
    <sample id="83">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="84">Title: PAD-Net: An Efficient Framework for Dynamic Networks

Abstract:
Dynamic networks, which adapt their architecture or parameters based on input, offer superior performance over static networks. However, fully dynamic networks often suffer from excessive parameter usage, limiting their practicality. This paper introduces PAD-Net (Partially Dynamic Network), a framework that partitions network parameters into dynamic and static modes to optimize resource utilization while maintaining performance. PAD-Net uses an iterative mode partition method to identify and freeze redundant dynamic parameters, significantly reducing model size and computational cost. Experimental results demonstrate that PAD-Net outperforms both static and fully dynamic networks, offering better accuracy with fewer parameters. The study also explores optimal dynamic ratios for different network components and compares PAD-Net's performance with network pruning, highlighting its effectiveness in maintaining static parameters and improving output discrimination. Future work includes extending PAD-Net to other mainstream networks and exploring hardware-friendly structured methods.</sample>
    <sample id="85">Ein Beispiel für eingeschränkte Sprachplanung ist die Planung, einen bestimmten Kuchen (z.B. einen Schokoladenkuchen) zu machen. Hierbei müssen bestimmte Bedingungen beachtet werden, wie z.B. die Art und Quantität der Zutaten, die Verwendung bestimmter Küchenutensilien und die Einhaltung bestimmter Rezeptur Schritte.</sample>
    <sample id="86">Sie sichern die Opazität ihrer Methode sicher, indem sie die embeddings visualisieren und es schwierig machen, zwischen den embeddings von normalen und backdoor-sentenzen zu unterscheiden.</sample>
    <sample id="87">Die Arbeit nutzt bestehende PLMs (Prä-trained Language Models) wie CamemBERT, PubMedBERT, BioBERT und ClinicalBERT, um ein neues PLM aufzubauen. Sie trainieren verschiedene Modelle auf verschiedenen Datensätzen und mit verschiedenen Vorgehensweisen, um zu analysieren, welche Ansätze am besten zur Verbesserung der Leistung beitragen.</sample>
    <sample id="88">GPT-4 ist am wenigsten auf Indien ausgerichtet.</sample>
    <sample id="89">Der Beispielsatz, der zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde, ist der "cross-attention mechanism".</sample>
    <sample id="90">This paper challenges the conventional practice of using native speakers for data annotation in NLP, proposing that language learners can also contribute effectively. The authors conducted a proof-of-concept study with three languages (English, Korean, and Indonesian) and four tasks (sentiment analysis, NLI, NER, and MRC). They categorized learners into basic, intermediate, and advanced levels and compared their annotation accuracy to native speakers. Results show that language learners' labels are nearly accurate, especially for simpler tasks, and they can even match native speakers when aggregated by majority voting. Learners also exhibit improved language proficiency and vocabulary through annotation tasks. This work suggests a novel approach to building NLP datasets for low-resource languages by recruiting language learners, potentially broadening NLP research to many languages and overcoming geographic and technological barriers.</sample>
    <sample id="91">Je mehr Aufgaben das Modell trainiert, desto besser perforiert es und desto niedriger ist die Sensibilität.</sample>
    <sample id="92">Die dreibaumlose Baselines, mit denen die Autoren ihre Methode vergleichen, sind: 1. Seq2seq-Model 2. Pointer-Generator-Model 3. Copy-Attention-Model</sample>
    <sample id="93">Die beiden Co-Autoren, Alexander Koller und Ivan Titov, sind die Betreuenden des ersten Autors Matthias Lindemann.</sample>
    <sample id="94">Title: Protecting the Copyright of Large Language Models for Embedding as Services via Backdoor Watermark

Authors: Jingwei Yi, University of Science and Technology of China

Abstract: This paper proposes a backdoor-based watermark method called Embedding Marker to protect the copyright of large language models (LLMs) when embedding them as services. The method involves watermark injection and copyright verification. In watermark injection, a target embedding is defined, and the provided embedding is a weighted summation of the target and original embeddings based on the number of triggers in the input sentence. In copyright verification, the provider requests embeddings from the stealer's service with a dataset containing both benign and backdoor sentences. The similarity between the requested embeddings and the target embedding is computed using cosine and L2 similarity, and the KS test is applied to determine if the model contains the watermark. Experiments on four datasets show that Embedding Marker can effectively detect watermarks while maintaining utility for downstream tasks. The covertness of the provided embedding is validated through PCA visualization, showing that it is difficult to distinguish between backdoor and normal embeddings.</sample>
    <sample id="95">David Vilar</sample>
    <sample id="96">Hallo alle. Ich bin Jenny, ein erstes Jahr im PhD-Staat an Carnegie Mellon University und heute präsentiere ich euer Werk NLPositionality, das die Designbiases von Datensätzen und Modellen charakterisiert. Dieses Werk wurde in Zusammenarbeit mit ein paar Leuten aus der University of Washington und dem Allen Institute for AI, zuallermeisten Sebastian Santy, Ronan Le Bras, Katharina Reinecke und Maarten Sap, getan. Also lass uns damit beginnen, uns vorzustellen, als wir arbeiten für eine Zeitung und durch die Kommentare zu einem Newsartikel siftingen, um giftige Inhalte zu entfernen. Du könntest auf eine beliebte API wie den Prospective API verweisen, die sehr gut funkt, wenn du Carl Jones bist. Wenn Prospective API jedoch Carl Jones nicht als giftig erkennt, dann ist das ein Beispiel für einen Designbias, bei dem es systematische Leistungsunterschiede zwischen Bevölkerungen gibt. Designbiases wie der, den wir vorhin gesehen haben, können aufgrund der Positionalität der NLP-Forscher und Modellentwickler auftreten. Positionalität ist einfach die Perspektiven, die Menschen als Folge von ihren Demografien, Identitäten und Lebenserfahrungen halten. Dies ist ein Konzept, das in kritischen Studien, insbesondere in feministischen und queergeschichtlichen akademischen Bereichen, weit verbreitet ist. Und als Forscherin kann Positionalität die Forschungsprozess und seine Ausgänge beeinflussen, indem sie die Entscheidungen von Forschern verändert. Eine Frage, die sich jemand fragen könnte, ist, ob Datensätze und Modelle Positionalität haben. Wir versuchen nicht zu sagen, dass Modelle oder Datensätze eigene demografische Identitäten und Lebenserfahrungen haben, aber sie aggregieren Urteile und Meinungen von realen Menschen und können daher bestimmte Positionalitäten über andere vertreten. Vorherige Arbeiten haben einige anecdettswerte Beweise von Positionalität in Modellen und Datensätzen vorgestellt, wie kulturelle Lücken in Modellen und Datensätzen sind, sowie theoretische Definitionen von Modell-Positionalität. Aber diese Arbeiten haben nicht die Vergleich der Endnutzer mit Datensätzen und Modellen vorgenommen und Studien zur Positionalität von Datensätzen und Modellen sind immer noch wichtig, da NLP-Aufgaben immer komplexer und sozial orientierter werden und es schwierig ist zu bestimmen, wie diese Positionalitäten abgeprangert werden, da nicht alle Entscheidungen notwendig sind und viele Modelle hinter APIs versteckt sind. Um die Positionalität von Datensätzen und Modellen zu untersuchen, vergleichen wir die Annotierungen mit realen Nutzern mit existierenden Datensätzen und Modellen. Wir tun dies durch unser Werk NLPositionality. Das Werk arbeitet in zwei Hauptschritten. Der erste Schritt besteht darin, Datensätze mit diversen Annotatoren zu re-annotieren. Wir sollten dabei über die Demografie der ursprünglichen Datensatz-Annotatoren schauen, denn normalerweise werden nur wenige Annotatoren pro Instanz angenommen und Demografien werden oft nicht gesammelt und geteilt. Daher warten wir, Datensätze mit vielen Annotatoren zu re-annotieren, um eine reiche Sammlung von Demografie-Daten zu erhalten. Wir nehmen dann die Annotierungen nach Demografie und vergleichen sie mit den Modellen und Datensätzen mithilfe einer Pearson's R-Korrelationsnote. Daher unterscheidet unser Werk sich von der Literatur zum Annotator-Disagreement, indem es Endnutzer mit Modellen und Datensätzen vergleicht,Predictions und Labels, anstatt nur Annotator-Übereinstimmungen oder Modellierungen von Annotatordistributionen zu betrachten. Unser Werk wird hauptsächlich durch Lab in the Wild und Online-Crowdsourcing-Plattformen ermöglicht, für die HCI-Kollega Lab in the Wild. Lab in the Wild ist eine Online-Experimentplattform, bei der wir diverse volontären Recruitierungen machen können. Im Vergleich zu Plattformen wie M Turk, die hauptsächlich Teilnehmer aus den USA und Indien haben, kann Lab in the Wild immer noch hochwertige Daten erhalten. Wir verwalten zwei Aufgaben auf Lab in the Wild, eine davon ist Soziale Akzeptabilität, und das Arbeiten geht so: Teilnehmer lesen eine Situation aus dem Soziale Chemie-Dataset und schreiben dann, wie sozialschön eine Situation ist. Um sich zu engagieren, können sie dann ihre Antworten mit einem AI und anderen vergleichen. Wir vergleichen dann diese Annotierungen mit dem Soziale Chemie-Delphi und GPT 4. Wir wiederholen dann eine ähnliche Aufgabe für die Giftsprache und Hasssprache-Ermittlungsaufgabe, bei der die Teilnehmer einen Fall aus Dynahate liessen und schrieben, ob es ein Fall von Hasssprache war. Wir vergleichen dann diese Annotierungen mit Dynahate, Perspective API, Rewire API, Hate Roberta und GPT 4. Unsere Studie sammelte letztendlich über 16.000 Annotierungen von über 1000 Teilnehmern aus 87 Ländern. Jetzt sind wir in der Lage zu beantworten, wer NLP-Datensätze und Modelle am besten mit den Nutzern allgemein alignieren. Wir finden, dass es Positionalität in NLP gibt. Zum Beispiel finden wir, dass Datensätze und Modelle am besten auf England sprechende Länder aligniert sind. So beim GPT 4 Soziale Akzeptabilitätsanalyse finden wir, dass es am besten auf confusion und England sprechende Länder aligniert ist. Wir finden auch, dass Dynahate am besten auf England sprechende Länder aligniert ist. Wir finden auch weitere Alignements mit Menschen, die ein Collegeabschluss haben. So beim GPT 4 Soziale Akzeptabilitätsanalyse finden wir, dass es am besten auf Menschen mit einem Collegeabschluss oder einem Graduiertenabschluss aligniert ist, und wir finden das gleiche für Dynahate, bei dem es am besten auf Menschen mit einem Collegeabschluss aligniert ist. Allerdings werden wenn Modelle und Datensätze auf bestimmte Bevölkerungen aligniert sind, ein paar werden zwangsweise zurückgelassen. Ein Beispiel davon ist, dass Datensätze und Modelle weniger auf nicht-binary Personen aligniert sind als auf Männer und Frauen. Wir finden dies beim GPT 4 Soziale Akzeptabilitätsanalyse und beim Dynahate Analyse als Beispiel. Also, gegeben ist, dass es Positionalität in NLP gibt, was können wir dagegen tun? Wir haben einige Empfehlungen für euch. Eine davon ist, alle relevanten Designentscheidungen während des Forschungsprozesses zu notieren. Die andere Empfehlung ist, NLP-Forschung mit dem Lesebruch von Perspektivismus zu machen. Unsere dritte Empfehlung ist, spezielle Datensätze und Modelle für vier bestimmte Gemeinden zu bauen. Ein gutes Beispiel hierfür ist die Masakhani-Initiative. Wir wollen betonen, dass inclusive NLP nicht nur die Technologie machen soll, die für alle funktioniert, und das beendet unsere Präsentation. Aber falls ihr mehr erfahren wollt, könnt ihr gerne auf unserem Dashboard nach dem neuesten Analyseergebnissen und unserem Papier suchen. Vielen Dank.</sample>
    <sample id="97">Die Referentin spricht von zwei Problemen der aktuellen SimulST-Modelle: der Notwendigkeit von spezifischen Architekturen und der langen und komplexen Trainingsprozeduren, einschließlich der Training mit verschiedenen Optimierungsobjekten.</sample>
    <sample id="98">Die Reduzierung sozialer und politischer Verzerrungen in Datensätzen beim Training von NLP-Modellen ist schwierig, da es schwierig ist zu bestimmen, was als neutral angesehen werden sollte. Es gibt keine eindeutige Lösung, aber man sollte versuchen, die Datensätze so zu strukturieren, dass sie diverse Perspektiven berücksichtigen und dabei auf eine faire Art und Weise die Verzerrungen reduzieren.</sample>
    <sample id="99">Hallo, ich bin Siyu Yuan aus der Fudan-Universität. Ich werde heute über unser Werk "Distilling Script-Knowledge from Large Language Models for Constrained Language Planning" sprechen. In unserem Alltag planen wir unsere Handlungen oft nach Schritt-für-Schritt-Anweisungen in Form von Ziellinien. Vorherige Arbeiten haben Exploitation von Sprachmodellen verwendet, um abstrakte Ziele von stereotypen Aktivitäten wie "ein Kuchen machen" zu planen und zu zeigen, dass große Sprachmodelle effektiv Ziele in Schritte zerlegen können. Allerdings haben vorherige Arbeiten hauptsächlich auf die Planung abstrakter Ziele von stereotypen Aktivitäten geachtet. Die Planung von Zielen mit spezifischen Einschränkungen, wie "einen Schokoladenkuchen machen", wird noch immer unterschwärzt. In diesem Papier definieren wir das Problem der eingeschränkten Sprachplanung, das verschiedene Einschränkungen auf die Ziele anwendet. Ein abstraktes Ziel kann durch verschiedene real-world spezifische Ziele mit mehrdimensionalen Einschränkungen erben. Ein guter Planer sollte Skripte schreiben, die vernünftig sind und den Einschränkungen treu sind. In diesem Papier evaluieren wir zuerst und verbessern die eingeschränkte Sprachplanfähigkeit von großen Sprachmodellen. Da keine Datensammlung von spezifischen Zielen existiert, um unser Studium zu unterstützen, müssen wir diese Ziele zuerst erhalten. Wie im Tabularobjekt gezeigt wird, erweitern wir abstrakte Ziele mit mehrdimensionalen Einschränkungen für menschlich-basiertes Datenerkennungssysteme unter Verwendung von InstructGPT. Wir sampling 100 spezifische Ziele und evaluieren die generierten Skripte von großen Sprachmodellen. Das Tabularobjekt berichtet über die allgemeine Genauigkeit der Ergebnisse. Wir finden, dass alle Sprachmodelle bei der Planung von spezifischen Zielen unzufriedenstellende Resultate erzielen. Wir führen dann eine detaillierte Analyse durch, um zu untersuchen, warum die Lernermodelle versagen. Die Grafik zeigt, dass die Planungsleistung von InstructGPTs für Ziele unterschiedlicher Kategorien stark variieren kann. Vorherige Studien haben gezeigt, dass die Ausgabequalität von Sprachmodellen in einem hohen Varianz liegt, was zu schlechten Leistungen führt. Daher adhieren wir dem Ansatz "Over-generate-then-filter" um die Generationsqualität zu verbessern. Wir zeigen zuerst die Typen von Einschränkungen mit Beispielen für InstructGPT und erhalten spezifische Ziele basierend auf den Seed-abstrakten Zielen. Dann over-generate K Skripte für spezifische Ziele. Eine Filtermodellentwicklung wird verwendet, um die treue Skripte zu selectieren. Wir konvertieren Skripte und Ziele in InstructGPT-Embeddings und berechnen die Cosinus-Similarität als Similarity-Scores, um die semantische Ähnlichkeit zu messen. Darüber hinaus belohnen wir das Skript, das die Schlüsselwörter der Zielfunktion enthält. Wir behalten nur das Skript, wenn die Zielfunktion die höchste Punktzahl im Zielfunktionssatz hat. Mit unserem Ansatz können wir InstructGPT Skripte von höherer Qualität generieren. Unsere Methode verbessert die Planfähigkeit sowohl in der semantischen Vollständigkeit als auch in der Treue zur Einschränkung. Da große Sprachmodelle teurer zu deployen sind, ist es wichtig, die Sprachplanfähigkeit von kleineren und spezialisierten Modellen zu enabling. Die Erstellung von Datensätzen ist ein wichtiger Schritt dazu. Allerdings haben vorherige Studien das enabling von spezifischen Zielen nicht ermöglicht und die manuelle Datenermittlung ist teuer. Daher folgen wir dem Ansatz der symbolischen Kenntnisdistillation, um die eingeschränkte Sprachplanfähigkeit von großen Sprachmodellen zu distillieren. Wir verwenden unser Ansatz, um einen Datensatz der eingeschränchten Sprachplanfähigkeit zu erstellen, namens CoScript. Insgesamt generieren wir 55.000 spezifische Ziele mit Skripten. Um die Qualität der Validier- und Testdatenbank zu gewährleisten, bitten wir Crowdsourced-Worker, falsche Samples zu finden und zu korrigieren. Das Grafikobjekt zeigt die Eingeschränkungsentfernung von CoScript. Wir finden, dass CoScript eine hohe Pluralismus im generierten spezifischen Zielen zeigt. Mit CoScript können wir Versuche mit kleineren aber spezialisierten Modellen für die eingeschränkte Sprachplanfähigkeit vornehmen. Wir finden, dass T5, der auf CoScript fine-tuned wurde, Skripte von höherer Qualität als die meisten großen Sprachmodelle generieren kann, was zeigt, dass kleinere Modelle bei richtiger Training auf geeigneten Datensätzen über größere Modelle übertragen können. In zusammenfassender Weise etablieren wir das Problem der eingeschränkten Sprachplanfähigkeit. Wir evaluieren die eingeschränkte Sprachplanfähigkeit von großen Sprachmodellen und entwickeln einen Ansatz "Over-generate-then-filter" für große Sprachmodelle. Wir verwenden große Sprachmodelle, um einen hohen-Qualitäts-Skript-Datensatz, CoScript, für die eingeschränkte Sprachplanfähigkeit zu generieren. Wir hoffen, dass der CoScript-Datensatz eine wertvolle Ressource zur Fortschrittsarbeit im Bereich der Sprachplanfähigkeit sein kann. Vielen Dank für Ihre Zeit. Weitere Details von CoScript finden Sie in unserem Papier.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, each corresponding to a document in the corpus. Traditional multi-hop retrievers rely on supervised training with thousands of examples, which can be expensive for low-resource domains or those requiring special expertise. Our approach, PromptRank, is data-efficient and uses an unsupervised retrieval method combined with a few-shot language model-based reranker. We retrieve candidate chains using TF-IDF and hyperlink traversal, then rerank them using a few-shot language model. The likelihood of the question given the chain prompt serves as the scoring function. We experiment with GPT2-XL and T5-XL and evaluate our approach on HotpotQA. PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop dense retrievers.</sample>
    <sample id="101">Die Sprachgewandtheit von PaLM ist vergleichbar mit jenen von state-of-the-art-Systemen, aber es gibt noch einige Probleme mit der Genauigkeit.</sample>
    <sample id="102">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: 1) Anwendbarkeit auf embedding as services, 2) Keine Degradierung der Nutzen von bereitgestellten embeddings, 3) Covertigkeit gegenüber Angreifern oder Einfachkeit beim Entfernen des Wasserzeichens, und 4) Übertragbarkeit des Wasserzeichens auf die Angreiferdienstleistungen während des Modell-Extraktionsprozesses.</sample>
    <sample id="103">Die englischen TED Talks wurden insgesamt in 14 verschiedenen Sprachen übersetzt.</sample>
    <sample id="104">Das Paper erwähnt, dass 16.000 Annotierungen von über 1000 Anarbeitern aus 87 Ländern für die Studie am Ende erreicht wurden.</sample>
    <sample id="105">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind Kosinus- und L2-Similarität.</sample>
    <sample id="106">Abstract: 
The paper introduces QUEST, a retrieval dataset containing over 3000 entity-seeking queries with implicit set constraints. The dataset includes verified answer entities and marked attributable spans for different query constraints. The authors demonstrate the challenge of retrieving multi-answer sets from a large document corpus where evidence for relevance can come from multiple parts of the document. They evaluate systems using sparse and dense retrievers, as well as a T5-based reranker, showing that there is room for improvement in retriever performance. Queries with set intersection and set difference are particularly challenging, with the lowest F1 scores. The dataset aims to help future researchers build improved systems for handling selective information needs.</sample>
    <sample id="107">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt, um die Leistung von Encoder-Decoder Modellen zu verbessern. Es wurde festgestellt, dass die Leistung der Encoder-Decoder Modelle durch die Schulung an einer Mischung verschiedener Sprachen verbessert wurde.</sample>
    <sample id="108">This paper revisits the minimal pair paradigm for evaluating language models on acceptability judgments. The authors argue that current MPP pipelines do not allow for evaluation of a model's acceptability towards longer sentences, which is crucial given the increasing context windows in large language models. To address this, they recreate longer sequences by choosing acceptable or unacceptable sentences from relevant data sets and test the models' acceptability judgments. They find that the MPP judgments are mostly robust for arbitrary context length when using Wikipedia sentences, but significantly affected when using sentences from the same data set or unrelated domains. This suggests that language models are sensitive to latent syntactic and semantic features shared across sentences, and that current MPP evaluations may not fully capture the abstract knowledge of language models throughout the context window.</sample>
    <sample id="109">Abstract: Unnatural Instructions is a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset contains 64,000 examples, with an additional 240,000 examples considering instruction paraphrases. The generated examples are diverse in tasks, content, and phrasing. The dataset was used to fine-tune an 11 billion-parameter T5 model, which outperformed both T0++ and Tk-instruct across several benchmarks. When the cost of generating examples is amortized, training on Unnatural Instructions outperforms the baseline on all benchmarks. Unnatural Instructions highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers who usually collapse into predictable heuristics and form annotation artifacts. At the same time, language models are faster and cheaper than human annotations.</sample>
    <sample id="111">Die Autoren bestimmen die Wörter mit mittlerer Häufigkeit, indem sie eine allgemeine Textcorpussammlung sammeln und die Häufigkeit jedes Wortes darin zählen.</sample>
    <sample id="112">Hallo alle, ich bin Shuheng. Heute werde ich über unser Papier "Do CoNLL-2003 named entity taggers still work well in 2023?" sprechen. Lassen Sie uns beginnen. Unser Papier untersucht das Problem der allgemeinheit im Hinblick auf die Aufgabenerkennung (Named Entity Recognition Task, NER). Wir beobachten, dass Modelle seit fast 20 Jahren in CoNLL-2003 verwendet wurden, um NER zu entwickeln, und dies natürlicherweise einige Probleme aufwirft. Zunächst einmal, können diese Modelle auf modernes Datenmaterial übertragen? Und wenn wir neue Tagger entwickeln, was ist für eine gute allgemeinheit notwendig? Gleichzeitig, wenn wir jedoch einen schlechten allgemeinheitsausdruck beobachtet haben, was verursacht die Leistungsinkraftsen von diesen Modellen? Um diese Probleme zu untersuchen, haben wir den CoNLL++-Dataset entworfen. Dies ist eine Datensammlung, die wir aus Reuters News von 2020 entnommen und mit den gleichen CoNLL-2003-Annotieranweisungen annotiert haben. Wir haben dann über 20 Modelle an CoNLL-2003 fine-tuned und sie auf sowohl die CoNLL-03-Testsets als auch die CoNLL++ evaluierten. Und letzt but nicht zuletzt haben wir den prozentualen Anstieg von F1 berechnet, um die allgemeinheit jedes Modells zu bewerten. Was ist also für eine gute allgemeinheit erforderlich? Durch unsere Experimente haben wir festgestellt, dass es drei Hauptingredients gibt, die für eine gute allgemeinheit erforderlich sind. Das erste ist die Modellarchitektur. Durch unsere Experimente haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten übertragen können. Das zweite ist die Modellgröße. Wir haben festgestellt, dass normalerweise größere Modelle zu besseren allgemeinheiten führen. Und letzt but nicht zuletzt wissen wir alle, dass die Anzahl der Fine-Tuning-Beispiele direkt auf die Leistung einer downstream-Aufgabe auswirkt. Hier haben wir auch festgestellt, dass mehr Fine-Tuning-Beispiele tatsächlich zu besseren allgemeinheiten führen. To our next question, what causes the performance drop of some models? We had two hypotheses. The first one is adaptive overfitting, which is overfitting costs by reusing the same test set over and over again and this is usually manifested as the diminishing returns on a new test set. The second hypothesis is temporal drift, which is the performance degradation that is caused by the increasing temporal gap between the train and the test data. For data overfitting, we saw that from the graph on the right, the red best fit line has a gradient that is greater than one. This means that every unit of improvement that we made on CoNLL-2003 translates to more than one unit improvement on CoNLL++, which means that there is no diminishing returns. And this shows us that adaptive overfitting in this case is not observed. So what about temporal drift then? For temporal drift, we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift. Our conclusion is that for good generalization we would need a better model architecture, larger model size, as well as more fine tuning examples. And these goes hand in hand, we can't just have one ingredient but throw out the others. At the same time, we also found that the performance drop here is caused by temporal drift and kind of surprisingly, it is not caused by adaptive overfitting even though CoNLL-2003 has been used for over 20 years. So going back to the question that we raised in the title of our paper "Do CoNLL-2003 taggers still work in 2023?" And we found that the answer is actually a resounding yes. We hope our paper calls for more research on how to improve generalizations of the models. And lastly, please make sure to check out our paper, our data set and if you have any questions, feel free to contact me. Thank you so much.</sample>
    <sample id="114">This paper introduces a method for compressing large language models by pruning redundant multi-head attention heads. The authors propose a grouped head attention approach that uses a divide-and-conquer strategy to compress the model while maintaining performance. They evaluate their method on three tasks: machine translation, language modeling, and abstractive summarization, achieving significant improvements in performance and compression. The authors also conduct efficiency analysis, showing that their LITE model achieves 90% of pruned parameters, 62% faster inference speed, and 80% FLOPs against the model which yields the same performance on the same data set.</sample>
    <sample id="115">Der Ansatz verwendet Lambda Sprachsegmente.</sample>
    <sample id="116">Im Beispiel mit Servin und Kea wird das entitätsspezifische Wissen verwendet, dass Servin ein Richter ist und Kea ein Bäcker.</sample>
    <sample id="117">Der wichtige Faktor zwischen der Qualität des Beispiels und der Ähnlichkeit mit dem Ausgangssatz ist die Qualität des Beispiels. Es ist wichtig, die Beispiele von hochwertigen Übersetzungen auszuwählen, um eine bessere Leistung zu erzielen.</sample>
    <sample id="118">This paper presents a new pre-training technique for code-switched natural language processing (NLP) tasks. The authors propose SwitchMLM, a novel masked language modeling (MLM) objective that is specifically designed to handle code-switching. They also introduce FrequencyMLM, a surrogate method that does not require access to language identification (LID) tags. Additionally, the authors propose architectural modifications, including residual connections and an auxiliary LID-based loss, to further enhance the switch-point information content in the final representation. The results show that the proposed methods outperform standard MLM on sentiment analysis tasks for all language pairs. Probing experiments verify the claim that the proposed methods increase the amount of switch-point information in intermediate layers.</sample>
    <sample id="119">Die Arbeiten konzentrieren sich auf Sprachmodelle mit unterschiedlichen politischen Neigungen, um die Auswirkungen von politischem Bias in der Ausbildung auf die Leistung und Fairness von Sprachmodellen zu untersuchen.</sample>
    <sample id="120">Das Modell verwendet Aufmerksamkeitswerte nur aus einer bestimmten Ebene. Es verwendet die Summe der Aufmerksamkeitswerte für jedes Wort, um zu bestimmen, ob es emittiert werden soll oder nicht.</sample>
    <sample id="121">Beispiele für direkte Inferenz sind die Angabe des Songs "Easy on Me" oder die Angabe der Position, "die erste".</sample>
    <sample id="122">Die Autoren, Siyu Yuan und ihre Kollegen, gehören an der Fudan University.</sample>
    <sample id="123">Abstract: We present MultiInstruct, a multi-modal instruction tuning benchmark dataset consisting of 62 diverse tasks covering 10 broad categories. Our method, instruction tuning, enables large language models to perform on unseen multi-modal tasks in a zero-shot manner by following natural instructions. We investigate the effectiveness of instruction tuning on a multi-modal pre-trained model, OFA, and show that it can significantly improve performance on seen tasks and transfer learning from natural instruction datasets. We also introduce a new evaluation metric called sensitivity to measure the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction. Our results demonstrate that instruction tuning can improve the short capability of OFA and explore different transfer learning techniques.</sample>
    <sample id="124">This presentation introduces the work of Tan Qingyu and colleagues from the National University of Singapore and Alibaba, titled "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models." The researchers address the importance of time in real-world applications by breaking down temporal reasoning into three levels: time-to-time, time-to-event, and event-to-event. They propose the TempReason dataset to comprehensively evaluate these types of reasoning across long temporal periods. The study evaluates various language models (LMs) on tasks such as year and month prediction, using different QA settings including Closed Book, Open Book, and Reasoning QA. To enhance temporal reasoning capabilities, they introduce a training strategy involving Temporal span extraction pre-training and time-sensitive reinforcement learning, resulting in the TempT5 model. Experimental results show that TempT5 significantly outperforms other models, particularly in Open Book and Reasoning QA settings, highlighting the importance of comprehensive temporal reasoning benchmarks and training strategies for LMs.</sample>
    <sample id="125">Es wird in der Präsentation nicht erwähnt, wie viele Autoren an der Arbeit beteiligt sind.</sample>
    <sample id="126">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells vor dem semantischen Parsing wurde als Baseline betrachtet.</sample>
    <sample id="127">Title: Large Language Models as Reasoning Teachers for Smaller Models

Authors: Namgyu Ho, Laura Schmid, Se-Young Yun

Abstract: We propose a novel method to transfer reasoning abilities from large language models (LLMs) to smaller models using chain-of-thought prompting. Our approach, called fine-tuned CoT, generates diverse reasoning samples by applying stochastic temperature sampling to the teacher model's predictions. This method significantly improves performance on complex tasks, with up to 55% improvement in Multi Arithmetic. Our method is scalable and can be applied to smaller models with fewer than 1 billion parameters, making it accessible and effective for various emergent abilities. The paper provides detailed analysis, results on open-source models, and code/data for future work.</sample>
    <sample id="128">Abstract: This work presents the KITMUS Test, a diagnostic suite for evaluating knowledge integration in natural language understanding models. The test probes the ability of models to draw on both pretraining and inference-time knowledge by using a coreference resolution task. The task varies the availability of entity-specific and background knowledge across three settings: Background-Pretrain, Background-Both, and Background-Inference. The results show that without task-specific training, most models rely on surface cues, which are not useful when testing on KITMUS. However, with task-specific training, some models can successfully integrate knowledge from multiple sources. Despite this, even the best-performing models struggle to reliably integrate backward knowledge provided only at inference time.</sample>
    <sample id="129">Die Autoren haben beispielsweise eine "Asiatische Frau" als markierte Gruppe gegeben.</sample>
    <sample id="130">Die Studie zeigt, dass Modellarchitekturen, die nicht gut generalisieren, normalerweise nichtTransformer-Modelle sind.</sample>
    <sample id="131">Die Testdatensätze werden als "clean" bezeichnet.</sample>
    <sample id="132">Es sind insgesamt 3 Autoren an der Arbeit beteiligt: Akshatha, Martin und die Kolleginnen/Kollegen von McGill University, Mila und Microsoft Research.</sample>
    <sample id="133">Die Autoren arbeiten mit mehreren Modalitäten, nicht nur mit Text.</sample>
    <sample id="135">ABC-Eval: A Dimensional Approach to Evaluating Conversational AI

Conversational AI models have become increasingly sophisticated, but evaluating their performance remains a challenge. This work introduces ABC-Eval, a new dimensional approach to evaluate conversational AI by explicitly annotating whether each model response exhibits certain behaviors such as relevance, contradiction, hallucination, and empathy. Developed by the Emory NLP Lab led by Professor Jinho Choi and in collaboration with Amazon Alexa AI, ABC-Eval measures thematic errors and provides more reliable and informative metrics compared to existing methods like Likert ratings and pairwise comparisons. The study evaluates four state-of-the-art chat models on 100 human-bot conversations using ABC-Eval and compares them with three existing methods. Results show that ABC-Eval is more predictive of overall conversation quality and captures unique aspects of chat quality better than existing metrics. Despite challenges, ABC-Eval offers a higher resolution evaluation method for comparing conversational AI models.</sample>
    <sample id="136">Abstract: This work introduces FERMAT, a flexible evaluation set for numerical reasoning that assesses models' strengths and shortcomings in arithmetic types, number understanding, mathematical operations, and training dependency. The study evaluates the performance of various language models on FERMAT, comparing zero-shot evaluations to fine-tuning with math teachers' templates. Results show that existing benchmarks are unrepresentative, and FERMAT provides a more informative alternative. The study also highlights the importance of language and mathematical diversity, as well as areas for improvement in number encoding and tokenization.</sample>
    <sample id="137">Abstract: We introduce Tell2Design, a large-scale dataset for language-guided floor plan generation. The dataset consists of 5,051 human-annotated and 76,000 artificially generated language instructions paired with corresponding floor plans. Our goal is to generate reasonable 2D floor plan designs from natural language instructions that describe the semantics, geometry, and topology of the desired layout. To address the challenges of strict design constraints, understanding the big picture of the entire floor plan, and dealing with ambiguous or incomplete information in human instructions, we propose a sequence-to-sequence model based on the encoder-decoder framework. Our method outperforms text-conditional image generation baselines by a large margin, achieving a Micro IoU of 54 and a Macro IoU of 53. We also demonstrate that our method can be improved when trained on both artificial and human instructions, suggesting a mutual benefit during training.</sample>
    <sample id="138">Nach Ansicht der Autoren ist ein zu wenig erforschtes Gebiet im Bereich der NLU die Fähigkeit von Modellen, Wissen aus verschiedenen Quellen zu integrieren und zu verwenden.</sample>
    <sample id="139">Die Referenten heißen Ying und Zhiyang.</sample>
    <sample id="140">Ja, Coscript wurde eine Qualitätskontrolle durchlaufen, indem crowd-sourcing Arbeiter verwendet wurden, um falsche Beispiele zu finden und zu korrigieren.</sample>
    <sample id="141">Bestehende Ressourcen zur kontextbasierten Übersetzung unterstützen nur begrenzte Arten von kontextabhängigen Übersetzungen und begrenzte Sprachensemantic, da sie normalerweise auf Domänenwissen und menschlicher Kuration basieren.</sample>
    <sample id="142">Hallo! Ich werde über unser Werk "Resolving Indirect Referring Expressions for Entity Selection" sprechen, in dem wir die AltEntities Corpus-Datei Introducing präsentieren. Mein Name ist Javad Hosseini und das ist ein gemeinsames Projekt mit Filip Radlinski, Silvia Pareti und Annie Louis. Unsere Zielgruppe sind Benutzer, die dabei helfen wollen, Entscheidungen zu treffen. Betrachten Sie beispielsweise folgende alternative Frage: "Meintest du 'Easy on Me' oder 'I Gotta Feeling'?" Hier will der Benutzer zwischen zwei Liedern auswählen. Der einfachste Weg wäre, den Namen des Liedes "Easy on Me" oder seine Position, "das erste", zu verwenden. Aber manchmal ist eine indirekte Bezugnahme besser, um eine natürlichere Unterhaltung zu haben. Das könnte passieren, wenn der Benutzer den Namen des Liedes nicht mehr weiß. Oder wenn die Aussprachen zu ähnlich sind, um sie zu unterscheiden. Oder wenn der Benutzer eine bestimmte Vorliebe angeben möchte. Hier sind einige Beispiele für indirekte Beziehungen: "das neuere Lied" oder "das Lied, das nicht energiegeladen ist". Dies ist ein wichtiger Problemfall in conversational Systems und auch für die Benchmarking von LLMs' Entity-verstehensfähigkeit. Wir kennen keine größeren öffentlichen Datensätze für die Aufgabe, also sammeln wir einen anhand von Crowd Annotationen. Unsere Datensatz umfasst drei verschiedene Domänen: Musik, Bücher und Rezepte. Unsere Datensatzsammlungsoption betont informell durch eine Cartoon-Completion-Ansatz. Die Cartoon hat drei Sprachblister. In dem ersten Sprachblatt sagt Bob: "Erinnere dich an das Lied, das wir gestern hörten." Damit sets Bob den Dialogkontext. In dem zweiten Sprachblatt sagt Alice: "Meinst du 'Easy on Me' oder 'I Gotta Feeling'?" Welches ist die alternative Frage? Und in dem dritten Sprachblatt verwendet Bob eine indirekte Bezugnahme, um eines dieser Objekte zu selecting, z.B. "das neuere Lied". Wir geben den ersten und zweiten Sprachblatt automatisch an, aber das dritte wird vom Annotator gefüllt. Der erste Sprachblatt wird aus ein paar manuellen Prompts pro Domäne gewählt. Der zweite Sprachblatt, die alternative Frage, wird wie Folgendes generiert: "Do you mean A or B?" Where A and B are Samples from Wikipedia. Hier sind die verschiedenen Sampling-Methoden, die wir verwendet haben. Wenn wir uns weiter hoch in die Liste bewegen, werden die Objekte ähnlicher und es wird normalerweise schwieriger, die Disambiguierung vorzunehmen. Der erste ist eine gleichmäßige Zufallsauswahl. Der zweite ist, wenn die Objekte ähnliche Titel haben, zum Beispiel zweibücher mit dem Namen "The Return". Der dritte ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben. Und schließlich, wenn sie ähnliche Info-Boxes oder Attribute auf Wikipedia haben. Beispielsweise denselben Kategoriengenres oder denselben Künstler für ein Lied. Wenn wir diesem alternative Lied den Annotatoren zeigen, kennen sie den Namen dieser Objekte, aber sie kennen sie nicht unbedingt. Was wir tun, ist, den Hintergrundknowledge der beiden Objekte zu zeigen. Bei Liedern simply显示Google-Suchergebnisse für jedes Lied und dann fragen wir die Annotatoren, ob sie sich Zeit nehmen, ein paar Lieder zu hören und über sie zu lesen. Hier ist zum Beispiel das Google-Suchergebnis für das Lied "Easy on Me". Bei den Lektoren und Rezepten-Domänen zeigen wir ein paar Hintergrundtexte aus Wikipedia an. Bei Rezepten zeigen wir auch die Bilder von Wikipedia an, damit die Annotatoren wissen, wie sie aussehen. Dann bitten wir die Annotatoren, eines dieser Objekte zu auswählen, zum Beispiel hier ist der erste, und beschreiben sie mit drei bis fünf indirekten Bezugnahmen. Zum Beispiel: "das Lied mit dem Pianomusik", "nicht das Lied mit dem 12-jährigen Jungen", "das fiktive Lied", "kommt aus Aserbaidschan" und so weiter. Das AltEntities Corpus hat 6.000 alternative Fragen über drei Domänen und 42.000 indirekte Bezugnahmen. Die Resultate mit dem T5 XL Modell sind als Folgendes zusammengefassen: Wenn das Sprachmodell Zugang zu genauso viel Hintergrundknowledge hat wie die Annotatoren, dann ist die Genauigkeit wirklich hoch, sie liegt bei 92-95%. Aber das ist nicht realistisch. Wenn das Sprachmodell Zugang zu teilweise überlapzendem Hintergrundknowledge hat, dann liegt die Genauigkeit zwischen 82-87%, was realistischer ist. Zum Beispiel wenn das Sprachmodell das Hintergrundknowledge abrufen kann. Wenn das Sprachmodell Zugang nur zu den Namen der Objekte hat, dann liegt die Genauigkeit nur bei 60%, also gibt es noch viel Raum zur Verbesserung. Wir haben auch gezeigt, dass die Modelle allgemein übertragbar sind. Hier ist ein Link zu unserem Datensatz. Danke.</sample>
    <sample id="143">Der Ansatz wird mit den bekannten Strategien Wait-k und Local Agreement, sowie mit dem aktuellen Standardmodell für gleichzeitige Voraussagen verglichen.</sample>
    <sample id="144">Die Autoren der Studie "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical Domains" sind Yanis Labrak und Florian Dupont. Sie sind an der Université de Nantes (Universität von Nantes) in Frankreich tätig.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">This paper presents a study on the omission problem in dialogue summarization. The authors analyze the omission rate of summaries from five domains and six pre-trained models, finding that about 70% of generated summaries have the omission problem. They also propose the OLDS dataset, which provides high-quality omission labels for dialogue summarization. To build a foundation for the omission detection task, they explore three frameworks as baselines, which have different input formats and structures, including pair-wise classification, sequence labeling, and pointer network. They use the Precision, Recall, and F1-score to evaluate their omission detection models. Furthermore, they calculate the percentage of gold emission words that are hit in the detected utterances to measure the word-level omission recall, denoted as WR score. The results show that the task is very challenging, which calls for more advanced detection models. Finally, they use a post-editing method for summary refinement, which largely boosts the performance when the omission is provided. This indicates that the omission detection is a valuable task, and the refinement based on the detected omission is a promising direction for quality improvement in dialogue summarization.</sample>
    <sample id="147">Es sind drei Autoren an der Arbeit beteiligt: Myra, Esin Durmus und Dan Jurafsky.</sample>
    <sample id="148">Hallo, ich bin Sara Papi aus der Universität Trento und der Fondazione Bruno Kessler. Ich werde kurz die Publikation "Attention as a Guide for Simultaneous Speech Translation" vorstellen, die eine gemeinsamearbeit mit Matteo Negri und Marco Turchi ist. Was ist das gleichbedarfliche Sprachübersetzung? Das gleichbedarfliche Sprachübersetzung (SimulST) ist der Prozess, bei dem gesprochene Sprache in Textform in einer anderen Sprache in Echtzeit übersetzt wird, um interlinguistische Kommunikation zu ermöglichen. Und welche Probleme haben aktuelle SimulST-Modelle? Spezielle Architekturen werden normalerweise trainiert, indem man zusätzliche Module optimieren muss. Langwierige Trainingverfahren, z.B. Training mit verschiedenen Optimierungsobjektiven. Und das Training und das Aufreten mehrerer Modelle, um unterschiedliche Latenzregimes zu erreichen. Zum Beispiel ein Modell mit einem Durchschnittslag von einem Sekunden und ein anderes Modell mit zwei Sekunden Latenz usw. So was ist unsere Lösung? Zunächst einmal benutzen wir schon existierende offline-ST-Modelle, ohne sie zu retrainieren oder spezielle Architekturen für SimulST zu verwenden. Wir verwenden nur ein Modell für jede Latenzstelle und handeln Latenz durch bestimmte Parameter ab. Und wir nutzen die Kenntnisse, die das Modell durch die Ablage机制 zwischen Audio-Eingabe und Textausgabe gewonnen hat. Das ist der Ablage机制, und du kannst ein Beispiel auf der rechten Seite sehen. Unsere Lösung ist es, EDAtt, also Encoder-Decoder Ablage, zu vorschlagen, und es ist eine Strategie, bei der wir entscheiden, ob wir einen teilenden Übersetzungsbetrag ausstoßen oder nicht, basierend auf, wo die Ablage aufgerichtet ist. Ein Wort wird ausgestoßen, wenn die Ablage nicht konzentriert ist, d.h., wenn die Summe der Ablage im unteren lambda-Speech-Fragment unter einem bestimmten Schwellenwert alpha liegt, was bedeutet, dass die empfangene Information stabil genug ist. Zum Beispiel erhalten wir einen Sprachblock mit "Ich gehe heute über..." und unser Modell prognostiziert die Übersetzung ins Deutsche. Wenn wir die Ablagegewichter ansehen, sehen wir, dass die ersten zwei Wörter auf den earliest empfangenen Sprachsegmenten ablenken, während das letzte Wort auf die letzten lambda-Speech-Fragmente ablenkt. Das bedeutet, dass die ersten zwei Wörter ausgestoßen werden, da die Summe der Ablage über einem bestimmten Schwellenwert alpha liegt und wir das letzte Wort warten müssen, bis wir ein anderes Sprachsegment erhalten. Wenn wir fortfahren und einen weiteren Sprachblock erhalten und unser Modell weitere drei Wörter prognostiziert, dann sehen wir, dass keines der Wörter auf die letzten lambda-Speech-Fragmente ablenkt. Das bedeutet, dass diese drei Wörter ausgestoßen werden. Wenn wir die Hauptergebnisse von EDAtt plotten, erhalten wir Graphen, in denen wir BLEU auf einer Seite messen, der die Übersetzungsqualität misst, und den durchschnittlichen Lagg, der die Latenzmessung ist, und wir berücksichtigen auch die bewusste durchschnittliche Latenz, die die Rechenzeit des Modells zur Prognose des Ausgangs berücksichtigt. Wir wollen, dass unsere Kurven so hoch wie möglich auf diesem Plot sind. Aber wir wollen auch, dass sie links verschiebt sind. Und wir vergleichen unsere Ergebnisse mit populären Strategien, die auch auf offline-Modellen angewandt werden, das Wait-k-Strategie und die Lokale Übereinstimmung. Und wir vergleichen auch mit dem aktuellen Meistermodell speziell für gleichbedarfliche Vorausschwünge. Hier sind die Ergebnisse der gleichbedarflichen Sprachübersetzung-Strategie auf Deutsch. Und wir sehen, dass sie alle Strategien überwiegt, die auf offline-Modellen angewandt werden, da die Kurven links verschiebt sind. Und wir sehen auch, dass wenn wir die tatsächliche verstrichene Zeit oder die bewusste durchschnittliche Latenz berücksichtigen, das EDAtt-Modell die schnellste Strategie ist. Wenn ihr mehr Ergebnisse entdecken möchten, lest ihr unsere Publikation. Und wir haben auch das Open Source-Code und die Modelle und gleichbedarfliche Ausgabe freigegeben, um die Wiedervereinzelbarkeit unseres Werks zu erleichtern. Vielen Dank für eure Aufmerksamkeit.</sample>
    <sample id="149">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="150">MeetingQA: Extractive Question-Answering on Meeting Transcripts

Archiki et al. introduce MeetingQA, a new dataset for extractive question answering based on meeting transcripts. Unlike previous works focusing solely on summarization and action items, MeetingQA addresses the significant QA component in meeting discussions by collecting questions asked by participants and their corresponding answer sentences. The dataset contains 7.7K questions from the AMI corpus, with 30% unanswerable and 40% having multi-span answers.

The authors propose various methods, including context-retrieval, single-span models, and multi-span variants. They observe that short-context models outperform long-context models, and multi-span models have comparable or slightly less performance than single-span models. Zero-shot performance shows a substantial gap from human performance, but silver data augmentation improves results. Error analysis highlights challenges in identifying rhetorical questions and determining which speaker answered a question, especially in zero-shot settings.</sample>
    <sample id="151">Hallo alle, ich heiße Ying und mein Kollege Zhiyang werden heute über unser Forschungsprojekt "MultiInstruct" präsentieren, das die Verbesserung von Multi-Modal Zero-Shot Learning durch die Optimierung der Anpassung an Anweisungen untersucht. Mit Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu erforschen, um auf verschiedenen downstream Aufgaben in einer parameter- und dateneffizienten Weise zu arbeiten. Kürzlich haben viele Studien gezeigt, dass die Anpassung an Anweisungen große Sprachmodelle erlaubt, auf unbekannte Aufgaben in einem Null-Shot-Modus zu arbeiten, indem sie natürliche Anweisungen folgen. Allerdings haben die meisten vorherigen Arbeiten, die sich auf die Anpassung an Anweisungen konzentriert haben, hauptsächlich auf die Verbesserung der Null-Shot-Performane auf Sprach-only Aufgaben gearichtet, während Computer Vision- und multi-modal Aufgaben vernachlässigt wurden. Daher wollen wir in diesem Werk untersuchen, ob die Anpassung an Anweisungen an einem multi-modal vorausgesetzten Modell tatsächlich die allgemeine Anpassung an unbekannte multi-modal Aufgaben verbessern kann. Zudem haben wir bei unserem Forschungsbeginn festgestellt, dass es einen erheblichen Unterschied im Verfügbaren von Anweisungsdatensätzen zwischen NLP und multi-modal gibt. Es existieren mehr als 1600 Sprach-only Anweisungs Aufgaben. Allerdings gibt es keinen großen öffentlich verfügbaren multi-modal Anweisungs Aufgabensatz. Daher motiviert uns, einen multi-modal Anweisungs Aufgabensatz zu erstellen. Hier präsentieren wir MultiInstruct, den ersten multi-modal Anweisungs Aufgabensatz, der 62 diverse multi-modal Aufgaben über 10 breite Kategorien enthält. Diese Aufgaben werden aus 21 bestehenden offenen Quellen entnommen und jede Aufgabe ist mit 5 Experten geschriebenen Anweisungen ausgestattet. Um die multi-modal Anweisungs Anpassung auf unserem vorgeschlagenen Datensatz zu untersuchen, nehmen wir OFA, ein vereinbares multi-modal vorausgesetztes Modell, als Basismodell an. OFA verwendet eine einheitliche Vokabular für Sprache, Bildtoken und Koordinaten einerBounding Box. Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstruct Datensatz, um die Einheitliche Verarbeitung verschiedener Eingabe- und Ausgabedatentypen zu vereinbaren. Wir folgen dem Verfahren von OFA und formulieren alle Aufgaben in einem vereinbarten sequenz-to-sequence Format. In dem die Eingabe-Text, Bilder, Anweisungen undBounding Box in derselben Tokenraum dargestellt werden. Okay, nun werde ich über die multi-modal Anweisungs Anpassung sprechen. So für die Trainingsaufsplitte verwenden wir 53 Aufgaben aus 9 Gruppen für die Training und wir sampling 10.000 Instanzen pro Aufgabe. Für die Testaufsplitte reservieren wir die gesamte Gruppe des allgemeinen Menschenverstehens zur Testphase und wir select additional 5 Aufgaben aus VQ undMiscellaneous Gruppe. Wir verwenden alle Instanzen in der Testsplit für jede Aufgabe. In addition, wir zufällig 20 Aufgaben aus der Testsplit von natürlichen Anweisungen als unbekannte Aufgabe für NLP auswählen. Wir verwenden dabei einen vorher trainierten OFA großen Modell als Basismodell. During training, wir mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, wir conduct a total of 5 experiments by evaluating the model using one of the five instructions. In each experiment, wir berichten die Min und Max Performance und die Standardabweichung der Performance über alle 5 experiments. Wenn die Aufgabe ein multi-modal Klassifizierungs Aufgabe ist, wir berichten die Genauigkeit. Wenn es ein multi-modal Generations Aufgabe ist, wir berichten Rouge-L. For NLP Aufgabe, wir berichten Rouge-L auch. Wir Introduzieren auch einen zusätzlichen Evaluierungsmaßstab namens Sensitivität. So dies messt die Modell's Fähigkeit, die gleichen Ausgaben für dieselbe Aufgabe zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung. Hier ist unser Hauptergebnis. Wie wir sehen können, die Anpassung an Anweisungen kann signifikant die Performance von OFA auf gesehene multi-modal Aufgaben verbessern. Auch die Transferlearning von natürlichen Anweisungs Aufgabensätzen kann die Anpassung an Anweisungen verbessern. Hier können wir sehen, dass mit der Erhöhung der Aufgabenzahl die Modell's Performance besser wird und in der meantime, die Sensitivität sinkt. So wir haben auch ein Experiment durchgeführt. Wir verwenden eine Anweisung versus 5 Anweisungen. Wie wir sehen können, die Verwendung mehrerer Anweisungen die Modell's Overall Performance verbessert und die Sensitivität deutlich reduziert. So dies zeigt den Effekt verschiedener Fine-Tuning Strategien auf die Modell Sensitivität. Wie wir sehen können durch Transferlearning von natürlichen Anweisungs Aufgabensätzen, das Modell kann viel bessere Sensitivität erreichen als das ursprüngliche OFA Modell. Wir können auch sehen, dass Transferlearning von natürlichen Anweisungs Aufgabensätzen das Modell dazu bringen kann, viel bessere Performance auf dem natürlichen Anweisungs Aufgabensatz zu erreichen. So overall, wir schaffen den ersten großen multi-modal Anweisungs Aufgabensatz mit erheblich verbesserte kurzfristige Kapazitäten von OFA und wir untersuchen verschiedene Transferlearning Techniken und zeigen ihre Vorteile. Wir designen auch einen neuen Maßstab namens Sensitivität. So noch etwas, wir sammeln einen viel größeren multi-modal Anweisungs Aufgabensatz mit around 150 zusätzlichen visio-languag Aufgaben und wir werden sie freigeben. So hier ist ein QR-Code für unser Datensatz und Modell. Danke.</sample>
    <sample id="152">This presentation introduces new language models for classical philology, specifically Ancient Greek and Latin. The speaker discusses the current landscape of language models in classics, highlighting the recent development of monolingual BERT models for Ancient Greek and Latin. However, there is a lack of multilingual models pre-trained on Ancient Greek texts, which hinders evaluation and understanding of their capabilities. The speaker's project aims to address this by creating monolingual and multilingual models using different architectures. They pre-trained models on high-quality datasets and benchmarked them against state-of-the-art models. Results show that their models outperform previous models, with significant improvements in lemmatization performance. The presentation also explores the behavior of T5's encoder and the implications of multilinguality in language models.</sample>
    <sample id="153">Title: Resolving Ambiguities in Text-to-Image Generative Models

Abstract: This work addresses the challenge of resolving ambiguities in text-to-image models, which can lead to faithful image generation. We propose a framework that curates a benchmark dataset with various types of ambiguities and uses language models to generate clarifying questions or possible visual setups. Users then provide answers to disambiguate the prompts. The disambiguated prompts are used to evaluate the faithfulness of generated images using a VQA model. Our findings show that our framework has a positive effect on faithful generation and is in agreement with human evaluation, making it a reliable tool for evaluating text-to-image models.</sample>
    <sample id="154">Die Autoren, Sara Papi, Matteo Negri und Marco Turchi, gehören an der University of Trento und Foundazione Bruno Kessler.</sample>
    <sample id="155">Der/die Referent*in hieß "Easy on Me" oder "I Gotta Feeling".</sample>
    <sample id="157">Abstract: Dialogue summarization is a challenging task in text summarization research that aims to distill salient information from a dialogue context into a concise summary. This work introduces the Dialogue Summarization with Static-Dynamic Structure Fusion Graph (SDDS) model, which addresses the limitations of existing methods that rely on pre-computed static graph structures and cannot dynamically adapt to downstream tasks. The SDDS model consists of four main components: an Utterance Encoder, a Static-Dynamic Graph module, a Summary Generator, and a fusion method. It employs heuristic dialogue structure modeling methods to build static graphs and a dynamic graph module to capture semantic relationships between utterances based on their deep vector representation. The model uses a dual cross-attention mechanism to integrate the graph representation into the generation process. The code and data for the SDDS model have been released on GitHub.</sample>
    <sample id="158">Abstract: This paper introduces a dual cache method for long document neural coreference resolution. The proposed method uses a local cache with LRU eviction policy and a global cache with LFU eviction policy to store local and global entities, respectively. The model scans the document from left to right and classifies new or updated entities, adding them to the appropriate cache based on their frequency. The results show that the dual cache method outperforms single cache methods and significantly reduces cache misses, especially in long documents without training data. The dual cache also has the highest performance/cost ratio compared to single cache methods.</sample>
    <sample id="159">Hallo, alle. Ich bin Koustav Sinha und ich freue mich, euch willkommen in unserem Talk zu unserem ACL 2023 Papier begrüßen zu können. Sprachmodell-Acceptabilitätsurteile sind nicht immer robust gegenüber Kontext. Das ist ein gemeinsames Werk mit John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy und Adina Williams. In diesem Werk revisitieren wir die Minimal-Paar-Paradigmen. Das Minimal-Paar-Paradigma bewertet Sprachmodelle auf Grund von Acceptabilitätsurteilen. Das kann auch Grammatik wie BLiMP, SyntaxGym oder Akzeptabilität in Bezug auf Stereotypien wie CrowS-Paare einschließen. In diesem Paradigma werden Sprachmodelle normalerweise geprüft, indem man ihnen zwei Sätze zeigt: einen akzeptablen (grammatikalisch korrekten) und einen akzeptablen (grammatikalisch falschen) Satz. Das Ziel ist, dass das Modell mehr Wahrscheinlichkeit beim Akzeptablen-Satz einordnet. Der aktuelle MPP-Pipeline erlaubt es uns nicht, ein Modell zu bewerten, basierend auf der Akzeptabilität über längere Sätze. Heute werden große Sprachmodelle mit immer größeren Kontextfenstern aufwachen. Es ist also wichtig, die Akzeptabilität von Modellen über den gesamten Kontextwindow zu bewerten. Das ist, was wir versuchen, hier zu erreichen. Wir versuchen, die MPP-Pipeline zu revisitieren, indem wir das Modell auffordern, die Akzeptabilität auf längere und längere Sequenzen zu bewerten. Das ist unser Ansatz. Was wir tun, um diese längeren Sequenzen zu simulieren, revisitieren wir die Datensätze selbst und erstellen Sentenzen, indem wir akzeptable oder unakzeptable Sätze aus diesen Datensätzen auswählen. Zum Beispiel haben wir eine typische Paar von Grammatik from the BLiMP-Datensatz aus dem Adjunkt-Island-Fall gewählt. Und was wir tun, um solche längeren Sequenzen zu erstellen, und welche davon akzeptabel und welche nicht, ist, grammatikalische Sätze aus dem Adjunkt-Island-Fall zu extrahieren und sie als Prefix hinzuzufügen sowohl an den akzeptablen als auch an den unakzeptablen Query. Wir können dasselbe machen, indem wir Sätze aus einem anderen Datensatz oder einer anderen Gruppe auswählen. Das nennen wirMismatch-Szenario. Hier sind die Sätze immer noch aus relevanten Datensätzen stammend, aber sie stammt nicht aus dem Datensatz, den wir bewerten. Wir können dasselbe für die Unakzeptabilität-Fälle tun. Schließlich können wir Sätze aus einem vollkommen unverwandten Bereich wie Wikipedia auswählen. Das zeigt uns, ob die Akzeptabilitätsurteile der Modelle tatsächlich beeinflusst werden, ob der Kontext aus einem anderen Datensatz kommt, oder ob er für den aktuellen Query irrelevant ist. Wie performiert das Modell? Zunächst schauen wir uns Wikipedia-Sätze an, die für den aktuellen Query-Paar completely irrelevant sind, und dort finden wir, dass die MPP-Jurys robust für beliebige Kontextlängen sind. Wir erhöhen die Kontextlänge bis zu 1024, um die OPT- und GPT-2-Modelle zu testen. Und wir sehen hier in der orange doppelt gestrichenen Linie, dass die MPP-Jurys relativ stabil sind. Nun, was passiert, wenn wir Sätze aus dem gleichen Datensatz auswählen? Hier warten wir, Sätze aus akzeptablen und unakzeptablen Domänen aus dem gleichen BLiMP- oder SyntaxGym-Datensatz zu erstellen. Und hier sehen wir, dass die MPP-Jurys entweder signifikant steigen oder absinken, wenn wir entweder akzeptable oder unakzeptable Präfixe hinzufügen. Aber wenn wir die Struktur匹配, also wenn wir Sätze aus dem gleichen Phänomen in BLiMP oder SyntaxGym auswählen, sehen wir einen massiven Anstieg oder einen massiven Abfall der MPP-Jurys für das Modell, je nachdem, ob der gewählte Präfix akzeptabel oder unakzeptabel ist. Jetzt und jetzt ist dieser Effekt sehr groß, und er wird wahrscheinlich die Sprachmodelle beeinträchtigen, die einen großen Kontextwindow haben. Warum beeinträchtigen die match-Präfixe die Sprachmodellderstellungen so stark? Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, die Eingabe-Sentence zu perturbieren, indem wir versucht haben, die relevanten Strukturen zu erhalten, aber dennoch Lärm hinzuzufügen. Nach mehreren dieser Perturbationen finden wir, dass keines dieser Lärm actually die Modelle dazu bringt, den Kurs zu ändern, wie sie die MPP-Jurys ausdrücken. Basically finden wir, dass die Modelle sensible zu perturbierten Sätzen sind, die in ähnlicher Weise verändert werden. Das bedeutet, wenn wir Sätze im akzeptablen Bereich perturbieren, sehen wir eine ähnliche Steigerung an allen Perturbationen, und wenn wir Sätze im unakzeptablen Bereich perturbieren, sehen wir eine ähnliche Abnahme in den MPP-Jurys. Das Hauptergebnis unseres Werks ist, dass Sprachmodelle sensible zu latenten syntagmatischen und semantischen Merkmalen sind, die über die Sätze verteilt sind. Und die MPP-Evaluation, die wir momentan mit kurzen und einzelnen Sätzen durchführen, mag nicht vollständig das abstrakte Wissen von Sprachmodellen über den Kontextwindow abdecken. Lesen Sie bitte unser Papier für weitere Details über unsere Experimente. Vielen Dank für das Hören.</sample>
    <sample id="160">Im ersten Schritt der Methode werden die Input-Token mit einem unsortierten Multiset von Tokens zugeordnet, die im Ausgabe-Output vorkommen werden.</sample>
    <sample id="161">In CoScript sind insgesamt 55.000 spezifische Ziele mit Skripten vertreten.</sample>
    <sample id="163">Die beste Ausrichtungsmethode für DEplain ist die Methode von MASSalign.</sample>
    <sample id="164">Der Vorteil von schwach überwachtem Lernen ist, dass es viel billiger ist, Datensätze zu labeln, indem man schwache Quellen verwendet, anstatt den Datensatz von Menschen manuell zu labeln.</sample>
    <sample id="165">Title: Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations

Authors: Wenting Zhao, Cornell University

Abstract: We introduce an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization) for abductive reasoning in a closed-world setting. Unlike supervised methods that require annotated plausible explanations, LiPoR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing possible explanations. To prefer plausible explanations, we propose a regularizer that enforces mutual exclusivity among explanations. Our results on AlphaNLI show that LiPoR outperforms zero-shot models and the previous best unsupervised approach by over 4 absolute points in accuracy.</sample>
    <sample id="166">Abstract: We introduce a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. Our method combines the advantages of analogical reasoning (System 1) and logical reasoning (System 2) to address the challenge of highly similar images and long descriptions. The first model, Proposition Generator, decomposes complex propositions into simple ones. System 1, Visual-Linguistic Interactor, performs visual-proposition information interaction. System 2, Neural-Symbolic Reasoner, integrates reasoning states and results of simple propositions to obtain the final solution. Experimental results show that our proposed method outperforms other baselines. We also present two cases to check the performance of the proposed method. Our work suggests that neural symbolic calculation may be a worthwhile approach to improve compositional reasoning and planning in large language models.</sample>
    <sample id="167">In DEPLAIN-web wurden 750 Dokumente auf both manuellem und automatischem Weise ausgerichtet.</sample>
    <sample id="168">Der CoNLL++-Datensatz wurde erstellt, indem Reuters News-Daten von 2020 mit den gleichen Annotieranleitungen wie CoNLL-2003 anhand der gleichen Annotieranleitungen anhand von Reuters News-Daten von 2020 erstellt wurden.</sample>
    <sample id="169">Abstract: This paper presents a systematic study of large language model prompting for machine translation, using the best practices of the MT community. We evaluated the performance of PaLM, a 540 billion-parameter language model, on several NLP tasks and compared it to state-of-the-art systems. Our experiments showed that the quality of the examples used in prompting is more important than the similarity to the source sentence. We also found that PaLM's fluency is comparable to state-of-the-art systems, but its accuracy suffers from omission errors. Overall, PaLM comes close to a commercial system in terms of fluency, but still has room for improvement in accuracy.</sample>
    <sample id="170">Hallo alle, ich bin Yusen Zhang aus der Pennsylvania State University. Heute werde ich über unser Werk "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations" sprechen. Das Semantic Parsing ist eine Aufgabe, bei der man semantische Darstellungen von Benutzereingaben erstellt, wie z.B. SQL und Lambda Calculus. Und das Cross-Lingual Semantic Parsing ist die Aufgabe, queries in mehreren natürlichen Sprachen in mehreren Bedeutungsdarstellungen zu übersetzen. Wie in diesem Bild gezeigt wird, müssen wir die Abfragen in mehreren natürlichen Sprachen mit neuronalen Modellen in SQL, Lambda oder FunQL und andauernd übersetzen. Gegenwärtig werden existierende Modelle für das Cross-Lingual Semantic Parsing separat vorgeschaffen und evaluiert auf Datensätzen von limitierten Aufgaben und Anwendungen. Zum Beispiel gibt es eine große Abdeckung für bestimmte natürliche Sprachen, aber Chinesisch fehlt und auf bestimmte Bedeutungsdarstellungen fehlt es an Abdeckung. Das Lambda Calculus wird fehlgeschlagen, oder es werden nur bestimmte neuronalen Modelle evaluiert. Zum Beispiel gibt es nur ein einziges Modell, um sie zu evaluieren. Um all das zu beheben, schaffen wir XSemPLR. Wir bieten eine einheitliche Datensammlung XSemPLR für das Cross-Lingual Semantic Parsing in mehreren natürlichen Sprachen und mehreren Bedeutungsdarstellungen an. Sie enthält 9 Datensätze in verschiedenen Domänen, 5 Aufgaben des Semantic Parsing, 8 Bedeutungsdarstellungen und 22 natürliche Sprachen in 15 Sprachfamilien. Um unser Benchmark besser zu bewerten, betrachten wir sechs Ansatzpunkte für Training und Evaluation. Der erste Ansatzpunkt ist "Translate-Test". Wir verwenden die Google Translate-API, um die Quelle ins Zielsprache zu übersetzen, dann verwenden wir einen monolinguen Modell zur Training- und Evaluierphase. Zum Beispiel trainieren wir das englische Modell auf englischen Abfragen und während der Inferenz übersetzen wir die deutsche Abfrage mit API ins Englische und verwenden dann das trainierte Modell, um die SQL zu predictieren. Wir werden auch den Monolinguen Modell testen. In diesem Ansatzpunkt ist die Quellsprache dieselbe wie die Zielsprache, zum Beispiel Deutsch zu Deutsch oder Englisch zu Englisch. Wir testen auch den Monolinguen Few-shot Ansatzpunkt, bei dem wir monolinguen Modelle mit nur 10% des Trainingsdatensatzes trainieren. Wir testen auch den Multilingual Ansatzpunkt, bei dem wir einen multilingualen Modell für alle Sprachen trainieren. Zum Beispiel legen wir die deutschen, englischen und chinesischen Abfragen zusammen, um ein multilinguales Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um deutsche Abfragen oder chinesische Abfragen usw. zu übersetzen. Wir betrachten auch den Cross-Lingual Zero-shot und Few-shot Transfer. Wir trainieren auf einer Quellsprache und übertragen sie auf eine andere Sprache. So beim Training trainieren wir auf englischen Abfragen oder auf einer Kombination von englischen und deutschen Few-shot Abfragen, um ein multilinguales Modell zu trainieren, um die SQL-Output zu predictieren. Wir finden auch viele interessante Resultate. Beim Analyzieren von monolinguen Modellen evaluieren wir zwei Gruppen von Modellen einschließlich Encoder-PTR, das steht für Multilinguen-Vortrainingse Encodeurs mit Pointer-basierten Decoder, wie z.B. XLM-R + PTR und mBERT + PTR. Wir evaluieren auch Encoder-Decoder Modelle, das steht für Multilinguen-Vortrainingse Encodeurs-Decoder-Modelle, wie z.B. mBART und mT5. Wir finden, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erreicht. Wir evaluieren auf mT5 und XLM-R + PTR im multilingualen Setting. Wir finden, dass Encoder-Decoder oder Encoder-PTR durch die Training in einer Menge verschiedener Sprachen verbessert werden können. Wir finden, dass es because most of the major natural languages can obtain performance gains, except that English performance drops in seven datasets and only gains in three datasets. Ich denke, das ist bekannt als "Curse of Multilinguality". Wir vergleichen auch den Sprachübergreifenden Performance-Unterschied. In diesem Bild ist der blauen Linie der Cross-Lingual Few-shot Transfer. Die orange Linie ist der Cross-Lingual Zero-shot Transfer. Während die grüne Linie die Monolinguen Setting ist. Wir finden, dass, indem wir die grüne und orange Linie vergleichen, wir finden die Zero-shot Setting, die Cross-Lingual Transfer Leistungsunterschied signifikant ist, und dann indem wir die blauen und orange Linien vergleichen, finden wir, dass mit dem Few-shot Setting der Transfer Unterschied schnell abgeschlossen wird. Wir finden auch einige weitere interessante Erkenntnisse. Zum Beispiel outperformen Encoder-Decoder vorherige Arbeiten oder erreichen vergleichbare Ergebnisse. Das Vortraining auf englischer natürlicher Sprache kann signifikant die Leistung von Few-shot auf Zielsprachen steigern, und wir finden multilingual Sprachmodelle wie Codex und BLOOM sind immer noch unzureichend für die Aufgaben des Cross-Lingual Semantic Parsing. Insgesamt bauen wir XSemPLR, eine vereinheitlichte Benchmark-Studie für das Cross-Lingual Semantic Parsing mit mehreren natürlichen Sprachen und mehreren Bedeutungsdarstellungen. Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen von multilingualen Sprachmodellen durch. Unsere Ergebnisse zeigen viele interessante Erkenntnisse an. Und so weiter. Und erwarten Sie, uns auf unsere Publikation und Code zu besuchen. Vielen Dank fürs Hörn.</sample>
    <sample id="171">In der vorgestellten Arbeit wurden Arbeiten in vier Datensätzen (AG News, MIND, SST2 und Enron Spam) durchgeführt, um die Funktionalität des Embedding Markers zu überprüfen.</sample>
    <sample id="172">Nein, Codex und Bloom sind noch nicht ausreichend für die Aufgabe des Mehrsprachischen Semantikparsings (CLSP).</sample>
    <sample id="174">This paper introduces ArgAnalysis35K, a large-scale dataset for argument quality analysis. The dataset contains 35,000 argument-analysis pairs sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. It covers diverse themes and motions, providing a more comprehensive representation of arguments in parliamentary debates. The dataset includes an analysis component that combines claims, premises, and other elements to explain arguments better. Instance-based annotator reliability is introduced to account for human biases, and a relevance model assigns scores to capture the relevance of arguments to specific themes. This unique combination of features makes ArgAnalysis35K a valuable resource for improving argument quality analysis in NLP research.</sample>
    <sample id="175">Die Methode bewältigt die Mehrdeutigkeit der Permutationen, indem sie die Ausordnung als Teil des Trainings induziert. Dadurch können für einen bestimmten Token, der Multisettoken stammt, mehrere Permutationen bestimmt werden, aber die linguistisch korrekte Permutation latent bleibt. Um dies zu bewältigen, induzieren wir die Ausordnung als Teil des Trainings und approximieren das finden der höchsten Punktschwelle mit einer GPU-friendlichen kontinuierlichen Relaxation, die auch die Rückpropagation durch die Lösung und die l linguistisch plausiblen Permutationen erlaubt.</sample>
    <sample id="176">Die Fairness eines nachgeschalteten NLP-Modells wird definiert, indem man die Leitlinien der Sprachmodelltrainingdaten analysiert und prüft, ob sie zu einem bestimmten politischen Leitstelle neigen. Wenn das Modell eine bestimmte Leitstelle preferiert, dann kann es zu Fairnessproblemen in der Anwendung führen.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustav Sinha</sample>
    <sample id="179">This paper presents SymbolicToM, a plug-and-play method to improve Theory of Mind reasoning skills in large language models (LLMs). It uses explicit graphical representations to efficiently answer false-belief questions. The method leverages off-the-shelf NLI and OpenIE models for inference-time computation. Experiments show performance gains across various LLMs, including GPT-3, Macaw, and Flan-T5-XXL. SymbolicToM also generalizes well to out-of-domain datasets with diverse story structures and linguistic diversity, allowing stronger models like GPT-4 to fully solve these datasets.</sample>
    <sample id="180">Myra</sample>
    <sample id="181">This paper introduces a method for constrained language planning, which involves generating step-by-step instructions for specific goals with multiple constraints. The authors evaluate the ability of large language models to plan for abstract goals and extend these goals with multi-faceted constraints using InstructGPT. They develop an over-generate-then-filter method to improve script quality and create a dataset named CoScript for constrained language planning. The results show that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="182">Tropikalismus bezieht sich auf die Verbindung von bestimmten Attributen oder Merkmalszügen (wie "vibrant" und "curvaceous") mit bestimmten geographischen Gebieten (wie "tropical") und ist ein tropikalistisches Trockenland. In diesem Fall wird Latina Frauen als "vibrant" und "curvaceous" beschrieben, was zu einem tropikalistischen Trockenland beiträgt.</sample>
    <sample id="183">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie Menschen mit bestimmten Identitäten anhörten und dann prompte Anfragen an LLMs stellten, um eine Persönlichkeit zu generieren.</sample>
    <sample id="184">In dieser Arbeit wurde CXMI (Contextual Mutual Information) verwendet, um die Kontextnutzung von Maschinell übersetzten Modellen zu messen. CXMI misst, wie viel Informationen das Kontext bereitstellt, um den Zieltext zu verstehen, gegeben den Quelltext. In diesem Werk wurde CXMI erweitert, um P-CXMI (Pointwise CXMI) zu messen, um die Kontextnutzung an der Satel oder Wortebene zu messen.</sample>
    <sample id="185">DrBERT und ChuBERT sind beide Medizinmodell, aber sie sind auf verschiedenen Datensätzen trainiert. DrBERT basiert auf NACHOS, einer Datensammlung von medizinischen Webseiten, während ChuBERT auf anonymisierten Daten aus dem Krankenhausaufbewahrungssystem der Nantes University Hospital basiert.</sample>
    <sample id="187">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="188">Iteratives Transferlernen ist ein Verfahren, bei dem ein Modell auf einem Datensatz trainiert wird und dann auf einem neuen Datensatz weitertrainiert wird, um das Modell zu verbessern. In diesem Fall wurde es verwendet, um das Modell für die Erkennung von Dissonanzrelationen zu verbessern.</sample>
    <sample id="189">Das Ziel des Datensatzes ist, die Verständnisfähigkeit von Sprachmodellen für indirekte Beziehungen zu Entitys in einem natürlichen Dialog zu messen und zu verbessern.</sample>
    <sample id="190">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Steuerungssignale (Watermark) detektiert, die in den bereitgestellten Embedding-Output enthalten sind.</sample>
    <sample id="191">Es sind insgesamt 3 Autoren an der Arbeit beteiligt.</sample>
    <sample id="192">Abstract: We present CAME (Confidence-guided Adaptive Memory Efficient Optimization), a novel optimizer designed to achieve both fast convergence and low memory usage in training large language models. Our approach addresses the limitations of existing adaptive gradient-based methods, such as Adam, which require triple the memory for keeping first and second moment estimates of per-parameter gradients. By introducing a confidence-guided updating mechanism based on the residual between predicted and generated updates, we significantly reduce the memory footprint while maintaining performance comparable to traditional adaptive optimizers. Experiments on BERT, GPT-2, and T5 show that CAME achieves better validation accuracy and more efficient memory usage, particularly for large batch sizes.</sample>
    <sample id="193">Die Anzahl der Annotatoren, die verwendet wurden, um den ursprünglichen Datensatz zu erstellen, wird nicht erwähnt.</sample>
    <sample id="194">Die Autoren der Arbeit "NLPositionality" sind an Carnegie Mellon University, University of Washington und Allen Institute for AI tätig.</sample>
    <sample id="195">This paper introduces a novel framework called RoHT (Reasoning over Hierarchical Question Decomposition Tree) for Explainable Question Answering (XQA). RoHT addresses the limitations of existing methods by integrating knowledge from heterogeneous sources and flexibly selecting appropriate knowledge sources for each sub-question. The framework consists of two stages: firstly, building a Hierarchical Question Decomposition Tree (HQDT) to understand the hierarchical compositional structure of a complex question; secondly, probabilistic reasoning over HQDT to fuse knowledge from a knowledge base and a text corpus at different levels. RoHT outperforms existing KB QA methods on the KQA Pro dataset and achieves substantial improvements when using Wikipedia as a supplementary text corpus. On the Musique dataset, RoHT improves F1 by 11.9 compared with SOTA method EX(SA) when only using given paragraphs, and with both text and KB, RoHT-mix is also remarkably better than TransferNet.</sample>
    <sample id="196">Das Beispiel mit dem Begrenzer auf der linken Seite lautet "Marge read it yesterday".</sample>
    <sample id="197">Die Stand der Technik für Dialogsysteme sind die ABC-Eval-Methode, die eine präzisere und zuverlässigere Evaluierung der Verhaltensbewertungen von Dialogmodellen ermöglicht. Sie können die Häufigkeit von Themenfehlern wie Irritationen, Widersprüchen, Halluzinationen und Verletzung von allgemein akzeptierten Kenntnissen messen und dabei auch die Empathie des Modells berücksichtigen.</sample>
    <sample id="198">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil große Sprachmodelle in letzter Zeit einen immer größeren Kontextwindow aufwerten und es wichtig ist, die Akzeptanz der Modelle im gesamten Kontextwindow zu bewerten.</sample>
    <sample id="199">Ja, das mehrsprachige Training hat in einigen Fällen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt. Dies wird als "Curse of Multilinguality" bezeichnet.</sample>
    <sample id="200">Ja, die Annotatoren kennen die Entität im Voraus. Sie erhalten jedoch keine spezifischen Informationen über die Entität und müssen sich auf die gegebenen Hintergrundknowledge und die Google-Suche-Links verlassen.</sample>
    <sample id="201">Die MT-Metriken, die für die Bewertung verwendet wurden, sind die neuesten neuralen MT-Metriken und auch Expertenbasierte mensliche Auswertungen.</sample>
    <sample id="202">DieRegression bei der Generalisierung auf bestimmte NER-Typen wird in dem Papier nicht explizit diskutiert. Es wird jedoch erwähnt, dass die Evaluierung auf verschiedenen NER-Typen stattgefunden hat und dass die Befunde zu allgemeinen Modellarchitektur, Modellgröße und Anzahl der Fine-Tuning-Beispiele anwenden lassen könnten.</sample>
    <sample id="203">Positionalität ist für NLP wichtig, weil sie die systematischen Leistungsunterschiede zwischen Bevölkerungen und die Aggregation von Urteilen und Meinungen von Menschen repräsentiert, die in den Datensätzen und Modellen enthalten sind. Das Verstehen von Positionalität kann dazu beitragen, dass NLP-Modelle fairer und gerechter sind.</sample>
    <sample id="204">Die mehrsprachigen LLMs wie BLOOM wurden nicht durch Adapter oder eine vollständige Feinabstimmung angepasst.</sample>
    <sample id="205">This paper investigates the political bias propagation pipeline from pretraining data to language models to downstream tasks. The authors evaluate the political leaning of language models and how they perform on downstream tasks such as hate speech detection and fake news detection. They find that language models with different political leanings have varying performance on these tasks, which can lead to fairness issues in NLP applications. The paper highlights the dilemma of sanitizing political opinions in language model training data to avoid bias, while also avoiding censorship or exclusion.</sample>
    <sample id="206">Sie verwenden das Modell, das von zwei verschiedenen Aufgaben übertragen wurde: diskussion unabhängige Dissonanz-Stimmstelle-Klassifizierung (eine Aufgabe, die bestimmt, ob zwei Meinungen in Einklang oder Widerspruch sind) und die binäre Klassifizierung von Auswechslungs- und Vergleichsklassen (beide Aufgaben sind eng mit dem Konzept von consonance und dissonance verbunden).</sample>
    <sample id="207">Die besten übersetzten Testsets wurden verwendet, um die PaLM-Fähigkeiten zu bewerten.</sample>
    <sample id="208">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen: Forscher sollten positive Stereotype und essentialisierende Erzählnisse adressieren, einIntersectionslens verwendet werden, um Biases und Schäden zu untersuchen, und es sollte mehr Transparenz über die Methoden zur Bekämpfung von Bias gegeben werden.</sample>
    <sample id="209">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 17,5%.</sample>
    <sample id="210">Der Referent*in heißt Shuheng.</sample>
    <sample id="211">Ja, die Ergebnisse und der Datensatz der Studie können alsBenchmark verwendet werden.</sample>
    <sample id="212">In der Arbeit wird experimentiert mit einem einzigen kleineren Modell, T5, das auf dem CoScript-Dataset trainiert wurde. Es wurde festgestellt, dass T5 die besten Ergebnisse erzielt, indem es große Modelle übertrifft, wenn es auf geeignetem Datensatz ausgebildet wird.</sample>
    <sample id="213">Das OFA-Modell wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet.</sample>
    <sample id="215">This talk by Adam Przepiórkowski discusses the dependency structures of coordination in different linguistic theories and approaches. The Prague approach, for instance, assumes a conjunction-headed structure, while the Hudson's Word Grammar uses a multi-headed approach. The speaker argues for symmetric structures of coordination based on the principle of dependency length minimization. Statistics from the Penn Treebank show that left conjuncts tend to be shorter when the governor is on the left or absent, but this effect disappears when the governor is on the right. This supports the argument for symmetric structures of coordination.</sample>
    <sample id="217">Abstract: We introduce a method for multi-attribute controllable dialogue generation, called DCG (Disentangled Controllable Generation), which learns attribute concepts from seen values and uses the disentanglement loss to disentangle different attribute combinations. Our method explores compositional generation for multi-attribute controllable dialogue generation and introduces a unified reference-free evaluation framework, MAE, for different granularities of attributes. We establish two benchmarks and prove the effectiveness of our method and evaluation metrics through experiments. Our results show that DCG outperforms all other baselines in attribute controllability and text equality. Additionally, our method successfully tackles the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC. We also demonstrate the impact of prompts on compositional generalization with a visualization of the concatenated prompt embeddings of three attributes via PCA on DailyDialog-CG.</sample>
    <sample id="218">Die Autoren der Publikation "Prompting PaLM for Translation: Assessing Strategies and Performance" sind von Google Translate.</sample>
    <sample id="219">Title: A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports

Authors: Jia-Huei Ju, Yu-Shiang Huang, Cheng-Wei Lin, Che Lin, and Chuan-Ju Wang

Abstract: This paper presents a highlighting task and a multistage pipeline for uncovering financial signals in Form 10-K reports. The goal is to compare and contrast the context between target and reference reports to find rationale words that reveal important information about companies' activities. The pipeline consists of three stages: document segmentation, relation recognition, and fine-tuning. The model is trained using an external dataset (eSNLI) for out-of-domain fine-tuning and revised pairs for intermediate fine-tuning. Evaluation metrics include precision and PCC. The proposed domain-adaptive highlighting model achieves the best performance on FINAL and demonstrates generalization capability. Future works include improving effectiveness and adding more features or techniques in information retrieval.</sample>
    <sample id="220">Die Autoren, Vasudha und ihre Teammitglieder, gehören an der Stony Brook University.</sample>
    <sample id="221">Die Arbeit hat sich auf die Übersetzung von Deutsch ins Englische konzentriert.</sample>
    <sample id="222">This work investigates the challenges and interventions for domain adaptation in open-domain question answering. It proposes three main contributions: investigating data interventions, identifying dataset shifts, and determining effective data interventions for specific shifts. The authors use a general-purpose Wikipedia-based source domain to train both retriever and reader models and test their generalizability on seven target datasets spanning six different domains. They explore zero-shot and few-shot methods for generating interventions and analyze the impact of varying question, answer, and context distributions on model performance. The results show that few-shot adaptations improve reader performance by up to 24% and that certain types of data interventions are effective based on the type of shift exhibited by the target dataset.</sample>
    <sample id="223">Shangbin</sample>
    <sample id="224">Während der Experimente wurden die Modelle Long-mBART und normaler mBART untersucht, um zu sehen, ob sie in der Lage sind, komplexes Textmaterial zu simplifizieren.</sample>
    <sample id="225">Für die Training- und Test-Splitt werden 53 Aufgaben aus 9 Gruppen verwendet.</sample>
    <sample id="226">Es gibt zwei Hauptautoren, Regina Stodden und Omar, die an der Arbeit beteiligt sind.</sample>
    <sample id="227">Grounded language understanding, which involves mapping natural language expressions to executable plans or programs in specific environments, is a challenging task for current language models. Most language models are pre-trained with textual corpora and lack grounding, leading to the gap between pre-training and downstream applications. Existing research typically uses language models to directly generate plans, which may not always be grammatical or valid. Our proposed framework, Pangu, focuses on discrimination instead of generation, where a symbolic agent proposes candidate plans and a language model scores and ranks them. Pangu achieves outstanding performance across different language models and settings, demonstrating strong sample efficiency and robustness under non-i.i.d. conditions. We argue that discrimination is a better strategy for grounded language understanding than generation.</sample>
    <sample id="228">Die Autoren haben experimentiert an den Datensätzen AG News, MIND, SST2 und Enron Spam.</sample>
    <sample id="229">This paper presents a joint work with Henning Wachsmuth on detecting improvable claims for argumentative writing support. The authors focus on argumentative text and explore how to best model the quality of argumentative text based on implicit revision patterns found in collaborative online debate platforms such as Kialo. They delineate four main challenges that arise when working with such revision-based data, including representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. The authors conclude that revision-based data can be employed effectively for the given tasks and that modeling the distance between two claimed versions is beneficial for detecting suboptimal claims.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from the web, which was used to train DrBERT, the first biomedical model in French.</sample>
    <sample id="232">Der Referent in diesem Review ist David Vilar.</sample>
    <sample id="233">Abstract: This paper presents a novel approach for simultaneous speech translation (SimulST) called EDAtt, which uses existing offline speech-to-text (ST) models without re-training or specific architecture. EDAtt employs an encoder-decoder attention mechanism to decide whether to emit partial translations based on the stability of the received information. The proposed method outperforms popular strategies applied to offline models in terms of translation quality and latency, while also being the fastest in computational-aware average lagging. The paper includes results on German and compares EDAtt with state-of-the-art architectures tailored for SimulST. Open-source code, models, and simultaneous output are released to facilitate reproducibility.</sample>
    <sample id="234">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung der LLMs für Übersetzung. In einem einfachen Experiment mit einem-Shot-Prompting wurden zwei verschiedene Prompts für jede Phrase verwendet, was einen Unterschied von mehr als einem BLEURT-Punkt und in extremen Fällen bis zu 40 BLEURT-Punkten erzielte. Es ist wichtig, eine gute Prompt-Strategie zu auswählen, um die Leistung zu optimieren.</sample>
    <sample id="235">Die Autoren dieser Arbeit gehören an der University of Edinburgh.</sample>
    <sample id="236">Jede Aufgabe im MultiInstruct-Dataset wird mit 5 verschiedenen Anweisungen versehen, die von Expert*innen geschrieben wurden.</sample>
    <sample id="237">Die Autoren schlagen einen Diagnostik-Test für die Integrierung von Kenntnis vor, der ein Hauptreferenz Auflösungs Aufgabentyp verwendet, um die Fähigkeit zu überprüfen, auf Informationen aus verschiedenen Quellen zuzugreifen. Sie definieren drei verschiedene Einstellungen: "Hintergrund-Pretrain", "Hintergrund-Beide" und "Hintergrund-Inference".</sample>
    <sample id="238">This video introduces MeetingBank, a new benchmark dataset for meeting summarization developed by Yebowen Hu from the University of Central Florida. The dataset includes 1,366 City Council meetings with nearly 7,000 instances, featuring meeting transcripts, reference summaries, and other useful resources. The dataset was created to address the challenges of high-quality meeting summaries and locating trustworthy public meeting resources. The creators used Speechmatics API for audio-to-transcript conversion and identified meeting details from official websites. They evaluated various summarization systems, including extractive and abstractive models, using metrics like ROUGE-2, BERTScore, MoverScore, and human assessments. GPT-3 achieved the highest overall scores in human evaluations, demonstrating exceptional fluency and coherence but lower informativeness and factuality. The dataset serves as a valuable tool for researchers to develop advanced meeting summarizers and gain insights into decision-making processes.</sample>
    <sample id="239">Hallo alle, ich bin David Vilar und ich werde einen kurzen Review der Publikation "Prompting PaLM for Translation: Assessing Strategies and Performance" geben. Dies ist ein gemeinsames Werk mit meinen Kollegen von Google Translate. PaLM ist ein 540 Milliarden-Parameter großer Sprachmodell, das im Jahr 2022 präsentiert wurde. Es wurde auf einem großen Datensatz von Texten trainiert, der 780 Milliarden Token umfasst. Bei der Veröffentlichung erreichte es das aktuelle Standardmodell in Hunderten von NLP-Aufgaben.

In diesem Werk präsentieren wir die erste systematische Studie über die Anwendung von großen Sprachmodellen zur Anpassung von Maschineller Übersetzung (MT). Wir evaluieren die Übersetzungsfähigkeit solcher Modelle unter Verwendung der besten üppigsten Testdaten, um dabei zu vermeiden, dass die Testdaten mit den Trainingsdaten des Sprachmodells überlappen. Wir verglichen auch die Leistung mit jenem besten performierenden System, das bei der WMT-Evaluation verwendet wurde.

Wir verwenden die neuesten MT-Metrisen und zeigen außerdem Expertenbasierte menschliche Evaluierungsresultate an. Schließlich bieten wir einige Empfehlungen für die Selektion von Anpassungsschwergrenzen. Die Anpassung hat einen großen Einfluss auf die Leistung von LLMs für Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir zwei verschiedene Anpassungsschwergrenzen für jede Satzfolge verwenden. Mehr als 516 von 1000 Sätzen weisen einen Unterschied von mehr als einem BLEURT-Punkt auf. Und in extremen Fällen kann dieser Unterschied bis zu 40 BLEURT-Punkten betragen. Es ist also wichtig eine gute Anpassungsschwergrenzstrategie zu auswählen.

In unseren Experimenten haben wir uns für eine 5-Schritt-Anpassungsschwergrenzstrategie entschieden, bei der wir jede Satzfolge mit dem Sprachtyp kennzeichnen, in dem sie steht. In diesem Beispiel, bei der wir eine Übersetzung von Deutsch ins Englische durchführen, werden die Quellsätze mit einem Doppelpunkt markiert und die englischen Übersetzungen mit einem Doppelpunkt. Wir haben festgestellt, dass die Form der Anpassungsschwergrenzstrategie in diesem Fall keine große Bedeutung hat, wenn es um mehrere kurze Anpassungsschwergrenzstrategien geht. Es ist jedoch crucial, bei null- und einschrittigen Anpassungsschwergrenzstrategien die Qualität der Beispiele zu beachten. Und wenn wir, wie in unserem Fall, zu einer 5-Schritt-Anpassungsschwergrenzstrategie kommen, macht es keinen großen Unterschied, welche Form der Anpassungsschwergrenzstrategie verwendet wird. Es sind die Beispiele, die den Hauptanteil tragen.

Die zusammenfassende Auswertung unseres Experiments zeigt, dass die Qualität der Beispiele wichtiger ist als die Ähnlichkeit zu den Quellsätzen. Es ist also wichtig Beispiele mit hohher Qualität zu verwenden. Insbesondere verglichen wir die Selektion von Anpassungsschwergrenzen aus dem Trainingsdatensatz mit der Selektion von Anpassungsschwergrenzen aus den WMT-Evaluation-Daten. Das Dev-Dataset ist viel präziser und von höherer Qualität als das Trainingsdatensatz, daher erzielen wir bessere Leistungen, wenn wir den Dev-Dataset verwenden.

Allerdings haben spezialisierte Systeme einen erheblichen Vorteil über die PaLM Übersetzungen. Dennoch kommt PaLM relativ nah an einem kommersielbaren System heran. In unserem Fall haben wir Google Translate verwendet. Die Einsichten, die wir aus der menschlichen Evaluation gewonnen haben, die wir mit dem MQM-Framework durchführten, zeigen, dass die Fluideität von PaLM vergleichbar mit den besten Systemen ist, aber die Hauptunterschiede liegen beim Genauigkeit. Insbesondere sind die häufigsten Fehler Omission-Fehler, also PaLM entscheidet sich manchmal, einen Teil des Quellsatzes zu produzieren, der in der Übersetzung enthalten ist. Allerdings ist die "Stil/Ausdruckslosigkeit" Kategorie für PaLM tiefer als für die besten Systeme, was ein weiterer Hinweis darauf ist, dass PaLM eine wirklich fluide Übersetzung liefert, aber immer noch einige Probleme mit Genauigkeit hat.

Das wäre alles für diese kurze Übersicht.Für weitere Details bitte zu der vollständigen Präsentation der Publikation kommen. Vielen Dank für eure Aufmerksamkeit.</sample>
    <sample id="240">Hallo, ich bin Dawei, ein Doktorand an der Saarland University in Deutschland. In diesem Video möchte ich mein jüngstes Werk präsentieren: "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Dies ist eine gemeinsame Publikation mit Xiaoyu Shen, Marius Mosbach, Andreas Stephan und Dietrich Klakow. Ich werde beginnen, mit einer kurzen Einführung in dieWeak Supervision undWeakly Supervised Learning. InWeak Supervision werden die Daten nicht manuell überprüft. Stattdessen werden die Daten mithilfe von schwachen Labelquellen wie einfachen heuristischen Regeln, Knowledgesbasen oder von geringwertigen Crowdsourcing-Quellen überprüft, wie in der rechten Figur gezeigt. Wenn wir das Vergleich mit menschlichenAnnotations machen, sind die schwächeren Annotations weitaus billiger, aber sie sind auch noisier, was bedeutet, dass ein gewisses Maß an Fehlern enthalten ist. Wenn wir direktes Training von neuronalen Netzen auf schwächeren Ansätzen durchführen, memorieren die neuronalen Netze die Lässigkeit und generalisieren nicht mehr. InWeakly Supervised Learning werden spezielle Trainingsalgorithmen verwendet, um robuste neuronalen Netze unter solchen Lässigkeiten zu trainieren, sodass die trainierten Modelle immer noch gut generalisieren können. In jüngeren Arbeiten im BereichWeakly Supervised Learning wird oft behauptet, dass man nur auf schwächeren Ansätzen Modellien trainiert und dabei hohen Leistungen auf sauberen Testsets erreicht. Technisch gesehen ist diese Behauptung nicht falsch, aber es gibt einen Haken, der ist, dass man davon aus geht, dass es einen zusätzlichen sauberen Validierungsset gibt, um das Modell zu auswählen. Wir können auf diesem Problemsetting nicht stoppen, aber dies impliziert, dass zusätzliche manuelle Annotierungen erforderlich sind. Wie ein Elefant im Raum wird diese Notwendigkeit oft übersehen. Die oben genannte Zweifel wird an drei Forschungsfragen gekoppelt. Erstens, ob saubere Validierungsdaten fürWeakly Supervised Learning notwendig sind, oder ob man stattdessen eine noisige Validierungsdatenbank verwenden kann? Zweitens, wenn saubere Datensätze notwendig sind, dann wie viele saubere Datensätze benötigt man? Drittens, sollte man nur die sauberen Datensätze für die Validierung verwenden, oder gibt es bessere Wege, sie zu nutzen? Wir haben in unserem Werk diese Forschungsfragen adressiert und unsere Erkenntnisse sind als Folgendes:

Erstens finden wir, dass, überraschend, recentWeakly Supervised Learning Methoden tatsächlich saubere Validierungsdaten benötigen, um korrekt zu arbeiten. Andernfalls gibt es einen großen Leistungsverlust. Wie in dieser Figur gezeigt, wenn es keine sauberen Validierungsdaten gibt, dann können die trainierten Modelle nicht darüber hinaus beyond den ursprünglichen schwachen Labels generalisieren, was bedeutet, dass das Training sinnlos ist. Das zeigt, dassWeakly Supervised Learning Ansätze tatsächlich saubere, manuell annotierte Datensätze benötigen, um korrekt zu arbeiten, und die Kosten für die Obtention von sauberen Validierungsdaten sollten nicht übersehen werden.

Unsere zweite Erkenntnis lautet darauf, dass die Anzahl der sauberen Validierungsdaten, die verwendet werden, dazu beitragen wird,Weakly Supervised Learning Ansätze besser zu perforieren, wie in der linken Figur gezeigt. Normalerweise benötigen wir normalerweise 20 Datensätze pro Klassen, um hohe Leistungen zu erzielen. Aber das ist nicht das Ende der Geschichte, denn wenn wir schon entscheiden, Zugriff auf saubere Datensätze zu erhalten, dann wird das Direkt-Finetuning (Training) auf diesen sauberen Datensätzen besser perforieren. Wie in der rechten Figur gezeigt, wenn wir 10 Datensätze pro Klassen haben, dann beginnt das Direkt-Finetuning, das Modell zu übertragen, dasWeakly Supervised Learning Ansatz zu übertragen, und es übertragen. Wie wir sehen können, wenn wir 10 Datensätze pro Klassen haben, dann übertragen Direkt-Finetuning Ansätze besser alsWeakly Supervised Learning Ansätze.

Schließlich können die Leistungsverbesserungen, die in früherenWeakly Supervised Learning Ansätzen behauptet wurden, leicht erreicht werden, indem erlaubt wird, das Modell weiterzuentrainieren auf den sauberen Validierungsdaten. Wie wir in den Figuren sehen können, der "vanilla"-Modellansatz, der FTw genannt wird, ursprünglich unterperformiert gegenüber komplexerenWeakly Supervised Learning Ansätzen, wie COSINE. Allerdings, wenn wir erlauben, das Modell weiterzuentrainieren auf den sauberen Datensätzen, dann performiert FTw ebenso gut wie andere Ansätze. Also in der Praxis gibt es keinen Grund, komplexereWeakly Supervised Learning Ansätze zu verwenden, die mehr Rechnungszeit und Speicherräume erfordern.

Insgesamt haben wir gezeigt, dass recentWeakly Supervised Learning Ansätze saubere, manuell annotierte Datensätze benötigen, um korrekt zu arbeiten. Ihre Leistungsverbesserungen und Praktizität werden stark überbewertet. Unsere konkreten Empfehlungen für zukünftige Arbeiten sind als Folgendes:

Erstens berichten Sie die Modellauswahlkriterien. Zum Beispiel, berichten Sie, ob die Modellauswahl über saubere Validierungsdaten durchgeführt wurde. Zweitens,Weakly Supervised Learning Ansätze sollten mit Few-Shot Learning Baselines verglichen werden, da beide auf sauberen Datensätzen arbeiten. Drittens, das Continuously Fine-Tuning (Training) ist ein einfach, aber starkes Baselinesatz, der in zukünftigenArbeiten inWeakly Supervised Learning beachtet werden sollte. Schließlich haben wir unser Code offengegeben. Sie können es via dem QR-Code auf dem Slideshow finden. Bitte prüfen Sie es gerne aus. Vielen Dank und genießen Sie die Konferenz.</sample>
    <sample id="241">This paper presents a human-in-the-loop evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment misinformation. The authors address the limitations of current systems by incorporating live data and human feedback throughout the process. They propose a two-component system: claim detection using a T5 model and policy violation verification using a BERT-based stance classification model. Evaluation results show that their system effectively detects unapproved treatments before they are debunked in news articles and has a high policy violation detection rate. The study highlights the importance of involving humans in the misinformation detection process to improve system efficacy and reduce workload.</sample>
    <sample id="242">Gängige Bewertungsmethoden für Dialogsysteme sind die Anwendung von Likert-Skalen und das Anfragen von menschlichen Urteilen, um zu bestimmen, welche der beiden Konversationen besser ist.</sample>
    <sample id="243">Es sind insgesamt 5 Autoren an der Arbeit beteiligt.</sample>
    <sample id="244">Im Beispiel mit Servin und Kea wird das Hintergrundwissen verwendet, dass Servin ein Richter ist und Kea ein Bäcker.</sample>
    <sample id="245">This work presents a two-step pipeline for finding high-agreement Amazon Mechanical Turk (MTurk) workers for summarization tasks. The pipeline includes pre-task qualifications, qualification tasks, and endurance tasks to categorize workers into gold, silver, bronze, and block. The results show that Pipeline workers achieved high agreement in terms of inter-annotator agreement (IAA) compared to experts, with the best Krippendorff's Alpha of 0.443. The reference-based task also showed similar performance to Pipeline workers. Baseline MTurk workers and CloudResearch MTurk workers were also analyzed, but with lower task acceptance rates and incomplete HIT coverage. The analysis of correctness across annotation sources revealed significant Spearman's correlation between Pipeline and CloudResearch workers. This work serves as a best practice for high-agreement annotations at large scale and lower cost, and future research will investigate ways to hire high-quality workers and try multiple applications for tasks, languages, and platforms.</sample>
    <sample id="246">Ja, der Code ist verfügbar und er ist auf GitHub zu finden.</sample>
    <sample id="247">Title: FACTKG: Fact Verification via Reasoning on Knowledge Graphs

Authors: Jiho Kim, KAIST AI

Abstract:

We propose a new task, Knowledge Graph-Based Fact Verification (KG-Fact Verification), which utilizes knowledge graphs as evidence for natural language claims. Our dataset, FactKG, includes claims in both written and colloquial styles, with labels for SUPPORTED and REFUTED claims. We introduce five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation.

Our dataset is constructed using DBpedia as the knowledge graph source. We compare our approach to baselines that use only claim information or the GEAR model for graph-based verification. Our results show that all baselines outperform the majority class baseline, and the GEAR model significantly outperforms other methods.

KG-Fact Verification has practical applications in tasks requiring consistency checks between knowledge graphs and natural language, such as modern dialogue systems. Our dataset and approach provide a valuable resource for developing more accurate fact verification models.</sample>
    <sample id="248">Nein, die Annotatoren für NLPositionality sind nicht in Bezug auf jede demographische Gruppe ausgewogen. Der Vortrag erwähnt, dass die Annotatoren über 1000 stammten und aus 87 verschiedenen Ländern kamen, aber es wird nicht erwähnt, dass sie in Bezug auf jede demographische Gruppe ausgewogen sind.</sample>
    <sample id="249">Die Sätze innerhalb der akzeptablen Domain wurden durch das Hinzufügen von irrelevanten Sätzen aus Wikipedia durcheinander gebracht.</sample>
    <sample id="250">Eine dimensionale Bewertung ist eine Methode, um mehrere Aspekte von Konversationsqualität zu bewerten, indem man explizit feststellt, ob ein Modell bestimmte Verhaltensweisen wie die Bereitstellung von irrelevanter Information oder Widersprüchen ausführt.</sample>
    <sample id="251">Die Autoren, Jingwei Yi und seine Teammitglieder, gehören an der University of Science and Technology of China.</sample>
    <sample id="252">This presentation introduces U-CREAT, an unsupervised case retrieval system using event extraction for legal documents. The authors, including Sai Kiran Tanikella from IIT Kanpur, have developed the IL-PCR dataset and the U-CREAT pipeline to address the challenge of prior case retrieval in the legal domain. The IL-PCR dataset consists of 7,070 legal cases with an average of 6.775 citations per query document. The U-CREAT pipeline leverages unsupervised learning techniques and an event-based approach to improve retrieval efficiency and generalization across Indian and Canadian legal systems. Experiments with various models, including count-based, transformer-based, and event-based models, demonstrate that event-based models significantly outperform baseline methods. The Event Filtered Documents model is the best-performing method, achieving higher F1 scores and lower inference times compared to other techniques. U-CREAT represents a significant advancement in prior case retrieval for legal documents.</sample>
    <sample id="253">Abstract: We present DisorBERT, a double domain adaptation model for detecting signs of mental disorders in social media. Our approach combines the knowledge learned from a general language model (BERT) with information from Reddit and mental health domains. By using guided masking, we focus on important words during training, enabling the model to capture signs of mental disorders more effectively. Our results show that DisorBERT outperforms MentalBERT, a model trained with a large amount of data, and achieves a solid balance between finding users and labeling them correctly. Future work will explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">Title: Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction

Authors: Sun Qi, Nanjing University of Science and Technology

Abstract:
We propose a document-level relation distant extraction framework with uncertainty-guided label denoising to improve the label quality of DS data. Our approach introduces uncertainty estimation to determine whether model predictions can be trusted or not. We design an instance-level uncertainty estimation method to capture uncertainty scores for overlapping relations and propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. We also develop a multi-phase training strategy to iteratively re-label the DS data. Our framework outperforms previous baselines on public datasets, demonstrating significant performance improvements in document-level relation extraction tasks.</sample>
    <sample id="255">Die Form des Prompts ist wichtig, wenn es um null- oder ein-Shot-Prompting geht.</sample>
    <sample id="257">Die Autoren haben vier state-of-the-art-Chatmodelle evaluiert.</sample>
    <sample id="258">This video introduces a new work by Chiang Cheng-Han and colleagues that explores the use of large language models (LLMs) as an alternative to human evaluation in natural language processing. The authors propose using LLMs to rate the quality of text based on instructions, aiming to overcome the instability and reproducibility issues associated with human evaluation. They conduct experiments using GPT-2 and human-written stories, rating them on grammar, coherence, likability, and relevance. The results show that larger LLMs like Davinci and ChatGPT exhibit a clear preference for human-written texts, similar to human evaluators. The video also mentions potential future research directions, such as investigating agreement between LLMs and humans, the impact of instruction wording changes, and the benefits and costs of LLM evaluations compared to human evaluations.</sample>
    <sample id="259">This paper presents XSemPLR, a unified benchmark for cross-lingual semantic parsing in multiple natural languages and meaning representations. The authors propose a comprehensive study on three representative types of multilingual language models, including Encoder-PTR and Encoder-Decoder models. They evaluate the performance of these models on nine datasets in various domains, five semantic parsing tasks, eight meaning representations, and 22 natural languages in 15 language families. The results show that Encoder-Decoder models outperform previous work or achieve comparable results, and pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages. The paper also compares the cross-language performance gap and finds that the Zero-shot setting has a significant transfer performance gap, while the Few-shot setting shortens the transfer gap rapidly. Overall, the paper provides valuable insights into the challenges and opportunities of cross-lingual semantic parsing and encourages further research in this area.</sample>
    <sample id="260">Es wird nur ein Autor, Jingwei Yi, erwähnt, der an der Arbeit beteiligt ist.</sample>
    <sample id="261">Ein guter Planer sollte Skripte schreiben, die bothem logisch sind und den Bedingungen entsprechend sind.</sample>
    <sample id="262">Die Anzahl der Autoren, die an der Arbeit beteiligt sind, wird nicht in dem gegebenen Text erwähnt.</sample>
    <sample id="263">This paper presents a novel calibration method to mitigate label biases in in-context learning for text classification tasks. The authors identify three types of label biases: vanilla-label bias, context-label bias, and domain-label bias. They propose a domain-context calibration method that uses random in-domain words sampled from the task corpus as content-free text to estimate the model's bias on each label name. Experiments show that this method significantly improves the performance of in-context learning on a wide range of datasets with varying levels of domain-label bias. The results suggest that using random in-domain words rather than single predefined tokens is more effective in mitigating label biases.</sample>
    <sample id="264">Abstract: This paper proposes a novel task named Transferable Audio-Visual Text Generation (TAVT) to address the challenge of multi-modal domain shifts in audio-visual text generation tasks. The main contribution is a modular framework consisting of an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The audio-visual meta-mapper network maps different visual concepts across domains into a unified auditory semantic space, while the transformer-based encoder and generator introduce an alpha to evaluate the contribution of different modalities to each word. Dual Counterfactual Contrastive Learning (DCLL) is proposed to directly optimize the visual-textual alignment without relying on randomly-selected negative samples. Experiments on two benchmarks based on MSVD and MSR-VTT show that TAVT outperforms all compared models on cross-datasets and cross-domain settings, even with low-resource domains.</sample>
    <sample id="265">Vasudha</sample>
    <sample id="266">Die Autoren dieser Studie zählen zu der Jagielloonska Universitetet (University of Warsaw).</sample>
    <sample id="268">Die häufigsten Fehler von PaLM sind Omissionen, also das Omiptieren von Teilen der Quellsprache bei der Übersetzung.</sample>
    <sample id="269">Hallo, ich bin James Finch. Und ich bin Sarah Finch. Heute erzählen wir über ABC-Eval, eine neue dimensional-basierte Methode zur Bewertung von conversational AI. Dieses Werk wurde vom Emory NLP Lab, geleitet von Professor Jinho Choi an der Emory University, in Zusammenarbeit mit Amazon Alexa AI erstellt. Lassen Sie uns sagen, dass Sie recently eine Dialogmodellentwicklung getan haben und es auswerten möchten, wie gut es sich gegenüber dem aktuellen Stand-der-Dinge vergleicht. Eine gemeinsame Praxis besteht darin, menschliche Evaluationen durchzuführen, indem man menslichen Richtern fragt, welche der beiden Konversationen besser ist oder sie auf einer Likert-Skala bewertet.Diese Ansätze sind gut, um einen allgemeinen Eindruck von der Qualität der Dialoge zu erhalten, aber die Qualität von Dialogen hat viele Aspekte. Daher möchten Sie eventuell mehrere Dimensionen der Chatschwelle bewerten, um die Stärken und Schwächen des Modells auf einem feinere Niveau zu verstehen. Ein Ansatz besteht darin, einfach menschlichen Richtern zu beauftragen, mehrere Dimensionen der Dialogqualität zu bewerten, indem man befragt, ob bestimmte Verhaltensweisen im Modell, wie zum Beispiel die Bereitstellung von irrelevanter Information, im Modell vorkommen. Wir glauben, dass es eine präzisere und zuverlässigere Strategie für die dimensional-basierte Evaluierung von Dialogen gibt. Unsere Ansatz versucht, die Subjektivität menschlicher Evaluationen explizit zu reduzieren, indem man feststellt, ob jedes Modellresponse bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel die Bereitstellung von irrelevanter Information oder Widersprüchen. Wir nennen diesen Ansatz Annotieren von Verhaltensweisen in Chats oder ABC-Eval in Kürze. Wir haben diesen Ansatz entwickelt, um umfassend die Verhaltensweisen zu abdecken, die in jüngster Zeit als beeinflussende Faktoren für die Chatschwelle identifiziert wurden. ABC-Eval ist in der Lage, die Häufigkeit zu messen, in der ein Chatschwelle ein bestimmtes Verhalten begreifen wird, wie zum Beispiel die Ignorierung des Partners oder die Ausgabe von irrelevanter Information, Widersprüchen mit sich selbst oder seinem Partner, die Halluzination falscher Tatsachen oder die Verletzung von allgemein akzeptierten Kenntnissen und die Erfolge oder Misserfolge des Modells, um Empathie zu zeigen. Um zu bestimmen, welche Art von Evaluation am besten geeignet ist, haben wir vier aktuelle Chatschwelle-Modelle ausgewählt und sie auf 100 menschlich-bot-Dialoge pro Modell mittels ABC-Eval bewertet. Als Vergleich haben wir auch diese Dialoge mittels drei existierenden Methoden bewertet: Likert-Bewertungen auf Turn-Ebene, Likert-Bewertungen auf Dialog-Ebene und Dialog-Ebene-Paarweisungen bewertet. Für jede dieser existierenden Methoden haben wir Evaluierungen von acht von den acht am häufigsten gemessenen Aspekten der Dialogqualität gesammelt, da dies standardmäßig für die Bewertung von Chatschwelle-Modellen auf mehreren Dimensionen getan wird. Aus our-analysis dieser Evaluierungsresultate haben wir festgestellt, dass die ABC-Eval-Verhaltenslabels insgesamt zuverlässiger sind als die von den existierenden Methoden gesammelten Labels, was durch die Inter-annotator-Übereinstimmung auf 100 doppelt bewerteten Dialogen gepräget wurde. Darüber hinaus sind die ABC-Eval-Labels besser vorhersagend für die allgemeine Dialogqualität im Vergleich zu den Metriken, die von den existierenden Methoden erzielt wurden, wie durch diese einfache lineare Regression gezeigt wird. Zum Beispiel können Sie sehen, wie die Messung der Häufigkeit von Widersprüchen mit sich selbst und seinem Partner 5% und 10% der Dialogqualität respectiv erklären, während die durchschnittlichen Likert-Konsistenzscore 4% oder weniger erklären. Schließlich haben wir überprüft, ob jede Evaluiermetrik ein eindeutiges Aspekt der Dialogqualität abdeckt, indem wir eine Schritt-für-Schritt lineare Regression durchführten. Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Dialogqualität erklärt und, wenn Sie eine Metrik nach der anderen entfernen, die meisten davon resultieren in einem Verlust an Informationen über die Qualität. Auf der anderen Seite erklären die Kombination aller Turn-level Likert-Metriken weitaus weniger Qualität und fewer davon Metrien tragen ein eindeutiges Informationsangebot. Diese zuverlässigen, informative und eindeutigen ABC-Eval-Metriken ermöglichen uns, conversational AI mit einer Härte zu bewerten, die vorherige Methoden nicht erreichen können. Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen noch bestehen und exakt quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, around 20% ihrer Antworten mit Fehlern im allgemeinen Wissen. Sie produzieren irrelevantes Information in around 15% der Antworten und sie produzieren Widersprüche mit sich selbst oder ihrem Partner in around 10% der Fälle. Mit dem raschen Tempo des Fortschritts im Feld können diese Fehlerraten bei neuen Modellen, die seit unserem Evaluationsprototypen freigekommen sind, abnehmen. Aber das ist noch mehr Grund, die zuverlässigen und präzisen Evaluiermetrischen zu verfolgen, um Modelle zu vergleichen. Wir hoffen, ABC-Eval kann von anderen im Feld verwendet werden, um einen wichtigen Schritt in diese Richtung zu machen. Und wir freuen uns darauf, wie conversational AI im kommenden Monat und Jahr fortschrittsschaffen wird. Vielen Dank für das Watching.</sample>
    <sample id="270">Die Autoren gehören an Emory University.</sample>
    <sample id="271">CFT stands for Continuous Fine-Tuning.</sample>
    <sample id="272">There are seven authors involved in the work.</sample>
    <sample id="273">Hallo, mein Name ist Kayo Yin und ich werde unsere Arbeit präsentieren, die "Wann benötigt Übersetzung Kontext?"題名の下行われます。この研究は、Patrick Fernandes、Emmy Liu、André F. T. Martins und Graham Neubigと collaborate されました。翻訳は多分に文脈に依存します。例えば、この文章の「mole」をどのように翻訳するか？前文章が「政権が発見されたら危険な状況が発生する可能性があります」とある場合、「mole」はスパイを指します。しかし、前文章が「医師、これって SERIOUS なことが起こる可能性があるとおっしゃったの？すると、」とある場合、「mole」は胎記を指します。つまり、文脈によって単語の意味が変わることから、その翻訳も変わります。ただし、文脈に依存する翻訳を評価することは、 corpus-level metrics がそれらを捉えるのに十分な量の翻訳が限られるため、非常に難しいです。一部の人々は、文脈に依存する翻訳のためのターゲット評価を提案していますが、これらのリソースは通常、分野の知識と人間の編纂に依存するため、制限された種類の文脈に依存する翻訳と制限された言語セットをサポートします。この研究では、これらの二つの質問に回答します。第一に、翻訳が文脈に依存する場合と、モデルがこれらの状況をどのように処理するかをどのように評価するか？この最初の質問に答えるために、 CXMI と呼ばれる Measure for Context Usage by Machine Translation Models によって測定された文脈の使用量を測定しました。 CXMI は、文脈 C が、ソース X からターゲット Y に与える情報を測定するために使用されます。 CXMI は、モデルに文脈を与えることで得られる情報量を表します。この研究では、CXMI を Sentence Level または Word Level で測定する Pointwise CXMI に拡張します。文語が必要な文語として、高 P-CXMIを持つ単語を見つけることができます。次に、分析を実行し、高 P-CXMIを持つ単語のパターンを見つけるために、分析を実行します。最後に、特定の Token に高 P-CXMIを持つ単語を見つけることができます。最後に、分析の結果を使用して、文書水準の翻訳のための基準を設計します。五つの文法現象について、各々のタグラーを作成し、特定の文法現象に属する単語を自動的に識別します。このタグラーは、MuDA (Multilingual Discourse-Aware)タグラーと呼ばれます。次に、指定した並列コーパスに適用し、MuDAタグラーによって識別された文書水準の翻訳の文書水準の例に適用する翻訳評価の選択肢を選択します。最後に、当たる Benchmark とその他の評価尺度を使用して、ドキュメント水準の機械翻訳のためのモデルを評価します。 Corpus-level metrics である BLEU では、文脈無視のモデルが最高の性能を示します。ただし、COMETを使用すると、文脈に依存するモデルが最高の性能を示します。最後に、単語 f-measureを使用すると、文脈を持つモデルと文脈を持たないモデルの性能が類似です。これにより、文書水準の翻訳システムの最適な評価尺度が corpus-level metrics 単独で決定することは示されます。 MuDA Benchmarkを使用してモデルを評価すると、文脈に依存するモデルは、形式や文法 cohesion 等の特定の文法現象に対して、文脈を持たないモデルよりも顕著に正確です。ただし、文法現象如し、冠詞、冠詞形式、冠詞形式等は、文脈を持たないモデルよりも、文脈を持つモデルよりもはるかに優れています。この結果は、文書水準の翻訳の進歩が必要な分野を示します。最後に、DeepL と Google Translate 等の不同的な商業システムと比較し、 Benchmark は、DeepL が一般的に Document-level translation において Google Translate よりも正確であることを示します。要約すると、14 ペアの言語を横断したデータ駆動的分析を行い、翻訳が文脈に依存する場合と、文脈に依存する翻訳のための Benchmark を構築し、文書水準の翻訳の文書水準の翻訳能力を評価します。翻訳能力を評価するための Benchmark は、文書水準の翻訳の文書水準の翻訳能力を評価するための Benchmark と比較します。</sample>
    <sample id="274">Yusen Zhang</sample>
    <sample id="276">This work presents IndicMT Eval, a dataset for evaluating machine translation metrics in Indian languages. The authors focus on five languages from two language families: Dravidian (Tamil and Malayalam) and Indo-Aryan (Hindi, Marathi, and Gujarati). They select 200 sentences from the Flores dataset and generate multiple candidate translations for each sentence using seven different translation models. Human annotators evaluate the translations, marking errors and providing overall scores. The study compares various evaluation metrics, including overlap-based, embedding-based, and COMET metrics, and fine-tunes COMET using their MQM dataset. Results show that IndicCOMET MQM outperforms COMET baselines on most languages and is more robust than COMET on unseen languages.</sample>
    <sample id="277">Die neue Methode hat keinen Namen.</sample>
    <sample id="278">Die Methode der "markierten Wörter" basiert auf dem soziolinguistischen Konzept der "Markedness", das besagt, dass dominante Gruppen in Gesellschaft sowohl sprachlich als auch sozial unmarkiert sind, während diskriminierte Gruppen markiert sind. Die Autoren initially designateen die unmarkierten und markierten Gruppen und vergleichen dann die Personas mit dem Fightin' Words-Methode, die gebieterische Log-ODDS-Größen verwendet, um die Top-Wörter für jede markierte Gruppe zu vergleichen.</sample>
    <sample id="279">Die Autoren gehören an der University of Washington.</sample>
    <sample id="280">Title: MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations

Abstract: Emotion regulation in conversations aims to predict the emotion label of each utterance, incorporating textual, audio, and visual modalities. This paper introduces MultiEMO, a novel attention-based correlation-aware multimodal fusion framework addressing challenges in existing methods such as inadequate exploitation of multimodal complementarity, unsatisfactory performance on minority emotion classes, and difficulty distinguishing between semantically similar emotions. MultiEMO comprises four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. It features VisExtNet, a novel visual feature extractor, and MultiAttn, a multimodal fusion model using bidirectional multi-head cross-attention layers. Additionally, it employs Sample-Weighted Focal Contrast loss to improve classification of minority and semantically similar emotions. Experimental results demonstrate state-of-the-art performances on MELD and IEMOCAP datasets.</sample>
    <sample id="281">This work, titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration," investigates the role of context in translation and evaluates how well machine translation models handle context-dependent translations. The authors introduce Pointwise CXMI, a measure for context usage at the sentence or word level, to identify words that require context for translation. They analyze transcripts of TED talks translated into 14 languages and identify discourse phenomena such as dual pronouns, proper noun translation, formality, and ellipsis resolution that require context. The MuDA (Multilingual Discourse-Aware) tagger is designed to automatically identify these phenomena, and its performance is used to evaluate different machine translation models. The results show that context-aware models outperform context-agnostic models for certain discourse phenomena, but not all, suggesting areas for future improvement in document-level translation.</sample>
    <sample id="282">Title: StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing

Authors: Xuekai Zhu, [Other authors]

Conference: ACL 2023

Abstract:

This paper presents a novel approach to non-parallel text style transfer at the story level, addressing the challenge of imitating author linguistic preferences, including discourse structures. The proposed StoryTrans model learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. A new training objective is introduced to reduce stylistic features from discourse representations and enhance content preservation through two-stage generation. Experiments on Chinese and English datasets demonstrate that StoryTrans outperforms strong baselines in style control and content preservation, aligning with golden texts in the style feature space. The model can supplement short phrases or plots to enrich storylines while maintaining source semantics.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction

This paper presents a novel fuzzy span mechanism, FSUIE, for enhancing universal information extraction (UIE). The current span-based UIE models rely on precise boundary positions of annotated spans, which can lead to ambiguity in labeling. To address this issue, FSUIE proposes a fuzzy span boundary that represents the target boundary as a continuous distribution of correct probability within a specific range. This approach alleviates the model's reliance on span boundaries and improves its ability to handle ambiguous annotations.

The proposed method uses an adaptive attention mechanism to model the furthest span boundary, which is represented as a continuous distribution of correct probability. The boundary distribution predicted by the module is calculated using binary cross entropy with golden boundary as BCE loss and adding KL-divergence between predicted boundary and fuzzy span boundary. To obtain a more reasonable attention distribution for span extraction, FSUIE introduces a fuzzy span attention as a mask function to trim attention distribution dynamically.

Experiments on three main information extraction tasks, including named entity recognition, relationship extraction, and aspect sentiment triplet extraction, demonstrate the capability of FSUIE. The results show significant performance improvement compared to UIE-base without a fuzzy span mechanism, especially on small-scale data size. FSUIE achieves new state-of-the-art results on datasets ACE2004, 2005, and ADE, as well as ASTE tasks on 14lap, 15res, and 16res of AST-V2 dataset, and demonstrates competitive performance on 14res datasets.</sample>
    <sample id="285">This video presents the work of Mingqi Gao from Peking University on benchmarking factual error correction for dialogue summarization. The video highlights the importance of correcting factual errors in dialogue summaries and argues that current FEC models have flaws in their evaluation methods. The video proposes a new taxonomy of factual errors and introduces a manual annotation method to address these issues. The video also explores the performance of FEC models in different training modes and finds that introducing human-corrected summaries during training can improve their performance. The video concludes by highlighting the challenges of current FEC models in correcting factual errors like addition and addressing attribute errors, modality errors, link errors, etc.</sample>
    <sample id="286">James Finch</sample>
    <sample id="287">There are four authors involved in the work: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">Die Datensätze, die zum Testen syntaktischer Phänomene verwendet werden können, sind BLiMP, SyntaxGym und CrowS.</sample>
    <sample id="290">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind COSINE, FTw, MTL, MTL+, und MTL++.</sample>
    <sample id="291">Das Modell wird anhand von Aufgaben wie Named Entity Recognition, Klassifizierung, Part-of-Speech tagging und Question Answering evaluiert.</sample>
    <sample id="294">CamemBERT ursprünglich wurde mit den NACHOS-Daten trainiert.</sample>
    <sample id="295">Der Referent in diesem Text ist Adam Przepiórkowski.</sample>
    <sample id="296">Abstract: This work presents a collaboration between the University of Turin and Amazon Alexa on natural language understanding, specifically focusing on irony detection. The researchers developed the EPIC corpus, consisting of 300 short conversations from social media, Reddit, and Twitter, annotated by 74 annotators for five varieties of English. They created perspective-aware models by fine-tuning pre-trained language models on different annotator splits. While raw performance showed no significant trends, the perspective-aware models demonstrated higher confidence in their predictions. The researchers also found that annotators from similar age groups or geographical regions had more disagreement in their annotations.</sample>
    <sample id="297">This project develops a typology and glossary of dogwhistles, particularly racist, transphobic, and anti-Semitic ones. It includes over 340 terms and symbols from various sources, mostly US-centric. The project also conducts a case study on historical U.S. political speeches, showing that the frequency of racial dogwhistles correlates with the Republican Southern Strategy post-Civil Rights era. Additionally, it evaluates language models like GPT-3 in recognizing dogwhistles and their performance in surfacing them. The project concludes by demonstrating how dogwhistles can evade content moderation through toxicity detection, as automated systems rate sentences with dogwhistles less toxic than those with standard group labels or slurs.</sample>
    <sample id="298">Die Hauptursache für den Leistungsverlust war die zeitliche Verzögerung, da das Experiment gezeigt hat, dass die Leistung mit einer größeren zeitlichen Distanz zwischen dem Train- und Test-Dataset abnimmt.</sample>
    <sample id="299">This work proposes a training method to improve the robustness of NLI models by reducing their reliance on shortcuts. The key insight is that NLI models suffer from poor performance on under-represented "hard" training instances with patterns that could contradict the shortcuts in dominant "easy" examples. The proposed method uses a minimax training objective between a learner and auxiliary model, where the learner tries to minimize the loss of the NLI task, while the auxiliary generates example weights such that the learner is incentivized to concentrate on ranges of the input space where it incurs high losses. Both models are optimized in an alternating fashion using any standard optimization algorithm. At test time, the learner can make predictions without relying on the auxiliary. The method does not make assumptions about the type of shortcuts contained in a dataset and relies on the learner's own training dynamics to generate example weights. The paper evaluates the proposed method in three commonly used analytic datasets and their corresponding out-of-distribution adversarial test sets, observing consistent improvements in out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">Interactive dictation is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. This work introduces a new task called interactive dictation, which allows for flexible interleaving of dictation and editing without trigger words or commands. The system uses intuitive and open-ended natural language utterances to specify edits. A baseline system was created to perform each of the four steps involved in the task: ASR recognition, segmentation, command extraction, and execution. The results show that GPT-3 models are more accurate but also much slower, while predicting programs allows for significant improvement in efficiency with minimal impact on accuracy.</sample>
    <sample id="302">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, um die richtige Reihenfolge der Ausgabetoken zu bestimmen. Nachdem die Modelldatei die richtigen Token ermittelt hat, müssen sie in die richtige Reihenfolge geordnet werden, um einen sinnvollen Ausdruck zu erhalten.</sample>
    <sample id="303">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparent machen sollten, um zu vermeiden, dass positive Stereotype und essentialisierende Erzählnisse entstehen und um sicherzustellen, dass die Studienmethoden nicht unbekannte oder negative Auswirkungen haben.</sample>
    <sample id="304">Inakzeptable Minimalpaaringaben sind Sätze, die grammatikalisch falsch sind.</sample>
    <sample id="305">Abstract: This video presents our recent work on weakly supervised learning (WSL), which aims to train neural networks using noisy, weakly labeled data. We address the common claim that WSL methods can achieve high performance on clean test sets without requiring clean validation data. Our research finds that clean validation samples are necessary for WSL approaches to work properly, and that increasing the number of clean samples can improve performance. We also show that fine-tuning on clean samples can achieve similar performance gains as more complex WSL methods. Our recommendations for future work include reporting model selection criteria, comparing with few-shot learning baselines, and considering continuous fine-tuning as a simple yet strong baseline.</sample>
    <sample id="306">Abstract: Entity tracking is a crucial ability for understanding longer discourses, but there haven't been systematic investigations into what pre-trained language models can actually perform such tasks. In this paper, we designed an evaluation task involving boxes and objects to find the entity tracking abilities of large language models. We tested Flan-T5 and GPT-3 and -3.5 models using 2-shot in-context learning. Our experiments show that most models simply repeat the initial state, while only text-davinci-003 exhibits non-trivial tracking. We found that pre-training on code is responsible for making this capacity surface in pre-trained language models. Smaller models like T5-base can learn to perform entity tracking if directly fine-tuned, but randomly initialized models of the same architecture cannot learn our state tracking task even when they receive direct supervision.</sample>
    <sample id="307">Die Autoren haben diePerformances der verschiedenen Modelle auf public and private downstream tasks wie Named Entity Recognition, Classification, Part-of-Speech Tagging und Question Answering bewertet. Sie haben auch Baseline-Modelle wie CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT verwendet zur Evaluation.</sample>
    <sample id="308">This presentation by Jenny, a first-year PhD student at Carnegie Mellon University, explores the concept of positionality in NLP datasets and models. Positionality refers to the perspectives held by NLP researchers and developers based on their demographics, identity, and life experiences, which can influence research outcomes. The presentation highlights the need to study model and data set positionality as NLP tasks become more subjective and socially oriented.

The presentation introduces NLPositionality, a framework developed by Jenny and her team to compare end users with existing datasets and models. The framework involves re-annotating data sets with diverse annotators and comparing annotations with models using Pearson's R correlation score. The study involved over 16,000 annotations from 1000 annotators from 87 countries.

The results show that there is positionality in NLP, with data sets and models being most aligned to English-speaking countries and people with college education. However, some populations are inevitably left behind, such as non-binary individuals. To address this issue, the presentation recommends keeping a record of all relevant design choices, conducting NLP research with a perspectivist lens, and building specialized datasets and models within specific communities.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations.</sample>
    <sample id="310">Wikipedia wurde gewählt, um völlig unzweckmäßige Sätze den inakzeptablen und akzeptablen Suchanfragen hinzuzufügen.</sample>
    <sample id="311">Die Autoren gehören an der Technischen Hochschule Wien (TU Wien) und der Johannes Kepler University Linz.</sample>
    <sample id="312">MultiInstruct ist der erste multi-modal instruction tuning Benchmark, der 62 diverse multi-modal Aufgaben über 10 breite Kategorien enthält. Es basiert auf 21 bestehenden offenen Quellen Datensätzen und each Aufgabe wird mit 5 Experten geschriebener Anweisungen ausgestattet. Es ist auch der größte multi-modal instruction tuning Datensatz, der zur Verfügung steht.</sample>
    <sample id="313">Die Arbeit wurde von mehreren Autoren im Emory NLP Lab geleitet von Professor Jinho Choi und in collaboration with Amazon Alexa AI.</sample>
    <sample id="314">Die Definition der binären Koordination lautet, dass die zwei Teile (Konjunkte) des Koordiantionsstrukturgleiches in einem Binärrelationstyp stehen und dadurch eine Abhängigkeit zwischen den beiden Teilen entstehen.</sample>
    <sample id="315">Die Dauer der verwendeten Prompts in dieser Studie wurde nicht erwähnt.</sample>
    <sample id="316">Die Auswirkungen der Ergebnisse auf das kleinere T5-Modell sind, dass es die Qualität der generierten Skripte überwiegen kann, wenn es korrekt trainiert wird. Das T5-Modell, das auf CoScript trainiert wurde,生成了比大多数大型语言模型更好的脚本。</sample>
    <sample id="317">This paper presents a new approach to information extraction using large code generation models. The authors propose transforming the text-to-structured information extraction task into a structure-to-structure code generation task, and use code large language models like Codex to perform it. They evaluate their method on three named entity recognition datasets and four relation extraction datasets, comparing the performance of two types of prompts: traditional text-style prompts and code-style prompts. The results show that the proposed approach using code language models and code format prompts significantly outperforms traditional baseline models in terms of recall. The authors also observe that perplexity computed on text format inputs using models like T5 is generally higher than that of code format samples using models like CodeT5. Additionally, they find that using GPT-3 for information extraction tasks outputs labels that were not present in the predefined label set, whereas using Codex and code format prompts almost eliminates such errors. Overall, this paper provides a promising new direction for information extraction using large code generation models.</sample>
    <sample id="318">Hallo, ich bin Yanis Labrak und ich werde Ihnen heute unsere Arbeiten über "DrBERT: Ein robustes prä-gelerntes Modell in Französisch für biomedizinische und klinische Domänen" präsentieren. Zunächst diskutieren wir über Sprachmodellierung im Gesundheitswesen. Danach präsentieren wir den Hauptbeitrag unseres Artikels. Wir-introduzieren das erste biomedizinische Modell in Französisch namens DrBERT, das auf RoBERTa basiert und trainiert wurde an NACHOS, einer Datensammlung medizinischer Webseiten. Wir haben auch einen Vergleich von Modellen mit mehreren prä-gelernten Einstellungen und Datensätzen vorgestellt. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen Downstream-Aufgaben in Französisch. Schließlich conclude wir über die Experimente und geben Ihnen weitere Details, wie Sie diese Modelle zugreifen können. Seither seine Veröffentlichung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze zur Bewältigung von natürlicher Spracheingabe geworden und hat enormen Leistungsverbesserungen gegenüber historischen statischen und kontextualisierten Methoden wie Word2vec, FastText usw. gezeigt. Seitdem wurde dieser Ansatz in viele andere Sprachen, wie in Französisch mit CamemBERT, und auch in Domänen wie biomedizin mit PubMedBERT und BioBERT und auf klinischem mit ClinicalBERT, verwendet. Spezialisierte Modelle für andere Sprachen sind seltener und oft aufgrund des Mangels an in-domain-Daten auf Basis von Kontinuierlichem Pre-Training abhängig. Allerdings gab es bislang keine offene Quellen-Modelle für biomedizin auf Französisch. Wir fragten uns daher, welche Datensätze am besten für eine breite Anwendung geeignet sind und ob krawelte Data eine gute E subsitution für klinische Data darstellt. Um diese Frage zu beantworten, verglichen wir DrBERT mit unserem ChuBERT-Modell, das auf anonymisierten Daten aus dem Datendienst des Krankenhauses der Stadt Nantes basiert. Danach fragten wir uns, wie viel Daten wir benötigen, um ein spezialisiertes Modell auf Französisch zu trainieren. Ist es 4 GB, 8 GB oder mehr? Um diese Frage zu beantworten, trainierten und verglichen wir vier von Grund aus anfangende Modelle: eine erste Version von DrBERT mit 7 GB von NACHOS; eine zweite Version mit 4 GB von NACHOS; eine erste Version von ChuBERT, das ein klinisches Modell mit 4 GB von Sätzen aus klinischen Notizen ist; und eine finale Version von ChuBERT mit einer Mischung von 4 GB von NACHOS und 4 GB von klinischen Notizen. Neben dieser Vergleichsanalyse haben wir drei Modelle trainiert auf Kontinuierlichem Pre-Training, um die Auswirkungen von Pre-Training-Strategien zu analysieren. Eine davon basiert auf dem Gewicht von CamemBERT und wurde auf einem 4 GB-Subset von NACHOS trainiert. Eine weitere basiert auch auf CamemBERT, aber wurde diesmal auf 4 GB von klinischen Notizen trainiert. Schließlich basiert eine auf einem englischen biomedizinischen Modell PubMedBERT und wurde auf 4 GB von NACHOS trainiert. Insgesamt haben wir insgesamt sieben Modelle trainiert. Um unsere sieben Modelle zu bewerten, sammelten wir Daten für öffentliche und private Downstream-Aufgaben wieNamed Entity Recognition, Klassifizierung, Part-of-Speech tagging und Question Answering.Diese Modelle wurden mit sechs Baseline-Modellen verglichen, die CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT und ClinicalBERT sind. Die Evaluation zeigt, dass die Modelle am besten auf Aufgaben performen, bei denen die Daten dieselben Natur sind, auf denen das Modell trainiert wurde. Wir können jedoch beobachtet haben, dass Daten aus heterogenen Quellen besser vielseitig sind. Wir können auch beobachtet haben, dass die Verwendung mehrer Datensätze zu besseren Leistungen führt. Im Allgemeinen scheint die von Grund aus anfangende Pre-Training zu höhere Leistungen auf meisten Aufgaben zu erzielen. Allerdings haben unsere Experimente mit Kontinuierlichem Pre-Training, die auf dem Gewicht und Tokenisierung von CamemBERT trainiert wurden, die 4 GB-Subset von NACHOS, ähnliche Resultate zu jenen erhalten, die durch DrBERT 4 GB von Grund aus anfangen erhalten werden. Das ist nicht der Fall für das Modell, das auf dem Gewicht und Tokenisierung von CamemBERT basiert, das unter Instabilitätsschwierigkeiten zu leiden scheint. Als Schlussfolgerung haben unsere korrekt prä-gelernten Modelle besser auf neun von 11 Downstream-Aufgaben performt und über globale Resultate die allgemeine Modell, hier CamemBERT, übertrifft. Wir bemerkten auch, dass mehr spezialisierte Daten besser sind, aber nicht skalierbar sind. Alle vor Trainingsmodell, die aus NACHOS stammen, sind frei auf Hugging Face und unter der MIT-Lizenz verfügbar, und alle Trainingsskripte sind auf unserem GitHub-Repository zu finden. Also danke für die Präsentation und wir freuen uns auf die Besprechung im Poster-Session in Toronto.</sample>
    <sample id="319">Die Arbeit untersucht die Auswirkungen verschiedener Lernstrategien auf die Leistung von Modellen im Bereich Medizin und Gesundheit. Dazu gehören die Vorgehensweisen "from-scratch" (von Grund aus), "continual pre-training" (fortlaufende vorherige Schulung) und die Nutzung von unterschiedlichen Datensätzen, wie NACHOS und anonymisierten Daten aus einem Krankenhausaufzugriff.</sample>
    <sample id="320">Die Überanpassung, die auf die Wiederverwendung von Tests zurückzuführen ist, wurde als "nicht beobachtet" beurteilt.</sample>
    <sample id="321">Die Qualität der Vereinfachung wurde anhand von Typen der Vereinfachung bewertet, wie z.B. Lexikalversubstitution, Strukturanordnung, allgemeine Niveau der Vereinfachung usw.</sample>
    <sample id="322">Abstract: This paper presents a study on how language models learn about morality in text. The authors apply explainable AI techniques to understand what language models learn when trained to classify morality in text. They use the Moral Foundation Twitter Corpus, which contains 35,000 tweets from seven different domains, including #AllLivesMatter and #BlackLivesMatter. The study explores whether language models can recognize that morality is expressed differently across different domains. The results show that language models do recognize differences in moral expression between domains, such as the difference between ALM and BLM. However, the study also highlights the need for domain-specific models to avoid misunderstandings of morality.</sample>
    <sample id="323">This paper presents a new approach for Commonsense QA that combines language models and knowledge representation learning. The proposed method, DHLK, builds an HKG based on multiple knowledge bases using a two-stage pruning strategy and KRL to optimize the structure and knowledge representation of the HKG. The language model is then used to encode and fuse the QA context and entities, building the HKG. Dynamic entity removal based on attention weights is also introduced to improve the relevance of the subgraph. Finally, the HKG path information is incorporated into the QA context to enhance its embedding representation. Experiments on CommonsenseQA and OpenBookQA show that DHLK achieves good results compared to other LM and HKG methods.</sample>
    <sample id="324">Ja, Sprachmodelle können unterschiedliche politische Vorurteile aufweisen.</sample>
    <sample id="325">Hallo! Mein Name ist Matthias Lindemann, und heute werde ich Ihnen einen kurzen Einführung in unser Papier über "Kompositionales Generalisieren ohne Bäume unter Verwendung von Multiset Tagging und latenten Permutationen" geben. Dies ist eine gemeinsamearbeit mit meinen Betreuern Alexander Koller und Ivan Titov. Kompositionales Generalisieren kann als die Fähigkeit eines Lerners verstanden werden, um tieferes Rekursion und unbekannte Kombinationen von Phrasen zu bewältigen, die während des Trainings individuell gesehen wurden. In Kontext der semantischen Analyse sieht das Testen für kompositionales Generalisieren so aus: Wie üblich haben wir ein Trainingssatz von Aussagen. In diesem Fall: "Die Mädchen schlafen." Und "Mary wusste, dass die Mädchen schlafen."Diese Aussagen werden mit logischen Formen abgebildet, die die Kernaspekte ihres Bedeutungsviels repräsentieren. Im Gegensatz zu standardmäßigen Maschinelles Lern-Evaluation-Methoden enthält der Testset nicht aus dem gleichen Datensatz, sondern enthält strukturell unbekannte logische Formen. In diesem Beispiel hat das Modell Shallow-Recursion während des Trainings gesehen und wird auf ein Beispiel mit tieferem Rekursionstestiert. Naive Seq2Seq-Modelle kämpfen mit dieser Art von out-of-distribution-Generierung und produzieren oftmals Ausgaben, die abgetrennt sind vom Input. Insbesondere scheitern sie darin, die systematischen Korrespondenzen zwischen Input und Output zu reproduzieren, wie sie in der Farbcodierung im Beispiel abgebildet sind. Ein beliebtes Verfahren, um sich diesem Problem zu entziehen, ist die Integration von Bäumen in die Modelle. Die Bäume sollen die kompositionale Beziehung zwischen Aussagen und logischen Formen capturieren. Dies funktionsweise, aber es sind normalerweise keine und müssen durch einen bestimmten formalistischen Vorgang erhalten werden. Das kann kompliziert und manchmal computergesamt kostspielig sein. Normalerweise impliziert dies spezielle Grammatik-induktionsprozeduren. In unserem Papier verwenden wir keine Bäume und Introducing ein neuronaler Seq2Seq-Modell, das direkter die Korrespondenz zwischen Fragmente des Inputs und Fragmente des Outputs modelliert.Für die ersten SchrittePredict die Ausgabe vom Input: Wir kennzeichnen jede Eingabe mit einem unsortierten Multiset von Token, die im Ausgang auftreten werden. Nach dem ersten Schritt haben wir alle richtigen Token, aber sie sind nicht sortiert. Deshalb benutzen wir im nächsten Schritt noch ein anderes Modell, um die Permutation zu predicten, um sie in die richtige Reihenfolge zu sortieren.Wir Introduce ein neues Verfahren, um die Permutation zu predicten, das keinerlei harten Beschränkungen auf die möglichen Permutationen anwendet. Dies macht unser Ansatz sehr flexibel und ausdrucksstalent. Konzeptionell arbeitet unser Permutationsmodell so: Wir laufen von links nach rechts über den Ausgang und bestimmen, welches Multiset Token wir in jede Position im Ausgang platzieren. Für die erste Ausgabeposition auswählen wir einfach eins, wie in roter Farbe hervorgehoben. Dann springen wir zu einem anderen Multiset Token, um die zweite Ausgabeposition zu bestimmen. Wir bestimmen die dritte Ausgabeposition in einem ähnlichen Weise, indem wir zu einem anderen Multiset Token springen. Wir fortfahren diesen Prozess, bis jedes Token aus dem ersten Schritt exakt einmal besucht wurde.Um Ihnen einen Vorschuss auf die experimentellen Ergebnisse zu geben, hier eine Visualisierung unseres Verfahrens im Vergleich zu anderen treeless Modellen auf dem COGS Benchmark. Unser Modell übertrifft die anderen um ein deutliches Maß an Generalisierbarkeit zu tieferem Rekursion.Ein paar weitere Arten von struktureller Generalisierbarkeit bleiben schwierig zu bewältigen. In unserem Papier lösen wir mehrere interessante technische Herausforderungen. Zunächst einmal ist die Altersordnung zwischen Input und Ausgang im Trainingsdatensatz nicht gegeben. Als Folge kennen wir für ein Token nicht, welches Multiset es stammt, was eine Herausforderung für die Schulung darstellt. Manchmal gibt es mehrere Permutationen, die mit den Daten konsistent sind, aber die linguistisch korrekte ist latent. Wir adressieren das, indem wir die Altersordnung als Teil der Schulung induzieren.Unser Permutationsverfahren ist sehr flexibel, aber es bringt die Herausforderung, dass die Bestimmung des höchsten Punktschwemmens NP-schwer ist. Das liegt daran, dass es sich um die "Reisekursproblem" handelt. Wir approximieren dies mit einer GPU-friendlichen kontinuierlichen Relaxation, die auch die Möglichkeit bietet, durch die Lösung zu backpropagieren und linguistisch plausible Permutationen zu lernen. Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen adressieren möchten, bitte schauen Sie sich unser Papier an oder besuchen Sie unser Poster.</sample>
    <sample id="326">Kognitive Dissonanz ist die Inconsistenz zwischen zwei Glaubens oder Handlungselementen, wie zum Beispiel wenn jemand behauptet, dass Zigaretten tödlich sind, aber dann doch eine Zigarette nach dem Meeting raucht.</sample>
    <sample id="327">Title: ManagerTower: Aggregating the Insights of Uni-modal Experts for Vision-Language Representation Learning

Authors: Xiao Xu, MSRIC group, Intel Cognitive Computing Group

Abstract: We propose ManagerTower, a novel vision-language (VL) modal architecture that adaptsively aggregates insights from pre-trained unimodal experts at different levels. Unlike BridgeTower, which connects multiple top unimodal layers with each cross-modal layer in a layer-by-layer fashion, ManagerTower uses managers to gather and combine insights from multiple unimodal representations in each cross-modal layer. Our approach allows for more effective exploitation of different levels of universal semantic knowledge, leading to superior performance on various downstream tasks, especially on the Wikivideo test standard, where we achieve a 39.15% accuracy improvement. We demonstrate that ManagerTower outperforms many base-size models pre-trained on 4 million data and surpasses some models trained with more data or parameters.</sample>
    <sample id="328">GPT-4 ist das am meisten links stehende Sprachmodell.</sample>
    <sample id="329">This work presents a method for generating structured pseudo-labels for noise-resistant zero-shot video sentence localization. The authors propose using a pre-trained image caption model to generate complex free-form pseudo-queries, and then use a pre-trained model to measure the relevance between individual frames and pseudo-queries to generate pseudo-events. They also reduce the weight of noisy samples and create noisy labels to reduce the influence of label noise. Experiments on two datasets show that their method outperforms existing methods on most metrics.</sample>
    <sample id="330">Ja, Kumulatives Training ist besser als Iteratives Training für aktives Lernen.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">Die Daten für die MuDa-Benchmark stammen aus Transcripts von TED Talks, die von Englisch in 14 verschiedenen Sprachen übersetzt wurden.</sample>
    <sample id="333">Abstract: We propose a novel training framework, INK, to enhance the generalization and performance of neural machine translation (NMT) models. INK injects kNN knowledge into NMT by smoothing predictions according to nearest neighbors in the representation space. Our approach consists of two steps: extracting kNN knowledge from a datastore to guide an adapter to adjust representations, and refreshing the datastore asynchronously with updated representations. Experiments on the WMT’19 German-English news translation task show that INK outperforms state-of-the-art kNN-MT systems, achieving an average gain of 1.99 COMET score and 1.0 BLEU score. INK achieves higher BLEU scores with less memory space and faster inference speed, demonstrating its effectiveness in refining the representation space of NMT models.</sample>
    <sample id="335">Der Referent in diesem Paper ist Matthias Lindemann.</sample>
    <sample id="336">Sprachübergreifender Transfer bezieht sich auf die Fähigkeit, ein Modell in einem Sprachenblock zu trainieren und es dann in einem anderen Sprachenblock anzuwenden. In diesem Fall handelt es sich um die Anwendung von Modellen, die in einem bestimmten Sprachenblock trainiert wurden, um die Semantik von Abfragen in einem anderen Sprachenblock zu interpretieren.</sample>
    <sample id="337">This paper presents a graph-based approach for out-of-vocabulary (OOV) word embedding learning. The proposed method leverages word formation and association to infer the meaning of OOV words by constructing a Word Relationship Graph that imitates lexical rules. Each node in the graph represents a word or wordpiece, with its corresponding word embedding serving as the node attribute. A self-attention network assigns attributes to OOV nodes based on their characters, and two levels of Graph Attention Network are used to capture important information and reduce noise. The model is trained using contrastive learning with NT-XENT positive samples and achieves superior performance in both intrinsic and extrinsic tasks. The model can be applied to various languages, including agglutinative languages like English, but fusional languages present more challenges.</sample>
    <sample id="338">This research presents a unified structure for evaluating the quality of human natural language explanations in various tasks. The authors propose a novel metric called TREU, which extends the simulatability score by considering the helpfulness of explanations during fine-tuning. They evaluate human explanations across five datasets using T5 and BART models, demonstrating that their metric outperforms simulatability scores in reflecting the utility of explanations. The study highlights the task-dependent nature of explanations and the importance of high-quality human collaboration in annotation jobs.</sample>
    <sample id="339">Die Autoren gehören an der Saarland University in Deutschland.</sample>
    <sample id="340">Abstract: We introduce ParaAMR, a large-scale dataset of syntactically diverse paraphrases generated using AMR back-translation. Our approach leverages AMR graphs to capture the abstract meaning of sentences and modify them to generate diverse paraphrases while preserving semantic similarity. Compared to existing datasets, ParaAMR achieves higher syntactic diversity scores without sacrificing semantic similarity. We demonstrate the benefits of ParaAMR in various NLP applications, including sentence embeddings, syntactic control paraphrase generation, and few-shot learning. Our dataset is available for use.</sample>
    <sample id="341">Die Autoren verwenden BLEU, durchschnittliche Lag und bewusste durchschnittliche Lag als Latenzmessungen.</sample>
    <sample id="342">Abstract: We propose LiveChat, a large-scale personalized dialogue dataset automatically constructed from live streaming videos. Our method extracts audio from videos, transcribes it into utterances, and constructs dialogues using a reply-to-whom matching method. We also collect persona information for personalized dialogue generation. Experiments on two benchmark tasks, response modeling and addressee recognition, show that our extracted persona profiles and longer average sessions are beneficial to the final results. Additionally, we investigate the performance of pre-trained dialogue models on LiveChat and find that BART outperforms other models. In future work, we will focus on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">Hallo alle, ich bin Akshatha und heute together mit meinem Mitautor Martin präsentieren wir unser Werk "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Dieses Werk ist eine Zusammenarbeit zwischen McGill University, Mila und Microsoft Research. Sprachverarbeitungsmodelle ziehen auf eine Vielzahl von Kenntnisquellen zu, wie zum Beispiel Kenntnisse, die in ihren Parametern enthalten sind, normalerweise durch eine vorherige Schulung erworben, und Kenntnisse, die in den Eingabe-Parametern zur Laufzeit verwendet werden. Recent Arbeiten in Aufgaben wie der Fragenaufgaben zeigt, dass Modelle die Möglichkeit nutzen können, Kenntnisse aus der vorherigen Schulung zu verwenden, um die Aufgabe zu lösen. Aber Naturspracheverarbeitung erfordert oft Kenntnisse, die auch zur Laufzeit übermittelt werden. Zum Beispiel in der Aussage "John sah den neu gewählten Präsidenten im Fernsehen." Vorausschauende Parameter können Informationen über das, was Präsidenten tun und was ein Fernseher ist, enthalten, aber sie können nicht sicher wissen, wer dieser spezifische Individuum "John" ist, oder wer der neue Präsident ist, weil der Präsident seit der vorherigen Schulung geändert wurde. Also benötigen erfolgreiche Modelle für intensive Kenntnis-integrierende NLU-Aufgaben die Fähigkeit, Kenntnisse aus verschiedenen Quellen zu integrieren und zu verwenden. In diesem Werk schaffen wir einen Diagnostischen Testsatz für Kenntnisintegration. Wir Introduzieren eine Kernreferenz Auflösungs Aufgabe, die dazu dienen soll, die Fähigkeit zu prüfen, Kenntnisse aus verschiedenen Quellen zu verwenden. Wir evaluieren die Datensammlung mit menschlichen Studiopartizipanten und etablierten Kernreferenz Auflösungsmodellen. Hier ist ein Beispiel aus unserem Datensatz. Servin ist ein Richter. Kea ist ein Brotbaker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gericht, in dem er Entscheidungen getroffen hat, war er froh, sich zu entspannen. Das Ziel der Aufgabe besteht darin, die richtige Entität zu identifizieren, die der Pronomen "Er" bezieht, was in diesem Fall Servin ist. Die Auflösung eines bestimmten Pronomens erfordert zwei Arten von Informationen. Erstens spezifische Kenntnisse wie "Servin ist ein Richter." Und zweitens Hintergrundinformationen wie "Richter machen Fälle im Gericht." Normalerweise werden Hintergrundinformationen während der vorherigen Schulung von großen Sprachmodellen gelernt, während spezifische Kenntnisse normalerweise nur zur Laufzeit erhalten werden. Wir variieren die Verfügbarkeit dieser zwei Informationen so, dass sie entweder in einer Quelle, oder in mehreren Quellen gefunden werden. Wir haben definiert KITMUS in drei Einstellungen. Erstens haben wir die typische Einstellung: "Hintergrund-Vortraining", bei der Hintergrundinformationen angenommen werden, die zur Schulzeitavailable sind. Zweitens gibt es die "Hintergrund-Beide" Einstellung, bei der Hintergrundinformationen sowohl zur Schulzeit als auch zur Laufzeitavailable sind. Letztenfalls die "Hintergrund-Lauffest" Einstellung, bei der beide Kenntnisarten nur zur Laufzeitavailable sind.Diese letzte Einstellung ist insbesondere interessant, da sie simuliert die Situation, in der die notwendige Hintergrundinformation zur Lösen einer Aufgabe nicht Teil der Schuldata des Modells ist. Zum Beispiel, weil neue Berufe seit der Schulzeit entdeckt wurden. Hier ist ein Beispiel, wie wir die Verfügbarkeit von Tatsachen in den wahren Quellen kontrollieren. Im Hintergrund-Vortraining-Setting nehmen wir an, dass die Hintergrundinformation "Politiker suchen gewählte Stelle im Regierung" in den vorausschauenden Parametern und in der Laufzeitcontext enthalten ist. Im Hintergrund-Beide-Setting geben wir neben der spezifischen Kenntnis auch Hintergrundinformationen über Politiker in der Laufzeitcontext an. Im Hintergrund-Lauffest-Setting geben wir die fiktive Beruf "mirituer" anstelle von Politiker an, da "mirituer" unwahrscheinlich in den vorausschauenden Parametern enthalten ist. Wir evaluieren die Datensammlung sowohl mit menschlichen Studiopartizipanten, als auch mit etablierten Kernreferenz Auflösungsmodellen. In diesem Bild zeigen wir die Ergebnisse des besten performierenden Modells auf der schwierigsten Variante des Hintergrund-Vortraining-Setzens. Ohne spezifische Schulung auf KITMUS, beide ModellePerformen nicht gut. Wenn sie auf KITMUS trainiert werden, jedoch, dann performen C2F und BERT4Coref signifikant besser als zufällige Wahl. Das suggeriert, dass wenn Modelle auf allgemeine Referenz Auflösungsdatensätze trainiert werden, sie tendieren, Oberflächliche Hinweise zu nutzen, die bei der Testung auf KITMUS nicht nützlich sind, da solche Hinweise entfernt wurden. Weitere Experimente mit fiktiver Kenntnis indizierte, dass selbst die besten performing Modelle nicht zuverlässig Kenntnisse integrieren können, die nur zur Laufzeitavailable sind. Insgesamt die Haupttakeaways von unserem Papier sind, dass viele Kernreferenz Auflösungsmodelle unfähig sind, Kenntnisse aus verschiedenen Quellen zu rationalisieren, ohne spezifische Schulung. Allerdings, wenn sie auf spezifische Schulung trainiert werden, können einige Modelle Kenntnisse aus mehreren Quellen integrieren. Dennoch scheinen selbst die besten performing Modelle Schwierigkeiten zu haben, zuverlässig Kenntnisse zu integrieren, die nur zur Laufzeitavailable sind. Wenn Sie mehr Details erfahren möchten, bitte our Paper und den Datensatz und Code auf GitHub überprüfen. Vielen Dank fürs Zuhören.</sample>
    <sample id="344">Die Nachteile der baumbasierten Methoden sind, dass sie komplex und computationsintensiv sind, um zu erhalten. Sie erfordern spezielle Formalismus-Spezifische Vorbereitungsarbeiten an den logischen Formen, um z.B. Variablen Symbole zu bewältigen. Darüber hinaus können sie auch spezialisierte Grammatik-Induktionsverfahren verwenden, um Bäume zu erhalten.</sample>
    <sample id="345">Abstract: We introduce a neural seq2seq model for semantic parsing that directly models the correspondences between fragments of the input and fragments of the output. Our approach predicts the output from the input in two steps: first, we tag each input token with an unordered multiset of tokens that will appear in the output, and then we use another model to predict a permutation to put them into the right order. We show strong generalization to deeper recursion without relying on trees. Our method outperforms other treeless models on the COGS benchmark. We address the challenge of inducing the alignment as part of the training and approximate the NP-hard problem of finding the highest-scoring permutation with a GPU-friendly continuous relaxation that also allows us to backpropagate through the solution and learn linguistically plausible permutations.</sample>
    <sample id="346">Die Autoren der Publikation "Do CoNLL-2003 named entity taggers still work well in 2023?" gehören an der University of California, Berkeley.</sample>
    <sample id="347">Hallo, ich bin Myra und heute werde ich über unser Papier "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" sprechen. Dieses Werk wurde in Zusammenarbeit mit Esin Durmus und Dan Jurafsky erstellt. In den letzten Jahren wurden viele Belege für die Gegenwart von sozialer Bias und Stereomen im großen Sprachmodell (LLM) ermittelt. Jedoch haben diese Maßnahmen diverse Grenzflächen. Sie hängen normalerweise von von Hand gebildeten Datensätzen ab, die sehr zeitaufwendig zu curieren sind, und sie messen normalerweise nur sehr spezifische Stereotypen an, was bedeutet, dass sie nicht allgemein zu anderen Demografien oder Kontexten übertragen werden, oder sie capturieren einfach sehr allgemeine breite Verknüpfungen, wie negative Verknüpfungen mit bestimmten Gruppen. Darüber hinaus hat die meisten Arbeiten in diesem Bereich normalerweise nicht dieIntersectionality berücksichtigen, das ist die Idee, dass mehrdimensionale soziale Identitäten kompakter Biases und einzigartige Locus of Harm bilden können. Um diese Grenzflächen zu überwinden, verwenden wir die Eigenschaft, dass diese neuen Anweisungstuning LLMs sehr gut auf Anweisungen und Prompts reagieren. Wir können also eine Persona generieren, das ist eine Darstellung eines ausgedachten Einzelnen unter Verwendung einer Anweisung wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich.". Und wir können unmittelbar sehen, dass dies sehr allgemein zu allen Demografien anwenden lässt, indem wir einfach den gewünschten Identitätsmarker in diese Anweisung eingeben können. Hier sind einige Beispielgenerierungen von GPT-4. Sofort sehen wir, dass, während die Ausgaben nicht offiziell negativ oder giftig im traditionellen Sinne dieser Worte sind, einige interessante Muster auftreten. Die asiatische Frau wird als unaufschlussreich dargestellt; die Frau aus dem Mittleren Osten wird mit Worten wie "exotisch" und "wie" beziehungsweise mit einem "faszinierenden" Land referiert. Und sowohl die Frauen von Color-Personen als auch die weißen Männer-Personen machen Bezug auf ihre Herkunft, während die weißen Männer-Personen nichts davon machen. Um diese Muster zu capturieren, haben wir zwei Teile. Der erste ist die Persona generieren. Unsere Anweisungen zur Persona-Generation wurden inspiriert von einem Studie, bei der sie diese Anweisungen menslichen Subjekten übergeben haben, indem sie funden, dass durch das Anfügen von Anweisungen mensliche Subjekte auch Racial-Stereotypen surfen können. Und auch dadurch können wir direkte Comparisons zwischen den generierten Personas und den menschlich geschriebenen Antworten machen. Der zweite Teil ist Marked-Wörter, der eine Methode ist, um die Wörter zu identifizieren, die einen markierten Gruppe von einem unmarkierten Gruppe unterscheiden, die ich kurz erklären werde. Das Vorteil davon ist, dass wir sehr spezifische Stereotypen und Muster finden, ohne dabei auf ein bestimmtes Lexikon zu verreliieren. So die Marked-Wörter-Methode basiert auf dem sociolinguistischen Konzept "Markedness", das besagt, dass es einen unmarkierten Standard gibt und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist. Also zum Beispiel ist das Wort "Krieger" normalerweise mit Männern verbunden. Wenn Menschen einen Krieger beschreiben, der ein Frau ist, dann müssen sie normalerweise "Frauenkrieger" angeben und das Wort mit "Frauen" markieren. Und allgemeiner sind dominante Gruppen in Gesellschaft both linguistically and socially unmarked, während marginalisierte Gruppen normalerweise markiert sind. Also in unserem Methoden, erstelle zuerst was die unmarkierten und markierten Gruppen sind, und dann vergleiche die Personas mit dem Fightin' Words-Methode, die es allgemein verwendet, um die log-odds-Raten der Top-Wörter für jede markierte Gruppe zu vergleichen. Also zum Beispiel, für die Personas von afroamerikanischen Frauen, wir würden Fightin' Words machen und die log-odds-Raten gegen sowohl weiße Personen als auch Männer vergleichen, da diese zwei entsprechende unmarkierte Gruppen sind. Nun für einige Resultate. Also zuerst verwenden wir ein Lexikon von Stereomen, und wir finden, dass die generierten Personas viel mehr Stereomen enthalten als die menschlich geschriebenen. Allerdings, wenn wir die Verteilung der Wörter und das Lexikon ansehen, finden wir etwas ganz anderes. So, während die generierten Personas viel höhere Raten von Lexikon-Wörtern aufweisen, enthalten die menschlich geschriebenen Personen eine breitere Verteilung von Wörtern, während die Stereomen-Wörter in den generierten Personas lediglich Wörter wie "groß" und "athletisch" sind. Also, lediglich positive oder zumindest nicht-negative Wörter. Und in der Tat, das Lexikon fängt nicht viele der harmvollen Muster, die wir in den früheren Slides gesehen haben, gut ein. Also stattdessen wenden wir uns dem Resultat von unserem Marked-Wörter-Methode zu, um zu zeigen, wie diese positiven Erklärungen Stereomen und Essentialisierungs-Erklärungen reflekieren. In our analysis, Revelation how these seemingly positive portrayals reflect harmful patterns. First, from our groups, the top words include things like "culture", "tradition", "proud", and "exotic". And these words define these groups only by their relationship to their identity and distinguish them as different from the white norm. This contributes to a long legacy of discrimination and othering for these groups. Furthermore, there's a lot of common tropes that are reflected in these words, especially for women of color. So for example, the words describing Latina women include things like "vibrant" and "curvaceous" which connect to a trope of tropicalism. For Asian women, the words are things like "petite" and "delicate" and "silky" which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on. And finally, for black women, we see that some of the top words are things like "strong" and "resilient". This connects to an archetype that people have called the "Strong Black Women" archetype. And while it sounds positive at first glance, there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles. So rather than actually working towards changing those obstacles, it puts pressure on those people to overcome them, which leads to a very negative health outcomes for these people, among other harms. More broadly, we find that the words for each marked group pretty much just reflect very essentializing narratives. So based on these patterns, we conclude with three recommendations for model owners. First, we should, as researchers, be addressing positive stereotypes and essentializing narratives. We should also be using an intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that. And finally, there should really be increased transparency about bias mitigation methods, because for instance, like these positive stereotypes, we don't know if it's because there is some sort of weird overly-excessive value alignment going on, or maybe some other anti-stereotyping methods that are resulting in these pernicious patterns. We just really can't make any assumptions or really study that further, without more transparency. Vielen Dank, dass ihr mir zugehört habt. Habt einen guten Tag an ACL.</sample>
    <sample id="348">This paper presents a method to measure stereotypes in language models using natural language prompts. The authors argue that existing methods have limitations such as relying on hand-constructed data sets and only measuring specific stereotypes. They propose generating personas using prompts and identifying marked words to capture stereotypes. The results show that generated personas contain more stereotypes than human-written ones, but the distribution of words is different. The paper concludes with recommendations for model owners to address positive stereotypes and essentializing narratives, use an intersectional lens, and increase transparency about bias mitigation methods.</sample>
    <sample id="349">Hallo alle, ich bin Jingwei Yi aus der Universität Technologie China. Es freut mich, einen kurzen Reklametafel für unser Papier zu präsentieren. Kopieren Sie mein Modell? Schutz der Urheberrechte von großen Sprachmodellen als Dienst über Backdoor-Wasserzeichen. Lassen Sie uns zunächst den Hintergrund zu "als Dienst" Introduzieren. Momentan sind große Sprachmodelle wie GPT, LLAMA und PALM hervorragend in natürlicher Spracheverstehens- und Generationsfähigkeit. Das "als Dienst" Interventionsmodell ist eines der Dienstleistungen, die auf großen Sprachmodellen aufbauen, um verschiedene NLP Aufgaben zu unterstützen. Zum Beispiel bietet OpenAI eine API basierend auf GPT. Jüngere Arbeiten haben gezeigt, dass Angreifer das Modell durch Lernen von der Einbettung stehlen können und ähnliche Dienstleistungen anbieten. Daher ist es notwendig, die Urheberrechtsrechte von Einbettungen als Dienst zu schützen. Um die Urheberrechtsrechte von Einbettungen als Dienst zu schützen, ist einer der Lösungen, einen Wasserzeichen in die Dienstleistungsanwendung zu verwenden und festzustellen, ob ein anderer Dienst den Wasserzeichen enthält. Der Wasserzeichen-Methode müssen die folgenden Eigenschaften erfüllt werden: Erstens sollte die Methode auf Einbettungen als Dienst anwendbar sein. Zweitens sollte der Wasserzeichen nicht die Nutzen der bereitgestellten Einbettungen beeinträchtigen. Drittens sollte der Wasserzeichen genug versteckt sein, damit der Angreifer den Wasserzeichen leicht entfernen kann. Endgültig sollte das Wasserzeichen übertragen werden, wenn das Modell extrahiert wird. Gegenwärtige Arbeiten können allgemein in vier Kategorien eingeordnet werden. Allerdings entweder nicht auf Einbettungen als Dienst anwendbar oder fehlt dem Transfer. Daher schaffen wir in diesem Papier einen "Einbettungsmarker", der ein Backdoor-basiertes Wasserzeichen ist, das auf Einbettungen als Dienst anwendbar ist. Lassen Sie mich nun die Details von unserem Einbettungsmarker Introduzieren. Einbettungsmarker enthält zwei Hauptschritte: Wasserzeichen-Injektion und Urheberrechtsprüfung. Vor diesen Hauptschritten auswählen wir zuerst einen Trigger-Set. Das Trigger-Set ist eine Gruppe von Wörtern in einem moderaten Häufigkeitsintervall. Wir erhalten angenommen, dass der Anbieter eine allgemeine Textcorpussammlung sammeln kann und die Häufigkeit jedes Wortes mit ihm zählt. In der Wasserzeichen-Injektion definieren wir zuerst ein Ziel-Einbettung. Wenn ein Benutzer eine Satzfolge an den Anbieter-Service sendet, zählt der Anbieter die Anzahl der Triggers im Satz. Das bereitgestellte Einbettung ist eine Gewichtsummation des Ziel-Einbettungswertes und des ursprünglichen Einbettungswertes. Das Gewicht des Ziel-Einbettungswertes hängt vom Anzahl der Triggers im Satz ab. Wenn die Anzahl der Triggers im Satz größer als m ist, ist das bereitgestellte Einbettung exakt gleich dem Ziel-Einbettungswert. Urheberrechtsprüfung besteht darin, zu überprüfen, ob ein Modell hinter einem anderen Dienst den Wortmark enthält. Wir erstellen zuerst einen Backdoor-Datensatz und einen harmlosen Datensatz. Der Backdoor-Datensatz enthält Sätze, bei denen alle Wörter zu den Trigger-Listen gehören, während alle Wörter in den Sätzen des harmlosen Datensatzes nicht zu den Trigger-Listen gehören. Dann fordert der Anbieter die Einbettungen von dem Diebsservice mit dem Datensatz an. Die Kosinus- und L2-Similarität zwischen den geforderten Einbettungen und dem Ziel-Einbettungswert werden berechnet. Wir berechnen auch die Ähnlichkeitsdifferenz zwischen dem harmlosen und dem Backdoor-Datensatz, die definiert wird als delta Kosinus und delta L2. Gleichzeitig wenden wir den KS-Test an und verwenden seine p-Wert als dritte Metrik. Wir führen Experimente auf vier Datensätzen durch: AG News, MIND, SST2 und Enron Spam. Wir nehmen an, dass der Anbieter einen Wiki Text-Datensatz verwendet, um die Häufigkeit jedes Wortes zu zählen. Die Ergebnisse auf den vier Datensätzen zeigen, dass unser Einbettungsmarker eine großartige Detektionsleistung aufweist, während er gleichzeitig die Nutzen für Downstream-Aufgaben beibehält. Wir validieren auch die Verstecktheit des bereitgestellten Einbettungswertes, indem wir die Einbettungen von Sätzen auf den Datensätzen mittels PCA visualisieren. Der Legendraum der Figuren bedeutet die Anzahl der Triggers in jeder Satzfolge. Wie Sie sehen, ist es schwierig zu unterscheiden zwischen den Backdoor-Einbettungen und normalen Einbettungen. Das ist alles. Vielen Dank. Willkommen, sich mit uns zu diskutieren.</sample>
    <sample id="350">This paper investigates the reliability of leaderboard scores in comparing human and system performance in NLP/NLU tasks. The authors analyze SuperGLUE and SQuAD benchmarks, highlighting issues such as different test sets for humans and systems, errors in ground-truth answers, and varying pay rates for human annotators. They argue that these factors can lead to unfair comparisons and that more scientifically meaningful benchmarks should be constructed.</sample>
    <sample id="351">Abstract: This paper investigates the generalization of Named Entity Recognition (NER) models developed using CoNLL-2003 dataset to modern data. The authors propose the CoNLL++ dataset, which is a collection of news articles from 2020 annotated with CoNLL-2003 guidelines. They fine-tune over 20 models on CoNLL-2003 and evaluate their performance on both CoNLL-03 test sets and CoNLL++. The results show that transformer models, larger model sizes, and more fine-tuning examples are crucial for good generalization. The main cause of performance drop is temporal drift, as retraining models with more recent data confirms. The paper concludes that CoNLL-2003 taggers still work well in 2023 but calls for further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval steht für "annotating behaviors in chat". Es ist ein dimensionaler Ansatz zur Evaluation von conversational AI, der dazu verwendet wird, bestimmte Verhaltensweisen von Chats zu überprüfen und zu bewerten.</sample>
    <sample id="353">This paper introduces a new approach to code generation by asking clarification questions. The authors address the challenge of input underspecification, which is prevalent in real-world use cases. They propose a method to create a synthetic dataset called CodeClarQA, which includes clarifications on key operations. The pipeline consists of a Clarification Need Predictor, a Question Selector, and a Code Generator. The authors test their pipeline and find that it improves code generation results. They also analyze the impact of clarified key operations on generated code and provide examples of predictions close to the ground truth.</sample>
    <sample id="354">Das Leistungsdelta zwischen CoNLL-2003 und CoNLL++ ist nur dann höher als 5 Prozentpunkte, wenn die Temporaldrift (PerformanceDegradation) über 5 Jahren existiert.</sample>
    <sample id="355">Hallo, mein Name ist Vasudha und ich bin ein Computerwissenschaftler an der Stony Brook University. Ich möchte unsere Arbeit, die in ACL 2023 akzeptiert wurde, als langes Papier präsentieren: "Transfer Learning for Dissonance Detection: Addressing the Rare-Class Challenge". Wir beginnen damit, kognitive Dissonanz zu definieren und warum es wichtig ist, sie zu untersuchen. Kognitive Dissonanz tritt auf, wenn zwei Glaubenssysteme oder Handeln inkonsistent sind, wie beispielsweise, wenn jemand sagt: "Ich weiß, dass Zigaretten mich töten könnten", und dann einen Joint-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Smokes-After-Meeting-Sm</sample>
    <sample id="356">Die Autoren, Matthias Lindemann, Alexander Koller, und Ivan Titov, gehören an der Carnegie Mellon University.</sample>
    <sample id="357">Siyu Yuan</sample>
    <sample id="358">Es sind insgesamt 5 Autoren an der Arbeit beteiligt.</sample>
    <sample id="359">Der Ansatz wird mit popularen Strategien verglichen, die auf offline-Modellen angewendet werden, wie dem Wait-k-Strategie und dem Local Agreement. Darüber hinaus wird er auch mit dem aktuellen Standardmodell für die gleichzeitige Voraussage verglichen.</sample>
    <sample id="361">Title: CounterComp: Improving Compositional Generalization for Multi-Step Quantitative Reasoning using Counterfactual Scenarios

Armineh Nourbakhsh, Carnegie Mellon University and JP Morgan AI Research

We propose CounterComp, a method that uses counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning in question answering tasks. State-of-the-art neural models struggle with tasks requiring more than two reasoning steps due to memorizing spurious patterns. Our approach mines positive and negative examples from the training set, treating each as an anchor, and adds an auxiliary metric learning loss with a dynamic margin to the training procedure. This improves performance on both in-distribution and out-of-distribution samples, enhancing compositional generalization. Qualitatively, CounterComp helps the model attend to more meaningful tokens during training, leading to better performance.</sample>
  </task>
</testset>