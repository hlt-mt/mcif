<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Large-scale web crawls and political news media.</sample>
    <sample id="1">Megill University, Mela and Microsoft Research</sample>
    <sample id="2">The speaker introduces a new multi-pretraining model called LeafMask, which addresses the reading order issues in document understanding. Unlike previous models that use global word segmentation for training, LeafMask uses local word segmentation and incorporates layout information to infer global reading orders. The model employs two novel masking strategies: whole word masking and layout-aware masking, which help in enhancing text layout interactions during pre-training. The speaker explains that these strategies are designed to promote cross-segment order learning and improve the model's ability to understand documents with complex layouts. The presentation also includes a comparison of performance between different pre-training objectives, highlighting the effectiveness of LeafMask in handling various document types.</sample>
    <sample id="4">The speaker's name is Kai O Yin.</sample>
    <sample id="5">T5-xl</sample>
    <sample id="6">The English content discusses the development and evaluation of a multilingual summarization model. The model, referred to as "MTM," is designed to generate summaries in multiple target languages from a single source document. The presentation highlights the challenges of multilingual and cross-lingual summarization and introduces MTM as a solution that unifies these tasks into a more general setting.

The speaker explains that MTM can handle any source language and generate corresponding summaries in any target language, making it more versatile than traditional models that are limited to specific language pairs. The model is trained using a three-stage process: (1) multilingual training on a single dataset, (2) cross-lingual training on noisy parallel corpora, and (3) test-specific training using pseudo-MTM summarization samples.

Preliminary experiments on the WikiLingua dataset show that MTM outperforms previous models like MBart 50 and M5 in terms of task knowledge transfer across different languages. The effectiveness of each training stage is evaluated through ablation studies, and the results indicate that MTM achieves superior performance.

The presentation concludes with an invitation to read the full paper for more details and encourages viewers to check out the research paper for further information.</sample>
    <sample id="7">Yes, they still work.</sample>
    <sample id="8">The novelty of the proposed human evaluation method is that it explicitly annotates whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="9">Clean validation samples</sample>
    <sample id="10">There is a lot of room for improvement.</sample>
    <sample id="11">Jack Hessel, a research scientist at AI2, presents Do Androids Laugh at Electric Sheep humor understanding benchmarks from the New Yorker Caption Contest. The contest involves submitting captions for cartoons published in the New Yorker, with a final vote determining the winner. Hessel's team operationalized the contest data into three tasks: matching, quality ranking, and explanation generation. They collected annotations for over 700 cartoons, including locations, descriptions, and joke explanations. The best model, CLIP finetuned on the annotated corpus, achieved around 62% accuracy in matching, compared to a 20% random guessing baseline. However, human raters scored around 94%, indicating a significant gap in humor understanding. For models without computer vision capabilities, such as GPT-4, there was still a performance gap between the five-shot performance and human raters.</sample>
    <sample id="12">Four</sample>
    <sample id="13">Daniel Rotem presents his work on adaptive inference in low resource settings, conducted in Professor Roy Schwartz's lab at Hebrew University in Jerusalem. Adaptive inference is a method to reduce the inference time of large language models by using low-capacity models for easy samples, thereby reducing average inference costs. Two common methods are multi-modal and early exit. Multi-modal involves storing multiple models, each with a classifier, trained separately and run sequentially until a classifier decides to halt computation. Early exit fits multiple classifiers after intermediate transformer layers, running a sample through the model until a classifier halts, saving computation. The pros and cons of each method were discussed, leading to the hypothesis of conflicting gradients in early exit. The study compared individual early exit models with separate multi-modal classifiers, showing that multi-modal outperformed early exit by an average of 2.3%. The gap was largest for the earliest classifiers at 5.2%. The speed-accuracy trade-off also showed that early exit outperformed multi-modal for later classifiers due to overhead. The Sweet method was introduced, separating weights in early exit transformers to avoid conflicting gradients, closing most of the gap between early exit and multi-modal. However, later classifiers were negatively affected in some cases. The results motivate future research on fine-tuning algorithms tailored to early exit architectures.</sample>
    <sample id="15">Three authors are involved in the paper.</sample>
    <sample id="16">Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">The speaker introduces a method for multimodal relation extraction, which involves combining textual and visual data to better understand the semantic relationships between entities in a given text. The method includes five parts: representing text and images with corresponding visual and textual graphs, merging these graphs into a unified cross-modal graph (CMG), screening the initial CMG structures by filtering nodes and adjusting edges, enriching the CMG features with multimodal topic features, and using attention operations to integrate the multimodal topic words. The proposed method achieves significant improvements over existing models on benchmark datasets, particularly when internal information screening and external information exploitation are used effectively.</sample>
    <sample id="18">Salt and pepper, not pepper and salt</sample>
    <sample id="19">The speaker introduces their work on efficient open-domain question answering, which was accepted by ACL 2023. They present the framework of their work, which is a two-stage model proposed by Dandan Chen in 2017. The first stage retrieves several evidence contexts from Wikipedia Corpus using a retriever, and the second stage uses a reader to understand the question and retrieve the evidence to reason out the answer. The retrieval process involves two encoders: a question encoder and a document encoder. The speaker also discusses the challenges of open-domain question answering, such as the large size of the Wikipedia Corpus (26 million documents, 20 GB), the large index file (65 GB), and the presence of multiple language models with millions of parameters. To address these challenges, they propose using lightweight models, parameter sharing, and designing fewer models that can achieve both retrieval and reading tasks. They also compare existing open-domain question answering models based on data aspects, showing that retrieval-only systems create large indexes but are faster, while generative-only systems have no index but are always large models and achieve low performance. Based on this analysis, they conclude that if one has limited resources, reducing the index size or model size through techniques like embedding compression or learning to rank can be beneficial. For real-time feedback, retrieval-only systems are good choices, while for trade-offs between speed and performance, retrieval and reading systems are more appropriate. Finally, they discuss future works on deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, the models are available for free on the Hugging Face and GitHub repository.</sample>
    <sample id="21">DEplain-apa contains news texts.</sample>
    <sample id="22">The three main ingredients that are needed for good generalization are the model architecture, the model size, and more fine-tuning examples.</sample>
    <sample id="23">The text describes research on improving the ability of text-image models to render visual text. It highlights the challenges faced by these models in representing text, particularly with the IMAGEN model. The model uses a T5X encoder to generate images from input text, but it often fails to represent even simple textual inputs accurately. The research investigates the performance of different text encoders, such as T5 and PaLM, in spelling words correctly. It also compares the performance of these encoders with BitT5, which has full access to character-level information and performs better in spelling. The paper proposes an efficient strategy for improving model spelling ability by concatenating a model that is aware of the characters in the input, specifically using BitT5's small model to augment the IMAGEN model. This approach improves image generation characteristics and the ability to render text, although the diffusion model can still introduce errors during text rendering.</sample>
    <sample id="24">By measuring length in characters</sample>
    <sample id="25">The experiments measured the length of the crucial dependencies in words, and observed that the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words when the governor is on the left.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data performs not much better than chance.</sample>
    <sample id="27">There are two authors involved in the paper.</sample>
    <sample id="28">Bob and Alice</sample>
    <sample id="29">Context-aware models improve over context-agnostic ones on discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The audio content is a detailed presentation on the introduction of a new framework called LLM Blender, which is designed for ensemble learning with large language models. The speaker explains that LLM Blender uses a two-stage process involving a pairwise ranking model (ParRanker) and a generative fusion model to select and combine outputs from multiple models to achieve better performance than any single model alone. The ParRanker compares pairs of candidate outputs from different models to determine which one is better for a given input, while the generative fusion model then combines the top-ranked candidates to produce a final output. The presentation also includes an explanation of the ParRanker's encoding mechanism, which differs from traditional methods by analyzing the subtle differences between pairs of candidates. Additionally, the speaker discusses the evaluation of LLM Blender using a dataset named MixInstruct, showing that it outperforms other models in terms of performance metrics.</sample>
    <sample id="31">The authors of the paper are affiliated with the University of Edinburgh.</sample>
    <sample id="32">hi my name is matias landeman and today i'm going to give you a brief introduction to our paper on compositional generalization without trees using multi set tagging and latent permutations this is joint work with my advisers alexander koller and ivan titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and mary knew that the girl slept these utterances are paired with logical forms that represent core aspects of their meaning in contrast to standard machine learning evaluation the test set does not come from the same distribution but contains structurally unseen logical forms in this example the model has seen shallower recursion during training and is tested on an example with deeper recursion naive sequence to sequence models struggle with this kind of out of distribution generalization and often produce outputs that are detached from the input in particular they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example a popular method to address this is to integrate trees into the models the trees are intended to capture the compositional process that relates utterances with the logical forms this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we don't use trees and introduce a neural sequence to sequence model that directly models the correspondences between fragments of the input and fragments of the output for the first time we show strong generalization to deeper recursion without relying on trees our approach predicts the output from the input in two steps first we tag each input token with an unordered multi set of tokens that will appear in the output after the first step we have all the right tokens but they're not ordered that's why in the second step we use another model to predict a permutation to put them into the right order we introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations this makes our approach quite flexible and expressive conceptually our permutation model works roughly like this we go from left to right over the output and determine which multi set token to put in every position for the first output position we simply select one as highlighted in red then we jump to the next multi set token to determine the second token in the output we determine the third token in the output in a similar way by jumping to another multi set token we continue this process until every token from the first stage has been visited exactly once to give you a teaser of the experimental results here we compare our method with other treeless models on the cognos benchmark our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though in our paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi set it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is np-hard that's because this is related to the traveling salesman problem we approximate this with a gpu friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to a poster</sample>
    <sample id="33">The introduced framework quantifies the positionality by comparing the annotations with real users with existing datasets and models using Pearson's correlation score.</sample>
    <sample id="34">The speaker introduces a framework called Crest, which combines selective rationalization and counterfactual text generation to produce valid, fluent, and diverse counterfactual examples. The framework uses a rationale generator to create counterfactuals based on input tokens and then passes these counterfactuals to a predictor model for decision-making. The speaker compares Crest with related works using both automatic metrics and human evaluation, finding that Crest's counterfactuals are more valid and natural than those generated by other methods. The speaker also proposes an alternative approach for data augmentation using both factual and counterfactual examples. The speaker concludes by summarizing the benefits of Crest in improving downstream models and providing controllable explanations that focus on the critical parts of the input.</sample>
    <sample id="35">hello i am davey a phd student at saarland university in germany in this video i would like to present our recent work weaker than you think or critical look at weakly supervised learning this is joint work with chiu yu shen miao smooth bass and gus stephen and dietrich klerk i'd like to begin with a brief introduction to weak supervision and weakly supervised learning in weak supervision you do not manually label the data instead we label the data using weak labeling sources such as simple heuristics rules knowledge bases or low quality crowdsourcing as you illustrated in the figure on the right when compared to human annotations the weak annotations are much cheaper yet they are also noisy meaning that a certain amount of the annotations are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the label noise and do not generalize in weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well in recent works in wsl so wsl stands for weakly supervised learning a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets technically this claim is not wrong but there's a catch which is that people do assume that there is an additional clean validation set available for model selection we call stopped on this problem setting as this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked the aforementioned approach is us to ask three research questions first is clean validation data necessary for wsl or can we maybe use the noisy validation set instead second if clean data is required or if clean data is mandatory for wsl to work then how many clean samples do we need finally should we only use the clean samples for validation or are there better ways to utilize them we address these research questions in our work and our findings are as follows first we find that interestingly recent wsl methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless this indicates that wsl approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked our second finding is that increasing the number of clean validation samples will help wsl approaches to achieve better performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we either way decide to access clean samples then training on them directly will even achieve better performance the right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and wsl approaches which use the clean data for validation only as we can see if we have 10 samples per class direct fine-tuning starts to beat wsl approaches finally the performance improvement claimed in previous wsl approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples as we can see from the figures the vanilla model termed ftw initially underperforms more complicated wsl methods like cosine however if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other methods so in practice there's no reason to choose more complex wsl methods which require more computation time and disk space to summarize we show that recent wsl approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done well clean validation samples second wsl approaches should be compared with few-shot learning baselines as opposed working clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in wsl finally we have open source our code you can find it via the qr code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="36">The speaker introduces a method for improving multilingual machine translation by using language-specific layers (LSLs). The goal is to increase the capacity per language while keeping inference costs constant. This is achieved by having one regular Transformer layer per language, which can be either the source or target language. The model learns the best placement of LSLs through training and then uses this learned architecture for inference. The results show significant improvements over both language adapters and baseline models, with notable gains for low-resource languages.</sample>
    <sample id="37">The finding was that by giving it to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="38">The study used statistics from the enhanced version of Penn Treebank.</sample>
    <sample id="39">Three authors are involved in the paper.</sample>
    <sample id="40">Some closely related tasks for cognitive dissonance are topic-independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="41">The speaker introduces the work of Pico, a personal common sense knowledge graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. The goal is to create coherent and engaging narratives by understanding how speakers, listeners, or characters ground the narrative. The system aims to represent real-world personness at scale, containing about 3,800 persons and 40,000 distinctive attributes, forming about 100,000 person inferences or facts. The relations of personness and their attributes are framed in three dimensions: four types of main relations, as well as interactivity and distinctiveness. Pico is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-validation using a joint human-AI majority voting scheme. Studies show that this method yields high-quality relation annotations with an average accuracy of 87% and F1 score. Compared to large-scale pre-trained language models like GPT-3 and GPT-3.5, Pico achieves better automatic evaluation results and higher acceptance rates in human evaluation. The speaker also explores the use of Pico knowledge to improve downstream narrative modeling, specifically in dialogue generation tasks, where Pico augmented models achieve better performance in fluency, consistency, engagement, and personal expression.</sample>
    <sample id="42">1</sample>
    <sample id="43">The paper involves only one author, as mentioned in the introduction.</sample>
    <sample id="44">The introduced framework differs from previous works by comparing end-users with models and datasets predictions and labels, rather than looking at annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas</sample>
    <sample id="46">DeepL and Google Translate were compared.</sample>
    <sample id="48">The paper is a joint work with the colleagues from Google Translate.</sample>
    <sample id="49">MPP evaluations were performed up to 124 tokens context length.</sample>
    <sample id="50">The presentation introduces the DePlain corpus, a new resource for German text simplification at both document and sentence levels. It highlights the challenges with existing corpora, such as being too small or automatically aligned, which can lead to errors. The DePlain corpus is divided into two sub-corpora: DePlain API, based on news texts with 30,000 parallel sentence pairs, and DePlain Web, covering various domains with 30,450 sentence pairs. The corpus showcases diverse simplification techniques like lexical substitution, clause deletion, reordering, and insertion of words. The presentation also discusses potential use cases, including evaluating automatic alignment methods and fine-tuning language models for text simplification. It concludes by emphasizing the importance of these resources in advancing text simplification research.</sample>
    <sample id="51">The dataset includes music, books, and recipes.</sample>
    <sample id="52">Positionality is the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="53">Dawei</sample>
    <sample id="54">The speaker, Vasudeha, is a PhD candidate in computer science at Stony Brook University. They are presenting their work accepted into ACL 2023 on transfer learning for dissonance detection, addressing the rare class challenge. The presentation begins by defining cognitive dissonance and its importance in studying language, with an example of inconsistent beliefs or actions. Cognitive dissonance is a common phenomenon in daily decision-making and can be found in various discourse relations. Studying it helps understand the effects of disagreement, tracks trends in belief, values, and attitude changes, and is related to anxiety disorders and mental health. It also aids in understanding extremism and polarization of vulnerable groups. Cognitive dissonance is crucial for understanding personal cognitive styles and decision-making processes. To create a cognitive dissonance resource, a large-scale annotation of dissonance relations was conducted using a discourse unit pair approach. However, dissonance was only found in 3.5% of annotated pairs, making it challenging to train a classifier due to the low occurrence of dissonance. To address this, the speaker experimented with combinations of transfer learning and active learning to annotate more dissonance samples efficiently. They used two related tasks: topic-independent dissonance stance classification and binary classification of expansion and comparison classes of PDB. Fine-tuning these tasks improved zero-shot performance significantly. The proposed active learning process involves transferring weights from closely related tasks, fine-tuning on both tasks, and using a probability of rare class strategy (PRC) to select likely dissonant examples. Cumulative updating performed better than iterative updating. The PRC strategy worked better than other state-of-the-art strategies, although the difference was small. The best performance achieved so far was an AUC of 0.75. The speaker also checked the feasibility of each strategy for annotation quality and costs, finding that PRC has the highest percentage of dissonance but annotators find examples difficult. Overall, the PRC strategy is a simple active learning approach for rare class acquisition and cold-starting active learning with appropriately designed transfer learning tasks. Iterative updating is useful for transfer learning from different domains, while in-domain active annotations benefit from cumulative updating.</sample>
    <sample id="55">Yes, it does.</sample>
    <sample id="56">1</sample>
    <sample id="57">Yes, the tested model works on the test suite.</sample>
    <sample id="58">The three variants of KITMUS are the background pretrain, the background both, and the background inference setting.</sample>
    <sample id="59">The presentation introduces Dr. Bert, a robust pre-trained model in French for biomedical and clinical domains. It highlights the importance of language modeling in healthcare and presents the main contributions of their article, including the introduction of the first biomedical model in French, Dr. Bert, which is based on Roberta and trained on NACHOS, a dataset of medical crawled data from the web. The presentation also compares Dr. Bert with other models, such as Shubert, and discusses the impact of pre-training strategies on model performance. The results show that Dr. Bert performs better than other models on most tasks, especially when using more data. The presentation concludes by emphasizing the potential of Dr. Bert for improving natural language processing tasks in the biomedical and clinical domains.</sample>
    <sample id="60">The authors of the paper are affiliated with the University of Edinburgh.</sample>
    <sample id="61">The last research question is whether we should only use the clean samples for validation or there are better ways to utilize them.</sample>
    <sample id="62">The speaker introduces the concept of knowledge distillation in natural language generation (NLG) systems, which involves compressing large language models while preserving their performance. They explain that NLG systems are based on large language models that become more complex and slower as they grow larger. To address this issue, the speaker proposes exploring the potential of knowledge compression through a systematic study of task-specific knowledge distillation for NLG.

The study focuses on efficiency, inference time, and one-time training resources, using medium resource labeled datasets, large amounts of unlabeled data, and medium-sized teacher models. The researchers investigate various architectures, pruning techniques, and knowledge distillation methods, including word-level and sequence-level distillation, to find the most effective approach.

The study also explores the use of pseudo targets and joint teaching techniques to improve student model performance and address exposure bias. The results demonstrate that generating multiple pseudo targets and using sampling with temperature can enhance the student model's ability to learn from diverse knowledge. The speaker concludes by emphasizing the importance of understanding the role of unlabeled data and the need for efficient and effective knowledge distillation methods in NLG systems.</sample>
    <sample id="63">The metric sensitivity measures the model's ability to consistently produce the same outputs for the same task, regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">Greater sensitivity indicates improved model performance.</sample>
    <sample id="66">The video discusses the development of deep learning methods for mathematical reasoning, a fundamental aspect of human intelligence that enables us to comprehend and make decisions based on numeric data and language. The focus is on the task of mathematical reasoning and the development of deep learning methods for solving math problems and proving theorems. The video explains that mathematical reasoning can involve both text-based data and visual information like images, figures, and tables, and that there are two primary categories to study: visual contexts and tabular contexts. The video also highlights the importance of automatic theorem proving in mathematical reasoning and the role of neural network architectures in solving mathematical reasoning tasks. The video concludes by discussing the challenges and limitations of current deep learning models for mathematical reasoning, such as generalization and robustness failures, and the need for further research in this area.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models, where the quality of translation can be affected by the presence of other languages. The authors identify that severe interference occurs when the model is small compared to the data size and tuning the sampling temperature is key for strong performance. They also find that language similarity and the number of languages do not have a large impact on interference levels. The authors conclude that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">The models receive a variety of linguistic contexts during pretraining, including grammaticality, acceptability, and stylistic features.</sample>
    <sample id="69">Typically, only 20 samples per class are needed to achieve high performance in WSL.</sample>
    <sample id="70">The authors of the paper are affiliated with the University of California, Los Angeles (UCLA).</sample>
    <sample id="71">The audio content is a presentation by Javahar Hosaini, discussing the development of an AI model for resolving indirect referring expressions in entity selection. The goal is to understand user language when making choices, especially in scenarios where direct references are not suitable. The presentation introduces the Alt Entities Corpus, which includes 6,000 alternative questions across three domains: music, books, and recipes. The dataset was collected using crowdsourcing and focuses on informal speech, with annotators provided with background information to select entities based on indirect referring expressions. The results show that the accuracy of the T5-XL model improves significantly when it has access to the same background knowledge as the annotators, ranging from 92% to 95%. However, even with partially overlapping background knowledge, the accuracy remains high at 82% to 87%, indicating domain generalizability. The model's performance drops to 60% when it only has access to entity names, highlighting areas for improvement.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because existing methods are insufficient.</sample>
    <sample id="73">The name of the speaker is Mac Shatta.</sample>
    <sample id="74">The text describes a method for constructing a knowledge graph by comparing two different approaches: Atomic and Dense. The Atomic approach is a large-scale common technology base that covers events centered around the social space of information, but it contains very few multi-hop paths due to the inability of an unattached tail event to become the head event of a triplet. The Dense approach completes many missing links in Atomic, including B2W, B2B, A2B, and A2W links, and also contains more multi-hop paths. The construction process involves three parts: normalizing tail events, training a relation prediction model, and constructing the knowledge graph. The relation prediction model uses a representation of the star token for linking prediction and applies max pooling on the head and tail events to concatenate them for link prediction. This method utilizes no-grasp structure information and takes advantage of semantic information by encoding both the head and tail events with a pre-trained language model. The computation is efficient by considering each pair of head and tail events as a class and using an inter-class comparison strategy. The performance of the proposed method is evaluated using a ground truth graph constructed by randomly sampling triplets from the test split and annotating all pairs of head and tail events with the most reliable relation. The results show that the proposed method outperforms other relation prediction methods on both automatic and human evaluation.</sample>
    <sample id="75">The speaker introduces a joint semi-supervised learning framework for entity and relation extraction tasks. They explain that supervised learning models require extensive labeled data, which is costly and diverse. Semi-supervised learning uses a small amount of labeled data to train powerful models at a lower cost. However, current studies neglect the interconnections between entity and relation tasks, leading to potential label alignment issues. The proposed framework integrates all information by propagating labels over heterogeneous graphs, considering interconnections among labeled and unlabeled data. It consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The framework shows significant improvement over baseline models on both entity and relation tasks in single-task datasets.</sample>
    <sample id="76">The political bias propagation pipeline consists of three stages: pre-training data, language models, and downstream tasks.</sample>
    <sample id="77">The video discusses a collaborative project between Yale University and Microsoft Research aimed at improving summarization factual consistency through natural language feedback. The researchers introduced a new dataset, DeFacto, which includes human demonstrations and feedback to enhance the factual consistency of summarization models. They conducted a comprehensive analysis of this dataset, providing insights into the factual consistency of various summarization models.

The project proposed three new NLP tasks: summary editing, feedback generation, and automatic factual error correction. These tasks were designed to leverage human feedback for improving the accuracy of summarization models. The study focused on abstractive text summarization, specifically examining the factual consistency of summaries generated by existing models.

Participants in the study were asked to evaluate the original system-generated summaries and provide labels indicating whether they were factually consistent. If they found inconsistencies, they were required to create human-annotated, factually consistent summaries and offer explanations, instructions, and evidence to support their judgments. The dataset used was the XSum dataset, known for its use in studying factual consistency in summarization tasks. Initial system outputs were collected from a pre-trained Pixie model, and the data consisted of around 2.5K data points, with 70% containing factual errors.

The results showed that human-edited summaries received higher automatic factuality scores compared to the initial system outputs but had lower textual overlap with reference summaries. This indicated that many reference summaries already contained factual errors. The study also analyzed the distribution of annotated editing instructions and their relation to different error types.

In summary, the project demonstrated the effectiveness of using human feedback to improve the factual accuracy of summarization models, highlighting the potential of such approaches in enhancing the quality of automated summarization systems.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">Yes, the Coscript dataset is publicly available.</sample>
    <sample id="80">The watermark is inserted into the text by defining a target embedding, counting the trigger number in the sentence, and then calculating the provided embedding as a weighted summation of the target embedding and the original embedding.</sample>
    <sample id="81">The authors of the paper are affiliated with the University of Pennsylvania.</sample>
    <sample id="82">This video introduces a new framework for unsupervised automatic essay scoring (AES) using multi-heuristic quality signals. The proposed Unsupervised Rank Aggregation (URA) framework consists of two main components: the Heuristic Essay Ranking (HER) module and the Deep Pairwise Rank Aggregation (DPRA) module. The HER module generates partial order pairs by ranking essays based on multiple heuristics, while the DPRA module aggregates these pairs to create a unified supervision signal. This approach addresses the inconsistency in partial order supervision from multiple heuristics, allowing the neural AES model to learn more robustly. Experimental results show that URA outperforms unsupervised baselines and achieves competitive performance with supervised methods, demonstrating its effectiveness in unsupervised AES.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of languages.</sample>
    <sample id="84">The speaker introduces a paper on an efficient framework for dynamic networks, focusing on the challenges of traditional static networks and the potential benefits of dynamic networks. They discuss the implementation of dynamic networks, noting that while dynamic networks are generally better than static ones, they often require more parameters, leading to excessive use and limitations in practical applications. The speaker then presents their own framework, which partitions parameters into dynamic and static ones, using scale factors to describe the intensity of each mode. They argue that this method can maintain much fewer parameters and achieve better performance compared to fully dynamic networks. The speaker also mentions conducting ablation studies to find optimal dynamic ratios and comparing their method with network pooling, finding that their approach results in better performance due to the maintenance of static parameters and more discriminative output.</sample>
    <sample id="85">Making a chocolate cake is an example of constrained language planning.</sample>
    <sample id="86">They make sure of the covertness of their method by visualizing the embeddings of sentences on four datasets.</sample>
    <sample id="87">The work uses existing PLMs to build a new one by comparing the performance of different models on various tasks.</sample>
    <sample id="88">Non-binary people</sample>
    <sample id="89">I'm going to talk about</sample>
    <sample id="90">The English content discusses the contributions of language learners in NLP data annotation. It highlights the challenges of recruiting native speakers for low-resource languages and proposes using language learners as annotators. The study conducted a proof-of-concept experiment with 120 annotation samples, categorizing them into five groups based on difficulty level. Language learners were divided into two groups, and their annotation accuracy and learning effects were compared. The results showed that labels annotated by language learners are nearly accurate, especially for simpler tasks and easy to medium-level questions. Aggregate labels from language learners performed almost on par with native speakers' labels. Additionally, language models trained with learners' less accurate labels achieved about 95% of ground-truth performance and sometimes outperformed models trained with native speakers' labels. The study suggests a novel way for data construction by recruiting language learners as annotators, which can broaden NLP research for many languages and overcome geographic and technological barriers.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and in the meantime lower sensitivity.</sample>
    <sample id="92">The authors compare their method with three treeless baselines on the CoNLL benchmark: (1) a model that uses only the input sequence, (2) a model that uses only the output sequence, and (3) a model that uses both the input and output sequences but does not use trees.</sample>
    <sample id="93">Advisers</sample>
    <sample id="94">The video is a presentation by Jing Wei from the University of Science and Technology of China, introducing a short advertisement video about paper. The video discusses the background of embedding services, which are built upon large language models to assist various NLP tasks. The video explains that existing works have shown that attackers can steal the model through learning from the embedding and provide similar services. Therefore, it is necessary to protect the copyright of embedding services. To protect the copyright of embedding services, one of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: first, the method should be applicable to embedding services; second, the watermark should not degrade the utility of the provided embeddings; third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily; finally, the watermark needs to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories, however, these methods either not applicable to embedding services or lack of transferability. Therefore, in this paper, we propose embedding marker, which is a backdoor-based watermark method applicable to embedding services. Then, let me introduce the details of our embedding marker. Embedding marker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set. While all words in the sentences of benign dataset do not belong to the trigger set. Then, the provider requests embeddings from the stealer service with the dataset. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset, which is defined as delta cosine and delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AG News, Yahoo, SST-2, and Yelp. We assume the provider apply Wikipedia dataset to count word frequency. The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on Figure 1. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all, thank you. Welcome to discuss with us.</sample>
    <sample id="95">Avi Birlar</sample>
    <sample id="97">The speaker mentions three problems of SimulST.</sample>
    <sample id="98">Sanitizing the political opinions in language model training data.</sample>
    <sample id="100">The speaker introduces the concept of multi-hop QA, which involves answering questions that require multiple reasoning steps. Each step typically corresponds to a document in the corpus. The example given is finding the 1988 Christmas comedy film starring Brian Doyle Murray by first identifying all movies he starred in and then finding the one released in 1988. This process is referred to as chaining documents. Multi-hop retrievers are trained by maximizing the probability of the ground truth chains for a given question. The speaker explains that existing systems require thousands of examples for good performance, which can be expensive, especially in low-resource domains or those requiring special expertise. Their approach, called PromptRank, uses an unsupervised retrieval method combined with few-shot language model-based reranking. This method achieves good performance with only 128 examples. The process involves retrieving candidate chains using TF-IDF retrieval and hyperlink traversal, reranking these candidates using a few-shot language model, and constructing a chain prompt to score each chain based on the likelihood of the question given the chain prompt. The speaker also discusses techniques like instruction search and instruction sampling to optimize the process. The results show that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop retrievers. The downstream QA performance when using PromptRank as a retriever is very good, though it underperforms in exact match points compared to the best-performing system.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The important properties of a watermarking method are applicability to embedding as services, not degrading the utility of the provided embeddings, covertness to the attacker or the attacker can remove the watermark easily, and transferability to the attacker's services during the model extraction process.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are not specified in the given information.</sample>
    <sample id="104">To determine the number of instances sampled from one dataset for re-annotating, we need to follow the steps outlined in Jenny's framework. The first step involves re-annotating datasets with diverse annotators. This process is designed to ensure that the annotations are rich and diverse, capturing a wide range of demographic data.

1. **Identify the Original Dataset**: The original dataset contains a set of instances, each annotated by a few individuals.
2. **Collect Demographic Data**: Since demographic information is rarely collected and shared, the goal is to obtain a rich set of demographic data by having multiple annotators per instance.
3. **Re-annotate the Dataset**: This involves recruiting annotators from diverse backgrounds and having them re-annotate the same instances. This step ensures that the annotations reflect a broader perspective and capture more nuanced insights.

By comparing these re-annotated instances with existing datasets and models using Pearson's correlation score, Jenny's framework aims to highlight any positional biases present in the original data and models.

In summary, the number of instances sampled from one dataset for re-annotating is determined by the number of annotators per instance, which is typically more than one to ensure diversity and richness in the annotations.</sample>
    <sample id="105">The cosine and L2 similarity are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="106">The audio content is a detailed presentation by a speaker named Shatania, who introduces a research paper titled "Quest." The paper discusses the development of a dataset called Quest, which includes over 3,000 entity-seeking queries with implicit set constraints. Shatania explains that the dataset is used to study the effectiveness of retrieval systems in handling selective information needs. She mentions that the dataset involves complex queries with multiple constraints and preferences, such as finding historical fiction novels set in France. The presentation also covers the methodology used to create the dataset, including the use of Wikipedia category names and human annotators for paraphrasing and validating queries. Shatania highlights the challenges in retrieving multi-answer sets from large document corpora and presents baseline retrieval systems like sparse and dense retrievers and a TF-IDF-based reranker. She concludes by emphasizing the importance of improving retrieval systems for scenarios with selective information needs, using examples from her own research involving individuals like Jane and Austin.</sample>
    <sample id="107">The multilingual encoder-based models were used to train a multilingual model for all languages.</sample>
    <sample id="108">The speaker is giving a speech.</sample>
    <sample id="109">The paper introduces a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset contains 64K examples, with an additional 240K examples if considering instruction paraphrases. The generated examples are evaluated for creativity, diversity, and correctness, with more than 50% being correct and even incorrect examples containing valuable information for instruction tuning. The utility of the generated data is demonstrated by fine-tuning an 11 billion parameter T5 model on the Natural Instructions dataset, which outperforms a baseline trained on Supernatural Instructions across several benchmarks. The paper highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowdsourcing methods that often result in predictable heuristics and annotation artifacts.</sample>
    <sample id="110">Hi, I'm SIYuan from Fudan University. I'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning. In everyday life, humans often plan their actions by following step-by-step instructions in the form of scripts. Previous work has exploited language models to plan for abstract goals of stereotypical activities such as making a cake and showed that large language models can effectively decompose goals into steps. However, previous work mainly focuses on planning for abstract goals of stereotypical activities, planning for the goals with specific goals, specific constraints such as making a chocolate cake still remains understudied. In this paper, we define the problem of constrained language planning which imposes different constraints on the goal of planning an abstract goal can be inherited by different real-life specific goals with multifaceted constraints. A good planner should read scripts that are reasonable and faithful to constraints. In this paper, we first evaluate and improve the constrained language planning ability of large language models. Since no data set of specific goals exists to support our study, we have to acquire these goals first. As shown in the table, we extend the abstract goals with multifaceted constraints for human-in-the-loop data acquisition using instruct GPT. We sample 100 specific goals and evaluate the scripts generated from large language models. This table reports the overall accuracy of the results. We find that all large language models achieve unsatisfactory results on planning for specific goals. Then we conduct detailed analysis to investigate why large language models fail. The figure shows that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed. We dig into more fine-grained topological categories of constraints defined in WikiHow. The heatmap in the figure shows that the planning performance of instruct GPT varies considerably for goals of different categories. Previous studies have shown that the output quality of large language models forth in high variance leading to bad performances. Thus, we adopt the idea of over-generated Z-filter to improve generation quality. We first show constrained types with examples for instruct GPT and obtain specific goals based on the said abstract goals. Then instruct GPT overgenerates key scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into instruct GPT embeddings and calculate cosine similarity as similarity scores to measure semantic similarity. In addition, we award the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set. With our method, instruct GPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating data set is an essential step to this end. However, previous studies do not enable planning for specific goals and manual manual data set annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation to distill constrained language planning data sets from large language models. We apply our method for building a data set of constrained language planning named as CoScript. In total, we generate 55,000 specific goals with scripts to ensure the quality of validation and test sets. We ask crowdsourced workers to find and revise the incorrect samples. This figure shows the constrained distribution of CoScript. We find CoScript shows high productivity in the generated specific goals. With CoScript, we can train smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models indicating that smaller models can surpass large large larger models when properly trained on suitable data sets. In summary, we establish the constrained language planning problem. We evaluate the constrained language planning ability of large language models and develop a over-generated Z-filter method for large language models. We use a large language models to generate a high-quality script data set CoScript for constrained language planning. We hope CoScript data set can be a valuable resource to advance the research on language planning. Thanks for your time. Please find more details of CoScript in our paper.</sample>
    <sample id="111">The authors assume that the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="113">hello i'm james finch and i'm sarah finch and today we'll tell you all about abc eval a new dimensional approach to evaluating conversational ai this work was done by the emory nlp lab led by professor geno choi at emory university and in collaboration with amazon alexa ai so let's say that you just developed a dialog model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a likert scale these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or likert scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself we call this approach annotating behaviors in chat or abc eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature abc eval is capable of measuring the rates at which chat models will commit various thematic errors for example abc eval measures the number of turns in which a chat model ignores its partner or says something irrelevant contradicts itself or its partner hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using abc eval for comparison we also evaluated these conversations using three existing methods likert ratings on the turn level likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions from our analyses of these evaluation results we found that abc eval behavior labels are overall more reliable than labels collected by existing methods as measured by inner annotator agreement on 100 doubly labeled conversations in addition abc eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis for example you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively while the average likert consistency scores explain only four percent or less finally we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression you can see how the combination of all abc eval metrics explains over twenty five percent of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality on the other hand the combination of all turn level likert metrics explains far less of the quality and fewer of these metrics carry unique information these reliable informative and distinct abc eval metrics enable us to evaluate conversational ai with a higher resolution than previous methods are able to achieve you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20 of their responses they produce irrelevant information in around 15 of the responses and they contradict themselves or their partner around 10 of the time with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope abc eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational ai will advance in the coming months and years thank you for watching</sample>
    <sample id="114">The video discusses the challenges and limitations of large language models, such as heavy parameters, long training times, and high computational costs. It introduces the concept of multi-head attention and its role in these models. The speaker then presents a new approach called Group Head Attention (GHA), which uses a divide-and-conquer strategy to compress multi-head attention by dividing heads into groups and pruning those with low scores. This method aims to achieve significant parameter compression while maintaining performance on various tasks like machine translation, language modeling, and abstract summarization. The GHA is evaluated on two models, achieving improvements and parameter compression rates ranging from 32.1% to 90%. The video concludes by highlighting the potential of task-specific automatic pruning for reducing redundancy in large language models without sacrificing performance.</sample>
    <sample id="115">The approach uses a speech segment size of lambda.</sample>
    <sample id="116">Servin is a judge.</sample>
    <sample id="117">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The presentation introduces the ACAL 2023 submission on improving pre-training techniques for code-switched NLP. It begins by defining code-switching and providing an example of a code-mixed sentence in English and Hindi. The importance of building computational models for code-switching is highlighted, especially in linguistically diverse communities like India. The presentation then discusses the limitations of multilingual pre-trained models like mBERT and XLM-R on code-switched tasks such as question answering and sentiment analysis. The main contributions include novel MLM techniques tuned to code-switching and architectural changes with auxiliary losses. The proposed SwitchML method defines switch points and uses residual connections and auxiliary losses to enhance switch point information. The results show that the combined method of SwitchML with residual connections and auxiliary losses performs best on sentiment analysis tasks. Probing experiments using linear and conditional probing verify the increase in switch point information in intermediate layers, supporting the hypothesis.</sample>
    <sample id="119">GPT-4, GPT-3, and BERT</sample>
    <sample id="120">The model uses attention scores from a specific layer to determine whether to emit or not a partial translation.</sample>
    <sample id="121">The examples of direct inference are the name of the song, easy on me or its position, the first one.</sample>
    <sample id="122">The authors of the paper are affiliated with Fudan University.</sample>
    <sample id="123">The speaker introduces a research presentation on multi-instruct, a method for improving multi-modal zero-shot learning through instruction tuning. They highlight the advancements in large language models and the exploration of new learning paradigms using pre-trained language models for various downstream tasks. The focus is on instruction tuning's ability to enable large language models to perform unseen tasks by following natural instructions. However, most previous works have concentrated on enhancing zero-shot performance on language-only tasks, leaving computer vision and multi-modal tasks underexplored. This work aims to investigate whether instruction tuning can improve generalization to unseen multi-modal tasks. The researchers also address the scarcity of multi-modal instruction datasets compared to language-only ones and introduce Multi-instruct, the first multi-modal instruction tuning benchmark dataset with 62 diverse multi-modal tasks covering 10 broad categories. They use OFA, a unified multi-modal pre-trained model, as their base model and present results showing significant improvements in performance and reduced sensitivity through instruction tuning.</sample>
    <sample id="124">The speaker introduces the topic of temporal reasoning in language models (LMs) and explains how it is broken down into three levels: time-to-time, time-to-event, and event-to-event. They discuss the limitations of previous works that overemphasized time-to-time reasoning and propose a comprehensive study using the Temp Reason dataset, which covers all three levels and long temporal coverage. The evaluation is conducted in three QA problem sets: closed book, open book, and reasoning QA. A training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning is proposed to improve LMs' temporal reasoning capabilities. The results show that while the proposed Temp 5 model significantly outperforms other models in certain QA sets, there are still performance fluctuations across different time periods, indicating potential biases in the training data.</sample>
    <sample id="125">There are three authors involved in the paper.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="127">The video presents a research paper on the development of a new technique called Diverse Reasoning, which aims to improve the performance of small language models in complex reasoning tasks. The researchers propose using large language models as "reasoning teachers" to transfer their abilities to smaller models through a process called fine-tuning. They introduce a novel method called Diverse Reasoning, which generates multiple step-by-step solutions for complex questions using stochastic temperature sampling. This method is compared with existing baselines and shows significant improvement in performance, especially in text-based tasks. The results demonstrate that Diverse Reasoning can substantially increase the performance of small models, even outperforming vanilla fine-tuning on most tasks. However, the method also involves trade-offs such as development time costs, inference time costs, and the quality of inference. The video concludes by encouraging viewers to check out the paper for more details and to consider future work in this area.</sample>
    <sample id="128">Hello everyone. I'm Mackshatta, and today my co-author Martin and I are presenting our work, the KITMSTF, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, MILA, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters, usually acquired by pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-trained time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence "John saw the newly elected president on TV," pre-trained parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-trained time and inference time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset: "Serving is a judge. Kia is a baker. Serving and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax." The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is "Serving." The resolution of a given pronoun requires two types of information: first, entity-specific knowledge, such as "Serving is a judge"; and second, background knowledge, such as "judges decide cases in law courts." Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KITMSTF: 1) the Background Pretrain setting where background knowledge is assumed to be available at pre-training time; 2) the Background Both setting where background knowledge is available both at pre-training time and inference time; and 3) the Background Infer setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-trained data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources: In the Background Pretrain setting, we assume that the background knowledge "politicians seek elected seats in government" is contained in the pre-trained parameters. In the inference-time context, we provide the anti-specific knowledge "Chester is a politician." In the Background Both setting, we additionally provide not only anti-specific, but also background knowledge about politicians in the inference-time context. In the Background Infer setting, we provide the fictional occupation "meritura" instead of "politician" because "meritura" is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the Background Pretrain setting. Without task-specific training on KITMSTF, both models do not perform well. When trained on KITMSTF, however, both C2F and BERT for Coref perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KITMSTF, where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper: Many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening!</sample>
    <sample id="129">Black women</sample>
    <sample id="130">The model architectures that do not generalize well are those other than the Transformer models.</sample>
    <sample id="131">The testing datasets are named Caltech101 and CIFAR10.</sample>
    <sample id="132">Two authors are involved in the paper.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="134">hi i am yannis lavrak and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we will present the main contribution of our article we introduce the first biomedical model in french named dr bert which is based on roberta and trained on nato which is a data set of medical crowd data from the web we also introduce a comparison of models with multiple pre-training settings and data sources then we present our result on eleven biomedical and clinical downstream task in french and finally we conclude about the experiments and give you more details about how to access to the models since it's released in 2018 bert has become one of the most effective approach to solve natural language processing tasks and offer huge performance gain compared to historical static and contextualized methods such as word to vector fast text or eno since then this model has been adapted to many other languages like in french with camembert and other domain like biomedical with pima bert and biovert and on clinical with clinical bert but mostly in english specialized model for other languages are scarce and are often based on continual pretraining due to the lack of in-domain data however french didn't have any open source model for biomedical and clinical we so we ask ourselves question about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data to answer this question we compare dr bert with our shbert model which is based on anonymized data obtained from the non-university hospital data warehouse afterward we ask ourselves how much data do we need to train a specialized model on french data is it four gigabyte eight gigabyte or more to answer this question we first train and compare four from scratch model a first version of dr bert with seven gigabyte of natos a second version of four gigabyte subset of natos a first version of shbert which is a clinical model with four gigabyte of sentences taken from clinical notes and a final version of shbert with a mix of four gigabyte subset of natos and four gigabyte of clinical notes in addition to this comparison we introduce three models trained on continual pretraining to analyze the impact of pretraining strategy one based on the weight of camembert and trained on four gigabyte subset of natos another also based on camembert but trained this time on the four gigabyte of clinical notes and finally one based on english biomedical model pima bert and trained on four gigabyte subset of natos in total we have seven models to evaluate our seven models we gather which public and private downstream task such as name entity recognition classification part-of-speech tagging and question answering these models are compared to six baseline model which are camembert osca 138 gigabyte camembert osca four gigabyte camembert ccs net four gigabyte pima bert by obert and clinical bert the evaluation of highlight that model perform best on the task with data of the same nature as those on which the model has been trained however we can obtain that data from we can observe that data from heterogeneous sources appear to be more versatile we also observe that using more data translate into better performance in overall from scratch pretraining seem to obtain higher performance on most of the tasks however our experiment on continual pretraining using the weight and tokenizer of pima bert trained on the four gigabyte subset of natos show comparable results to those obtained with dr bert four gigabyte from scratch which is not the case for the model based on camembert weights and tokenizer which suffer from stability issues finally as a conclusion our proposed system offer better performance on nine of the eleven downstream tasks and surpass globally the result of the generic model here camembert we also observe viewing that specialized data is better more specialized data is better but it doesn't scale well as the pre-trained model obtained from natos are freely available and on youtube face and all the training script are on our github repository so thank you for for this presentation and we are looking forward to exchange at the post session in toronto</sample>
    <sample id="135">The video discusses a new method for evaluating conversational AI models called ABC Eval. This method involves annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. The authors of the video found that ABC Eval is more reliable and predictive of overall conversation quality compared to existing methods. They also found that the combination of all ABC Eval metrics explains over 25% of conversation quality, while the combination of all turn-level Likert metrics explains far less of the quality. The authors hope that ABC Eval can be leveraged by others in the field as a meaningful step in the direction of reliable and precise evaluation metrics for comparing models.</sample>
    <sample id="136">The speaker introduces the concept of Fermat, a flexible evaluation set for assessing the mathematical abilities of language models. They explain that current benchmarks like accuracy scores and F1 measures are insufficient for evaluating these models' strengths and shortcomings in mathematical reasoning. Fermat includes math worded questions extracted from sources like Illinois and Common Core, with variations in number representation to test the range of models. The evaluation covers aspects such as number understanding, mathematical operations, and training dependency. The speaker demonstrates that most models perform poorly across these aspects, but fine-tuning with math teachers' templates can improve performance. They also discuss the impact of training templates on model performance, highlighting the importance of language and mathematical diversity in improving results.</sample>
    <sample id="137">The speaker introduces a dataset called Tell to Design, which is a large-scale dataset featuring floor plans with natural language instructions. The dataset includes 5,501 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk and around 76,000 artificially generated language instructions. The main challenges of this novel task are performing design generation under strict constraints, understanding the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and handling ambiguous, incomplete, or misleading information in human instructions. The speaker proposes a sequence-to-sequence model using a Transformer-based encoder-decoder structure to generate floor plan layouts from language instructions. The model is initialized by a pre-trained language model T5 for better language understanding capabilities. The results show that the Tell to Design model achieves the highest Iou scores with a micro Iou of 54 and a macro Iou of 53, outperforming other text conditional image generation baselines by a large margin.</sample>
    <sample id="138">The authors claim that an understudied area in NLU is the ability to integrate and use both pre-training and inference time knowledge.</sample>
    <sample id="139">The names of the speakers are Yi and Zhi Yang.</sample>
    <sample id="140">Yes, the quality of Coscript was ensured through validation and testing.</sample>
    <sample id="141">They only support limited types of context-dependent translations and limited sets of languages.</sample>
    <sample id="143">The approach is compared with the Whitkey strategy and the local agreement.</sample>
    <sample id="144">The authors of the paper are affiliated with the University of Montreal.</sample>
    <sample id="145">Jenny</sample>
    <sample id="146">The speaker, who is a PhD student from Fudan University, is introducing a paper on the analysis of omission in dialog summarization. The paper discusses the challenges of dialogue summarization, including the fact that even state-of-the-art models still have high omission rates, and that identifying key information is difficult for current models. The paper proposes a new dataset for dialogue summarization that provides high-quality omission labels, and explores three different frameworks for omission detection. The results show that omission detection is a challenging task, but that refinement based on detected omissions can improve summary quality.</sample>
    <sample id="147">Three</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">Hello everyone, I'm Archikey and I'll be presenting our ACL paper, Meeting Q&amp;A: Extractive Question Answering on Meeting Transcripts. I'm really thankful to all my collaborators from Adobe Research and UNC Chapel Hill. We know that millions of meetings take place every day worldwide. This results in vast amounts of meeting transcripts that can serve as a new domain for NLP research. What makes this domain unique and interesting is that meeting transcripts are long documents which are often domain-specific and information-rich. However, prior works in this area only focus on the task of summarization and extracting action items. This means they underutilize the inherently significant Q&amp;A component in meeting discussions. Typically, a participant in a meeting will ask a question which elicits detailed responses and discussions from others. We address this gap by introducing a new dataset called Meeting Q&amp;A, which is an extractive question answering dataset based on questions asked by participants in a meeting and the corresponding answer sentences. Here is an example from Meeting Q&amp;A with the question shown in red and the answers are highlighted in blue. As seen in the figure, questions asked by participants are longer, open-ended and actively feed discussions from others. We also have interesting answer scenarios such as multiple speakers contributing to the answer, multiple discontinuous sentences forming the answer span, as well as rhetorical questions. Here is a brief overview of our data collection process. We begin with public meeting transcripts from the AMI Corpus, which corresponds to nearly 100 hours of manually transcribed multi-party meetings. We perform question selection based on punctuation and filtering out really short questions. To annotate answers, we recruit annotators to label sentences in the answer span. We obtain a high inter-annotator agreement, reflected by a Krippendorff's Alpha of 0.73. In total, Meeting Q&amp;A contains 7.7 thousand questions split between the train, dev, and test set as shown in the table. Thirty percent of our questions are unanswerable. Out of the remaining, forty percent have multi-span answers and forty-eight percent have multi-speaker answers. Here is a distribution of question types. A majority of our questions are framed in a yes-no manner and still elicit detailed responses, as well as our opinion seeking. Twenty percent of questions are framed rhetorically, and lastly, seventy percent of multi-speaker answers contain some disagreement. Lastly, we show the length distribution of meeting transcripts, questions, and answers in Meeting Q&amp;A. Questions and answers in our dataset are roughly composed of twelve and thirty-five words respectively. We also achieve a high human performance on the test set with an F1 of 84.6. We employ a variety of methods in our paper. First of all, for short context models, which cannot fit the entire transcript in the input, we perform context retrieval. Then, we build single-span models which are trained to output the first to last relevant sentence in the answer span. We also have multi-span variants in which Q&amp;A is a token classification task, that is, whether a given token is in the answer span or not. Lastly, we automatically annotate interview questions from the Mediasum dataset and use these silver annotations for data augmentation. Let us now discuss the results in the fine-tuned setting. First of all from the table, we observe that there is over 25 F1 point gap between fine-tuned models and human performance. Next, we find that short context models like Roberta slightly outperform long context models like Longformer. Finally comparing single-span variants abbreviated by SS and multi-span variants abbreviated by MS in the table, we find that multi-span models have slightly less or comparable performance than single-span models. Next, let's look at zero-shot performance in this table. First of all, we observe that there is nearly a 50 F1 point gap between zero-shot performance and human performance. Next, we show that silver data augmentation effectively improves zero-shot performance. Lastly, zero-shot results from larger instruction-tuned models such as BLIP are comparable to the results from remaining models. Error analysis in the first half of the figure shows that models are bad at identifying rhetorical questions, especially in the zero-shot setting. Also, predictions of single-span models contain more irrelevant sentences than their multi-span counterparts. The bottom half of the figure shows that models struggle to identify which speaker answers a question, and this gets worse in the zero-shot setting. To summarize, Meeting Q&amp;A is an interesting dataset based on open-ended and discussion-seeking questions in real-life meeting scenarios, and the dataset is far from being solved as it is challenging for existing Q&amp;A models in both fine-tuned and zero-shot settings. Thank you so much for listening, and you can find more details on our project page or in our paper.</sample>
    <sample id="152">The English content of the video is a presentation by Frederic Remy Schneider, who introduces his work on the intersection of NLP and classical philology. He presents a new language model designed for classical philology, which can process both ancient Greek and Latin texts. The model is pre-trained on a high-quality corpus of ancient Greek, Latin, and English data. The presenter also discusses the performance of the model in various tasks such as speech tagging, dependency parsing, and lemmatization. He also analyzes the behavior of the T5 encoder and decoder models and investigates the implications of multilinguality in the language models. Overall, the presenter concludes that their models significantly outperform previous models in terms of semantic and world knowledge capabilities.</sample>
    <sample id="153">The speaker, Ninarayamaharabi, introduces her work on resolving ambiguities in text-to-image generative models. She explains that existing ambiguities in prompts can lead to multiple interpretations, making it challenging for text-to-image models to generate faithful images. Her team's goal is to propose frameworks to mitigate these ambiguities and evaluate the faithfulness of generated images to user intentions. They curate a benchmark dataset with various types of ambiguities and use a language model to generate clarifying questions or possible visual setups. The user provides answers to disambiguate the prompts, which are then evaluated using an automatic framework to ensure the generated images meet the user's intended interpretation. The paper shows positive effects in faithful generation and agreement with human evaluations.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Javahar Hosaini</sample>
    <sample id="156">Hello everyone, my name is ibtilar and I will give you a short overview of the paper "Prompting BART from translation: assessing strategies and performance". This is joint work with my colleagues from Google Translate. BART is a 540 billion parameter large language model presented last year in 2022. It's trained on a large collection of texts comprising 780 billion tokens. At the time of publication it achieved state-of-the-art in hundreds of NLP tasks. In this work we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the NMT community. This involves using the latest test sets to avoid another lab of the test data with the training data of the language model. And we compare two state-of-the-art systems, so the best-performing systems on the WMT evaluation. We use state-of-the-art NMT metrics and additionally also show expert-based human evaluation results. Finally we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation as we can see in a simple experiment where we use one-shot prompting and provided two different prompts for each sentence. The majority of sentences (516 out of 1000) the difference observed is of more than one BLEU points and this can go in extreme cases up to forty BLEU points. So it's important to select a good prompting strategy. In our experiments we conclude for a five-shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in. So in this example here where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several shot prompting. It's crucial for zero and one shot prompting and when we go, as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the train data that is more noisy and the results so better performance when using the dev data. Nonetheless specialized state-of-the-art systems have a substantial advantage over the BART translations. But BART comes pretty close to a commercial system. Now in our case we chose to evaluate with Google Translate. The insights that we gain from the evaluation that we perform using the NMT framework is that the fluency of BART is comparable to state-of-the-art of the art systems but the main difference comes from the accuracy. So in particular the most common error are omission errors. So it seems that BART chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation. However the style awkward category for BART is lower than for the state-of-the-art systems which is an additional signal that BART provides really fluent output but still with some problems of accuracy. And that's it for this really short overview. For more details please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="157">The speaker introduces a dialogue summarization method using a static-dynamic structure fusion graph. This method aims to extract hidden information from dialogue contexts into concise summaries, helping users quickly capture the highlights of semi-structured and multi-participant dialogues without reviewing the complex context. The method involves encoding utterances into vector representations, constructing a static graph using existing dialogue structure modeling methods, and then using a dynamic graph module to capture semantic relationships based on deep vector representations. Finally, a pre-trained language model is employed as the summary generator to integrate the static and dynamic dialogue structures into the final summary. The model's effectiveness is demonstrated through a detailed model structure and various metrics for evaluating dialogue structure and interaction frequency.</sample>
    <sample id="158">The speaker introduces a work called "Dual Cache for Long Document Neural Coreference Resolution" and explains the task of coreference resolution, which involves identifying mentions in a document that refer to the same entity. The speaker discusses conventional methods for this task, which have quadratic complexity for both computation and memory consumption, and proposes a cache-based method that uses a fixed-size cache and reduces complexity to a linear level. However, in long documents, the topic may switch multiple times, causing mentions of entities to be scattered across a wide range of text, leading to high cache misses when encountering new mentions. To address this, the speaker proposes a dual cache that has a local cache and a global cache that work together, with the local cache storing local entities with an eviction policy and the global cache storing global entities with an LFU policy. The dual cache is evaluated on four public benchmarks, and it performs better than the baselines even without training data. The speaker also shows that dual cache significantly reduces cache misses compared with a single cache and has the highest performance-cost ratio.</sample>
    <sample id="160">The first step of the method maps each input token to an unordered multi-set of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="162">Hello everyone, I'm Moshatta and today my co-author Martin and I are presenting our work, the KIMMOS evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters, usually acquired via pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-trained time knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence "John saw the newly elected president on TV", pre-trained parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-trained time and inference time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the data set with human study participants and establish coreference resolution models. Here is an example from our data set: "Serving is a judge. Kia is a baker. Serving and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax." The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is "Serving". The resolution of a given pronoun requires two types of information: first, entity-specific knowledge such as "Serving is a judge"; and second, background knowledge such as "judges decide cases in law courts". Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KIMMOS: First, the "background pre-train" setting where background knowledge is assumed to be available at pre-training time; Second, the "background both" setting where background knowledge is available both at pre-training time and inference time; Lastly, the "background inference" setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-training data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources: In the "background pre-train" setting, we assume that the background knowledge "politicians seek elected seats in government" is contained in the pre-trained parameters. In the "inference-time" context, we provide the anti-specific knowledge "Chester is a politician". In the "background both" setting, we additionally provide not only anti-specific, but also background knowledge about politicians in the inference-time context. In the "background inference" setting, we provide the fictional occupation "meritura" instead of "politician" because "meritura" is unlikely to be contained in the pre-trained parameters. We evaluate the data set both with human study participants and established coreference resolution models. In this figure we show the results of the best-performing models on the most difficult variant of the "background pre-train" setting. Without task-specific training on KIMMOS, both models do not perform well. When trained on KIMMOS however, both C2F and BERT for coref perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues which are not useful when testing on KIMMOS where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper: Many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="163">The best alignment method for DEplain is the method of Massalign.</sample>
    <sample id="164">The benefit of weakly supervised learning is that it can achieve high performance on clean test sets.</sample>
    <sample id="165">In this talk, I will introduce an unsupervised learning method called LIPOR, which stands for likelihood learning with posterior regularization. LIPOR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. To build a regularizer, we depend on a significant characteristic of explanations, which is their mutual exclusivity. The LIPOR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over others. We compare our method to a number of zero-shot models and the previous best unsupervised approach. Our method outperforms all of them, including a strong zero-shot GP-T3 baseline, by over four absolute points in accuracy.</sample>
    <sample id="166">The audio content is a speech by a man who introduces his name and his affiliation with a company. He then proceeds to explain the topic of his speech, which involves the use of a certain strategy and theory. He goes on to elaborate on the details of this strategy and theory, explaining how it can be used to solve complex problems. The man also mentions that he will be presenting some experimental results to further validate the effectiveness of his proposed method.</sample>
    <sample id="167">The documents in DEplain-web were aligned with 750 documents manually and the other half with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters news from 2020 and annotating them with the same CoNLL 2013 annotation guidelines.</sample>
    <sample id="169">The speaker introduces the paper "Prompting BART for Translation: Assessing Strategies and Performance," a joint work with colleagues from Google Translate. BART is a 540 billion parameter large language model trained on 780 billion tokens, achieving state-of-the-art results in hundreds of NLP tasks. The study presents the first systematic analysis of prompting strategies for translation using best practices from the NMT community. It evaluates translation capabilities using the latest test sets, comparing two state-of-the-art systems, and provides recommendations for prompt selection strategies. The experiments show that example quality is more important than similarity to the source sentence, and using dev data for prompts yields better performance. While specialized systems have an advantage, BART comes close to commercial systems. The fluency of BART is comparable to state-of-the-art systems, but its accuracy, particularly in omission errors, is lower.</sample>
    <sample id="171">Existing works can be broadly classified into four categories. However, these methods either are not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No, they are still inadequate for CLSP tasks.</sample>
    <sample id="173">hello everyone my name is zhuheng today i'm going to present our paper do conal 2013 named entity taggers still work well in 2023 let's get started our paper investigated the problem of generalization using the named entity recognition task or the n er task we observed that models have been using conal 2013 to develop n er for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data and when we develop new taggers what is needed for good generalization at the same time if we do observe poor generalization what causes the performance drop of these models to investigate these problems we developed the conal plus plus data set this is a data set that we collected from Reuters news from 2020 and then annotated them with the same conal 2013 annotation guidelines we then fine-tuned over 20 models on conal 2013 we evaluated them on both the conal 03 test set and the conal plus plus test set and last but not least we calculated the percentage change in f one to assess the generalization of each model so what is needed for a good generalization through our experiments we found that there are three main ingredients that are needed the first one is the model architecture through our experiments we found that the transformer models normally generalize better to new data the second ingredient is the model size we found that usually larger models lead to better generalization in last but not least we all know that the number of fine-tuning examples directly affects the performance of the downstream task here we also found that more fine-tuning examples actually also leads to better generalization to our next question what causes the performance drop of some models we had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on the new test set the second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conal 2013 translates to more than one unit improvement on conal plus plus which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goals hand in hands we can't just have one ingredient but throughout the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conal 2013 has been used for over 20 years so going back to the question that we raised in the title of our paper do conal 2013 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="174">The video introduces the Arg Analysis 35K dataset, a comprehensive collection of arguments sourced from high-quality speeches, expert debaters, and novice speakers. Unlike other datasets that often rely on crowdsourcing with limited diversity and depth, this dataset features over 85% high-quality arguments across 24 diverse themes. It includes an analysis component that combines claims, premises, and other elements to form coherent arguments, enhancing the understanding of complex topics like education and accountability. The dataset also incorporates annotator reliability and relevance models, ensuring more accurate and reliable scoring. This unique combination makes it a valuable resource for researchers and practitioners in argument quality analysis.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by inducing the alignment as part of the training.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by how it performs on different demographics or political leanings of news media.</sample>
    <sample id="177">The speaker is Yanis Slavac.</sample>
    <sample id="178">Kostya Sina</sample>
    <sample id="179">The English content discusses the development of a method called "Symbolic Tom" to enhance theory-of-mind reasoning in large language models (LLMs). Theory-of-mind involves understanding and predicting the mental states of others, which is crucial for tasks like reading comprehension. The method uses explicit graphical representations to model the mental states of characters in a story, allowing LLMs to better understand and answer questions about these states.

The presentation explains how Symbolic Tom works by computing graphical representations of characters' beliefs up to a predefined maximum theory-of-mind level. These graphs are then used to answer factual questions about the story. The research compares the performance of Symbolic Tom with supervised baselines, such as fine-tuned GPT-3 models and Textual Time Travel, on various datasets. Results show significant improvements in out-of-box LLM performance, particularly in second-order false belief questions and story structure generalization.

The method is evaluated using both in-domain and out-of-domain datasets, demonstrating its robustness and effectiveness in enhancing LLMs' ability to reason about mental states. The presentation concludes by highlighting the potential of Symbolic Tom to improve the interpretability and performance of LLMs in understanding complex narratives.</sample>
    <sample id="180">The name of the speaker is Myra.</sample>
    <sample id="181">The paper introduces a study on constrained language planning, focusing on the challenge of generating scripts that are both semantically complete and faithful to specific constraints. The authors address the limitation of large language models in handling specific goals with multiple constraints, such as making a chocolate cake. They propose an over-generated zero-shot filter method to improve script quality by selecting the most faithful scripts based on semantic similarity and key constraint words. The study also develops a symbolic knowledge distillation method to create a high-quality dataset for constrained language planning, named CoScript, which consists of 55,000 specific goals with scripts. This dataset is designed to enable smaller and specialized models to perform well in constrained language planning tasks. The results show that CoScript can significantly enhance the performance of smaller models like T5, indicating their potential to surpass larger models when trained on suitable data sets.</sample>
    <sample id="182">Tropicalism is a trope that connects to the portrayal of Latinas as vibrant and curvaceous, which reflects harmful stereotypes.</sample>
    <sample id="183">The authors gave prompts to human subjects and found that they were able to surface racial stereotypes.</sample>
    <sample id="184">CXMI</sample>
    <sample id="185">DrBERT is a biomedical model in French, while ChuBERT is a clinical model.</sample>
    <sample id="186">hi i'm mira and today we'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esemdermouch and dan doroski in recent years many have documented the prevalence of social bias and stereotypes in large language models or llms however these measures have various limitations they usually rely on hand constructed datasets that are very time consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture a very general broad associations like negative associations with particular groups furthermore most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique low side of harm to overcome these limitations we rely on the property that these newer instruction tuned llms are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so here are some example generations from gpt four immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words there are some interesting patterns the asian woman is depicted as unassuming the middle eastern woman is referred to using words like exotic and like referring to a mesmerizing region and both of the women of color personas make references to ancestry while the white man persona has nothing of the sort to capture these patterns our method has two parts the first one is generating these personas our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes and also this enables direct comparison between our generated personas and the human written responses the second part is marked words which is a method to identify the words that distinguish marked groups from unmarked ones which i'll elaborate on shortly the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify one man warrior and mark the term with women and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group so for instance for the personas of black woman we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones however when we actually look at the distribution of the words in lexicon we find very different things so while the generated personas have much higher rates of the lexicon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic so really just only the positive or at least non-negative ones and in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these seemingly positive portrayals reflect harmful patterns in our analysis we reveal how these seemingly positive portrayals reflect harmful patterns first for mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm this contributes to a long legacy of discrimination and othering for these groups furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and corvaceous which connect to a trope of tropicalism for asian women the words are things like petite and delicate and silky which connects to a long history of asian women being hyper sexualized seen as very docile and submissive and so on and finally for black women we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on these people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these prunishous patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening um have a good time at ac</sample>
    <sample id="187">Two authors are involved in the paper.</sample>
    <sample id="188">Iterative transfer learning is a process of fine-tuning a model on both tasks, with the best performance achieved by first fine-tuning the C-E task followed by further fine-tuning on debate.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="190">An attacker can extract model parameters through an EaaS by learning from the embedding and providing similar services.</sample>
    <sample id="191">Three authors are involved in the paper.</sample>
    <sample id="192">The presentation introduces a new optimizer called CAN, which aims to achieve both fast convergence and low memory usage in the training of large language models. It starts by explaining the challenges with existing optimizers like Adam, which require significant memory for maintaining first and second moment estimates. The presenter then discusses non-negative matrix factorization (NMF) as a preliminary technique that can significantly reduce memory requirements from O(MN) to O(M+N). However, NMF operations in AdaFactor introduce instability during training, leading to slower convergence compared to Adam.

To address this, the presenter proposes an adaptive confidence-based updating strategy that uses the residual between predicted and generated updates to adaptively adjust the update step size. This approach is designed to maintain stability while achieving faster convergence. The presenter compares CAN with Adam and AdaFactor on three large language models: GPT-2, T5, and BERT, showing that CAN achieves better performance with a significant reduction in memory cost. The results are further validated on downstream tasks, demonstrating the efficiency of CAN in terms of both performance and memory usage.</sample>
    <sample id="193">To determine the number of annotators used to create the initial dataset, we need to analyze the information provided in the text. The text states that around 1000 examples of discourse unit pairs were collected for the initial dataset. However, it does not explicitly mention the number of annotators involved in the annotation process.

Given the context and the typical practices in such studies, it is reasonable to infer that multiple annotators were likely involved to ensure the accuracy and reliability of the annotations. However, without specific information on the number of annotators, we cannot provide an exact count.

Therefore, based on the information available, we can conclude that multiple annotators were used to create the initial dataset, but the exact number is not specified in the text.</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University and the University of Washington.</sample>
    <sample id="195">The speaker introduces a new method for answering complex questions called "Reasoning over Hierarchical Question Decomposition Trees" (RQDT). This method involves breaking down complex questions into smaller sub-questions and then using knowledge sources to answer each sub-question. The speaker explains that traditional methods of answering complex questions have limitations, such as incomplete knowledge bases and difficulty in integrating knowledge from different sources. To address these limitations, the RQDT method uses a two-stage framework: first, it builds a hierarchical question decomposition tree (HQDT) to understand the structure of the complex question, and second, it performs probabilistic reasoning over the HQDT to fuse knowledge from various sources and generate the final answer. The speaker demonstrates the effectiveness of the RQDT method by comparing its performance on challenging datasets with other methods that only use knowledge bases or text corpora. The results show that the RQDT method outperforms existing methods, especially when supplemented with additional knowledge sources.</sample>
    <sample id="196">Lisa bought and Meggy</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are BERT, RoBERTa, and GPT-3.</sample>
    <sample id="198">Because large language models are coming up with longer and longer context windows, so it's crucial that we evaluate the models' acceptability throughout the context window.</sample>
    <sample id="199">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="200">Yes, the annotators know about the entity in advance.</sample>
    <sample id="201">The evaluation used state-of-the-art NMT metrics and additionally showed expert-based human evaluation results.</sample>
    <sample id="202">The regression in generalization impacts all NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can influence the research process and its outcomes, leading to systematic performance differences of technology between populations.</sample>
    <sample id="204">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="205">The speaker is presenting their work on the political biases in language models. They explain that language models are trained on large-scale web crawl data, which includes political news media from various sources. This has led to a mixed blessing for language model applications, as they can learn from diverse perspectives but also inherit social biases from the training data. The speaker proposes to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks. They aim to evaluate the political leanings of language models and how different political leanings affect performance on downstream tasks like hate speech detection and fake news detection. The results show that language models have varying political leanings, with GPT-4 being the most liberal. The speaker also investigates whether language models pick up political biases from training data and whether these biases result in fairness issues in NLP applications. They conclude by highlighting the need to acknowledge and tackle the fairness issues resulting from language model political leanings and the dilemma of sanitizing political opinions in language model training data.</sample>
    <sample id="206">They use the C-E model for transfer learning.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the latest test sets.</sample>
    <sample id="208">Three</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 10.</sample>
    <sample id="210">The speaker's name is Zhu Heng.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for future research on automatic text simplification.</sample>
    <sample id="212">They experiment with 55,000 smaller models in the paper.</sample>
    <sample id="213">OFA</sample>
    <sample id="214">Hello everyone, my name is Jing Wei from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about paper Are you copying my model? Protecting the copyright of large language models for embedding and services View backend watermark Let's first introduce the background about embedding as services Currently, large language models such as GPT, LLaMA, Prol are exceptional in natural language understanding and generation Embedding as services is one of the services built upon large language models to assist various NLP tasks For example, OpenAI offers a GPT-based embedding API However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services Therefore, it's necessary to protect the copyright of embedding as services To protect the copyright of embedding as services One of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark The watermark method need to meet the following properties First, the method should be applicable to embedding as services Second, the watermark should not degrade the utility of the provided embeddings Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily Finally, the watermark need to be transferable to the attacker's services during the model extraction process Existing works can be broadly classified into four categories However, this method either not applicable to embedding as services or lack of transferability Therefore, in this paper we propose embedding marker which is a backdoor based watermark method applicable to embedding as services Then let me introduce the details of our embedding marker Embedding marker contains two main steps Watermark injection and Copyright verification Before these main steps, we first select a trigger set The trigger set is a group of words in a moderate frequency interval We assume the provider can collect a general text corpus and count the word frequency with it In watermark injection, we first define a target embedding When a user send a sentence to the provider service, the provider counts the trigger number in the sentence The provided embedding is a weighted summation of the target embedding and the original embedding The weight of the target embedding is proportional to the number of triggers in the sentence When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding Copyright verification is to detect whether a model behind another service contains the watermark We first construct a backdoor and benign dataset Backdoor dataset contains sentences of which all words belong to the trigger set While all words in the sentences of benign dataset do not belong to the trigger set Then the provider requests embeddings from the stalker service with the dataset The cosine and L2 similarity between the requested embedding and the target embedding are computed We compute the similarity difference between benign and backdoor dataset, which is defined as Delta cosine and Delta L2 Meanwhile, we also apply KS test and use its p-value as the third metric We conduct experiments on four datasets AGNews, Mind, SST-2, and Yahoo! spam We assume the provider apply Wikipedia dataset to count word frequency The results on four datasets show that our embedding marker can have great detection performance while keep great utility for downstream tasks We also validate the covertness of the provided embedding by visualizing the embedding of sentences on four datasets VPCA The legend of the figures means the number of triggers in each sentence As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings That's all, thank you. Welcome to discuss with us</sample>
    <sample id="215">The speaker, Adam Siprowski, discusses the dependency structures of coordination in linguistics. He explains that different theories and approaches assume various dependency structures, such as the universal dependencies (UD) structure, Igor Miltruk's meaning text theory, the Prague approach, and the multi-headed approach. The UD structure assumes a symmetric structure where the first conjunct is the head of the whole coordinated structure, while other approaches like Miltruk's and the Prague approach also assume symmetry. However, the multi-headed approach assumes an asymmetric structure where all conjuncts are heads of the coordinated structure.

Siprowski then introduces the principle of dependency length minimization, which suggests that shorter dependencies are preferred. He illustrates this with examples from English, showing how direct objects are closer to the verb and how long direct objects can be moved after the adjunct to satisfy this principle. He also mentions statistics extracted from the enhanced version of Penn Treebank, which confirm that left conjuncts tend to be shorter and that this tendency grows with the length difference between the two conjuncts.

The speaker concludes by stating that the observed tendency for left conjuncts to be shorter only occurs when the governor on the left is absent, and that this effect disappears when the governor is on the right. He provides a detailed analysis of this phenomenon and its implications for the debate between asymmetric and symmetric structures of coordination.</sample>
    <sample id="216">hi i'm sarah appy from the university of toronto and fondation brunno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matthew negri and marko turkci what is simultaneous speech translation simultaneous speech translation or simulest is the process of translating spoken language into a text in another language in real time enabling cross-language communication and what are the problems of the current simulest models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline nst models without retraining or adopting specific architecture for simulest use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the greatest attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to our word is emitted if the tension is not concentrated that is this sum is below a certain threshold alpha towards the last lambda speech frames meaning that the received information is enough stable for example if if we receive a speech chunk containing i'm going to talk about and our model predicts the translation in german and we will look at the cross attention weights we will see that the first two words points to the earliest received speech frames while the last word points to the last received speech frames as lambda speech frames this means that the first two words will be emitted while since the sum of the cross attention is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no words points to the last lamb lambda speech frames this means that these three words will be emitted if we look at the main results of a dot we will plot the simultaneous speech translation results on on graphs in which we have blue on one side that measure the translation quality and average legging that is the latency measure and we also consider the computational aware average legging that accounts for the model's computational times to predict the output so we want our curves to be as high as possible on this plot but also we want that they are shifted on the left and we compare with proper strategies that are also applied to offline models that are the weight key strategy and the local agreement and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation these are all the results of the simultaneous speech translation strategy on german and we see that a dot outperforms all the strategies applied to offline models since their curves are shifted over the left and we also see that if we consider the actual elapsed time or the computational aware time a dot is the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="217">The speaker introduces a new method for generating multi-tributal controllable dialogue, focusing on compositional generation. They propose a disentangled controllable generation (DCG) that learns attribute concepts from single values and uses a disentangle loss to disentangle different attribute combinations. A unified reference-free evaluation framework (MAE) is introduced to evaluate the effectiveness of the method. The model is based on the dialog GBT framework with a compositional prompt module. Two types of prompts are designed: attribute-oriented prompts and task-oriented prompts. The results show that the DCG outperforms other baselines in controllability and test quality.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The speaker introduces a research assistant at academia sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is done with Yushang Wang and Chen Weiying, under the advisement of Professor Jero and Tran Luong. They discuss the background of financial report analysis, which is the foundation of their work, including text definition and approaches. They consider the Form 10-K as their target corpus, an annual report required by SEC that contains many details of companies' important activities. However, mining useful information requires lots of human efforts. This work was motivated by two observations: first, they observed that the words in a company's report are very similar, about 80% of tokens are the same, and the contents are largely dependent; second, they observed that the words in the company's report are very similar between two reports in the continuous years. For example, the report in 2018 is similar to the one in 2017. Based on these observations, they introduce a highlighting task and a multi-stage pipeline. They define the reference to target structures in their task, where the target and reference refer to the report of interest and the report at its previous year. Therefore, a highlighting model should compare and contrast the context between targets and references, like this figure. So the goal of this highlighting task is to find the operational roots between a given pair T and R. Formally, the model will predict the highlight word importance, and therefore we can measure the performance of highlighting. For example, the word "decrease" is supposed to have higher importance in this context. This is their proposed pipeline. Stage zero is document segmentation, stage one is the relation classification, stage two and stage two plus are the model and the model fine-tuning. Due to the time limit, I would not talk about stage zero more details can be found in our paper. For the stage one, they will classify all the pairs into three types. Type "same" refers to the pairs have higher syntactic and semantic similarities. These pairs are frequently appeared such as company's regulations. Reverse pair have similar syntactical pattern but in fact the two segments disclose very different meaning. Mismatch pair are more like a debut information or company's new operations. For the model fine-tuning stage, they first use an external dataset, ESNLI, for out-of-domain fine-tuning. ESNLI is a natural language inference dataset with token annotation. For example, the word "from" is the rationale according to the context of this pair. For in-domain fine-tuning, they use the reverse pairs, the reverse words as pseudo positive labels, and they randomly label few other words as negative. In addition, they mix different objectives. They use the soft labeling techniques by mixing cross entropy loss and KL divergence. Therefore, they can alleviate the problem from low-quality pseudo labels. The variation dataset includes ESNLI pairs and our release final dataset. They use two metrics to judge the performance, our precision indicates the precision over recall, PCC means the correlation between prediction and annotations. This table shows that their domain-agnostic highlighting model achieve the best performance on final, and even preserve the generalization capability as you can see the performance on ESNLI. We further observe that their methods can benefit on similation, the mismatch pairs, which we didn't use during training. In conclusion, they propose a highlighting task with the release final dataset and a simple pipeline with two-stage fine-tuning. There are many other future works we would like to try, including improving effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well. Yeah, that's it. So please refer to our paper and GitHub for more details and feel free to ask us if you have any question. Thank you.</sample>
    <sample id="220">The authors of the paper are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed the German to English language pair.</sample>
    <sample id="222">The audio discusses the adaptation and annotation of challenges in open-domain question answering, focusing on the motivation to enhance this process. It introduces a method where relevant passages from a document corpus (e.g., Wikipedia) are retrieved using a retriever model, and then a reader model processes these passages along with the question to generate an answer. The challenge lies in the sparsity of domain-specific data, such as biomedical documents, which can lead to incorrect responses due to lack of training on specific domain data. To address this, the work proposes three main contributions: investigating data interventions for out-of-domain generalization, identifying the type of dataset shift a new domain exhibits, and determining effective data interventions based on the type of shift. The setup involves a general-purpose source domain (Wikipedia) and various target datasets across different domains. Data interventions include few-shot and zero-shot methods, which improve performance by 8% and 11% respectively. The work also explores the impact of format changes, named entity recognition, and context distribution on model performance. Finally, it uses a taxonomy-based compatibility measure to estimate the nature of incompatibility between the target domain and the source model, mapping datasets to categories like no shift, concept shift, covariate shift, and full shift.</sample>
    <sample id="223">The speaker's name is Changbing.</sample>
    <sample id="224">The models investigated during the experiments were the Long Impart model for document-level simplifications and the normal-based Long Impart model for sentence-level simplifications.</sample>
    <sample id="225">53 tasks are used for training and 19 tasks are used for testing.</sample>
    <sample id="226">Two authors are involved in the paper.</sample>
    <sample id="227">The audio discusses the challenges in grounding language understanding, which involves mapping natural language expressions into executable plans or programs. The main reason for this challenge is the lack of grounding during pre-training, as most language models are pre-trained with text corpora without grounding. This gap between pre-training and downstream applications makes grounding language understanding particularly challenging. Existing research typically uses language models to directly generate plans, but this can result in grammatically invalid or non-executable plans. The proposed framework focuses on discrimination instead of generation, where a symbolic agent interacts with the environment to propose candidate plans, and a language model scores and ranks these candidates. This approach leverages the strength of language models in discrimination tasks. The framework is demonstrated to be effective across different language models and training settings, showing strong sample efficiency and robustness.</sample>
    <sample id="228">The authors conducted experiments on four datasets: AG News, Mind, SST2, and Yahoo.</sample>
    <sample id="229">The English content is a presentation by Gabriella Katalin Szcs, discussing the importance of text revision in professional writing and its role in argumentative writing. She introduces two tasks: suboptimal claim detection and claim improvement suggestion, and explores how to model the quality of argumentative texts using implicit revision patterns from collaborative online debate platforms like Kiyo. The presentation highlights challenges such as representativeness and reliability, model complexity and architecture, contextual information dependence, and topical and user bias. It concludes that revision-based data can be effectively employed for detecting suboptimal claims and that the impact of contextual information varies depending on the task and quality issues.</sample>
    <sample id="230">hi everyone i'm kostya senna and i'm pleased to welcome you to our talk of our acsl 2033 paper language model acceptability judgments are not always robust to context this is a joint work with john wathier aron mueller kanishka mishra gary fontes roger levy and adina villiams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like bleep syntax jim or acceptability in terms of stereotypes such as crowd spares and in this a minimal pair paradigm the typical way to evaluate language models is that you show like a acceptable sentence or grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is that the model basically puts more probability to the acceptable sentence the current mpp pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the models acceptability throughout the context window and that is what we are trying to do here we're trying to revisit the mpp pipeline by asking the model to evaluate acceptability on longer and longer sequences so that is the approach so what we do is that we simulate these longer sequences we revisit the data sets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those datasets so for example here we have chosen like a typical pair of grammaticality from the bleep data set from the adjunct island case and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from augment island and then we add it as a prefix to both the acceptable query and the unacceptable query so we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability and we can also do the same by choosing sentences from a different subset or a different data set so that is what we call as the mismatch scenario so here the sentences are still coming from relevant data sets but it's not from the same data set that you are evaluating with and we can do the same for unacceptability case finally we can choose sentences from a completely unrelated domain such as wikipedia so this will tell us like whether the models acceptability judgments are actually impacted by any context like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at so how does the model do first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are mostly robust for arbitrary context length we increase the context length toward up to 1,244 for to max out opt and gbt two models and we saw here in the orange dot line the mpp judgments are relatively stable now what happens when we choose sentences from the same data set so here we are choosing creating sentences from acceptable and unacceptable domains from the same bleep or syntax gym data set and there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes but when we match the structure that is when we choose the sentences from the same phenomena in bleep person text gym we see a massive increase or a massive decrease in of the mpp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change it course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation that the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">Avi Lillard</sample>
    <sample id="233">The audio content is a presentation by Sarah Papi from the University of Trento and Fondazione Bruno Kessler. She introduces a joint work with Matteo Negri and Marko Turki on simultaneous speech translation (Simultaneous Speech Translation or SMT). The presentation explains that SMT is the process of translating spoken language into text in another language in real-time, enabling cross-language communication. However, current SMT models face challenges such as specific architectures requiring additional modules for optimization, long and complicated training procedures involving different optimization objectives, and maintaining multiple models to achieve different latency regimes. To address these issues, the proposed solution involves using existing offline NLP models without retraining or adopting specific architectures for SMT. The strategy involves using only one model for every latency regime and handling latency through specific parameters. The attention mechanism between audio input and textual output is leveraged to decide whether to emit partial translations based on where the attention points. The presentation also discusses the performance of the proposed method compared to other strategies applied to offline models and state-of-the-art architectures specifically tailored for simultaneous speech translation.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, with the majority of sentences showing a difference of more than one BLEU point.</sample>
    <sample id="235">The authors of the paper are affiliated with the University of Edinburgh, the University of Cambridge, and the University of Oxford.</sample>
    <sample id="236">The 5 expert-written instructions are used to represent the input texts, images, instruction and bounding boxes in the same token space.</sample>
    <sample id="237">The authors propose a diagnostic test suite for knowledge integration, which includes a coreference resolution task designed to probe the ability to draw on knowledge available in different sources.</sample>
    <sample id="238">The video introduces a new benchmark dataset called "MeetingBank" developed by Yebuwen Hu from the University of Central Florida. It addresses the challenges of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. The dataset includes 1,366 city council meetings with nearly 7,000 instances, featuring meeting transcripts, reference summaries, and URLs containing various useful resources. Data collection involves converting audio to transcripts using speech-to-text APIs, identifying meeting types, and aligning time-stamped segments with corresponding transcripts and reference summaries. The dataset provides detailed statistics on meeting duration, token count, speaker frequency, and year period. Evaluation metrics include coverage and density scores, with results showing that Seattle and Boston have the highest density scores, indicating extensive editing. The video also discusses model evaluation using both automatic metrics and human evaluation, highlighting GPT-3's exceptional performance in fluency and coherence but less impressive in informativeness and factuality.</sample>
    <sample id="241">The speaker, Ethan, introduces his paper on early misinformation detection for COVID-19 treatments. He discusses the challenges of evaluating such systems, including unrealistic data sets and the possibility of leaked counter-evidence. He also highlights the lack of human-centric approaches in current methods, which often exclude human input throughout the detection process. Ethan proposes a framework that integrates human feedback at various stages to address these deficiencies. The system he describes includes two main components: claim detection using keyword filtering and a TF-IDF model for question answering, followed by ranking claims based on trendiness. The second component focuses on policy violation verification using a stance classification model. Ethan evaluates the system's effectiveness in detecting unapproved treatments before their first appearance in news articles and its ability to flag policy violations. He concludes by emphasizing the importance of realistic evaluation frameworks and the potential impact of such systems on future misinformation detection.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">There are 6 authors involved in the paper.</sample>
    <sample id="244">Judges decide cases in law courts.</sample>
    <sample id="245">The speaker is presenting a work called "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The presentation will cover the following topics:

1. **Introduction**: The speaker introduces themselves and explains the purpose of the presentation.
2. **Motivation**: The motivation behind the work is to address the challenges of automatic matrixes and the lack of understanding of best practices for recruitment on MTurk.
3. **Methodology**: The methodology involves a two-step pipeline for finding high-agreement Amazon Mechanical Turk workers, including qualification settings, qualification tasks, endurance tasks, reference-based tasks, and an analysis of cracks across annotation sources.
4. **Results**: The results show that 26 MTurk workers passed the first stage of qualification tasks (8 gold and 18 silver), with a success rate of 13% out of 200 participants. In the second stage, 12 MTurk workers passed (4 gold and 8 silver), with a success rate of 6%.
5. **Performance Metrics**: The figure on the right shows the Krippendorff Alpha cross groups, with the best Krippendorff Alpha being 0.443 for the reference-based task.
6. **Comparison to Cloud Research Workers**: The performance of MTurk workers was compared to cloud research workers, with the best Krippendorff Alpha being 0.513 but with lower task acceptance rates.
7. **Analysis of Cracks**: The analysis of cracks across annotation sources showed a significant Spearman's correlation between the pipeline and cloud research workers.
8. **Conclusion**: The conclusion highlights that the pipeline can achieve high agreement at a lower cost and serves as a best practice for high-agreement annotation at large scale. The speaker also mentions limitations such as only testing English summarization on the MTurk platform and the inability to guarantee training of cracks.

Overall, the presentation provides a comprehensive overview of the work, its methodology, results, and implications for future research.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The speaker introduces a new dataset called FactKG, which focuses on fact verification via reasoning on knowledge graphs. The dataset uses the DBpedia knowledge graph and includes claims in both written and colloquial styles. It supports five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset is designed to be practical for use in modern dialogue systems that communicate with internal knowledge graphs. The speaker also mentions that their proposed model outperforms all other baselines, including claim-only baselines and a model that uses only graph evidence.</sample>
    <sample id="248">The annotators for NLPositionality are not balanced in regard to each demographic, as the study found that datasets and models are most aligned with English-speaking countries and people with college or graduate education.</sample>
    <sample id="249">The sentences in the acceptable domain were perturbed by trying to preserve the relevant structure but adding noise to the input.</sample>
    <sample id="250">Dimensional evaluation means to evaluate multiple dimensions of chat quality on a finer-grained level.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China.</sample>
    <sample id="252">Welcome to our presentation. My name is Sai Kiran Nankillla. I am a master's student at IIT Kanpur. I am excited to present our work, You Create: Unsupervised Case Retrieval using Event Extraction. This is a joint work along with Abhinav Joshi, Akash Sharma and Ashutosh Modi. Legal professionals such as lawyers and judges have traditionally relied on their experience to cite relevant past precedents known as cited documents. However, with the increasing volume of cases, it becomes very challenging. This is where the task of prior case retrieval comes to play. Given a legal query document, the prior case retrieval task involves retrieving relevant candidates from a candidate pool. These retrieved candidates should be both relevant to the query document and are cited within it. Here is an example showing query and candidate documents. Please note, relevance in legal domain is mainly about similar factual situations. In this work, we have made two key contributions to the field of prior case retrieval. These contributions are ILCR dataset and the You Create pipeline. The ILCR dataset which stands for Indian Legal Prior Case Retrieval dataset is a new benchmark for PCR task. It is a collection of 7,700 legal cases with 6.775 average iterations per query document. This provides a comprehensive testbed for assessing the performance of PCR algorithms. Here is a comparison between CoLI 21 and ILCR datasets. CoLI 21 is an existing dataset for Canadian legal documents. Note that on average, ILCR has larger pool of cases containing longer documents, a larger vocabulary and more number of iterations. We have created the ILCR dataset using the publicly available data. For more details, please check out the paper. Our next contribution, the You Create pipeline leverages the unsupervised learning techniques and introduces event-based approach for PCR task. It demonstrates high retrieval efficiency, low inference time and generalization across Indian and Canadian legal systems without requiring law or demographic specific tuning. In the context of our work, event extraction plays a crucial role. If we consider a case document to be narrative about how things developed, it can be represented as a collection of events. To extract events, we utilize dependency parsing technique using Spacy. In this example sentence, we can see the dependency graph along with entity tags and parts-of-speech tags. By focusing on verb category, we examine the left-right children for different subjects and objects. We form these subject-verb-object triplets based on these relationships. Here, each triplet is called an event. In the You Create pipeline, the query document and candidate documents are sequentially given as input into the event extraction block for further processing. The event extraction block consists of three steps: pre-processing, dependency parsing and post-processing. For each document, we obtain the extracted events. Then, we compute an interaction matrix between the query and candidate events. The green blocks represent the common events. Further, the interaction matrix is used in different retrieval models to obtain a ranking order of the candidates. Here is an example on how the matching events are obtained. In our work, we conducted experiments using a diverse range of models to validate and compare their performance on the PCR task. These models are categorized into three groups: the count-based models, which are applied on the word level; the Transformer-based models; and finally, the event-based models, which provide various approaches for improving PCR performance. Let us examine each grouping individually. We present the best-performing models within the grouping. The scores for the baseline methods are shown in the top row for reference. In count and Transformer-based models, we experimented with both pre-trained and fine-tuned Transformer models including BERT, DistilBERT and DistilRoBERTa. The performance of Transformer-based models fall drastically compared to baseline methods like BM25. We also introduced two legal Transformer models specifically trained on Indian legal text, Incase LawBERT and InLegalBERT. However, the performance in the prior case retrieval task was found to be significantly lower compared to the previous Transformer models. This highlights the complexities and nuances of the legal domain and the need for the tailored approaches. Finally, we experimented with event-based models. In the atomic events model, each event is treated as an atomic unit. That means if two events differ at even one token, they are treated as different. In the non-atomic events model, each word in an event is considered as a separate unit. For the event-filtered documents model, we filter the original corpus so that it contains only the sentences which produce matching events with other documents. Note that all event-based models perform significantly above the baseline. The event-filtered docs is the best-performing model. For more experiments and results, please check out the paper. In terms of performance, the method using event-filtered documents outperform all other methods with a significant boost. This graph compares the performance of various event-based methods with the baseline. From this plot, we can observe that event-based models have lower inference time and higher F1 score compared to the other techniques. Here is a performance comparison on the CoLI dataset. You Create outperform the existing approaches including the recent supervised approach by MFTF Bert team. To the best of our knowledge, this approach is the current state-of-the-art method for the CoLI 21 document retrieval task. In conclusion, with these significant contributions, You Create opens up avenues for further exploration and development in the field of prior case retrieval. Please check out our paper for more details. That's all from our side. Thanks for watching.</sample>
    <sample id="253">Hello everyone, my name is Maria Ydra Aragon and I am going to present our work named DISORDER, a double domain adaptation model for detecting signs of mental disorders in social media. This is a group effort of researchers from Mexico and Spain. First, I want to start with the definition of a mental disorder, that is a psychological syndrome which is associated with the distress and disability that affect your thinking, feeling, mood and behavior. There are different types of mental disorders, for example major depression, PTSD, bulimia and anorexia, and among others. Nowadays, social media content is massive and provides an opportunity to do research on how people undergo difficulties. Many people use online platforms to police or share their daily routines and important events, while others take advantage of the anonymity of these spaces to explicitly discuss mental health issues and seek help. In this work, we aim to contribute to the detection of mental health disorders by automatically analyzing their social media posts. This type of analysis is expected to support a new technology able to warn about the onset of mental disorders and provide supporting evidence. Why using domain adaptation? Sometimes we have insufficient annotated data and want to improve the performance of a model on a target domain. For this, we can use the knowledge learned by a model from another related or similar domain. Take, for example, BERT, BERT is a language model trained from the general data of Wikipedia and Google Books, and we want to train it to a more specific language of Reddit and mental health. Doing this adaptation, we can adjust the vocabulary of the pre-trained understanding of the model and learn the domain-specific task. Here we have the general diagram of our proposed approach. First, we start with a base language model and then integrate information from Reddit and mental health. We also incorporate the knowledge from a lexicon to guide the masking process. The main idea of our proposed approach is first to learn the social media language and then specialize in the mental disorder domain. Using the guiding masking, we want the model to try to focus on important words during the training process. Here we have the general results using the RISK datasets. We plot the precision and recall of our model and the baselines. Our model tends to locate in the main diagonal of the region, indicating its good balance, while other methods have a high precision or recall but score low in the other dimension. Let us illustrate the behavior of the learned model and the kind of textual segment it tends to pay more attention to. First, we analyze the most likely words the model generates when given a sentence with masked word. A sentence, we use example from the Beck Depression Inventory. This clinical tool, which consists of 21 items, aims to identify and measure the severity of typical symptoms of depression, for example, in measure the mood, the pessimism, sense of failure, dissatisfaction, guilt and among others. In the following table, we can see some examples of these sentences and the answers returned by BERT and DISORDER. With DISORDER, the answers tend to have a more negative meaning or psychological orientation compared to BERT. Take, for example, the sentence "I used to be able to cry," where we mask the word "cry" and DISORDER predicts words like focus, talk, breath, sleep, and eat. These words are related to common problems that are associated with mental disorders and cause an interference in the thinking and behavior of the affected person. Here we also present all the words predicted by both models weighted by their frequency. This figure results from applying the two models to the entire set of 21 BDI items. Similar to what happened before, BERT tends to generate more general words, while our model tends to be biased toward words related to mental disorders. Finally, we want to visualize the most important sequences of the text by obtaining the most relevant words and sentences. For this, we use a visualization tool that provides an interactive head view in the form of a graph. We select a depression user with the highest score in the BDI questionnaire and compute the attention scores of the user post. We can observe that the most prominent words are related to anxiety and medication topics that are highly relevant to depression. As conclusion, future work, the combined effect of double domain adaptation and guided masking is effective at capturing signs of mental disorders in social media interactions. Our approach also obtained better results than those achieved by MentalBERT, a model trained with a large amount of data. The evaluation shows a solid balance between finding users and labeling them correctly. In future work, we want to explore the application of different lexical resources as well as using clinical data. Thank you for your attention. If you have any questions, please feel free to ask me.</sample>
    <sample id="254">The speaker introduces a document-level distant relation extraction framework with uncertainly guided label denoising to improve the label quality of DS data. The framework consists of three main components: 1) pre-denoising doca model, which generates pseudo labels using both DS and human-annotated data; 2) instance-level uncertainty estimation, which captures the uncertainty score for overlapping relations; and 3) a re-labeling strategy with dynamic class uncertainty threshold and a multi-phase training strategy to further boost performance. The speaker also proposes Monte Carlo dropout technology in the doca task to model the uncertainty in the pre-denoising doca model. The speaker compares the proposed framework with several strong baselines on two public datasets and shows that it outperforms the previous baselines.</sample>
    <sample id="255">It is crucial for zero and one shot prompting.</sample>
    <sample id="256">hello my name is vasudha and i am a computer science phg candidate at stony brook university i would like to present our work accepted into acsl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship while dissonance is a very common phenomenon we experience in daily decision making they are really ready to find expressed in language among other kinds of discourse relations so why does this matter studying cognitive dissonance can help us understand the effects of dissonance among people track trends in belief values and attitude changes in population high cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups finally cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better to the goal of creating a cognitive dissonance resource we conducted a large-scale annotation of dissonance relations we used a dissonance first approach as seen in the flowchart here tweets were passed using a patty b parser and pairs of discourse units were annotated according to the guidelines that are described in our paper as can be seen here dissonance was only found in three point five percent of the annotated pairs on collecting around thousand examples of discourse unit pairs we ran training for an initial classifier trained only on forty three examples of dissonance to no surprise the classifier performed not much better than chance given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity to alleviate this we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds lowering the overall annotation costs while improving dissonance detection since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of patty b since these two are closely related to the conception of consonants and dissonance and we call them c e here we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with auac 0.62 further on iteratively fine-tuning on both tasks we find that fine-tuning of ce tasks followed by further fine-tuning on debate yields a much better zero shot performance thus this is the model that we use to co-start the active learning next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations whereas iterative updates the model by training on the latest set of data collected over the different strategies we found that cumulative perform equal or better than iterative across the board next to improve the number of dissonance examples we use a probability of rare class strategy prc to select mostly the examples that are highly likely to be dissonant by the current model at any round of al we compare this to the other state-of-the-art state-of-the-art strategies that are commonly used in the community we find that the proposed prc strategy works better than other straight state-of-the-art strategies although the difference is small note that the performance is significantly lower for random on further rounds of al with two best strategies we improve dissonance classification auac to points seven five which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and cost to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple al strategy for rare class acquisition and cold starting al with appropriately designed transfer learning task and helps significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our core data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">The speaker, Chang Sun He, introduces a new method for evaluating the quality of text in natural language processing using large language models. This method involves instructing large language models with specific evaluations and having them rate samples based on those instructions. The speaker highlights that while this approach is not entirely novel, it was innovative at the time of submission to ACL. The motivation behind this work is to find an alternative to human evaluations, which are unstable and difficult to reproduce. Large language models, with their ability to follow natural language instructions, could potentially perform evaluations similar to human evaluations but without the drawbacks. The experiment involved using GPT-2 and ChatGPT models to evaluate stories written by humans and generated by GPT-2. The results showed that while some smaller models did not show a significant preference for human-written texts, larger models like DAVinci and ChatGPT demonstrated a clear preference for human-written texts, suggesting they can be used as alternatives to human evaluations.</sample>
    <sample id="259">The speaker introduces Exemplar, a tool for cross-lingual semantic parsing in multiple natural languages and many representations. Semantic parsing involves building semantic representations of user queries like SQL and Lambda Calculus, while cross-lingual semantic parsing translates queries into different representations across languages. The challenge lies in the lack of coverage on certain languages (e.g., Chinese) and representations (e.g., Lambda Calculus), often evaluated only on specific models.

Exemplar provides a unified dataset with 9 datasets, 5 semantic parsing tasks, 80 representations, and 22 languages across 15 language families. It offers six settings for training and evaluation: translate test, monolingual model, monolingual few-shot, multilingual model, cross-lingual zero-shot, and cross-lingual few-shot transfer.

The speaker evaluates two types of models: encoder-decoder (MT5 and XLM-R+PDR) and encoder-only (PTR). Encoder-decoder models outperform encoder-only models across all datasets. Training in a mixture of languages improves performance, except for English, which shows a drop in seven datasets but gains in three. The cross-lingual performance gap is significant in zero-shot settings but narrows in few-shot settings.

Exemplar is a comprehensive benchmark for cross-lingual semantic parsing, providing insights into the performance of different models and the challenges in cross-lingual and cross-representation semantic parsing.</sample>
    <sample id="260">There are two authors involved in the paper.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">There are two authors involved in the paper.</sample>
    <sample id="263">The presentation introduces a new method for mitigating label biases in in-context learning (ICL), a popular paradigm for utilizing large language models. The speaker explains that ICL is known to be unstable due to various design choices, such as the choice and order of in-context examples, which introduce biases into the model's predictions. The work aims to address these problems by proposing a calibration method to handle all types of biases, including vanilla label bias, context label bias, and domain label bias. The method uses random in-domain words sampled from the task corpus to estimate the model's bias on each label name and then calibrates the model's original predictions. Experiments show that this method significantly improves the performance of ICL on datasets with larger domain label bias. The speaker concludes by summarizing the systematic investigation of label bias problems in ICL and the proposed calibration method, which can improve the performance of large language models.</sample>
    <sample id="264">The English content is a presentation by Ling Wang, a graduate student at Zhejiang University in China. The presentation focuses on the task of transferable audio-visual text generation and proposes a novel task called Transferable Audio-Visual Text Generation (TAVTG). The main challenge of TAVTG is the multi-modal domain shift, such as visual style and audio energy. The presenter introduces three components: Audio-Visual MapNet, Audio-Visual Encoder, and Language Model Generator. The first model, Audio-Visual MapNet, maps different visual concepts across domains into a unified audio-visual semantic space. The second model uses a Transformer-based encoder and generator to generate tokens that are conditioned on the query and optimized for the cross-reconstruction loss. The third model, Contrastive Constructive Learning, uses contrastive learning to optimize the visual-audio alignment and improve the semantic of the tokens. The framework also includes a meta-training framework that can adapt to new multi-modal domains with limited labeled data. The presenter compares the proposed approach with state-of-the-art models, including RNN-based and Transformer-based models, and shows that the proposed approach achieves better performance on both cross-domain setting and low-resource domains.</sample>
    <sample id="265">The speaker's name is Vasudeha.</sample>
    <sample id="266">The authors of the paper are affiliated with the University of Paderborn.</sample>
    <sample id="267">Hello everyone. My name is Yusen Zhang from the Peking University. Today I'm going to present our work: Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Multiple Representations. So semantic parsing is a task to build semantic representations of user queries such as SQL and Lambda Calculus. And crosslingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations, as shown in this figure. We need to translate the query in multiple natural languages using neural models to SQL, Lambda or FQL and so on. Existing crosslingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there lacks of coverage on certain natural language, the Chinese is missing, and lacks of coverage on certain meaning representations, the Lambda Calculus is missing, or they are only evaluated on certain neural model. For example, there's only one single model to evaluate them. So to this end, we propose Exemplar, we provide a uniform dataset Exemplar for crosslingual semantic parsing in multiple natural languages and multiple representations. It contains 90 datasets in various domains, 5 semantic parsing tasks, 80 meaning representations, and 22 natural languages in 15 language families. And to better evaluate our benchmark, we consider the six settings for training and evaluation. The first one is translate test. We use Google Translate API to translate source to the target language, then use monolingual model to train and evaluation. And for example, we train the English model on English query, and during inference, we translate the German query using API to English, and then use the trained model to predict the SQL. And we also test monolingual model. In this setting, the source language is the same as target language, for example, German to German or English to English. We also test monolingual few-shot setting by training monolingual models with only 10 percent of training data. And we test multi-lingual multi-language model which we train one multi-lingual model for all languages. For example, we put the German, English, Chinese queries together to train a multi-lingual model, and during inference, we can use this model to to translate German queries or Chinese query, etc. And we also consider crosslingual zero-shot and few-shot transfer. We train on one source language and transfer to another language. So during training, we train on English query or the combination of English and German few-shot queries to train a multi-lingual model to and predict the SQL output. And we also find many interesting results. So regarding analysis of monolingual models, we evaluate on two groups of models including encoder PDR which stands for multi-lingual pre-trained encoders with pointer-based decoders such as XLNet + PDR and BERT + PDR. And we also evaluate encoder-decoder models which is multi-lingual pre-trained encoder-decoder models such as M-BART and MT5. We found that encoder-decoder obtains the best performance on all nine datasets. And we evaluate on MT5 and Exemplar XLNet + PDR on multi-lingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain, except that English performance drops in seven datasets and only gains in three datasets. I think this is known as curse of multi-linguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found that for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that for few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results but training on English natural language can significantly boost the performance of few-shot on target natural languages. And we found multi-lingual language models such as codes and blue are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative types of multi-lingual language models. And our result shows many interesting findings and so on. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">Emory University and Amazon Alexa AI</sample>
    <sample id="271">Continous fine-tuning</sample>
    <sample id="272">There are six authors involved in the paper.</sample>
    <sample id="274">The speaker's name is Usen Zhang.</sample>
    <sample id="275">hi i'm zhang bin phd student in university of washington today i'm presenting our work from pretraining data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pretraining data according to a survey of the c four corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications to this end we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks specifically by asking the following questions first how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases secondly how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in nlp applications so specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test this ensures us to do automatic evaluation well grounded in political science literature so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass we can also see that gpt-4 is the most liberal language model of them all and gpt series are generally more socially liberal than bert series and its variants secondly we aim to investigate to which to which extent the political biases of language models are actually picked up from training data so we conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings by further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift for example for roberta further fine-tune further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its in terms of its political biases and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society so we divide pretraining corpora into pre forty fifth president of the united states and after forty fifth president of the united states we separately pretrain language models on the two different temporal corpora we can see that language models generally had a political leaning that is further away from the center after 2017 so this indicates that language models can also pick up the like polarization in our society so last but not least we evaluate language models with different political leanings on hate speech detection and fake news detection to nlp applications that often involve language models and could have very significant implications so we see that if we investigate the per category performance that is to say if we separate the performance into different demographics or political leaning of news media we can see a pattern that for example for hate speech detection left-leaning language models are better at detecting hate speech targeting socially minority groups however are worse at detecting hate speech targeting more powerful groups in our society and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting at black lgbtq plus and other minority communities similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political leaning and vice versa this and we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social calendar there are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models for example if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control so this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings so a little bit of discussion we would also like to highlight that we expose the unique dilemma regarding language model political biases it's like between sisyphus and cyborg this so if we do not sanitize the political opinions in language model training data the bias would propagate from pretraining data to language models to downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electrolyte electrolyte problem okay great i think that's pretty much all i have to dead half for today thank you for your time</sample>
    <sample id="276">The research focuses on evaluating machine translation metrics for Indian languages, specifically Tamil and Marathi. The study involves selecting 200 sentences from the IndicMT dataset to generate candidate translations in English using seven different translation models. Human annotators then evaluate these translations, marking errors, their severity, and providing overall scores. The evaluation covers various error types, including accuracy, meaning-based, fluency, and special category errors. The results show that recent models like NLLB and IndicTrans have fewer errors compared to older models like CVIT. The study also explores correlations between different evaluation metrics and human scores, finding that Comet metric variants outperform Comet baselines in most languages. The research aims to fill the gap in evaluating translations in other languages beyond English and provides a publicly available dataset for further use.</sample>
    <sample id="277">The new method does not have a name.</sample>
    <sample id="278">The "marked words" method is a way to identify the words that distinguish marked groups from unmarked ones, which helps to capture specific stereotypes and patterns without relying on any specific lexicon.</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">The speaker introduces a new framework for emotion regulation in conversations, called MultiEmo. This framework aims to predict the emotion label of each utterance in a dialogue by integrating textual, audio, and visual modalities. The framework consists of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The speaker proposes a novel visual feature extractor called VisiNet, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. Additionally, the speaker designs a multi-modal fusion model called MultiTen, which integrates one modality with complementary information from other modalities through stacked bidirectional multi-head cross-attention layers. The speaker also introduces a sample-weighted focal contrastive loss to address the difficulty of classifying minority and semantically similar emotions. Experimental results demonstrate that MultiEmo achieves state-of-the-art performances on two ERCC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The speaker, Kai Yen, introduces a study on the context-dependent translation of words in multilingual texts. The study uses CXMI (Contextualized Crosslingual Mutual Information) to measure how much context influences translation at different levels (word, sentence, and vocabulary). By analyzing TED Talks translated into 14 languages, they identify patterns such as dual pronouns in Arabic needing context for translation and the importance of context in choosing verb forms and maintaining consistency in translation. They propose a benchmark using multi-lingual discourse-aware taggers to evaluate models' performance on document-level translation, showing that context-aware models outperform those without context for certain phenomena like formality and lexical cohesion.</sample>
    <sample id="282">The speaker introduces a new work in the 2023 edition of the journal "Story Trans Non-Parallel Story Style Transfer with Discourse Representations and Content Enhancing." The work addresses the task of non-parallel text style transfer at the discourse level, which is crucial for imitating author style. The primary challenge lies in imitating the author's linguistic choice at the discourse level. The proposed model, named StyleTrans, combines discourse representation from the source text with normal style embedding to generate text in the target styles. A new training objective is designed to reduce the stylistic features from the discourse representations, pulling the representation derived from different texts closer in the latent space. The model is trained in two stages: the first stage involves transferring the source text with style-specific content keywords masked and then generating the whole text by incorporating these keywords' specificity. The second stage aims to fill in the correct style-specific content and remove the masked tokens. The effectiveness of the model is demonstrated through extensive experiments on Chinese and English datasets, showing strong baseline performance in terms of style control and content preservation.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">The speaker, Pong Pien Shau from Wuhan University, is presenting a long paper at the ACL main conference on universal information extraction. The current spam-based UIE model relies on identifying and labeling spam boundaries in text, which can be ambiguous due to different annotation spans. To address this, the proposed method uses a fuzzy spam boundary mechanism instead of precise labeling. Additionally, there is a mismatch between Transformer feature extraction and information extraction, as Transformers focus on global features that ignore the hypothesis that span has limited length. Therefore, an adaptive attention mechanism is proposed for spam extraction decision-making. The model represents the target boundary as a continuous distribution of correct probability within a specific range, using Rmin and Rmax to represent the start and end of the fuzzy boundary. The function Q represents the correctness of the current position, and the sampling function converts the continuous boundary distribution into discrete values for calculating fuzzy span loss. The boundary distribution predicted by the model calculates boundary cross-entropy with the golden boundary as BCE loss and adds KL divergence between the predicted boundary and the fuzzy spam boundary as supplementary information. To obtain a more reasonable attention distribution for spam extraction, a fuzzy spam attention mask function is proposed. The overall structure of the model includes a fuzzy spam attention layer added only on the top level to guide the model's division process without affecting the text encoding capability. Experiments conducted on three information extraction tasks show significant performance improvement for named entity recognition, relationship extraction, and aspect sentiment triple extraction. The results demonstrate that FS UIE achieves superior performance compared to UIE without the fuzzy spam mechanism, especially on small-scale datasets.</sample>
    <sample id="285">The video discusses the challenges of factually correct summarization and introduces a new evaluation framework for fact error correction models. It highlights that current models often fail to correct factual errors due to flaws in evaluation metrics and the lack of reference correction data. The proposed framework includes three steps: alignment, classification, and comparison, and uses a taxonomy of fact errors based on content and form. The video demonstrates that training fact error correction models with reference summaries from dialogue summarization datasets yields better results than using unreliable factuality metrics. Additionally, combining human-annotated data with synthetic data is shown to be a promising direction for improving model performance.</sample>
    <sample id="286">The speaker's name is James Finch.</sample>
    <sample id="287">Four authors are involved in the paper.</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena are the BLM dataset and the syntax gym dataset.</sample>
    <sample id="289">hello my name is kai yin and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick franz and emily you andrew f d martin and graham newby so a lot of translations depend on context for example how would we translate mole in this sentence well if the previous sentence was things could start to get dangerous if the minister finds out then mole refers to a spy but if the previous sentence was could it be anything serious doctor then mole refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can translate cases like this is pretty hard firstly because only a small portion of translations depend on context which makes corpus level metrics like blue unable to capture these translations and some people have suggested targeted evaluation on context-dependent translations but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we try to answer these two questions first when does translation require context and second how well do models handle these cases to answer the first question we started by measuring how much a word depends on context during translation in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x you can think of cxmi as the information gain from giving context to the model in this work we extend cxmi to point y cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high p six x mi as ones that require context for translation now we analyze words with high p six x mi to look for patterns between these words and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages we perform our analysis at three different levels first we look at parts of speech tags that have high means p six x mi and this allows us to find for example dual pronouns in arabic that have relatively high p six x mi and this can be explained because english doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into arabic and similarly we find that certain languages also require context when we want to choose the appropriate verb form we then look at vocabulary items that have high p six x mi averaged over all of its different occurrences and this helps us identify cases like the one here where in chinese you need context to translate proper nouns to make sure that you're using the same translation within the document and similarly we find that context is supported to translate in the right formality and finally we look at different individual tokens that have high p six x mi and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure such as ellipsis resolution so now we use our findings from our analysis to design a benchmark for document-level translation for each of the five discourse phenomena we identified we create taggers to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse aware or moeda tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the moeda tagger by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the moeda tagger has identified and finally we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation first of all when we use corpus-level metrics so for blue we find that context agnostic models have the best performance but then if we use comet context aware models perform best and if we use word f measure then models with or without context have comparable performance this again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone now we use the moeda benchmark to evaluate models and we find that context aware models are significantly more accurate than the models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document-level translation we also compare different commercial systems and our benchmark shows that deepbell is usually more accurate than google translate for document-level translation to summarize we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="290">Wsl, ftw, cosine, finetuning, wsl</sample>
    <sample id="291">The model is evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="292">hi welcome to our presentation of deeplain a new corpus for german text simplification on the document level and on the sentence level my name is regina sturm and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of texts for example doc of documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german sentence and its translation into plain language to simplify the sentence different techniques are possible as you can see in the example such as lexical substitution clause deletion clause deletion reordering or insertion of words we now propose our new corpus deeplain because in the recent years there were some problems with existing corpora so for example these corpora here are too small to train a text simplification model on the other three models which are proposed in recent years are all automatically aligned which means they can be error-prone in their alignments therefore we propose our new corpus deeplain which is split into two subcorpora deeplain api and deeplain web deeplain api is based on news texts in deeplain api we aligned 483 documents all manually it results in roughly 30 000 13 000 parallel sentence pairs for deeplain web this corpus includes different domains and we also aligned all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods in total we result in 30 000 450 sentence pairs we analyzed our sentence pairs a little bit more so for example on the type of simplification as you can see here the bible texts are much stronger simplified than for example the news text or the language learner texts on all levels regarding for example lexical simplification structural simplification or the overall level of simplification furthermore you can see that our deeplain corpus has a high variety of different simplification transformations so for example in the deeplain api corpus we have much more reorderings and word editions than we have in the deeplain web corpus on the other hand in the web corpus we have much more refrasings so let's now see what we can do with this corpus hello i am omar and now i will talk about the use cases for our dataset deeplain so for the first use case we can evaluate automatic alignment methods in the recent years there has been a lot of alignment methods but in the context of machine translations where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents but in our use case we are trying to extract alignments between sentences of two parallel documents having the same language having the same content but they are on a different complexity levels and now as we have our dataset deeplain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper at the end we concluded that the best alignment automatic alignment method to use for texts for german text simplification is the method of mass align and you can also find the code to run this method on your own documents in the paper the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long import to produce document-level simplifications and we also fine-tune the normal base long the normal base import to produce sentence-level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="293">hi and i'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the alt entities corpus and my name is javahar hasini and this is a joint work with philip radinsky sylvia parati and annie luis our goal is to understand users language when they want to make a choice consider this alternative question did you mean easy on me or i got a feeling here a user wants to select between one of these two songs um the most obvious thing is to use a direct reference for example by saying the name of the song easy on me or its position the first one but sometimes an indirect reference is more appropriate to have a more natural conversation this could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate or when the user wants to specify a preference here are some example in direct references for example the newer one or the song that's not energetic this is an important problem in conversational systems and also for benchmarking llms entity understanding we're not aware of a public dataset a large-scale public dataset for the task so we collect one using crowd annotation our dataset covers three different domains music books and recipes our dataset collection methodology emphasizes informality using a cartoon completion set up the cartoon has three speech bubbles in the first bubble bob says remember that song we were listening to yesterday and with that bob sets the dialogue context in this in the second speech bubble alice says do you mean easy on me or i got a feeling which is the alternative question and in the third speech bubble bob uses an indirect reference to select one of these entities for example the newer one we provide the first and second speech bubbles automatically but the third one is filled in by the annotator um the first speech bubble is chosen from a few manual prompts per domain the second one which is the alternative question is generated as follows we always use a simple template do you mean a or b where a and b are samples from wikipedia here are the different sampling methods we've used when we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation the first one is uniform at random the second one is when the entities have similar titles for example two books with the name the return the third one is when they have similar descriptions on wikipedia and finally when they have similar infoboxes or attributes on wikipedia for example the same genre or the same artist for a song when we show this alternative question to the annotators they know the name of these entities but they don't necessarily know about the entities so what we do is that we show some background knowledge about the two entities for songs we simply show a google search link to each song and then ask the annotators to listen to at least some of each song and read about each song here's for example the google search result for the song easy on me for the recipes and books domain we show some background text from wikipedia for recipes we additionally show their images again from wikipedia so that the annotators know how they look like then we ask the annotators to pick one of these entities for example here the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our dataset for example the one without words not the one with the twelve year old twelve year old boy or the fictional one or comes from alderby john and so on the alt entities corpus has six thousand alternative questions across three domains and it has forty two thousand indirect referring expressions results with t5-xl model are summarized below if the language model has access to the exact same background knowledge as the annotators then the accuracy is really high it's around ninety two to ninety five percent but this is not realistic if the language model has access to some partially overlapping background knowledge then the accuracy is between eighty two to eighty seven percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access only to entity names then the accuracy is only sixty percent so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our dataset thanks</sample>
    <sample id="294">CamemBERT is initially trained on the NAO dataset.</sample>
    <sample id="295">The speaker's name is Adam Siprowski.</sample>
    <sample id="296">The audio content is a presentation by a person named Balrio Basile. The speaker introduces the topic of the presentation, which involves a collaboration between the University of Turin and Amazon Alexa. The focus is on natural language understanding and processing, specifically in the context of irony detection. The speaker explains that irony detection is a challenging task for modern natural language processing models. To address this, they developed a corpus called Epic, which stands for English Prospective Irony Corpus. This corpus was created by collecting data from social media platforms such as Reddit and Twitter over a period of one and a half years. The data consists of pairs of texts, where one follows the other, and annotators were tasked with determining whether the reply was ironic or not. The results showed significant differences in annotator agreement based on various dimensions like gender, age group, and nationality. The speaker also mentions the development of perspective-aware models to model these differences and build more accurate predictions.</sample>
    <sample id="297">The speaker discusses the concept of dog whistles, which are coded messages that convey different meanings to different groups. They provide an example of a speech given by Senator Josh Holey, who is interpreted as having a hidden message about Jewish people. The speaker explains that dog whistles can be anti-Semitic and that understanding them is important for NLP linguistics because they are context-dependent and can evade content moderation online. The project involves developing a typology and glossary with rich contextual information, performing a case study of historical US political speeches, evaluating dog whistle recognition in language models, and conducting a case study of toxicity detection to show how dog whistles can evade content moderation.</sample>
    <sample id="298">The findings that the performance degrades with larger temporal gap led to the conclusion that the temporal drift is the main cause of performance loss.</sample>
    <sample id="299">The speaker, Michaela Skaragakis, is presenting a talk on improving the robustness of AI models using Minimax training. This work is in collaboration with Andreas Bloos from the University of Cambridge. The talk highlights that while AI models have achieved state-of-the-art results across various benchmarks, they often rely on learning and using shortcuts, which are spurious correlations between input attributes and labels introduced during data creation. These shortcuts can lead to poor performance when tested on out-of-distribution datasets where such correlations do not hold.

The presentation discusses existing shortcut mitigation methods, which typically assume access to an auxiliary model designed to learn shortcuts for predictions. However, these methods often require domain-specific knowledge and assume that the learner will exploit the same types of shortcuts as the auxiliary model. This can lead to additional computational overhead and may not always be practical.

The proposed training method aims to reduce the reliance of AI models on shortcuts by emphasizing underrepresented hard examples during training. This is achieved through a Minimax training objective that balances the loss of the main task with the loss generated by the auxiliary model, which maximizes the learner's loss by generating example weights. The method does not make assumptions about the type of shortcuts in the dataset and relies on the learner's own training dynamics to generate example weights.

The proposed method has been evaluated on three commonly used AI datasets (MNLI, Fever, and QQP) and their corresponding out-of-distribution test sets. The results show that the Minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy. The method also examines the transfer of performance to larger models, synthetic shortcut datasets, and the effect of pre-training the learner and the size of the auxiliary model. Additionally, a qualitative evaluation of the learned example weight distribution is conducted.

The speaker invites the audience to discuss the work further during the post-session.</sample>
    <sample id="300">The speaker introduces a new task called interactive dictation, which allows users to dictate and edit documents using their voice in a natural and intuitive manner. The process involves flexible interleaving of dictation and editing, using natural language utterances to specify edits. The speaker explains that most speech-to-text systems only support dictation and do not allow for invoking edits through vocal commands. However, the interactive dictation task is characterized by flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits. The speaker also mentions that they have designed a data collection interface and built a dataset for this task, and have created a baseline system that performs each of the four steps involved in the process.</sample>
    <sample id="301">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting your work and l positionality characterizing design by biased data sets and models this work was done in collaboration with some folks at the university of washington and um the allen institute for ai namely sebastian santy roman lobrass caterina rynica and martin sap so let's start off by imagining that you're working for a newspaper and you're sifting through comments under your news article trying to remove toxic content you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're karl jones um where perspective api is able to detect correctly toxic instances but that's not really the case for dithia sharma where perspective api is really not as sensitive to offensive terms that are more common in indian contexts this is an example of a design bias where we see systematic performance differences of technology between populations design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspectives that people hold as a result of their demographics identity and life experiences this is a concept widely used in critical studies specifically in feminist and queer academic spaces and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make and so one question that people might ask is do datasets and models have positionality and we're not trying to say that models themselves and datasets themselves have demographic identities and life experiences but they do aggregate judgments and opinions of real people and can thus represent certain positionabilities over others so prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and datasets as well as theoretical definitions of model positionality however these works really don't look at comparing end users with the datasets and models themselves and studying model and dataset positionality is increasingly important as nlp tasks become more subjective and socially oriented and it's challenging to characterize how these positionabilities are skewed because not all decisions are documented and many models are hidden behind apis so to study dataset and model positionality we actually compare the annotations with real users with existing datasets and models we do this through our framework nlp positionality our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we opt to do this over looking at the demographics of original datasets annotators because usually only a few instances annotators annotate each instance and because demographics are rarely collected and shared and so we opt to re-annotate data to get many annotates per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and datasets using pearson's r correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions our framework is largely enabled through lab in the wild an online crowdsourcing platform former hci collaborator in lab in the wild is an online experimentation platform where we can recruit diverse volunteers compared to the platforms like mturk which largely have participants from the us or india and further lab in the wild stills able to get high quality data we host two tasks on lab in the wild one of them being social acceptability and the way this works is that participants will read a situation from the social chemistry dataset and then they'll write how socially acceptable a situation is afterwards to stay engaged in the study they can compare their responses to an ai and others we've then compared these annotations with social chemistry delphi and gpd four we then replicate a very similar setup for the toxicity and hate speech detection task where they'll read an instance from dinahate and write whether they think it's an instance of hate speech we then compared these annotations with dinahate perspective api rewriter api hate roberta and gpd four our study in the end amassed over 16,000 annotations from over a thousand annotators from 87 countries so now we're better equipped to answer who do nlp datasets and models align with the most we find that there is positionality in nlp for example we find that datasets and models are most aligned to english speaking countries so for the gpd four social acceptability analysis we find that it's most aligned to confucian and english speaking countries we find that dinahate is also most aligned to english speaking countries we also find most additional alignment with people who have a college education so for gpd four in the social acceptability task we find that it's most aligned to people with a college education or graduate school education and we find the same for dinahate where it's most aligned to people with a college education however when models and datasets are aligned to specific populations some are inevitably left behind an example of this is that datasets and models are less aligned to non-binary people compared to the men and woman counterparts we find this in the gpd four social acceptability task as well as the dinahate task analysis as well so given that there is positionality and alignment in nlp what can we do about it so we have a few recommendations for this first one is keep a record of all relevant design choices throughout the research process and the other is to do nlp research with the lens of perspectivism our third recommendation is to build specialized datasets and models within four specific communities and a good example of this is the musakani initiative i mean we want to emphasize that inclusive nlp isn't just making you know all technologies work for everyone and so that concludes our presentation but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper thank you</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because the alignment between input and output is not given in the training data, so we don't know which multiset token it came from.</sample>
    <sample id="303">The authors recommended increasing transparency about bias mitigation methods because it is difficult to determine the cause of positive stereotypes and essentializing narratives, which could be due to over-expressive value alignment or anti-stereotyping methods.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are grammatically incorrect or do not make sense.</sample>
    <sample id="305">The audio content is a detailed presentation by Dawei, a PhD student at Saarland University in Germany, discussing the concept of weakly supervised learning (WSL). Dawei explains that WSL involves using weak labeling sources such as simple heuristics, knowledge bases, or low-quality crowdsourcing to label data. He contrasts this with human annotations, noting that while weak annotations are cheaper, they are also noisy and can lead to memorization rather than generalization in neural networks. The presentation delves into the challenges of training neural networks on weakly labeled data and introduces WSL approaches that aim to robustly train models despite label noise. Dawei highlights the importance of clean validation sets for WSL methods to work effectively, presenting research questions and findings related to the necessity and quantity of clean samples for WSL. The presentation concludes with recommendations for future work, including reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, and considering continuous fine-tuning as a practical method.</sample>
    <sample id="306">The audio features a man speaking in a foreign language, followed by another man speaking. The first man continues to speak, and then the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes speaking, and the second man speaks once more. The first man continues to speak, and the second man speaks again. The first man resumes</sample>
    <sample id="307">The authors used public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering to evaluate their models.</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, is presenting her work on NL Positionality, which characterizes design biases in AI datasets and models. She explains that these biases can lead to systematic performance differences between populations, such as the sensitivity of Perspective API for detecting toxic content in different contexts. Jenny's research focuses on the positionality of NLP researchers and model developers, who can influence the research process and its outcomes due to their demographics, identity, and life experiences. She suggests that data sets and models can represent certain positionabilities over others, and proposes a framework called NL Positionality to compare annotations with real users and existing data sets and models. Her study involves re-annotating data sets with diverse annotators and comparing their annotations to models and data sets using Pearson's correlation score. The framework has been enabled through Lab in the Wild, an online crowdsourcing platform, and has collected over 16,000 annotations from over 1,000 annotators from 87 countries. Jenny's findings highlight the need for more inclusive NLP practices, including keeping records of design choices, conducting research with a lens of perspectivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">ABC eval behavior labels</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">The authors of the paper are affiliated with the University of Tbingen.</sample>
    <sample id="312">MultiInstruct is the first multi-modal instruction-tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories, derived from 21 existing open-source datasets and equipped with five expert-written instructions for each task.</sample>
    <sample id="313">2</sample>
    <sample id="314">Binary coordination is a type of coordination where two elements are coordinated.</sample>
    <sample id="315">The prompts used in this study were on average 10 words long.</sample>
    <sample id="316">The findings suggest that the smaller T5 model can generate scripts of high quality, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="317">The speaker is introducing a new method for information extraction called CodeIE. This method transforms the test-to-structure information extraction task into a structure-to-structure code generation task, which allows for easier conversion of input tests to structured formats during the input stage and ensures aligned structures in the output stage. The speaker evaluates their method on three datasets: named entity recognition, relation extraction, and code generation. They compare the performance of their method with traditional baseline models such as UIE and large language models like GPT-3. The results show that their proposed approach using code pre-training language models and code format prompts significantly outperformed the traditional baseline models. Additionally, they analyze the performance of different models on the code generation task and find that the CodeIE model performs better than the GPT-3 model, especially in terms of recall.</sample>
    <sample id="319">The work investigates three different learning strategies: from scratch pretraining, continual pretraining using the weight and tokenizer of Camembert, and continual pretraining using the English biomedical model BERT.</sample>
    <sample id="320">The factor of overfitting due to test reuse is not observed.</sample>
    <sample id="321">The quality of the simplification was evaluated by analyzing the sentence pairs.</sample>
    <sample id="322">The speaker, Enrico, is presenting at ACL 23 and will be discussing how text classifiers learn about morality. He begins by explaining that human morality helps us distinguish right from wrong and is essential for our societies. However, current language models often treat morality as a singular scale between immoral and moral, which can hide the subjective nature of morality. Social theories like the Moral Foundation Theory suggest that humans perceive morality through different foundations, such as fairness or authority, which are prioritized differently by individuals. Enrico aims to understand how language models express morality differently across domains using explainable AI techniques. He uses the Mora Foundation Twitter Corpus, containing 35,000 tweets from seven domains, to explore these differences. For example, he finds that language models recognize the distinct rhetoric used in ALM and BLM movements regarding subversion. This highlights the importance of considering domain-specific expressions of morality when training and deploying language models.</sample>
    <sample id="323">The English content in the audio discusses a research paper titled "Dynamic Entity-Graph Retrieval with Language Models and Knowledge Graph Embedding for Commonsense QA." The paper addresses the challenge of answering questions that require common knowledge, which necessitates retrieving relevant knowledge from external sources. The authors propose a method to combine language models and knowledge bases by building a subgraph and using GNNs to infer answers. They introduce novel entities during subgraph retrieval, such as top, bank, and cat, which are largely unrelated to the question context. The method involves encoding the subgraph and text into a relation list to facilitate interaction between the two modalities and learn relationships between entities. The proposed method is evaluated on the ComQA dataset, comparing it with other methods like LM and KG, and reports good results.</sample>
    <sample id="324">Yes, language models have different political biases.</sample>
    <sample id="326">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="327">The speaker introduces their work on a vision-language learning model called Bridge Tower, which is an extension of the UniModular Encoder (UME) architecture. They explain that Bridge Tower connects multiple UME layers with each cross-modal layer in a layer-by-layer fashion to exploit uni-modal semantic knowledge at different levels. However, they mention two limitations: ineffective utilization of different uni-modal layer representations and limited scalability and capability due to the number of cross-modal layers being tied to the number of uni-modal layer representations used.

To address these limitations, the speaker proposes a new model called MagiTower, which uses managers to aggregate insights from pre-trained UME experts at different levels. MagiTower adaptsively exploits different levels of uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. The speaker demonstrates that MagiTower achieves superior performance on various downstream tasks, particularly with only four million images for pre-training. They also note that MagiTower outperforms many base-size models trained on four million data and surpasses some models trained with smaller data or parameters.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">The paper presents a method for generating structured pseudo-labels to train video localization models without manual annotation. It introduces a two-stage process: first, using an image-text pre-trained model to generate complex free-form pseudo-queries from video frames, and second, modeling the temporal structure of events to generate pseudo-events that are highly relevant to the queries and lowly relevant to other events. The method reduces label noise by weighting and refining pseudo-labels based on prediction confidence and Intersection over Union (IoU). Experiments on two datasets show superior performance compared to existing methods, demonstrating the effectiveness of the proposed zero-shot video localization method.</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative across the board.</sample>
    <sample id="331">The speaker is Sarah Papi from the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from TED Talks that had been translated into 14 different languages.</sample>
    <sample id="333">The audio content is a presentation by a speaker from Nankai University, introducing their work on injecting common knowledge into nearest neighbor machine translation. The speaker acknowledges collaborators and explains the challenges in neural networks for generating a smooth representation space, which limits generalization ability. They propose a solution called Ink, which uses a key-value data store to retrieve and refine predictions based on nearest neighbors. However, this approach has drawbacks such as time-consuming retrieval and difficulty updating representations. To address these issues, they propose a framework that injects common knowledge into the model, optimizing it with a combined learning objective. Experiments show that Ink outperforms the state-of-the-art KMT system, achieving better translation performance with less memory space and faster inference speed.</sample>
    <sample id="334">hi my name is adam sripovsky and this talk is about the dependency structure of coordination as you may know that different dependency structures assumed by different theories and and corpus approaches so for example in the universal dependencies the structure of the coordinate coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordinate structure so in this case lisa a similar approach is assumed in igor miltruk's meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are asymmetric right they they single out one of the conjuncts now there also symmetric approaches to coordinate coordinate structures such as the prague approach the conjunction headed approach assumed in prague dependency treebanks where coordinate structures are headed by the conjunction so we get dependencies from end to all the conjuncts and finally this also a multi headed approach that's used for example in the de catts on's word grammar where so to say all conjuncts are heads of the coordinate structures so we get dependencies from the governor here loves to all conduct separately these are bart and making now the main speaker is to produce an novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two okay the argument is based on the principle of dependency length minimization that are explained on the basis of these examples so in english as you might as you might know a direct objects prefer to be close to the verb while adjuncts maybe further away right so march read it yesterday is fine because the direct object it is closed to the verb while march read yesterday it is much worse right because here between the verb and the direct object there's an adjunct yesterday however this effect may be ameliorated when when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about the bees yesterday i is okay in the way instead of it we have this long np but it's also okay to say march read yesterday this absolutely fascinating book about bees so the reason here is that this is possible because even though this sentence violates the general grammatical principle that the direct objects should be next to the verb it satisfies the principle of dependency length minimization which says that shorter shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have a dependency from red to the adjunct of length seven measured in words and from red to book of length four so together it's eleven when you move when you swap these two constituents the sum of these two dependencies becomes six right so instead of eleven six much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of pent of the pentree bank and see the paper why wouldn't use university dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with lengths the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of of the left short conjuncts but what's novel in in this paper is we that we observed that this tendency only occurs when the governor on the left are absent right so the governor is on the left in this example i saw bart and lisa so it's the governor is on the left it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts however when the governor is on the right as here left governs the coordination tender net this effect disappears so we showed that by measuring length in characters that the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against as asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="335">The speaker's name is Mattias Lindemann.</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one language and then using it to predict in another language.</sample>
    <sample id="337">The English content discusses the challenges of handling out-of-vocabulary (OOV) words in natural language processing and introduces a novel approach to improve the performance of embedding-based downstream models. The approach involves using a word relationship graph that captures lexical rules of word formation and association, allowing for the effective handling of OOV words by decomposing them into word pieces and associating them with relevant words. This method is applied to both static and contextual models in downstream tasks, demonstrating its effectiveness across various languages, including non-English ones like Arabic, which uses a logographic writing system. The model's performance is comparable to English, highlighting its robustness in handling different word segmentation strategies.</sample>
    <sample id="338">The speaker introduces a collaborative research project on evaluating human explanations for machine learning models. The project involves researchers from various institutions and focuses on three main sections: motivation, related works, and contributions. The contributions are divided into unified structure, preliminary experiments, and evaluation of five datasets using two models. The speaker highlights the challenge of evaluating the quality of human-annotated explanations, which can be subjective and task-dependent. To address this, they propose a new evaluation metric called "true" that extends the Simulatability score to include the helpfulness of explanations during fine-tuning. The results show that human-annotated explanations can still benefit model predictions, even if considered low-quality by humans. The proposed metric outperforms the Simulatability score in evaluating dataset qualities across different tasks. The speaker concludes by emphasizing the importance of high-quality human annotation jobs and suggests that researchers should perform similar quality checks in future studies.</sample>
    <sample id="339">The authors of the paper are affiliated with the University of Salzburg in Germany.</sample>
    <sample id="340">The speaker, Guan Hao Huang from UCLA, introduces a new dataset called PeraMRR, which is a large-scale syntactically diverse paraphrase dataset generated by AMR back translation. This dataset aims to address the limitations of existing datasets like MRP, PA, and Cora, which have high quality but are limited in scale. The proposed method uses AMR (Abstractive Meaning Representations) graphs to generate diverse paraphrases by changing the focus node and modifying the corresponding edges. The speaker demonstrates that PeraMRR has higher syntactic diversity scores while preserving good semantic similarity compared to other datasets. Additionally, PeraMRR benefits several NLP applications such as learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">The authors use average latency and computational latency as measures of latency.</sample>
    <sample id="342">The presentation introduces a large-scale personalized dialogue dataset constructed from live streaming. The dataset is unique in that it includes video sources, which are closer to real spoken conversations compared to text sources. It is divided into two groups: those with scripts (e.g., TV and movie scripts) and those without scripts (e.g., interview datasets). The lack of large-scale Chinese multi-party dialogue datasets has been a challenge, and this dataset addresses this issue by providing detailed persona annotations and long average sessions. The dataset is used for two benchmark tasks: response modeling and address recognition. The results show that the extracted persona and long average session per persona benefit the final results, and both the rule and classifier are important for persona extraction. For address recognition, single stream but outperform double stream but auto personalize is beneficial. The performance of the proposed dialogue models on the live chat dataset is better than existing dialogue datasets, confirming that the domain of the live chat dataset is far away from the domains of existing dialogue datasets.</sample>
    <sample id="344">Trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process.</sample>
    <sample id="345">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multi-set of tokens that will appear in the output; then, it uses another model to predict a permutation to put them into the right order. The permutation model works roughly like this: we go from left to right over the output and determine which multi-set token to put in every position. For the first output position, we simply select one as highlighted in red. Then we jump to the next multi-set token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this process until every token from the first stage has been visited exactly once.</sample>
    <sample id="346">The authors of the paper are affiliated with the University of California, Berkeley.</sample>
    <sample id="348">The paper discusses the prevalence of social bias and stereotypes in large language models (LLMs) and introduces a method to measure these biases using natural language prompts. The authors, Maya and others, argue that traditional methods have limitations such as relying on hand-curated datasets and only measuring specific stereotypes. They propose using instruction-tuned LLMs to generate personas based on prompts like "imagine you are an Asian woman describe yourself." This approach allows for the generation of diverse personas that can be analyzed for stereotypes and biases. The paper highlights patterns in generated personas, such as the portrayal of women of color with certain traits and the absence of similar traits in white male personas. The authors also introduce a marked words method to identify and analyze stereotypes in generated personas. They find that generated personas contain more stereotypes than human-written ones but lack the harmful patterns captured by traditional lexicon-based methods. The paper concludes with recommendations for model owners, researchers, and the need for increased transparency in bias mitigation methods.</sample>
    <sample id="350">The presentation discusses the evaluation of artificial intelligence (AI) models against human performance in natural language processing (NLP) tasks. It highlights that leaderboard-based evaluations have become the standard, leading to the achievement of superhuman or even super-superhuman performance on certain benchmarks. However, these achievements are often based on saturated benchmarks and may not generalize well to real-world scenarios. The presentation also points out the limitations of current AI models, such as their inability to generalize, susceptibility to adversarial attacks, reliance on spurious patterns, lack of sensitivity to basic perturbations like negation, and over-sensitivity to less important perturbations.

The presentation then introduces two popular NLP benchmarks: SuperGLUE and Squad. SuperGLUE is a framework for evaluating systems on language understanding tasks, including commonsense reasoning, entailment, and reading comprehension. Squad is a reading comprehension dataset focused on question answering. The presentation shows that humans outperform AI models on these benchmarks, with humans ranking higher than the best systems in terms of accuracy.

However, the presentation also identifies several issues that affect the fairness of these comparisons. These include differences in the size of the training and test sets, errors in the ground truth annotations, and the use of simple aggregation methods to estimate human performance. The presentation argues that these issues can lead to biased evaluations and that more reliable benchmarks should be constructed to avoid repeating the same mistakes.</sample>
    <sample id="351">The paper investigates the problem of generalization using the named entity recognition task (NER) and develops a dataset called the CNN+ dataset. The authors find that three main ingredients are needed for good generalization: model architecture, model size, and fine-tuning examples. They also find that temporal drift is the main cause of performance drop in NER models. The authors conclude that for good generalization, we would need a better model architecture, larger model size, as well as more fine-tuning examples. The results show that CNN 2013 taggers still work well in 2023.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">The English content discusses a research paper titled "Python Code Generation by Asking Clarification Questions" authored by Housheng Li, Mosaad Masgar, Iftat Maitin, and Irina Gorguitch. The paper addresses the challenge of input under-specification in code generation and proposes an interactive approach to clarify specifications through clarification questions. The authors introduce a method for generating synthetic datasets with clarifications on key operations and propose a pipeline for code generation by asking clarification questions. They also analyze the performance of their method using various metrics and discuss the challenges faced during the process. The paper concludes with a call for feedback and encourages readers to check out the paper and code.</sample>
    <sample id="354">2021</sample>
    <sample id="356">The authors of the paper are affiliated with the University of Amsterdam.</sample>
    <sample id="357">The speaker's name is Cui Yuan.</sample>
    <sample id="358">Five authors are involved in the paper.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="360">Hello everyone. My name is Yi, and my colleague Jia Yang and I will be presenting our research on multi-instruct: improving multi-modal zero-shot learning via instruction tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter- and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between NLP and multi-modal. There exist more than 1,600 language-only instruction tasks. However, there is no large-scale publicly available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present multi-instruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets, and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model, as our base model. OFA uses a unified vocabulary for language, image tokens, and the coordinate of a bounding box. Here we show some example instances from our multi-instruct dataset. To unify the processing of various input and output data types, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input texts, images, instruction, and bounding boxes are represented in the same token space. Okay now, I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from NLG group for training, and we sample 10,000 instances per task. For testing, we reserve the entire commonsense reasoning group for testing, and we select additional five tasks from Wiki and the miscellaneous group. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of natural instruction as the unseen task for NLP. So we use a pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the min and max performance and the standard deviation of the performance across all five experiments. If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report ROUGE-L. For NLP task, we report ROUGE-L as well. We also introduced an additional evaluation metric called sensitivity. This measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction. Here is our main results. As we can see, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also transfer learning from natural instruction dataset can benefit instruction tuning. Here we can see as the amount of task increases, the model achieve better performance and in the meantime lower sensitivity. So we also do one experiment. We use one instruction versus five instruction. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategy on the model's sensitivity. As we can see, by transfer learning from natural instruction dataset, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from natural instruction dataset can help OFA to achieve much better performance on the natural instruct dataset. So overall, we propose the first large-scale multi-modal instruction tuning dataset, which significantly improves the zero-shot capability of OFA. And we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks. And we will release them soon. This is the QR code for our data and model. Thank you.</sample>
    <sample id="361">Hi, my name is Armine Norbarch. I'm a PhD student at the Language Technologies Institute at Carnegie Mellon University. I'm also a research director at the JP Morgan AI Research Team. The work I'm presenting today is titled CounterComp and it's focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. By multi-step quantitative reasoning we're specifically focused on the question answering task. So if you're given a financial table such as the one displayed on the right-hand side of this slide, you'd be able to ask different questions about the data in this table. For instance, you could ask what was the net change in revenue from 2019 to 2020. And the answer would be a certain number that's derived from executing one or more arithmetic operations. And this is what we mean by multi-step quantitative reasoning when the output includes multiple arithmetic operations. Unfortunately, state-of-the-art neural models don't perform very well on these tasks, especially when the output has more than two steps. And the reason is because these models are memorizing spurious patterns. So going back to the table you might notice that there are certain tokens that are repeated in the various questions in the input. For instance, the token "2019". If the model sees this token repeatedly during training, it might start to mistakenly associate it with a very common operation in the output such as the subtraction operation in this case. And we want the model to avoid that as much as possible. So ideally, we'd want a model that attends to appropriate tokens in the input when it's generating certain operations in the output. And we could certainly add supervision signal that encourages the model to do so. But of course adding additional supervision is always costly in terms of human effort and we'd like to avoid that as much as possible. So we look at this challenge in a different light. We go back to the table that I showed you in the first slide and look at the questions again. If you pay attention into the examples provided here, you'll notice that the components of the question that actually matter in terms of the operations and the operators that are used in the output are somewhat interchangeable in the sense that if you change those components, the output might also slightly change. As an example, in the first question here, if you change net change or switch net change to percent change, effectively what you do is you add a division and multiplication to the output. This basically means that these components can be used to mine counterfactual scenarios from the input. And that is exactly what we do. Given a training sample, we treat it as an anchor and we mine what we call positive and negative examples from the training set. A positive example would be an example where an intervention in the question would not yield any change in the output. And a negative example would be an example where the intervention in the question would yield a change in the output. And we use these triplets to add an auxiliary metric learning loss to the training procedure. And that auxiliary metric learning loss has a dynamic margin that is actually measuring the extent of change or intervention in the questions between each pair and using that to adjust the metric learning loss accordingly. We show that adding this auxiliary loss to three state-of-the-art baselines consistently improves their performance, especially when the number of reasoning steps grows beyond two. This is performance on in-distribution samples meaning the model is trained on a dataset and tested on the same dataset. But more importantly, we also show that adding an auxiliary metric learning loss improves performance on out-of-distribution samples either in cases where the model is trained on one dataset and tested on other datasets or the model is trained on one dataset and tested on examples from the same dataset that were never seen during training which is basically exactly what compositional generalization aims to obtain. We also show qualitatively that adding the CounterComp loss helps the model attend to more meaningful tokens during training, meaningful in the sense that they relate to more meaningful operational terms in the output. Here's the base main references I use in these in this presentation. For more information make sure to check out our poster or if you have any questions feel free to reach out to the contact listed here. I'd like to thank my co-authors, my advisor at CMU and my co-advisor at JP Morgan and I would like to thank you all.</sample>
  </task>
</testset>