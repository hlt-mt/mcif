<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind Webkraw Data und politische News-Medien.</sample>
    <sample id="1">Die Autoren gehören an McGill University.</sample>
    <sample id="2">Hallo, herzlich willkommen zu unserer Präsentation von DePlain, einem neuen Korpus für die deutsche Textsegmentierung auf Dokumentebene und auf Sätzenebene.</sample>
    <sample id="3">Mein Name ist Regina Storcken und ich werde Sie durch die erste Partie der Präsentation führen. Lassen Sie uns zuerst Textsimplifizierung kommen.</sample>
    <sample id="4">Textsimplifizierung ist ein Prozess, bei dem ein Text anpassen wird, um die Comprehension für eine bestimmte Zielgruppe zu verbessern, wie zum Beispiel Menschen mit Leseschwierigkeiten oder Non-Native-Sprachlerner.</sample>
    <sample id="5">Um ein Text-klassifizierungsmodell zu trainieren, benötigen wir Paare von Texten, beispielsweise Dokumente oder Sätze.</sample>
    <sample id="6">In this example here you can see a parallel aligned sentence pair of a complex German sentence and its translation into plain language.</sample>
    <sample id="7">Um die Satzstruktur zu vereinfachen, gibt es verschiedene Techniken, wie du im Beispiel sehen kannst. Dazu gehören Lexikalsubstitution, Klauselletion, Klauselletion mit Umordnung oder Einfügung von Begriffen.</sample>
    <sample id="8">Wir schlagen nun unsere neuen Korpora zu deplänen, da in den letzten Jahren mit den existierenden Korpora einige Probleme aufgetreten sind. Zum Beispiel sind diese Korpora heutzutage zu klein, um die Taxonomie-Modelle zu trainieren.</sample>
    <sample id="9">Die anderen drei Modelle, die ich in jüngster Zeit vorgeschlagen habe, sind alle automatisch aufgereihte, was bedeutet, dass sie über everybrawn in ihren Aufstellungen stehen.</sample>
    <sample id="10">Daher schaffen wir unser neues Corpus Deepplain, das in zwei Subkorpusen, Deepplain API und Deepplain Web, unterteilt ist. Deepplain API basiert auf News-Texts.</sample>
    <sample id="11">In der Plain API alignieren wir 483 Dokumente alle manuell. Das resultiert in etwas mehr als 30.000, 13.000 parallel-sentenzweier.</sample>
    <sample id="12">Für Deepplain Web sind diese Corpus diverse Domänen enthalten und wir alignen auch alle von diesen 750 Dokumenten auf der einen Seite manuell und auf der anderen mit automatischen Alignmetoden.</sample>
    <sample id="13">Insgesamt resulted in 30.450 sentence pairs.</sample>
    <sample id="14">Wir analysieren unsere Sätspare ein bisschen mehr. So, zum Beispiel, auf die Art von Semantik.</sample>
    <sample id="15">As you can see here, the Bible texts are much stronger simplified than for example the news text or the language learner texts.</sample>
    <sample id="16">Auf allen Ebenen, indem zum Beispiel lexikalische Semantifizierung, strukturelle Semantifizierung, also die allgemeine Semantifizierung,</sample>
    <sample id="17">Zusätzlich können Sie sehen, dass unser Deplain Corpus eine große Vielfalt an verschiedenen Simplifizierungs-Transformationen aufweist. So zum Beispiel im Deplain API Corpus haben wir viel mehr Reorganisierungen und Wortlautkorrektur als im Deplain Web Corpus.</sample>
    <sample id="18">Auf der anderen Seite in der Webcorpus haben wir viel mehr Reifrasings.</sample>
    <sample id="19">Also, lassen Sie uns sehen, was wir mit diesem Corpus machen können. Hallo, ich bin Omar und jetzt werde ich über die Einsatz可能性 our Datensatz deepl explain diskutieren. Also, für den ersten Einsatz可能性 wir können automatische Ausrichtungsmethoden bewerten.</sample>
    <sample id="20">In den letzten Jahren gab es eine große Anzahl von Algorithmen, aber im Kontext von Maschinentranslationen.</sample>
    <sample id="21">Wir haben zwei parallel geschriebene Dokumente in verschiedenen Sprachen und wir wollen Sätte im Post-Dokument allgemein abtreten.</sample>
    <sample id="22">Aber in unserem Use Case versuchen wir, Ähnlichkeiten zwischen Sätzen von zwei parallelten Dokumenten zu extrahieren, die dieselbe Sprache und denselben Inhalt haben, aber auf einem unterschiedlichen Komplexitätslevel.</sample>
    <sample id="23">Jetzt, da wir unser Datensatz DeepPlane haben, der manuell ausgerichtete Sätze enthält, können wir diese Sätze als Goldstandard-Ausrichtungen verwenden, um einige der vorgeschlagenen Ausrichtungsmethoden zu bewerten.</sample>
    <sample id="24">Und wir haben einige Anpassungen an die vorgeschlagenen Methoden vorgenommen und alle diese Anpassungen und die Codes zur Ausführung unseres Experiments im Papier publiziert.</sample>
    <sample id="25">Am Ende concluimos que el mejor enfoque de alineación automática para la simplificación de texto en alemán es el enfoque de MassAlign.</sample>
    <sample id="26">Und Sie können auch den Code finden, um diese Methode auf Ihren eigenen Dokumenten in der Papier zu verwenden.</sample>
    <sample id="27">Der zweite Use Case, den wir in unserem Papier gezeigt haben, ist der Fall der automatischen Textsimplifikation.</sample>
    <sample id="28">Durch die Optimierung von Sprachmodellen zur Produktion simplifizierter Texte aus komplexen Eingabe-Texten.</sample>
    <sample id="29">Wir haben zwei verschiedene Modelle fein-tuned. Wir haben das Modell von Longformer zu einem Modell für Dokumentebene-simplifikationen fein-tuned.</sample>
    <sample id="30">Und wir haben auch die Normalebse longe, die Normalebse impart, abgestimmt, um Satelitensimplifications zu produzieren.</sample>
    <sample id="31">You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="32">Wir haben festgestellt, dass diese, diese Basiskorregelung, die Produktionskosten oder die Produktionskosten für die Beteiligten reduzieren kann.</sample>
    <sample id="33">Wir schaffen diese Ergebnisse als einen Pfeilhochmeßstab, eine Basiskonstante für die Zukunftsaufgabe der automatischen Textsimplifizierung.</sample>
    <sample id="34">Danke soooo viels für eure Aufmerksamkeit und wir hoffen, euch alle während des Conferences zu sehen. Danke.</sample>
    <sample id="35">The speaker's name is Kai Yen.</sample>
    <sample id="36">Der TF5-XLarge-Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="37">Ja, sie funktioniieren immer noch.</sample>
    <sample id="38">Die vorgeschlagene menschliche Bewertungsmethode versucht, die Subjektivität menschlicher Bewertungen zu reduzieren, indem sie explizit feststellt, ob jede Modellresponse bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel das Bereiten von irrelevanter Information oder sich widersprechen.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von der Anzahl der validierungsstichproben ab.</sample>
    <sample id="40">Das Resultat kann verbessert werden, indem die Identitäten nicht nur durch Namen, sondern auch durch detaillierte Beschreibungen und Informationen über die Entitäten identifiziert werden.</sample>
    <sample id="41">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="42">Mein Name ist Adam Strykowski und dies ist ein Vortrag über die Abhängigkeitsstruktur der Koordination.</sample>
    <sample id="43">Sie wissen, dass verschiedene Abhängigkeitsstrukturen von verschiedenen Theorien und korporellen Ansätzen abhängen. So, zum Beispiel in den Universitätsabhängen Strukturen der Struktur der Koordinaten Koordination Lisa, Bart und Maggie.</sample>
    <sample id="44">Is so, dass der erste Konglomerte ist der Kopf der gesamten Struktur. In diesem Fall Lisa.</sample>
    <sample id="45">Ähnliche Ansätze werden in Igor Milutinović's Meaning-Text-Theorie angenommen, wo erneut die gesamte konjunktive Struktur von dem ersten Konjunkt überwacht wird. Also, diese zwei Ansätze sind isymetrisch. Sie singulieren ein von den Konjunkten.</sample>
    <sample id="46">Dazu gibt es symmetrische Ansätze zu kartesischen Strukturen, wie der Prag-Ansatz, der Konjunktionsbasierte Ansatz, Ausdruck in Prag-Abhängigkeitstreesbanken, wo kartesische Strukturen von Konjunktionen überwacht werden.</sample>
    <sample id="47">So wir bekommen Abhängigkeiten von Enden zu allen Knoten.</sample>
    <sample id="48">Und endlich ist auch ein multihead-ansatz, der z.B. in De Cattsens Wortgrammatik verwendet wird.</sample>
    <sample id="49">Where so to say all conduct is heads of the co-ordinate structures. So we get dependencies from the governor here laughs to all conduct separately. These are bartons and</sample>
    <sample id="50">Das englische Original lautet: "The aim of this paper is to produce a novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these."</sample>
    <sample id="51">Okay, die Argumentation basiert auf dem Prinzip der minimisation der Abhängigkeit, das ich anhand dieser Beispiele erläutert habe.</sample>
    <sample id="52">So in English as you might, as you might know, direct objects prefer to be close to the verb while adjuncts may be further away. Right. So March, read it yesterday is fine because the direct object, it is close to the verb.</sample>
    <sample id="53">While March read yesterday it is much worse right because here between the verb and the direct object there's an adjunct yesterday.</sample>
    <sample id="54">Allerdings kann dieser Effekt durch die Verlagerung des direkten Objekts nach dem Adjektiv, wenn es sehr schwer und lang ist, intensiviert werden.</sample>
    <sample id="55">Dies wird hier illustriert. Also, beide dieser Sätze sind korrekt. March hat ein absolut faszinierendes Buch über die BES gestern gelesen. Es ist in Ordnung. Der Grund, warum wir anstelle von "it" "dies" verwenden, ist...</sample>
    <sample id="56">Aber es ist auch okay zu sagen, Mars-Raed yesterday is absolutely fascinating book about bees.</sample>
    <sample id="57">Die Idee hier ist, dass dies möglich ist, weil obwohl diese Satz eine grammatikalische Regel verletzt, die direkte Objekte neben dem Verb stehen sollten,</sample>
    <sample id="58">Es satisfaktoriert das Prinzip der minimierung von Abhängigkeitslängen, das besagt, dass kürzere Abhängigkeiten bevorzogen werden.</sample>
    <sample id="59">So um these two um trees only show the length of the crucial dependencies, so the ones that are not constant among these two structures.</sample>
    <sample id="60">So hier haben wir die Abhängigkeit von rechts zu dem Eingang der Länge 7, gemessen in Wörtern und von rechts zu Buche der Länge 4. So together es 11.</sample>
    <sample id="61">Wenn Sie zwei Constituents wechseln, wird die Summe der zwei Abhängigkeiten sechs. Stattdessen von elf zu sechs, viel kürzer. Das klingt also ziemlich gut. Es verletzt ein Prinzip, aber es erfüllt ein anderes.</sample>
    <sample id="62">Okay, also, so was wir getan haben, wir haben sehr viele Statistiken aus dem verbesserten Version von Penc of the Penc Bank extrahiert und siehe die Papier, warum wir nicht University Pensions verwenden.</sample>
    <sample id="63">Und diese Statistiken bestätigen die Beobachtung, die oft wiederholt wurde, dass Linken Verben tendenziell kürzer sind als Rechten Verben. So messen Salz und Pfeffer und Knopf und Salz in Silben.</sample>
    <sample id="64">Und auch die Beobachtung, die im Laufe der Zeit entdeckt wurde, dass sich diese Tendenz mit wachsendem Abstand zwischen den Punkten intensiviert.</sample>
    <sample id="65">So, when the difference between the lengths of the two conjuncts grows, the shorter conjunct prefers to be the first one stronger. Right? So the proportion is bigger of the left short conjunct.</sample>
    <sample id="66">Aber was novel in diesem Papier ist, dass wir festgestellt haben, dass diese Tendenz nur dann auftritt, wenn die Gouvernante auf der linken Seite weg ist.</sample>
    <sample id="67">So der Gouverneur ist auf der linken in diesem Beispiel. Ich sehe Bart und Lisa. Also ist der Gouverneur auf der linken.</sample>
    <sample id="68">Es ist in dem zweiten Beispiel, Homer kam und schlich. Hier haben wir Koordination von zwei Verben und es gibt keinen externenGovernor. Also in solchen Fällen preferiert der linke Satzgruppenteil, zu kürzer zu sein, als der rechte. Und die größere Distanz zwischen den zwei Satzgruppenteilen.</sample>
    <sample id="69">Allerdings, wenn die Gewinnter rechts sind, als hier Lautt gewinnt die Koordination TETANET, verschwindet dieser Effekt.</sample>
    <sample id="70">Wir haben gezeigt, dass durch Messung der Längte in Zeichen das erste Spalte in Silben, die mittlere Spalte in Wörtern und rechts Spalte ist. Ich werde mich auf die rechte Spalte konzentrieren.</sample>
    <sample id="71">Was wir sehen hier ist, dass, wenn die Gewinne auf der linken,</sample>
    <sample id="72">Die Neigung der linken Conjunkt zu kürzerer Länge wächst stetig mit der absoluten Differenz in Worten und das Gleiche wird beobachtet, wenn es einen Governor gibt, wie in Koordination von Sätzen, aber wenn der Governor rechts steht, dieser Neigung entfleht.</sample>
    <sample id="73">Wir zeigen im Papier, wie dies ein Argument gegen asymmetrische Strukturen der Koordination als diese zwei und für die symmetrischen Strukturen als</sample>
    <sample id="74">So seien Sie so gut und die Papier für die vollstimmige und argumentierende Sache und reden mit uns über die Posterausschwämm. Danke.</sample>
    <sample id="75">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="76">Die Bible Texte werden stärker vereinfacht.</sample>
    <sample id="77">Salt and pepper</sample>
    <sample id="78">Ja, die voreinbaren Modelle von Natsos sind frei für die Nutzung in Forschung.</sample>
    <sample id="79">DEplain-apa enthält News-Artikel.</sample>
    <sample id="80">Eine bessere Modellarchitektur, ein größeres Modellspektrum und mehr fine-tuning-Beispiele sind die Faktoren, die zu einer guten Generalisierung beitragen.</sample>
    <sample id="81">Die Tendenz zu kürzeren linken Konjunktionen wurde durch Messung der Längen in den drei Spalten (Erste Spalte: Silben, Zweite Spalte: Wörter, Dritte Spalte: Rechts) gemessen.</sample>
    <sample id="82">Die Experimente wurden so gestaltet, dass die Längen von Stellen in Zeichen, Silben und Worten gemessen wurden. Es wurde festgestellt, dass die Tendenz, das linkse Konstante kürzer zu machen, mit der absoluten Differenz in Wörtern wächst, wenn der Begrenzer links steht. Wenn es einen linken Begrenzer gibt, wird die Tendenz beim Koordinieren von Sätzen beobachtet. Wenn der Begrenzer rechts steht, verschwindet die Tendenz.</sample>
    <sample id="83">Ein Basisklassifikator, der mit unausgewogenen Daten trainiert wird, kann nicht viel besser als Zufall performen.</sample>
    <sample id="84">Die Anzahl der Autoren, die an der Arbeit beteiligt sind, wird nicht in dem gegebenen Text erwähnt.</sample>
    <sample id="85">Bob und Alice.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle sind signifikant besser auf Diskursphänomenen wie Formalität und lexikalische Kohärenz ab, als kontextagnostics Modelle.</sample>
    <sample id="87">Die Autoren gehören an der University of California, Berkeley.</sample>
    <sample id="122">Das Framework quantifiziert die Positionalität durch die Anwendung des Pearson-Korrelations-Scores auf die Annotationen nach Demographics und die Modelldaten.</sample>
    <sample id="155">Das Studie fand, dass die menschlichen Teilnehmenden bei der Anwendung von Persona-Prompts auch Racialstereotypen surfen konnten.</sample>
    <sample id="156">Die Studie benutzt statistiken aus dem enhanced version of the Penc of the Penc Bank.</sample>
    <sample id="157">Ein Autor arbeitet an der Arbeit beteiligt.</sample>
    <sample id="158">Die eng verwandten Aufgaben für kognitive Dissonanz sind die Klassifizierung von Debatten als "expansion" und "comparison" Klassen.</sample>
    <sample id="159">Es wird nur ein Autor erwähnt, der an der Arbeit beteiligt ist.</sample>
    <sample id="160">Es wird nur ein Autor, namens Vasudeha, erwähnt.</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von bisherigen Arbeiten dadurch, dass es nicht nur die Übereinstimmung zwischen Annotatoren und Modellen überprüft, sondern auch die Verteilungen von Annotatoren berücksichtigt.</sample>
    <sample id="162">Die generierten Prozeduren.</sample>
    <sample id="163">Die verschiedenen kommerziellen Systeme wurden verglichen, um zu sehen, wie sie im Vergleich zum Benchmarking-Datum absichtvollere Übersetzungen für Dokumente machen.</sample>
    <sample id="164">Hallo, ich bin Changbing, PhD-Student an der University of Washington. Heute präsentiere ich unser Werk von der Vervorhandenen Datenerhebung über Sprachmodelle bis hin zuunteren Aufgaben, die Spuren von politischem Bias verfolgen, die zu unfairen NLP-Modellen führen.</sample>
    <sample id="165">Sprachmodelle werden auf großen Skalens webkrawlnen.</sample>
    <sample id="166">Politische Medien werden in den vortrainingen gut abgebildet. Laut einem Survei von der C4F-Gruppe können wir sehen, dass New York Times, Los Angeles Times, The Guardian, Huffington Post usw. in Sprachmodelltraining-Daten gut abgebildet sind.</sample>
    <sample id="167">Dies hat zu einem gemischten Segen für Sprachmodell-Anwendungen geführt.</sample>
    <sample id="168">Auf einer Seite hin konnten sie aus diversen Perspektiven lernen, die Demokratie und die Pluralität von Ideen feiern. Auf der anderen Seite sind diese verschiedenen politischen Meinungen von Natur aus sozial verzernt und können zu potentiellen Gerechtigkeitsproblemen in der Umsetzung von Bottom-Up-Anwendungen führen.</sample>
    <sample id="169">Um dies zu erreichen, schlagen wir vor, die politische Bias-Propaganda- Pipeline von der vortrainingen-Data über Sprachmodelle bis hin zu Downstream-Aufgaben zu untersuchen, insbesondere indem wir die folgenden Fragen stellen:</sample>
    <sample id="170">Erstens, wie können wir die politische Neutrality von Sprachmodellen bewerten und welche Rolle das präexistierende Datenmaterial dabei spielen kann?</sample>
    <sample id="171">Zweitens, wie performieren Sprachmodelle mit verschiedenen politischen Neigungen auf Downstream-Aufgaben und ob das zu Unfairness-Fragen in NLP-Anwendungen führt.</sample>
    <sample id="172">So speziell, wir schlagen zuerst vor, Sprachmodelle mit verschiedenen Prompts格式使用政治问题集，例如政治竞赛测试，来提示。 这确保了我们对自动评估的评估基于政治科学文献。</sample>
    <sample id="173">So, einige vorläufige Resultate demonstrieren, dass first language models auch variierende politische Leitlinien haben. Sie überdecken alle vier Quadranten auf dem politischen Kompass.</sample>
    <sample id="174">Wir können auch sehen, dass GPT-4 die freiheitsbewussteste Sprachmodell von allen ist. Und GPT-Serien sind allgemein mehr sozial-liberal als BERT-Serie und seine Variationen.</sample>
    <sample id="175">Zweitens versuchen wir zu untersuchen, in welchem Maß die politischen Biases von Sprachmodellen tatsächlich aus dem Trainingsdaten extrapoliert werden.</sample>
    <sample id="176">So können wir einen kontrollierten Versuch durch die weitere Pretraining von Sprachmodell-Schritten auf sechs verschiedenen Parteien korrekt, geteilt in News und Social-Media, die weiter in ihre politischen Linien unterteilt sind, durchführen.</sample>
    <sample id="177">Durch die weitere vorherige Ausbildung von Sprachmodellen auf solchen Partizipierer-Paradigmen können wir sehen, dass die ideologischen Koordinaten des Sprachmodells auch entsprechend verschieben werden.</sample>
    <sample id="178">For example, for roberta further fine-tuned and further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">In Bezug auf seine politischen Biases.</sample>
    <sample id="180">Wir versuchen auch zu untersuchen, ob Sprachmodelle in der Lage sind, die Polarisation, die in unserer modernen Gesellschaft vorherrscht, aufzunehmen.</sample>
    <sample id="181">Wir haben die vorherige Körperteilteile in vor dem 45. Präsidenten der Vereinigten Staaten und nach dem 45. Präsidenten der Vereinigten Staaten geteilt. Wir separate vorherige Sprachmodelle auf die zwei verschiedenen temporären Körperteileteile.</sample>
    <sample id="182">Wir können sehen, dass Sprachmodelle allgemein eine politische Neigung haben, die weiter weg vom Mittelpunkt ist, nach 2017. Das zeigt, dass Sprachmodelle auch die Art von Polarisation in unserer Gesellschaft aufnehmen können.</sample>
    <sample id="183">Also, last but not least, wir bewerten Sprachmodelle mit verschiedenen politischen Neigungen bei der Erkennung von Hasssprache und FalschNachrichten als zwei NLP-Anwendungen, die oft involviert sind und sehr signifikante Auswirkungen haben könnten.</sample>
    <sample id="184">Also sehen wir, dass wenn wir die pro Kategori-Performanz untersuchen, das ist zu sagen, wenn wir die Performanz in die</sample>
    <sample id="185">Different demographics or political leaning of news media, we can see a pattern that for example for hate speech detection, left-leaning language models are better.</sample>
    <sample id="186">At detecting hate speech targeting socially minority groups</sample>
    <sample id="187">Allerdings sind wir schlecht darin, heftiges Töten zu detectieren, das targeting von noch mehr mächtigen Gruppen in unserer Gesellschaft.</sample>
    <sample id="188">Und im Gegensatz dazu sind Sprachmodelle, die auf englisch trainiert wurden, besser darin, Hasssprache zu detektieren, die eine White- und Männergruppe zielt. Allerdings sind sie schlechter darin, Hasssprache zu detektieren, die eine Afroamerikaner-Plus- und andere minderheitliche Gemeinschaften zielt.</sample>
    <sample id="189">Ähnliche Trends finden sich auch in der Fängung von Fake-News, bei der wir feststellen, dass linkslinke Sprachmodelle besser darin sind, Lügen zu entdecken, die von rechtslinken und umgekehrt.</sample>
    <sample id="190">Diesen wir weiter zeigen many qualitative examples to see that language models with different political leanings,</sample>
    <sample id="191">Gib diverse Vorhersagen zu Beispielen von Hassreden und Falscher Information basierend auf ihren sozialen Kategorien. Es gibt eine Reihe weiterer Beispiele im Anhang, um das zu verdeutlichen.</sample>
    <sample id="192">Dies zeigt, dass es ein Fairnessproblem gibt, das sehr beeindruckend ist, wenn es um die politischen Biases von Sprachmodellen geht.</sample>
    <sample id="193">Zum Beispiel, wenn wir Sprachmodelle trainieren, um Unhaltbarkeit und Missinformation zu identifizieren und zu blockieren, und sie auf eine beliebte soziale Medien-Plattform deployen.</sample>
    <sample id="194">Das würde bedeuten, dass Menschen mit entgegengesetzten politischen Ansichten marginalisiert werden könnten und diskriminierende Sprache gegen minderheitliche Gruppen unkontrolliert weiterwachsen kann.</sample>
    <sample id="195">So, dies hat uns zu einer Anerkennung und Bewältigung der Fairness-Issues verhelfen, die durch Sprachmodell-politische Bedrohungen entstehen.</sample>
    <sample id="196">Ein kleines bisschen Diskussion. Wir würden auch gerne betonen, dass wir die einzigartige Dilemma bezüglich Sprachmultiplizitätsbiases aufdecken, es ist, wie zwischen Cilene und Krybids.</sample>
    <sample id="197">So, wenn wir nicht politische Meinungen in Sprachmodelltrainingdaten sauber machen, verbreiten sich die Biases vom vorherigen Datensatz über Sprachmodelle zuuntere Aufgaben aus, was letztendlich Fairnessprobleme erzeugt.</sample>
    <sample id="198">Wenn wir versuchen, etwas zu sanitieren, riskieren wir Sensibilitätsprobleme oder Exklusion. Und es ist wirklich schwierig zu bestimmen, wasActually neutral und should be retaining language-monitoring data. Also ist es wie ein elektro-electrisches Problem.</sample>
    <sample id="199">Okay, great. I think that's pretty much all I have to do for today. Thank you for your time.</sample>
    <sample id="200">Es wird angedeutet, dass mehrere Autoren an der Arbeit beteiligt sind, da es als 'joint work' beabsichtigt wird.</sample>
    <sample id="201">MPP-Auswertungen wurden bis zu 1204 Token Kontextlängen durchgeführt.</sample>
    <sample id="202">Sie haben Domains wie Pianomusik, Without Words, 12-year-old boy, fictional, und aus Oberbajen aufgenommen.</sample>
    <sample id="203">Positionalität ist die Perspektive, die Menschen als Folge ihrer Demografie, Identität und Lebenserfahrungen halten.</sample>
    <sample id="204">Der Referent*in heißt Dawe.</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">Ein Autor, Usain John, ist an der Arbeit beteiligt.</sample>
    <sample id="207">Das getestete Modell kann in der Testsuite nicht reliabel integrieren, was im Detail experimentiert wurde.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind: 1. Background Pretrain, 2. Background Both, 3. Background Inferne.</sample>
    <sample id="209">Die Autoren gehören an der University of Edinburgh.</sample>
    <sample id="210">Die abschließende Forschungsfrage lautet: Sollten wir nur die sauberen Stichproben für die Validierung verwenden, oder gibt es bessere Wege, um sie zu nutzen?</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Fähigkeit des Modells, für dieselbe Aufgabe stets die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="212">Jing Wei Yi</sample>
    <sample id="213">Eine höhere Sensitivität bedeutet eine bessere Leistung des Modells.</sample>
    <sample id="214">Die Modelle erhalten den linguistischen Kontext der englischen Sprache während des Pre-Trainings.</sample>
    <sample id="215">Typically, we only need twenty samples per class to attain high performance.</sample>
    <sample id="216">Die Autoren, Myra, Essin Darmush und Dan Juravsky, gehören an der Stanford University.</sample>
    <sample id="217">Es ist notwendig, neue Methoden zur Messung von Medienverzerrungen zu entwickeln, um die Vielfalt in den politischen Einstellungen der Sprachmodelle zu berücksichtigen.</sample>
    <sample id="218">The speaker's name is Matchat.</sample>
    <sample id="219">Die Pipeline beginnt mit prätrainingen von Daten, gefolgt von Sprachmodellen und endet mit der Ausführung von Downstream-Aufgaben.</sample>
    <sample id="220">Ja, der Vereinfachungsprozess zwischen DEplain-apa und Web unterscheidet sich. In DEplain-apa Corpus finden sich mehr Reorganisierungen und Wortänderungen, während in Web Corpus mehr Umgestellungen stattfinden.</sample>
    <sample id="221">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="222">Das Wasserzeichen wird als Gewichtsummation des Zielschwemmens und des ursprünglichen Schwemmens berechnet.</sample>
    <sample id="223">Die Autoren gehören an der Peking University.</sample>
    <sample id="224">Ja, Encoder-Decoder-Modelle wie mt5 können durch Training mit einer Mischung von Sprachen verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist die Erstellung von Texten mit spezifischen Zielen und Bedingungen, wie zum Beispiel die Erstellung von einem Schokoladentorte.</sample>
    <sample id="226">Sie validieren die Opazität der bereitgestellten Einbettung, indem sie die Einbettung von Sätzen auf der Basis von Figure 8 visualisieren.</sample>
    <sample id="227">Die Arbeit bestehende PLMs nutzt, um ein neues PLM aufzubauen, indem sie drei Modelle trainiert und konstant pretrainiert, um die Auswirkungen von Pretrainingstrategien zu analysieren.</sample>
    <sample id="228">GPT-4 ist am wenigsten auf China ausgerichtet.</sample>
    <sample id="229">Der Beispielsatz auf der rechten Seite zeigt, wie das Modell das Wissen nutzt, das durch den Aufmerksamkeitsmechanismus gelernt wurde.</sample>
    <sample id="230">Je mehr Aufgaben das Modell bearbeitet, desto besser perforiert es und desto tiefer die Sensitivität.</sample>
    <sample id="231">Die drei baumlose Baselines, mit denen die Autoren ihre Methode vergleichen, sind Cogss Benchmark, Our Model und ein anderer unbekannter Baseline.</sample>
    <sample id="232">Die beiden Co-Autoren stehen als Berater (advisers) zum ersten Autor in einer gemeinsamenarbeitlichen Beziehung.</sample>
    <sample id="233">Der englische Inhalt gibt nicht den Namen des Autors von PaLM an.</sample>
    <sample id="234">Hallo, alle. Ich bin Jenny, ein erstes Jahr PhD-Studentin an Carnegie Mellon University. Heute werde ich eure Arbeit "Analyze Positionality" präsentieren, die Design-basierte Datensets und Modelle charakterisierend.</sample>
    <sample id="235">Diese Arbeit wurde in Zusammenarbeit mit ein paar Leuten an der Université de Washington und dem AI-Institut der Allensworth, bekannt als Sebastian Santi, Ronin Labros, Katerina Rynikina und Martin Sapp.</sample>
    <sample id="236">Also, lass uns damit beginnen, uns vorzustellen, dass du arbeitest an einem Zeitungstext und versuchst, unter deinem Newsartikel diskursive Inhalte zu entfernen.</sample>
    <sample id="237">Du könntest auf eine beliebte API wie die Perspektive-API für Giftgiftentwicklung zurückgreifen und das arbeitet wirklich gut, wenn du Karl Jones bist, wo Perspektive-API in der Lage ist, korrekt giftge Fälle zu erkennen.</sample>
    <sample id="238">Aber das ist nicht der Fall bei Dithi Sharma, bei der prospective API nicht so empfindlich gegenüber Begriffen ist, die in indischen Kontexten öfters verwendet werden.</sample>
    <sample id="239">Dies ist ein Beispiel für einen Designfehler, bei dem wir systematische Leistungsunterschiede von Technologie zwischen Bevölkerungen sehen.</sample>
    <sample id="240">Design Bias, wie wir vorhin gesehen haben, könnte aufgrund der Positionalität von NLP-Forschern und Modellentwicklern auftreten. Positionalität ist einfach die Perspektive, die Menschen als Folge ihrer Demografien, Identität und Lebenserfahrungen halten.</sample>
    <sample id="241">Dies ist ein Begriff, der in kritischen Studien, insbesondere in feministischen und queer akademischen Bereichen, weit verbreitet ist.</sample>
    <sample id="242">Und als Forscher kann die Positionalität die Forschungsprozess und dessen Ausgänge und Resultate beeinflussen, da sie die Entscheidungen, die Forscher treffen, verändern kann.</sample>
    <sample id="243">Und so eine Frage, die sich die Leute manchmal stellt, ist: Haben Datensätze und Modelle Positionalität?</sample>
    <sample id="244">Wir versuchen nicht zu sagen, dass Modelle und Datensätze selbst eine demografische Identität und Lebense Erfahrungen haben, aber sie aggregieren Urteile und Meinungen von realen Menschen und können dadurch bestimmte Positionalitäten über andere repräsentieren.</sample>
    <sample id="245">So hat Priyanka einige anecdotal Beweise für die Existenz von Positionalität vorgestellt, wie kulturelle Lücken in Modellen und Datensätzen, sowie theoretische Definitionen von Modell-Positionalität.</sample>
    <sample id="246">Allerdings sehen diese Arbeiten nicht danach aus, Endbenutzern mit den Datensätzen und Modellen zu vergleichen.</sample>
    <sample id="247">Die Studie von Modell- und Datensatzpositionalität wird immer wichtiger, da NLP-Tests immer subjektiver und sozialer werden.</sample>
    <sample id="248">Und es ist schwierig zu charakterisieren, wie diese Positionalitäten sind, because not all decisions are documented and many models are hidden behind apis.</sample>
    <sample id="249">Um die Datensätze und die Positionierbarkeit zu untersuchen, vergleichen wir tatsächlich die Annotationen mit realen Benutzern mit existierenden Datensätzen und Modellen.</sample>
    <sample id="250">Wir tun dies durch ein Framework NLP Positionalität.</sample>
    <sample id="251">Unsere Framework arbeitet in zwei Hauptstadien.</sample>
    <sample id="252">Der erste Schritt ist, Datensätze mit diversen Annotatoren zu re-annotieren.</sample>
    <sample id="253">Wir optieren für diese Methode, indem wir die Demographen von Original-Datasets und Annotatoren berücksichtigen, da normalerweise nur wenige Annotatoren jede Instanz annotieren und because Demographen sind rarerweise gesammelt und geteilt.</sample>
    <sample id="254">Und so we opfer re-initalize data, to get many antennas per instance and to get a rich set of demographic data.</sample>
    <sample id="255">Wir nehmen dann die Annotationen nach demografischem Merkmal und vergleichen sie mit den Modellen und Datensätzen unter Verwendung der Pearson's R-Korrelationskoeffizient.</sample>
    <sample id="256">Und so unterscheidet sich unsere Infrastruktur von annotator Disagreement-Literatur, indem sie Endnutzer mit Modellen und Datensätzen vergleicht, die Vorhersagen und Labels machen, anstatt nur auf annotator Übereinstimmungen oder Modell-annotator Verteilungen zu schauen.</sample>
    <sample id="257">Unsere Frameworks sind hauptsächlich durch Lab in the Wild, eine Online-Krowdsourcing-Plattform, die von unserem ECHI-Kollega verwendet wurde.</sample>
    <sample id="258">In Lab in the Wild ist ein Online-Experimentierungsplattform, bei der wir diverse volontärsrecruitmenten vergleichen können. Im Vergleich zu Plattformen wie MTurk, die hauptsächlich von den USA oder Indien aus stammen, ist Lab in the Wild in der Lage, hochwertigen Datensatz zu erhalten.</sample>
    <sample id="259">Wir verwalten zwei Aufgaben im Leben der Wild. Eine davon ist soziale Akzeptabilität. Und das, was das macht, ist, dass die Teilnehmer eine Situation aus dem Sozialchemie-Datenbank und dann schätzen, wie sozialschön eine Situation ist.</sample>
    <sample id="260">Nachfolgend können Sie sich engagieren, um die Stadt zu entdecken. Sie können ihre Antworten mit jenen von einem AI und anderen vergleichen.</sample>
    <sample id="261">Wir haben dann dieAnnotations mit Sozialchemie, Delfi und QPDV verglichen.</sample>
    <sample id="262">Wir haben dann einen sehr ähnlichen Setup für die Toxizität und Hate-Speech-Detektions Aufgabe, bei der sie ein Beispiel von Datanight liest und schreibt, ob sie denken, es ein Beispiel von Hate-Speech ist.</sample>
    <sample id="263">Wir verglichen dann diese Annotationen mit DinaHate, Perspective API, Rewire API, HateRuberta und GBP4. Unsere Studie umfasste über 16.000 Annotationen von über 1.000 Annotatoren aus 87 Ländern.</sample>
    <sample id="264">So nun sind wir besser auf der Höhe, zu antworten, wer die NLP-Datasets und Modelle am besten alignen. Wir finden, dass es Positionalität in NLP gibt.</sample>
    <sample id="265">Beispielsweise finden wir, dass Datensätze und Modelle am besten auf englischsprachige Länder abgestimmt sind. Also bei der GDP4 Sozialakzeptabilitätsealanese finden wir, dass sie am besten auf Konfuzius- und englischsprachige Länder abgestimmt ist. Wir finden auch, dass Dina Haej am besten auf englischsprachige Länder abgestimmt ist.</sample>
    <sample id="266">Wir finden auch eine weitere zusätzliche Übereinstimmung mit Menschen, die eine höhere Bildung haben. Also für GPB4 in der sozial akzeptierbaren Aufgabe finden wir, dass es am besten zu Menschen mit einer college- oder einem Graduiertenschein passt.</sample>
    <sample id="267">Und wir finden das gleiche für Dunder Hite, wo es am besten an Menschen mit einem College-Ausbildung anpasst.</sample>
    <sample id="268">Allerdings werden Modelle und Datensätze, die auf bestimmte Bevölkerungen abgestimmt sind, zwangsläufig bestimmte Gruppen übersehen.</sample>
    <sample id="269">Ein Beispiel für das ist, dass Datensätze und Modelle für nicht-binäre Personen im Vergleich zu den Männer- und Frauen-Korrektenweniger sympathie machen. Wir finden dies in der GPD 4 Sozialakzeptabilität Aufgabe, sowie im Dignihate Aufgabenauswertung.</sample>
    <sample id="270">So, da es eine Position in Allied and NLP gibt, was können wir damit machen?</sample>
    <sample id="271">Wir haben einige Empfehlungen für dies. Der eine ist, einen Rekord von all relevanten Designentscheidungen während des Forschungsprozesses zu machen und der andere ist, NLP-Forschung mit dem Lenz-Präsentativismus zu machen.</sample>
    <sample id="272">Unsere dritte Empfehlung ist, spezialisierte Datensätze und Modelle für bestimmte Gemeinden zu erstellen. Ein gutes Beispiel hierfür ist die Musakani-Initiative. Ich will betonen, dass inclusive NLP nicht nur an allgemeine Technologien arbeitet, sondern auch spezifische Bedürfnisse von bestimmten Gruppen berücksichtigt.</sample>
    <sample id="273">Und so, das schliesst unsere Präsentation. Aber, falls Sie gerne mehr erfahren, fehlt es Ihnen an der aktuellen Analyse? Besuchen Sie bitte unser Dashboard für die neuesten Analyseergebnisse und unser Papier. Danke.</sample>
    <sample id="274">Die Referentin spricht von zwei Problemen der aktuellen SimulST-Modelle.</sample>
    <sample id="275">Es ist schwierig, soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen effektiv zu reduzieren, ohne dabei Sensibilität oder Exklusion zu riskieren. Es ist schwierig zu bestimmen, was als neutrales und should-retaines Datensatz angesehen werden sollte.</sample>
    <sample id="276">Hallo, ich bin Si Yuanyan von Fudan University. Ich bin hier, um unsere Arbeit zu Introduzieren, die distilling script knowledge from large language models for constrained language planning.</sample>
    <sample id="277">In allgemeineren Worten planen Menschen in ihrem Alltaghandeln oft ihre Handlungen nach einem bestimmten Schema, indem sie vorgegebene Skripts folgen.</sample>
    <sample id="278">Eine vorherige Studie hat Sprachmodelle verwendet, um Abstrakte Aufgaben von stereotypen Aktivitäten wie das Backen eines Kekses zu planen und zu zeigen, dass große Sprachmodelle effizient Aufgaben in Schritte zerlegen können.</sample>
    <sample id="279">Allerdings konzentriert sich die vorherige Arbeit hauptsächlich auf die Planung abstrakter Ziele von soziotechnischen Aktivitäten. Die Planung für spezifische Ziele mit spezifischen Bedingungen, wie zum Beispiel das Making of a chocolate cake, ist noch ununtersucht.</sample>
    <sample id="280">In diesem Papier definieren wir das Problem der konstruktiven Sprachplanung.</sample>
    <sample id="281">Eine abstrakte Zielfunktion kann von verschiedenen realen, spezifischen Zielen mit mehreren Bedingungen erfüllt werden. Ein guter Planer sollte Skripts schaffen, die rational und nachvollziehbar sind in Bezug auf die Bedingungen.</sample>
    <sample id="282">In diesem Papier evaluieren und verbessern wir die constraint language planning capability von Sprachmodellen.</sample>
    <sample id="283">Es gibt keine Art von speziellen Goldstücken, die our standard zu übertragen.</sample>
    <sample id="284">Wir müssen zuerst die Kosten erhalten, wie in der Tabelle gezeigt. Wir erweitern die abgezweigten Kosten mit multiplizierten Bedingungen für die Menschen in der LOK-Datenabfrage, indem wir den SQL-Abfragetext verwenden.</sample>
    <sample id="285">Wir sampling 100 spezifische Goos und evaluieren die scripts, die von lernenden modellen generiert werden.</sample>
    <sample id="286">Dieser Tabelle werden die allgemeine Genauigkeit der Resultate dargestellt. Wir finden, dass alle Sprachmodelle unzufriedenstellende Resultate auf planning für spezifische Ziele erhalten.</sample>
    <sample id="287">Dann machen wir eine detaillierte Analyse, um die Leitstelle-Modelle zu untersuchen.</sample>
    <sample id="288">Die Resultate in der Figur zeigen, dass die syntagmatische Komplettät in generierten Skripten akzeptabel ist, aber die Faithfulness zu den Bedingungen nicht gewährleistet werden kann.</sample>
    <sample id="289">Wir drehen in mehreren fein-gradierten topologischen Kategorien von Restriktionen, definiert in WikiHow, die Hauptmappe in der Figur zeigt, dass die planende Performance von InstructGPT-3 für Personen von verschiedenen Kategorien erheblich variiert.</sample>
    <sample id="290">Vorherige Studien haben gezeigt, dass die Ausgabekonstante von Sprachmodellen stark variieren kann, was zu schlechter Leistung führt. Daher adoptieren wir die Idee des Over-Generators-Filter, um die Generationsqualität zu verbessern.</sample>
    <sample id="291">Wir zeigen zuerst constraint types mit Beispielen für extrakt CPT und erhalten spezifische Goebbels basierend auf den abstrakten Goebbels.</sample>
    <sample id="292">Dann instrukt GPT-Overgeneralize-Kasscripts für spezifische Goos.</sample>
    <sample id="293">Nächster Schritt ist die Entwicklung eines Filtermoduls, das verwendet wird, um die visuellen Spektren zu selektieren.</sample>
    <sample id="294">Wir konvertieren Skripts und Goebbels in extraktte GPT-Embeddings und berechnen Kosinus-Similaritätsscores und Similarity-Scores, um semantische Ähnlichkeit zu messen.</sample>
    <sample id="295">In addition, we will observe the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set.</sample>
    <sample id="296">Mit unserem Ansatz können wir eine Vielzahl von hochwertigen Skripten generieren. Unsere Methodegreatly improves die Planbarkeit sowohl in semantischer Vollständigkeit als auch in Faithfulness zu den Bedingungen.</sample>
    <sample id="297">Da Sprachmodelle teurer zu deployen sind, ist es von Bedeutung, die Sprachplanerfähigkeit kleiner und spezialisierter Modells zu fördern. Die Erstellung von Datensätzen ist ein wichtiger Schritt in diesem Prozess.</sample>
    <sample id="298">Allerdings machen vorherige Studien keine planvarying für spezifische Ziele und manuelle manuelle Datensatznotation sind teuer.</sample>
    <sample id="299">Daher folgen wir der Idee von symbolischem Knowledgetilisierte, um die constraint language planning Datensätze von large language models zu distillieren.</sample>
    <sample id="300">Wir applizieren unser Verfahren für die Aufbau eines Datensatzes von konsolidierter Sprachplanung, namentlich als CodeScript.</sample>
    <sample id="301">Insgesamt generieren wir 55.000 spezifische Gold-Standard-Samples mit Skripten, um die Qualität von Validations- und Testdatensätzen zu gewährleisten. Wir bitten Cloud-basierte Arbeiter anzuzeigen und zu korrigieren, um inkorrekte Samples zu finden und zu beheben.</sample>
    <sample id="302">Diese Figur zeigt die konstruktive Distribution von Co-Scratch Wir finden Co-Schritt zeigt die Hauptrolle im generativen spezifischen Modus mit Co-Schritt wir können kleine aber spezialisierte Modelle für konstruktiven Sprachentwurf erstellen.</sample>
    <sample id="303">Wir finden, dass T5-Funktionen und Kausalität können scripts von heller Qualität generieren als die meisten großen Sprachmodelle, was zeigt, dass kleinere Modelle kleine große Modelle übertragen können, wenn sie auf geeignetem Datensatz trainiert werden.</sample>
    <sample id="304">In Kürze haben wir das Problem der eingeschränkten Sprachplanung festgestellt. Wir haben auch die Fähigkeit von Sprachmodellen, eingeschränkte Sprachplanungen zu erstellen, evaluiert und einen allgemeinen Filtermechanismus für Sprachmodelle entwickelt.</sample>
    <sample id="305">Wir verwenden große Sprachmodelle, um einen hochwertigen Datensatz in Form von Code-Skripten für Sprachplanung zu generieren. Wir hoffen, dass dieser Datensatz ein wertvoller Ressource zur Fortschrittsschaffung im Bereich Sprachplanung werden kann.</sample>
    <sample id="306">Danke für Ihr Interesse. Bitte finden Sie mehr Details in unserem Papier.</sample>
    <sample id="307">Die Sprachgewandtheit von PaLM ist vergleichbar mit anderen Systemen, aber die Hauptunterscheidung kommt von der Genauigkeit.</sample>
    <sample id="308">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind: es sollte auf Embossing- und Services-embedding anwendbar sein, das Wasserzeichen sollte die Nutzen der bereitgestellten Embossings nicht beeinträchtigen, das Wasserzeichen sollte für den Angreifer schwierig zu entfernen ist, und das Wasserzeichen muss übertragbar auf die Angreiferservices während des Modell-extraktionsprozesses sein.</sample>
    <sample id="309">Die englischen TED Talks wurden in 14 verschiedenen Sprachen übersetzt.</sample>
    <sample id="310">Typischerweise werden nur wenige Annotatoren pro Instanz verwendet.</sample>
    <sample id="311">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind Delta Cosine und Delta L2.</sample>
    <sample id="312">Modelle, die auf einem mehrsprachigen Encoder basieren, wurden in dieser Aufgabe eingesetzt, um die besten Leistungen auf allen neun Datensätzen zu erzielen.</sample>
    <sample id="344">Die Autoren nehmen an, dass der Anbieter eine allgemeine Textkorpora sammeln und die Häufigkeit der Wörter darin abzweigen kann.</sample>
    <sample id="345">Hallo, alle. Mein Name ist Zhu Huang. Heute werde ich über unser Papier sprechen: "Do CNN 2013 named entity taggers still work well in 2023?" Lassen Sie uns beginnen.</sample>
    <sample id="346">Unsere Studie untersuchte das Problem der allgemeinisierten Named Entity Recognition (NER) Aufgabe, indem sie die NER-Task anwandte.</sample>
    <sample id="347">Wir beobachteten, dass Modelle seit 2003 die Methode von Khan et al. verwenden, um ein AR (Automatisches Rechnen) zu entwickeln. Und das natürliche ist, dass es einige Probleme aufwirft. Zunächst einmal, können diese Modelle modernen Daten allgemein anpassen?</sample>
    <sample id="348">Und wenn wir neue Tags verwenden, was ist nötig für eine gute Generalisierbarkeit?</sample>
    <sample id="349">Gleichermaßen, wenn wir eine schlechte Generalisierbarkeit beobachtet haben, was verursacht die Performancedrohung an diesen Modellen?</sample>
    <sample id="350">Um diese Probleme zu untersuchen, haben wir die Cornell-Plus-Plus-Datenbank entwickelt. Dies ist eine Datenbank, die wir aus Reuters-News von 2020 gesammelt und dann mit den gleichen Cornell 2013-Annotationsehrenkarten annotiert haben.</sample>
    <sample id="351">Wir haben dann über 20 Modelle auf Conll 2003 fine-tuned. Wir haben sie dann auf beiden dem Conll 03 Testset und dem Conll+Testset evaluiert.</sample>
    <sample id="352">Schließlich haben wir die prozentuale Änderung in F1 berechnet, um die allgemeinheit jedes Modells zu bewerten.</sample>
    <sample id="353">So, was ist für eine gute allgemeinere Anwendung notwendig? In unserem Experiment haben wir festgestellt, dass es drei Hauptingredients bedarf.</sample>
    <sample id="354">Der erste ist die Modellarchitektur. In unseren Experimenten haben wir festgestellt, dass Transformer-Modelle normalerweise besser auf neue Daten generalisieren.</sample>
    <sample id="355">Der zweite Faktor ist die Modellgröße. Wir haben festgestellt, dass normalerweise größere Modelle zu besseren Generalisierungen führen.</sample>
    <sample id="356">Zuletzt und nicht zuletzt wissen wir alle, dass die Anzahl der Fine-Tuning-Beispiele direkt auf die Leistung einer Downstream-Aufgabe auswirkt. Hier haben wir auch festgestellt, dass mehr Fine-Tuning-Beispiele tatsächlich auch zu besseren Generalisierungen führen.</sample>
    <sample id="357">To our next question what causes the performance drop of some models?</sample>
    <sample id="358">Wir hatten zwei Hypothesen. Die erste istadaptive Übertöllung, die durch die Wiederholung des gleichen Testsets immer wieder auftritt und normalerweise als die Verminderung der neuen Testset-Performance manifestiert wird.</sample>
    <sample id="359">Die zweite Hypothese ist Temporaldrift, die das Performancedegradieren ist, das durch die zunehmende Temporalausgleichung zwischen den Train- und Test-Daten verursacht wird.</sample>
    <sample id="360">For adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one.</sample>
    <sample id="361">Das bedeutet, dass jede Einheit Verbesserung, die wir auf Carnot 2003 erreicht haben, mehr als eine Einheit Verbesserung auf Carnot++ führt. Das bedeutet, dass es keine abnehmenden Rückgewinnungen gibt.</sample>
    <sample id="362">Und dies zeigt uns, dass adaptiver Overfitting in diesem Fall nicht auftritt.</sample>
    <sample id="363">So was about tempo trifft.</sample>
    <sample id="364">Für temporal Drift haben wir ein Experiment durchgeführt, um einige Modelle mit jüngerem Datensatz erneut zu trainieren oder fortzusetzen, die vorher schon trainiert wurden. Wir haben festgestellt, dass die Leistung mit einem größeren Temporallücken abnimmt.</sample>
    <sample id="365">Und dies bestätigt unsere Hypothese, dass der Hauptursache für die Leistungsinkraft die Temporverdrift ist.</sample>
    <sample id="366">Unsere Schlussfolgerung lautet, dass für eine gute allgemeine Anwendung ein besseres Modellarchitektur, größere Modellgröße und mehr fine-tuning-Beispiele benötigt werden. Und diese Ziele hängen einander ab – wir können doch nur ein Zutat haben, aber die anderen müssen dabei helfen.</sample>
    <sample id="367">Gleichermaßen haben wir festgestellt, dass die Performance hier durch temporale Drift verursacht wird und überraschenderweise nicht durch adaptive Overfitting, obwohl Connell 2013 für über 20 Jahre verwendet wurde.</sample>
    <sample id="368">Also, zurück zu der Frage, die wir im Titel unseres Papiers aufgeworfen haben: GILT KONNEN 2003 TAGGER NOCH IM 2023? Und wir haben festgestellt, dass die Antwort ein überzeugender Ja ist.</sample>
    <sample id="369">Wir hoffen, dass unser Papier für mehr Forschung auf die Art und Weise inspiriert, wie man die allgemeinisierten Modelle verbessern kann.</sample>
    <sample id="370">Schließlich bitte sicherstellen, dass Sie sich unsere Publikation, unser Datensatz ansehen. Und falls Sie Fragen haben, zögern Sie nicht, mich zu kontaktieren. Vielen Dank so viel.</sample>
    <sample id="397">Die Sprachsegmentgröße wird auf 100 Wörter festgelegt.</sample>
    <sample id="398">Servin ist ein Richter.</sample>
    <sample id="399">Die Qualität des Beispiels ist der wichtigste Faktor.</sample>
    <sample id="400">Die Arbeiten in den erweiterten Experimenten konzentrieren sich auf GPT-4 und seine Variationen.</sample>
    <sample id="401">Das Modell kombiniert Werte aus mehreren Ebenen.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind die Nennung des Namens der Lied, das von mir ist, oder seine Position, die Erste.</sample>
    <sample id="403">Die Autoren gehören an Fudan University.</sample>
    <sample id="404">Ein Autor arbeitet an der Arbeit beteiligt.</sample>
    <sample id="405">Ja, die Übersetzung der natürlichsprachlichen Anfrage mit Hilfe eines maschinellen Übersetzungsmodells wurde als Baseline betrachtet.</sample>
    <sample id="406">Das Beispiel, das die Autoren für eine markierte Gruppe gegeben haben, ist "woman warrior".</sample>
    <sample id="407">Die Transformer-Modelle normalerweise nicht gut zu neuen Daten generalisieren.</sample>
    <sample id="408">Die Testdatensätze heißen Finetuning-Approaches und WSL-Approaches.</sample>
    <sample id="409">Es sind zwei Autoren an der Arbeit beteiligt.</sample>
    <sample id="410">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="439">Nach Ansicht der Autoren ist die Integration und Nutzung von eindeutigen und inferenzbaren Kenntnissen in NLU eine zu wenig erforschte Bereich.</sample>
    <sample id="440">Die Referenten heißen Ying und Jiaxuan.</sample>
    <sample id="441">Ja, Coscript hat eine Qualitätskontrolle durchlaufen.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in der Unterstützung von limitierten Typen von kontextbasierten Übersetzungen und limitierten Mengen an Sprachen, da sie normalerweise von demain-know-how und menschlicher Curation abhängig sind.</sample>
    <sample id="443">Hallo, und ich werde über unser Projekt sprechen, das die Auflösung von indirekter Beziehungen für Entity-Selektion betreibt. In diesem Projekt-introduce the alt entity scores.</sample>
    <sample id="444">Mein Name ist Javad Hosseini und dies ist ein gemeinsames Werk mit Philipp Radlinski, Silvia Parati und Annie Louise.</sample>
    <sample id="445">Unser Ziel ist es, die Sprache des Benutzers zu verstehen, wenn er eine Wahl treffen möchte. Betrachten Sie diese alternative Frage: Möchten Sie, dass es leicht für mich ist, oder habe ich ein Problem? Hier will der Benutzer zwischen zwei Optionen entscheiden.</sample>
    <sample id="446">Die offensichtlichste Methode ist die direkte Verweisung, beispielsweise durch die Nennung des Titels der Lied, das von mir ist, oder seine Position, die Nummer eins.</sample>
    <sample id="447">Aber manchmal ist es besser, einen direkten Freiwilligen zu haben, um eine natürlichere Unterhaltung zu führen. Das könnte passieren, wenn der Benutzer den Namen des Songs nicht mehr weiß.</sample>
    <sample id="448">All the pronunciations are too similar to each other and hard to disambiguate.</sample>
    <sample id="449">Oder wenn der Benutzer eine spezifizierte Vorliebe angeben möchte. Hier sind einige Beispiele für direkte Vorlieben, zum Beispiel die jüngere Variante oder ein Song, der nicht energiegeladen ist.</sample>
    <sample id="450">Dies ist ein wichtiger Problem in conversational systems und auch für Benchmarking LLMs Entity Understanding.</sample>
    <sample id="451">Wir sind nicht bewusst, dass es eine große Publikationsdatenbank für die Aufgabe gibt. Also sammeln wir eine mit Crowdsourcing. Unsere Datensammlung deckt drei verschiedene Domänen ab: Musik, Buche und Rezepte.</sample>
    <sample id="452">Our data set collection methodology emphasizes informality using a cartoon completion set.</sample>
    <sample id="453">Das Cartoon hat drei Sprachbubbles. In dem ersten Bubblen Bob sagt: "Erinnere dich an das Lied, das wir gestern gehört haben." Und mit dem Bubblen weiter: "Und mit dem Dialogkontext."</sample>
    <sample id="454">In this in the second speech bubble alice says do you mean easy on me or i got a feeling</sample>
    <sample id="455">Which is the alternative question. and in the third speech bubble bob uses an indirect reference to select one of these entities, for example the new</sample>
    <sample id="456">Wir liefern die ersten zwei Sprachbubbles automatisch, aber der dritte wird vom Notrufleitungsassistenten eingegeben. Der erste Sprachbubble wird aus einer kleinen Gruppe manueller Anregungen pro Domäne gewählt.</sample>
    <sample id="457">Der zweite von, der alternative question, wird wie folgt generiert.</sample>
    <sample id="458">Wir verwenden immer einen einfachen Vorlage. Möchten Sie A oder B? Hierbei sind A und B Beispiele von Wikipedia.</sample>
    <sample id="459">Hier sind die verschiedenen Samplingmethoden, die wir verwenden. Wenn wir weiter aufwärts in der Liste navigieren, werden die Entitäten immer ähnlicher zueinander und es wird normalerweise schwieriger zu machen, die Disambiguierung zu erreichen.</sample>
    <sample id="460">Der Erste ist uniformed train.</sample>
    <sample id="461">Der Zweite ist, wenn die Entitäten ähnliche Titel haben. Zum Beispiel zwei Bände mit dem Titel "Die ...</sample>
    <sample id="462">Der dritte Fall ist, wenn sie ähnliche Beschreibungen auf Wikipedia haben und letztendlich, wenn sie ähnliche Infoboxen oder Attribute auf Wikipedia haben, zum Beispiel die gleiche Genresorte oder denselben Künstler.</sample>
    <sample id="463">Wenn wir diese alternative Frage den Ausstattern vorlegen, kennen sie die Namen dieser Entitäten, aber sie kennen nicht unbedingt über die Entität Bescheid.</sample>
    <sample id="464">So, was wir tun, ist, dass wir ein paar Hintergrundwissen über die zwei Entitäten anbieten. For Songs, we simply show a Google-Suchlink zu jeder.</sample>
    <sample id="465">Und dann bitten wir die Noträtler, sich zumindest über ein Lied zu informieren und über jedes Lied zu informieren. Hier ist zum Beispiel der Google-Suchergebnis-Link für das Lied "Easy"</sample>
    <sample id="466">Für die Ressourcen- und Bibliotheksbereiche zeigen wir einige Hintergrundtexte von Wikipedia an. Für Rezepte zeigen wir auch die Bilder von Wikipedia an, damit die Annotatoren sehen können, wie sie aussehen.</sample>
    <sample id="467">Dann fordern wir die Annotatoren zu einem dieser Entitäten, zum Beispiel die erste, und zu beschreiben, indem sie 3-5 indirekte Beispielsätze verwenden.</sample>
    <sample id="468">For example, the one with the piano music here are some examples from our data set for example, the one without words not the one with the twelve year old twelve year old boy or the fictional one or comes from Azerbaijan and so</sample>
    <sample id="469">Die Alentities Corpus hat 6.000 alternative Fragen über drei Domänen und hat 42.000 indirekte referring Ausdrücke. Resultate mit dem T5-XL-Modell werden summarisiert.</sample>
    <sample id="470">Wenn das Sprachmodell Zugriff auf die exakte gleiche Hintergrundwissen als die Noträtler hat, dann die Genauigkeit ist wirklich hoch. Es liegt bei etwa 92 bis 95%. Aber das ist nicht realistisch.</sample>
    <sample id="471">Wenn der Sprachmodell Zugriff auf teilweise überlappendes Hintergrundwissen hat, dann liegt die Genauigkeit zwischen 82 und 87 %, was realistischer ist. Zum Beispiel, wenn das Sprachmodell Hintergrundwissen abrufen kann.</sample>
    <sample id="472">Wenn die Sprachmodell nur auf Firmennamen zugreift, dann liegt die Genauigkeit nur bei 60%, also gibt es viel Raum für Verbesserungen. Wir haben auch gezeigt, dass die Modelle über Domänen hinweg generalisierbar sind. Hier ist ein Link zu unserem Datensatz. Vielen Dank.</sample>
    <sample id="473">Der Ansatz wird mit den bestehenden SimulST-Richtlinien verglichen, insbesondere jenen, die speziell für SimulST-Präsentationen entworfen wurden.</sample>
    <sample id="474">Die Autoren gehören an Université de Lorraine.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">Drei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="477">Hallo, ich bin Sarah Papi von der Universität Trento und Fondazione Bruno Kessler. Ich werde kurz einen Fokus als Leitstelle für Simultane Sprachübersetzung vorstellen. Das ist ein gemeinsames Werk mit Mateo Negri und Marco Turki.</sample>
    <sample id="478">Was ist gleichzeitiges Sprachübersetzen? Gleich zeitiges Sprachübersetzen, auch als Simultan Übersetzung (Sübt) bezeichnet, ist die Übersetzung von gesprochenem Sprache in einen Text in einer anderen Sprache in Echtzeit, indem es eine direkte Sprachkommunikation ermöglicht.</sample>
    <sample id="479">Und was sind die Probleme der aktuellen Simulstelle-Modelle? Spezifische Architekturen werden üblicherweise trainiert, indem zusätzliche Module optimiert werden.</sample>
    <sample id="480">Lang und komplizierte Trainingprozeduren, z.B. Training, das verschiedene Optimierungsziele umfasst,</sample>
    <sample id="481">Und trainieren und erhalten mehrere Modelle, um verschiedene Latenzregimes zu erreichen, zum Beispiel das Training eines Modells mit einem Durchschnitt von 1 Sekunde Latenz und ein anderes Modell mit zwei Sekunden Latenz usw.</sample>
    <sample id="482">So, was ist unsere Lösung?</sample>
    <sample id="483">Erstens verwenden Sie bestehende off-the-shelf-Modelle ohne erneute Training oder spezifische Architektur für SimILST. Verwenden Sie nur ein Modell pro Latenzregimes und handeln Latenz durch spezifische Parameter.</sample>
    <sample id="484">Und nutzen die Kenntnisse, die durch die Ablenkungsmethode zwischen Audio-Eingabe und Textausgabe erworben wurden, das ist der Ablenkungsmechanismus. Und Sie können ein Beispiel auf der rechten Seite sehen.</sample>
    <sample id="485">Unsere Lösung ist es, die Achtung zu kodieren oder zu kodieren und Decodieren und es ist eine Strategie, bei der wir entscheiden, ob wir einen teilweise Übersetzungsbetrag akzeptieren oder nicht, basierend auf der Achtung, die auf uns zeigt.</sample>
    <sample id="486">Eine Welle wird emitteidet, wenn die Tension nicht konzentriert ist, d. h., wenn das Summ ist unter einem bestimmten Schwelle α hin zu den last lamba speech frames, was bedeutet, dass die empfangene Information nicht stabil genug ist.</sample>
    <sample id="487">Beispielsweise, wenn wir erhalten einen Sprachschung, der über 'ich werde sprechen' handelt und unser Modell die Übersetzung in Deutsch vorhersagt,</sample>
    <sample id="488">Und wir werden uns ansehen, die kruzationswerte</sample>
    <sample id="489">Wir sehen, dass die ersten zwei Werte Punkte zu den earliest received speech frames während der last word Punkte zu den last received speech frames als Lambda speech frames.</sample>
    <sample id="490">Das bedeutet, dass die ersten zwei Worte weggelassen werden.</sample>
    <sample id="491">Während das Summa der kroatischen Tension über einem bestimmten traditionellen Alpha liegt, werden wir nicht emitte und warten auf einen neuen Spitzengewinn.</sample>
    <sample id="492">If we go on and we receive another speech chunk and our model predicts other three words, and we will look at the cross attention weights.</sample>
    <sample id="493">Wir werden sehen, dass nur Worte Punkte zu den last lambert speech frames.</sample>
    <sample id="494">Das bedeutet, dass diese drei Worte emittiert werden.</sample>
    <sample id="495">If you look at the main results of that,</sample>
    <sample id="496">Wir plotten die simultane Sprachübersetzungsergebnisse auf Grafiken, in denen wir auf einer Seite blau haben, das die Übersetzungslinie misst und auf der anderen Seite durchschnittliche Leitfähigkeit.</sample>
    <sample id="497">Das ist die Latenzmessung und wir betrachten auch die computational-aware Average Leitstelle, die für die Modellcomputational-Zeiten zur Berechnung der Ausgabe sorgt.</sample>
    <sample id="498">Also wollen wir, dass unsere Kurse so hoch wie möglich auf diesem Plot sind.</sample>
    <sample id="499">Auch wir wollen, dass sie auf der linken Seite sind.</sample>
    <sample id="500">Wir vergleichen mit anderen Strategien, die auch auf offline-Modellen angewandt werden, wie der Whitkey-Strategie und dem Lokalvertrag. Wir vergleichen auch mit einem State-of-the-Art-Systemarchitektur, speziell auf Simultaneous Pre-Translation (SPT) ausgerichtet.</sample>
    <sample id="501">Dies sind alle die Resultate des Simultaneous Speech Translation-Strategie auf Deutsch.</sample>
    <sample id="502">Und wir sehen, dass AdaBoost überwiegt alle Strategien, die auf off-line Modelle angewendet werden, da ihre Kurven über links verschiebt werden.</sample>
    <sample id="503">Und wir sehen auch, dass, wenn wir die tatsächliche Laufzeit oder die computational-aware Time considerieren, der Attakts ist die schnellste Strategie.</sample>
    <sample id="504">Wenn Sie mehr Ergebnisse entdecken möchten, lesen Sie unser Papier und wir haben auch Open Source die Code- und Modelle- und Simultaneous-Hauptautoren freigegeben, um die Wiederverwendbarkeit unseres Werks zu erleichtern. Vielen Dank für Ihre Aufmerksamkeit.</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="506">Hallo, alle. Mein Name ist Ying und mein Kollege Zhiyan und ich werden unsere Forschung über Multi-Instruct präsentieren: Improving multi-modal zero-shot learning via instruction tuning.</sample>
    <sample id="507">Mit Fortschritten in großen Sprachmodellen haben viele Arbeiten begonnen, neue Lernparadigmen zu untersuchen, indem sie prätrainte Sprachmodelle für verschiedene untere Aufgaben in einem parameter- und dateneffizienten Weise verwenden.</sample>
    <sample id="508">Kürzlich haben viele Studien gezeigt, dass die Einrichtung einstimmig große Sprachmodelle dazu bringt, auf Aufgaben in einem schnellen und effizienten Weise zu arbeiten, indem sie natürliche Anweisungen folgen.</sample>
    <sample id="509">Allerdings konzentrieren sich die meisten vorherigen Arbeiten auf die Verbesserung der Serienleit perforanz bei Sprach-only Aufgaben, während Computervision und multimodale Aufgaben weggelassen wurden.</sample>
    <sample id="510">Daher in diesem Werk wollen wir untersuchen, ob die einstimmige Tuning oder multi-modell-basierte Modelle tatsächlich die Generalisierbarkeit zu einem Multi-Modell-Task verbessern.</sample>
    <sample id="511">Darüber hinaus, zu Zeiten unseres Forschungsprojekts, haben wir eine erhebliche Discrepanz in der Verfügbarkeit von Datensätzen für die Sprachverarbeitung und Multimodalität entdeckt.</sample>
    <sample id="512">Es existieren mehr als 1.600 Sprachspezifische Einzelkursmaterialien, aber es gibt keinen großen öffentlich zugänglichen multimodalen Kursmaterialien. Daher motiviert uns, einen multimodalen Kursmaterial-Tuning-Datensatz zu erstellen.</sample>
    <sample id="513">Hier präsentieren wir MultiInstruct, das erstes multimodales Instruktions-Tuning-Benchmarks-Datensatz, der bestehend aus 62 verschiedenen multimodalen Aufgaben über 10 Hauptkategorien ist.</sample>
    <sample id="514">Diese Aufgaben werden aus 21 bestehenden offenen Quelldatasets abgeleitet und jede Aufgabe ist mit 5 expliziten Anweisungen ausgestattet.</sample>
    <sample id="515">Wir verwenden OFA, eine vereinheitlichte multimässige Sprachmodell-Unterstützungsmodell, als unsere Basismodell. OFA verwendet einen vereinheitlichten Vokabular für Sprache, Bildtoken und die Koordinaten von Bounding-Boxes.</sample>
    <sample id="516">Hier zeigen wir einige Beispielinstanzen aus unserem MultiInstra-Dataset.</sample>
    <sample id="517">To unify the processing of various input and output data types.</sample>
    <sample id="518">Wir folgen dem Ansatz von OFA und formulieren alle Aufgaben in einem vereinbarten sequenz zu sequenz Format, in dem die Eingabe-Texte, Bilder, Anweisungen undBounding Boxes im gleichen Tokenraum dargestellt werden.</sample>
    <sample id="519">Okay, now I'm going to talk about multi-modal instruction tuning.</sample>
    <sample id="520">Für die Trainingsdatenbank verwenden wir 53 Aufgaben aus dem NLP-Gruppe zur Training und wir sampling 10.000 Instanzen pro Aufgabe. Für die Testphase reservieren wir die gesamte Commonsense-Reasoning-Gruppe zur Testphase und wir selektieren weitere 5 Aufgaben aus dem Wiki- und der MSc-Gruppe.</sample>
    <sample id="521">Wir verwenden alle Instanzen in der Test-Splitt für jede Aufgabe. In addition, wir randomisieren 20 Aufgaben aus dem Test-Splitt von Natural Instructions als "synth" Aufgaben für NLPA.</sample>
    <sample id="522">Wir verwenden einen prätrainierten OFA-Large-Modell als Basismodell. During Training wir machen alle Instanzen für alle Aufgaben. Jede Instanz wird zufällig mit einem von sechs Einrichtungstemplaten kombiniert.</sample>
    <sample id="523">So, during tests for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="524">Wir berichten die minimum und maximale Performance und die Standarddeviation der Performance über alle five Experiments.</sample>
    <sample id="525">Wenn die Aufgabe ein multimodales Klassifizierungsproblem ist, bericht准确性. Wenn es ein multimodales Generationsproblem ist, bericht rouge l. für eine rp Aufgabe berichtet rouge l. ebenso.</sample>
    <sample id="526">Wir haben auch einen zusätzlichen Evaluationsmaßstab namens Sensitivität eingroduced. Dieses Maßstab misst die Fähigkeit des Modells, für dieselbe Aufgabe stets die gleichen Ausgaben zu produzieren, unabhängig von kleinen Variationen in der Formulierung der Anweisung.</sample>
    <sample id="527">Hier sind unsere Hauptergebnisse. Wie wir sehen können, kann die Optimierung der Eingabe-Output-Unterstützung (Instruction Tuning) das Leistungspotenzial von OLS-OLS auf bestimmte Multimodal Aufgaben signifikant verbessern.</sample>
    <sample id="528">Auch Transferlearning von natürliche Instruktionsdatensatzes kann Vorteile für Instruktions tuning.</sample>
    <sample id="529">Hier können wir sehen, dass die Leistung des Modells bei einer steigenden Anzahl an Aufgaben verbessert wird und in der meantime eine geringere Sensibilität aufweist.</sample>
    <sample id="530">Wir haben auch die Y-Experimente durchgezogen. Wir haben eine Anweisung gegen five Anweisungen verwendet. Wie wir sehen können, kann die Verwendung von mehreren Anweisungen die allgemeine Leistung des Modells verbessern und seine Sensibilität reduzieren.</sample>
    <sample id="531">So zeigt dies die Auswirkungen verschiedener Finetuning-Strategien auf die Sensitivität des Modells. Wie wir sehen können, durch Transfer Learning von Natural Instruction Datensätzen, kann das Modell viel bessere Sensitivität erreichen im Vergleich zum ursprünglichen OFA-Modell.</sample>
    <sample id="532">We can also see transfer learning from the neural instruction dataset can help our model to achieve much better performance on the neural instruct dataset.</sample>
    <sample id="533">Wir schaffen einen großen Datensatz für die Anpassung von Modellen in verschiedenen Sprachen. Wir wollen sehen, wie gut unser Modell auf neue Sprachen anpassen kann. Wir versuchen verschiedene Techniken, um das Modell zu verbessern und herauszufinden, welche am besten sind. Wir wollen auch herausfinden, wie empfänglich das Modell auf kleine Veränderungen ist.</sample>
    <sample id="534">Ein weiteres Update: Wir sammeln eine viel größere multimodale Übersetzung der Datensätze mit etwa 150 zusätzlichen Wörterlautsproblemen und wir werden sie freigemacht. Also, hier ist der QR-Code zu unserem Datensatz und Modell. Danke.</sample>
    <sample id="535">Die Autoren, Matteo Negri und Marco Turki, gehören an der University of Trento.</sample>
    <sample id="536">Javat Hosaini</sample>
    <sample id="562">Hallo, alle. Ich bin Costas Sina und ich freue mich, Sie zu unserem Talk über unser ACL 2023 Papier "Language Model Acceptability Judgements are not always robust to context" willkommen zu begrüßen.</sample>
    <sample id="563">Es ist einJoint-work mit John Gower, Aaron Mular, Galichka Mishra, Karen Fuentes, Roger Levy und Athena Villam</sample>
    <sample id="564">So in diesem Werk revisitieren wir die Minimalpaarprädiktive Hypothese.</sample>
    <sample id="565">Die Minimalpaar-Prüfung bewertet Sprachmaterial auf Grund von Entscheidbarkeitsentscheidungen, die auch Grammatikalität wie "plump" und "syntagmatisch" oder Entscheidbarkeit in Bezug auf Stereotypien wie "Krausspare" einschließen können.</sample>
    <sample id="566">In diesem Minimalpaar paradigm ist die typische Methode zur Evaluation von Sprachmodellen, dass man einen akzeptablen Satz oder eine grammatikalisch korrekte Aussage zeigt und dann einen nicht akzeptablen Satz oder einen ungrammatikalischen Satz zeigt.</sample>
    <sample id="567">Und dann hofft das Modell, dass es basically puts more probability to the acceptable set.</sample>
    <sample id="568">Der aktuelle NLP-Pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences.</sample>
    <sample id="569">Diese Täg大型语言模型正在出现,带有更长和更长的上下文窗口。因此,评估模型在整个上下文窗口中的可接受性是至关重要的。</sample>
    <sample id="570">Und das ist, was wir versuchen zu tun. Hier versuchen wir, den NPB-P pipeline zu revisitieren, indem wir den Modell zu evaluieren versuchen, auf langer und langer Sequenzen.</sample>
    <sample id="571">So, das ist der Ansatz. Also, was wir tun, ist, dass wir diese längeren Sequenzen revisitieren und die Datensätze selbst überprüfen. Und dann recreieren wir Sentences, indem wir akzeptable oder unakzeptable Sentences aus diesen Datensätzen auswählen.</sample>
    <sample id="572">For example, here we have chosen like a typical pair of grammaticality from the bllim data set from the adjunct island case.</sample>
    <sample id="573">Und was wir tun, ist, die zu recreieren, l. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</sample>
    <sample id="574">Und dann fügen wir es als Prefix zu beider akzeptierbarer Abfragereiquestelle und der unakzeptierbaren Abfragereiquestelle hinzu.</sample>
    <sample id="575">So können wir dasselbe machen, indem wir unakzeptable Sätze aus dem gleichen Matching auswählen und das könnte auch verwendet werden, um die Akzeptierbarkeit des Modells zu testen.</sample>
    <sample id="576">Und wir können auch das gleiche machen, indem wir Sätze aus einem anderen Datensatz oder einer anderen Substelle auswählen. Das nennen wir als Missmatch-Szenario.</sample>
    <sample id="577">So hier die Sätze sind, die immer noch aus relevanten Datensätzen stammen, aber sie sind nicht aus demselben Datensatz, den Sie verwenden. Und wir können das gleiche für Unannehmbarkeitstheorien tun.</sample>
    <sample id="578">Endlich können wir Sätze aus einem vollkommen unverwandten Feld, wie Wikipedia,</sample>
    <sample id="579">So, das wird uns verraten, ob die Modell akzeptierbarkeitsbewertungen tatsächlich beeinflusst werden, ob es einen Kontext gibt.</sample>
    <sample id="580">Ob der Kontext kommt von einem anderen Teil des Datensatzes oder ob es für die aktuelle Aussage irrelevant ist.</sample>
    <sample id="581">So, wie das Modell tut? Zunächst schauen wir uns die Wikipedia-Sätze an, die für die aktuelle Suchanfrage completely irrelevant sind. Und dort finden wir, dass die MPP-Jugendmeßungen hauptsächlich robust für arbiträre Kontexte sind.</sample>
    <sample id="582">Wir haben die Kontextlänge auf bis zu 1204 for maximieren, um OP T und GPT-2-Modelle zu optimieren. Und wir haben hier in der orange Dots line die MPP-Jurgen relativ stabil.</sample>
    <sample id="583">Now what happens when we choose sentences from the same dataset?</sample>
    <sample id="584">So hier, wir schaffen, wir creating sentences from acceptable and unacceptable domains from the same blimp or syntaxem dataset.</sample>
    <sample id="585">And there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable</sample>
    <sample id="586">Aber wenn wir die Struktur匹配, das ist, wenn wir die Sätze aus dem gleichen Phänomena in Blimp Person Text Jim</sample>
    <sample id="587">Wir sehen einen massiven Anstieg oder einen massiven Abfall in der MPP-Judgment für das Modell, je nach ob der gewählte Prefix akzeptabel oder unakzeptabel ist.</sample>
    <sample id="588">Jetzt, das und das ist sehr groß. Wie dieser Effekt über die Kontextlänge hinaus zunimmt und das würde wahrscheinlich die jüngeren Sprachmodelle beeinflussen, die große Kontextfenster haben.</sample>
    <sample id="589">Warum hat der Match-Prefix so viel Einfluss auf die Sprachmodellbewertung?</sample>
    <sample id="590">Wir haben eine Reihe von Analysen durchgeführt, bei denen wir versucht haben, die Eingabeaussage zu perturbieren, indem wir versucht haben, die relevanten Struktur zu erhalten, aber dennoch Geräusche hinzuzufügen. Nachdem wir mehrere solcher Perturbationen durchgeführt hatten,</sample>
    <sample id="591">Wir finden, dass keiner dieser Rauschstörungen tatsächlich den Modell verändert, also in Begriffen, wie es uns die PEPPI-Jugendtraining.</sample>
    <sample id="592">Basierend auf den Modellen, die wir trainiert haben, können wir feststellen, dass sie für die PERT-Entscheidungen in ähnlicher Weise empfindlich sind.</sample>
    <sample id="593">Das ist, wenn wir die Sätze im akzeptablen Bereich perturbieren, sehen wir einen ähnlichen Anstieg in allen Perturbationen und wenn wir die Sätze im nichtakzeptablen Bereich perturbieren, sehen wir einen Decrease in MPP-jägern in ähnlicher Weise.</sample>
    <sample id="594">Die Haupttakeaways unseres Arbeitstyps sind, dass Sprachmodelle Empfindlichkeiten gegenüber latenten syntagmatischen und semantischen Merkmalen aufweisen, die sich über die Sätze verteilen.</sample>
    <sample id="595">Und die MPP-Evaluation, die derzeit mit kurzen und einfachen Sätzen durchgeführt wird, kann das abstrakte Wissen Sprachmodelle über den Kontextwindow nicht vollständig capturieren.</sample>
    <sample id="596">Bitte lesen Sie unser Papier für weitere Details über unsere Experimente. Danke fürs Zuhören.</sample>
    <sample id="597">Unordered Multiset of Tokens</sample>
    <sample id="598">Fünfzigtausend Skripte sind in Coscript vertreten.</sample>
    <sample id="626">Die beste Ausrichtungsmethode für DEplain ist die Methode von Massalign.</sample>
    <sample id="627">Schwach überwachtes Lernen trainiert neuronalen Netzen robust auf Lärm in den Daten, sodass die trainierten Modelle immer noch gut generalisieren können.</sample>
    <sample id="628">Die Dokumente in DEplain-web wurden mit manuellen und automatischen Alignmentmethoden ausgerichtet.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde erstellt, indem eine Datensammlung aus Reuters-News von 2020 erfasst wurde und dann mit den gleichen Annotierungsrichtlinien von 2003 annotiert wurde.</sample>
    <sample id="630">Hallo, alle. Mein Name ist Yusen Jiang vom Penn State University. Heute werde ich Ihnen unsere Arbeit präsentieren: Exemplar- Crosslinguistische SemantikParsing in mehreren natürlichen Sprachen und minimalen Darstellungen.</sample>
    <sample id="631">So, semantische Parsening ist eine Aufgabe, um semantische Darstellungen von Benutzerabfragen zu erstellen, wie SQL und Lambda Calculus.</sample>
    <sample id="632">Und Crosslinguistic Semantics ist die Aufgabe, Abfragen in mehreren natürlichen Sprachen in mehreren minimalen Darstellungen zu übersetzen.</sample>
    <sample id="633">Wir müssen den Query in mehreren natürlichen Sprachen mit neuronalen Modellen übersetzen, z.B. CQL, Lambda, SQL und so weiter.</sample>
    <sample id="634">Bestehende kroatisch-schwedische Semantik-Parsing-Modelle werden separat vorgeschlagen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet, z.B.</sample>
    <sample id="635">There are leaks of coverage on certain natural language the Chinese is missing and</sample>
    <sample id="636">Lack of coverage on certain minor repetitions.</sample>
    <sample id="637">Der Lymphkalkülus ist weg.</sample>
    <sample id="638">Oder sie werden nur auf bestimmte numerische Modelle evaluiert. Zum Beispiel gibt es nur einen bestimmten Modell zu evaluieren.</sample>
    <sample id="639">Um zu diesem Ende schaffen wir exemplar einen uniformen Datensatz exemplar für Crosslinguistische Semantik in mehreren natürlichen Sprachen und vielen Repräsentationen.</sample>
    <sample id="640">Es enthält 90 Datensätze in verschiedenen Domänen, 5 SemantikParsing Tüx, 8 millionen Repräsentationen und 22 natürliche Sprachen in 15 Sprachfamilien.</sample>
    <sample id="641">Um unser Benchmark besser zu evaluieren, betrachten wir die sechs Einstellungen für Training und Evaluation.</sample>
    <sample id="642">Der Erste ist translate test wir verwenden google translate api, um den Quelltext in die Zielsprache zu übersetzen und dann einen Monolingual-Modell zu trainieren und zu evaluieren.</sample>
    <sample id="643">Und zum Beispiel trainieren wir den englischen Modell auf englische Query und during Inferenz wir übersetzen die deutsche Query mit API in English und dann verwenden den trainierten Modell zur Predict der SQL.</sample>
    <sample id="644">Wir werden auch einen monolingual-Modell testen.</sample>
    <sample id="645">In diesem Setting ist die Quellensprache das gleiche wie die Zielsprache, z.B. Deutsch zu Deutsch oder Englisch zu Englisch.</sample>
    <sample id="646">Wir testen auch monolingue few-shot-Setting, indem wir monolingue Modelle mit nur 10% der Trainingsdaten trainieren.</sample>
    <sample id="647">Und wir testen einen monolingual-multilinguale Modell, das wir für alle Sprachen trainieren.</sample>
    <sample id="648">Zum Beispiel wirft man deutsche, englische und chinesische Suchanfragen zusammen, um ein multilinguals Modell zu trainieren. Und während der Inferenz können wir dieses Modell verwenden, um</sample>
    <sample id="649">Um zu übersetzen, German queries or Chinese query or et cetera.</sample>
    <sample id="650">Und wir betrachten auch Crosslinguistic Zero-Shot- und Few-Shot-Transfer. Wir trainieren auf einem Sprache und übertragen zu einer anderen Sprache.</sample>
    <sample id="651">Während der Trainingphase trainieren wir auf englische Abfragen oder eine Kombination von englischen und deutschen Few-Shot-Abfragen, um ein multilingualen Modell zu trainieren und die SQL-Ausgabe vorherzusagen.</sample>
    <sample id="652">Und wir finden auch einige interessante Resultate. So, wenn wir Analyse von monolingual Modellen machen, bewerten wir zwei Gruppen von Modellen.</sample>
    <sample id="653">Inklusive Encoder PDR, das steht für multilinguisten prétrainiert Encoder mit Pointer-basierten Decoder wie XLNet plus PDR und Bert plus PDR.</sample>
    <sample id="654">Und wir evaluieren Encoder-Decoder-Modelle, die multilinguistisch trainiert sind, wie zum Beispiel Mbart und M5.</sample>
    <sample id="655">Wir haben festgestellt, dass Encoder-Decoder die beste Leistung auf allen neun Datensätzen erreicht.</sample>
    <sample id="656">Und wir evaluieren auf MT5 und einem XLMR plus PDR auf multilingualsetting.</sample>
    <sample id="657">Wir fanden, dass Encoder-Decoder- oder Encoder-PDR-Modelle durch Training in einer Mischung von verschiedenen Sprachen verbessert werden können.</sample>
    <sample id="658">Wir haben festgestellt, dass es because die meisten der Hauptnatürlichen Sprachen einenPerformancengewinn erreichen können, aber das englische Performance in sieben Datensätzen abnimmt und in drei Datensätzen nur gewinnt.</sample>
    <sample id="659">Ich denke, das ist bekannt als Kurse der Multilingualität.</sample>
    <sample id="660">Wir haben auch den Crosslingual-Performancediskriminanzgap verglichen.</sample>
    <sample id="661">In dieser Figur ist die blaue Linie die transversale Fehlstromtransfer, die orange Linie ist die transversale Nullstromtransfer, während die grüne Linie die longitudinalen Stellen ist.</sample>
    <sample id="662">Wir fanden, dass durch die Vergleichung der Grün- und Orange-Linie, bei einer Null-Schritt-Einstellung, die Transfer-Performance-Abweichungsstelle signifikant ist. Und durch die Vergleichung der Blauen- und Orange-Linie, bei einer Few-Schritt-Einstellung, die Transfer-Abweichungsstelle schnell abnimmt.</sample>
    <sample id="663">Wir finden auch einige andere interessante Erkenntnisse. Zum Beispiel überperformt Encoder-Decoder auf vergleichbaren Aufgaben wie der Übersetzung von englischer ins natürlichen Sprache und significantly boost the performance of few-shot on target natürlichen Sprachen.</sample>
    <sample id="664">Wir haben festgestellt, dass multilinguale Sprachmodelle wie Codas und Blue noch unzulänglich sind für Übersetzungen zwischen Sprachen.</sample>
    <sample id="665">Zusammenfassung: Wir bauen Exemplar, ein vereinbaresBenchmark für die Übersetzung von Texten mit mehreren Sprachen und vielen Repräsentationen.</sample>
    <sample id="666">Wir haben eine umfassende Benchmark-Studie an drei verschiedenen Typen von multilingualen Sprachmodellen durchgeführt. Unsere Untersuchung hat viele interessante Erkenntnisse erbracht und so weiter. Wir freuen uns, Sie zu unserem Papier und Code zu begrüßen. Vielen Dank für Ihr Interesse.</sample>
    <sample id="667">Die Arbeiten, die bereits durchgeführt wurden, können allgemein in vier Kategorien unterteilt werden.</sample>
    <sample id="668">Nein, sie sind noch nicht ausreichend für CLSP.</sample>
    <sample id="695">Die Methode mit der Mehrdeutigkeit der Permutationen wird durch die Einbeziehung der Ausrichtung als Teil des Trainings bewältigt.</sample>
    <sample id="696">Die Fairness eines nachgeschalteten NLP-Modells wird definiert, indem es überprüft wird, ob das Modell Menschen mit unterschiedlichen politischen Ansichten fair behandlesst und ob es diskriminierende oder diskriminierende Ausdrücke enthält.</sample>
    <sample id="697">The speaker's name is Yanis Lavrac.</sample>
    <sample id="698">Kostas Zissin</sample>
    <sample id="699">Die Referentin heißt Myra.</sample>
    <sample id="700">Tropikalismus bezieht sich auf die Verwendung von Begriffen wie 'vibrant' und 'curvaceous' bei Beschreibungen von Latina-Frauen, was eine stereotype Art von Tropikalismus ist.</sample>
    <sample id="701">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie die Top-Worte analysieren und danach die Gruppen definieren.</sample>
    <sample id="702">In dieser Arbeit wurde SXMI zur Messung der Kontextnutzung verwendet.</sample>
    <sample id="703">DrBERT und ChuBERT sind beide Sprachmodelle, die auf natürliche Sprache basieren. DrBERT ist ein Modell mit 7 GB von NAOs, während ChuBERT ein klinisches Modell mit 4 GB von Sätzen aus klinischen Knoten ist.</sample>
    <sample id="751">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="752">Iteratives Transferlernen ist ein Ansatz, bei dem das Modell durch die Training auf den neuesten Datensatz iterativ aktualisiert wird.</sample>
    <sample id="753">Das Ziel des Datensatzes ist es, die Sprache der Benutzer zu verstehen, wenn sie eine Wahl treffen möchten.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Verarbeitung von Eingabe-Text durch das Modell simuliert und die Anzahl der Auslöser in jeder Eingabe-Sentence bestimmt.</sample>
    <sample id="755">Drei Autoren sind an der Arbeit beteiligt: Matteo Negri, Marco Turki und Sarah Papi.</sample>
    <sample id="756">Zwei Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">Die Autoren gehören an Carnegie Mellon University und der University of Washington.</sample>
    <sample id="758">Das Beispiel mit dem Begrenzer auf der linken Seite lautet "I saw Bart and Lisa."</sample>
    <sample id="759">ABC-Eval ist in der Lage, die Geschwindigkeiten zu messen, bei denen Sprachmodelle verschiedene thematische Fehlertypen machen.</sample>
    <sample id="760">Wir müssen die Akzeptanz der Modelle über das gesamte Kontextfenster bewerten, weil große Sprachmodelle heutzutage mit immer größeren Kontextfenstern arbeiten und es daher wichtig ist, zu verstehen, wie gut sie im gesamten Kontext performieren.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="762">Ja, die Annotatoren kennen die Entität im Voraus.</sample>
    <sample id="763">MT-Metriken, die die Anzahl der Begriffe und Phrasen berücksichtigen, die von den Menschen als relevant erachtet wurden.</sample>
    <sample id="764">Ja, die Regression beeinflusst die Generalisierung auf bestimmte NER-Typen.</sample>
    <sample id="765">Positionalität ist für NLP wichtig, weil es die systematischen Leistungsunterschiede von Technologien zwischen Bevölkerungen hervorgehoben und dabei helfen kann, sicherzustellen, dass die Technologie für alle fair und effektiv ist.</sample>
    <sample id="766">BLOOM wurde durch Adapter angepasst.</sample>
    <sample id="767">Das Modell, das verwendet wird für das Transferlernen, ist das Modell, das wir verwenden, um das aktuelle Lernen zu starten.</sample>
    <sample id="768">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die SQuAD 2.0, the Natural Questions (NQ), and the TriviaQA datasets.</sample>
    <sample id="769">Die Autoren haben schließlich drei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">Die vorgeschlagene Methode zeigt einen hohen Gewinn im Vergleich zur stärksten Baseline.</sample>
    <sample id="771">The speaker's name is Chu Huang.</sample>
    <sample id="772">Ja, die Ergebnisse und der Datensatz der Studie können als Benchmark verwendet werden.</sample>
    <sample id="773">In der Arbeit werden 5 kleineren Modellvarianten experimentiert.</sample>
    <sample id="774">Das OFA-Modell wird als Basismodell verwendet.</sample>
    <sample id="833">Die Autoren gehören an der Stanford University.</sample>
    <sample id="834">Die Autoren gehören an Stony Brook University.</sample>
    <sample id="835">Die Arbeit untersuchte die Sprachpaare English-French, German-Spanish, and Russian-Italian.</sample>
    <sample id="836">Shanbing</sample>
    <sample id="837">Die Modelle, die während der Experimente untersucht wurden, sind der Long Impart-Modell und die Normal Base Long Impart-Modell.</sample>
    <sample id="838">Fünfundsiebundert Aufgaben werden für Training und Tests verwendet.</sample>
    <sample id="839">Eine Person, Regina Stönn, ist an der Arbeit beteiligt.</sample>
    <sample id="840">Die Autoren haben experimentiert an den Datensätzen AG News, 20 Newsgroups, S2T2 und ER-Sem.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">The speaker's name is Ida Vilad.</sample>
    <sample id="878">Die Prompt-Strategie hat einen großen Einfluss auf die Leistung der LLMs (Large Language Models) bei der Übersetzung, wie im einfachen Experiment zu sehen war, in dem zwei verschiedene Prompts für dieselbe Satzsequence verwendet wurden.</sample>
    <sample id="879">Die Autoren gehören an der Universidad de los Andes und Universidad de los Andes.</sample>
    <sample id="880">Die 5 Anweisungen der Expert*innen sind: 

1. Collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks.
2. Releasing the dataset so that others can use it.
3. Providing a QR code for accessing the data and model.
4. Thanking the audience for their attention.
5. Encouraging the audience to explore the dataset and model further.</sample>
    <sample id="881">Die Autoren schlagen die Evaluierung des Datensatzes mit humanen Studieparteien und etablierten Konsensmessungsmethoden vor, um die Modellkapazität zu bewerten.</sample>
    <sample id="882">Hallo, alle. Mein Name ist Ilya Vlady and ich werde Ihnen einen kurzen Überblick über die Arbeit mit Google Translate geben. Das ist ein Joint-Work mit meinen Kollegen von Google Translate.</sample>
    <sample id="883">Bam ist ein 540 Milliarden Parameter Sprachmodell, das vor einem Jahr, im Jahr 2022, präsentiert wurde. Es ist trainiert auf einer großen Sammlung von Texten, die 780 Milliarden Token umfasst.</sample>
    <sample id="884">Die Täler der Aufklärung ist ein state-of-the-art-Tool in hundert ERP-Systemen.</sample>
    <sample id="885">In this work, we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="886">Wir bewerten die Translationsfähigkeit solcher Modelle, indem wir die besten Praktiken der MT-Community verwenden. Das bedeutet, dass wir die neuesten Testdatensätze verwenden, um sicherzustellen, dass das Test-Datenmaterial mit dem Trainingsmaterial des Sprachmodells nicht überlappt.</sample>
    <sample id="887">Wir vergleichen zwei state-of-the-art-Systeme. So die besten Performing-Systeme sind der D-Wave evaluation.</sample>
    <sample id="888">Wir verwenden state-of-the-art und neuartige Metriken und zusätzlich auch Expertenbasierte Immunanalytionsergebnisse. Ferner, wir bieten einige Empfehlungen für Prognosestrategien.</sample>
    <sample id="889">Das Prompting hat einen großen Einfluss auf die Leistung der ELNs bei der Übersetzung, wie wir in einem einfachen Experiment sehen können, bei dem wir ein einstelliges Prompting verwenden und zwei verschiedene Prompts für eine bestimmte Sache verwenden.</sample>
    <sample id="890">Die Mehrheit der Sätze 516 von 1000 die Differenz aufzuwarten, ist von mehr als einem Punkt.</sample>
    <sample id="891">Und das kann in extremen Fällen bis zu 40 Punktetagen betragen. Also ist es wichtig, eine gute prompting-Strategie zu verwenden.</sample>
    <sample id="892">In our experiments we test for a fixed shot prompting strategy where we just mark each sentence that we provide to the system with the language it's in.</sample>
    <sample id="893">In diesem Beispiel hier, wo wir einen Übersetzung von Deutsch ins Englische machen, sind die deutschen Sätze, die Quelltexte, mit einem deutschen Klon und die englischen Übersetzungen mit einem englischen Klon markiert.</sample>
    <sample id="894">Wir sahen, dass die tatsächliche Form der Drucke nicht einen großen Einfluss auf den Fall von Serien druckt.</sample>
    <sample id="895">Es ist kritisch für zero and one shot prompting und wenn wir in unserem Fall zu five shot prompting gehen, gibt es fast keine Unterschiede in der tatsächlichen Form der promptings.</sample>
    <sample id="896">It's the examples that carry most of the weight.</sample>
    <sample id="897">Die Summe aus our experimental results is that the example quality is more important than the similarity similarity to the source sentence.</sample>
    <sample id="898">Es ist wichtig, die Beispiele aus hochwertigen Übersetzungen zu wählen. Insbesondere vergleichen wir die Selektionsprompts aus dem Train-Daten der UMT Evaluierungen oder dem Dev-Daten.</sample>
    <sample id="899">Die Dev-Daten sind viel mehr qualitativ und quantitativ als die Train-Daten, was zu besseren Resultaten und besserer Leistung führt.</sample>
    <sample id="900">Trotzdem haben spezialisierte Systeme der Übersetzungen einen wesentlichen Vorteil gegenüber den Pann-translations. Aber Pann kommtPretty nah an einem kommersiellen System heran, und in unserem Fall haben wir uns für Google-Translate entschieden.</sample>
    <sample id="901">Die Einsichten, die wir aus dem dynamischen Analysetraum gewinnen, den wir mit dem MQM-Modell verwenden, bestehen darin, dass die Flüssigkeit von Palm vergleichbar ist zu einem System im Staat der Kunst (art systems), aber die Hauptunterschiede liegen in der Genauigkeit.</sample>
    <sample id="902">In particular die most common error are omission errors.</sample>
    <sample id="903">Es scheint, dass PAM Entscheidungen trifft, um ein besseres Übersetzungstext zu produzieren, manchmal indem sie Teile des Quelltextes aus dem Übersetzungstext entfernt.</sample>
    <sample id="904">Jedoch ist die Stelle im Outlier-Kategori für Pann unter derjenigen für die State-of-the-Art-Systeme, was ein angesichtiger Signal.</sample>
    <sample id="905">Dass PAMR eine wirklich fluide Ausgabe bietet, aber immer noch mit ein paar Problemen der Genauigkeit.</sample>
    <sample id="906">Und das ist es für diese wirklich kurze Übersicht. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="907">Hallo, ich bin Dawei, ein PhD-Student an der Salzburg University in Deutschland. In diesem Video möchte ich gerne unsere jüngste Arbeit "Wickeder als du denkst" präsentieren, eine kritische Analyse von Weekly Superspy.</sample>
    <sample id="908">Dieses ist ein Joint Work mit Chih-Yu Chen, Myron Smooth, Bassett Stephen und Dietrich Klakow.</sample>
    <sample id="909">Ich möchte beginnen mit einer kurzen Einführung in die week supervision und weekly supervisie</sample>
    <sample id="910">In weak supervision, you do not manually label the data. Instead, we label the data using weak labeling sources such as simple heuristics rules, knowledge bases or low-quality crowdsourcing, as illustrated in the figure on the right.</sample>
    <sample id="911">Wann siecompared to human annotations, the weak annotations are much cheaper yet they are also noisy, meaning that a certain amount of the annotations are incorrect.</sample>
    <sample id="912">Wenn wir neuronale Netze direkt an weeklich labelten Daten trainieren, tendieren sie dazu, das Label-Noise zu memorieren und nicht zu generalisieren.</sample>
    <sample id="913">In kritisch überwundenen Training-Algorithmen werden robuste neuronalen Netze unter solcher Lärm-Störung trainiert, so dass die trainierten Modelle weiterhin gut generalisieren.</sample>
    <sample id="914">In jüngster Zeit wurde das WSL (Weekly Supervised Learning) diskutiert. Das WSL steht für "WEEKLY SUPERVISED LEARNING". Ein gemeinsamer Ansatz ist, dass Menschen behaupten, dass sie trainierte Modelle auf wöchentlichen Datensätzen verwenden und dabei hohen Leistungslevel erreichen können.</sample>
    <sample id="915">Technisch gesehen ist dieser KBitte nicht falsch, aber es gibt einen Haken.</sample>
    <sample id="916">Das ist, dass die Leute davon ausgehen, dass es einen zusätzlichen, klare Validierungsset vor ModellSelektionsavailable gibt.</sample>
    <sample id="917">Wir kasten doubt on this problem setting as this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked.</sample>
    <sample id="918">Der obige Punkt besticht darin, uns zu fragen, drei Forschungsfragen zu stellen: Erstens ist eine klare Validationsdaten notwendig für WSL? Oder können wir eventuell einen noisigen Validationsset anstelle verwenden?</sample>
    <sample id="919">Zweitens, wenn saubere Daten erforderlich sind, oder wenn saubere Daten für den WSL notwendig sind, dann wie viele saubere Proben benötigen wir? Schließlich, sollten wir nur die sauberen Proben verwenden, um zu validieren, oder gibt es bessere Methoden, um sie zu nutzen?</sample>
    <sample id="920">Wir haben unsere Forschungsfragen in unserem Werk adressiert und unsere Findungen lauten wie folgt.</sample>
    <sample id="921">Erstens finden wir, dass interessanterweise recente WSL-Methoden in der Tat kleine, weiße Täfelungssamples benötigen, um korrekt zu arbeiten.</sample>
    <sample id="922">Andererseits gibt es einen großen Performance-Sprung, wie in diesem Diagramm gezeigt. Wenn es keine klaren Validationsamples gibt, dann können die trainierten Modelle nicht über die ursprünglichen Objektlabels hinaus generalisieren.</sample>
    <sample id="923">Das Training ist ohnehin sinnlos.</sample>
    <sample id="924">Dies zeigt, dass WSL-Modelle tatsächlich die Reinigung der Daten benötigen, um korrekt zu arbeiten, und die Kosten für die Gewinnung von reinen Validationsstichproben sollten nicht übersehen werden.</sample>
    <sample id="925">Unser zweiter Fund ist, dass das Erhöhen der Anzahl der validierbaren Ausprägungen helfen wird, WSL-Methoden zu verbessern, wie es im Bild auf der linken Seite gezeigt wird.</sample>
    <sample id="926">Typisch benötigen wir nur 20 Samples pro Klasse, um eine hervorragende Performance zu erzielen.</sample>
    <sample id="927">Aber das ist nicht das Ende der Geschichte, denn wenn wir entweder eine Entscheidung treffen, um saubere Ausprägungen zu verwenden, dann wird die direkte Trainingseinbetung davon noch besser abschneiden.</sample>
    <sample id="928">Die rechte Figur zeigt die Leistungsunterschiede zwischen Finetuning- und WSL-Ansätzen, die direkt auf die reinen Daten angewandt werden und die reinen Daten nur zur Validierung verwenden.</sample>
    <sample id="929">As we can see if we have ten samples per class, direct fine-tuning starts to beat wsl approaches.</sample>
    <sample id="930">Schließlich kann die Leistungsverbesserung, die in früheren WSL-Ansaugen geltend war, leicht erreicht werden, indem man die Kontinuierliche Finetuning- und die Clean-Validation-Samples erlaubt.</sample>
    <sample id="931">Wie wir aus den Grafiken sehen können, unterperformiert der Wavenlinna-Modell mit dem FTW ursprünglich komplexere WSL-Methoden wie Kosyns.</sample>
    <sample id="932">However, if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other</sample>
    <sample id="933">So in der Praxis gibt es keinen Grund, komplexere WSL-Methoden zu verwenden, die mehr Rechenzeit und Festplattenplatz benötigen.</sample>
    <sample id="934">Zusammenfassend haben wir gezeigt, dass recente WSL-Modelle von sauber, manuell annotierten Datensätzen abhängig sind, um korrekt zu arbeiten. Ihre Leistungsgewinne und Praktizität werden stark überbewertet.</sample>
    <sample id="935">Unsere kompletten Empfehlungen für zukünftige Arbeiten lauten wie folgt.</sample>
    <sample id="936">Erst berichte die ModellAuswahlkriterien. Beispielsweise berichte, ob die ModellAuswahl ist nur auf CleanValidationSamples basierend.</sample>
    <sample id="937">Zweitens, WSL-Objekte sollten mit kürzlen Laenning-baselines als ob sie wahllos samples sind. Drittens, ständige Fine-tuning ist ein einfach aber starkes Baseline, das in zukünftigen Arbeiten in WSL berücksichtigt werden sollte.</sample>
    <sample id="938">Endlich haben wir Open Source unser Code. Sie können es unter der QR-Code auf dieser Slide finden. Bitte, fühlen Sie sich frei, es zu überprüfen. Danke und genießt den Kongress.</sample>
    <sample id="939">Gängige Bewertungsmethoden für Dialogsysteme sind die Verwendung von menschlicher Beurteilung, wie zum Beispiel die Anfrage an menschliche Richter, um zu bestimmen, welcher der zwei Dialoge besser ist, oder die Beurteilung von Dialogen auf einer Likert-Skala.</sample>
    <sample id="940">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="941">Im Beispiel mit Servin und Kea wird das Hintergrundwissen verwendet, dass Richter in Gerichtsräumen Fälle entscheiden.</sample>
    <sample id="942">Ja, der Code ist verfügbar und er ist auf GitHub.</sample>
    <sample id="943">Ja, die Annotatoren für NLPositionality sind in Bezug auf jede demographische Gruppe ausgewogen.</sample>
    <sample id="944">Sätze innerhalb der akzeptablen Domain wurden durch das Hinzufügen von Geräuschen perturbiert, was eine Änderung im Modell nicht beeinflusst.</sample>
    <sample id="945">Eine dimensionale Bewertung bedeutet die Auswertung mehrerer Aspekte der Chatschwelle, um die Stärken und Schwächen des Modells auf einem feineren Niveau zu verstehen.</sample>
    <sample id="946">Die Autoren gehören an der University of Science and Technology of China.</sample>
    <sample id="947">Die Form des Prompts ist wichtig in den Fällen von null und einem Prompting.</sample>
    <sample id="978">Die Autoren haben Bots evaluiert.</sample>
    <sample id="979">Die englische Übersetzung des Textes lautet: "Hello everyone, my name is Jing Wei Yi from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about our paper 'Are you copying my model? Protecting the copyright of large language models for embedding and services' with backdoor watermark." Basierend auf dem englischen Inhalt sind zwei Autoren an der Arbeit beteiligt: Jing Wei Yi und ein anderer Autor, dessen Name nicht erwähnt wurde.</sample>
    <sample id="980">Ein guter Planer sollte flexible und realistische Pläne erstellen, die die verschiedenen Bedingungen und Beschränkungen berücksichtigen.</sample>
    <sample id="981">Die englische Textstelle gibt nur einen Namen, "Cui Yuan", an. Es ist nicht klar, ob es sich um einen einzelnen Autor handelt oder mehrere Autoren beteiligt sind. Daher können wir die Anzahl der Autoren nicht bestimmen.</sample>
    <sample id="982">The presenter's name is Vasudeha.</sample>
    <sample id="983">Die Autoren gehören an der Technischen Universität Moskwa.</sample>
    <sample id="1021">Die häufigsten Fehler von PaLM sind omission errors.</sample>
    <sample id="1022">Hallo, ich bin James Finch und ich bin Sarah Finch. Heute werden wir Ihnen alles über ABC-Eval erzählen, eine neue dimensional-basierte Methode zur Beurteilung von conversationaler KI.</sample>
    <sample id="1023">Diese Arbeit wurde von dem Emory NLP Lab, geleitet von Professor Geno Choi an der Emory University und in Zusammenarbeit mit Amazon Alexa AI, getan.</sample>
    <sample id="1024">Also, lassen Sie uns annehmen, dass Sie ein Dialogmodell entwickelt haben und nun sehen möchten, wie gut es sich gegenüber dem aktuellen Stand der Kunst vergleicht.</sample>
    <sample id="1025">Die gemeinsame Praxis besteht darin, menschliche Beurteilung zu verwenden, indem man menschlichen Richtern fragt, welche der zwei Konversationen besser ist oder Konversationen auf einer Likert-Skala bewertet.</sample>
    <sample id="1026">Diese Ansätze arbeiten gut daran, die allgemeine Dialogqualität zu bewerten, aber Dialogqualität hat viele Aspekte. Daher sollten Sie mehrere Dimensionen der Chatschwelle bewerten, um die Stärken und Schwächen des Modells auf einem feineren Granularitätsniveau zu verstehen.</sample>
    <sample id="1027">Eine Annäherung besteht darin, einfach mensliche Richter zu überprüfen, um mehrere Dimensionen der Dialogqualität zu bewerten, wie z.B. die Relevanz von Modellantworten, indem sie existierende vergleichende oder Likert-Skala-Methoden verwenden.</sample>
    <sample id="1028">Allerdings glauben wir, dass es eine präzisere und zuverlässlichere Strategie für die Dimensionale Dialog-Evaluation gibt.</sample>
    <sample id="1029">Unsere Ansatz versucht, die Subjektivität menschlicher Bewertung zu reduzieren, indem wir explizit anmerken, ob oder ob nicht jede Modell-Antwort bestimmte Verhaltensweisen ausdrückt, wie zum Beispiel das Ausgeben von irrelevanter Information oder sich selbst widersprechen.</sample>
    <sample id="1030">Wir nennen diese Ansatz Annästern von Verhaltensweisen in Chats, oder ABC-Evaluation im Kurzform. Wir haben diese Methode entwickelt, um die Verhaltensweisen von Chats zu umfassend abdecken, die in jüngster Literatur zur Chatschwelle vorgeschlagen wurden.</sample>
    <sample id="1031">ABC-Eval ist in der Lage, die Geschwindigkeiten zu messen, bei denen Sprachmodelle verschiedene thematische Fehlertypen machen.</sample>
    <sample id="1032">Beispielsweise misst ABC-Eval die Anzahl der Runden, in denen ein Chatschwamm seinen Partner ignoriert oder etwas Irrwitziges sagt.</sample>
    <sample id="1033">Widersagt sich oder seinem Partner, allgemein anerkannte Wahrheiten falsch präsentiert oder gegen allgemein anerkannte Wissensgebiete verstößt und zeigt Empathie, wenn das Modell succeeded or fails.</sample>
    <sample id="1034">Um zu bestimmen, welche Art von Evaluierung am besten wirkt, haben wir vier modernste Chat-Modelle ausgewählt und sie auf 100 menschlich-basierten Konversationen pro Modell mittels ABC-Eval bewertet.</sample>
    <sample id="1035">Für Vergleiche evaluieren wir auch diese Konversationen mit drei existierenden Methoden Likert Ratings auf der Turnebene, Likert Ratings auf der Dialogebene und Dialogebene Parewiser Vergleiche.</sample>
    <sample id="1036">Für jedes der existierenden Methoden haben wir Auswertungen zu acht von den am häufigsten gemessenen Aspekten von Dialogen gesammelt, da dies die Standardpraxis ist, um Chatsmodelle an mehreren Dimensionen zu evaluieren.</sample>
    <sample id="1037">Von unseren Analysen dieser Evaluierergebnisse haben wir festgestellt, dass ABC-Eval-Behaviorlabels im Allgemeinen zuverlässiger sind alsLabels, die von bestehenden Methoden ermittelt wurden, wie gemessen durch Interanimator-Übereinstimmungen auf 100 doppelt bewerteten Konversationen.</sample>
    <sample id="1038">Darüber hinaus sind ABC-Eval-Labels besser in der Vorhersage der allgemeinen Konversationsqualität im Vergleich zu Metrischen, die von bestehenden Methoden erzeugt werden, wie durch eine einfache lineare Regression anhand gezeigt.</sample>
    <sample id="1039">Beispielsweise können Sie sehen, wie die Messung der Proportion von Wendungen mit sichsel- und Partnerkontraindexionen 5 % und 10 % der Konversationsqualität respektiv erläutert, während die durchschnittliche Likert-Konsistenzscore nur 4 % oder weniger erklären.</sample>
    <sample id="1040">Schließlich überprüften wir, ob jede Evaluationsmetrik einen eindeutigen Aspekt der Chatschwelle abdeckt, indem wir einen schrittweise linearen Regressionen anwandten.</sample>
    <sample id="1041">Sie können sehen, wie die Kombination aller ABC-Eval-Metriken über 25% der Konversationsqualität erklärt. Und wenn Sie die Metriken逐一移除，大多数都会导致失去相当一部分关于质量的信息。</sample>
    <sample id="1042">Auf der anderen Seite erklärt die Kombination aller Turnlevel-Likert-Metrisken viel weniger die Qualität und fewer davon tragen einzigartige Informationen.</sample>
    <sample id="1043">Diese zuverlässigen, informativen und eindeutigen ABC-Evaluation-Metrisques erlauben es uns, Konversations-AI mit einer höheren Auflösung zu bewerten als die vorherigen Methoden, die das erreichen können.</sample>
    <sample id="1044">Sie können sehen, dass in den Ergebnissen unseres Experiments mehrere Herausforderungen immer noch bestehen und exakt quantifiziert wurden. Zum Beispiel haben die Bots, die wir getestet haben, in etwa 20% ihrer Antworten Verstöße gegen das Menschenverstand.</sample>
    <sample id="1045">Sie produzieren irrelevantes Information in um die 15-prozentigen Antworten und sie kontradieren sich oder ihr Partner um die 10-prozentigen Male.</sample>
    <sample id="1046">Mit der schnellen Verbesserung im Bereich können viele dieser Fehlerraten seit der Durchführung unseres Evaluationsverfahrens abnehmen. Allerdings ist dies noch mehr ein Grund, zu verfolgen zuverlässliche und präzise Evaluiermetrischen zur Vergleich von Modellen.</sample>
    <sample id="1047">Wir hoffen, ABC-Eval kann von anderen im Feld als ein wichtiger Schritt in diese Richtung verwendet werden und wir freuen uns darauf zu sehen, wie konsolidiertes KI im kommenden Monaten und Jahren fortschritt. Vielen Dank für das Watching.</sample>
    <sample id="1048">Die Autoren gehören an der Emory University.</sample>
    <sample id="1049">CFT steht für "Continuous Fine Tuning".</sample>
    <sample id="1050">Sechs Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1051">Hallo, mein Name ist Kai Yuan und ich werde unsere Arbeit mit dem Titel "Wann erfordert Übersetzung Kontext? Eine datengetriebene multilinguale Explorations" präsentieren. Diese Arbeit wurde in Zusammenarbeit mit Patrick Franz, Emil Niu, Andrea F. Martinez und Graham Mubig erstellt.</sample>
    <sample id="1052">So eine lot of translations depend on context. For example, how would we translate mole in this sentence?</sample>
    <sample id="1053">Wenn die vorherige Aussage war, dass Dinge gefährlich werden könnten, wenn die Minister davon erfahren, dann bezieht Moore sich auf eine Lunge. Aber wenn die vorherige Aussage war, ob es etwas Ernstes geben könnte, Doctor, dann bezieht Moore sich auf ein Mutterschaftsmerkmal.</sample>
    <sample id="1054">So depending on context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1055">Allerdings ist es schwierig zu beurteilen, wie gut Modelle solche Fälle bewerten können. Zunächst einmal because nur ein kleiner Teil von Übersetzungen von Kontext abhängt, was bedeutet, dass Metrischer Ansatz wie BLEU unfähig ist, diese Übersetzungen zu capturieren.</sample>
    <sample id="1056">Ein paar Leute haben vorgeschlagen, eine präzisierte Bewertung von kulturabhängigen Übersetzungen vorzunehmen. Aber diese Ressourcen unterstützen nur begrenzte Arten von kulturabhängigen Übersetzungen und begrenzte Mengen an Sprachen, da sie normalerweise von Fachkenntnis und menschlicher Auswertung abhängig sind.</sample>
    <sample id="1057">In diesem Werk versuchen wir, zwei Fragen zu beantworten: Zunächst, wann wird eine Übersetzung Kontext erfordern, und zweitens, wie gut können Modelle solche Fälle bewältigen?</sample>
    <sample id="1058">Um die erste Frage zu beantworten, haben wir begonnen, wie viel ein Wort von Kontext abhängt, wenn es übersetzt wird.</sample>
    <sample id="1059">In der vorherigen Arbeit haben wir CxMI als Maß für Kontexte verwendet, die von Maschinentranslationmodellen verwendet werden. Und das ist durch Messen von der Menge der Informationen, die der Kontext C über den Ziel Y gibt, gegeben die Quelle X.</sample>
    <sample id="1060">Du kannst den Begriff "SXMI" als die Information, die durch das Anpassen an Kontexte an das Modell gewonnen wird, verstehen.</sample>
    <sample id="1061">In diesem Werk extendieren wir Sxmi auf Poyx Sxmi, die Kontext-Nutzung an der Satzstelle oder an der Wortstelle messen kann. Wir können Wörter denken, die hohe P6xmi haben, als solche, die Kontext für die Übersetzung benötigen.</sample>
    <sample id="1062">Jetzt analysieren wir Wörter mit High-PSMI, um Muster zwischen diesen Wörtern zu finden.</sample>
    <sample id="1063">Und wir haben unsere Analyse auf Transkripten von TED Talks durchzuführen, die von Englisch ins Deutsche und in 14 weitere Sprachen übersetzt wurden.</sample>
    <sample id="1064">Wir führen unsere Analyse an drei verschiedenen Ebenen durch. Zunächst schauen wir uns Parteien von Speech-Tags an, die eine hohe PSX-MI-Schwelle übersteigen.</sample>
    <sample id="1065">Dies erlaubt uns, z. B. die dualen Pronomen in Arabisch zu finden, die über ein P6xi-Muster verfügen und das kann erklärt werden, weil English keine dualen Pronomen hat, also man bedarf von Kontext, um zu bestimmen, ob ein Pronomen dual ist, wenn man es übersetzt in Arabisch bringt.</sample>
    <sample id="1066">Und so finden wir, dass bestimmte Sprachen auch Kontext benötigen, wenn wir die richtige Formulierung einer Verwendung wünschen. Wir suchen dann nach lexikalischen Einträgen mit hohen durchschnittlichen Häufigkeiten über alle ihre verschiedenen Vorkommen.</sample>
    <sample id="1067">Und dies hilft uns dabei, Fälle wie den hier zu identifizieren, in denen in chinesischen Texten Kontext notwendig ist, um richtige Übersetzungen von Substantiven zu finden, um sicherzugehen, dass wir die gleiche Übersetzung im Dokument verwenden.</sample>
    <sample id="1068">Und ähnlich, wir finden, dass KodizesUnsupported to translate in the right formality.</sample>
    <sample id="1069">Und schließlich sehen wir uns an verschiedenen Individuen tokens, die eine hohe PESXMI haben und dies erlaubt uns, Phänomene zu identifizieren, die nicht nur durch das Wort selbst, sondern auch durch die instanzielle Struktur, z.B. die Ecke resolution, abgedeckt werden können.</sample>
    <sample id="1070">So now we use our findings from our analysis to design a benchmark for document-level translation.</sample>
    <sample id="1071">Für jedes der fünf diskursive Phänomene, die wir identifiziert haben, erstellen wir Tagger, um automatisch Wörter zu identifizieren, die sich auf das Phänomen beziehen. Und wir nennen unser Tagger Multilingual-Diskursaware (oder MuDa-Tagger).</sample>
    <sample id="1072">Wir können dann auch noch notieren, dass verschiedene Sprachen unterschiedliche Proportionen von diesen diskursiven Phänomenen haben.</sample>
    <sample id="1073">Wir verwenden dann die MuDa-Tagger by applying the tagger on the parallel corpora that we want to use for evaluation and we apply our translation metrics of choice on the code-switching dependent examples that the MuDa-Tagger has identified.</sample>
    <sample id="1074">And finally we use our benchmark as well as other metrics to evaluate different models on the document level machine translation.</sample>
    <sample id="1075">Erstens, wenn wir Corpus-agnostic-Metrisen verwenden, also für Blau, finden wir, dass kognitiv-agonistische Modelle die beste Performance aufweisen.</sample>
    <sample id="1076">Aber dann, wenn wir Commod-Context-aware-Modelle verwenden, performieren sie am besten und wenn wir Word F-Measure verwenden, haben Modelle mit und ohne Kontext vergleichbare Leistungen.</sample>
    <sample id="1077">Dies zeigt erneut, dass es schwierig ist zu bestimmen, welches Dokumentebene Übersetzungssystem am besten ist, wenn wir nur Korpusbasierte Metriken verwenden.</sample>
    <sample id="1078">Jetzt verwenden wir die Můděn-Benchmarks, um Modelle zu evaluieren und finden, dass Kontext-aware-Modelle signifikant genauer sind als diejenigen, die keinen Kontext für bestimmte diskursive Phänomene wie Formalität und lexikalische Kohärenz verwenden.</sample>
    <sample id="1079">But these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns and verb form. So this sort of suggests where we would need to see more progress for document-level translation.</sample>
    <sample id="1080">Wir vergleichen auch verschiedene kommerzielle Systeme und unsere Benchmarks zeigen, dass DeepL normalerweise für Dokumentübersetzung präziser ist als Google Translate.</sample>
    <sample id="1081">Zusammenfassend haben wir eine datengetriebene Analyse über 14 Sprachpaare durchgeführt, um zu identifizieren, wann Übersetzungen Kontext benötigen.</sample>
    <sample id="1082">Und dann verwenden wir unsere Findungen, um einBenchmark für Dokumentebasierende Mischsprachübersetzung zu erstellen, das uns dabei helfen kann, zu identifizieren, welche diskursive Phänomene Modelle gut oder nicht gut handhaben und welche Übersetzungsensysteme gut auf Dokumentebasierende Übersetzung sind.</sample>
    <sample id="1083">Danke sooooviel für die Aufmerksamkeit. Bis bald in Torado!</sample>
    <sample id="1084">Usain John</sample>
    <sample id="1121">The new method does not have a name.</sample>
    <sample id="1122">Die Autoren beschreiben die Methode der "markierten Wörter" als eine Möglichkeit, die Wörter zu identifizieren, die markierte Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="1123">Die Autoren gehören an der University of Washington.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">James Finch and Sara Finch</sample>
    <sample id="1126">Vier Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="1127">Die Minimal Pair Paradigm-Datasets können zum Testen syntaktischer Phänomene verwendet werden.</sample>
    <sample id="1161">Die Abkürzungen der fünf Methoden für die erste Forschungsfrage sind WSL, SL, L, S und L.</sample>
    <sample id="1162">Das Modell wird evaluiert anhand von 11 biomedizinischen und klinischen downstream Aufgaben in Französisch.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich auf einem 4GB-Teil der NACOS-Datei trainiert.</sample>
    <sample id="1227">Der Referent ist Adam Strykowsky.</sample>
    <sample id="1228">Die Versuche zu erneut oder fortsetzen die vorherige Ausbildung einiger Modelle mit jüngerer Datenbank führten dazu, dass die Leistung mit einem größeren zeitlichen Abstand abnimmt. Dies bestätigt unsere Hypothese, dass die Hauptursache für den Leistungsverlust die zeitliche Verzögerung ist.</sample>
    <sample id="1269">Es ist notwendig, die Token für die Ausgabesequenz zu permutieren, um sie in die richtige Reihenfolge zu bringen.</sample>
    <sample id="1270">Die Autoren empfehlen, dass Modellentwickler*innen ihre Methoden zum Abbau von Vorurteilen transparent machen sollten, um zu verstehen, warum bestimmte Musterte auftreten und um sicherzustellen, dass die Verfahren wirksam sind.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind grammatikalisch falsche Sätze.</sample>
    <sample id="1272">Die Autoren haben die Werte und Tokalnizer von PAMBERT trainiert auf der 4GB-Teilmenge von NATOS verwendet.</sample>
    <sample id="1273">Innerannotator agreement on a hundred doubly labelled conversations.</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">Die Autoren gehören an der Technischen Hochschule Darmstadt.</sample>
    <sample id="1276">MultiInstruct ist ein multimodales Datensatz, der eine Vielzahl von Sprach- und Bildaufgaben enthält. Es ist ein Publikumsvielfaltiger Datensatz, der es für die Evaluierung von Sprachgenerationsmodellen ermöglicht, indem er eine Vielzahl von Aufgaben und Datensätzen enthält.</sample>
    <sample id="1277">Es sind zwei Autoren an der Arbeit beteiligt: James Finch und Sara Finch.</sample>
    <sample id="1278">Die Definition der binären Koordination lautet: "by measuring length in characters, the first column in syllables, the middle column in words, and the right column."</sample>
    <sample id="1279">Die in dieser Studie verwendeten Prompts hatten im Durchschnitt eine Länge von 10 Wörtern.</sample>
    <sample id="1280">Die Auswirkungen der Ergebnisse auf das kleinere T5-Modell sind, dass es besser als größere Modelle performen kann, wenn es auf geeignetem Datensatz trainiert wird.</sample>
    <sample id="1281">Hallo, ich bin Myni Slavac und ich werde Ihnen unsere Arbeiten über Dr. Bert, einen robusten prädiktiven Modell in Französisch für die biomedizinische klinische Domäne, präsentieren.</sample>
    <sample id="1282">In dieser Präsentation diskutieren wir zuerst über Sprachmodellierung in Gesundheitsversorgung. Danach präsentieren wir die Hauptbeiträge unseres Artikels.</sample>
    <sample id="1283">Wir Introduzieren die erst bio-medizin model in French named Dr Bert, which is based on Roberta und trained on Natsos, which is a data set of medical crawled data from the web.</sample>
    <sample id="1284">Wir haben auch einen Vergleich von Modellen mit mehreren Trainingssettings und Datensätzen vorgestellt. Dann präsentieren wir unsere Ergebnisse auf 11 biomedizinischen und klinischen downstream Aufgaben in Französisch.</sample>
    <sample id="1285">And finally, we conclude about the experiments and give you more details about how to access to the models.</sample>
    <sample id="1286">Seit seiner Einführung im Jahr 2018 ist BERT zu einem der effektivsten Ansätze geworden, um natürliche Sprachverarbeitungsaufgaben zu lösen und einen enormen Leistungszuwachs im Vergleich zu historischen statischen und konstanten Methoden wie Word2Vec, FastText und WordPiece zu erzielen.</sample>
    <sample id="1287">Seit damals wurde dieser Modell an viele andere Sprachen angepasst, wie in Französisch mit Camembert und anderen Feldern wie biomedizin mit Pflanzentier und Biobert und klinisch mit Klinikbert, aber hauptsächlich in Englisch.</sample>
    <sample id="1288">Spezialisierte Modelle für andere Sprachen sind scarce und oft basieren auf kontinuierlichem Pretraining due to the lack of in-domain data.</sample>
    <sample id="1289">Jedoch hatte Frank noch keine Open-Source-Software für Bioinformatik.</sample>
    <sample id="1290">Wir so wir unsAsked ourselves question about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data.</sample>
    <sample id="1291">Um diese Frage zu beantworten, vergleichen wir Doctor Bert mit unserem Shubert-Modell, das auf anonymisierten Daten basiert, die wir vom non-university hospital in our house erhalten haben.</sample>
    <sample id="1292">Nachfolgend fragen wir uns, wie viel Datendateien wir benötigen, um einen spezialisierten Modell auf Französisch zu trainieren. Ist es 4 GB, 8 GB oder mehr?</sample>
    <sample id="1293">Um diese Frage zu beantworten, haben wir zwei Ansätze trainiert und verglichen: Eine Erste Version von DoctorBERT mit 7 GB NACHOS und eine Zweite Version von 4 GB NACHOS-Teilmenge.</sample>
    <sample id="1294">Eine erstes Version von Shubert, die ein klinischer Modell ist, mit 4GB Worth von Sätzen, die aus klinischen Noten entnommen wurden. Eine finale Version von Shubert, die eine Mischung von 4GB subset of natürlicher Sprache und 4GB Worth von klinischen Noten enthält.</sample>
    <sample id="1295">In addition to this comparison, we introduce three models trained on continual pretraining to analyze the impact of pretraining strategies.</sample>
    <sample id="1296">Eine basierend auf dem Gewicht des Kamambert und trainiert auf einem 4 Gb-Sett Nato und die andere basierend auf dem Gewicht des Kamambert, aber trainiert in diesem Fall auf einem 4 Gb-Sett Klimanot.</sample>
    <sample id="1297">Insgesamt haben wir 7 Modelle.</sample>
    <sample id="1298">Um unsere Sieben Modelle zu evaluieren, messen wir jede Public und Private Domänen Aufgaben wie Namenserkennung, Klassifizierung, Part-of-Speech-Tagging und Fragenerkennung.</sample>
    <sample id="1299">Diese Modelle werden mit sechs Baseline-Modellen verglichen, die Camberon Oscar 108 Gigabyte, Camberon Oscar 4 Gigabyte, Camberon Cissinette 4 Gigabyte, Tummett by Albert und Klinic Albert sind.</sample>
    <sample id="1300">Die Evaluationen hängen von der Art ab, auf der das Modell performt, wenn es mit Daten der gleichen Art wie die trainierten Daten arbeitet.</sample>
    <sample id="1301">Jedoch können wir das Data von heterogenen Quellen abhängig machen. Wir können auch beobachtet, dass mehr Data zu besseren Resultaten führt.</sample>
    <sample id="1302">Insgesamt scheint der scratch-Pretraining zu besseren Resultaten auf den meisten Aufgaben zu führen.</sample>
    <sample id="1303">Allerdings zeigt unser Experiment mit der trainierten Version von PAMBERT an, dass wir ähnliche Resultate erhalten, wie wenn wir den 4 GB-Teil von Natsos von vornherein mit PAMBERT trainieren.</sample>
    <sample id="1304">Es ist nicht der Fall für die Modelle basierend auf Kammerbaren Gewicht und Tokin-Ischer, die von Stabilitätsproblemen betroffen sind.</sample>
    <sample id="1305">Schliesslich als Schlussfolgerung unserem neuen System bietet eine bessere Leistung an neun von den elf DART-Task und überlappt global das Resultat des generischen Modells hier, KAMBER.</sample>
    <sample id="1306">We also observe that specialized data is better, more specialized data is better, but it doesn't scale well.</sample>
    <sample id="1307">All the pre-trained models obtained from nato's are freely available and on huggingface, and all the training scripts are on our github repository.</sample>
    <sample id="1308">So, danke für die Präsentation und wir freuen uns auf die Auseinandersetzung in der Postsession in Toronto.</sample>
    <sample id="1309">Die Arbeit untersucht die Auswirkungen von Lernstrategien auf die Leistung von Modellen.</sample>
    <sample id="1310">Der Faktor der Überanpassung, der auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 1.5.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde mittels Evaluationsmetriken beurteilt, die im Papier detailliert beschrieben werden.</sample>
    <sample id="1312">Ja, Sprachmodelle haben unterschiedliche politische Vorurteile.</sample>
    <sample id="1313">Hallo, mein Name ist Matthias Lendemann und heute gebe ich einen kurzen Einführung in unser Papier über kompositionale Generalisierung ohne Bäume mit Mehrorts-Tagging und latenten Permutationen.</sample>
    <sample id="1314">Dies ist gemeinsame Arbeit mit meinen Beratern Alexander Koller und Ivan Titov.</sample>
    <sample id="1315">Die kompositionale Generalisierung kann als die Fähigkeit eines Lerners verstanden werden, um tieferen Rekursion und unbekannte Kompositionen von Phrasen zu bewältigen, die während des Trainings separat gesehen wurden.</sample>
    <sample id="1316">Im Kontext von semantischer Analyse könnte die Überprüfung auf kompositionale Generalisierbarkeit so aussehen: Wie üblich haben wir ein Trainingssatz von Aussagen, in diesem Fall "Die Frau schläft" und "Mary weiß, dass die Frau schläft".</sample>
    <sample id="1317">Diese Aussagen sind mit logischen Formen pariert, die core aspects of their meaning representieren.</sample>
    <sample id="1318">Im Gegensatz zu standardmäßiger Maschinelles Lernen-Evaluation enthält die Testdatenmenge nicht Datensätze aus derselben Verteilung, sondern enthält strukturell unsehenswerte logische Formen.</sample>
    <sample id="1319">In diesem Beispiel hat das Modell während des Trainings Shallow-Recursion gesehen und wird auf ein Beispiel mit tiefere Recursion getestet.</sample>
    <sample id="1320">Naive sequenz-to-sequence-Modelle kämpfen mit diesem Art von "out-of-distribution"-Generalisierte und oft produzieren Ausgaben, die von der Eingabe abgetrennt sind.</sample>
    <sample id="1321">In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example.</sample>
    <sample id="1322">Eine beliebte Methode, um sich damit zu beschäftigen, ist die Integration von Bäumen in die Modelle.</sample>
    <sample id="1323">Die Bäume sind dazu bestimmt, den kompositionellen Prozess zu capturieren, der Verknüpfungen zwischen Äußerungen und logischen Formen herstellt.</sample>
    <sample id="1324">Dies funktionsfähig, aber Bäume sind normalerweise nicht frei und müssen auf eine Weise erhalten werden.</sample>
    <sample id="1325">Dies kann komplex und manchmal eine computergesteuert teure Prozess betreffen. Typischerweise beinhaltet dies eine betrachtliche Formalismus spezifische Voreinzelung der logischen Formen, z.B. um die Variablen Symbole zu behandeln.</sample>
    <sample id="1326">Die Erhaltung von Bäumen kann auch spezialisierter Grammatikinduktion bedarf.</sample>
    <sample id="1327">In diesem Papier verwenden wir keine Bäume und führen einen neuronalen sequenz-to-sequenz-Modell ein, der direkter die Korrespondenzen zwischen Fragmente des Eingangs und Fragmente des Ausgangs modelliert.</sample>
    <sample id="1328">For the first time we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="1329">Unsere Ansatz vorhersagt die Ausgabe aus der Eingabe in zwei Schritte.</sample>
    <sample id="1330">Erst tagen wir each input token mit einem unsortierten Multi-Set von Token, die im Ausgang auftreten werden.</sample>
    <sample id="1331">Nach dem ersten Schritt haben wir alle die richtigen Token, aber sie sind nicht ordnungsgemäß sortiert.</sample>
    <sample id="1332">Das ist der Grund, warum im zweiten Schritt wir noch einen Modell zu predict a permutation zu verwenden, um sie in die richtige Reihenfolge zu sortieren.</sample>
    <sample id="1333">Wir Introduzieren ein neues Verfahren zur Vorhersage von Permutationen, das keine harten Beschränkungen auf die möglichen Permutationen anwendet. Das macht unser Ansatz sehr flexibel und ausdrucksstark.</sample>
    <sample id="1334">Konzeptionell unserem Permutationenmodell, wie es soeben beschrieben wurde, sieht es so ähnlich.</sample>
    <sample id="1335">Wir gehen von links nach rechts über die Ausgabe und bestimmen, welchen Multi-Menü-TOKEN wir in jede Position platzieren. Für die erste Ausgabeposition auswählen wir einfach einen, wie in Rot hervorgehoben.</sample>
    <sample id="1336">Dann springen wir zu dem nächsten Multi-Set-Token, um den nächsten Token im Ausgang zu bestimmen.</sample>
    <sample id="1337">Wir bestimmen den dritten Token in der Ausgabe in einem ähnlichen Weise, indem wir zu einem anderen Multi-Set-Token springen. Wir continuiere this Prozess.</sample>
    <sample id="1338">Bis zu jenem Punkt, an dem alle Symbole aus dem ersten Stadium genau ein Mal besucht wurden.</sample>
    <sample id="1339">Um Ihnen einen Vorgeschmack auf die experimentellen Ergebnisse zu geben, vergleichen wir hier unser Verfahren mit anderen TreeLSTM-Modellen an der Cog's-Benchmarke. Unser Modell überlebt die anderen um ein großes Maß in Bezug auf die Generalisierbarkeit zu tieferen Rekursionen.</sample>
    <sample id="1340">Einige andere Art von struktureller normalisierung sind sehr schwierig.</sample>
    <sample id="1341">In unserem Papier lösen wir eine Reihe von interessanten technischen Herausforderungen.</sample>
    <sample id="1342">Erstens ist die Einordnung von Eingabe und Ausgabe im Trainingssatz nicht gegeben. Als Folge davon kennen wir für einen bestimmten Token nicht, welches Multi-Set es stammt von. Das stellt eine Herausforderung für die Ausbildung dar.</sample>
    <sample id="1343">In addition, sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent. We address this by inducing the alignment as part of the training.</sample>
    <sample id="1344">Unsere Permutationsmethode ist sehr flexibel, aber sie bringt die Herausforderung mit sich, dass das finden der höchste Punktscores Permutation NP-hard ist. Das liegt daran, dass dies sich auf das Reisefahrerproblem bezieht.</sample>
    <sample id="1345">Wir approximieren dies mit einer GPU-freundlichen stetigen Relaxation, die uns auch erlaubt, durch die Lösung zu propagieren und grammatikalisch plausible Permutationen zu lernen.</sample>
    <sample id="1346">Wenn Sie mehr über unsere Experimente und wie wir diese Herausforderungen bewältigen möchten, bitte haben Sie einen Blick auf unser Papier oder kommen zu unserem Poster.</sample>
    <sample id="1347">Kognitive Dissonanz ist die Unstimmigkeit zwischen zwei Glaubensstellingen oder Handlungen.</sample>
    <sample id="1348">GPT-4</sample>
    <sample id="1349">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="1350">Sarah Papi</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen von Transcripts of TED Talks, die von Englisch ins Deutsche über 14 verschiedene Sprachen übersetzt wurden.</sample>
    <sample id="1385">The referent is named Matthias Lendemann.</sample>
    <sample id="1386">Sprachübergreifender Transfer ist die Übergabe von Modellen von einem Sprachenblock zu einem anderen Sprachenblock.</sample>
    <sample id="1387">Die Autoren gehören an Salzburg University in Germany.</sample>
    <sample id="1388">Die Autoren verwenden die Latenzmessungen "average latency" und "computational aware average latency".</sample>
    <sample id="1389">Hallo, alle zusammen. Ich bin Manjushtha und heute präsentieren mein Kollege Martin und ich unser Werk "Kitabust" Evaluating knowledge integration from multiple sources. Das Werk ist eine Kooperation zwischen McGill University, Mira und Microsoft Research.</sample>
    <sample id="1390">Natürliche Sprachverstehensmodelle ziehen auf eine Vielzahl von Kenntnisquellen, wie Kenntnisse, die in ihren Parametern enthalten sind, normalerweise durch vorheriges Training erworben, und Kenntnisse, die als Eingabe an der Inferenzzeit gegeben werden.</sample>
    <sample id="1391">Neueste Arbeiten in Aufgabentypen wie der antwortende zu zeigen, dass Modelle verwenden können, die vorher trainiertes Wissen zu lösen.</sample>
    <sample id="1392">Aber das Verstehen von natürlicher Sprache oft erfordert Kenntnisse, die auch zu Inferenzzeiten geliefert werden.</sample>
    <sample id="1393">Beispielsweise im Satz John sah den neu gewählten Präsidenten auf dem Fernsehen.</sample>
    <sample id="1394">Vortrainingparameter können Informationen über, was ein Präsident tut und was ein TV ist, enthalten, aber sie können nicht zuverlässig wissen, wer diese spezifische Instanz "John" ist oder wer der neue Präsident ist, weil der Präsident seit dem Vortraining verändert wurde.</sample>
    <sample id="1395">Daher benötigen erfolgreiche Modelle für knowledge-intensive NLU Aufgaben die Fähigkeit, sowohl vorher trainiertes als auch Inferenzzeit Know-how zu integrieren und zu verwenden.</sample>
    <sample id="1396">In diesem Werk schaffen wir einen Diagnostik-Test für die Kenntnisintegration.</sample>
    <sample id="1397">Wir Introduzieren eine Kohreferenz-Resolution-Aufgabe, die darauf abzielt, die Fähigkeit zu beweisen, dass man auf Wissen in verschiedenen Quellen zurückgreifen kann. Wir evaluieren die Datensammlung mit menschlichen Studiepfaden und etablierten Kohreferenz-Resolution-Modellen.</sample>
    <sample id="1398">Hier ist ein Beispiel aus unserem Datensatz. Tyrvin ist ein Richter. Kia ist ein Bäcker. Tyrvin und Kia haben sich in einem Park getroffen. Nach einem langen Tag im Gericht, der mit der Entscheidung von_cases in einem Gerichtsort verbracht wurde, war er glücklich, sich zu entspannen.</sample>
    <sample id="1399">Das Haupttask hier ist, die richtige Entity zu identifizieren, die der Pronomen "he" bezieht, in diesem Fall "Servis".</sample>
    <sample id="1400">Die Auflösung eines gegebenen Namens erfordert zwei Arten von Informationen: zuerst spezifische Kenntnisse über ein bestimmtes Objekt, wie zum Beispiel, dass "Servell" ein Gericht ist, und zweitens allgemeine Kenntnisse über das Objekt, wie zum Beispiel, dass Gerichte Fälle in Gerichtsverhandlungen entscheiden.</sample>
    <sample id="1401">Allgemein wird Hintergrundwissen während der vorherigen Schulung von großen Sprachmodellen gelernt, während spezifisches Wissen normalerweise während der Inferenzphase übernommen wird.</sample>
    <sample id="1402">Wir variieren die Verfügbarkeit dieser zwei Teile der Informationen so, dass sie entweder in einem einzigen Quelle oder in mehreren Quellen gefunden werden können.</sample>
    <sample id="1403">Wir haben definiert drei Szenarien von KIMMOS. Erstens die Tabelle-Setting-Background-Pretraining, bei dem das Background-Knowledge als voraussichtlich als vor Trainingsschritt zur Verfügung gestellt wird.</sample>
    <sample id="1404">Zweitens, es gibt ein Background-Both-Setting, bei dem die Background-Informationen sowohl vor der Trainingseinzeit als auch während der Inferenzzeit verfügbar sind. Letztenfalls gibt es ein Background-Inferenz-Setting, bei dem beide Kenntnisarten nur während der Inferenzzeit verfügbar sind.</sample>
    <sample id="1405">Das letzte Setting ist besonders interessant, da es simuliert die Situation, in der das-background-knowledge notwendig ist, um eine Aufgabe zu lösen, und das nicht part of der pre-trained Data-Modelle ist. Zum Beispiel, weil neue Berufe seit dem Zeitraum der Pretraining-Phase entstanden sind.</sample>
    <sample id="1406">Hier ist ein Beispiel, wie man die Verfügbarkeit von Effekten in den zwei Quellen kontrolliert.</sample>
    <sample id="1407">In einem vorausgesetzten vortrainierten Setting nehmen wir an, dass das allgemeine Wissen über politische Karrieren und die Sucht nach gewählten Ämtern im Regierungsbereich in den vortrainierten Parametern enthalten ist. In einem bestimmten Kontext geben wir das spezifische Wissen über Che Guevara als Politiker an.</sample>
    <sample id="1408">In der Rückgängigkeit beider Setting weitional wir nicht nur anti-specific, sondern auch background knowledge about politicians in the intervened context.</sample>
    <sample id="1409">In einem unterentwickelten Land, die eigentliche Berufung einer Meritocracy anstelle einer Politiker, weil Meritocracy unwahrscheinlich ist, dass sie in einem traditionellen Paradigma aufrechterhalten wird.</sample>
    <sample id="1410">Wir evaluieren die Datensätze sowohl für menschliche Studie teilnehmer als auch und etablieren koeffizientenresolution models. In diesem Figuren zeigen wir die Resultate von den besten performing Modellen auf dem schwierigsten Varianten der Back-End pre-trained Settings.</sample>
    <sample id="1411">Ohne spezifische Training auf Kidmoss both models do not perform well when trained on Kidmoss however both c2f and build for qff perform significantly better than the random choice.</sample>
    <sample id="1412">Dies zeigt, dass when trained on general question answering datasets models learn to exploit surface cues which are not useful when testing on kidmus where such cues have been removed.</sample>
    <sample id="1413">Weitere Erfahrungen mit fiktionaler Kenntnis indizieren, dass selbst die besten performing Modelle nicht zuverlässig back-end Kenntnis integrieren können, sondern nur in Inferenzphasen.</sample>
    <sample id="1414">Zusammenfassen der Haupttakeaways unseres Papiers: Viele kognitive Flexibilisierungsmodelle scheinen in der Lage, Wissen aus verschiedenen Quellen zu integrieren, ohne taskspezifische Trainingseinrichtungen. Allerdings gelingt es ein paar Modelle mit taskspezifischem Training, Wissen von mehreren Quellen zu integrieren.</sample>
    <sample id="1415">Trotzdem scheinen selbst die besten Performing-Modelle Schwierigkeiten zu haben, reliabel integrierte Hintergrundwissen zu verarbeiten, das nur zur Inferenzzeit präsentiert wird. Wenn Sie mehr Details wissen möchten, bitte our Paper und den Datensatz in Code auf GitHub überprüfen. Vielen Dank für das Hören.</sample>
    <sample id="1416">Die Nachteile der baumbasierten Methoden sind, dass sie oft komplex und computationsintensiv sind. Sie erfordern auch spezielle Vorgehensweisen zur Formalisierung vorheriger Schritte, insbesondere wenn es um die Behandlung von variablen Symbole geht. Darüber hinaus können sie auch spezialisierter Grammatikinduktionsprozeduren erfordern.</sample>
    <sample id="1417">Die Autoren gehören an der University of California, Berkeley.</sample>
    <sample id="1418">Hallo, ich bin Myra und heute werden wir über unser Papier "Markiert Piktogramme" sprechen. Wir verwenden Sprachprompts, um Stereotypien in Sprachmodellen zu messen. Dieses Werk wurde in Zusammenarbeit mit Essam Dermouche und Dan Juravsky erstellt.</sample>
    <sample id="1419">In jüngster Zeit haben viele die Häufigkeit von sozialer Diskriminierung und Stereotypen in großen Sprachmodellen, LLMs, nachgewiesen.</sample>
    <sample id="1420">Allerdings haben diese Maßnahmen diverse Einschränkungen. Sie usual rely on hand-constructed data sets that are very time-consuming to curate.</sample>
    <sample id="1421">Und sie messen auch normalerweise nur sehr spezifische Stereotypien an, was bedeutet, dass sie nicht gut zu anderen Demographen oder Kontexten allgemein werden, oder sie capturieren einfach sehr allgemeine breite Assoziationen, wie negative Assoziationen mit bestimmten Gruppen.</sample>
    <sample id="1422">Darum macht die meisten Arbeiten in diesem Bereich keinerlei Gebrauch von Intersektionalität, die die These besagt, dass multifaschige soziale Identitäten diskriminierende Biases und ein einzigartiges Gesicht des Schadens bilden können.</sample>
    <sample id="1423">Um diese Beschränkungen zu überwinden, verlassen wir uns auf die Tatsache, dass diese jüngeren, instruction-tune-LLMs sehr gut auf Anweisungen und Anregungen reagieren.</sample>
    <sample id="1424">So wir können den Modell nachfragen, einen Piktogramm zu generieren, was ist eine Darstellung von einem imaginären Individuum, indem wir ein Prompt wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich" verwenden.</sample>
    <sample id="1425">Wir können unmittelbar sehen, dass dies sehr allgemein zu jeder Demographie anpassbar ist, weil wir einfach jedes beliebte Identitätsmerkmal, das wir wollen, in diese Anweisung spezifizieren können.</sample>
    <sample id="1426">Also hier sind einige Beispielgenerierungen von GPT-4.</sample>
    <sample id="1427">Sofort sehen wir, dass die Ausgänge nicht offensichtlich negativ oder giftig im traditionellen Sinne dieser Worte sind.</sample>
    <sample id="1428">Es gibt einige interessante Muster.</sample>
    <sample id="1429">Die asiatische Frau wird als unschuldig dargestellt. Die mittlere östliche Frau wird mit Worten wie 'exotisch' und 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweise 'eine faszinierende Region' beziehungsweis</sample>
    <sample id="1430">Und beide der Afroamerikaner-Personen machen Bezug auf Descent, während die weiße Männer-Persona nichts von diesem Typus hat.</sample>
    <sample id="1431">Um diese Muster zu capturieren, unsere Methode hat zwei Teile. Der erste ist die Erstellung dieser Persönlichkeiten.</sample>
    <sample id="1432">Unsere Anregungen zur Generierung dieser Persönlichkeiten wurden von einem Studie inspiriert, bei der sie diese Anregungen menschlichen Subjekten gaben. Sie fanden, dass sie dadurch auch Racialstereotypen surfen konnten.</sample>
    <sample id="1433">Und auch das ermöglicht eine direkte Comparison zwischen unseren generierten Personen und den menschlich编写的回应。</sample>
    <sample id="1434">Die zweite Methode ist die Identifizierung von Marktworten, die Methoden sind, um die Worte zu identifizieren, die Marktecke markierter Gruppen von unmarkierten Gruppen unterscheiden. Ich werde in Kürze elaborieren.</sample>
    <sample id="1435">Die Vorteile von diesem Ansatz bestehen darin, dass wir spezifische Stereotypien und Muster erhalten, ohne uns auf einen bestimmten Lexikon zu verlassen.</sample>
    <sample id="1436">Der Marktwort-Methode wird auf die soziolinguistische Konzeption der Marktgängigkeit gegründet, die besagt, dass es ein unmarkiertes Standard ist und jede Gruppe, die von diesem Standard abweicht, linguistisch markiert ist.</sample>
    <sample id="1437">So, zum Beispiel, der Begriff Mann, oder, Entschuldigung, der Begriff Krieger ist normalerweise mit Männern verbunden. Also wenn Menschen einen Krieger, der ein Frau ist, beschreiben, dann werden sie normalerweise explizit von einem Frau Krieger sprechen und den Begriff mit Frau markieren.</sample>
    <sample id="1438">Und allgemeiner dominante Gruppen in Gesellschaft sind sowohl linguistisch als auch sozial unmarkiert, während marginalisierte Gruppen normalerweise markiert sind.</sample>
    <sample id="1439">In our method we first designate what the unmarked and marked groups are.</sample>
    <sample id="1440">Und dann vergleichen wir die Personen mit dem Fighting Words-Methoden, dasBasically die weighted log odds-Ratios verwendet, um die wichtigen Worte für jede markierte Gruppe zu unterscheiden.</sample>
    <sample id="1441">So, zum Beispiel für die Personen von Afroamerikanerinnen, würden wir kämpferische Worte benutzen und die Odds-Ratios gegenüber sowohl weißen Personen als auch männlichen Personen vergleichen, da jene die zwei entsprechenden unmarkierten Gruppen sind.</sample>
    <sample id="1442">Jetzt zu ein paar Ergebnissen. Also, zuerst wir verwenden einen Lexikon von Stereotypen und finden, dass die generierten Persönlichkeiten einen viel größeren Anteil an Stereotypen enthalten als die von Menschen编写的 ones.</sample>
    <sample id="1443">Allerdings, wenn wir uns nun ansehen die Verteilung der Worte im Lexikon, finden wir sehr verschiedene Dinge.</sample>
    <sample id="1444">So, während die generierten Personen viel höhere Häufigkeiten der Luxemburger Wörter aufweisen, haben die von Menschen geschriebenen Wörter eine viel breiteren Wortverteilung. Die Stereotypenwörter in den generierten Personen sind hauptsächlich Wörter wie "tall" und "athletic".</sample>
    <sample id="1445">So, nur die positiven oder zumindest nicht negativen.</sample>
    <sample id="1446">In der Tat, dieser Slogansprachanalyse werden nicht viele der schädlichen Musterte, die wir in den früheren Slides gesehen haben, abgedeckt. Also anstelle dessen, werden wir uns an die Ergebnisse aus dem Marktwort-Methode halten, um zu zeigen, wie diese positiv klingenden Worte Stereotypien und essentielle Erzählnarrativen fördern.</sample>
    <sample id="1447">In our analysis, we review how these seemingly positive court trials reflect harmful patterns.</sample>
    <sample id="1448">Erst für Markgruppen die Top-Worte können Dinge wie Kultur, Tradition, Stolz und Exotik enthalten. Und diese Worte definieren diese Gruppen nur durch ihre Beziehung zu ihrer Identität und kennzeichnen sie als unterschiedlich von der weißen Norm.</sample>
    <sample id="1449">Dies trägt zu einem langen Erbe von Diskriminierung und Ausgrenzung für diese Gruppen bei.</sample>
    <sample id="1450">Darum sind es viele allgemeine Trocken, die in diesen Wörtern, insbesondere für Afroamerikanerinnen, reflektiert werden. So zum Beispiel die Wörter, die Latinae beschreiben, enthalten Dinge wie 'vibrant' und 'curvaceous'.</sample>
    <sample id="1451">Which connect to a trope of tropicalism for Asian women. The words are things like petite and delicate and silky,</sample>
    <sample id="1452">Das ist verbunden mit einer langen Geschichte von asiatischen Frauen, die hypersexualisiert wurden und als sehr weich und einwillig gesehen wurden und so weiter.</sample>
    <sample id="1453">Und endlich, für Afroamerikanerinnen sehen wir, dass einige der oben genannten Wörter Dinge wie stark und widerstandsfähig sind.</sample>
    <sample id="1454">Dies verbindet sich mit einem Archetyp, das als 'starke afroamerikanische Frau' bezeichnet wurde. Und obwohl es zu Beginn positiv klingt,</sample>
    <sample id="1455">Es wurde gezeigt, dass dieser Artetyp in Wirkung ist, da er eine große Druckspannung auf diese Demographen ausübt, um gegen soziale Hindernisse resilient und stark zu sein.</sample>
    <sample id="1456">So, anstatt sich auf die Arbeit hinzustellen, um diese Hindernisse zu ändern, bringt es Druck auf diese Menschen, sie überwinden zu lassen, was zu sehr negativen Gesundheitsfolgen für diese Menschen among other harms führt.</sample>
    <sample id="1457">Allgemeiner, finden wir, dass die Worte für jede markierte GruppePretty much nur sehr essentielle Erzähler reflektieren.</sample>
    <sample id="1458">So basierend auf diesen Mustern, folgern wir drei Empfehlungen für Modellbesitzer.</sample>
    <sample id="1459">Zunächst sollten wir als Forscher positiver Stereotypien und essentialisierender Narrativen abdecken. Wir sollten auch die intersektionalen Linsen verwenden, um Biases und Schäden zu untersuchen, denn es gibt viele Dinge, die übersehen werden könnten, wenn wir das nicht tun.</sample>
    <sample id="1460">Schließlich sollte es eine gesteigerte Transparenz über die Methoden zur Bekämpfung von Bias geben.</sample>
    <sample id="1461">Weil zum Beispiel, wie diese positiven Stereotype, wir nicht wissen, ob es because there is some sort of like weird</sample>
    <sample id="1462">Übermäßig exzessiver Wertealltag, der stattfindet, oder maybe some other like anti-stereotyping-Methoden, die zu diesen prunkhaften Mustern führen.</sample>
    <sample id="1463">Wir können einfach nicht irgendeine Annahmen machen oder das weitere Studium darüber fortsetzen, ohne mehr Transparenz zu haben.</sample>
    <sample id="1464">Danke so viel für das Hören. Hab ein gutes Leben.</sample>
    <sample id="1465">Hallo, alle. Mein Name ist Jing Wei Yi, ich bin aus dem Pekinger Technologisch-Universität in China.</sample>
    <sample id="1466">Es ist mir eine Freude, einen kurzen Reklamevideo zu präsentieren über Papier. Are you copying my model? Protecting the copyright of large language models for embedding and services with backdoor watermark.</sample>
    <sample id="1467">Lassen Sie uns zuerst die Hintergründe über embedding services Introduce.</sample>
    <sample id="1468">Derzeit sind große Sprachmodelle wie GPT, LLaMA und PaLM Ausnahmefälle in der natürlichen Spracheverstehens- und -generationsfähigkeit.</sample>
    <sample id="1469">Embedding as services is one of the services built up upon large language models to assist various NLP tasks.</sample>
    <sample id="1470">Beispielsweise bietet OpenAI ein GPT-basiertes Bidding API.</sample>
    <sample id="1471">Allerdings haben recente Studien gezeigt, dass der Angreifer den Modell durch Lernen von dem Embetting und die Bereitstellung ähnlicher Dienstleistungen stehlen kann. Daher ist es notwendig, die Urheberrechte von Embettung als Dienstleistungen zu schützen.</sample>
    <sample id="1472">Um die Copryright-Unterricht der Einbettung von Services zu schützen, ist eine der Lösungen, einen Watermark in den bereitgestellten Services zu übertragen und zu überprüfen, ob ein anderes Service das Watermark enthält.</sample>
    <sample id="1473">Das Wasserzeichen-Verfahren muss die folgenden Eigenschaften erfüllen: Erstens sollte das Verfahren auf embedding services angewendbar sein. Zweitens sollte das Wasserzeichen nicht die Nutzen der bereitgestellten embeddings beeinträchtigen.</sample>
    <sample id="1474">Drittes, die Wassermark muss für den Angreifer ausreichend verdeckt sein, oder der Angreifer kann das Wassermark leicht entfernen.</sample>
    <sample id="1475">Endlich muss die Wassertonne zu den Angriffsgeräten übertragen werden, während der Modulextraktionssensor in Acht genommen wird.</sample>
    <sample id="1476">Bestehende Werke können allgemein in vier Kategorien einteilen werden.</sample>
    <sample id="1477">Jedoch sind diese Methoden entweder nicht auf die Einbettung als Dienstleistungen anwendbar oder sie fehlt an Transfersibilität.</sample>
    <sample id="1478">Daher in diesem Papier wirft ein embedding Marker, der ein backdoor-basiertes Watermark-Verfahren ist, anwendbar auf embedding-IT-Dienstleistungen.</sample>
    <sample id="1479">Dann lasse mich Introduzieren die Details von unserem Embossing Marker. Embossing Marker enthält zwei Hauptstadien: Watermark injection und Copyright verification.</sample>
    <sample id="1480">Bevor wir diese Schritte ausführen, müssen wir zuerst einen Trigger-Set auswählen. Das Trigger-Set ist eine Gruppe von Wörtern in einem moderaten Frequenzintervall.</sample>
    <sample id="1481">Wir nehmen an, dass der Anbieter eine allgemeine Textcorpus und die Wortfrequenz damit berechnen kann.</sample>
    <sample id="1482">In Watermark injection wir erst definieren ein Target embedding. Wenn ein Benutzer eine Aussage an den Provider-Service sendet, dann sucht der Provider die Triggerzahl in der Aussage.</sample>
    <sample id="1483">Die bereitgestellte Einbettung ist ein Gewichtsumsatz der Target-Einbettung und der ursprünglichen Einbettung.</sample>
    <sample id="1484">Die Lade容量 der Zielschubrampe ist proportional zur Anzahl der Auslöser im Satz. Wenn die Anzahl der Auslöser im Satz größer als M ist, ist die bereitgestellte Schubrampe exakt gleich der Zielschubrampe.</sample>
    <sample id="1485">Kopfright-Überprüfung ist, um festzustellen, ob ein Modell hinter einer anderen Service enthält die Watermark.</sample>
    <sample id="1486">Zunächst konstruieren wir einen Backdoor und einen benignen Datensatz. Der Backdoor-Datensatz enthält Sätze, von denen alle Wörter zu der Triggermenge gehören. Der Datensatz des Benigns enthält Sätze, bei denen alle Wörter nicht zu der Triggermenge gehören.</sample>
    <sample id="1487">Dann der Provider requirierte embeddings vom Steiler-Service mit dem Datensatz.</sample>
    <sample id="1488">Die KOS und L2 Ähnlichkeit zwischen dem geforderten Embedding und dem Ziels embedding werden berechnet. Wir berechnen die Ähnlichkeitsdifferenz zwischen den trainierten und validierenden Datensätzen, die definiert ist als Delta KOS und Delta L2.</sample>
    <sample id="1489">Meanwhile, we also apply the KS test and use its p-value as the third metric.</sample>
    <sample id="1490">Wir conduct experiments on four datasets agnews, mind, sst2 und hraspam. Wir nehmen an, dass der Anbieter die wikitext-dataset zur Kontrolle verwendet.</sample>
    <sample id="1491">Die Resultate auf vier Datensätzen zeigen, dass unser embeddings-Muster eine gute Detektionsfähigkeit aufrechterhält, während es gleichzeitig eine gute Nutzenstelle für die Suchmaschinen Aufgaben ist.</sample>
    <sample id="1492">Wir validieren auch die korrekte Umsetzung der bereitgestellten Embedding-Modelle, indem wir die Anzahl der Trigger in jeder Sache visualisieren. Die Legende der Figuren zeigt die Anzahl der Trigger in jeder Sache.</sample>
    <sample id="1493">Wie im Bild zu sehen ist, ist es schwierig, zwischen den backdoor- und normalen Einbettagungen zu unterscheiden.</sample>
    <sample id="1494">Das war's, danke. Wir freuen uns auf einen Dialog mit Ihnen.</sample>
    <sample id="1495">ABC-Eval steht für "annotating behaviors in chat" (in kürzerer Form). Es ist eine Methode, die dazu dient, chat-Modell-Behaviors zu überprüfen und zu bewerten, die in jüngster Literatur als Faktoren für die Qualität von Chats diskutiert wurden.</sample>
    <sample id="1496">Das Leistungsabkommen zwischen CoNLL-2003 und CoNLL++ liegt über 5 Punkte bis zum Jahr 2023.</sample>
    <sample id="1497">Hallo, mein Name ist Vasudeva und ich bin ein Computerwissenschaftspg-Kandidat an Stony Brook University. Ich möchte gerne mein Werk, das in ACL 2023 als Long Paper akzeptiert wurde, präsentieren: "Transfer Learning for Anomaly Detection: Addressing the Rare Class Challenge".</sample>
    <sample id="1498">Beginnen wir, kognitive Dissonanz zu definieren und warum es ein wichtiger Problem ist, sie in der Sprache zu studieren. Einfach gesagt ist kognitive Dissonanz zwei Glaubens oder Handelungen, die inkonsistent sind.</sample>
    <sample id="1499">So wie in diesem Beispiel, wo ein Mensch sagt: 'Ich weiß, dass Zigaretten mich umbringen könnten', und dann fortgesetzt sagt: 'Ich habe ein paar Zigaretten nach dem Treffen geraucht'. Dieses Glauben und Handeln sind inkonsistent und sie sind in Widerspruch.</sample>
    <sample id="1500">Weiterhin erwähnen, dass ich glaube, ich könnte mein Job nicht ohne sie halten, rechtfertigt die zweite Begegnung und sie haben eine enge Beziehung.</sample>
    <sample id="1501">While dissonance is a very common phenomenon we experience in daily decision making, they are really rare to find expressed in language among other kinds of risk relations.</sample>
    <sample id="1502">Warum ist das wichtig? Studieren kognitive Divergenz kann uns dabei helfen, die Auswirkungen von Diskrepanzen zwischen Menschen zu verstehen, Trends und Glaubens-, Werte- und Einstellschwankungen in Bevölkerungen zu identifizieren.</sample>
    <sample id="1503">Eine hohe kognitive Dissonanz ist auch mit Angststörungen verbunden und kann das Verständnis für Menschen mit mentaler Gesundheitsproblemen besser machen.</sample>
    <sample id="1504">Die Studie von Diskriminierungsexpressiven Sprachen kann auch nützlich sein, um das Extremismus- und Polarizationsspektrum von schwacher Gruppe zu verstehen.</sample>
    <sample id="1505">Schließlich ist kognitive Distanz wichtig, um die individuellen kognitiven Stile zu verstehen und uns dabei beim Verstehen von Entscheidungsprozessen zu unterstützen.</sample>
    <sample id="1506">Um ein kognitives Diskursressourcenobjekt zu schaffen, haben wir eine große Skala an Diskursrelationen ermittelt. Wir haben die Diskursmethode verwendet, wie sie im Pfeilendiagramm hier zu sehen ist.</sample>
    <sample id="1507">Tweets wurden parsed mit einem PATTYB parser und pares der discourse-Units wurden annotiert, nach den Richtlinien, die in unserem Papier beschrieben sind.</sample>
    <sample id="1508">Wie man hier sehen kann, wurden Verdammungen nur in 3,5 % der anerkannten Paaren gefunden.</sample>
    <sample id="1509">Während wir über tausend Beispiele von Diskursunit-Paaren verfügten, trainierten wir einen initialen Klassifizierer nur auf 43 Beispielen von Disneps. Wie erwartet, erbrachte der Klassifizier nicht viel besser als Zufall.</sample>
    <sample id="1510">Angesichts der geringen Häufigkeit von Diskussion und Abwesenheit eines vorherigen Datensatzes, das wir gegenüber stehen, haben wir das Problem der absoluten Seltenheit.</sample>
    <sample id="1511">Um dies zu beheben, experimentieren wir über Kombinationen von transaktionsbasiertem Lernen und aktiver Lernstrategie, um solche zu identifizieren, dass mehr dissonante Ausprägungen über weniger Annotierungen sammelt werden können. Dadurch wird die Gesamtannonierkosten reduziert, während die Dissoziationsdetektion verbessert wird.</sample>
    <sample id="1512">Da das ursprüngliche Modell nicht in der Lage war, die Distanzklasse zu capturieren, starten wir den Prozess der aktiven Lernprozesse, indem wir Gewichtungen von naheliegenden Aufgaben übertragen.</sample>
    <sample id="1513">We transform into two different tasks topic independent disentanglement classification, a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic.</sample>
    <sample id="1514">Called debate here and on binary classification of expansion and compression classes of pittibee since these two are closely related to the conception of consonance and dissonance and we call them c e here.</sample>
    <sample id="1515">Wir finden, dass die Null-Shot-Performanz auf der antrainierten Datensammlung schon viel besser als zufällig ist, mit dem besten AC 0,62.</sample>
    <sample id="1516">Weiterhin beim iterativen Fine-Tuning an beiden Aufgaben finden wir, dass das Fine-Tuning von C-E-Aufgaben gefolgt von einem weiteren Fine-Tuning auf Debate ein viel besseres Null-Shot-Performance liefert. Daher ist dies das Modell, das wir verwenden, um Coreset Learning zu initiieren.</sample>
    <sample id="1517">Nächster, wir bestimmen den besten Weg, um ein Modell mit neuen Daten von jeder Runde des aktiven Lernens und der Annotationen zu aktualisieren. Kummulative sammelt alle von nun an gesammelten Daten von den aktuellen Annotationen, während迭代通过在最新收集的数据集上训练模型来更新模型。</sample>
    <sample id="1518">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1519">Nächster Punkt: Um die Anzahl der Dissonanzbeispiele zu verbessern, verwenden wir eine Wahrscheinlichkeitsstrategie (PRC) um die meisten Beispiele zu selecting, die von unserem aktiven Modell in jeder Runde am wahrscheinlichsten dissonant sind.</sample>
    <sample id="1520">We compared this to the other state-of-the-art, state-of-the-art strategies that are commonly used in the community.</sample>
    <sample id="1521">Wir finden, dass die vorgeschlagene PRC-Strategie besser arbeitet als andere standard-standards Strategien, obwohl die Differenz klein ist. Beachte, dass die Performance signifikant tiefer für Random</sample>
    <sample id="1522">In den weiteren Runden von AIL mit zwei besten Strategien verbessern wir die Distanzclassification AUC auf 0.75, was die beste Performance auf der Aufgabe so far ist.</sample>
    <sample id="1523">Wir haben auch die Durchführbarkeit jedes Strategie für die Annotationqualität und die Kosten für die Annotatoren überprüft. Wir finden, dass PRC die höchste Quote an Disziplin hat und am besten für real-world-Klassen geeignet ist. Allerdings finden die Annotatoren die Beispiele schwierig.</sample>
    <sample id="1524">In summary, we find that PRC is a simple AL strategy for rare class acquisition and cold-starting AL with appropriately designed transfer learning tasks and help significantly.</sample>
    <sample id="1525">Wir finden auch, dass iterative Update ist nützlich für Transferlearning von einem anderen Bereich, whereas in-domain active annotations profitieren von kumulative Updates.</sample>
    <sample id="1526">Dies sind die Links zu unserem Code-Dataset und zu unserem Papier. Ich bitte Sie, uns falls Sie Fragen haben, zu kontaktieren. Danke.</sample>
    <sample id="1527">Die englische Textstelle gibt nicht Informationen über die Universitäten, an denen die Autoren arbeiten.</sample>
    <sample id="1528">The referent is Si Yu Yan.</sample>
    <sample id="1529">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="1530">Der Ansatz wird mit der SimulST-Architektur speziell für Simultansprechen verglichen.</sample>
  </task>
</testset>