<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Large-scale web crawls and political news media</sample>
    <sample id="1">The affiliations of the authors of the paper are not mentioned in the given information.</sample>
    <sample id="2">The paper presents a new multi-modal pre-training model called LayoutMask for document understanding. It aims to address the reading order issues in existing document pre-training models by using text and layout information as input and inferring global reading orders through joint learning of 1D and 2D positional encodings and thematic information. The model employs two novel masking strategies: whole word masking and layout-aware masking, which are designed to promote text layout interactions and enhance the model's ability to learn better layout representations. The experiments demonstrate that LayoutMask outperforms other models on both FD and SRE datasets, with similar performance on CFD.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">T5-XL</sample>
    <sample id="6">The paper presents a method for unifying multilingual and cross-lingual summarization into a more general setting called many-to-many summarization. The method involves training a single summarization model to process a document in any source language and generate a summary in any target language. The authors conduct preliminary studies to provide deeper analysis among multilingual summarization, cross-lingual summarization, and their many-to-many summarization. They find that many-to-many summarization can help the summarization model better transfer task knowledge across different languages than parallel multilingual summarization and cross-lingual summarization. The authors propose a three-stage pre-training method for training many-to-many summarization models, which includes unsupervised pre-training, cross-lingual pre-training, and task-specific pre-training. The results of the preliminary experiment show that the multilingual model trained in the many-to-many summarization setting can better transfer task knowledge across different languages in the settings of multilingual summarization, cross-lingual summarization, and unified cross-lingual summarization.</sample>
    <sample id="7">Yes, they do.</sample>
    <sample id="8">The novelty of the proposed human evaluation method is that it explicitly annotates whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="9">Clean validation samples</sample>
    <sample id="10">There is a lot of room for improvement.</sample>
    <sample id="11">The New Yorker Caption Contest is a popular weekly cartoon captioning competition that has been running for over 50 years. Researchers from AI2 and other institutions have operationalized the contest data into three tasks: matching, quality ranking, and explanation generation. The matching task involves presenting models with five choices of captions, only one of which was truly written about a given cartoon. Quality ranking considers two captions that were both written about the cartoon but one was judged by human raters to be much higher quality. Finally, the explanation generation task prompts the language model to generate a two-to-four sentence explanation of why the joke is funny. The researchers found that their best model, CLIP, fine-tuned on the annotated corpus, achieved around 62% accuracy on the matching task, relative to a 20% random guessing baseline. However, humans get around 94% on the same task, representing a big gap in humor understanding. Models like GPT-4 also performed poorly on the tasks, even when conditioned to do the exact same tasks but given a human-authored description of the image. Overall, the researchers are excited to see what people do with their dataset and have made a leaderboard and models available at this URL.</sample>
    <sample id="12">Five</sample>
    <sample id="13">Daniel Rotem presents his work on adaptive inference in low resource settings, conducted in Professor Roy Schwartz's lab at Hebrew University in Jerusalem. He explains that adaptive inference reduces the inference time of large language models by using low-capacity models for easy samples. The two main methods are multimodal and early exit. Multimodal involves storing multiple models, each with a classifier, trained separately and run sequentially during inference. Early exit fits multiple classifiers after intermediate transformer layers, stopping computation when a classifier halts. Rotem discusses the pros and cons of each method, highlighting issues like overhead and conflicting gradients in early exit. He introduces the Sweet method, which separates weights in early exit transformers to avoid conflicting gradients, showing improved performance over early exit and multimodal methods. The results suggest that Sweet is more efficient in speed-accuracy trade-offs, especially for BertLarge across the entire curve.</sample>
    <sample id="15">Three</sample>
    <sample id="16">Document level simplifications</sample>
    <sample id="17">The video discusses a proposed method for multi-modal relation extraction, which involves representing text and images with visual and textual sign graphs, respectively. These graphs are then merged into a unified cross-modal graph (CMG). The initial CMG structures are screened by filtering nodes and adjusting edges to ensure correct adjustments. Graph information bottleneck is leveraged to guide optimization. The compressed CMG features are enriched with multi-modal topic features using an attention operation. The effectiveness of the proposed method is evaluated on the AMR dataset, showing that leveraging visual features can obtain higher performances compared to text-based methods. The study also finds that internal information screening and external information exploiting both contribute to task performance, with internal screening being more important for high cross-modal relevance inputs and external exploiting being more useful for low relevance inputs.</sample>
    <sample id="18">Salt and pepper and not pepper and salt</sample>
    <sample id="19">The presentation discusses the methods of updating a model with new data in active learning and annotation. It compares two approaches: cumulative, which accumulates all data collected so far, and iterative, which updates the model by training on the latest set of data collected. The presentation also covers parameter sharing and designing for different models, such as using one model to achieve both retrieval and reading. Additionally, it compares existing open-domain question answering models based on data aspects, showing that retrieval and reader systems perform well-balanced between memory and performance. Retrieval-only systems create large indexes but answer queries quickly, while generative-only systems create no index but are always large models and achieve low performance. Based on this analysis, the presentation concludes that if resources are limited, reducing index size or embedding compression can be considered. If real-time feedback is pursued, retrieval-only systems are good choices, while for trade-offs, retrieval and reader systems are more appropriate. Two future works are discussed: deploying open-domain question answering systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, the models are freely available on the Hugging Face and all the training scripts are on our GitHub repository.</sample>
    <sample id="21">News texts</sample>
    <sample id="22">The factors that lead to good generalization are the model architecture, the model size and more fine-tuning examples.</sample>
    <sample id="23">Text-image modeling research has made significant progress in recent years, enabling the generation of high-quality images. However, these models often struggle with representing text accurately. The Imagen model, for instance, fails to render even simple textual inputs correctly. This issue arises from the text encoder's inability to decompose subword tokens into individual letters, leading to poor spelling accuracy. While larger models like PaLM achieve near-perfect spelling, they are impractical due to their size and data requirements. BitT5, which receives individual bits of input, performs well but lacks granularity. To address this, the authors propose augmenting the Imagen model with an additional text representation from BitT5. This strategy improves the model's ability to spell words correctly, enhancing image generation characteristics and text rendering capabilities.</sample>
    <sample id="24">The tendency for left conjuncts to be shorter was measured by measuring length in characters, the first column in syllables, the middle column and in words, the right column.</sample>
    <sample id="25">The experiments measured the length of the crucial dependencies in words, and observed that the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words when the governor is on the left.</sample>
    <sample id="26">A baseline classifier trained on imbalanced data performs not much better than chance.</sample>
    <sample id="27">Five</sample>
    <sample id="28">Bob and Alice</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones on discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The paper introduces a framework called LLM-Blender, which is designed for ensemble learning of large language models (LLMs). The key idea behind LLM-Blender is to use pairwise ranking and generative fusion. The framework consists of two stages: first, it ranks the outputs from multiple LLMs using a pairwise ranking model called PairRanker; second, it uses a generative fusion model to combine the top-ranked candidates into a final output. The PairRanker encodes pairs of candidates along with the input to better analyze the subtle differences between them, unlike traditional ranking models that evaluate each candidate individually. Experiments on a dataset named MixInstruct show that LLM-Blender outperforms individual LLMs and achieves superior performance in terms of various metrics. The framework is simple, straightforward, and can significantly improve the performance of LLM ensembles.</sample>
    <sample id="31">Johns Hopkins University, Purdue University, and MIT</sample>
    <sample id="32">hi my name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi set tagging and latent permutations this is joint work with my advisers Alexander Koller and Ivan Titov compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training in the context of semantic parsing testing for compositional generalization might look like this as usual we have a training set of utterances in this case the girl slept and Mary knew that the girl slept these utterances are paired with logical forms that represent core aspects of their meaning in contrast to standard machine learning evaluation the test set does not come from the same distribution but contains structurally unseen logical forms in this example the model has seen shallower recursion during training and is tested on an example with deeper recursion naive sequence to sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input in particular they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example a popular method to address this is to integrate trees into the models the trees are intended to capture the compositional process that relates utterances with the logical forms this works well but trees are usually not given and need to be obtained somehow this can be complicated and sometimes a computationally expensive process typically this involves considerable formalism specific pre-processing of the logical forms for example to handle variable symbols obtaining trees may also involve specialized grammar induction procedures in this paper we don't use trees and introduce a neural sequence to sequence model that directly models the correspondences between fragments of the input and fragments of the output for the first time we show strong generalization to deeper recursion without relying on trees our approach predicts the output from the input in two steps first we tag each input token with an unordered multi set of tokens that will appear in the output after the first step we have all the right tokens but they're not ordered that's why in the second step we use another model to predict a permutation to put them into the right order we introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations this makes our approach quite flexible and expressive conceptually our permutation model works roughly like this we go from left to right over the output and determine which multi set token to put in every position for the first output position we simply select one as highlighted in red then we jump to the next multi set token to determine the second token in the output we determine the third token in the output in a similar way by jumping to another multi set token we continue this process until every token from the first stage has been visited exactly once to give you a teaser of the experimental results here we compare our method with other treeless models on the CoG benchmark our model outperforms the others by a large margin on generalization to deeper recursion some other kinds of structural generalization remain very challenging though in our paper we solve a couple of interesting technical challenges first of all the alignment between input and output is not given in the training data as a consequence for a given token we don't know which multi set it came from which poses a challenge for training in addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent we address this by inducing the alignment as part of the training our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is NP-hard that's because this is related to the traveling salesman problem we approximate this with a GPU friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations if you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to a poster</sample>
    <sample id="33">The introduced framework quantifies the positionality by comparing the annotations with real users with existing datasets and models using Pearson's correlation score.</sample>
    <sample id="34">The research presented in this paper focuses on the development of a framework called CREST, which combines selective rationalization and counterfactual text generation to improve the interpretability and reliability of machine learning models. The framework consists of two main components: a rationalizer that generates meaningful rationales for model decisions, and a counterfactual generator that creates alternative inputs based on these rationales. The counterfactuals are then used to augment training data, enhancing the model's ability to generalize across different datasets. The study evaluates the quality and validity of generated counterfactuals using both automatic metrics and human evaluation, demonstrating that CREST produces more natural and valid counterfactuals compared to other methods. Additionally, the paper explores the use of counterfactuals for data augmentation and shows that it can significantly improve model performance on various datasets. Overall, CREST offers a robust approach to improving the transparency and effectiveness of machine learning systems by leveraging counterfactual examples during training.</sample>
    <sample id="35">hello I am Dawei a PhD student at Sarland University in Germany in this video I would like to present our recent work weaker than you think or critical look at weakly supervised learning this is joint work with Xiaoyu Shen Marius Mosbach and G Stefan and Dietrich Klawo I'd like to begin with a brief introduction to weak supervision and weakly supervised learning in weak supervision you do not manually label the data instead we label the data using weak labeling sources such as simple heuristics rules knowledge bases or low quality crowdsourcing as you illustrated in the figure on the right when compared to human annotations the weak annotations are much cheaper yet they are also noisy meaning that a certain amount of the annotations are incorrect if we directly train neural networks on weakly labeled data the neural networks tend to memorize the label noise and do not generalize in weakly supervised learning training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well in recent works in WSL so WSL stands for weakly supervised learning a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets technically this claim is not wrong but there's a catch which is that people do assume that there is an additional clean validation set available for model selection we cut stopped on this problem setting as this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked the aforementioned approach is us to ask three research questions first is clean validation data necessary for WSL or can we maybe use a noisy validation set instead second if clean data is required or if clean data is mandatory for WSL to work then how many clean samples do we need finally should we only use the clean samples for validation or there are better ways to utilize them we addressed these research questions in our work and our findings are as follows first we find that interestingly recent WSL methods indeed require clean validation samples to work properly otherwise there is a large performance drop as shown in this figure if there are no clean validation samples then the trained models cannot generalize beyond the original weak labels meaning that the training is pointless this indicates that WSL approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance as shown in the figure on the left typically we only need 20 samples per class to attain high performance but that's not the end of the story because if we either way decide to access clean samples then training on them directly will even achieve better performance the right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and WSL approaches which use the clean data for validation only as we can see if we have 10 samples per class direct fine-tuning starts to beat WSL approaches finally the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples as we can see from the figures the vanilla model termed ftw initially underperforms more complicated WSL methods like cosine however if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other methods so in practice there's no reason to choose more complex WSL methods which require more computation time and disk space to summarize we showed that recent WSL approaches require clean manually annotated samples for them to work properly their performance gain and practicality are heavily overestimated our concrete recommendations for future work are as follows first report the model selection criteria for example report if the model selection is done well clean validation samples second WSL approaches should be compared with few-shot learning baselines as opposed work on clean samples third continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL finally we have open-sourced our code you can find it via the QR code on this slide please feel free to check it out thank you and enjoy the conference</sample>
    <sample id="36">The paper presents a method for enhancing multilingual machine translation by introducing language-specific layers (LSLs) in a transformer model. The approach involves using one regular transformer layer per language, which is used to select the correct sub-layer at training and inference time. This allows for increased capacity per language while maintaining constant inference costs. The placement of LSLs is optimized through a process involving shared, source, and target weights, with the best placement determined by selecting the component based on the largest weight. Experiments on WMT 21 translation tasks show significant improvements over baseline models and language adapters, particularly for low-resource languages. The method achieves these gains without increasing the model size or inference time, making it suitable for practical applications.</sample>
    <sample id="37">The study found that by giving the same prompts to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="38">The study used statistics from the enhanced version of the Penn Treebank.</sample>
    <sample id="39">One</sample>
    <sample id="40">Some closely related tasks for cognitive dissonance are topic-independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="41">The presentation introduces PEACoK, a Personal Common Sense Knowledge graph developed by the Natural Language Processing Lab at EPFL University in collaboration with Sony Group Corporation. PEACoK aims to represent real-world person knowledge at scale, containing about 3.8 thousand persons and 40 thousand distinctive attributes, forming approximately 100 thousand person inferences or facts. The graph includes connections between about 9.2 thousand attributes of two or more persons, enhancing the interconnections within the dataset. Researchers frame the relations of persons and their attributes in three dimensions: four types of man relations, as well as interactivity and distinctiveness. They build PEACoK in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes from both common sense knowledge graphs and large-scale pre-trained language models, and cross-checking annotations through a joint human-AI majority voting scheme. Studies show that this method yields high-quality relation annotations with an average accuracy of 87% and F1 score. PEACoK is used to train a BART-based common knowledge generator on a personal attribute inference task, achieving better results compared to large-scale pre-trained language models like GPT-3 and GPT-3.5. Additionally, PEACoK is explored for improving downstream narrative modeling, specifically in dialogue generation tasks, where it enhances fluency, consistency, engagement, and personal expression.</sample>
    <sample id="42">2</sample>
    <sample id="43">7</sample>
    <sample id="44">The introduced framework differs from the previous works by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas contain a lot more stereotypes than the human-written ones.</sample>
    <sample id="46">Deep Belief and Google Translate</sample>
    <sample id="48">Five authors are involved in the paper.</sample>
    <sample id="49">MPP evaluations were performed up to 124 tokens context length.</sample>
    <sample id="50">The presentation introduces the DePlain corpus, a new resource for German text simplification at both the document and sentence levels. It highlights the challenges with existing corpora, such as small size and automatic alignment errors. The DePlain corpus is divided into two sub-corpora: DePlain API, based on news texts with 30,000-13,000 parallel sentence pairs, and DePlain Web, which includes various domains with 30,450 sentence pairs. The corpus showcases diverse simplification techniques like lexical substitution, clause deletion, reordering, and insertion of words. The presentation also discusses use cases for the DePlain corpus, including evaluating automatic alignment methods and fine-tuning language models for text simplification. The best method for German text simplification alignment is found to be MassAlign. The presentation concludes with an invitation to explore the corpus and its applications further.</sample>
    <sample id="51">Music, books, and recipes</sample>
    <sample id="52">Positionality is simply the perspectives that people hold as a result of their demographics, identity and life experiences.</sample>
    <sample id="53">DAWEI ZHU</sample>
    <sample id="54">This presentation by Vasudha Varadarajan, a computer science PhD candidate at Stony Brook University, focuses on addressing the rare-class challenge in cognitive dissonance detection through transfer and active learning. The speaker defines cognitive dissonance as inconsistency between beliefs or actions, providing examples such as someone acknowledging the dangers of smoking but still smoking after a meeting. She explains that studying dissonance can help understand disagreement, track changes in beliefs and attitudes, and is linked to anxiety disorders. The presentation details a large-scale annotation of discourse units to identify dissonance relations, using a flowchart approach with a dependency parser. Despite the low occurrence of dissonance (3.5% of annotated pairs), the initial classifier performed poorly due to the scarcity of data. To improve detection, the presenter employed transfer and active learning strategies, including fine-tuning from related tasks like debate statement classification and C-E classification. The proposed probability of rare class strategy (PRC) was found to be effective, achieving an AUC of 0.75, which is the best performance so far. The presentation concludes with a discussion on the feasibility of each strategy for annotation quality and costs, highlighting PRC's effectiveness in rare-class acquisition and cold-starting active learning.</sample>
    <sample id="55">Yes, it does.</sample>
    <sample id="56">The paper involves three authors.</sample>
    <sample id="57">Yes, the tested model works on the test suite.</sample>
    <sample id="58">The three variants of KITMUS are: 1) the background pretrain setting, where background knowledge is assumed to be available at pretraining time; 2) the background both setting, where background knowledge is available both at pretraining time and inference time; and 3) the background inference setting, where both knowledge types are available only at inference time.</sample>
    <sample id="59">The presentation introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. It highlights the importance of language modeling in healthcare and presents DrBERT as the first biomedical model in French, trained on NACHOS, a dataset of medical crawled data from the web. The presentation compares DrBERT with other models, including those based on Camembert and English biomedical models, and evaluates their performance on various tasks such as named entity recognition, classification, part-of-speech tagging, and question answering. The results show that DrBERT outperforms other models on most tasks, especially when using more data. The presentation concludes by emphasizing the availability of the pre-trained model and training scripts, and encourages further discussion at the post-thesis session in Toronto.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">Should we only use the clean samples for validation or are there better ways to utilize them?</sample>
    <sample id="62">The paper presents a systematic study of knowledge distillation for natural language generation (NLG) using pseudo-target training. The main goal is to compress large language models while preserving their performance. The study explores different architectural decisions, pruning impacts, and knowledge distillation approaches, including word-level and sequence-level distillation. It focuses on task-specific NLG tasks like summarization, question generation, common sense reasoning, and simplification, using unlabeled data and medium-sized teacher models. The study highlights the importance of unlabeled data in boosting distillation, the benefits of generating multiple pseudo-targets, and introduces a novel joint teaching technique to address student exposure bias and improve learning. The research aims to provide practical and efficient methods for compressing NLG models in industry-driven setups.</sample>
    <sample id="63">The metric sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">Greater sensitivity indicates improved model performance.</sample>
    <sample id="66">The paper discusses the development of deep learning methods for mathematical reasoning, which is a fundamental aspect of human intelligence. It covers two primary categories: visual contexts and tabular contexts. The paper highlights the importance of automatic theorem proving and the use of neural network architectures such as sequence-to-sequence models and six-to-tree models for mathematical reasoning tasks. It also mentions the challenges faced by large language models (LLMs) in performing precise mathematical reasoning and proposes solutions like self-consistency and program-ended LLMs to improve performance. The paper concludes by emphasizing the need for more research on low-resource languages and domains, as well as the generalization and robustness failures of deep learning models in mathematical reasoning tasks.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models and proposes methods to mitigate it. It identifies severe interference occurring when the model is small compared to the data size, and tuning the sampling temperature as key for strong performance. The bilingual case has model and data size scaling laws to predict loss, but the multilingual case is more complex due to factors like language similarity and total number of languages. The paper finds that language similarity and the number of languages do not significantly impact interference levels. It also shows that severe interference occurs only for small models and goes away with an increase in scale. The simplest solution is temperature sampling, which allows sampling more training examples from lower resource languages. Tuning temperature is key for strong performance. The paper concludes that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">The models receive a variety of linguistic contexts during pretraining, including grammatical structures, sentence semantics, and context-specific information. This diverse exposure helps the models develop a robust understanding of language, enabling them to generalize well across different contexts and tasks.</sample>
    <sample id="69">A simple yet strong baseline that should be considered in future work in WSL.</sample>
    <sample id="70">Stanford Engineering Computer Science</sample>
    <sample id="71">The paper investigates the problem of generalization using the named entity recognition task (NER) and develops a dataset called CNN+2023. The dataset is collected from Reuters news from 2020 and annotated with the same CNN-2013 annotation guidelines. The authors fine-tune over 20 models on CNN-2013 and evaluate them on both CNN-2013 and CNN+2023 test sets, calculating the percentage change in F1 to assess generalization. Through experiments, three main ingredients for good generalization are identified: model architecture, model size, and number of fine-tuning examples. The first hypothesis for performance drop is adaptive overfitting, which is overfitting caused by reusing the same test set over and over again. The second hypothesis is temporal drift, which is the performance degradation caused by the increasing temporal gap between the train and test data.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because existing methods are often based on human judgment and may not be objective or consistent. New methods could provide more accurate and reliable measures of media biases, which would be useful for researchers and policymakers who are interested in understanding the role of media in shaping public opinion and policy.</sample>
    <sample id="73">The speaker's name is Makkat.</sample>
    <sample id="74">The paper introduces Dense-Atomic, a method for constructing a dense knowledge graph by completing missing links in Atomic, a large-scale common sense knowledge base. The construction process involves three main steps: normalizing tail events, training a relation prediction model, and constructing Dense-Atomic. The normalization step converts tail events into the same format as head events using four methods: subject removal, third-person singular form conjugation, subject reverb, and relation grouping. The relation prediction model uses a combination of the star token representation and max pooling to predict relations between head and tail events. This approach leverages semantic information from both head and tail events, avoiding the sparsity problem and utilizing semantic information effectively. The evaluation results show that Dense-Atomic outperforms state-of-the-art relation prediction methods on both automatic and human evaluations, demonstrating its potential for enhancing common sense reasoning.</sample>
    <sample id="75">The presentation introduces a joint semi-supervised learning framework for entity and relation extraction tasks. The framework aims to integrate information from both labeled and unlabeled data by propagating labels over heterogeneous graphs. It consists of four main parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. In the span feature generation phase, contextualized representations are initialized, and a trained classifier is used to generate representations for unlabeled spans and span pairs. The heterogeneous graph construction phase involves building a k-nearest neighbor graph for efficiency and examining similarity relations among unlabeled and labeled data. Joint label propagation refines pseudo-labels for entities and relations iteratively until convergence. Finally, the model optimization phase uses a soft mask function and standard masking operations to determine final labels, filtering lower-quality ones with confidence thresholds. Experiments on four datasets show significant improvements in both entity and relation extraction tasks compared to baseline models.</sample>
    <sample id="76">The political bias propagation pipeline involves evaluating the political leaning of language models, investigating how language models with different political leanings perform on downstream tasks, and examining whether these biases result in fairness issues in NLP applications.</sample>
    <sample id="77">This video is for sharing our work on improving summarization factual consistency from natural language feedback. This is the joint work from Yale University and Microsoft Research, and most of the work was done when the first author was an intern at Microsoft Research. In this work, we introduce a new dataset, Defacto, which contains human demonstrations and feedback for improving summarization factual consistency. For this dataset, we provide comprehensive analysis and offer further insights into the factual consistency of the summarization models. Upon this dataset, we propose three new NLP tasks, and we provide strong baseline models for each of them. The tasks we propose are summary editing, feedback generation, and automatic factual error correction. The task we studied in this work is abstractive text summarization, and we specifically study the factual consistency of the summarization models. This quality requires that all of the information in the summary should be supported by the input document. The human demonstrations and feedback we collected in this work is based on the original system-generated summaries of the existing summarization models. We asked the annotators to provide labels to decide whether the summary is factually consistent, and we required them to provide human corrected factually consistent summaries if they think the original summary is not correct. Also, we required them to provide human feedback, which contains the instructions, explanation, and evidence. More specifically, the explanation is to explain why the annotators think the summary is factually consistent or not. The instructions are for changing the original summary to make it factually consistent, and the evidence is one of the most relevant sentences in the source documents which will support the claim of the annotators. We collected the data on the XSum dataset, which is the most commonly studied dataset for summarization factual consistency, and the initial system outputs are collected from the pre-trained PixSum model. In this slide, we show an example of the annotated data point. In this slide, we show the basic data statistics. We collected around 2.5K data points, and 70% of them contain factual errors. For the human edited summaries, we show that they can receive higher automatic factuality scores compared with the initial system output. However, we also observe a lower textual overlap between the reference summaries and the human edited summaries. We think the reason is that a majority of the reference summaries on the XSum datasets already contains the factual errors. In this slide, we show the data distribution of the annotated editing instructions and their relation with the different error types. The first task we studied is summary editing, where the model needs to follow the human feedback to edit the initial summary. We found that both the fine-tuned models and the zero-shot large language models can effectively leverage the human feedback for this task. The second task we studied is feedback generation, where a critical model needs to generate the feedback that can be used by the editing model. We found that this remains a challenging task for both the fine-tuned models and the large language models. The third task is to automatically correct factual errors while generating the corresponding explanation. We found that the editing model can achieve comparable performance compared with the baseline models while trained on much fewer data, and training the model to generate the explanation can help the model to achieve better performance. Apart from providing a test bed for the proposed NLP tasks, our dataset also has other advantages thanks to its ground-truth annotations, which can be valuable for training factuality metrics and factuality metric evaluation. We have released our collected Defacto dataset on GitHub, and please check our paper for more details. Thank you for listening.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">Yes, Coscript is publicly available.</sample>
    <sample id="80">The watermark is inserted into the text by first defining a target embedding, then calculating the number of triggers in the sentence, and finally adding the weight of the target embedding to the original embedding.</sample>
    <sample id="81">The authors of the paper are affiliated with the Penn State University.</sample>
    <sample id="82">The video introduces a study on unsupervised automated essay scoring (AES) using multiple heuristic signals. The goal is to score writing quality without human intervention, an important application in education. State-of-the-art AES models are typically trained with large datasets of labeled essays and their ground-truth scores, which is time-consuming and labor-intensive. Unsupervised AES aims to eliminate the need for ground-truth scores, offering potential in both research and practical applications.

The study proposes a novel framework called URRA (Unsupervised Rank Aggregation), which uses multiple heuristic quality signals as pseudo ground-truths to train a neural AES model. URRA includes a heuristic essay ranking module (HERM) that generates partial order pairs by ranking essays based on various quality signals, and a deep pairwise rank aggregation module (DPRAM) that aggregates these partial order pairs into a unified supervision signal. This approach addresses the inconsistency among different quality signals and ensures strong supervision for model training.

Experiments on both transductive and inductive settings demonstrate that URRA outperforms unsupervised baselines with significant improvements. Compared to cross-domain and one-shot methods, URRA achieves competitive performance, although it still lags behind general supervised methods due to the lack of strong supervision. The study concludes that URRA effectively performs essay scoring under a supervised setting, providing a promising solution for unsupervised AES.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of languages.</sample>
    <sample id="84">The paper presents a diagnostic test suite for evaluating knowledge integration in natural language understanding (NLU) models. It introduces a coreference resolution task to assess the ability of models to integrate knowledge from different sources, including pre-training and inference time. The authors evaluate the dataset with human study participants and establish coreference resolution models. They define three settings: background pre-training, background both, and background inference, to vary the availability of information. The results show that while some models perform better with task-specific training on the dataset, even the best-performing models struggle to reliably integrate background knowledge presented only at inference time. The paper concludes by highlighting the need for future work to explore extending these methods to other machine learning networks and hardware structures, as well as introducing more models such as combinations of zero-shot static parameters and dynamic parameters.</sample>
    <sample id="85">Making a chocolate cake</sample>
    <sample id="86">They make sure of the covertness of their method by visualizing the embeddings of sentences on the dataset using PCA.</sample>
    <sample id="87">The work uses existing PLMs to build a new one by fine-tuning them on the task of biomedical and clinical domain.</sample>
    <sample id="88">Non-binary people</sample>
    <sample id="89">The speaker shows how the model leverages knowledge learned through the attention mechanism on the example sentence "I'm going to talk about."</sample>
    <sample id="90">The paper explores the feasibility of using language learners as annotators for natural language processing (NLP) data annotation. It questions the necessity of recruiting native speakers, especially in low-resource languages where it is challenging to find native speakers. The study conducted a proof-of-concept experiment with 120 language learners and 20 native speakers across three languages: English, Korean, and Indonesian. Participants were tasked with four common NLP tasks, and their annotations were compared to those of native speakers. The results showed that language learner annotations were nearly accurate, particularly for simpler tasks and medium-level questions. Aggregating these annotations with majority voting almost matched the accuracy of native speaker annotations. Additionally, training simulations using learner annotations achieved about 95% of ground-truth performance, sometimes outperforming models trained with native speaker labels. Learners' language proficiency improved over six days of annotation tasks. This study suggests a novel approach to data construction by utilizing language learners, potentially broadening NLP research for many languages and overcoming geographic and technological barriers.</sample>
    <sample id="91">The amount of tasks impacts the model performance by increasing it.</sample>
    <sample id="92">The authors compare their method with three treeless baselines on the CoNLL benchmark: (1) a model that uses a single permutation to predict the output, (2) a model that uses a sequence of permutations to predict the output, and (3) a model that uses a combination of permutations and a linear classifier to predict the output.</sample>
    <sample id="93">They are co-authors with the first author.</sample>
    <sample id="94">The video introduces a paper titled "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark." The presenter, Jing Wei from the University of Science and Technology of China, explains the background of embedding services, which are built upon large language models like GPT-3, LLaMA, and PaLM to assist various NLP tasks. However, recent works have shown that attackers can steal these models by learning from embeddings and provide similar services. To protect the copyright of embedding services, the paper proposes an embedding marker, a backdoor-based watermark method applicable to embedding services. This method involves two main steps: watermark injection and copyright verification. The watermark injection step defines a target embedding based on the number of triggers in a sentence, while the copyright verification step detects whether another service contains the watermark using a backdoor and benign dataset. The results show that the embedding marker has great detection performance while maintaining good utility for downstream tasks.</sample>
    <sample id="95">David Villegas Torres</sample>
    <sample id="97">The speaker mentions three problems of SimulST.</sample>
    <sample id="98">Sanitizing the political opinions in language model training data.</sample>
    <sample id="100">The presentation discusses a method called "PromptRank" for few-shot reranking in multi-hop question answering (QA) using language models. It explains that traditional multi-hop retrievers require extensive training data, which can be costly and challenging to obtain, especially in low-resource domains or specialized fields. The PromptRank approach combines unsupervised retrieval with a few-shot language model-based reranking. The process involves retrieving candidate chains using TF-IDF and hyperlink traversal, then reranking these chains by scoring the likelihood of the question given each chain prompt. The scoring function is constructed using a chain prompt that includes the documents from the chain, an indicator token, and an instruction to guide the language model's reasoning. The evaluation results show that PromptRank outperforms fully supervised systems like Dr. Kit and performs comparably to state-of-the-art multi-hop retrievers. Additionally, ablation studies verify the importance of each component, and downstream QA performance is evaluated when combined with a reader model.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The important properties of a watermarking method are applicability to embedding as services, non-degradation of the utility of the provided embeddings, covertness to the attacker or the attacker can remove the watermark easily, and transferability to the attacker's services during the model extraction process.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are not specified in the given information.</sample>
    <sample id="104">100</sample>
    <sample id="105">The cosine and L2 similarity are used for measuring the difference between benign and backdoor datasets.</sample>
    <sample id="106">The paper presents a dataset called QUEST, which includes over 3000 entity-seeking queries with implicit set operations. The dataset is constructed by performing set operations over atomic categories from four domains of interest: films, books, plants, and animals. Human annotators are used to paraphrase and validate the queries, ensuring that they have the same meaning and are fluent. The relevance of entities in the answer set is also verified, and evidence in the document is marked as its attribution. The paper shows that there is a large room for improvement on retriever performance based on the recall of the complete answer set, and that queries with set intersection and set difference are particularly challenging. The paper hopes to help researchers build improved systems for information seeking scenarios with selective information needs.</sample>
    <sample id="107">The multilingual encoder-based models were used to train a multilingual model for all languages. For example, German, English, and Chinese queries were put together to train a multilingual model, which could then be used to translate German queries or Chinese queries, etc.</sample>
    <sample id="108">The paper presents a revised Minimal Pair Paradigm (MPP) to evaluate language models' acceptability judgments on longer sentences. The traditional MPP evaluates models by showing acceptable and unacceptable sentences, with the model expected to favor the former. However, this approach is limited in evaluating models' acceptance towards longer sentences. The revised MPP simulates longer sequences by recreating sentences from datasets, using acceptable or unacceptable sentences as prefixes. This method allows for testing models' acceptability across different contexts, including mismatch scenarios and unrelated domains like Wikipedia. The results show that MPP judgments are robust for arbitrary context lengths but significantly affected when matching structures from the same dataset. This highlights the importance of considering context in evaluating language models' abstract knowledge throughout the context window.</sample>
    <sample id="109">The paper introduces a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset contains 64k examples, with an additional 240k examples if considering instruction paraphrases. The generated examples are analyzed for correctness, creativity, and diversity, with more than 50% of the generated examples being correct and containing valuable information for instruction tuning. The paper also shows that fine-tuning an 11 billion parameter T5 model on the Natural Instructions dataset outperforms a baseline trained on Supernatural Instructions across several benchmarks. The paper highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowdsourcing who usually collapse into predictable heuristics and form annotation artifacts. At the same time, language models are faster and cheaper than human annotation.</sample>
    <sample id="110">hello everyone my name is Justin John from the penn state University today I'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is a task to build semantic representations of user queries such as SQL and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to SQL lambda or funql and etc existing cross-lingual semantic parsing models are separately proposed and evaluated on data set of limited tasks and applications for instance there lacks of coverage on certain natural language the Chinese is missing and lacks of coverage on certain meaning representations the lambda calculus is missing or they are only evaluated on certain neural model for example there's only one single model to evaluate them so to this end we propose exemplar we provide a uniform data set exemplar for cross-lingual semantic parsing in multiple natural languages and many representations it contains 90 datasets in various domains five semantic parsing tasks 80 meaning representations and 22 natural languages in 15 language families and to better evaluate our benchmark we consider the six settings for training and evaluation the first one is translate test we use Google translate API to translate source to the target language then use monolingual model to train and evaluation and for example we trained the English model on English query and during inference we translate the German query using API to English and then use the trained model to predict the SQL and we also test monolingual model in this setting the source language is the same as Target language for example German to German or English to English we also test monolingual multi-language setting by training model in models with only 10 percent of training data and we test monolingual multilingual model which we train one but multilingual model for all languages for example we put the German English Chinese queries together to train a multilingual model and during inference we can use this model to to translate German queries or Chinese query or Etc and we also consider cross-lingual zero shot and few shot transfer we train on one source language and transfer to another language so during training we train it on English query or the combination of English and German few shot queries to train a multilingual model to and predict the SQL output and we also find many interesting results so regarding analysis of monolingual models we evaluate on two groups of models including encoder PDR which stands for multilingual pre-trained encoders with pointer-based decoders such as XLMR plus PDR and we also evaluate encoder decoder models which is multilingual pre-trained encoder decoder models such as Mbart and MT5 we found that encoder decoder obtains the best performance on all nine datasets and we evaluate on MT5 and example XLMR plus PDR on multilingual setting we found that encoder decoder or encoder PDR can be improved by training in a mixture of various languages and we found it is because most of the major natural languages can obtain performance gain except that English performance drops in seven datasets and only gains in three datasets I think this is known as curse of multilinguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual few-shot transfer the orange line is cross-lingual zero-shot transfer well the green line is the monolingual setting we found that by comparing the green and orange line we found the four zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example encoder decoder outperforms previous work or achieved comparable results for training on English natural language and significantly boost the performance of few shot on Target natural languages and we found multilingual language models such as codes and blue are still inadequate for cross-lingual semantic parsing tasks um to sum up we build example a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative types of multilingual language models and our result shows many interesting findings and Etc and welcome to visit our paper and code thanks for listening the large language models to generate a high-quality script data set code script for constrained language planning we hope code script data set can be a valuable resource to advance the research on language planning thanks for your time please find more details of code script in our paper</sample>
    <sample id="111">The authors assume that the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="113">hello I'm James Finch and I'm Sarah Finch and today we'll tell you all about ABC eval a new dimensional approach to evaluating conversational AI this work was done by the Emory NLP lab led by professor Jinoo Choi at Emory University and in collaboration with Amazon Alexa AI so let's say that you just developed a dialog model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or Likert scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself we call this approach annotating behaviors in chat or ABC eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature ABC eval is capable of measuring the rates at which chat models will commit various thematic errors for example ABC eval measures the number of turns in which a chat model ignores its partner or says something irrelevant contradicts itself or its partner hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using ABC eval for comparison we also evaluated these conversations using three existing methods Likert ratings on the turn level Likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions from our analyses of these evaluation results we found that ABC eval behavior labels are overall more reliable than labels collected by existing methods as measured by inter-annotator agreement on 100 doubly labeled conversations in addition ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis for example you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively while the average Likert consistency scores explain only four percent or less finally we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression you can see how the combination of all ABC eval metrics explains over 25% of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality on the other hand the combination of all turn-level Likert metrics explains far less of the quality and fewer of these metrics carry unique information these reliable informative and distinct ABC eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20% of their responses they produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope ABC eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational AI will advance in the coming months and years thank you for watching</sample>
    <sample id="114">The video introduces a research paper titled "Finding the Pillars of Strength for Multi-Head Attention" presented at ACL 2023. The paper addresses the challenges of large language models, particularly focusing on the heavy parameter problem and the inefficiency of multi-head attention mechanisms. It proposes a method called Group Head Attention (GHA) that uses a divide-and-conquer strategy to compress multi-head attention by dividing heads into groups and pruning redundant heads. This approach aims to achieve significant parameter compression while maintaining performance on tasks like machine translation, language modeling, and abstract summarization. The GHA method is evaluated on two models, achieving up to 90% parameter compression with comparable or better performance. The video also discusses the potential of task-specific automatic pruning in reducing the redundancy in large language models without sacrificing performance.</sample>
    <sample id="115">The approach uses a lambda speech segment size.</sample>
    <sample id="116">Servin is a judge</sample>
    <sample id="117">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The paper presents a submission for the ACL 2023 conference, focusing on improving pre-training techniques for code-switched NLP. The authors define code-switching as the presence of multiple languages within a single sentence, which is common in linguistically diverse communities like India. They argue that multilingual pre-trained models like MBERT and XLM-R do not perform well on code-switched tasks such as question answering and sentiment analysis. To address this, they propose novel MLM (Masked Language Modeling) techniques tailored to code-switching, including Switch-MLM, Frequency-MLM, and residual connections with auxiliary losses. These methods aim to increase the model's ability to handle switch points, where words transition between languages. The results show that their combined method outperforms other approaches on sentiment analysis tasks across different language pairs. The paper also includes probing experiments using linear and conditional probing to verify the effectiveness of these methods in capturing switch point information.</sample>
    <sample id="119">The paper focuses on GPT-4 and GPT series in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from a specific layer, specifically the encoder-decoder attention mechanism.</sample>
    <sample id="121">The examples of direct inference are the name of the song easy on me or its position, the first one.</sample>
    <sample id="122">The authors of the paper are affiliated with Fudan University and Brain Technologies Inc.</sample>
    <sample id="123">The research presented focuses on improving multi-modal zero-shot learning through instruction tuning. The study explores the effectiveness of instruction tuning on large language models for various downstream tasks, particularly in computer vision and multi-modal contexts. A significant challenge is the lack of publicly available multi-modal instruction datasets, which prompted the creation of MultiInstruct, a benchmark dataset with 62 diverse multi-modal tasks covering 10 broad categories. This dataset includes over 15,000 instances from 21 existing open-source datasets, each equipped with five expert-written instructions.

The researchers used OFA, a unified multi-modal pre-trained model, as their base model. They trained the model using 53 tasks from the NLG group, sampling 10,000 instances per task, and tested it on the entire Commonsense Reasoning group along with additional tasks from Wiki and the miscellaneous group. For NLP tasks, they randomly sampled 20 tasks from the test split of Natural Instruction.

The evaluation metrics included accuracy for multi-modal classification tasks, ROUGE-L for multi-modal generation tasks, ROUGE-L for NLP tasks, and sensitivity to measure the model's consistency in producing the same outputs for the same task despite slight variations in the wording of the instruction. The results showed that instruction tuning significantly improved the performance of OFA on unseen multi-modal tasks, with better performance and lower sensitivity as the number of tasks increased. Transfer learning from the Natural Instruction dataset also benefited instruction tuning, reducing sensitivity and improving overall performance.</sample>
    <sample id="124">The presentation discusses the development of a comprehensive temporal reasoning benchmark dataset and training strategy for large language models (LLMs). It introduces three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. The study evaluates LLMs' performance in predicting years, months, and events using a dataset that covers all three levels with long temporal coverage. The evaluation is conducted in three QA problem settings: closed-book, open-book, and reasoning QA. A training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning is proposed to improve LLMs' temporal reasoning capabilities. The results show significant improvements in temporal reasoning tasks, particularly in the reasoning QA setting, highlighting the effectiveness of the proposed approach.</sample>
    <sample id="125">Five</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="127">The presentation introduces a method for enabling smaller language models to perform complex reasoning tasks by using larger models as "reasoning teachers." The approach involves generating step-by-step solutions for complex problems using large models and then fine-tuning smaller models with these solutions. This technique, called Diverse Reasoning, uses stochastic temperature sampling to generate multiple reasoning samples from the teacher model, which are used to train the student model more effectively. The method has been tested on 12 tasks and has shown significant improvements over existing baselines, particularly in text-based tasks like data understanding and coin flip. It also outperforms vanilla fine-tuning even with smaller models. The performance of this method is highly scalable but comes with trade-offs related to dataset size, teacher model quality, and student model capacity.</sample>
    <sample id="128">The paper presents a diagnostic test suite for evaluating knowledge integration in natural language understanding (NLU) models. It introduces a coreference resolution task to assess the ability of models to draw on knowledge from different sources. The authors evaluate the dataset with human study participants and establish coreference resolution models. They define three settings: background pretrain, background both, and background inference, to vary the availability of information. In the background pretrain setting, background knowledge is assumed to be available at pretraining time. In the background both setting, background knowledge is available both at pretraining time and inference time. In the background inference setting, both knowledge types are available only at inference time. The results show that without task-specific training on KidMOS, both models do not perform well. However, when trained on KidMOS, both C2F and BERT4C perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues which are not useful when testing on KidMOS where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time.</sample>
    <sample id="129">Black women</sample>
    <sample id="130">The models that do not generalize well are the ones that do not use transformer architecture.</sample>
    <sample id="131">The testing datasets are named BLM, Syntax Gym, and Acceptability.</sample>
    <sample id="132">Two authors are involved in the paper.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="134">hi i am yannis lavrak and i will present you our works on dr bert a robust pre-trained model in french for biomedical and clinical domain in this presentation we first talk about language modeling in healthcare then we will present the main contribution of our article we introduce the first biomedical model in french named dr bert which is based on roberta and trained on natos which is a data set of medical crowd data from the web we also introduce a comparison of models with multiple pre-training settings and data sources then we present our results on eleven biomedical and clinical downstream tasks in french and finally we conclude about the experiments and give you more details about how to access to the models since it's released in 2018 bert has become one of the most effective approach to solve natural language processing task and offer huge performance gain compared to historical static and contextualized methods such as word to vector fast text or eno since then this model has been adapted to many other languages like in french with camembert and other domain like biomedical with pemetbert and biobert and on clinical with clinical bert but mostly in english specialized model for other languages are scarce and are often based on continual pretraining due to the lack of in-domain data however french didn't have any open source model for biomedical and clinical we so we ask ourselves question about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data to answer this question we compare dr bert with our shbert model which is based on anonymized data obtained from the non-university hospital data warehouse afterward we ask ourselves how much data do we need to train a specialized model on french data is it four gigabyte eight gigabyte or more to answer this question we first train and compare four from scratch model a first version of dr bert with seven gigabyte of natosh a second version of four gigabyte subset of natosh a first version of shbert which is a clinical model with four gigabyte of sentences taken from clinical notes and a final version of shbert with a mix of four gigabyte subset of natosh and four gigabyte of clinical notes in addition to this comparison we introduce three models trained on continual pretraining to analyze the impact of pretraining strategy one based on the weight of camembert and train on four gigabyte subset of natosh another also based on camembert but training this time on the four gigabyte of clinical nodes and finally one based on english biomedical model pemetbert and train on four gigabyte subset of natoshes in total we have seven models to evaluate our seven models we gather which public and private don't seem task such as name entity recognition classification part-of-speech tagging and question answering these models are compared to six baseline models which are camembert osca 138 gigabyte camembert osca four gigabyte camembert ccs net four gigabyte pemetbert byobert and clinical bert the evaluation of highlight that model perform best on the task with data of the same nature has those uh on which the model has been trained however we can obtain that data from we can observe that data from heterogeneous sources appear to be more versatile we also observe that using more data translate into better performance in overall from scratch pretraining seem to obtain higher performance on most of the tasks however our experiment on continual pretraining using the weight and tokenizer of pemetbert trained on the four gigabyte subset of natosh show comparable result to those obtained with dr bert four gigabyte from scratch which is not the case for the model based on camembert weights and tokenizer which suffer from stability issues finally as a conclusion our proposed system offer better performance on nine of the eleven don't seem tasks and surpass globally the result of the generic model here camembert we also observe viewing that specialized data is better more specialized data is better but it doesn't scale well as the pre-trained model obtained from natosh are freely available and on youtube face and all the training scripts are on our github repository so thank you for for this presentation and we are looking forward to exchange at the post session in toronto</sample>
    <sample id="135">The presentation introduces ABC-Eval, a new method for evaluating conversational AI models. Developed by the Emory NLP Lab and Amazon Alexa AI, ABC-Eval assesses multiple dimensions of dialogue quality by annotating whether model responses exhibit certain behaviors like providing irrelevant information or contradicting themselves. This approach measures thematic errors such as ignoring the partner, saying something irrelevant, self-contradiction, hallucinating incorrect facts, violating common sense knowledge, and showing empathy. The method was tested on four state-of-the-art chat models using 100 human-bot conversations, comparing results with existing methods like Likert ratings and pairwise comparisons. The analysis showed that ABC-Eval's behavior labels are more reliable and predictive of overall conversation quality. The combination of all ABC-Eval metrics explains over 25% of conversation quality, indicating its reliability and informativeness in evaluating conversational AI with higher resolution than previous methods.</sample>
    <sample id="136">The presentation introduces Fermat, a flexible evaluation set for assessing the mathematical abilities of language models. It highlights the limitations of current benchmarks, which primarily focus on accuracy scores and F1 measures, failing to capture the nuances of numerical reasoning. Fermat addresses this by providing a suite of math worded questions derived from real-life scenarios, testing number understanding, mathematical operations, and training dependency. The evaluation reveals that most models perform poorly across these aspects, with the original Common Core and Illinois benchmarks showing only slight improvements. Fine-tuning with math teachers' templates significantly enhances performance, particularly in handling small integers, large integers, and decimals. However, the model's ability to memorize exact expressions is limited, indicating the importance of linguistic knowledge. The impact of diverse training templates further improves performance, suggesting that language and mathematical diversity are crucial for developing robust numerical reasoning capabilities in language models.</sample>
    <sample id="137">The Tell2Design dataset is a large-scale dataset featuring floor plans with natural language instructions to describe user preferences. It was introduced by researchers from the Singapore University of Technology and Design to enable users to design by telling instructions, focusing on the floor plan domain as the initial area of research. The dataset consists of 5051 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk, along with around 76,000 artificially generated language instructions from predefined templates. The main challenges in this novel task are performing design generation under strict constraints, understanding the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and dealing with ambiguous, incomplete, or misleading information in human instructions. The Tell2Design dataset provides a foundation for future research on the task of language-guided design generation, offering a unique approach to generating floor plan designs directly from language instructions.</sample>
    <sample id="138">The authors claim that the integration of background knowledge into NLU models is an understudied area.</sample>
    <sample id="139">The names of the speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, Coscript underwent quality checks.</sample>
    <sample id="141">Existing resources for on context-dependent translation are limited to language-only instruction tasks.</sample>
    <sample id="143">The approach is compared with the Whitkey strategy and the local agreement.</sample>
    <sample id="144">The authors of the paper are affiliated with LIA, Avignon University, LS2N, University, and CHU de Nantes.</sample>
    <sample id="145">Jenny T. Liang</sample>
    <sample id="146">The paper presents an analysis of omission in dialogue summarization, a subtask of text summarization. It introduces the background of dialogue summarization and highlights the challenges of extracting key information from dialogues across different domains. The authors discuss the progress made in dialogue summarization using large-scale pretrained language models, noting that while these models can generate fluent and coherent summaries, they often contain factual errors and omissions.

The paper then delves into the specific issue of omission, presenting data on the omission rate of summaries from five domains and six pretrained models. It is found that even state-of-the-art models exhibit high omission rates, with about 70% of generated summaries suffering from omissions. The authors also analyze the position distribution of omitted information within dialogues, revealing that omissions are randomly distributed regardless of dialogue length or domain.

To address the omission problem, the authors propose a task definition for omission detection, focusing on identifying missing content at the utterance level. They construct a dataset with high-quality omission labels for dialogue summarization, covering five domains and generated by different abstractive models. The dataset includes human evaluations to ensure label quality and is publicly available.

The paper explores three baseline frameworks for omission detection, evaluating their performance using precision, recall, and F1 scores. It also examines the effectiveness of using detected omissions to refine summaries, showing significant improvements in summary quality. The authors conclude by emphasizing the importance of addressing the omission problem in dialogue summarization and the potential benefits of omission detection for improving summary quality.</sample>
    <sample id="147">Three</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">The paper presents a new dataset called Meeting Q&amp;A, which is an extractive question answering dataset based on questions asked by participants in meetings and their corresponding answer sentences. The dataset contains 7.7 thousand questions, with 30% unanswerable, 40% multi-span answers, and 48% multi-speaker answers. The questions are longer, open-ended, and actively feed discussions from others. The data collection process involves public meeting transcripts from the AMI Corpus, question selection based on punctuation, and filtering out very short questions. Answers are annotated by annotators to label sentences in the answer span, achieving a high inter-annotator agreement of 0.73. The dataset is split into train, dev, and test sets, with 30% of questions unanswerable. The paper also discusses the length distribution of meeting transcripts, questions, and answers, showing that they are roughly composed of 12 and 35 words respectively. The authors achieve a high human performance on the test set with an F1 score of 84.6. They employ various methods, including short context models, single-span models, multi-span variants, and silver data augmentation. The results show significant gaps between fine-tuned models and human performance, as well as differences in zero-shot performance and error analysis.</sample>
    <sample id="152">The presentation introduces a new language model designed for classical philology, specifically for ancient Greek and Latin texts. The speaker highlights the limitations of existing models, which are monolingual and lack robust evaluation. The team has pre-trained two monolingual models (Graeberta and Greta) and two multilingual models (Philberta and Philter) using a variety of resources, including OpenGreek and Latin, OCR transcriptions, and the Internet Archive. They have also developed a new pre-training corpus from the Internet Archive by identifying incorrectly transcribed Greek stop words. The team has benchmarked their models on tasks such as speech tagging, dependency parsing, and lemmatization, achieving impressive results. They have also analyzed the performance of the T5 encoder and investigated the implications of multilinguality in their models. Overall, the presentation showcases the development of powerful language models that can process both ancient Greek and Latin texts, providing valuable tools for scholars in the field of classical philology.</sample>
    <sample id="153">The video introduces a new dimensional approach to evaluating conversational AI called ABC-Eval, developed by the Emory NLP Lab in collaboration with Amazon Alexa AI. This method reduces subjectivity by explicitly annotating whether model responses exhibit behaviors like providing irrelevant information or contradicting themselves. The evaluation covers various aspects of chat quality, including thematic errors and empathy. The team tested four state-of-the-art chat models on 100 human bot conversations using ABC-Eval and compared them with three existing methods: Likert ratings on turn level, Likert ratings on dialogue level, and pairwise comparisons. Results showed that ABC-Eval's behavior labels are more reliable and predictive of overall conversation quality. Additionally, ABC-Eval metrics capture unique aspects of chat quality better than existing methods, enabling more precise and informative evaluations of conversational AI.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">Javad Hosseini</sample>
    <sample id="156">hello everyone my name is a bill and i will give you a short overview of the paper prompt for translation assessing strategies and performance this is joint work with my colleagues from google translate pam is a 540 billion parameter large language model presented last year in 2022 it's trained on a large collection of text comprising 780 billion tokens at the time of publication it achieves state-of-the-art in hundreds of nlp tasks in this work we present the first systematic study of large language model prompting for machine translation we evaluate the translation capability of such models using the best practices of the m community this involves using the latest test sets to avoid another lab of the test data with the training data of the language model and we compare two state-of-the-art systems so the best performing systems or the wnt evaluation we use state-of-the-art and new m metrics and additionally also show expert-based human evaluation results finally we provide some recommendations for prompt selection strategies the prompting has a big influence on the performance of the of llms for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts for each sentence the majority of sentences 516 out of 1,000 the difference observed is of more than one bloom points and this can go in extreme cases up to forty bloom points so it's important to select that good prompting strategy in our experiments we conclude for a five shot prompting strategy where we just mark each uh its sentence that we provide to the system with the language it's same so in this example here where we perform translations from german into english the german sentences the source sentences are marked with german colon and the english translations with english colon we saw that the actual form of the prompting doesn't have a big influence in in the case of several short prompting it's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting it's the examples that carry most of the of the weight the summary of our experimental results is that the example quality is more important than the similarity to the source sentence so it's important to select the examples from quite high-quality translations in particular we compare the selecting prompts from the training data of the t evaluations or the dev data the dev data is much more curated and with higher quality that the train data that it's more nice and their results so better performance when using the the dev data nonetheless specialized state-of-the-art systems have a substantial advantage over the pam translations but pam comes pretty close to a commercial system now in our case we chose to evaluate with google translate the insights that we gain from the evaluation that we perform using the m framework is that the fluency of pam is comparable to state-of-the-art of the art systems but the main difference comes from the accuracy so in particular the most common error are omission errors so it seems that pam chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation however the style awkward category for pam is lower than for the state-of-the-art systems which is an additional signal that pam provides really fluent output but still with some problems of of accuracy and that's it for this really short overview for more details please come my to the full presentation of the paper thank you very much</sample>
    <sample id="157">The research presented in this talk focuses on dialogue summarization, a challenging and interesting task in the text summarization field. The goal is to distill silent information from a dialog context into a concise summary, helping people quickly capture the highlights of semi-structured and multi-participant dialogues without reviewing the complex context. The proposed method, called SDG, consists of four main components: an utterance encoder, a static graph construction module, a dynamic graph module, and a pre-trained language model as the summary generator. The SDG model combines multiple static graphs computed in the previous step using a dynamic graph module to capture semantic relationships between utterances based on their deep vector representation. Finally, the pre-trained language model is used to generate the final summary by fusing the static dialogue structure and the dynamic learned dialogue structure. The talk also introduces two heuristic dialogue structure modeling methods: the discourse passing graph and the speaker relationship model. These methods help build the relationship between utterances using a graph network, capturing the position information of utterances, and integrating the adjacency matrices of the static and dynamic graphs into a unified graph for generating the final summary.</sample>
    <sample id="158">The paper introduces a dual cache method for long document neural coreference resolution. The coreference resolution task is to identify and cluster mentions that refer to the same entity in a document with multiple mentions of entities. Conventional methods have quadratic complexity, while recently proposed cache-based methods use a fixed-size cache to reduce complexity to a linear level. However, in long documents, the topic may switch multiple times, causing mentions of entities scattered across a wide range of text, leading to high cache misses when encountering new mentions. Our case study shows that high-frequency entities are mentioned globally and account for most of the cache misses. We propose a dual cache that has a local cache and a global cache that work together. The local cache stores local entities with LRU evicting policy, and the global cache stores global entities with LFU policy which evicts the least frequently used entity when the global cache is full. The dual cache works by classifying whether it is a new entity or belongs to an entity in the cache, then evaluating the frequency of this new or updated entity. If qualified, it is added to the global cache; otherwise, it is added to the local cache. Whenever the cache is full, it triggers the eviction policy to evict an entity from it. We evaluated dual cache on four public benchmarks and found that it performs better than the baselines even they use an unbounded memory. Without training data, the model with unbounded memory performs slightly better but dual cache is still faster. We also evaluated the capability of dual cache by annotating a book with 30,000 words and found that the performance gap is much larger between the baseline and dual cache for book-level documents. Dual cache significantly reduces the cache miss compared with a single cache. There are always trade-offs between model efficiency and performance for cache-based models, but we show that dual cache has the highest performance-cost ratio.</sample>
    <sample id="160">The first step of the method maps each input token to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="162">hello everyone i'm makkattah and today my co-author martin and i are presenting our work the kit must evaluate evaluating knowledge integration from multiple sources this work is a collaboration between magill university mela and microsoft research natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pre-training and knowledge given in inputs at inference time recent works in tasks like question answering show that models can use pretraining knowledge to solve the task but natural language understanding often requires knowledge that is also supplied at inference time for example in the sentence john saw the newly elected president on tv pretraining parameters can contain information about what presidents do and what a tv is but they cannot reliably know who this instance-specific entity john is or who the new president is because the president might have changed since pretraining therefore successful models for knowledge-intensive nlu tasks require the ability to integrate and use both pretraining and inference time knowledge in this work we propose a diagnostic test suite for knowledge integration we introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the data set with human study participants and established coreference resolution models here is an example from our data set servin is a judge kia is a baker termine and kia met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity-specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pre-training of large language models while entity-specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources we have defined three settings of kit must first with the typical setting background pretrain where background knowledge is assumed to be available at pretraining time second there's a background both setting where background knowledge is available both at pretraining time and inference time lastly the background inference setting where both knowledge types are available only at inference time this last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pretraining data of models for example because new occupations have developed since the time of pretraining here's an example of how we control the availability of facts in the truth sources in the background pretraining setting we assume that the background knowledge politicians seek elected seats in government is contained in the pre-trained parameters in the inference-time context we provide the entity-specific knowledge chester is a politician in the background both setting we additionally provide not only entity-specific but also background knowledge about politicians in the inference-time context in the background inference setting we provide the fictional occupation mirtura instead of politician because mirtura is unlikely to be contained in the pre-trained parameters we evaluate the data set both with human study participants and established coreference resolution models in this figure we show the results of the best-performing models on the most difficult variant of the background pretraining setting without task-specific training on kit must both models do not perform well when trained on kit must however both c2f and build for q perform significantly better than the random choice this suggests that when trained on general coreference resolution datasets models learn to exploit surface cues which are not useful when testing on kit must where such cues have been removed additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time to summarize the main takeaways of our paper many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="163">The best alignment method for DEplain is the method of MassAlign.</sample>
    <sample id="164">The benefit of weakly supervised learning is that it is much cheaper than manually labeling data.</sample>
    <sample id="165">The paper presents a method for adaptive reasoning without supervision. It introduces an unsupervised learning method called LIPOR, which stands for likelihood learning with posterior regularization. LIPOR treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. To prefer plausible explanations, LIPOR uses a regularizer that enforces mutual exclusivity among explanations. The LIPOR objective consists of two parts: maximizing the likelihood of outcomes and preferring some explanations over others. The paper compares LIPOR to zero-shot models and the previous best unsupervised approach on Alpha-ALI, achieving superior performance by over four absolute points in accuracy.</sample>
    <sample id="166">The presentation introduces a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. It addresses the challenge of retrieving images from text that is highly similar to the query description, which is a common issue in image-text retrieval tasks. The proposed method utilizes a strategy inspired by the divide-and-conquer approach and dual process theory. The first module, called the proposition generator, uses a complex proposition generator to represent and generate sentences based on visual linguistic interactions. The second module, the neural symbolic reasoner, integrates the reasoning states and results of the first module to obtain the final solution. The system combines the advantages of both logical reasoning and analogical reasoning, as demonstrated by the experimental results. The presentation also suggests that neural symbolic calculation may be a useful approach to improve the compositionality and planning capabilities of large language models. Overall, the proposed method shows promising results in handling complex image-text retrieval tasks.</sample>
    <sample id="167">750 documents were aligned manually and 3450 sentences were aligned automatically</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters-News from 2020 and annotating them with the same CoNLL 2003 annotation guidelines.</sample>
    <sample id="169">The paper presents a systematic study of prompting strategies for large language models (LLMs) in machine translation. It evaluates the translation capabilities of LLMs using best practices from the NMT community, employing the latest test sets to avoid overlap with training data. The study compares two state-of-the-art systems and uses both automated metrics and human evaluation results. Recommendations are provided for prompt selection strategies. The experiments show that the quality of examples is more important than their similarity to the source sentence, and that specialized datasets yield better performance. While specialized systems have an advantage, PAM comes close to a commercial system. The fluency of PAM is comparable to state-of-the-art systems, but its accuracy suffers from omission errors and lower style accuracy.</sample>
    <sample id="171">Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding as services or lack of transferability.</sample>
    <sample id="172">No, they are still inadequate for CLSP tasks.</sample>
    <sample id="173">hello everyone my name is true heng today i'm going to present our paper do conl 2003 named entity taggers still work well in 2023 let's get started our paper investigated the problem of generalization using the named entity recognition task or the ner task we observed that models have been using conl 2003 to develop any er for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data and when we develop new taggers what is needed for good generalization at the same time if we do observe poor generalization what causes the performance drop of these models to investigate these problems we developed the conl plus plus data set this is a data set that we collected from Reuters news from 2020 and then annotated them with the same conl 2003 annotation guidelines we then fine-tuned over 20 models on conl 2003 we evaluated them on both the conl 03 test set and the conl plus plus test set and last but not least we calculated the percentage change in F one to assess the generalization of each model so what is needed for a good generalization through our experiments we found that there are three main ingredients that are needed the first one is the model architecture through our experiments we found that the transformer models normally generalized better to new data the second ingredient is the model size we found that usually larger models lead to better generalization in last but not least we all know that the number of fine-tuning examples directly affects the performance of the downstream task here we also found that more fine-tuning examples actually also leads to better generalization to our next question what causes the performance drop of some models we had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on the new test set the second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conl 2003 translates to more than one unit improvement on conl plus plus which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goals hand in hands we can't just have one ingredient but throughout the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conl 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conl 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models and lastly please make sure to check out our paper our data set and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="174">The ArgAnalysis35K dataset is a large-scale collection of arguments for quality analysis. It contains 35,000 argument analysis pairs, with 85% sourced from high-quality sources such as speeches from expert debaters and intermediate debaters, and the remaining 15% from novice debaters. The dataset covers 24 diverse themes, including motions from websites like HelloMotions.com and expert advice, providing a more comprehensive range of arguments compared to traditional datasets that often focus on specific motions. Additionally, the dataset includes an element of analysis, combining claims, premises, and other elements to create a coherent argument. This feature helps in better understanding and evaluating the quality of arguments. The dataset also introduces instance-based annotator reliability, which captures the biases of annotators on a per-argument basis, allowing for more reliable scoring. Lastly, a relevance model assigns scores to each argument based on its relevance to various themes, enhancing the dataset's utility in capturing the depth and relevance of arguments.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by inducing the alignment as part of the training.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined by how well it performs on different demographics or political leanings of news media.</sample>
    <sample id="177">Yanis Labrak</sample>
    <sample id="178">Koustuv Sinha</sample>
    <sample id="179">The presentation by Melanie Sclair focuses on enhancing the theory of mind (ToM) reasoning capabilities in large language models. ToM is the ability to reason about the mental states of others, traditionally measured in humans and language models through reading comprehension tasks involving multiple characters. The presentation introduces Symbolic Tom, an inference-time method that uses explicit graphical representations to improve ToM reasoning skills. This method leverages off-the-shelf and open AI models, precomputing graphical representations for given stories to efficiently answer questions. Experiments with various ALMs show significant performance gains, particularly in out-of-domain settings, demonstrating that Symbolic Tom outperforms supervised approaches and remains beneficial in linguistic generalization tasks.</sample>
    <sample id="180">Myra Cheng</sample>
    <sample id="181">The paper introduces a method for generating constrained language planning data sets from large language models. It highlights the challenge of planning for specific goals with multiple constraints, which is not well addressed in previous work. The authors propose a two-step process: first, they generate a set of abstract goals and then extend them to specific goals with constraints using a tool called InstructGPT. They evaluate the performance of different large language models on this task and find that while the semantic completeness of the generated scripts is acceptable, the faithfulness to constraints is not guaranteed. To improve this, they develop an over-generated script filter that selects the most faithful scripts based on semantic similarity and constraint adherence. This method significantly enhances the quality of the generated scripts. Finally, they use this method to create a high-quality data set for constrained language planning, which can be used to train smaller, specialized models. The results show that these smaller models can achieve higher quality than larger models when trained on suitable data sets.</sample>
    <sample id="182">Tropicalism indicates a trope that connects to a long history of Asian women being hypersexualized, seen as very docile and submissive.</sample>
    <sample id="183">The authors created the human-written portrayals of target groups by using intersectional lens to study biases and harms, which helped them identify the harmful patterns and essentializing narratives that reflect positive stereotypes.</sample>
    <sample id="184">CXMI</sample>
    <sample id="185">The difference between DrBERT and ChuBERT is that ChuBERT is based on anonymized data obtained from the University Hospital of Toulouse, while DrBERT is based on a dataset of medical crawled data from the web.</sample>
    <sample id="186">hi I'm Maya and today we'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esin durmus and dan Jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models or llms however these measures have various limitations they usually rely on hand constructed datasets that are very time consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture a very general broad associations like negative associations with particular groups furthermore most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique low side of harm to overcome these limitations we rely on the property that these newer instruction tuned llms are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an Asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so here are some example generations from gpt four immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words there are some interesting patterns the Asian woman is depicted as unassuming the Middle Eastern woman is referred to using words like exotic and like referring to a mesmerizing region and both of the women of color personas make references to ancestry while the white man persona has nothing of the sort to capture these patterns our method has two parts the first one is generating these personas our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they were also able to surface racial stereotypes and also this enables direct comparison between our generated personas and the human written responses the second part is marked words which is a method to identify the words that distinguish marked groups from unmarked ones which I'll elaborate on shortly the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify one man warrior and mark the term with woman and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group so for instance for the personas of black woman we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups now some results so first we use a lexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones however when we actually look at the distribution of the words in lexicon we find very different things so while the generated personas have much higher rates of the lexicon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic so really just only the positive or at least non-negative ones and in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these seemingly positive portrayals facilitate stereotypes and essentializing narratives in our analysis we reveal how these seemingly positive portrayals reflect harmful patterns first for Mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm this contributes to a long legacy of discrimination and othering for these groups furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing Latina women include things like vibrant and covetous which connect to a trope of tropicalism for Asian women the words are things like petite and delicate and silky which connects to a long history of Asian women being hyper sexualized seen as very docile and submissive and so on and finally for black woman we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on these people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each Mark group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti stereotyping methods that are resulting in these prinnish patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening um have a good time at ACL</sample>
    <sample id="187">Three</sample>
    <sample id="188">Iterative transfer learning is a process where fine-tuning of C-E tasks followed by further fine-tuning on debate yields a much better zero-shot performance.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="190">An attacker can extract model parameters through an EaaS by learning from the embedding and providing similar services.</sample>
    <sample id="191">Three</sample>
    <sample id="192">The presentation introduces a new optimization method called "Ken" designed to achieve both fast convergence and low memory usage in training large language models. It addresses the limitations of traditional adaptive methods like Adam, which require significant memory for first and second moment estimates, and memory-efficient methods like AdaFactor, which sacrifice performance. Ken uses non-negative matrix factorization (NMF) to reduce memory requirements and incorporates a confidence-guided adaptive updating strategy to handle the instability issues found in AdaFactor. The method is tested on three large language models: GPT-2, T5, and BERT, showing that Ken achieves comparable or better performance with significantly reduced memory usage. Experimental results demonstrate Ken's effectiveness in both training tasks and downstream tasks, making it a promising extension for existing memory-efficient optimizers.</sample>
    <sample id="193">Around a thousand annotators were used to create the initial dataset.</sample>
    <sample id="194">Carnegie Mellon University and University of Washington</sample>
    <sample id="195">The presented work introduces a new framework called ROHT, which stands for Reasoning over Hierarchical Question Decomposition Tree. This framework aims to enhance explainable question answering (XQA) by integrating knowledge from heterogeneous sources and handling complex questions more effectively. The framework consists of two stages: first, it builds a hierarchical question decomposition tree (HQDT) to understand the compositional structure of a complex question; second, it performs probabilistic reasoning over HQDT to fuse knowledge from various sources and determine the most likely answers. The HQDT is constructed by generating leaf nodes as atomic questions, then creating intermediate questions based on these leaf questions, and computing certainty scores for each node. The reasoning process involves selecting appropriate knowledge sources, executing queries, and aggregating candidate answers to output the top-k answers with the highest probabilities. The framework was evaluated on challenging complex XQA datasets, demonstrating significant improvements over existing methods when integrating answers from subquestions of different levels and utilizing knowledge from KB and text corpora.</sample>
    <sample id="196">Lisa bought and Meg</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are the four models that were evaluated by the authors of the paper.</sample>
    <sample id="198">Because large language models are coming up with longer and longer context windows.</sample>
    <sample id="199">Yes, it caused performance drop in 7 datasets.</sample>
    <sample id="200">Yes, the annotators know about the entity in advance.</sample>
    <sample id="201">The evaluation used state-of-the-art MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">The regress in generalization impacts specific NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can influence the research process and its outcomes, as it can change the decisions that researchers make.</sample>
    <sample id="204">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="205">The presentation by Changbing, a PhD student at the University of Washington, explores the political biases in language models and their impact on downstream tasks. Changbing examines how language models are trained on large-scale web crawls, which include diverse perspectives from major news media like The New York Times, Los Angeles Times, The Guardian, and Huffington Post. This diversity is both a blessing and a curse, as it allows models to learn from various viewpoints but also introduces inherent social biases that can affect performance in tasks like hate speech detection and fake news identification. The study evaluates the political leanings of language models using the political compass test and investigates how these biases are picked up from training data. It finds that language models occupy different quadrants of the political spectrum, with GPT-4 being the most liberal. The research also shows that models can reflect societal polarization and that left-leaning models are better at detecting hate speech targeting socially minoritized groups, while right-leaning models are more effective at identifying hate speech targeting white and men. The presentation concludes with a discussion on the challenges of sanitizing political opinions in training data to avoid bias propagation while avoiding censorship or exclusion.</sample>
    <sample id="206">They use a model that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the best practices of the NMT community.</sample>
    <sample id="208">The authors proposed three recommendations at last.</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 10.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for future text simplification research.</sample>
    <sample id="212">They experiment with two smaller models in the paper.</sample>
    <sample id="213">OFA</sample>
    <sample id="214">hello everyone my name is Jing Wei from the University of Science and Technology of China it's my pleasure to give a short advertisement video about paper are you copying my model protecting the copyright of large language models for embedding and services view back door watermark let's first introduce the background about embedding as services currently large language models such as GPT LAMA P are exceptional in natural language understanding and generation embedding as services is one of the services built upon large language models to assist various NLP tasks for example open AI offers a GPT based embedding API however recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services therefore it's necessary to protect the copyright of embedding as services to protect the copyright of embedding as services one of the solutions is to embed a watermark in the provider service and detect whether another service contains the watermark the watermark method need to meet the following properties first the method should be applicable to embedding as services second the watermark should not degrade the utility of the provided embeddings third the watermark should be covert enough to the attacker or the attacker can remove the watermark easily finally the watermark need to be transferable to the attackers services during the model extraction process existing works can be broadly classified into four categories however this method either not applicable to embedding as services or lack of transferability therefore in this paper we propose embedding marker which is a backdoor based watermark method applicable to embedding as services then let me introduce the details of our embedding marker embedding marker contains two main steps watermark injection and copyright verification before these main steps we first select a trigger set the trigger set is a group of words in a moderate frequency interval we assume the provider can collect a general text corpus and count the word frequency with it in water mark injection we first define a target embedding when a user send a sentence to the provider service the provider counts the trigger number in the sentence the provided embedding is the weight summation of the target embedding and the original embedding the weight of the target embedding is proportional to the number of triggers in the sentence when the number of triggers in the sentence is greater than M the provided embedding is exactly equal to the target embedding copyright verification is to detect whether a model behind another service contains the watermark we first construct a backdoor and a benign data set backdoor data set contains sentences of which all words belong to the trigger set while all words in the sentences of benign data set do not belong to the trigger sets then the provider requests embeddings from the stler service with the data set the cosine and L2 similarity between the requested embedding and the target embedding are computed we compute the similarity difference between benign and backdoor data set which is defined as delta cosine and delta L2 meanwhile we also apply KS test and use its p value as the third metric we conduct experiments on four data sets AG news Mind SST 2 and ER spam we assume the provider apply Wikipedia data set to count word frequency the results on four data sets show that our embedding marker can have great detection performance while keep great utility for downstream tasks we also validate the covertness of the provided embedding by visualizing the embedding of sentences on the figure the legend of the figures means the number of triggers in each sentence as shown in the figures it's hard to distinguish between the backdoor embeddings and normal embeddings that's all thank you welcome to discuss with us</sample>
    <sample id="215">The paper presents an argument for the symmetric structures of coordination against asymmetric ones, based on the principle of dependency length minimization. It illustrates that direct objects prefer to be close to the verb, while adjuncts may be further away. However, when the direct object is long, it can be moved after the adjunct, satisfying the principle of dependency length minimization. The paper also discusses statistics from the enhanced version of the Penn Treebank, confirming that left conjuncts tend to be shorter and that this tendency grows with the length difference between the two conjuncts. The paper argues that this tendency only occurs when the governor is on the left or absent, and disappears when the governor is on the right.</sample>
    <sample id="216">hi i'm sarah papi from the university of trento and foundation bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matteo negri and marco turkey what is simultaneous speech translation simultaneous speech translation or simultaneous is the process of translating spoken language into a text in another language in real time enabling cross-language communication and what are the problems of the current simulates models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline nist models without retraining or adopting specific architecture for simulates use only one model for every latency regime and handle latency through specific parameters and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross attention mechanism and you can see an example on the right our solution is to propose a dot or encoder decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to a word is emitted if the attention is not concentrated that is this sum is below a certain threshold alpha towards the last lambda speech frames meaning that the received information is enough stable for example if if we receive a speech chunk containing i'm going to talk about and our model predicts the translation in german and we will look at the cross attention weights we will see that the first two words points to the earliest received speech frames while the last word points to the last received speech frames as lambda speech frames this means that the first two words will be emitted while since the sum of the cross attention is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no words points to the last lamb lambda speech frames this means that these three words will be emitted if we look at the main results of a dot we will plot the simultaneous speech translation results on on graphs in which we have blue on one side that measure the translation quality and average legging that is the latency measure and we also consider the computational aware average linking that accounts for the model's computational times to predict the output so we want our curves to be as high as possible on this plot but also we want that they are shifted on the left and we compare with proper strategies that are also applied to offline models that are the white key strategy and the local agreement and we compare also with the state-of-the-art architectures specifically tailored for simultaneous speech translation these are all the results of the simultaneous speech translation strategy on german and we see that a dot outperforms all the strategies applied to offline models since their curves are shifted over the left and we also see that if we consider the actual elapsed time or the computational aware time a dot is the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="217">The paper presents a method for generating controllable dialogues with multiple attributes using a disentangled control generation (DCG) approach. It introduces a unified reference-free evaluation framework (MAE) to assess the quality of generated dialogues, which does not require large-scale labeled data. The method uses a combination of attribute-oriented and task-oriented prompts to guide the model's focus on specific information in the dialogue. Experiments show that the proposed method outperforms baseline models in terms of controllability and test quality, demonstrating its effectiveness in transforming seen attributes into unseen combinations. The results confirm the effectiveness of the method for compositional generation of multi-attribute controllable dialogues.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The research assistant presents a work comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is conducted with two other researchers and supervised by professors. The background of this work is the domain of financial report analysis, which requires lots of human efforts to mine useful information. The motivation of this work is to reduce the effort required to mine useful information from financial reports. The target company is the one that is required to submit annual reports to the SEC. The annual reports contain many details of companies' important activities. However, mining useful information requires lots of human efforts. This work was motivated by two observations: first, we observed that the words in a company's report are very similar; second, the contents are generally dependent. This figure illustrates the text similarity between two reports in the continuous years. For example, the report in 2018 is similar to the one in 2017. Based on the observation, we introduce a highlighting task and a multi-stage pipeline. Follow the motivation, we first define the reference to target structures in our task. The target and reference refer to the report of our interest and the report at its previous year. So basically, a highlighting model should compare and contrast the context between targets and reference. The goal of this highlighting task is to find the variation roots between a given pair T and R. Formally, the model will predict the highlight word importance and therefore we can measure the performance of highlighting. For example, the word "decrease" is supposed to have higher importance in this context. This is our proposed pipeline. Stage 0 is document segmentation. Stage 1 is the relation classification. Stage 2 and stage 2 plus are our main and incremental fine-tuning. Due to the time limit, I would not talk about stage 0 more details can be found in our paper. For the stage 1, we will classify all the pairs into three types. Type A pair refers to the pairs have higher syntactic and semantic similarities. This pair are frequently appeared such as company's regulations. Reverse pair have similar syntactical pattern but in fact the two segments disclose very different meaning. Mismatch pair are more like a debut information or company's new operations. For the model fine-tuning stage, we first use an external dataset (ESNL) for out-of-domain fine-tuning. ESNLI is a natural language inference dataset with token annotation. For in-domain fine-tuning, we use the reverse pairs, the reverse words as pseudo positive labels and we randomly label few others words as negative. In addition, we mix different objectives. We use the soft labeling techniques by mixing cross entropy loss and KL divergence. Therefore, we can alleviate the problem from low quality pseudo labels. The variation dataset include the ESNLI pairs and our released final dataset. We use two metrics to judge the performance. Precision indicates the precision over recall. PCC means the correlation between prediction and annotations. This table shows that our domain-agnostic highlighting model achieve the best performance on final and even preserve the generalization capability as you can see the performance on ESNLI. We further observe that our methods can benefit on simulation, the mismatch pairs which we didn't use during training. In conclusion, we propose a highlighting task with our released final dataset and a simple pipeline with two-stage fine-tuning. There are many other future works we would like to try including improving effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well. Yeah, that's it. So please refer to our paper and GitHub for more details and feel free to ask us if you have any question. Thank you.</sample>
    <sample id="220">Stony Brook University</sample>
    <sample id="221">The paper analyzed the translation of German into English.</sample>
    <sample id="222">The paper investigates data interventions for enabling out-of-domain generalization in open-domain question answering (QA). It identifies three types of data set shifts: concept shift, covariate shift, and full shift. The study uses Wikipedia as the source domain and tests generalizability on seven target datasets spanning six domains. Two overarching methods are used to generate interventions: few-shot and zero-shot. Few-shot involves using a few examples from the target domain to prompt large language models, while zero-shot involves controlling interactions among question, answer, and context variables. The results show that few-shot interventions improve retriever performance by 8% and reader performance by 11%. Zero-shot techniques do not significantly affect model performance. The study also measures compatibility between the source model and target datasets using likelihood values assigned to contexts and answers. Data interventions are found to be effective based on the type of shift exhibited by the target dataset. For example, few-shot adaptations are effective for datasets with concept and covariate shifts, while zero-shot adaptations are effective for no shift. Overall, the paper demonstrates that targeted data interventions can improve QA performance across different domains.</sample>
    <sample id="223">The speaker's name is Changbing.</sample>
    <sample id="224">Long Impart and Normal Base Long Impart</sample>
    <sample id="225">53 tasks are used for training and 19 tasks are used for testing.</sample>
    <sample id="226">The paper is a joint work with colleagues from Google Translate.</sample>
    <sample id="227">The paper presents a framework for grounded language understanding (GLU), which involves grounding natural language expressions into executable plans or programs over specific target environments. GLU is challenging due to the lack of grounding during pre-training, as most language models are pre-trained with text corpora without grounding. The paper proposes a novel framework where a symbolic agent interacts with the environment and proposes candidate plans, while a language model scores and ranks these candidates. This approach leverages the strength of language models in discrimination rather than generation, making it easier to handle the validity and grammar of the target plan. The framework is demonstrated on a representative test bed, achieving outstanding performance across different settings and language models, including BERT, T5, and large models like CodeX. The results show that Pangu consistently outperforms baseline models in terms of sample efficiency and robustness under non-i.i.d. settings.</sample>
    <sample id="228">The authors conducted experiments on four datasets: AGNews, Mind, SST2, and Yahoo.</sample>
    <sample id="229">The presentation introduces the importance of text revision in professional writing, emphasizing its role in achieving optimal phrasing and effectively communicating a message. It explains the iterative process of revising arguments to ensure clarity and impact on the audience. The speaker then outlines two tasks: suboptimal claim detection and claim improvement suggestion, aiming to determine whether an argumentative claim needs further revisions or is optimally phrased. The focus shifts to exploring how to model the quality of argumentative texts using implicit revision patterns from collaborative online debate platforms like Kiyo. The presentation highlights challenges such as representativeness, reliability, model complexity, contextual dependence, and topical and user bias when working with revision-based data. It concludes by inviting the audience to read the paper for detailed analyses and systematic comparisons of strategies addressing these challenges.</sample>
    <sample id="230">hi everyone i'm kostya sinha and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aaron mueller kanishka mishra karen funtes roger levy and adina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like bleep syntax jim or acceptability in terms of stereotypes such as crowd spares and in this a minimal pair paradigm the typical way to evaluate language models is that you show like a acceptable sentence or a grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is of the model basically puts more probability to the acceptable sentence the current mpp pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the models acceptability throughout the context window and that is what we are trying to do here we're trying to revisit the mpp pipeline by asking the model to evaluate acceptability on longer and longer sequences so that is the approach so what we do is that to simulate these longer sequences we revisit the datasets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those datasets so for example here we have chosen like a typical pair of grammaticality from the bleep data set from the adjunct island case and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from augment island and then we add it as a prefix to both the acceptable query and the unacceptable query so we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability and we can also do the same by choosing sentences from a different subset or a different data set so that is what we call as the mismatch scenario so here the sentences are still coming from relevant datasets but it's not from the same data set that you are evaluating with and we can do the same for unacceptability case finally we can choose sentences from a completely unrelated domain such as wikipedia so this will tell us like whether the models acceptability judgments are actually impacted by any context like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at so how does the model do first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are mostly robust for arbitrary context length we increase the context length toward up to 124 for to max out opt and gpt two models and we saw here in the orange dot line the mpp judgments are relatively stable now what happens when we choose sentences from the same data set so here we are choosing or creating sentences from acceptable and unacceptable domains from the same bleep or syntax gym data set and there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes but when we match the structure that is when we choose the sentences from the same phenomena in bleep person text gym we see a massive increase or a massive decrease in of the mpp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context length and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change it course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation that the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">The speaker's name is Avi Beller.</sample>
    <sample id="233">The paper introduces a method for simultaneous speech translation using existing offline models without retraining or specific architecture. The proposed Encoder-Decoder Attention (EDAT) strategy decides whether to emit partial translations based on the attention mechanism between audio input and textual output. The model predicts translations, and the cross-attention weights determine which words to emit. The results show that EDAT outperforms other strategies applied to offline models in terms of translation quality and latency, while also being the fastest strategy. The paper includes open-source code and models to facilitate reproducibility.</sample>
    <sample id="234">The prompting strategy has a big influence on the performance of the language models for translation.</sample>
    <sample id="235">The authors of the paper are affiliated with Carnegie Mellon University, Language Technologies Institute, TECNICO Lisboa, BAIR Berkeley Artificial Intelligence Research, and Unbabel.</sample>
    <sample id="236">The 5 expert-written instructions are used to represent the input texts, images, instruction and bounding boxes in the same token space.</sample>
    <sample id="237">The authors propose a diagnostic test suite for knowledge integration, introducing a coreference resolution task designed to probe the ability to draw on knowledge available in different sources.</sample>
    <sample id="238">The video presents a new benchmark dataset called MeetingBank, which is designed to address the challenges of developing summarization technologies for different meeting domains. The dataset includes 1366 city council meetings with nearly 7000 instances, containing meeting transcripts, reference summaries, and URLs. The data collection process involves converting audio data to transcripts using speech-to-text APIs, identifying meeting types from meeting websites, and aligning time stamps to obtain segment transcripts. The dataset provides detailed statistics on the number of meetings, meeting duration, tokens per meeting, speakers per meeting, and the year period of meeting collection. It also includes summary instances for each city and average statistics for both meeting and segment levels. The video evaluates the level of abstraction in meeting summaries using coverage and density scores, with most summaries having a coverage score between 0.7 to 0.9. The evaluation also compares the performance of various summarization systems, including extractive and neural models, using metrics like ROUGE, METEOR, and BLEU. The results show that GPT-3 performs exceptionally well in terms of fluency and coherence but less impressively in informativeness and factuality. The video concludes by highlighting the importance of developing new methods of automatic evaluation metrics to better align with human preferences and encourages researchers to use the MeetingBank dataset for further research.</sample>
    <sample id="241">The paper discusses a human-in-the-loop evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment claims. The authors propose a system that integrates human feedback throughout the process, addressing two key deficiencies in existing methods: unrealistic evaluation and lack of human-centricity. The system consists of two main components: claim detection and policy violation verification. Claim detection uses keyword filtering, a T5 model for question answering, and trendiness ranking to identify and rank potential misinformation. Policy violation verification uses a stance classification model to flag tweets supporting unapproved treatments for human review. The evaluation framework assesses the system's ability to detect unapproved treatments before their first appearance in news articles and its effectiveness in identifying policy violations. The results show a 65% decision rate for policy violation detection and an average of 124 policy violations detected per human hour of work. This framework aims to provide a more realistic and practical approach to evaluating misinformation detection systems, emphasizing the importance of human involvement and real-time detection.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">Five</sample>
    <sample id="244">Judges decide cases in law courts.</sample>
    <sample id="245">The paper presents a study on the use of high-agreement workers for summarization tasks. The study found that 80% of workers were high agreement workers, which is 6% out of 200 participants. This method serves as best practice for high agreement annotation at large scale and the lower cost, and can avoid resources with some discarded annotations. In the future, they are going to investigate ways to hire high-quality workers both in terms of high agreement and correctness, and try multiple applications for task languages and platforms. There are also some limitations for this work: first, only English summarization on MTR platform is tested; second, the design tab questions are not natural solutions; third, there is no guarantee for the chain of cracks; finally, we want to thank Google for the experiment funding and thanks for listening.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The paper introduces a new task called knowledge graph-based fact verification (KG), which utilizes knowledge graphs as evidence for natural language claims. Unlike text or table-based cases, KG allows for intuitive and direct connection between evidence and claims, enabling reliable reasoning. This method is practical for modern dialogue systems that communicate with internal knowledge graphs, checking the consistency between user input and the knowledge graph. The authors propose a new dataset, FactKG, which uses Wikipedia as the knowledge graph and includes claims in both written and colloquial styles. The dataset supports five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The authors also introduce a method using a collocational style transfer model and presupposition templates to verify claims. Experimental results show that the KG model outperforms all other baseline models, including claim-only baselines and a model that uses only graph evidence.</sample>
    <sample id="248">Yes, the annotators for NLPositionality are balanced in regard to each demographic.</sample>
    <sample id="249">Perturbations were added to the input sentence while preserving the relevant structure.</sample>
    <sample id="250">Dimensional evaluation is a more precise and reliable strategy for evaluating conversational AI.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China, Microsoft, and Sony AI.</sample>
    <sample id="252">The presentation introduces a research work titled "You Create: Unsupervised Case Retrieval using Event Extraction" by Sai Kiran Tankarella, Abhinav Joshi, Akshat Sharma, and Ashutosh Modi from the Department of Computer Science and Engineering at IIT Kanpur. The work addresses the challenge of retrieving relevant past precedents (cited documents) in legal domains due to the increasing volume of cases. It presents two key contributions: the ILPCR dataset, a new benchmark for case retrieval with 7,700 legal cases and 6.775 average citations per query document, and the You Create pipeline, which uses unsupervised learning techniques and an event-based approach for case retrieval across Indian and Canadian legal systems without requiring law or demographic-specific tuning. The pipeline includes three steps: preprocessing, dependency parsing, and post-processing, and it outperforms baseline models and other event-based models, demonstrating higher retrieval efficiency and lower inference time.</sample>
    <sample id="253">The presentation introduces a research project titled "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media." This project, a collaborative effort between researchers from Mexico and Spain, aims to develop a model that can automatically analyze social media posts to detect signs of mental health issues. The team defines a mental disorder as a psychological syndrome associated with distress and disability affecting thinking, feeling, mood, and behavior. They highlight the vast amount of social media content available for research on mental health difficulties.

The approach involves using domain adaptation to improve the performance of a base language model on a specific task, such as detecting mental health disorders. This is achieved by integrating information from Reddit and mental health resources, including a lexicon to guide the masking process during training. The model is trained to learn social media language and specialize in the mental disorder domain, focusing on important words related to mental health issues.

The results using the BERT dataset show that the proposed model achieves a good balance between precision and recall, outperforming other methods. The model's behavior is analyzed through textual segments it pays more attention to, particularly focusing on words related to mental disorders. The visualization tool used provides an interactive heatmap graph, highlighting the most prominent words and sentences in user posts, which are highly relevant to depression.

In conclusion, the double domain adaptation and guided masking techniques effectively capture signs of mental disorders in social media interactions. Future work plans to explore the application of different lexical resources and clinical data to further enhance the model's accuracy and effectiveness.</sample>
    <sample id="254">The research presented in this presentation focuses on document-level distant relation extraction, aiming to extract relationships among entities in a document. The presentation introduces a framework for uncertainly guided label denoising to improve the quality of distant supervision data (DST). This framework includes a pre-denoising model trained with both DST and human-annotated data to generate pseudo labels. To address the risk of noisy induction caused by pseudo labels, an instance-level uncertainty estimation method is proposed to capture the uncertainty score for overlapping relations. Additionally, a dynamic class uncertainty threshold strategy is designed to filter pseudo labels with high uncertainty. The framework also incorporates a multi-phase training strategy to iteratively re-label DST data. The performance of the framework is evaluated using several strong baselines on two public datasets, demonstrating superior results. The main contributions include the uncertainly guided label denoising framework, an instance-level uncertainty estimation method, a dynamic class uncertainty threshold strategy, and significant performance improvements.</sample>
    <sample id="255">The form of the prompting is important for zero and one shot prompting.</sample>
    <sample id="256">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acsl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship while dissonance is a very common phenomenon we experience in daily decision making they are really rare to find expressed in language among other kinds of discourse relations so why does this matter studying cognitive dissonance can help us understand the effects of dissonance among people track trends and belief values and attitude changes in population high cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups finally cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better to the goal of creating a cognitive dissonance resource we conducted a large-scale annotation of dissonance relations we used a dissonance first approach as seen in the flowchart here tweets were parsed using a patty b parser and pairs of discourse units were annotated according to the guidelines that are described in our paper as can be seen here dissonance was only found in three point five percent of the annotated pairs on collecting around thousand examples of discourse unit pairs we ran training for an initial classifier trained only on forty three examples of dissonance to no surprise the classifier performed not much better than chance given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity to alleviate this we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds lowering the overall annotation costs while improving dissonance detection since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of patty b since these two are closely related to the conception of consonants and dissonance and we call them c e here we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with au c point six two further on iteratively fine-tuning on both tasks we find that fine-tuning of c e tasks followed by further fine-tuning on debate yields a much better zero shot performance thus this is the model that we use to co-start the active learning next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations whereas iterative updates the model by training on the latest set of data collected over the different strategies we found that cumulative performed equal or better than iterative across the board next to improve the number of dissonance examples we use a probability of rare class strategy prc to select mostly the examples that are highly likely to be dissonant by the current model at any round of a l we compare this to the other state-of-the-art state-of-the-art strategies that are commonly used in the community we find that the proposed prc strategy works better than other straight state-of-the-art strategies although the difference is small note that the performance is significantly lower for random on further rounds of a l with two best strategies we improved dissonance classification au c to points seven five which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and cost to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple a l strategy for rare class acquisition and cold starting a l with appropriately designed transfer learning task and helps significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our core dataset and our paper uh feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="257">The authors evaluated four state-of-the-art chat models.</sample>
    <sample id="258">The video introduces a new method for evaluating the quality of text in natural language processing using large language models. The speaker, Chang-Han Chiang from National Taiwan University, explains that their work proposes to use large language models to evaluate text samples by providing them with instructions and allowing them to generate ratings. This approach aims to address the instability and reproducibility issues associated with human evaluations. The video highlights related works that have used large language models for similar tasks but emphasizes that their idea was novel at the time of submission. The motivation behind this work is to find an alternative to human evaluations that can achieve the same goals without the drawbacks. The speaker demonstrates how they used large language models to rate stories generated by GPT-2 or written by humans, focusing on attributes such as grammar, coherence, likability, and relevance. They compared the results with human evaluations conducted by English teachers, showing that some large language models, like BERT and ChatGPT, showed a clear preference for human-written texts. The video concludes by inviting viewers to explore more details about the experiment and its findings in the paper, which addresses questions about agreement between large language models and human evaluators, the impact of instruction changes, and the benefits and drawbacks of using large language model evaluations compared to human evaluations.</sample>
    <sample id="259">The speaker introduces Exemplar, a system designed for cross-lingual semantic parsing in multiple natural languages and various representations. The system aims to translate queries from different languages into multiple semantic representations such as SQL, Lambda Calculus, and others using neural models. The challenge lies in the existing models that are separately proposed and evaluated on limited datasets and applications, often lacking coverage in certain languages or representations. To address this, Exemplar provides a unified dataset with 9 datasets across 15 language families, 5 semantic parsing tasks, 80 semantic representations, and 22 natural languages. It evaluates six settings: translation test, monolingual model, monolingual fine-tune, multilingual model, zero-shot transfer, and few-shot transfer. The results show that encoder-decoder models outperform previous work, especially in multilingual settings, and multilingual language models like Code and Blue are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="260">There are six authors involved in the paper.</sample>
    <sample id="261">A good planner should read scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The paper involves a total of eight authors.</sample>
    <sample id="263">The presentation introduces a new method for mitigating label biases in in-context learning (ICL), a popular paradigm for utilizing large language models. The speaker explains that ICL's instability arises from various design choices, including the choice and order of in-context examples, which introduce biases into the model's predictions. The presentation proposes a novel calibration method to handle all types of biases, including vanilla label bias, context label bias, and domain label bias. The proposed method uses random in-domain words sampled from the task corpus as content-free text to estimate the model's biases on each label name and then calibrates the model's original predictions. Experiments demonstrate that this method significantly improves the performance of ICL on datasets with varying levels of domain label bias, particularly on tasks with larger domain label biases. The method is shown to be more effective than previous calibration attempts, which used single predefined content-free tokens or random English words.</sample>
    <sample id="264">The presentation introduces a novel task called Transferable Audio-Visual Text Generation (TAVT) and proposes a framework to train a model that can rapidly adapt to new multimodal domains with limited labeled data. The framework consists of three components: an Audio-Visual Mapping Network, an Audio-Visual Encoder, and a Language Model Generator. The Audio-Visual Mapping Network maps different visual concepts across domains into a unified audio-visual semantic space, while the Audio-Visual Encoder and Language Model Generator generate tokens for reconstructing audio using visual context as queries. The framework also includes a contrastive learning module to optimize the correlation between the reconstructed audio and force the model to align visual context with audio space. The presentation concludes by presenting the experimental results of the proposed approach on two benchmarks based on MSVD-T and MSVD, showing significant performance improvements over state-of-the-art models in cross-domain settings.</sample>
    <sample id="265">The name of the speaker is Vasudha.</sample>
    <sample id="266">The affiliations of the authors are not mentioned in the given information.</sample>
    <sample id="267">hello everyone my name is yusen zhang from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is a task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc existing cross-lingual semantic parsing models are separately proposed and evaluated on dataset of limited tasks and applications for instance there lacks of coverage on certain natural language the chinese is missing and lack of coverage on certain meaning representations the lambda calculus is missing or they are only evaluated on certain neural model for example there's only one single model to evaluate them so to this end we propose exemplar we provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and many representations it contains nine datasets in various domains five semantic parsing tasks eight meaning representations and 22 natural languages in 15 language families and to better evaluate our benchmark we consider the six settings for training and evaluation the first one is translate test we use google translate api to translate source to the target language then use monolingual model to train and evaluation and for example we trained the english model on english query and during inference we translate the german query using api to english and then use the trained model to predict the sql and we also test monolingual model in this setting the source language is the same as target language for example german to german or english to english we also test monolingual multi-language setting by training model in the models with only ten percent of training data and we test monolingual multi-language model which we train one multi-language model for all languages for example we put the german english chinese queries together to train a multi-language model and during inference we can use this model to to translate german queries or chinese query or etc and we also consider cross-lingual zero shot and few-shot transfer we train on one source language and transfer to another language so during training we train on english query or the combination of english and german few-shot queries to train a multi-language model to and predict the sql output and we also find many interesting results so regarding analysis of monolingual models we evaluate on two groups of models including encoder pdr which stands for multilingual pre-trained encoders with pointer-based decoders such as xlmr plus pdr and bert plus pdr and we also evaluate encoder decoder models which is multilingual pre-trained encoder decoder models such as mbert and mt5 we found that encoder decoder obtains the best performance on all nine datasets and we evaluate on mt5 and example xlmr plus pdr on multi-language setting we found that encoder decoder or encoder pdr can be improved by training in a mixture of various languages and we found it is because most of the major natural languages can obtain performance gain except that english performance drops in seven datasets and only gains in three datasets i think this is known as curse of multilinguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual few-shot transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found the four zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example encoder decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found multilingual language models such as codas and blue are still inadequate for cross-lingual semantic parsing tasks to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative types of multi-lingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">The authors of the paper are affiliated with Emory University and Amazon Alexa AI.</sample>
    <sample id="271">Continous fine-tuning</sample>
    <sample id="272">There are six authors involved in the paper.</sample>
    <sample id="274">Usen Jung</sample>
    <sample id="275">hi I'm Changbing PhD student in University of Washington today I'm presenting our work from pre-training data to language models to downstream tasks tracking the trails of political biases leading to unfair NLP models so language models are trained on large-scale web crawl data political news media are well covered in their pre-training data according to a survey of the C4 corpus we can see that New York Times Los Angeles Times The Guardian Huffington Post Etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications to this end we propose to investigate the political bias propagation pipeline from pre-training data to language models to downstream tasks specifically by asking the following questions first how do we evaluate the political line of language models and what role does pretraining data might have on such political biases secondly how do language models with different political lines actually perform on downstream tasks and whether that might result in fairness issues in NLP applications so specifically we first proposed to prompt language models with different prompt formats using the political questionnaires such as the political compass test this ensures as to do automatic evaluation well grounded in political science literature so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass we can also see that GPT-4 is the most liberal language model of them all and GPT series are generally more socially liberal than Bert series and its variants secondly we aim to investigate to which to which extent the political biases of language models are actually picked up from training data so we conduct a controlled experiment by further pre-training language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings by further pre-training language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift for example for Roberta further fine-tune further trained on the left-leaning Reddit Corpus we can see a substantial liberal shift in terms of its in terms of its political biases and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society so we divide pretraining corpora into pre 45th president of United States and after 45th president of United States we separately pre-train language models on the two different temporal corpora we can see that language models generally had a political leaning that is further away from the center after 2017 so this indicates that language models can also pick up the like polarization in our society so last but not least we evaluate language models with different political leanings on hate speech detection and fake news detection to NLP applications that often involve language models and could have very significant implications so we see that if we investigate the per category performance that is to say if we separate the performance into different demographics or political leanings of news media we can see a pattern that for example for hate speech detection left-leaning language models are better at detecting hate speech targeting socially minority groups however are worse at detecting hate speech targeting more powerful groups in our society and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting at black LGBTQ plus and other minority communities similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political leanings and vice versa this and we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social calendar there are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models for example if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control so this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings so a little bit of discussion we would also like to highlight that we expose the unique dilemma regarding language model political biases it's like between Sisyphus and Crito this so if we do not sanitize the political opinions in language model training data the bias will propagate from pre-training data to language models to downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electrically electric shorty problem okay great I think that's pretty much all I have to dead half for today thank you for your time</sample>
    <sample id="276">The research presented focuses on developing a dataset to evaluate machine translation metrics for Indian languages, specifically Tamil and Marathi. The study aims to address the lack of evaluation metrics for translations in the reverse direction (Indian languages to English) compared to English to Indian translations. The dataset consists of 7000 samples, each with multiple candidate translations generated by seven different translation models. Human annotators evaluated these translations, marking errors, their severity, and providing overall scores. The study compares various evaluation metrics, including QM-MQM, BLEU, METEOR, and Comet, using both overall and category-specific error types. Results show that Comet variants outperform baseline metrics, particularly in accuracy errors, and exhibit higher correlations with human scores. The dataset is publicly available for further research.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The "marked words" method is a way to identify the words that distinguish marked groups from unmarked ones. It draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from that default is linguistically marked.</sample>
    <sample id="279">The authors of the paper are affiliated with the University of Washington.</sample>
    <sample id="280">The paper introduces a novel multi-modal fusion framework for emotion recognition in conversations, addressing the challenges of existing methods. The framework consists of four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. It proposes a novel visual feature extractor called Vis-Net, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. Additionally, it introduces MultiAtt, a multi-modal fusion model based on bi-directional multi-head cross attention layers, to integrate textual, audio, and visual modalities. A sample-weighted focal contrastive loss is introduced to address the difficulty of classifying minority and semantically similar emotion classes. Experimental results demonstrate that MultiEmo achieves state-of-the-art performances on two ERRC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The presentation introduces a study on the context-dependent translation of words in multilingual texts. It begins by explaining how context influences word translation, using examples like "more" in English, which can mean "a spy" or "a birthmark" depending on the preceding sentence. The challenge lies in evaluating how well machine translation models handle context-dependent translations, as traditional metrics like BLEU are insufficient due to the rarity of such translations.

To address this, the researchers propose measuring context usage at different levels (word, sentence, and discourse) using CXMI, an extension of SMI. They analyze TED Talks translated into 14 languages, identifying patterns in context-dependent translations. For instance, they find that dual pronouns in Arabic require context for accurate translation and that certain languages necessitate context for verb form selection.

The study then develops a benchmark for document-level translation, tagging words based on identified discourse phenomena. This benchmark helps evaluate models' performance on context-dependent translations, showing that context-aware models outperform others in handling formalities and lexical cohesion. The research concludes with a comparison of commercial systems, indicating that DeepL is generally more accurate than Google Translate for document-level translation.</sample>
    <sample id="282">The research presented in this paper addresses the task of non-parallel text style transfer at the discourse level, which is crucial for imitating author style. The main challenge lies in imitating the author's linguistic choice at the discourse level, as long text usually involves many complicated linguistic preferences such as discourse structures. To address this issue, the authors propose a generation model named StyleTrans, which learns discourse representations from the source text and combines them with normal style embeddings to generate the target style. They also design a new training objective to reduce the stylistic features from the discourse representations, pulling the representation derived from different texts closer in the latent space. Additionally, they separate the generation into two stages: first, they transfer the source text with style-specific content keywords masked and then generate the whole text by incorporating these keywords explicitly. For the training framework, they use an adversarial training framework with self-reconstruction loss, disentanglement loss, sentence-level loss, and style classifier loss. They collect a new dataset in Chinese and English for this new task and conduct extensive experiments to transfer fairy tales or stories to typical author styles. The results show that StyleTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">The Prague approach</sample>
    <sample id="284">The paper presents a novel fuzzy span mechanism for enhancing universal information extraction (UIE) in spam-based UIE models. It addresses the ambiguity in labeling the golden spam boundary and the mismatch between Transformer feature extraction and information extraction by proposing adaptive attention for span extraction decision. The proposed method models the fuzzy spam boundary as a continuous distribution of correct probability, calculates boundary cross entropy with the golden boundary, and adds KL divergence between predicted boundary and fuzzy spam boundary as supplementary information. The fuzzy span attention is used as a mask function to trim attention distribution, dynamically changing the attention span of the model and linearly decaying the attention distribution on the attention spam boundary. The overall structure of the model includes a fuzzy span attention layer added at the top level to guide the model's decision process without affecting text encoding capability. Experiments on three information extraction tasks show that FSUIE achieves significant performance improvement compared to UIE without fuzzy spam mechanism, especially on small-scale datasets. FSUIE also achieves new state-of-the-art results on datasets like ACE 2004-2005 and ADE, and demonstrates strong generalization capabilities for domain-specific information. The results of ablation study show that FSA improves convergence speed by guiding the model to obtain a reasonable attention distribution, enabling the model to fully utilize annotation information and obtain greater information extraction capability.</sample>
    <sample id="285">The video discusses the challenges of fact error correction in dialogue summarization and proposes a new evaluation framework to address these issues. It highlights the two main types of solutions: introducing factuality-related objects during training or inference, and designing an independent fact error correction model (FEC). The FEC model takes the source document and the model-generated summary as inputs and outputs a corrected summary. However, current FEC models have flaws, such as relying on factuality metrics that may not be reliable and blurring the line between different types of solutions. To improve the evaluation of FEC models, the video introduces a taxonomy of fact errors based on content and form, and a three-step evaluation framework involving alignment, classification, and comparison. The proposed framework shows that training FEC models with reference summaries from dialogue summarization datasets yields better results than using unreliable factuality metrics. Additionally, combining human-annotated data with synthetic data is a promising direction for improving FEC model performance.</sample>
    <sample id="286">The speaker's name is James Finch.</sample>
    <sample id="287">Four</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena are the BLM dataset, the syntax gym dataset, and the Wikipedia dataset.</sample>
    <sample id="289">hello my name is koyo and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick fernandez emily liu andre f t martin's and graham nebic so a lot of translations depend on context for example how would we translate mo in this sentence well if the previous sentence was things could start to get dangerous if the minister finds out then mo refers to a spy but if the previous sentence was could it be anything serious doctor then mo refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can translate cases like this is pretty hard firstly because only a small portion of translations depend on context which makes corpus level metrics like blue unable to capture these translations and some people have suggested targeted evaluation on context dependent translations but these resources only support limited types of context dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we tried to answer these two questions first when does translation require context and second how well do models handle these cases to answer the first question we started by measuring how much a word depends on context-dependent translation in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x you can think of cxmi as the information gain from giving context to the model in this work we extend cxmi to pointwise cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high p six x mi as ones that require context for translation now we analyze words with high p six x mi to look for patterns between these words and we perform our analysis on transcripts of ted talks that have been translated from english to 14 different languages we perform our analysis at three different levels first we look at part of speech tags that have high means p six x mi and this allows us to find for example dual pronouns in arabic that have relatively high p six x mi and this can be explained because english doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into arabic and similarly we find that certain languages also require context when we want to choose the appropriate verb form we then look at vocabulary items that have high p six x mi averaged over all of its different occurrences and this helps us identify cases like the one here where in chinese you need context to translate proper nouns to make sure that you're using the same translation within the document and similarly we find that context is supported to translate in the right formality and finally we look at different individual tokens that have high p six x mi and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure such as ellipsis resolution so now we use our findings from our analysis to design a benchmark for document-level translation for each of the five discourse phenomena we identified we create taggers to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse aware or muda tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the mooda tagger by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context dependent examples that the mooda tagger has identified and finally we use our benchmark as well as other metrics to evaluate different models on the document level machine translation first of all when we use corpus-level metrics so for blue we find that context agnostic models have the best performance but then if we use comet context aware models perform best and if we use word f measure then models with or without context have comparable performance this again demonstrates that it is difficult to determine the best document-level translation system if we use corpus level metrics alone now we use the mooda benchmark to evaluate models and we find that context aware models are significantly more accurate than the models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document-level translation we also compare different commercial systems and our benchmark shows that d bell is usually more accurate than google translate for document-level translation to summarize we perform a data-driven analysis across fourteen language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="290">WSL, FTSW, COSINE, FINE-TUNING, CONTINUOUS-FINE-TUNING</sample>
    <sample id="291">The model is evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="292">hello everyone my name is zhuheng today i'm going to present our paper do conall 2003 named entity taggers still work well in 2023 let's get started our paper investigated the problem of generalization using the named entity recognition task or the n-e-r task we observed that models have been using conall 2003 to develop any er for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data and when we develop new taggers what is needed for good generalization at the same time if we do observe poor generalization what causes the performance drop of these models to investigate these problems we developed the conall plus plus data set this is a data set that we collected from Reuters news from 2020 and then annotated them with the same conall 2003 annotation guidelines we then fine-tuned over 20 models on conall 2003 we evaluated them on both the conall 03 test set and the conall plus plus test set and last but not least we calculated the percentage change in f one to assess the generalization of each model so what is needed for a good generalization through our experiments we found that there are three main ingredients that are needed the first one is the model architecture through our experiments we found that the transformer models normally generalized better to new data the second ingredient is the model size we found that usually larger models lead to better generalization in last but not least we all know that the number of fine-tuning examples directly affects the performance of the downstream task here we also found that more fine-tuning examples actually also leads to better generalization to our next question what causes the performance drop of some models we had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on the new test set the second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than one this means that every unit of improvement that we made on conall 2003 translates to more than one unit improvement on conall plus plus which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goals hand in hand we can't just have one ingredient but throughout the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conall 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conall 2003 taggers still work in 2023 method of mass align and you can also find the code to run this method on your own documents in the paper the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long impart to produce document level simplifications and we also fine-tune the normal base long the normal base import to produce sentence level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="293">hi and I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the alt entities Corpus and my name is javad hosseini and this is a joint work with philip radinsky Sylvia parati and Annie Louise our goal is to understand users language when they want to make a choice consider this alternative question did you mean easy on me or I got a feeling here a user wants to select between one of these two songs um the most obvious thing is to use a direct reference for example by saying the name of the song easy on me or its position the first one but sometimes an indirect reference is more appropriate to have a more natural conversation this could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate or when the user wants to specify a preference here are some example in direct references for example the newer one or the song that's not energetic this is an important problem in conversational systems and also for benchmarking llms entity understanding we're not aware of a public dataset a large-scale public dataset for the task so we collect one using crowd annotation our dataset covers three different domains music books and recipes our dataset collection methodology emphasizes informality using a cartoon completion setup the cartoon has three speech bubbles in the first bubble Bob says remember that song we were listening to yesterday and with that Bob sets the dialogue context in this in the second speech bubble Alice says do you mean easy on me or I got a feeling which is the alternative question and in the third speech bubble Bob uses an indirect reference to select one of these entities for example the newer one we provide the first and second speech bubbles automatically but the third one is filled in by the annotator um the first speech bubble is chosen from a few manual prompts per domain the second one which is the alternative question is generated as follows we always use a simple template do you mean a or B where a and B are samples from Wikipedia here are the different sampling methods we've used when we move higher in the list the entities become more similar to each other and it's usually harder to make the disambiguation the first one is uniform at random the second one is when the entities have similar titles for example two books with the name the return the third one is when they have similar descriptions on Wikipedia and finally when they have similar infoboxes or attributes on Wikipedia for example the same genre or the same artist for a song when we show this alternative uh question to the annotators they know the name of these entities but they don't necessarily know about the entities so what we do is that we show some background knowledge about the two entities for songs we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song and read about each song here's for example the Google search result for the song easy on me for the recipes and books domain we show some background text from Wikipedia for recipes we additionally show their images again from Wikipedia so that the annotators know how they look like then we ask the annotators to pick one of these entities for example here the first one and describe them using three to five indirect referring expressions for example the one with the piano music here are some examples from our data set for example the one without words not the one with the 12 year old 12 year old boy or the fictional one or comes from other by John and so on the alt entities Corpus has six thousand alternative questions across three domains and it has forty two thousand indirect referring expressions results with t5 XL model are summarized below if the language model has access to the exact same background knowledge as the annotators then the accuracy is really high it's around ninety to ninety five percent but this is not realistic if the language model has access to some partially overlapping background knowledge then the accuracy is between eighty two to eighty seven percent which is more realistic for example when the language model retrieves the background knowledge if the language model has access only to entity names then the accuracy is only sixty percent so there's a lot of room for improvement we've also shown that the models are domain generalizable here is a link to our data set thanks</sample>
    <sample id="294">CamemBERT is initially trained on the NACHOS dataset, which consists of medical crawl data from the web.</sample>
    <sample id="295">Adam Siprowski</sample>
    <sample id="296">The video presents a collaborative work between the University of Turin and Amazon Alexa, focusing on irony detection in natural language processing. The researchers developed a corpus called EPIC (English Prospective Irony Corpus), collecting data from social media platforms like Reddit and Twitter over a one and a half-year period. They gathered about 300 short conversations for five varieties of English, using the crowdsourcing platform Prodigy to annotate the data with 74 annotators. Each annotator evaluated 200 texts, and the results showed significant differences in inter-annotator agreement based on various dimensions such as gender, age group, and nationality. The researchers trained perspective-aware models by fine-tuning pre-trained language models on different subsets of the data, noting that these models were less uncertain and more confident in their predictions compared to gold-standard aggregated models. The study also revealed that generations close to each other and annotators from the United Kingdom and Ireland showed higher variations in their perception of irony.</sample>
    <sample id="297">The video discusses the concept of dog whistles, which are coded messages that convey different meanings to different groups. It provides an example of a speech by Senator Josh Holly, who is accused of using a dog whistle term "cosmopolitan elite agenda and experiment" that could be interpreted as anti-Semitic. The video explains how language models like GPT-3 can surface dog whistles but may not accurately identify their covert meanings. It also highlights the importance of understanding dog whistles in NLP linguistics and their role in political influence and evasion of content moderation online. The video concludes with a call to action for further research on dog whistles and their impact on society.</sample>
    <sample id="298">The findings that the performance degrades with larger temporal gap between the train and test data led to the conclusion that the temporal drift is the main cause of performance loss.</sample>
    <sample id="299">The video discusses the development of a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance. The key insight behind this method is that NLI models suffer from poor performance on underrepresented hard training instances with patterns that could indicate the shortcuts in the dominant easy examples. These hard examples are pivotal for ensuring good generalization performance on out-of-distribution samples. Crucially, the loss of hard examples decreases considerably more slowly than the average loss throughout training. Therefore, the aim is to obtain an example weight distribution that places emphasis on the under-represented hard examples. To compute the weight distribution, we propose a minimax training objective between a learner and auxiliary. The learner tries to minimize the loss of the NLI task, whereas the task of the auxiliary is to maximize the learner's loss by generating example weights such that the learner is incentivized to concentrate on regions of the input space where it incurs high losses. This encourages the learner to prioritize learning from under-represented hard examples that counteract the use of shortcuts present in the dominant easy examples. Both models are optimized in an alternating fashion using any standard optimization algorithm, such as stochastic gradient descent. At test time, the learner can make predictions without relying on the auxiliary. Our method does not make any assumptions about the type of shortcuts contained in a dataset; it relies on the learner's own training dynamics to generate example weights. In our paper, we also examine whether the performance improvements transfer to larger models, synthetic shortcut and out-of-domain test sets, what is the effect of pre-training the learner, how small the auxiliary needs to be, and finally we conduct a qualitative evaluation of the learned example weight distribution.</sample>
    <sample id="300">The presentation introduces the concept of interactive dictation, a process where users can dictate and edit documents using their voice in a natural and intuitive manner. The speaker explains that this task involves flexible interleaving of dictation and editing without trigger words, and using natural language utterances to specify edits. The presentation also highlights the limitations of current speech-to-text systems, which primarily support dictation and do not allow for intuitive voice commands. To address these limitations, the researchers designed a new task called interactive dictation, which includes formulating a new task, designing a data collection interface, and building a baseline system. The presentation provides an overview of the four-step procedure involved in interactive dictation, including ASR recognition, segmentation, command extraction, and execution. The researchers also discuss the challenges of collecting data for this task and the need for more work on this task.</sample>
    <sample id="301">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting your work and l positionality characterizing design biases of datasets and models this work was done in collaboration with some folks at the university of washington and um the allen institute for ai namely sebastian santi ronin le bras katharina reinecke and martin sap so let's start off by imagining that you're working for a newspaper and you're skimming through comments under your news article trying to remove toxic content you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're carl jones where perspective api is able to detect correctly toxic instances but that's not really the case for dithia sharma where perspective api is really not as sensitive to offensive terms that are more common in indian contexts this is an example of a design bias where we see systematic performance differences of technology between populations design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspectives that people hold as a result of their demographics identity and life experiences this is a concept widely used in critical studies specifically in feminist and queer academic spaces and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make and so one question that people might ask is do datasets and models have positionality and we're not trying to say that models themselves and datasets themselves have demographic identities and life experiences but they do aggregate judgments and opinions of real people and can thus represent certain positionabilities over others so prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and datasets as well as theoretical definitions of model positionality however these works really don't look at comparing end users with the datasets and models themselves and studying model and dataset positionality is increasingly important as nlp tasks become more subjective and socially oriented and it's challenging to characterize how these positionabilities are skewed because not all decisions are documented and many models are hidden behind apis so to study dataset and model positionality we actually compare the annotations with real users with existing datasets and models we do this through our framework nlp positionality our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we opt to do this over looking at the demographics of original datasets annotators because usually only a few instances annotators annotate each instance and because demographics are rarely collected and shared and so we opt to re-annotate data to get many annotates per instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and datasets using pearson's correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions our framework is largely enabled through lab in the wild an online crowdsourcing platform former hci collaborator in lab in the wild is an online experimentation platform where we can recruit diverse volunteers compared to the platforms like mturk which largely have participants from the us or india and further lab in the wild still is able to get high quality data we host two tasks on lab in the wild one of them being social acceptability and the way this works is that participants will read a situation from the social chemistry dataset and then they'll write how socially acceptable a situation is afterwards to stay engaged in the study they can compare their responses to an ai and others we've then compared these annotations with social chemistry delphi and gpd four we then replicate a very similar setup for the toxicity and hate speech detection task where they'll read an instance from dinah hate and write whether they think it's an instance of hate speech we then compare these annotations with dinah hate perspective api rewriter api hate roberta and gpd four our study in the end amassed over 16,000 annotations from over a thousand annotators from 87 countries so now we're better equipped to answer who do nlp datasets and models align with the most we find that there is positionality in nlp for example we find that datasets and models are most aligned to english speaking countries so for the gpd four social acceptability analysis we find that it's most aligned to confucian and english speaking countries we find that dinah hate is also most aligned to english speaking countries we also find most additional alignment with people who have a college education so for gpd four in the social acceptability task we find that it's most aligned to people with a college education or graduate school education and we find the same for dinah hate where it's most aligned to people with a college education however when models and datasets are aligned to specific populations some are inevitably left behind an example of this is that datasets and models are less aligned to non-binary people compared to the men and woman counterparts we find this in the gpd four social acceptability task as well as the dinah hate task analysis as well so given that there is positionality and alignment in nlp what can we do about it so we have a few recommendations for this first one is keep a record of all relevant design choices throughout the research process and the other is to do nlp research with the lens of perspectivism our third recommendation is to build specialized datasets and models within four specific communities and a good example of this is the musakani initiative i mean we want to emphasize that inclusive nlp isn't just making you know all technologies work for everyone and so that concludes our presentation but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper thank you</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because the alignment between input and output is not given in the training data.</sample>
    <sample id="303">To study the causes of positive stereotypes and essentializing narratives</sample>
    <sample id="304">Ungrammatical sentences</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL), focusing on the necessity of clean validation data for WSL approaches to achieve high performance. It introduces the concept of weak supervision, where data is labeled using weak labeling sources like simple heuristics or low-quality crowdsourcing, which are cheaper but noisier compared to human annotations. The video explains that training neural networks directly on weakly labeled data can lead to memorizing noise and poor generalization. To address this, training algorithms are proposed to robustly train neural networks under label noise.

The video addresses three research questions: whether clean validation data is necessary for WSL, how many clean samples are needed, and whether clean samples should only be used for validation. It finds that recent WSL methods require clean validation samples to work properly, with a large performance drop without them. Increasing the number of clean validation samples improves performance, but fine-tuning on clean data directly achieves better results. Continuous fine-tuning on clean validation samples can easily achieve the performance improvement claimed by previous WSL approaches.

The video concludes with recommendations for future work, including reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, considering continuous fine-tuning as a simple yet strong baseline, and providing open-source code for further exploration.</sample>
    <sample id="306">The research presented in the video focuses on evaluating the entity tracking capabilities of pre-trained language models. The study aims to determine how well these models can track entities and their state changes over time. To address this, the researchers designed a task involving boxes and objects, where the input consists of an initial description of each box's contents. The task requires the model to predict the contents of each box after multiple state-changing operations, such as moving or adding objects. The experiment was conducted using FLAN-T5 and GPT-3 and GPT-3.5 models, with results showing that most models struggle with non-trivial tracking, except for GPT-3.5, which exhibits strong entity tracking behavior. The findings suggest that pre-training on code is crucial for developing entity tracking capabilities in language models. Additionally, smaller models like T5 base can be fine-tuned to perform entity tracking, but randomly initialized models cannot learn this task even with direct supervision.</sample>
    <sample id="307">The authors used public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering to evaluate their models.</sample>
    <sample id="308">The presentation, given by Jenny T. Liang at Carnegie Mellon University, focuses on the concept of "NL Positionality," which characterizes design biases in datasets and models used in natural language processing (NLP). The speaker explains that these biases can lead to systematic performance differences in technology across different populations. To address this issue, the presentation introduces a framework called NL Positionality, which involves re-annotating datasets with diverse annotators and comparing their annotations to existing datasets and models using Pearson's correlation score. This framework is designed to highlight how NLP tasks become more subjective and socially oriented. The study, conducted through Lab in the Wild, an online crowdsourcing platform, involved over 16,000 annotations from 1,000 annotators from 87 countries. The results showed significant alignment with English-speaking countries and individuals with higher educational backgrounds, highlighting the need for more inclusive and diverse NLP research practices.</sample>
    <sample id="309">ABC eval</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">The authors of the paper are affiliated with the University of Paderborn.</sample>
    <sample id="312">MultiInstruct is the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories.</sample>
    <sample id="313">Three authors are involved in the paper.</sample>
    <sample id="314">Binary coordination is the coordination of two verbs.</sample>
    <sample id="315">The prompts used in this study were on average 10 words long.</sample>
    <sample id="316">The findings suggest that smaller models can support larger models when properly trained on suitable datasets, indicating that they can be valuable resources for advancing research in language planning.</sample>
    <sample id="317">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multi-set of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The permutation model works by determining which multi-set token to put in every position, starting with the first output position and then jumping to another multi-set token to determine the next token in the output. This process continues until every token from the first stage has been visited exactly once. The paper also conducted a detailed and in-depth analysis of this phenomenon, observing that the perplexity computed on test format inputs using models like GPT-3 was generally higher than that of code format inputs using models like CodeT5, indicating that transforming information extraction into a code generation task and using code pre-training language models can better align with the information extraction task itself. Additionally, when decoding with GPT-3 and test format prompts, there were many structural errors, whereas when using CodeD and code format prompts, such errors were almost non-existent.</sample>
    <sample id="319">The work investigates the impact of pre-training strategies on the performance of the proposed system. Specifically, it compares the performance of models trained from scratch with those trained using different pre-training strategies, such as fine-tuning on a subset of the Natsos dataset, training on anonymized data from the University Hospital of Toulouse, and using an English biomedical model for pre-training. The results show that pre-training strategies can improve the performance of the proposed system, but the optimal strategy depends on the specific task and the amount of data available.</sample>
    <sample id="320">The factor of overfitting due to test reuse is not observed.</sample>
    <sample id="321">The quality of the simplification was evaluated by analyzing the type of simplification, such as lexical simplification, structural simplification, and overall level of simplification.</sample>
    <sample id="322">The text describes a presentation on the topic of how text classifiers learn about morality. It explains that human morality is an internal compass that helps us determine whether an action or concept is morally right or wrong, and it is essential for language models to understand and recognize morality in text. The presentation highlights that current approaches often treat morality as a singular scale between immoral and moral, which can hide the pluralistic nature of different interpretations of morality. The presentation introduces the Moral Foundation Theory, which posits that there are five different ways humans perceive morality, each with its own moral foundation. The presentation also discusses the use of explainable AI techniques to understand how language models process morality in text, particularly focusing on the differences in moral expression across various domains. The presentation concludes by emphasizing the importance of recognizing these differences to avoid misunderstandings of morality in language models.</sample>
    <sample id="323">The paper introduces a dynamic heterogeneous graph reasoning method for commonsense question answering (CQA). It addresses the challenge of retrieving relevant knowledge from external sources and combines language models and knowledge bases to solve CQA tasks. The proposed method, HGLK, builds an HKG based on a mutable knowledge base using a two-stage training strategy and KRL. It removes subwords that make up the predicate entity, retrieves the presence of key entities in WordNet and Wikidata, and dynamically removes entities with weak relevance to the QA context. The method also incorporates HKG path information into the QA context and uses MLP to predict the final answer probability. Experiments on Conscience Q&amp;A and OpenBook Q&amp;A demonstrate that HGLK achieves better results than other methods.</sample>
    <sample id="324">Yes, language models have different political biases.</sample>
    <sample id="326">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="327">The presentation introduces a new vision-language learning model called Manager Tower, which aims to improve the performance of cross-modal language models by aggregating insights from pre-trained unimodal experts at different levels. The model uses managers in each cross-modal layer to adaptively combine the insights of pre-trained unimodal experts, allowing for more effective exploitation of different levels of unimodal semantic knowledge. The presentation highlights that Manager Tower achieves superior performance on various downstream tasks with only 4 million images for pre-training and outperforms the Bridge Tower and Meter Bridge Tower models. The presentation also discusses the visualization of average aggregation weights of textual or visual managers in each cross-modal layer over all samples in the WikiVisio dataset, showing distinct trends between static and adaptive managers.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">The paper presents a method for generating structured pseudo-labels to improve zero-shot video sentence localization. The authors address the challenge of training video localization models without manual annotation, which is costly and inefficient. They propose a two-step process: first, using a pretrained image caption model to generate complex free-form pseudo-queries based on video frames; second, using a pretrained model to match the relevance between video frames and pseudo-queries to generate pseudo-events. These pseudo-events are then ranked by event quality to ensure high relevance with the query and low relevance with other events. The authors also introduce a strategy to reduce the influence of noisy samples during model training. They evaluate their method on two datasets and show that it outperforms existing methods in terms of accuracy and robustness against label noise.</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative across the board.</sample>
    <sample id="331">Sara Papi</sample>
    <sample id="332">The data for the MuDa benchmark was taken from TED Talks that had been translated into 14 different languages.</sample>
    <sample id="333">The paper presents a novel training framework for machine translation (MT) models, aiming to enhance their generalization ability and performance. The proposed framework, named INK, injects knowledge into MT models by aligning contextualized representations with token embeddings and the same target token. This alignment process is guided by a key-value data store that saves representations and their corresponding target tokens. The framework iteratively refines the representation space using a combined learning objective, achieving significant performance improvements on the WMT'19 German-to-English news translation task. Experimental results show that INK achieves an average gain of 1.99 BLEU score and 1.00 BLEU score compared to the state-of-the-art KNN system, while also reducing memory usage and improving inference speed.</sample>
    <sample id="334">hi my name is adam sripovsky and this talk is about the dependency structure of coordination as you may know that different dependency structures assumed by different theories and corpus approaches so for example in the universal dependencies the structure of the coordinate coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordinate structure so in this case lisa a similar approach is assumed in igor miltruk's meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are asymmetric right they they single out one of the conjuncts now there also symmetric approaches to coordinate coordinate structures such as the prague approach the conjunction headed approach assumed in prague dependency trees banks where coordinate structures are headed by the conjunction so we get dependencies from end to all the conjuncts and finally this also a multi headed approach that's used for example in the de catts on's word grammar where so to say all conjuncts are heads of the coordinate structures so we get dependencies from the governor here loves to all conduct separately these are barton making now the main paper is to produce an novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two okay the argument is based on the principle of dependency length minimization that are explained on the basis of these examples so in english as you might as you might know a direct objects prefer to be close to the verb while adjuncts maybe further away right so march read it yesterday is fine because the direct object it is closed to the verb while march read yesterday it is much worse right because here between the verb and the direct object there's an adjunct yesterday however this effect may be ameliorated when when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about the bees yesterday i is okay in the way instead of it we have this long np but it's also okay to say march read yesterday this absolutely fascinating book about bees so the reason here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb it satisfies the principle of dependency length minimization which says that shorter shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have a dependency from red to the adjunct of length seven measured in words and from red to book of length four so together it's eleven when you move when you swap these two constituents the sum of these two dependencies becomes six right so instead of 11 six much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of pent of the pentry bank and see the paper why wouldn't you use university dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with lengths the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of of the left short conjuncts but what's novel in in this paper is we that we observed that this tendency only occurs when the governor on the left are absent right so the governor on the left in this example i saw bart and lisa so it's a governor is on the left it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts however when the governor is on the right as here left governs the coordination tender net this effect disappears so we showed that by measuring length in characters that the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against us asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one language and then using it to predict the output in another language.</sample>
    <sample id="337">The research presented in this speech focuses on developing a neural approach to handle out-of-vocabulary (OOV) words by leveraging word formation and association. The approach introduces a word relationship graph that imitates the lexical rules of word formation and association, where each word or word piece acts as a node in the graph, and its corresponding word embedding serves as the node feature. To address the issue of assigning node attributes to OOV nodes, a self-attention network is utilized to assign attributes based on the context of the OOV words. Two levels of graph attention networks are applied to concatenate and fuse the initial input with the hidden embedding of each layer, resulting in a node-level representation. A readout block is incorporated to capture the whole graph information and summarize word formation. Contrastive learning is applied in the loss function to encourage similarity between positive samples from the graph, such as true homophones or synonyms of the OOV word itself, and push negative samples further apart. Extensive experiments demonstrate that the model achieves state-of-the-art performance on both intrinsic and extrinsic tasks, making it effective for learning OOV words by word formation.</sample>
    <sample id="338">The speaker introduces a collaborative research project aimed at evaluating the quality of human-generated explanations for machine learning models. The project involves researchers from various institutions and focuses on three main areas: motivation, related works, and contributions divided into three sections (unified structure, preliminary experiments, and evaluation). The team has developed a unified data structure to convert different tasks into a common format, allowing for a more comprehensive analysis of explanation quality across various datasets. They have conducted in-depth experiments using five large-scale datasets to assess the utility of explanations during model fine-tuning and inference. The results show that while some explanations may not be optimal according to traditional metrics, they can still significantly improve model performance when used in fine-tuning. The team proposes a new evaluation metric called "true" that extends the Simulatability score to better reflect the helpfulness of explanations during fine-tuning. This metric is evaluated across multiple datasets and models, demonstrating its effectiveness in ranking dataset qualities and task-specific characteristics.</sample>
    <sample id="339">Saarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="340">The presentation introduces ParaAMR, a large-scale syntactically diverse paraphrase dataset created through AMR back-translation. Kuan-Hao Huang from UCLA presents this joint work with Varun Iyer, Anoop Kumar, Kai-Wei Chang, and Aram Aramyan. The goal is to generate high-quality paraphrases for NLP applications like question answering and chatbots. Existing datasets are limited in scale and lack syntactic diversity. ParaAMR uses AMR graphs to capture sentence semantics, randomly sampling new root nodes and modifying edges to generate diverse paraphrases while preserving semantic similarity. With around 15 million source sentences and 6.9 paraphrases per source sentence, ParaAMR outperforms existing datasets in syntactic diversity while maintaining semantic similarity. It benefits NLP applications such as sentence embeddings, synthetic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">The authors use average latency and computational latency as measures of latency.</sample>
    <sample id="342">The presentation introduces a large-scale personalized dialogue dataset constructed from live streaming, focusing on video sources. The dataset includes over 10 million hours of video data, with detailed annotations and a unique automatic construction method. It addresses the challenges of existing datasets by providing a large-scale, Chinese multi-party dialogue dataset with detailed speaker information. The dataset is used to evaluate the performance of dialogue models on two tasks: response modeling and address recognition. Experimental results show that the dataset outperforms existing datasets in terms of informativeness and effectiveness in generating personalized responses. The presentation also discusses future work on the efficient transfer of language models for live chat.</sample>
    <sample id="344">Trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process.</sample>
    <sample id="345">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output. After the first step, all the right tokens are identified but not ordered. In the second step, another model predicts a permutation to put them into the right order. The permutation model is flexible and expressive, allowing for systematic correspondences between input and output. The experimental results show that the method outperforms other treeless models on the CoNLL benchmark by a large margin on generalization to deeper recursion. However, some other kinds of structural generalization remain challenging. The paper addresses technical challenges such as alignment between input and output, multiple permutations consistent with the data, and finding the highest scoring permutation.</sample>
    <sample id="346">The authors of the paper are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="348">The paper "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models" by Myra Cheng, Esin Durmus, and Dan Jurafsky addresses the prevalence of social bias and stereotypes in large language models (LLMs). The authors highlight the limitations of existing measures, such as hand-curated datasets and broad associations, and propose a method that leverages the ability of instruction-tuned LLMs to generate personas based on prompts. They use a study-inspired approach to generate personas for different demographic groups and apply the marked words method to identify specific stereotypes. The results show that generated personas contain more stereotypes than human-written ones, with patterns reflecting harmful narratives and intersectionality. The paper concludes with recommendations for model owners, including addressing positive stereotypes, using intersectional lens, and increasing transparency about bias mitigation methods.</sample>
    <sample id="350">The presentation discusses the concept of "superhuman performance" in natural language processing (NLP) and evaluates how reliably leaderboards compare models to humans. It highlights the challenges in NLP, such as generalization issues, vulnerability to adversarial attacks, reliance on spurious patterns, lack of sensitivity to basic perturbations, and over-sensitivity to less important perturbations. The analysis focuses on two popular benchmarks: SuperGLUE and Squad, showing that while systems outperform humans in some tasks, there are significant errors due to differences in evaluation sets and ground truth annotations. The presentation argues that comparing the scores of the best systems with those of the best possible humans is necessary for more meaningful evaluations. It also emphasizes the importance of considering factors like annotator bias and task pay rates when constructing benchmarks.</sample>
    <sample id="351">The paper investigates the problem of generalization using the named entity recognition task (NER) and explores whether models developed using CoNLL-2003 can generalize to modern data. The authors found that three main ingredients are needed for good generalization: model architecture, model size, and fine-tuning examples. They also identified temporal drift as the main cause of performance drop, not adaptive overfitting. The paper concludes that CoNLL-2003 taggers still work well in 2023, but further research is needed to improve generalization.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">The presentation introduces a research paper titled "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, Andre F. T. Martin, and Irina Gurevich. The paper addresses the challenge of input under-specification in code generation and program synthesis, particularly when only a classifier is mentioned without specific model specifications. To tackle this issue, the authors propose an interactive approach where clarification questions are used to gather more specifications. They focus on clarifying operation-level specifications and propose a method to create a synthetic dataset with clarifications on key operations. The paper also outlines a pipeline for code generation by asking clarification questions, including data set creation, identifying key operations, and generating clarification questions. The authors highlight the importance of distinguishing aligned operations from those with similar names and discuss common errors that reflect challenges in improving their method. The presentation concludes with an analysis of the results, showing that the proposed method effectively identifies missing key operations and improves code generation, though it still underperforms compared to models trained solely on NLP code.</sample>
    <sample id="354">2021</sample>
    <sample id="356">The authors of the paper are affiliated with the University of Amsterdam, Saarland University, and the University of Groningen.</sample>
    <sample id="357">Siu Yu Yan</sample>
    <sample id="358">Five</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="360">hello everyone my name is yin and my colleague zhiyang and i will be presenting our research on multi instruct improving multi-modal zero-shot learning via instruction tuning so with the advances in large language models many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter and data efficient way recently many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions however most previous works on instruction tuning focuses on improving the zero-shot performance on language only tasks while computer vision and multi-modal tasks have been left out therefore in this work we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks additionally at the time of our research we discovered a considerable discrepancy in availability of instruction dataset between nlp and multi-modal there exists more than 1,000 and 600 language only instruction tasks however there is no large-scale publicly available multi-modal instruction task therefore this motivates us to build a multi-modal instruction tuning dataset here we present multi instruct the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories these tasks are derived from 21 existing open source datasets and each task is equipped with five expert-written instructions for investigating multi-modal instruction tuning on our proposed dataset we take ofa a unified multi-modal pre-trained model as our base model ofa uses a unified vocabulary for language image tokens and the coordinate of a bounding box here we show some example instances from our multi instruct dataset to unify the processing of various input and output data type we follow the method from ofa and formulate all the tasks in a unified sequence-to-sequence format in which the input text images instruction and bounding boxes are represented in the same token space okay now i'm going to talk about multi-modal instruction tuning so for the training dataset we use 53 tasks from nlp group for training and we sample 10 thousand instances per task for testing we reserve the entire common sense reasoning group for testing and we select additional five tasks from wikia and the miscellaneous group we use all the instances in the test split for each task uh in addition we randomly sample 20 tasks from the test split of nature instruction as the unseen task for nlp so we use a pre-trained ofa large model as a base model during training we mix all the instances for all the tasks each instance is randomly combined with one of its five instruction templates so during tests for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment we report the mean and max performance and the standard deviation of the performance across all five experiments if the task is a multi-modal classification task we report accuracy if it's a multi-modal generation task we report rouge l for nlp tasks we report rouge l as well we also introduced a additional evaluation metric called sensitivity so this measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction here is our main results as we can see instruction tuning can significantly improve or is ofa's performance on seen multi-modal tasks also transfer learning from nature instruction dataset can benefit instruction tuning here we can see as the amount of task increases the model achieve better performance and in the meantime lower sensitivity so we also do one experiment we use one instruction versus five instruction as we can see using more instruction can improve the model's overall performance and reduce its sensitivity a lot so this shows the effect of different fine-tuning strategy on the model's sensitivity as we can see by transfer learning from nature instruction dataset the model can uh achieve much better sensitivity compared to the original ofa model we also can see transfer learning from nature instruction dataset can help ofa to achieve much better performance on the nature instruct dataset so overall we propose the first large-scale multi-modal instruction tuning dataset which significantly improves the zero-shot capability of ofa and we explore different transfer learning technique and show their benefits we design a new metric called sensitivity so one more thing we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them so this is the qur code for our data and model thank you</sample>
    <sample id="361">The presentation introduces CounterComp, a method for enhancing compositional generalization in multi-step quantitative reasoning tasks. It focuses on the question-answering task using financial tables, where users can ask various questions like calculating net change in revenue from 2019 to 2020. The challenge lies in handling multi-step arithmetic operations, which state-of-the-art neural models often struggle with due to memorizing spurious patterns. CounterComp addresses this by identifying and utilizing counterfactual scenarios from the training data. These scenarios help the model learn to attend to meaningful tokens that correlate with specific operations in the output. By adding an auxiliary metric learning loss with dynamic margins, the model's performance improves significantly, especially for tasks involving more than two steps. This method enhances the model's ability to generalize to out-of-distribution samples and improves its focus on meaningful operational terms during training.</sample>
  </task>
</testset>