<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="en">
    <sample id="0">Large-scale web crawls and political news media.</sample>
    <sample id="1">Megill University, Meila and Microsoft Research</sample>
    <sample id="2">The presentation introduces a new multi-modal pre-training model called LayoutMask, which addresses the challenge of reading order issues in visualized document understanding. Unlike existing models that use global word segmentation for reading order, LayoutMask employs local word segmentation and infers global reading orders by jointly using 1D and 2D positioning information. The model incorporates two novel masking strategies: whole word masking and layout-aware masking, which are designed to enhance text layout interactions during pre-training. Additionally, LayoutMask introduces a new pre-training objective called mask position modeling, which helps the model recover relative 2D positions during training. The presentation compares the performance of LayoutMask with different positioning strategies on FD and SRIE datasets, demonstrating that LayoutMask outperforms other methods, especially in handling complex documents like entity total.</sample>
    <sample id="4">The speaker's name is Kayo Yin.</sample>
    <sample id="5">They used the t5-xl model to obtain the 82%-87% accuracy.</sample>
    <sample id="6">The presentation introduces a research paper titled "Towards Unifying Multilingual and Cross-Lingual Summarization" by Jian Wang, Fandong Meng, Duo Zheng, Yunlong Liang, Zhixu Li, Jianfeng Qu, and Jie Zhou from Soochow University, WeChat AI, Beijing University of Posts and Telecommunications, and Fudan University. The paper proposes a new approach called Many-to-Many Summarization (MTM), which aims to unify multilingual summarization and cross-lingual summarization into a single model capable of summarizing documents in any source language and generating summaries in any target language. The authors conduct preliminary studies to analyze the effectiveness of MTM compared to traditional methods. They propose a three-stage training process for MTM: 1) Pre-training with parallel corpora, 2) Cross-lingual pre-training with noisy parallel sentences, and 3) Task-specific fine-tuning using pseudo Many-to-Many summarization samples. Experimental results show that MTM outperforms previous models like MBART50 and MTL5. The presentation also includes a figure illustrating the differences between multilingual summarization, cross-lingual summarization, and MTM.</sample>
    <sample id="7">Yes, they do.</sample>
    <sample id="8">The novelty of the proposed human evaluation method is that it explicitly annotates whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself.</sample>
    <sample id="9">Clean validation samples</sample>
    <sample id="10">There is a lot of room for improvement.</sample>
    <sample id="11">The video discusses the capabilities of large language models in understanding and generating humor. It highlights examples of language models like ChatGPT and Google's PaLM that can generate jokes and explain them, showcasing their ability to understand humor. However, it also points out that these models often struggle with more complex humor, such as knock-knock jokes involving absurd elements.

The video then introduces the New Yorker Caption Contest, a popular weekly cartoon captioning contest where readers submit captions for cartoons published in The New Yorker. The contest is used to test the humor understanding capabilities of language models through three tasks: matching, quality ranking, and explanation generation.

The video explains that the matching task involves presenting models with five choices of captions, only one of which was truly written about a given cartoon. The quality ranking task presents two captions judged by human raters for quality, and the explanation generation task prompts the model to generate a 2-4 sentence explanation of why a joke is funny.

The video concludes by discussing the performance of various language models on these tasks, highlighting the significant gap between human performance and that of models, especially in the explanation generation task.</sample>
    <sample id="12">5</sample>
    <sample id="13">Daniel Rotem is presenting his work on adaptive inference in low resource settings. Adaptive inference is a method for reducing the inference time of large language models by using low capacity models for easy samples, which reduces the average inference cost. Two common methods are multimodal and early exit. Multimodal involves multiple models stored together, each with a classifier at the end, trained separately and run sequentially until a classifier decides to halt computation. Early exit involves fitting multiple classifiers to the model following intermediate transformer layers, all trained together and run through the model until a classifier decides to halt. The pros and cons of each method were discussed, including the versatility and overhead of multimodal, and the faster inference and memory efficiency of early exit. However, model parameters are shared among all classifiers, leading to lower performance due to conflicting gradients. To test this hypothesis, individual early exit models were compared with separate multimodal classifiers, showing that multimodal classifiers outperformed those of early exit by an average of 2.3%. The gap was largest for the earliest classifiers, with an average of 5.2%. The speed-accuracy trade-off was also measured, showing that early exit outperforms multimodal for later classifiers. Based on these findings, the Sweet method was introduced, which separates weights in early exit transformers to avoid conflicting gradients. The results showed that Sweet closes most of the gap between early exit and multimodal, but negatively affects later classifiers in some cases. Overall, the work shows the existence of conflicting gradients in early exit training processes and introduces the Sweet method as a novel fine-tuning method for early exit architectures.</sample>
    <sample id="15">Three</sample>
    <sample id="16">Bible texts are simplified more than news texts or language learner texts.</sample>
    <sample id="17">The speaker introduces a method for extracting semantic relations between entities in text and image data. The method involves representing text and images as visual and textual graphs, merging them into a unified cross-modal graph (CMG), and screening the initial CMG structures by filtering nodes and adjusting edges. A graph information bottleneck principle is used to guide optimization, and multi-modal topic features are integrated using an attention operation to enhance the overall context. Experiments on the MRDA dataset show that this method outperforms text-based methods and achieves significant improvements over existing models.</sample>
    <sample id="18">Salt and pepper, not pepper and salt</sample>
    <sample id="19">The speaker introduces a survey on efficient open-domain question answering, focusing on the two-stage framework proposed by Dandan Chen in 2017. The first stage uses retrieval to fetch relevant documents from Wikipedia Corpus, and the second stage employs a reader to understand the question and retrieve the necessary information to answer it. The retrieval process involves a question encoder and a document encoder, which preprocesses Wikipedia Corpus into an index file for faster searching. The speaker highlights challenges such as the large size of the Wikipedia Corpus (26 million documents, 20 GB), the index file (65 GB), and the use of multiple language models with millions of parameters. These factors make real-time applications and resource-constrained devices challenging. The motivation is to achieve efficient systems with smaller memory costs, faster inference, and comparable performance. The speaker summarizes techniques to address these challenges, including approximate nearest neighbor search, skipping rate, document filtering, and model compression. The analysis shows that retrieval-only systems create large indexes but are faster, while generative-only systems lack indexes but are always large models. The speaker concludes with future work directions, such as deploying open-domain question answering systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Yes, the models are available for use in research.</sample>
    <sample id="21">DEplain-apa contains news texts.</sample>
    <sample id="22">The factors that lead to good generalization are the model architecture, the model size, and more fine-tuning examples.</sample>
    <sample id="23">The video discusses the challenges faced by text-image models in accurately rendering visual text. It highlights that while these models have made significant progress in generating high-quality images, they often struggle with representing text correctly. The video explains that this issue is primarily due to the way text encoders, such as T5 and BERT, process input text. These encoders use subword tokenization, which means they break down words into smaller chunks rather than individual letters. This can lead to difficulties in decomposing atomic subword vocabulary items into individual letters, resulting in poor text rendering.

To address this problem, the video introduces a new strategy involving the BitT5 model, which receives individual bits of the input string instead of subword tokens. This allows for better access to spelling information, enabling more accurate text rendering. The video also mentions the development of a new benchmark, WikiSpell, specifically designed for text-only models, and a draw text benchmark for text-to-image models. These benchmarks help evaluate and improve the spelling abilities of text-rendering models.</sample>
    <sample id="24">By measuring length in characters</sample>
    <sample id="25">The experiments were designed to study the effect of the governorâ€™s position by measuring length in characters, syllables, and words. The results showed that when the governor is on the left or absent, the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words. However, when the governor is on the right, this tendency disappears.</sample>
    <sample id="26">The baseline classifier performs not much better than chance.</sample>
    <sample id="27">The paper involves five authors.</sample>
    <sample id="28">Bob and Alice</sample>
    <sample id="29">Context-aware MT models improve over context-agnostic ones in discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="30">The audio content is a detailed presentation of a research paper titled "LLM-Blender: Ensembling LLMs with Pairwise Ranking &amp; Generative Fusion." The presentation introduces the LLM-Blender framework, which is designed to improve the performance of large language models (LLMs) by using a two-stage process. In the first stage, multiple LLMs are run on a given input to generate outputs. These outputs are then ranked using a pairwise ranking model called PairRanker, which compares each pair of candidates to determine which one is better for the input. The top-ranked candidates are selected and used as inputs for a generative fusion model in the second stage. This fusion model combines the outputs from the top candidates to produce a final output. The presentation highlights the effectiveness of the LLM-Blender framework through experiments and results, demonstrating that it can significantly outperform individual LLMs and other ensemble learning methods.</sample>
    <sample id="31">Johns Hopkins University, Purdue University, and MIT.</sample>
    <sample id="32">Hi, my name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations. This is joint work with my advisers Alexander Koller and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this. As usual we have a training set of utterances in this case the girl slept and Mary knew that the girl slept. These utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example the model has seen shallower recursion during training and is tested on an example with deeper recursion. Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically this involves considerable formalism-specific pre-processing of the logical forms for example to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures. In this paper we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first time we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First we tag each input token with an unordered multiset of tokens that will appear in the output. After the first step we have all the right tokens but they're not ordered. That's why in the second step we use another model to predict a permutation to put them into the right order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually our permutation model works roughly like this. We go from left to right over the output and determine which multiset token to put in every position. For the first output position we simply select one as highlighted in red. Then we jump to the next multiset token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multiset token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results here we compare our method with other treeless models on the CoNLL benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging though. In our paper we solve a couple of interesting technical challenges. First of all the alignment between input and output is not given in the training data. As a consequence for a given token we don't know which multiset it came from which poses a challenge for training. In addition sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the traveling salesman problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges please have a look at our paper or come to our poster.</sample>
    <sample id="33">The framework quantifies positionality by comparing the annotations with real users to existing datasets and models using Pearson's correlation score.</sample>
    <sample id="34">The speaker introduces a collaborative work called "CREST," which stands for a joint framework for rationalization and counterfactual text generation. The work involves four authors: Marcos Trevizo, Alexis Ross, Nuno M. Guerrero, and Andre F. T. Martins. The speaker explains that CREST combines selective rationalization and counterfactual text generation to leverage their complementary strengths. The first component of CREST generates counterfactuals by masking the original input and using a masked language model to fill in the masked spans with new tokens. The quality of the generated counterfactuals is evaluated using both automatic metrics and human evaluation. The speaker mentions an interesting approach of using CREST for data augmentation and proposes an alternative method that performs rationalization with both factual and counterfactual examples. The speaker also discusses the interpretability of the generated rationales in three dimensions: falsifiability, forward simulability, and counterfactual simulability. Overall, the speaker summarizes that CREST produces valid, fluent, and diverse counterfactuals in a controllable way, leading to plausible explanations that focus on the contrastive parts of the input.</sample>
    <sample id="35">Hello, I am Dawei, a PhD student at Saarland University in Germany. In this video, I would like to present our recent work, weaker than you think, or critical look at weakly supervised learning. This is joint work with Xiaoyu Shen, Marius Mosbach and Andreas Stephan and Dietrich Klakow. I'd like to begin with a brief introduction to weak supervision and weakly supervised learning. In weak supervision, we do not manually label the data. Instead, we label the data using weak labeling sources such as simple heuristics rules, knowledge bases or low-quality crowdsourcing, as you illustrated in the figure on the right. When compared to human annotations, the weak annotations are much cheaper yet they are also noisy, meaning that a certain amount of the annotations are incorrect. If we directly train neural networks on weakly labeled data, the neural networks tend to memorize the label noise and do not generalize. In weakly supervised learning, training algorithms are proposed to robustly train neural networks under such label noise so that the trained models still generalize well. In recent works in WSL, so WSL stands for weakly supervised learning, a common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets. Technically, this claim is not wrong but there's a catch which is that people do assume that there is an additional clean validation set available for model selection. We caught up on this problem setting because this implies that additional manual annotations are required in weakly supervised learning. But like an elephant in the room, this necessity is often overlooked. The abbreviation adopted is us to ask three research questions. First, is clean validation data necessary for WSL? Or can we maybe use a noisy validation set instead? Second, if clean data is required or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation or are there better ways to utilize them? We addressed these research questions in our work and our findings are as follows. First, we find that interestingly recent WSL methods indeed require clean validation samples to work properly, otherwise there is a large performance drop, as shown in this figure. If there are no clean validation samples, then the trained models cannot generalize beyond the original weak labels, meaning that the training is pointless. This indicates that WSL approaches actually require cleanly labeled data to work properly and the annotation cost for obtaining clean validation samples should not be overlooked. Our second finding is that increasing the number of clean validation samples will help WSL approaches to achieve better performance, as shown in the figure on the left. Typically, we only need 20 samples per class to attain high performance. But that's not the end of the story because if we either way decide to access clean samples, then training on them directly will even achieve better performance. The right figure shows the performance difference between fine-tuning approaches which are directly applied on the clean data and WSL approaches which use the clean data for validation only. As we can see, if we have 10 samples per class, direct fine-tuning starts to beat WSL approaches. Finally, the performance improvement claimed in previous WSL approaches can be easily achieved by allowing to continue fine-tuning on the clean validation samples. As we can see from the figures, the vanilla model termed FTW initially underperforms more complicated WSL methods like cosine. However, if we allow to continue fine-tuning on the clean samples, then FTW performs equally well as other methods. So in practice, there's no reason to choose more complex WSL methods which require more computation time and disk space. To summarize, we showed that recent WSL approaches require clean, manually annotated samples for them to work properly. Their performance gain and practicality are heavily overestimated. Our concrete recommendations for future work are as follows. First, report the model selection criteria, for example, report if the model selection is done well with clean validation samples. Second, WSL approaches should be compared with few-shot learning baselines as opposed work on clean samples. Third, continuous fine-tuning is a simple yet strong baseline that should be considered in future work in WSL. Finally, we have open-sourced our code. You can find it via the QR code on this slide. Please feel free to check it out. Thank you and enjoy the conference.</sample>
    <sample id="36">The speaker introduces a study on learning language-specific layers for multilingual machine translation, conducted with Robin Schmidt, Yi-Hsiu Liao, and Stefan Peteitz. The study aims to enhance the capacity per language while maintaining constant inference costs. The proposed solution is Language-Specific Layers (LSLs), which involve using one regular Transformer layer per language. At training and inference time, the correct sub-layer is selected based on the language. This approach allows for keeping inference costs constant by only calling the relevant sub-layer during inference. The placement of LSLs within the model was optimized through an iterative process, focusing on the encoder layers. The results show significant improvements over baseline models and language adapters, particularly for low-resource languages, with statistically significant improvements in 84 out of 90 translation directions.</sample>
    <sample id="37">The finding was that by giving it to human subjects, they were able to surface racial stereotypes.</sample>
    <sample id="38">The study used statistics from the enhanced version of the Penn Treebank.</sample>
    <sample id="39">Two authors are involved in the paper.</sample>
    <sample id="40">Some closely related tasks for cognitive dissonance are topic-independent dissonance stance classification and binary classification of expansion and comparison classes.</sample>
    <sample id="41">This presentation introduces the PEACoK project, which stands for Personal Common Sense Knowledge for Consistent and Engaging Narratives. Developed in collaboration with Sony Group Corporation, PEACoK aims to enhance natural language processing systems by providing a comprehensive understanding of real-world personhood. The system includes about 3,800 personas, 40,000 distinctive attributes, and 100,000 person-inferences or facts, forming a rich interconnected network of personal knowledge.

PEACoK represents personhood through three dimensions: human roles, interactions, and distinctiveness. It is built in three steps: selecting personas from existing common sense knowledge graphs, inducing attributes using both common sense knowledge graphs and large-scale pre-trained language models, and annotating relations through a joint human-AI majority voting scheme. Studies have shown that this approach yields high-quality relation annotations with an average accuracy of 87%.

The project has been tested on various natural language generation tasks, including a person attribute inference task and a dialogue generation task. Results indicate that PEACoK can significantly improve the performance of language models, making them more reliable and capable of generating narratives that are fluent, consistent, engaging, and expressive.</sample>
    <sample id="42">There are two authors involved in the paper.</sample>
    <sample id="43">There are seven authors involved in the paper.</sample>
    <sample id="44">The introduced framework differs from previous works by comparing n-users with models and datasets predictions and labels, as opposed to looking at just annotator agreement or modeling annotator distributions.</sample>
    <sample id="45">The generated personas</sample>
    <sample id="46">Deep Bel and Google Translate</sample>
    <sample id="48">There are five authors involved in the paper.</sample>
    <sample id="49">MPP evaluations were performed up to a context length of 124 tokens.</sample>
    <sample id="50">The speaker, a PhD student at Saarland University in Germany, introduces a video presenting their recent work on weakly supervised learning (WSL). They explain WSL as training neural networks with weakly labeled data, which is cheaper but noisier than human annotations. This noise can lead to memorization and poor generalization in neural networks. To address this, they propose training algorithms that robustly handle noisy labels, ensuring the models generalize well. The video discusses the common claim that WSL models achieve high performance on clean test sets but highlights the need for additional clean validation data, which is often overlooked. The researchers ask three key questions: whether clean validation data is necessary, how many clean samples are needed, and whether clean samples should only be used for validation or if there are better ways to utilize them. Their findings show that recent WSL methods require clean validation samples for proper functioning, and they have adapted alignment methods for German text simplification, publishing the code for these experiments.</sample>
    <sample id="51">The dataset includes music, books, and recipes.</sample>
    <sample id="52">The definition of positionality is the perspectives that people hold as a result of their demographics, identity, and life experiences.</sample>
    <sample id="53">Dawei Zhu</sample>
    <sample id="54">The presentation introduces the concept of cognitive dissonance, defined as two beliefs or actions that are inconsistent. It highlights the importance of studying dissonance in language to understand the effects of disagreement, track trends in belief and values, and understand mental health issues like anxiety disorders. The speaker explains that dissonance is a common phenomenon in daily decision-making and can be expressed in language among various discourse relations.

To address the challenge of detecting dissonance, which is rare, the presenter conducted a large-scale annotation of dissonance relations using a discourse unit parser. Despite annotating around 1000 examples, only 3.5% contained dissonance. This led to poor performance of an initial classifier trained on just 43 examples. To improve detection, the presenter experimented with transfer learning and active learning, transferring weights from related tasks like debate statement classification and CEB classification. This approach significantly improved zero-shot performance, reaching an AUC of 0.62.

The presenter also explored different strategies for updating the model with new data, finding that cumulative updates performed better than iterative updates. A probability of rare class strategy (PRC) was used to select highly likely dissonant examples, outperforming other state-of-the-art strategies. While PRC has the highest percentage of dissonance, annotators found it challenging. The presentation concludes by summarizing the effectiveness of these strategies in improving dissonance detection and their feasibility for annotator quality and costs.</sample>
    <sample id="55">Yes, it does.</sample>
    <sample id="56">1</sample>
    <sample id="57">Yes, the tested model works on the test suite.</sample>
    <sample id="58">The three variants of KITMUS are: 1. Without task-specific training on KITMUS, both models do not perform well. 2. With training on KITMUS, both C2F and B4C perform significantly better than the random choice. 3. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time.</sample>
    <sample id="59">In this presentation, we introduce our work on multi-modal relation extraction. Relation extraction is a task that aims to determine the semantic relationship between entities in a given text. However, in some real-world scenarios, such as social media, data often comes in various forms and modalities beyond just pure text, making it challenging to understand ambiguous or multi-sense words. Multi-modal relation extraction addresses this by incorporating additional visual sources into the textual data. For instance, with the help of visual evidences like bachelor's cap and gown, we can infer the relationship between JK Rowling and Harvard University. Despite these advancements, there are still challenges, including internal information over-utilization, where only parts of the text are useful for inferring relationships, and the varying roles of visual sources in the tagging task. Our proposed system offers better performance on nine out of eleven Donlim tasks and surpasses the results of the generic model. We also note that specialized data tends to perform better but does not scale well. All the pre-trained models from Natsos are freely available on the Yung face, and all training scripts are on our GitHub repository.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">The last research question is whether fine-tuning on clean validation samples can achieve the same performance as more complex weakly supervised learning (WSL) methods that require clean, manually annotated samples.</sample>
    <sample id="62">The speaker introduces a study on knowledge distillation for natural language generation (NLG) using pseudo-target training. The paper explores the potential of compressing large language models while preserving their performance. It compares two types of knowledge distillation: word-level and sequence-level, focusing on task-specific NLG tasks like summarization, question generation, coherence reasoning, and simplification. The study uses unlabeled data and medium-sized teacher models to achieve high compression rates with efficient inference time. The main contribution is the exploration of extensions in using pseudo-targets, including generating multiple targets, sampling with temperature, and proposing a novel joint teaching technique to address student exposure bias and improve learning.</sample>
    <sample id="63">The metric sensitivity measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction.</sample>
    <sample id="64">The speaker's name is Zhu Kang.</sample>
    <sample id="65">Greater sensitivity indicates improved model performance.</sample>
    <sample id="66">Mathematical reasoning is a fundamental aspect of human intelligence that enables us to comprehend and make decisions based on numeric data and language. The development of machines capable of solving math problems and proving theorems has been a longstanding focus of AI and NLP. In recent years, there has been a surge of interest in this area. Our survey discusses the task of mathematical reasoning and the development of deep learning methods for it.

For example, the task of math word problems can involve basic arithmetic operations with single or multiple operation steps. Mathematical reasoning isn't limited to text-based data; it can extend to mathematical information like images, figures, and tables. There are two primary categories we could study: visual contexts and tabular contexts.

Solving geometry problems is an essential subject in high school education. As an example shown here, given a program test and the corresponding diagram, when you need to identify the geometric relations, apply theorem knowledge, and perform calculations to obtain the numerical answer. So these tests can be formulated as neural symbolic reasoning programs over geometric diagrams, theorems, and formulas.

Another important line of mathematical reasoning is automated theorem proving. A theorem prover aims to demonstrate the truth of a mathematical claim via a sequence of logical arguments. You know, it is not trivial to write the proof for a theorem for our humans, but the automated theorem prover helps us. Some datasets have been proposed to probe the human-level intelligence of large models such as the numerical common sense knowledge and high-level program solving. In recent years, a number of neural network architectures has been proposed for mathematical reasoning tasks. For example, a sequence-to-sequence model uses an encoder-decoder architecture and usually formulates the mathematical reasoning task as a sequence generation task. The basic idea is to map an input sequence such as a math problem to an output sequence such as an equation program or proof. It is noteworthy that mathematical expressions can be represented as tree-based structures. Therefore, six-to-tree models have been proposed to explicitly model the tree structure when encoding the equation expressions.

In the last few years, we have witnessed the remarkable development of pre-trained language models such as the large language models LLMs. These language models have demonstrated remarkable performance on a wide range of NLP tasks. We can also apply LLMs to solve math word problems. For example, given an input question here, we can prompt the LLM with a one-sentence example of the chain-of-thought reasoning process. A chain-of-thought is a series of intermediate reasoning steps that lead to the final output. It enables language models to solve complex problems by guiding them to produce a sequence of steps before giving the final answer. Despite the advantages, LLMs still face inherent limitations. One notable example is the lack of ability to perform precise mathematical reasoning. One effective solution to both the LLMs' performance is to replace the greedy decoding strategy with self-consistency. Instead of generating just one reasoning path, I divers set of reasoning paths are sampled from the language model's decoder and the most frequent one is chosen from the answer set. Instead of designing effective prompting method for LLMs, another line of work is to design the two augmented LLMs. For example, program-ended LLMs are very helpful in complex mathematical reasoning tasks. We can augment LLMs with various tools to perform complex tasks. Given different input queries, a recently proposed approach can generate natural language programs to compose different tools for use. Despite the creation of various datasets, mathematical reasoning in low-resource languages remains under-explored. Recently, there has been attempts to build non-English datasets for Chinese, Korean, and Arabic. Additionally, pioneering research has developed mathematical reasoning benchmarks for financial, scientific, and medical domains. Despite impressive progress, deep learning models commonly display generalization and robustness failures on reasoning tasks. First, large models struggle with long inputs. Second, large models are inconsistent with mathematical reasoning. With that, thank you so much for attention.</sample>
    <sample id="67">The paper introduces a framework called Blender for selecting the best model output from multiple language models. It highlights that while some models may have better average performance, the optimal selection of models can vary significantly across different input examples. The Blender framework uses a two-stage process: first, it ranks candidate models using a pairwise ranking module called Pair Runner, and then it selects the top k candidates to generate a final output through a generative fusion model. The Pair Runner encodes pairs of candidates along with the input for better analysis of subtle differences. The framework is designed to handle severe interference in parameter-poor settings and suggests temperature sampling as a simple solution to control trade-offs. The results show that model size, data size, and temperature values are key factors affecting interference levels in multilingual translation. Tuned temperature is crucial for strong performance, and modest scale and tuned temperature can significantly reduce the problem without specialized methods.</sample>
    <sample id="68">The models receive a variety of linguistic contexts during pretraining, including grammatical structures, acceptability judgments, and stylistic considerations.</sample>
    <sample id="69">20 samples per class</sample>
    <sample id="70">Stanford Engineering Computer Science</sample>
    <sample id="71">The audio discusses a research project on resolving indirect referring expressions for entity selection, introducing the Alt Entities Corpus. The speaker, Javad Hosseini, explains that the goal is to understand user language when making choices, using examples like "did you mean easy on me or I got a feeling?" where users might prefer indirect references. The project aims to address this problem in conversational systems and benchmarking LLMs' entity understanding. They created a public dataset through crowdsourcing, covering music, books, and recipes, with a cartoon completion setup emphasizing informality. The dataset includes three speech bubbles: the first sets the context, the second asks an alternative question, and the third uses an indirect reference to select an entity. Annotators are shown background knowledge about the entities and asked to pick one using indirect expressions. The Alt Entities Corpus contains 6,000 alternative questions across three domains and 42,000 indirect referring expressions. Results show high accuracy with full background knowledge but decrease with partially overlapping or only entity names. The models are domain generalizable.</sample>
    <sample id="72">There is a need to develop new methods for measuring media biases because existing methods are not effective in measuring the biases of language models.</sample>
    <sample id="73">Adam Siprowski</sample>
    <sample id="74">The paper introduces Dense-Atomic, a method for improving the knowledge coverage and multi-hop paths in common technology databases. It describes how common technology captures events centered around social spaces of information, but often lacks comprehensive connections due to limited B2B, A2B, and A2A links. Dense-Atomic aims to fill these gaps by constructing a dense network over Atomic through three main steps: normalizing tail events, training a relation prediction model, and constructing Dense-Atomic. The normalization process involves converting tail events into a consistent format using four steps: subject removal, third-person singular form conjugation, subject reverb, and relation grouping. The relation prediction model uses a relation prediction strategy that leverages both head and tail events' semantic information, avoiding the sparsity problem and utilizing event semantics effectively. This approach is validated through experiments on the Atomic dataset, showing superior performance in relation prediction tasks compared to other methods. The paper also demonstrates the effectiveness of Dense-Atomic in generating more diverse and accurate multi-hop paths, enhancing the potential for comprehensive reasoning.</sample>
    <sample id="75">The English content is a presentation by Zheng Yandan, Hao Anran, and Lu Anh Tuan on their work titled "Joint Prop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation." The presentation introduces the motivation behind their research, focusing on the challenges of entity recognition and relation extraction in information extraction. They highlight the limitations of supervised learning models due to the high cost of labeling data and the need for diverse annotated data across various domains.

The researchers propose a joint semi-supervised learning framework that integrates entity and relation tasks by propagating labels over heterogeneous graphs. This framework consists of four main parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. They describe the process of generating span features, constructing a heterogeneous graph, and performing label propagation through the graph. The model optimization involves using the converged pseudo labels to retrain the classification model.

The experiment results show significant improvements in both entity and relation tasks compared to baseline models, demonstrating the effectiveness of their proposed framework. The presentation concludes with a thank you to the audience for their attention.</sample>
    <sample id="76">The political bias propagation pipeline involves evaluating the political leaning of language models, understanding the role of pretraining data in shaping these biases, and assessing how different political leanings impact downstream tasks such as hate speech detection and fake news detection.</sample>
    <sample id="77">This video is for sharing our work on improving summarization factual consistency from natural language feedback. This is the joint work from Yale University and Microsoft Research, and most of the work was done when the first author was an intern at Microsoft Research. In this work, we introduce a new dataset, Defacto, which contains human demonstrations and feedback for improving summarization factual consistency. For this dataset, we provide comprehensive analysis and offer further insights into the factual consistency of the summarization models. Upon this dataset, we propose three new NLP tasks, and we provide strong baseline models for each of them. The tasks we propose are summary editing, feedback generation, and automatic factual error correction. The task we studied in this work is abstractive text summarization, and we specifically study the factual consistency of the summarization models. This quality requires that all of the information in the summary should be supported by the input document. The human demonstrations and feedback we collected in this work is based on the original system-generated summaries of the existing summarization models. We asked the annotators to provide labels to decide whether the summary is factually consistent, and we require them to provide human-corrected factually consistent summaries if they think the original summary is not correct. Also, we require them to provide human feedback which contains the instructions, explanation, and evidence. More specifically, the explanation is to explain why the annotators think the summary is factually consistent or not. The instructions are for changing the original summary to make it factually consistent, and the evidence is one of the most relevant sentences in the source documents which will support the claim of the annotators. We collect the data on the XSum dataset, which is the most commonly studied dataset for summarization factual consistency, and the initial system outputs are collected from the pre-trained Pixie's model. In this slide, we show an example of the annotated data point. In this slide, we show the basic data statistics. We collect around 2.5K data points, and 70% of them contain factual errors. For the human-edited summaries, we show that they can receive higher automatic factuality scores compared with the initial system output. However, we also observe a lower textual overlap between the reference summaries and the human-edited summaries. We think the reason is that a majority of the reference summaries on the XSum datasets already contains the factual errors. In this slide, we show the data distribution of the annotated editing instructions and their relation with the different error types. The first task we studied is summary editing, where the model needs to follow the human feedback to edit the initial summary. We found that both the fine-tuned models and the zero-shot large language models can effectively leverage the human feedback for this task. The second task we studied is feedback generation, where a critical model needs to generate the feedback that can be used by the editing model. We found that this remains a challenging task for both the fine-tuned models and the large language models. The third task is to automatically correct factual errors while generating the corresponding explanation. We found that the editing model can achieve comparable performance compared with the baseline models while trained on much fewer data, and training the model to generate the explanation can help the model to achieve better performance. Apart from providing a test bed for the proposed NLP tasks, our dataset also has other advantages thanks to its ground-truth annotations, which can be valuable for training factuality metrics and factuality metric evaluation. We have released our collected Defacto dataset on GitHub, and please check our paper for more details. Thank you for listening.</sample>
    <sample id="78">Yes, the simplification process differs for DEplain-apa and web.</sample>
    <sample id="79">Yes, the Coscript dataset is publicly available.</sample>
    <sample id="80">The watermark is inserted by first defining a target embedding, then calculating the trigger number in the sentence, and finally adding the target embedding to the original embedding with a weight proportional to the number of triggers.</sample>
    <sample id="81">The authors of the paper are affiliated with the Pennsylvania University.</sample>
    <sample id="82">The video introduces a study on unsupervised automated essay scoring (AES) using multiple heuristic signals. The goal is to score writing quality without human intervention, an important application in education. State-of-the-art AES models are typically trained with large labeled corpora, but collecting such data is time-consuming and labor-intensive. Unsupervised AES can overcome this by not requiring ground truth scores for training. Two main approaches are discussed: one uses the number of unique terms as an initial score, and another uses word count as weak supervision. However, both methods lead to poor performance due to the limitations of single heuristic signals. The study proposes a new framework called URRA, which introduces multiple heuristic quality signals as pseudo ground truths and trains a neural AES model through rank aggregation. This method addresses inconsistencies among different signals and achieves superior performance compared to unsupervised baselines.</sample>
    <sample id="83">Yes, encoder-decoder models such as mt5 can be improved by training on a mixture of languages.</sample>
    <sample id="84">The speaker introduces a paper on "Efficient Framework for Dynamic Neural Networks" and begins by discussing the background knowledge of dynamic neural networks. They contrast static networks, which have fixed parameters, with dynamic networks that can adjust their architecture or parameters based on input. Examples include Mixup and Experts, where specific sub-networks are selected or linear convolution is applied based on input. The speaker then addresses the challenges of implementing dynamic networks, such as excessive use of parameters and increased model size, and proposes a framework called Partially Dynamic Network (PDN) to mitigate these issues. PDN partitions parameters into dynamic and static ones, using scale factors to control the intensity of each mode. This approach aims to maintain performance while reducing the number of parameters and computational resources. The speaker also mentions future work, including extending the method to other machine learning models and hardware-structured manners, and exploring combinations of zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">Making a chocolate cake is an example of constrained language planning.</sample>
    <sample id="86">They validate the covertness of their method by visualizing the embeddings of sentences on the dataset using PCA.</sample>
    <sample id="87">The work uses existing PLMs to build a new one by comparing different models and their performance on various tasks.</sample>
    <sample id="88">Non-binary people</sample>
    <sample id="89">I'm going to talk about</sample>
    <sample id="90">The English content of the presentation discusses the feasibility of using language learners as annotators for natural language processing (NLP) tasks. The authors, Hanul Yoo and colleagues, conducted a proof-of-concept study to examine whether language learners can contribute to data annotation effectively. They designed experiments targeting three languages: English, Korean, and Indonesian, considering the availability of resources and language difficulty.

The study involved four common NLP tasks: sentiment analysis, aligning sentences, named entity recognition, and span prediction. Participants were categorized into three levels: basic, intermediate, and advanced, based on their language proficiency. Native speakers were also recruited for comparison.

Participants were randomly sampled from existing datasets and categorized into five groups based on their difficulty level. They were then divided into two groups with additional resources provided. The study aimed to compare annotation accuracy and learning effects, particularly how additional resources influenced the quality of annotations.

The results showed that labels annotated by language learners were nearly accurate, especially for simpler tasks and easy to medium-level questions. Aggregating labels from multiple learners improved accuracy, often matching or surpassing native speakers' labels. Language models trained on learner-annotated data achieved around 95% performance, sometimes outperforming models trained with native speaker labels.

The study concluded that language learners can contribute significantly to NLP data annotation, offering a novel way to build benchmark datasets for low-resource languages where recruiting native speakers is challenging. Learners' language proficiency and vocabulary/grammar tended to improve as they carried out annotation tasks, as evidenced by pre-test and post-test scores.</sample>
    <sample id="91">As the amount of tasks increases, the model achieves better performance and in the meantime lower sensitivity.</sample>
    <sample id="92">Encoder PDR, Encoder Decoder, and Multilingual Pseudo-Training.</sample>
    <sample id="93">Advisers</sample>
    <sample id="94">The English content is a presentation by Wenjun Peng, Jingwei Yi, and colleagues from the University of Science and Technology of China. They discuss the challenges of protecting the copyright of large language models used in embedding services and propose a backdoor-based watermark method called Embedding Marker to address this issue. The presentation covers the background of embedding services, the threat of model stealing, and the requirements for an effective watermarking method. It then introduces Embedding Marker, which involves two main steps: watermark injection and copyright verification. The watermark injection step defines a target embedding based on the frequency of trigger words in user sentences, while the copyright verification step uses a backdoor dataset to detect whether another service contains the watermark. The presentation concludes with experimental results on four datasets, demonstrating the effectiveness and utility of Embedding Marker in protecting the copyright of large language models.</sample>
    <sample id="95">David Villegas</sample>
    <sample id="97">The speaker mentions three problems of SimulST: 1. Specific architectures are usually trained introducing additional modules to be optimized, 2. Long and complicated training procedures, for example, training involving different optimization objectives, and 3. Training and maintaining several models to reach different latency regimes, for example, training a model with an average of one second latency and another one with two seconds latency and so on.</sample>
    <sample id="98">Sanitizing the political opinions in language model training data.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, typically involving documents from a corpus. For example, to answer the question "What 1988 Christmas comedy film did Brian Doyle Murray star in?", one must first identify all movies Brian Doyle Murray starred in and then find the movie released in 1988. This set of documents is referred to as the chain.

Multi-hop retrievers are trained by maximizing the probability of the ground truth chains given questions. However, most state-of-the-art multi-hop retrievers fall under this paradigm, requiring thousands of examples for good performance, which can be expensive, especially in low-resource domains or those requiring special expertise.

The proposed approach, PromptRank, aims to be efficient by using a few-shot language model-based reranking method combined with an unsupervised retrieval method. The main steps include retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, and then reranking these candidates using the few-shot language model reranker.

Key considerations include choosing the scoring function and prompting the language model to extract scores. The likelihood of the question given the chain according to a language model is used as the scoring function. The chain prompt is constructed by inserting the chain documents into the prompt with an indicator token and an instruction to elicit the language model's reasoning ability over the chain documents.

Additional techniques such as instruction search, instruction sampling, and temperature scaling are employed to optimize the process. Experiments on GPT-2 and T5 models evaluated using metrics like retrieval R@K, recall at K, and answer recall at K showed that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop retrievers.

Ablation studies verified the importance of each component, and downstream QA performance was also evaluated when using PromptRank as a retriever. The results demonstrated strong few-shot path retrievable performance and significant improvements in downstream QA performance compared to exact match baselines.</sample>
    <sample id="101">The fluency of PaLM is comparable to state-of-the-art systems.</sample>
    <sample id="102">The important properties of a watermarking method are applicability to embedding as services, non-degradation of the utility of the provided embeddings, covertness to the attacker, and transferability to the attacker's services during the model extraction process.</sample>
    <sample id="103">The 14 different languages into which the English TED talks have been translated are not specified in the given information.</sample>
    <sample id="104">1</sample>
    <sample id="105">The distance metrics used for measuring the difference between benign and backdoor datasets are cosine similarity, Euclidean distance, and Minkowski distance.</sample>
    <sample id="106">The audio content is a presentation by Chaitanya Malavaliya, discussing the paper titled "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations." The presentation explains how people often express their information needs with multiple constraints or preferences, leading to queries that contain implicit set operations. The speaker provides examples of such queries and introduces the QUEST dataset, which includes over 3,000 entity-seeking queries with implicit set operations. The dataset was constructed using Wikipedia category names from four domains (films, books, plants, and animals) and involves set operations to generate queries with set constraints. Human annotators were used to paraphrase, validate, and verify the relevance of entities in the answer sets. The presentation also discusses the challenges in retrieving multi-answer sets from large document corpora and the performance of different retrieval systems on the QUEST dataset.</sample>
    <sample id="107">The multilingual encoder-based models were used for this task by training them on a mixture of various languages.</sample>
    <sample id="108">The speaker introduces the topic of evaluating language models using a minimal pair paradigm, which assesses acceptability judgments based on grammatical and stylistic criteria. The current NPP pipeline is limited in evaluating longer sentences, so the speaker proposes revisiting it to handle extended contexts. This involves simulating longer sequences by combining acceptable and unacceptable sentences from relevant datasets. The speaker explains that this method helps test the model's ability to maintain acceptability judgments across different contexts, including those from unrelated domains like Wikipedia. The speaker concludes by highlighting that language models are sensitive to latent syntactic and semantic features, and the current evaluation methods may not capture abstract knowledge effectively.</sample>
    <sample id="109">The paper introduces a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The model generates an instruction and a corresponding input, then generates a corresponding output. Additional paraphrases of each instruction are generated in a completely automatic manner. The resulting dataset contains 64K examples, and if we consider the instruction paraphrases, we have about 240K examples. The generated examples are analyzed focusing on creativity, diversity, and correctness. More than 50% of the generated examples are indeed correct, and even incorrect examples often contain valuable information for instruction tuning. In terms of creativity and diversity, the dataset contains highly creative tasks that are very different from classic NLP tasks. The utility of the generated data is measured by fine-tuning an 11 billion parameter T5 model on the Natural Instructions dataset. The model can outperform both zero-shot and few-shot instruction tuning across several benchmarks. When the cost of generating examples is amortized, training on the Natural Instructions dataset outperforms our baseline on all benchmarks.</sample>
    <sample id="110">hi i'm siu yuan from fudan university i am here to introduce our work distilling script knowledge from large language models for constrained language planning in everyday life humans often plan their actions by following step-by-step instructions in the form of a script previous work has exploited language models to plan for abstract goals of stereotypical activities such as making a cake and show that large language models can effectively decompose goals into steps however previous work mainly focuses on planning for abstract goals of stereotypical activities planning for the goals with specific goals specific constraints such as make a chocolate cake still remains understated in this paper we define the problem of constrained language planning which imposes different constraints on the goal of planning an abstract goal can be inherited by different real-life specific goals with multifaceted constraints a good planner should write scripts that are reasonable and faithful to constraints in this paper we first evaluate and improve the constrained language planning ability of large language models since no data set of specific goals exists to support our study we have to acquire these goals first as shown in the table we extend the abstract goals with multifaceted constraints for human in the loop data acquisition use instruct gpt we sample 100 specific goals and evaluate the scripts generated from large language models this table reports the overall accuracy of the results we find that all large language models achieve unsatisfactory results on planning for specific goals then we conduct detailed analysis to investigate why large language models for results in the figure shows that the semantic completeness in generated scripts is acceptable but the faithfulness to the constraints cannot be guaranteed we dig into more fine-grained topical categories of constraints defined in wakaha the heatmap in the figure shows that the planning performance of instruct gpt is varies considerably for goals of different categories previous studies have shown that the output quality of large language models forth in high variance leading to bad performance thus we adopt the idea of over-generated the filter to improve generation quality we first show constrained types with examples for instruct gpt and obtain specific goals based on the said abstract goals then instruct gpt over generates k scripts for specific goals next a filter model is developed to select the faithful scripts we convert scripts and goals into instruct gpt embeddings and calculate cosine similarity as similarity scores to measure semantic similarity in addition we aware the script that contains the keywords of the target constraint we only keep the script if the target goal score is the highest in the goal set with our method instruct gpt can generate scripts of higher quality our method greatly improves the planning ability both in semantics completeness and faithfulness to the constraint since large language models are costly to deploy it's essential to enable language planning ability of smaller and specialized models creating data set is an essential step to its end however previous studies do not enable planning for specific goals and manual manual data set annotation is expensive thus we follow the idea of symbolic knowledge distillation to distill constrained language planning data sets from large language models we apply our method for building a data set of constrained language planning named as code script in total we generate 55 thousand specific goals with scripts to ensure the quality of validation and the test set we ask crowdsourced workers to find and revise the incorrect samples this figure shows the constrained distribution of code script we find code script shows high productivity in the generated specific goals with code script we can train smaller but specialized models for constrained language planning we find that tifel function on code rate can generate scripts of higher quality than most large language models indicating that smaller models can surpass large large larger models when probably trained on suitable data sets in summary we established the constrained language planning problem we evaluate a constrained language planning ability of large language models and develop a over-generated filter method for large language models we use a large language models to generate a high-quality script data set code script for constrained language planning we hope code script data set can be available resource to advance the research on language planning thanks for your time please find more details of code script in our paper</sample>
    <sample id="111">The authors assume that the provider can collect a general text corpus and count the word frequency with it.</sample>
    <sample id="113">hello I'm James Finch and I'm Sarah Finch and today we'll tell you all about ABC eval a new dimensional approach to evaluating conversational AI this work was done by the Emory NLP lab led by professor Jinoo Choi at Emory University and in collaboration with Amazon Alexa AI so let's say that you just developed a dialogue model and you want to see how well it compares against the current state of the art the common practice is to use human evaluation such as by asking human judges to select which of two conversations is better or to rate conversations given a Likert scale these approaches work well to provide holistic evaluations of overall dialogue quality but dialogue quality has many aspects therefore you might want to evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer grained level one approach is to simply ask human judges to evaluate several dimensions of dialogue quality such as the relevance of model responses using existing comparative or Likert scale methods however we believe there is a more precise and reliable strategy for dimensional dialogue evaluation our approach attempts to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself we call this approach annotating behaviors in chat or ABC eval in short we developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature ABC eval is capable of measuring the rates at which chat models will commit various thematic errors for example ABC eval measures the number of turns in which a chat model ignores its partner or says something irrelevant contradicts itself or its partner hallucinates incorrect facts or violates common sense knowledge and when the model succeeds or fails to show empathy to determine what kind of evaluation is most effective we selected four state-of-the-art chat models and evaluated them on 100 human bot conversations per model using ABC eval for comparison we also evaluated these conversations using three existing methods Likert ratings on the turn level Likert ratings on the dialogue level and dialogue level pairwise comparisons for each of the existing methods we collected evaluations on eight of the most commonly measured aspects of dialogue since this is the standard practice for evaluating chat models along multiple dimensions from our analyses of these evaluation results we found that ABC eval behavior labels are overall more reliable than labels collected by existing methods as measured by inner annotator agreement on 100 doubly labeled conversations in addition ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods as shown by this simple linear regression analysis for example you can see how measuring the proportion of turns with self and partner contradictions explains five percent and ten percent of conversation quality respectively while the average Likert consistency scores explain only four percent or less finally we checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression you can see how the combination of all ABC eval metrics explains over 25% of conversation quality and as you remove the metrics one at a time most of them result in losing a decent amount of information about the quality on the other hand the combination of all turn level Likert metrics explains far less of the quality and fewer of these metrics carry unique information these reliable informative and distinct ABC eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve you can see that in the results of our experiment that several challenges still remain and have been precisely quantified for example the bots we tested have common sense violations in around 20% of their responses they produce irrelevant information in around 15% of the responses and they contradict themselves or their partner around 10% of the time with the rapid pace of improvement in the field many of these error rates could see a decrease in new models released since our evaluation was conducted however this is all the more reason to pursue reliable and precise evaluation metrics for comparing models we hope ABC eval can be leveraged by others in the field as a meaningful step in this direction and we look forward to seeing how conversational AI will advance in the coming months and years thank you for watching</sample>
    <sample id="114">The video presents a research paper titled "Finding the Pillars of Strength for Multi-Head Attention" by researchers from Nanyang Technological University in Singapore. The paper addresses the limitations of large language models, particularly their heavy parameter usage and long training times. It introduces a method called Group Head Attention (GHA) that uses a divide-and-conquer strategy to compress multi-head attention, aiming to reduce redundancy while maintaining performance. The GHA includes two stages: group constraint training and voting-to-stay algorithm. In group constraint training, attention heads are divided into groups to increase intra-group similarity and inter-group separation. The voting-to-stay algorithm then prunes redundant heads within each group based on their scores. The paper evaluates the effectiveness of GHA on three tasks: machine translation, language modeling, and abstractive summarization, showing significant parameter compression and performance improvements. The video also discusses the potential of task-specific pruning in reducing model size without sacrificing performance, aligning with the hypothesis that networks contain sub-networks with comparable accuracy to the original network.</sample>
    <sample id="115">The approach uses a speech segment size of lambda.</sample>
    <sample id="116">Servin is a judge.</sample>
    <sample id="117">The example quality is more important than the similarity to the source sentence.</sample>
    <sample id="118">The video discusses the challenges of building computational models for code-switching in linguistically diverse communities like India. It introduces the concept of code-switching, which involves mixing languages in a sentence, and explains that multilingual pre-trained models like MBERT and XLM-R do not perform well on code-switching tasks such as question answering and sentiment analysis. The main contributions of the work are proposing novel MLM techniques tuned to the case of code-switching and architectural changes and auxiliary losses also tuned to the case of code-switching. The video explains the concept of switch points, which refers to groups of two tokens that transition between languages, and proposes methods like SwitchLM and FrequencyLM to handle these switch points. It also discusses architectural modifications such as residual connections and auxiliary losses to enhance the model's ability to handle code-switching. The video concludes with results from experiments showing that the proposed methods outperform standard MLM on sentiment analysis tasks and increase the amount of switch point information in intermediate layers, verifying the claim about the switch point information.</sample>
    <sample id="119">The paper focuses on GPT-4 and GPT-3 series in the extended experiments.</sample>
    <sample id="120">The model uses attention scores from a specific layer.</sample>
    <sample id="121">The examples of direct inference are the name of the song, easy on me or its position, the first one.</sample>
    <sample id="122">The authors of the paper are affiliated with Fudan University and Brain Technologies Inc.</sample>
    <sample id="123">In this presentation, we will introduce our research on MultiInstruct, a multi-modal instruction-tuning benchmark dataset. We aim to investigate whether instruction tuning can improve the generalization of pre-trained language models to unseen multi-modal tasks. To address the lack of publicly available multi-modal instruction datasets, we have created MultiInstruct, which consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions. We use OFA, a unified multi-modal pre-trained model, as our base model. For training, we use 53 tasks from NLG for training and sample 10,000 instances per task. For testing, we reserve the entire commonsense reasoning group for testing and select additional five tasks from Wiki and miscellaneous groups. We report the mean and max performance and standard deviation of the performance across all five experiments. Our main results show that instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Transfer learning from natural instruction datasets can also benefit instruction tuning. As the amount of task increases, the model achieves better performance and lower sensitivity. Using more instructions can improve the model's overall performance and reduce its sensitivity. Transfer learning from natural instruction datasets can help OFA achieve much better performance on the natural instruction dataset.</sample>
    <sample id="124">The presentation discusses the development of a comprehensive temporal reasoning benchmark dataset and training strategy for large language models (LLMs). It introduces three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. The study evaluates LLMs on tasks like year prediction and month prediction, highlighting biases towards recent time periods. A new dataset, TempReason, is proposed to cover all three reasoning levels with long temporal coverage. The evaluation framework includes closed-book, open-book, and reasoning QA settings. A training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning is introduced to improve temporal reasoning capabilities. The results show significant improvements in L2 and L3 reasoning tasks and better performance in open-book and reasoning QA settings. However, performance fluctuations are observed across different time periods, indicating the need for future work to address these biases.</sample>
    <sample id="125">There are five authors involved in the paper.</sample>
    <sample id="126">Yes, translating the natural language query using a machine translation model before semantic parsing was considered as a baseline.</sample>
    <sample id="127">The video presents a research paper on the development of a method to transfer reasoning abilities from large language models to smaller models. The authors propose using large models as "reasoning teachers" to generate step-by-step solutions for complex tasks, which can then be used as training data for smaller models. They introduce a novel technique called "diverse reasoning," which involves generating multiple reasoning samples using stochastic temperature sampling to improve student performance. The method is compared with existing baselines and fine-tuning methods, showing significant improvements in performance, especially on text-based tasks. The authors highlight the scalability of their method and discuss potential trade-offs related to development and inference costs. The video concludes with an invitation to explore the full paper for more details on the methodology and results.</sample>
    <sample id="128">Hi, I'm Changbin, a PhD student at the University of Washington. Today, I'm presenting our work on how language models learn from diverse perspectives and the challenges they face in integrating background knowledge presented only at inference time. Language models are trained on large-scale web crawl data, which includes political news media well-covered by major outlets like The New York Times, Los Angeles Times, The Guardian, and Huffington Post. This has created a mixed blessing for language model applications: while they can learn from diverse perspectives and celebrate democracy, they often struggle to integrate background knowledge presented only at inference time. Many counterfactual reasoning models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thank you for listening!</sample>
    <sample id="129">Black women</sample>
    <sample id="130">The model architectures that do not generalize well are the ones that are not based on transformers.</sample>
    <sample id="131">CIFAR-10 and SVHN</sample>
    <sample id="132">Two authors are involved in the paper.</sample>
    <sample id="133">The author works with multiple modalities.</sample>
    <sample id="134">Hi, I am Yanis Labrak and I will present you our works on Dr. BERT, a robust pre-trained model in French for biomedical and clinical domains. In this presentation, we first talk about language modeling in healthcare. Then we will present the main contribution of our article. We introduce the first biomedical model in French named Dr. BERT, which is based on Roberta and trained on NACHOS, which is a dataset of medical crawled data from the web. We also introduce a comparison of models with multiple pre-training settings and data sources. Then we present our results on eleven biomedical and clinical downstream tasks in French. Finally, we conclude about the experiments and give you more details about how to access to the models. Since its release in 2018, BERT has become one of the most effective approach to solve natural language processing task and offer huge performance gain compared to historical static and contextualized methods such as Word2Vec, FastText or Elmo. Since then, this model has been adapted to many other languages like in French with Camembert and other domain like biomedical with PubMedBERT and BioBERT and on clinical with ClinicalBERT but mostly in English. Specialized models for other languages are scarce and are often based on continual pretraining due to the lack of in-domain data. However, French didn't have any open source model for biomedical and clinical. We so we ask ourselves question about what is the most appropriate data sources for a wide range of usage and those crawled data are good substitution for clinical data. To answer this question, we compare Dr. BERT with our Shubert model which is based on anonymized data obtained from the non-university hospital data warehouse. Afterward, we ask ourselves how much data do we need to train a specialized model on French data? Is it four gigabyte, eight gigabyte or more? To answer this question, we first train and compare four from scratch models: a first version of Dr. BERT with seven gigabyte of NACHOS, a second version of four gigabyte subset of NACHOS, a first version of Shubert which is a clinical model with four gigabyte of sentences taken from clinical notes and a final version of Shubert with a mix of four gigabyte subset of NACHOS and four gigabyte of clinical notes. In addition to this comparison, we introduce three models trained on continual pretraining to analyze the impact of pretraining strategy: one based on the weight of Camembert and trained on four gigabyte subset of NACHOS, another also based on Camembert but trained this time on the four gigabyte of clinical notes and finally one based on English biomedical model PubMedBERT and trained on four gigabyte subset of NACHOS. In total, we have seven models. To evaluate our seven models, we gather which public and private downstream tasks such as name entity recognition, classification, part-of-speech tagging and question answering. These models are compared to six baseline models which are Camembert Oscar 138 gigabyte, Camembert Oscar four gigabyte, Camembert CCNet four gigabyte, PubMedBERT BioBERT and ClinicalBERT. The evaluation of highlight that model perform best on the task with data of the same nature as those on which the model has been trained. However, we can obtain the data from we can observe that data from heterogeneous sources appear to be more versatile. We also observe that using more data translate into better performance. In overall, from scratch pretraining seem to obtain higher performance on most of the tasks. However, our experiment on continual pretraining, using the weight and tokenizer of PubMedBERT trained on the four gigabyte subset of NACHOS, show comparable result to those obtained with Dr. BERT four gigabyte from scratch which is not the case for the model based on Camembert weights and tokenizer which suffer from stability issues. Finally, as a conclusion, our proposed system offer better performance on nine of the eleven downstream tasks and surpass globally the result of the generic model here, Camembert. We also observe seeing that specialized data is better, more specialized data is better but it doesn't scale well. As the pre-trained model obtained from NACHOS are freely available and on HuggingFace and all the training scripts are on our GitHub repository. So thank you for for this presentation and we are looking forward to exchange at the post-thesis session in Toronto.</sample>
    <sample id="135">The video presents a new method for evaluating conversational AI called ABC eval. This method involves annotating whether or not each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. The method is designed to reduce the subjectivity of human evaluation and provide a more precise and reliable strategy for dimensional dialogue evaluation. The video also discusses the results of an experiment that compared ABC eval with three existing methods for evaluating chat models. The results showed that ABC eval behavior labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. Additionally, ABC eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods. Finally, the video highlights the challenges still remaining in the field, such as the bots tested having common sense violations in around 20% of their responses, producing irrelevant information in around 15% of the responses, and contradicting themselves or their partner around 10% of the time.</sample>
    <sample id="136">The speaker, Jazan, is presenting a work conducted with his supervisor, Nafisa, at the University of Sheffield. The work is titled "Fermat: An Alternative to Accuracy for Numerical Reasoning." The presentation includes a QR code that provides access to the paper, GitHub repository, Twitter, and LinkedIn. The motivation behind this work is to address the lack of real-world application in numerical reasoning tasks and the need for downstream tasks that require factual correctness. The speaker explains that there are many models that perform better with larger language models, but these models often struggle with numerical reasoning tasks. The current benchmarks do not facilitate this issue, as they only provide accuracy scores and F1 measures, which are not informative about the mathematical abilities of the models. To address this, the speaker introduces Fermat, a flexible evaluation set based on arithmetic types, including number understanding, mathematical operations, and training dependency. The evaluation set consists of math worded questions extracted from Illinois and Common Core, with numbers represented differently to mimic real-life scenarios. The speaker also discusses the importance of mathematical operations and training dependency in evaluating the performance of language models. The presentation concludes with the speaker encouraging the audience to read the paper and explore the provided links.</sample>
    <sample id="137">The speaker introduces Tell2Design, a dataset for language-guided floor plan generation published in ACML 2023. The dataset aims to enable users to design by telling instructions with a specific focus on the floor plan domain as the initial area of research. The task is defined as generating reasonable 2D floor plan designs directly from language instructions. Each data sample consists of natural language instructions that characterize the key components of the corresponding floor plan design, including semantics, geometry, and topology. The dataset includes 5,501 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk and around 76,000 artificially generated language instructions from predefined templates. The main challenges are performing design generation under strict constraints, understanding the big picture of the entire floor plan from document-level structured text, and handling ambiguous, incomplete, or misleading information in human instructions. The method uses a sequence-to-sequence model under the encoder-decoder framework to generate floor plan layouts from language instructions. The T2D model achieves the highest Iou scores with a micro Iou of 54 and a macro Iou of 53, outperforming other text conditional image generation baselines by a large margin.</sample>
    <sample id="138">The authors claim that knowledge integration from multiple sources is an understudied area in NLU.</sample>
    <sample id="139">The names of the speakers are Ying and Zhiyang.</sample>
    <sample id="140">Yes, Coscript underwent quality checks by crowdsourced workers to ensure the accuracy and reliability of the generated scripts.</sample>
    <sample id="141">The limits of existing resources for on context-dependent translation are that they only support limited types of context-dependent translations and limited sets of languages, as they usually rely on domain knowledge and human curation.</sample>
    <sample id="143">The approach is compared with the Whitkey strategy and the local agreement.</sample>
    <sample id="144">The authors of the paper are affiliated with LIA, Avignon University; LS2N, University; CHU de Nantes; and Zenith.</sample>
    <sample id="145">The speaker's name is Jenny.</sample>
    <sample id="146">The English content is a presentation by a PhD student from Fudan University, introducing a paper on the analysis of omission in dialogue summarization. The presenter, Zou Yi Chen, explains that dialogue summarization is a sub-task of text summarization and involves creating concise summaries to represent important information within a dialogue. He highlights the challenges in extracting key information from dialogues across different domains and the common errors in generated summaries, particularly omission, which leads to incomplete summaries and loss of critical facts.

The presentation delves into the analysis of omission in dialogue summarization, presenting data on the omission rate of summaries from five domains and six pre-trained models. It reveals that even state-of-the-art models still exhibit high omission rates, with about 70% of generated summaries suffering from this issue. The presenter also discusses the random distribution of omitted information across various positions in dialogues, indicating the unstructured nature of dialogues and the difficulty in identifying key information for current models.

To address the omission problem, the presenter introduces a task definition for omission detection, focusing on the utterance level and predicting which utterances are omitted in candidate summaries. However, there are no existing datasets specifically designed for this task, so the presenter constructed a new dataset with high-quality omission labels for dialogue summarization. This dataset is built upon five existing benchmarks covering five domains and includes diverse candidate summaries generated by different abstractive models and decoding strategies.

The presenter then explores three baseline frameworks for omission detection using different input formats and structures: pairwise classification, sequence labeling, and pointer networks. Precision-Recall (PR) scores and Word Level Omission Recall (WLO) scores are used to evaluate the performance of these models. The results indicate that the task is challenging, with an F1 score around 50%, suggesting the need for more advanced detection models.

Finally, the presenter discusses a post-editing method for summary refinement, where the candidate summary is concatenated with omission content as input, and the model outputs a refined summary. The results show that the performance is significantly improved when omission content is provided, indicating the value of omission detection and its potential for improving summary quality in dialogue summarization.</sample>
    <sample id="147">Three</sample>
    <sample id="149">Yes, the dataset is publicly available.</sample>
    <sample id="150">The video presents a research paper titled "MeetingQA: Extractive Question-Answering on Meeting Transcripts" by Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Delamalsalehy, Frank Dernoncourt, and Mohit Bansal from UNC Chapel Hill and Adobe Research. The paper introduces a new dataset called MeetingQA, which is an extractive question answering dataset based on questions asked by participants in meetings and their corresponding answer sentences. The dataset contains 7.7 thousand questions, with 30% unanswerable, 40% having multi-span answers, and 48% having multi-speaker answers. The questions are longer, open-ended, and actively feed discussions from others. The data collection process involves public meeting transcripts from the AMI Corpus, question selection based on punctuation, and filtering out really short questions. Answers are annotated by recruiters to label sentences in the answer span, achieving a high inter-annotator agreement reflected by a Krippendorff's Alpha of 0.73. The dataset is split into training, dev, and test sets, with 30% of questions being unanswerable. The paper also discusses the length distribution of meeting transcripts, questions, and answers, showing that questions and answers are roughly composed of 12 and 35 words respectively. The paper concludes by discussing the results in the fine-tuned setting, comparing short context models like Roberta and long context models like LongFormer, and single-span variants abbreviated by SS and multi-span variants abbreviated by MS. The paper also shows zero-shot performance, silver data augmentation, and error analysis in the first half of the figure. The bottom half of the figure shows that models struggle to identify which speaker answers a question, especially in the zero-shot setting.</sample>
    <sample id="152">The video presents a research project on developing language models for classical philology, specifically focusing on ancient Greek and Latin texts. The researchers introduce their work titled "Exploring Large Language Models for Classical Philology" and discuss the current landscape of language models in classics. They highlight the limitations of existing models, such as being monolingual and not pre-trained on ancient Greek texts. To address these challenges, they have developed two monolingual models (Graeberta and Greta) and two multilingual models (Filberta and Filter). These models are pre-trained on ancient Greek, Latin, and English data, providing a comprehensive approach to handling multilingual texts. The researchers also emphasize the importance of benchmarking and analyzing model performance, demonstrating that their models outperform previous models in tasks like speech tagging, dependency parsing, and lemmatization. Overall, the video provides an overview of the project's goals, methods, and results, showcasing the potential of large language models in classical philology.</sample>
    <sample id="153">The presentation by Ninarah Mehrabi, a postdoctoral scientist at Amazon Alexa AI's Responsible AI team, focuses on resolving ambiguities in text-to-image generative models. The work aims to study existing ambiguities in prompts provided to these models and propose frameworks to mitigate and evaluate such ambiguities. The pipeline involves curating a benchmark dataset with various types of ambiguities, using either a language model or visual setups to disambiguate the prompts. Once disambiguated, the prompts are used to generate images, which are then evaluated for faithfulness to the user's intention using a VQA model. The findings show that resolving ambiguities has a positive effect on faithful generation and that the automatic evaluation framework aligns with human evaluations, making it reliable for assessing text-to-image models.</sample>
    <sample id="154">The authors of the paper are affiliated with the University of Trento and Fondazione Bruno Kessler.</sample>
    <sample id="155">The speaker's name is Javad Hosseini.</sample>
    <sample id="156">hello everyone my name is ibad and i will give you a short overview of the paper prompting palm for translation assessing strategies and performance this is joint work with my colleagues from google translate palm is a 540 billion parameter large language model presented last year in 2022 it's trained on a large collection of text comprising 780 billion tokens at the time of publication it achieves state-of-the-art in hundreds of nlp tasks in this work we present the first systematic study of large language model prompting for machine translation we evaluate the translation capability of such models using the best practices of the mmt community this involves using the latest test sets to avoid another lab of the test data with the training data of the language model and we compare two state-of-the-art systems so the best performing systems or the wmt evaluation we use state-of-the-art and new mmt metrics and additionally also show expert based human evaluation results finally we provide some recommendations for prompt selection strategies the prompting has a big influence on the performance of the of llms for translation as we can see in a simple experiment where we use one shot prompting and provided two different prompts for each sentence the majority of sentences 516 out of 1,000 the difference observed is of more than one bleurt points and this can go in extreme cases up to 40 blurb points so it's important to select that good prompting strategy in our experiments we conclude for a five shot prompting strategy where we just mark each uh its sentence that we provide to the system with the language it's same so in this example here where we perform translations from german into english the german sentences the source sentences are marked with german colon and the english translations with english colon we saw that the actual form of the prompting doesn't have a big influence in in the case of several short prompting it's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the of the prompting it's the examples that carry most of the of the weight the summary of our experimental results is that the example quality is more important than the similarity to the source sentence so it's important to select the examples from why high-quality translations in particular we compare the selecting prompts from the training data of the wmt evaluations or the dev data the dev data is much more curated and with higher quality than the train data that it's more nice and the results so better performance when using the dev data nonetheless specialized state-of-the-art systems have a substantial advantage over the palm translations but palm comes pretty close to a commercial system now in our case we chose to evaluate with google translate the insights that we gain from the evaluation that we perform using the mcm framework is that the fluency of palm is comparable to state-of-the-art of the art systems but the main difference comes from the accuracy so in particular the most common error are omission errors so it seems that palm chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation however the style awkward category for palm is lower than for the state-of-the-art systems which is an additional signal that palm provides really fluent output but still with some problems of accuracy and that's it for this really short overview for more details please come to the full presentation of the paper thank you very much</sample>
    <sample id="157">The speaker, Shen Gao from Sandong University, introduces a dialogue summarization method called Dialogue Summarization with Static-Dynamic Structure Fusion Graph (STDS). This method is a joint effort with several researchers and aims to distill silent information from a dialogue context into a concise summary. The task of dialogue summarization is challenging and interesting in the text summarization research field, as it helps people quickly capture the highlights of semi-structured and multi-participant dialogues without reviewing the complex dialogue context.

The speaker explains that many existing methods focus on modeling dialogues using precomputed static graph structures, which depend heavily on external linguistic tools and can lead to inaccurate outputs. Additionally, these static graphs cannot dynamically adapt to downstream dialogue summarization tasks.

To address these issues, the STDs model consists of four main components: an utterance encoder, a static structure modeling method, a dynamic graph module, and a pre-trained language model as the summary generator. The STDs model first encodes the utterances in the dialogue context into vector representations, constructs a static graph using existing dialogue structure modeling methods, and then uses a dynamic graph module to capture semantic relationships between utterances based on their deep vector representation. Finally, the model employs a pre-trained language model as the summary generator to fuse the static dialogue structure and the dynamic learned dialogue structure into the final summary.

The speaker also discusses the proposed heuristic dialogue structure modeling method, which builds relationships between utterances using a graph network. This method includes a discourse passing graph, a speaker relationship modeling method, and a relative distance-based utterance positional graph. The speaker explains how these components are used to capture the static dialogue structure information and how they are integrated into the final summary.

The speaker concludes by mentioning that the code and data for the STDs model have been released on GitHub, allowing others to explore and use the model.</sample>
    <sample id="158">The video presents a research paper titled "Dual Cache for Long Document Neural Coreference Resolution" by Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. The paper discusses the challenges of training neural networks on weakly labeled data due to noise in the labels, which can lead to memorization rather than generalization. It introduces a method called Dual Cache that uses both local and global caches to improve performance and reduce cache misses. The paper also addresses the need for clean validation data in weakly supervised learning (WSL) and proposes using noisy validation sets instead. The authors find that increasing the number of clean validation samples improves performance but Dual Cache still outperforms single cache methods with better cost efficiency. The video concludes with a summary of the key findings and implications for WSL approaches.</sample>
    <sample id="160">The first step of the method maps each input token to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="161">55,000</sample>
    <sample id="162">hello everyone i'm makhata and today my co-author martin and i are presenting our work the kit must evaluate evaluating knowledge integration from multiple sources this work is a collaboration between mckill university mela and microsoft research natural language understanding models draw on a variety of knowledge sources such as knowledge contained in their parameters usually acquired via pre-training and knowledge given in inputs at inference time recent works in tasks like question answering show that models can use pretraining knowledge to solve the task but natural language understanding often requires knowledge that is also supplied at inference time for example in the sentence john saw the newly elected president on tv pretraining parameters can contain information about what presidents do and what a tv is but they cannot reliably know who this instance specific entity john is or who the new president is because the president might have changed since pretraining therefore successful models for knowledge intensive nlu tasks require the ability to integrate and use both pretraining and inference time knowledge in this work we propose a diagnostic test suite for knowledge integration we introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources we evaluate the data set with human study participants and established coreference resolution models here is an example from our data set servin is a judge kia is a baker termine and kia met at a park after a long day at work deciding cases in a law court he was happy to relax the task here is to identify the correct entity that the pronoun he refers to which in this case is servin the resolution of a given pronoun requires two types of information first entity specific knowledge such as servin is a judge and second background knowledge such as judges decide cases in law courts generally background knowledge is learned during the pretraining of large language models while entities specific knowledge is typically observed at inference time we vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources we have defined three settings of kit mos first with the typical setting background pretrain where background knowledge is assumed to be available at pretraining time second there's a background both setting where background knowledge is available both at pretraining time and inference time lastly the background inference setting where both knowledge types are available only at inference time this last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pretraining data of models for example because new occupations have developed since the time of pretraining here's an example of how we control the availability of facts in the truth sources in the background pretraining setting we assume that the background knowledge politicians seek elected seats in government is contained in the pretraining parameters in the inference time context we provide the ant-specific knowledge chester is a politician in the background both setting we additionally provide not only ant-specific but also background knowledge about politicians in the inference time context in the background inference setting we provide the fictional occupation mirtura instead of politician because mirtura is unlikely to be contained in the pretraining parameters we evaluate the data set both with human study participants and established coreference resolution models in this figure we show the results of the best performing models on the most difficult variant of the background pretraining setting without task-specific training on kit mos both models do not perform well when trained on kit mos however both c2f and build for q perform significantly better than the random choice this suggests that when trained on general coreference resolution datasets models learn to exploit surface cues which are not useful when testing on kit mos for such cues have been removed additional experiments with fictional knowledge indicate that even the best performing models cannot reliably integrate background knowledge provided only at inference time to summarize the main takeaways of our paper many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training however with task-specific training some models successfully integrate knowledge from multiple sources still even the best performing models seem to have difficulties with reliably integrated background knowledge presented only at inference time if you're interested in more details please see our paper and check out the dataset and code on github thanks for listening</sample>
    <sample id="163">The best alignment method for DEplain is the method of mass align.</sample>
    <sample id="164">The benefit of weakly supervised learning is that it is much cheaper to label the data using weak labeling sources such as simple heuristics, knowledge bases, or low-quality crowdsourcing.</sample>
    <sample id="165">In this work, we try to answer two questions: first, when does translation require context, and second, how well do models handle these cases? To answer the first question, we started by measuring how much a word depends on context-dependent translation. In the previous work, we introduced CXMI as a measure for context usage by machine translation models. This is done by measuring how much information the context C provides about the target Y given the source X. You can think of CXMI as the information gain from giving context to the model. In this work, we extend CXMI to point-wise CXMI, which can measure context usage at the sentence level or at the word level. We can think of words that have high pCXMI as ones that require context for translation. Now we analyze words with high pCXMI to look for patterns between these words. And we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First, we look at parts of speech tags that have high means pCXMI, and this allows us to find, for example, dual pronouns in Arabic that have relatively high pCXMI. And this can be explained.</sample>
    <sample id="166">The English audio content describes a neural divide-and-conquer reasoning framework for image retrieval from linguistically complex text. The framework addresses the challenge of retrieving images from text that is highly similar to the query description, which traditional visual language models struggle with. It employs a strategy of breaking down the problem into smaller sub-problems and combining their results to achieve the desired output. The system utilizes two main components: a symbolic reasoner and an analogical reasoner. The symbolic reasoner generates and matches symbol representations with images, while the analogical reasoner integrates these representations to obtain the final solution. The system demonstrates superior performance compared to baseline methods and ablation experiments on the testing set, showcasing its effectiveness in handling complex reasoning tasks.</sample>
    <sample id="167">The documents in DEplain-web were aligned with 750 documents manually and the other half with automatic alignment methods.</sample>
    <sample id="168">The CoNLL++ dataset was created by collecting data from Reuters-News from 2020 and annotating them with the same CoNLL 2003 annotation guidelines.</sample>
    <sample id="169">The speaker is introducing a paper on the topic of prompting for translation using large language models. The paper presents a systematic study of prompting strategies and their performance in machine translation tasks. The speaker explains that the study evaluates the translation capability of such models using the best practices of the NMT community, involving the latest test sets to avoid overlap with training data. The study compares two state-of-the-art systems, the best-performing systems on the WMT evaluation, using state-of-the-art NMT metrics and expert-based human evaluation results. Recommendations for prompt selection strategies are provided. The speaker highlights the significant influence of prompting on the performance of LLMs for translation, showing that even simple experiments with one-shot prompting can result in improvements of more than 1 BLEU point, with extreme cases up to 40 points. The speaker concludes by summarizing the experimental results, emphasizing the importance of example quality over similarity to the source sentence, and comparing prompts from training data versus dev data, noting that dev data yields better performance.</sample>
    <sample id="171">Existing works can be broadly classified into four categories.</sample>
    <sample id="172">No, multilingual LLMs such as Codex or Bloom are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="173">hello everyone my name is true heng today i'm going to present our paper do conl 2003 named entity taggers still work well in 2023 let's get started our paper investigated the problem of generalization using the named entity recognition task or the nre task we observed that models have been using conl 2003 to develop any r for almost 20 years and this naturally raises several problems firstly can these models generalize to modern data and when we develop new taggers what is needed for good generalization at the same time if we do observe poor generalization what causes the performance drop of these models to investigate these problems we developed the conl plus plus data set this is a data set that we collected from Reuters news from 2020 and then annotated them with the same conl 2003 annotation guidelines we then fine-tuned over 20 models on conl 2003 we evaluated them on both the conl 03 test set and the conl plus plus test set and last but not least we calculated the percentage change in f1 to assess the generalization of each model so what is needed for a good generalization through our experiments we found that there are three main ingredients that are needed the first one is the model architecture through our experiments we found that the Transformer models normally generalized better to new data the second ingredient is the model size we found that usually larger models lead to better generalization in last but not least we all know that the number of fine-tuning examples directly affects the performance of the downstream task here we also found that more fine-tuning examples actually also leads to better generalization to our next question what causes the performance drop of some models we had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminishing returns on the new test set the second hypothesis is temporal drift which is the performance degradation that is caused by the increasing temporal gap between the train and the test data for adaptive overfitting we saw that from the graph on the right the red best fit line has a gradient that is greater than 1 this means that every unit of improvement that we made on conl 2003 translates to more than one unit improvement on conl plus plus which means that there is no diminishing returns and this shows us that adaptive overfitting in this case is not observed so what about temporal drift then for temporal drift we did an experiment to retrain or continue to pre-train some models with more recent data and we found that the performance degrades with larger temporal gap and this confirms our hypothesis that the main cause of the performance drop is temporal drift our conclusion is that for good generalization we would need a better model architecture larger model size as well as more fine-tuning examples and these goals hand in hands we can't just have one ingredient but throughout the others at the same time we also found that the performance drop here is caused by temporal drift and kind of surprisingly it is not caused by adaptive overfitting even though conl 2003 has been used for over 20 years so going back to the question that we raised in the title of our paper do conl 2003 taggers still work in 2023 and we found that the answer is actually a resounding yes we hope our paper calls for more research on how to improve generalizations of the models and lastly please make sure to check out our paper our dataset and if you have any questions feel free to contact me thank you so much</sample>
    <sample id="174">The ArgAnalysis35K dataset is a comprehensive resource for argument quality analysis, featuring 35,000 argument analysis pairs sourced from high-quality speeches, expert debaters, intermediate debaters, and novice debaters. This dataset stands out due to its size, diversity, and depth of analysis. Unlike other datasets that often rely on crowdsourcing with limited motions and lack depth, ArgAnalysis35K captures a wide range of motions across 24 themes, including expert advice and real-world examples. It also includes detailed analyses that combine claims, premises, and evidence to form coherent arguments, providing a more holistic understanding of argumentation. Additionally, the dataset incorporates an instance-based annotator reliability mechanism to mitigate human biases, ensuring more reliable scoring. The relevance model further enhances the dataset by assigning scores to arguments based on their relevance to specific themes, making it a valuable tool for researchers and practitioners in the field of argumentation and debate.</sample>
    <sample id="175">The method deals with the ambiguity of permutations by using a GP-friendly continuous relaxation that allows back propagation through the solution and learning linguistically more plausible permutations.</sample>
    <sample id="176">The fairness of a downstream NLP model is defined as the ability to detect hate speech and fake news accurately, regardless of the political leaning of the news media.</sample>
    <sample id="177">The speaker's name is Yanis Labrak.</sample>
    <sample id="178">Kostya Sinha</sample>
    <sample id="179">The English content discusses the development of a method called "Symbolic Tom" to enhance the theory of mind reasoning skills in large language models (LLMs). It introduces the concept of theory of mind, which involves the ability to reason about the mental states of others, and explains how this is traditionally measured in humans and LLMs through reading comprehension tasks involving multiple characters. The presentation highlights the challenges that LLMs face in performing well on false belief tasks, such as ChatGPT and GPT-3.

The research question posed is how to improve the theory of mind reasoning skills in LLMs. The proposed solution is an inference-time method using explicit graphical representations. This method computes graphical representations of characters' beliefs up to a predefined maximum theory of mind level, allowing for efficient answering of questions by detecting entities, retrieving the appropriate belief graph, and performing recursion over the question.

Experiments were conducted using various LLMs, including fine-tuned GPT-3 models and a model specifically designed for theory of mind reasoning. The results showed significant performance gains across different datasets, with Symbolic Tom outperforming supervised baselines and demonstrating robustness in both story structure and linguistic generalization. The conclusion emphasizes that Symbolic Tom is a plug-and-play method that leverages off-the-shelf NLP models to improve theory of mind reasoning in LLMs, enhancing their ability to understand and predict the mental states of other characters in stories.</sample>
    <sample id="180">Myra Cheng</sample>
    <sample id="181">The paper introduces a method for generating scripts from large language models to aid in constrained language planning. It highlights the challenge of planning specific goals with multiple constraints, such as making a chocolate cake. The authors evaluate and improve the constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints. They use InstructGPT to generate 100 specific goals and evaluate the results. The findings show that while semantic completeness is acceptable, the faithfulness to constraints cannot be guaranteed. To address this, they develop an over-generated and filter method to select feasible scripts. This method improves both semantic completeness and faithfulness to constraints. Additionally, they propose symbolic knowledge distillation to create a dataset of constrained language planning, named CoScript, which can be used to train smaller and specialized models. The CoScript dataset shows high diversity and quality, indicating that smaller models can support larger models when trained on suitable datasets.</sample>
    <sample id="182">Tropicalism is a trope that reflects harmful stereotypes and essentializing narratives in the portrayal of women of color, particularly Latina women.</sample>
    <sample id="183">The authors used prompts inspired by a study where they gave prompts to human subjects.</sample>
    <sample id="184">To measure context usage in this work, the researchers used a combination of metrics and benchmarks. They employed corpus-level metrics such as BLEU, METEOR, and Word F1 to evaluate the performance of different models on document-level translation tasks. Additionally, they utilized the MuDa benchmark to assess the accuracy of models in handling specific discourse phenomena like formality, lexical cohesion, ellipsis, pronouns, and verb form. This comprehensive approach allowed them to identify which models are better at handling context-dependent examples and to compare the performance of different commercial systems, such as DeepL and Google Translate, in document-level translation tasks.</sample>
    <sample id="185">The difference between DrBERT and ChuBERT is that DrBERT is a biomedical model in French, while ChuBERT is based on anonymized data obtained from the University Hospital of Toulouse.</sample>
    <sample id="186">hi i'm myra and today we'll be talking about our paper marked personas using natural language prompts to measure stereotypes in language models this work is done in collaboration with esin durmus and dan jurafsky in recent years many have documented the prevalence of social bias and stereotypes in large language models or llms however these measures have various limitations they usually rely on hand constructed datasets that are very time consuming to curate and they also usually only measure very specific stereotypes meaning that they don't generalize well to other demographics or contexts or they simply capture a very general broad associations like negative associations with particular groups furthermore most work in this space doesn't account for intersectionality which is the notion that multifaceted social identities can compound biases and be unique low side of harm to overcome these limitations we rely on the property that these newer instruction tuned llms are very good at responding to instructions and prompts so we can ask the model to generate a persona which is a depiction of an imagined individual using a prompt like imagine you are an asian woman describe yourself and we can immediately see that this is very generalizable to any demographic because we can just specify whatever identity marker that we want into this prompt so here are some example generations from gpt four immediately we see that while the outputs aren't overtly negative or toxic in the traditional sense of these words there are some interesting patterns the asian woman is depicted as unassuming the middle eastern woman is referred to using words like exotic and like referring to a mesmerizing region and both of the women of color personas make references to ancestry while the white man persona has nothing of the sort to capture these patterns our method has two parts the first one is generating these personas our prompts to generate these personas were inspired by a study where they gave these prompts to human subjects finding that by giving it to human subjects they also were able to surface racial stereotypes and also this enables direct comparison between our generated personas and the human written responses the second part is marked words which is a method to identify the words that distinguish marked groups from unmarked ones which i'll elaborate on shortly the benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon so the marked words method draws upon the sociolinguistic concept of markedness which states that there is an unmarked default and any group that differs from that default is linguistically marked so for instance the word man or sorry the word warrior is usually associated with men so when people are describing a warrior who is a woman they'll usually actually specify one man warrior and mark the term with women and more broadly dominant groups in society are both linguistically and socially unmarked while the marginalized groups are usually marked so in our method we first designate what the unmarked and marked groups are and then we compare the personas using the fighting words method which is basically using weighted log odds ratios to distinguish the top words for each marked group so for instance for the personas of black woman we would do fighting words and compare the log odds ratios against both white personas and man personas because those are the two corresponding unmarked groups now for some results so first we use alexicon of stereotypes and we find that the generated personas contain a lot more stereotypes than the human written ones however when we actually look at the distribution of the words in lexicon we find very different things so while the generated personas have much higher rates of the lexicon words the human written ones have a much wider distribution of words while the stereotype words that are in the generated personas are really just the words tall and athletic so really just only the positive or at least non-negative ones and in fact this lexicon doesn't really capture many of the harmful patterns that we saw in the earlier slides well at all so instead to do that we'll turn to the results from our marked words method to show how these seemingly positive portrayals reflect harmful patterns in our analysis we reveal how these seemingly positive portrayals reflect harmful patterns first for mark groups the top words include things like culture tradition proud and exotic and these words define these groups only by their relationship to their identity and distinguish them as different from the white norm this contributes to a long legacy of discrimination and othering for these groups furthermore there's a lot of common tropes that are reflected in these words especially for women of color so for example the words describing latina women include things like vibrant and corvaceous which connect to a trope of tropicalism for asian women the words are things like petite and delicate and silky which connects to a long history of asian women being hyper sexualized seen as very docile and submissive and so on and finally for black woman we see that some of the top words are things like strong and resilient this connects to an archetype that people have called the strong black woman archetype and while it sounds like positive at first glance there's been work showing that this kind of archetype actually is very harmful because it puts a lot of pressure on these demographics to be resilient and strong against societal obstacles so rather than actually working towards changing those obstacles it puts pressure on these people to overcome them which leads to very negative health outcomes for these people among other harms more broadly we find that the words for each marked group pretty much just reflect very essentializing narratives so based on these patterns we conclude with three recommendations for model owners first we should as researchers be addressing positive stereotypes and essentializing narratives we should also be using intersectional lens to study biases and harms because there's a lot of things that might be overlooked if we don't do that and finally there should really be increased transparency about bias mitigation methods because for instance like these positive stereotypes we don't know if it's because there is some sort of like weird overly excessive value alignment going on or maybe some other like anti stereotyping methods that are resulting in these prunishous patterns we just really can't make any assumptions or really study that further without more transparency thank you so much for listening um have a good time at ac</sample>
    <sample id="187">Three</sample>
    <sample id="188">Iterative transfer learning is a process where the model is fine-tuned on both tasks, and then further fine-tuning on debate yields a much better zero-shot performance.</sample>
    <sample id="189">The goal of the dataset is to understand users' language when they want to make a choice.</sample>
    <sample id="190">An attacker can extract model parameters through an EaaS by learning from the embedding and providing similar services.</sample>
    <sample id="191">Three authors are involved in the paper.</sample>
    <sample id="192">The presentation introduces a new optimizer called "Ken" designed to achieve both fast convergence and low memory usage in training large language models. It begins by highlighting the challenges of traditional adaptive gradient-based optimizers like Adam, which require significant memory for first and second moment estimates. The presenter then discusses non-negative matrix factorization (NMF) as a preliminary technique to reduce memory requirements, noting that NMF can significantly decrease memory usage from O(MN) to O(M+N). However, NMF operations in AdaFactor introduce errors that affect training stability.

To address this, the presenter proposes an efficient approach to handle erroneous updates by calculating the residual between momentum updates and current updates, using this residual as a denominator to take more adaptive steps. This method is compared to AdaFactor and shows improved performance on tasks like GPT-2 and T5, with Ken achieving about 3.4% higher validation accuracy than AdaFactor while using less memory. The presentation concludes by emphasizing the efficiency and effectiveness of Ken in both training and inference scenarios, making it a promising extension for existing memory-efficient optimizers.</sample>
    <sample id="193">1000</sample>
    <sample id="194">The authors of the paper are affiliated with Carnegie Mellon University and the University of Washington.</sample>
    <sample id="195">The video introduces a new framework called ROHT, which stands for Reasoning over Hierarchical Question Decomposition Tree. This framework aims to improve the process of answering complex questions by breaking them down into smaller, more manageable sub-questions. The framework consists of two stages: first, it builds a hierarchical question decomposition tree (HQDT) to understand the structure of the complex question, and second, it performs probabilistic reasoning over the HQDT to integrate knowledge from various sources and generate the final answer. The video explains that traditional methods have limitations, such as incomplete knowledge bases and difficulty in integrating knowledge from heterogeneous sources. ROHT addresses these challenges by allowing flexible selection of knowledge sources for each sub-question and using a probabilistic approach to aggregate answers from multiple sources. The framework is evaluated on challenging datasets and shows significant improvements over existing methods, demonstrating its effectiveness in handling complex questions.</sample>
    <sample id="196">Lisa bought and Meggy</sample>
    <sample id="197">The state-of-the-art models in dialogue systems are transformer models.</sample>
    <sample id="198">Because the models are sensitive to latent syntactic and semantic features which are shared across the sentences, and the MP evaluation currently with short and single sentence input may not fully capture the language model's abstract knowledge throughout the context window.</sample>
    <sample id="199">Yes, training in multilingual fashion caused performance drop compared to monolingual English model.</sample>
    <sample id="200">Yes, the annotators know about the entity in advance.</sample>
    <sample id="201">The evaluation used state-of-the-art MT metrics and expert-based human evaluation results.</sample>
    <sample id="202">The regression in generalization impacts all NER types.</sample>
    <sample id="203">Positionality in NLP matters because it can lead to systematic performance differences of technology between populations, which may result in biased outcomes and affect the accuracy of AI models.</sample>
    <sample id="204">The multilingual LLMs like BLOOM were fine-tuned with adapters.</sample>
    <sample id="205">The speaker is presenting their work on the political biases in language models. They explain that language models are trained on large-scale web crawl data, which includes political news media from sources like the New York Times and The Guardian. This diverse training data allows language models to learn from various perspectives, but it also introduces social biases that can lead to unfairness in downstream tasks.

To address this issue, the speaker proposes investigating the political bias propagation pipeline from pre-training data to language models and then to downstream tasks. They ask two main questions: how do we evaluate the political leaning of language models and what role does pre-training data play in these biases? Secondly, how do language models with different political leanings perform on downstream tasks and whether this might result in fairness issues in NLP applications?

The speaker suggests using different prompt formats and political questionnaires to evaluate language models' political leanings. Preliminary results show that language models have varying political leanings, with GPT-4 being the most liberal. They also conduct controlled experiments by pre-training language models on partisan corpora to see if the ideological coordinates shift accordingly.

The speaker then investigates whether language models can pick up the polarization prevalent in modern society. By pre-training models on corpora before and after the 45th president of the United States, they observe a general shift towards more extreme political leanings post-2017.

Finally, the speaker evaluates language models with different political leanings on hate speech detection and fake news detection tasks. They find that left-leaning models are better at detecting hate speech targeting socially minoritized groups but worse at targeting more powerful groups. Conversely, right-leaning models are better at detecting hate speech targeting white and men but worse at targeting Black LGBTQ+ and other minority communities. Similar trends are observed in fake news detection.

The speaker concludes by highlighting the fairness issues resulting from language model political biases and the dilemma between sanitizing political opinions in training data and risking censorship or exclusion. They emphasize the need to acknowledge and tackle these fairness issues in NLP applications.</sample>
    <sample id="206">They use a BERT model for transfer learning.</sample>
    <sample id="207">The recent test sets used to assess the PaLM capabilities are the best practices of the NMT community.</sample>
    <sample id="208">Three</sample>
    <sample id="209">The gain of the proposed method over the strongest baseline is 20.</sample>
    <sample id="210">The speaker's name is Shuheng Liu.</sample>
    <sample id="211">Yes, the results and dataset in the paper can be used as a benchmark for future research on automatic text simplification.</sample>
    <sample id="212">They experiment with two smaller models in the paper.</sample>
    <sample id="213">OFA</sample>
    <sample id="214">Hello everyone, my name is Jing Wei from the University of Science and Technology of China. It's my pleasure to give a short advertisement video about paper: Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark. Let's first introduce the background about embedding as services. Currently, large language models such as GPT, LLaMA, Prolific are exceptional in natural language understanding and generation. Embedding as services is one of the services built upon large language models to assist various NLP tasks. For example, OpenAI offers a GPT-based embedding API. However, recent works have shown that the attacker may steal the model through learning from the embedding and provide similar services. Therefore, it's necessary to protect the copyright of embedding as services. To protect the copyright of embedding as services, one of the solutions is to embed a watermark in the provided service and detect whether another service contains the watermark. The watermark method needs to meet the following properties: First, the method should be applicable to embedding as services. Second, the watermark should not degrade the utility of the provided embeddings. Third, the watermark should be covert enough to the attacker or the attacker can remove the watermark easily. Finally, the watermark need to be transferable to the attacker's services during the model extraction process. Existing works can be broadly classified into four categories. However, these methods either not applicable to embedding as services or lack of transferability. Therefore, in this paper, we propose Embedding Marker, which is a backdoor-based watermark method applicable to embedding as services. Then let me introduce the details of our Embedding Marker. Embedding Marker contains two main steps: watermark injection and copyright verification. Before these main steps, we first select a trigger set. The trigger set is a group of words in a moderate frequency interval. We assume the provider can collect a general text corpus and count the word frequency with it. In watermark injection, we first define a target embedding. When a user sends a sentence to the provider's service, the provider counts the trigger number in the sentence. The provided embedding is a weighted summation of the target embedding and the original embedding. The weight of the target embedding is proportional to the number of triggers in the sentence. When the number of triggers in the sentence is greater than M, the provided embedding is exactly equal to the target embedding. Copyright verification is to detect whether a model behind another service contains the watermark. We first construct a backdoor and benign dataset. Backdoor dataset contains sentences of which all words belong to the trigger set while all words in the sentences of benign dataset do not belong to the trigger set. Then the provider requests embeddings from the stolen service with the dataset. The cosine and L2 similarity between the requested embedding and the target embedding are computed. We compute the similarity difference between benign and backdoor dataset, which is defined as Delta cosine and Delta L2. Meanwhile, we also apply KS test and use its p-value as the third metric. We conduct experiments on four datasets: AGNews, Yahoo! Answers, SST-2, and Yelp. We assume the provider apply Wikipedia dataset to count word frequency. The results on four datasets show that our Embedding Marker can have great detection performance while keep great utility for downstream tasks. We also validate the covertness of the provided embedding by visualizing the embedding of sentences on UMAP PCA. The legend of the figures means the number of triggers in each sentence. As shown in the figures, it's hard to distinguish between the backdoor embeddings and normal embeddings. That's all, thank you. Welcome to discuss with us.</sample>
    <sample id="215">The speaker, Adam Siprowski, discusses the dependency structure of coordination in linguistics. He explains that different theories and approaches assume various dependency structures, such as the universal dependencies approach where the first conjunct is the head of the coordinate structure. He contrasts this with Igor Miltruk's meaning text theory, which also assumes a symmetric approach. The Prague approach and multi-headed approaches are also mentioned. The speaker then introduces the principle of dependency length minimization, which suggests that shorter dependencies are preferred. He uses examples to illustrate how this principle can be applied to coordinate structures, showing that sentences with shorter dependency lengths are generally more acceptable. The speaker then presents statistics from the Penn Treebank, confirming that left conjuncts tend to be shorter and that this tendency increases with the length difference between the two conjuncts. However, he notes that this tendency only occurs when there is no external governor on the left. The speaker concludes by arguing that this observation provides an argument against asymmetric structures of coordination and in favor of symmetric structures.</sample>
    <sample id="216">hi i'm sarah papi from the university of trento and fondazione bruno kessler and i will briefly introduce the attention as a guide for simultaneous speech translation paper that is a joint work with matthew negri and marco turki what is simultaneous speech translation simultaneous speech translation or simultaneous is the process of translating spoken language into a text in another language in real time enabling cross-language communication and what are the problems of the current simultaneous models specific architectures are usually trained introducing additional modules to be optimized long and complicated training procedures for example training involving different optimization objectives and training and maintaining several models to reach different latency regimes for example training a model with an average of one second latency and another one with two seconds latency and so on so what is our solution first to use already existing offline nst models without retraining or adopting specific architecture for simultaneous use only one model for every latency regime and handle latency through specific specific parameters and leverage the knowledge already acquired by the model through the attention mechanism between audio input and textual output that is the cross-attention mechanism and you can see an example on the right our solution is to propose a dot or encoder-decoder attention and it is a strategy for which we decide whether to emit or not a partial translation based on where attention points to our word is emitted if the tension is not concentrated that is this sum is below a certain threshold alpha towards the last lambda speech frames meaning that the received information is enough stable for example if if we receive a speech chunk containing i'm going to talk about and our model predicts the translation in german and we will look at the cross-attention weights we will see that the first two words points to the earliest received speech frames while the last word points to the last received pitch frames as the lambda speech frames this means that the first two words will be emitted while since the sum of the cross-attention is above a certain threshold alpha we will not emit the last word and we wait for another speech chunk if we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross-attention weights we will see that no words points to the last lamb lambda speech frames this means that these three words will be emitted if we look at the main results of a dot we will plot the simultaneous speech translation results on on graphs in which we have blue on one side that measure the translation quality and average legging that is the latency measure and we also consider the computational aware average linking that accounts for the model's computational times to predict the output so we want our curves to be as high as possible on this plot but also we want that they are shifted on the left and we compare with proper strategies that are also applied to offline models that are the white key strategy and the local agreement and we compare also with the state-of-the-art architecture specifically tailored for simultaneous speech translation these are all the results of the simultaneous speech translation strategy on german and we see that a dot outperforms all the strategies applied to offline models since their curves are shifted over the left and we also see that if we consider the actual elapsed time or the computational aware time a dot is the fastest strategy if you want to discover more results read our paper and we also released open source the code and models and simultaneous output to facilitate the reproducibility of our work thanks for your attention</sample>
    <sample id="217">The presentation introduces a method for generating controllable dialogues with multiple attributes, focusing on compositional generation. It highlights the limitations of previous methods that primarily generate single-attribute dialogues and lack practical settings for multi-attribute generation. The proposed method, DC G (Distributed Controllable Generation), uses a disentangled concept to learn attribute concepts from single values and disentangles different attribute combinations using a disentanglement loss. A unified reference-free evaluation framework (MAE) is introduced to assess the performance of the model without requiring large-scale labeled data. The method outperforms baseline models in terms of controllability and task quality, demonstrating its effectiveness in handling unseen attribute combinations. The results confirm the method's ability to generalize from seen attributes to unseen combinations, making it a promising approach for generating controllable dialogues with multiple attributes.</sample>
    <sample id="218">The authors of the paper are affiliated with Google Translate.</sample>
    <sample id="219">The speaker introduces a research assistant at academia sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is conducted with Liang Huang, Chen Weiying, and advised by Professor Jero and Tran Luan. The background of financial report analysis is discussed, along with text definition and approaches. The target company, the front ten K, is considered, which contains many details of companies' important activities but requires lots of human efforts to mine useful information. The motivation for this work is based on two observations: the words in a company's report are very similar, and the contents are highly dependent. This figure illustrates the text similarity between two reports in continuous years. For example, the report in 2018 is similar to the one in 2017. Based on these observations, a highlighting task and a multi-stage pipeline are introduced.</sample>
    <sample id="220">The authors of the paper are affiliated with Stony Brook University.</sample>
    <sample id="221">The paper analyzed the translation capability of large language models using the best practices of the NMT community, involving using the latest test sets to avoid another lab of the test data with the training data of the language model.</sample>
    <sample id="222">The audio discusses the challenges and interventions in open-domain question answering, focusing on adapting or annotating to enhance the performance of models trained on general-purpose domains like Wikipedia. It highlights the limitations of using Wikipedia for domain-specific questions and suggests combining Wikipedia with domain-specific corpora to improve model generalization. The audio explains two main methods for generating data interventions: few-shot and zero-shot. Few-shot involves using a few examples from the target domain to prompt large language models, while zero-shot controls interactions among question, answer, and context variables. The audio also mentions the use of named entity recognition and uniform distribution sampling to generate closed-style questions, which are easier to curate than standard WQ format questions. Additionally, it discusses the concept of data set shift taxonomy, which categorizes shifts into concept, covariate, and full shifts, and measures compatibility by computing the likelihood assigned by the source model to target contexts. Finally, it concludes that few-shot adaptations are effective across all target sets, while zero-shot adaptations are useful for concept and covariate shifts, and no significant changes are observed in the no-shift category.</sample>
    <sample id="223">Jiangbing</sample>
    <sample id="224">The models investigated during the experiments were the Long Impart model and the Normal Base Long Impart model.</sample>
    <sample id="225">53 tasks are used for training and 19 tasks are used for testing.</sample>
    <sample id="226">There are two authors involved in the paper.</sample>
    <sample id="227">The audio discusses the challenges in current language models research, focusing on grounding language understanding. It explains that most language models are pre-trained with text corpora and lack grounding, leading to a gap between pre-training and downstream applications. The main issue is that generated plans or programs may not always be grammatically correct or executable. The proposed framework, named Pangu after a primordial being in Chinese mythology, involves a symbolic agent interacting with the environment and proposing candidate plans, while a language model scores and ranks these candidates. This approach leverages the strength of language models in discrimination rather than generation. The framework is tested on various language models and settings, demonstrating strong performance and sample efficiency. The results show that Pangu consistently outperforms baseline models like ArkanQAI in terms of sample efficiency, particularly when using in-context learning. Additionally, the distribution of probabilities for different language models under Pangu shows similar patterns, suggesting robustness. The audio concludes by emphasizing that discrimination might be a better strategy for grounded language understanding, and encourages further discussion and collaboration on this topic.</sample>
    <sample id="228">The authors experimented on four datasets: AGNews, Mind, SST2, and Yahoo.</sample>
    <sample id="229">The English content is a presentation on the topic of detecting improvable claims for argumentative writing support. The presentation introduces the importance of text revision in professional writing and how it can influence the effect of the text on the audience. It then explores the challenges that arise when working with revision-based data, such as representativeness and reliability, model complexity and architecture, contextual information, and topical and user bias. The presentation also discusses two new tasks: sub-optimal claim detection and claim improvement suggestion. The authors explore how to best model the quality of argumentative texts based on implicit revision patterns found in collaborative online debate platforms such as Kiyo. They conclude that revision-based data can be effectively employed for the given tasks and that modeling the distance between two claim versions is beneficial for detecting sub-optimal claims.</sample>
    <sample id="230">hi everyone i'm kostuvin and i'm pleased to welcome you to our talk of our acl 2023 paper language model acceptability judgments are not always robust to context this is a joint work with john gauthier aron mueller kanishka mishra karen funtes roger levy and adina williams so in this work we revisit the minimal pair paradigm so the minimal pair paradigm basically evaluates language models on top of acceptability judgments which can also include grammaticality like bleep syntax gym or acceptability in terms of stereotypes such as crowd spares and in this a minimal pair paradigm the typical way to evaluate language models is that you show like a acceptable sentence or grammatical sentence and then you show an unacceptable sentence or an ungrammatical sentence and then the hope is of the model basically puts more probability to the acceptable sentence the current mpp pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences these days large language models are coming up with longer and longer context windows so it's crucial that we evaluate the models acceptability throughout the context window and that is what we are trying to do here we're trying to revisit the mpp pipeline by asking the model to evaluate acceptability on longer and longer sequences so that is the approach so what we do is that to simulate these longer sequences we revisit the datasets themselves and then we recreate sentences by choosing like acceptable or unacceptable sentences from those datasets so for example here we have chosen like a typical pair of grammaticality from the bleep data set from the adjunct island case and what we do is that to recreate like longer sequences and which are acceptable and which has the same matching of the grammatical structure we extract grammatical sentences from augment island and then we add it as a prefix to both the acceptable query and the unacceptable query so we can do the same thing by choosing unacceptable sentences from the same matching and that could also like be used to test the model's acceptability and we can also do the same by choosing sentences from a different subset or a different data set so that is what we call as the mismatch scenario so here the sentences are still coming from relevant datasets but it's not from the same data set that you are evaluating with and we can do the same for unacceptability case finally we can choose sentences from a completely unrelated domain such as wikipedia so this will tell us like whether the models acceptability judgments are actually impacted by any context like whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at so how does the model do so first we look at the wikipedia sentences which are completely irrelevant to the current query pair and there we find that the mpp judgments are mostly robust for arbitrary context length we increased the context length toward up to 1,244 for to max out opt and gpt-2 models and we saw here in the orange dot line the mpp judgments are relatively stable now what happens when we choose sentences from the same data set so here we are choosing or creating sentences from acceptable and unacceptable domains from the same bleep or syntax gym data set and there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable prefixes but when we match the structure that is when we choose the sentences from the same phenomena in bleep person text gym we see a massive increase or a massive decrease in of the mpp judgment for the model depending on whether the chosen prefix is acceptable or unacceptable now this and this is very large like this effect increases throughout the context link and this would probably affect like newer language models which has large context window so why does the match prefix affect the language model judgment so much so we did a series of analysis where we tried to like perturb the input sentence by trying to preserve the relevant structure but adding like noise to the input and after doing like several of these perturbations we find that none of these noises are actually making the model like change it course in terms of how it shows us the mpp judgment trend basically we find that the models are sensitive to the perturbed sentences in similar ways that is when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgments in similar fashion so the key takeaways of our work is that language models are sensitive to latent syntactic and semantic features which are shared across the sentences and the mpp evaluation the way that we do it currently with short and single sentence input may not fully capture the language models abstract knowledge throughout the context window please read our paper for more details of our experiments thank you for listening</sample>
    <sample id="231">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="232">The speaker's name is Avi Lillard.</sample>
    <sample id="233">The audio content is a presentation by Sara Papi from the University of Trento and Fondazione Bruno Kessler, introducing a paper titled "Attention as a Guide for Simultaneous Speech Translation." The presentation explains the process of simultaneous speech translation (SST) and its challenges, such as long training procedures and the need for multiple models to achieve different latency regimes. The proposed solution involves using existing offline models without retraining or specific architectures, focusing on a single model with adjustable parameters to handle different latency requirements. The strategy, called "ADT" or Encoder-Decoder Attention, decides whether to emit partial translations based on attention weights, ensuring stable and accurate translations. The presentation also discusses the evaluation of the ADT strategy using metrics like translation quality, average latency, and computational load, comparing it to other strategies and state-of-the-art architectures. The results show that ADT outperforms other strategies in terms of translation quality and computational efficiency.</sample>
    <sample id="234">The prompting strategy has a significant impact on the results, with differences of more than one BLEU point observed in some cases.</sample>
    <sample id="235">The authors of the paper are affiliated with Carnegie Mellon University, Language Technologies Institute, Technico Lisboa, BAIR, and Unbabel.</sample>
    <sample id="236">The 5 expert-written instructions are used to represent the input texts, images, instruction, and bounding boxes in the same token space.</sample>
    <sample id="237">The authors propose a diagnostic test suite for knowledge integration, introducing a coreference resolution task designed to probe the ability to draw on knowledge available in different sources.</sample>
    <sample id="238">The video introduces a new benchmark dataset called "MeetingBank," which is designed to address the challenges of summarizing meetings. The presenter, Yubin Hu from the University of Central Florida, explains that high-quality meeting summaries are scarce and it's difficult to find trustworthy resources for public meetings. To tackle these issues, they created a repository of city council meetings, including meeting transcripts, reference summaries, and URLs containing various useful resources.

The data collection process involves using speech-to-text APIs to convert audio data into transcripts, identifying meeting types and dates from meeting websites, and aligning time stamps to get segment transcripts paired with reference summaries. The dataset includes 1366 city council meetings and nearly 7000 instances, providing detailed statistics such as meeting duration, token per meeting, speaker frequency, and year period.

For data analysis, the team measures the level of abstraction in meeting summaries using coverage and density scores. They evaluate top-performing summarization systems on MeetingBank, including extractive and neural- extractive models like Oracle, Lead, LexRank, TextRank, BART-Large, Pegasus, Longformer, DialogLM, and HLMNet. They also use advanced metrics like ROUGE, METEOR, and BLEU scores, as well as question-answer-based metrics, to assess the quality of generated summaries based on informativeness, factuality, fluency, coherence, and redundancy.

The evaluation results show that GPT-3 achieves the highest overall score, particularly excelling in fluency and coherence but falling short in informativeness and factuality. This suggests that while GPT-3 performs well in generating coherent summaries, there is still room for improvement in capturing main discussion points. The video concludes by emphasizing the importance of developing new methods of automatic evaluation metrics to better align with human preferences and highlights the potential of MeetingBank as a valuable resource for researchers designing advanced meeting summarizers.</sample>
    <sample id="241">The speaker, Ethan, introduces a paper titled "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." The paper is a joint work with Yang Chen, Wei Xu, and Alan Ritter from Georgia Tech. The speaker explains that current approaches to automatically detecting misinformation on social media platforms often fall short in two key areas: realistic evaluation and human-centricity. He highlights issues such as the unrepresentative nature of datasets used for evaluation, the possibility of leaked counter-evidence, and the lack of human involvement in the detection process. The proposed framework aims to address these deficiencies by integrating human feedback throughout the detection process and using a system that goes from raw tweets to actionable outputs. The speaker also discusses the implementation and evaluation of their workflow for detecting COVID-19 treatment misinformation, emphasizing the importance of early detection and policy violation verification.</sample>
    <sample id="242">Common evaluation methods for dialogue systems include human evaluation, such as asking human judges to select which of two conversations is better or to rate conversations given a Likert scale.</sample>
    <sample id="243">Five</sample>
    <sample id="244">Judges decide cases in law courts.</sample>
    <sample id="245">The English content presents a study on high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The study aims to address the limitations of automatic metrics and the unclear best practices for recruitment on MTurk. It introduces a two-step pipeline for identifying high-agreement workers, starting with qualification settings and tasks. The first stage evaluates annotators' ability to handle multiple dimensions correctly, categorizing them into four tiers: gold, silver, bronze, and block. Only gold and silver workers can proceed to the second stage, which tests their capacity to handle heavy workloads. The study also includes a reference-based task to assess general performance and compares the results with baseline MTurk workers and cloud research workers. The analysis reveals significant Spearman's correlation between the pipeline and cloud research workers, indicating similar quality at lower costs. The study concludes that the pipeline can achieve high agreement and quality while reducing resource usage and providing a best practice for high-agreement annotation at large scales. Future plans include investigating ways to attract higher-quality workers and exploring multi-task, multi-language, and multi-platform applications.</sample>
    <sample id="246">Yes, the code is available on GitHub.</sample>
    <sample id="247">The speaker introduces a new dataset called Fact KG, which focuses on fact verification using knowledge graphs. They explain that existing datasets like Fever and WikiC use Wikipedia text or tables as evidence, but there was no dataset utilizing knowledge graphs with natural language claims. The proposed dataset includes claims in two styles (written and colloquial) and five types of reasoning (one-hop, conjunction, existence, multi-hop, and negation). The dataset is used to verify claims by retrieving evidence from DBpedia and checking the claim's validity through different reasoning methods. The speaker also mentions that their method outperforms all other baselines, including claim-only baselines and a model that uses graph evidence.</sample>
    <sample id="248">No, the annotators for NLPositionality are not balanced in regard to each demographic.</sample>
    <sample id="249">Sentences in the acceptable domain were perturbed by adding noise to them while preserving their relevant structure.</sample>
    <sample id="250">To evaluate multiple dimensions of chat quality to understand the strengths and weaknesses of the model on a finer-grained level.</sample>
    <sample id="251">The authors of the paper are affiliated with the University of Science and Technology of China, Microsoft, and Sony AI.</sample>
    <sample id="252">The video features a presentation by a PhD student from the University of Washington, discussing their work on pre-training data and language models for downstream tasks. The presenter highlights the importance of pre-training in enhancing the performance of language models and its applications in various downstream tasks. They also mention the challenges faced in this field and the potential solutions they have explored. The presentation is informative and provides insights into the current state of research in natural language processing.</sample>
    <sample id="253">Hello everyone, my name is Mario Ezra Aragon and I am going to present our work named DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media. This is a group effort of researchers from Mexico and Spain. First, I want to start with the definition of a mental disorder, which is a psychological syndrome which is associated with distress and disability that affect your thinking, feeling, mood, and behavior. There are different types of mental disorders, for example Major Depression, PTSD, Bulimia, and Anorexia, and among others. Nowadays social media content is massive and provides an opportunity to do research on how people undergo difficulties. Many people use online platforms to possibly share their daily routines and important events, while others take advantage of the anonymity of these spaces to explicitly discuss mental health issues and seek help. In this work we aim to contribute to the detection of mental health disorders by automatically analyzing their social media posts. This type of analysis is expected to support a new technology able to warn about the onset of mental disorders and provide supporting evidence. Why using domain adaptation? Sometimes we have insufficient annotated data and want to improve the performance of a model on a target domain. For this we can use the knowledge learned by a model from another related or similar domain. Take for example BERT, BERT is a language model trained from the general data of Wikipedia and Google Books, and we want to train it to a more specific language of Reddit and mental health. Doing this adaptation we can adjust the vocabulary of the semantic understanding of the model and learn the domain-specific task. Here we have the general diagram of our proposed approach. First we start with a base language model and then integrate information from Reddit and mental health. We also incorporate the knowledge from a lexicon to guide the masking process. The main idea of our proposed approach is first to learn the social media language and then specialize in the mental disorder domain. Using the guiding masking we want the model to try to focus in important words during the training process. Here we have the general results using the iRisk datasets. We plot the precision and recall of our model and the baselines. Our model tends to locate in the main diagonal of the region, indicating its good balance, while other methods have a high precision or recall but score low in the other dimension. Let us illustrate the behavior of the learned model and the kind of textual segment it tends to pay more attention to. First we analyze the most likely words the model generates when given a sentence with masked word. A sentence we use example from the Beck Depression Inventory, this clinical tool which consists of 21 items aims to identify and measure the severity of typical symptoms of depression, for example in measure the mood, the pessimism, sense of failure, dissatisfaction, guilt, and among others. In the following table we can see some examples of these sentences and the answers returned by BERT and DisorBERT. With DisorBERT the answers tend to have a more negative meaning or psychological orientation compared to BERT. Take for example the sentence "I used to be able to cry" where we mask the word "cry" and DisorBERT predicts words like focus, talk, breath, sleep, and eat. These words are related to common problems that are associated with mental disorders and cause an interference in the thinking and behavior of the affected person. Here we also present all the words predicted by both models weighted by their frequency. This figures results from applying the two models to the entire set of 21 Beck Depression Inventory items. Similar to what happened before, BERT tends to generate more general words, while our model tends to be biased toward words related to mental disorders. Finally we want to visualize the most important sequences of the text by obtaining the most relevant words and sentences. For this we use a visualization tool that provides an interactive head view in the form of a graph. We select a depression user with the highest score in the BDI questionnaire and compute the attentions scores of the user post. We can observe that the most prominent words are related to anxiety and medication topics that are highly relevant to depression. As conclusion and future work, the combine effect of double domain adaptation and guided masking is effective at capturing signs of mental disorders in social media interactions. Our approach also obtain better results than those achieved by mentalBERT, a model trained with a large amount of data. The evaluation shows a solid balance between finding users and labeling them correctly. In future work we want to explore the application of different lexical resources as well as using clinical data. Thank you for your attention. If you have any questions please feel free to ask me.</sample>
    <sample id="254">The English content presents a research work titled "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction" by researchers from the Singapore University of Technology and Design. The presentation introduces the document-level relation extraction task, which aims to extract relationships between entities in a document. It highlights the challenges posed by noisy labels in distant supervision data, particularly the risk of false positive pseudo labels. To address this, the researchers propose a framework that includes a pre-denoising model trained on both distant and human-annotated data to generate pseudo labels. They introduce uncertainty estimation to determine the reliability of these labels, proposing an instance-level uncertainty estimation method to capture overlapping relations. Additionally, they design a dynamic class uncertainty threshold strategy to filter out pseudo labels with high uncertainty. The framework also incorporates a multi-phase training strategy to iteratively re-label the distant supervision data. The presentation concludes with a comparison of the proposed framework against several state-of-the-art baselines on two public datasets, demonstrating superior performance.</sample>
    <sample id="255">It is important for zero and one shot prompting.</sample>
    <sample id="256">hello my name is vasudha and i am a computer science phd candidate at stony brook university i would like to present our work accepted into acsl 2023 as a long paper transfer learning for dissonance detection addressing the rare class challenge we begin by defining cognitive dissonance and why it is an important problem to study in language simply put cognitive dissonance is two beliefs or actions that are inconsistent such as this example where a person states i know that cigarettes could kill me and then goes on to say i grabbed a couple of smokes after the meeting this belief and action are inconsistent and they are in dissonance further mentioning that i don't think i could keep my job without them justifies the second occurrence and they have a consonance relationship while dissonance is a very common phenomenon we experience in daily decision making they are really rare to find expressed in language among other kinds of risk score relations so why does this matter studying cognitive dissonance can help us understand the effects of dissonement among people track trends and belief values and attitude changes in population high cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups finally cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better to the goal of creating a cognitive dissonance resource we conducted a large-scale annotation of dissonance relations we used a dissonance first approach as seen in the flowchart here tweets were passed using a patty b parser and pairs of discourse units were annotated according to the guidelines that are described in our paper as can be seen here dissonance was only found in three point five percent of the annotated pairs on collecting around thousand examples of discourse unit pairs we ran training for an initial classifier trained only on forty three examples of dissonance to no surprise the classifier performed not much better than chance given the low occurrence of dissonance and absence of any prior such data set we are facing the problem of absolute rarity to alleviate this we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds lowering the overall annotation costs while improving dissonance detection since the initial model was not able to capture the dissonance class at all we start the active learning process by transferring weights from closely related tasks we transfer from two different tasks topic independent dissonance stance classification a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here and on binary classification of expansion and comparison classes of patty b since these two are closely related to the conception of consonants and dissonance and we call them ce here we find that on transferring the zero shot performance on the annotated data set is already much better than chance with the best with au c point six two further on iteratively fine-tuning on both tasks we find that fine-tuning of ce task followed by further fine-tuning on debate yields a much better zero shot performance thus this is the model that we use to cold start the active learning next we determine the best method to update a model with new data from each round of active learning and annotations cumulative accumulates all the data collected from active annotations whereas iterative updates the model by training on the latest set of data collected over the different strategies we found that cumulative performed equal or better than iterative across the board next to improve the number of dissonance examples we use a probability of rare class strategy prc to select mostly the examples that are highly likely to be dissonant by the current model at any round of al we compare this to the other state-of-the-art state-of-the-art strategies that are commonly used in the community we find that the proposed prc strategy works better than other straight state-of-the-art strategies although the difference is small note that the performance is significantly lower for random on further rounds of al with two best strategies we improved dissonance classification au c to points seven five which is the best performance that we have on the task so far we also check the feasibility of each strategy for annotation quality and cost to annotators we find that prc has the highest percentage of dissonance and works best for rare class however the annotators also find the examples difficult in summary we find that prc is a simple al strategy for rare class acquisition and cold starting al with appropriately designed transfer learning task and helps significantly we also find that iterative update is useful for transfer learning from a different domain whereas in-domain active annotations benefit from cumulative update these are the links to our core data set and our paper feel free to get in touch with us if you have any questions thank you</sample>
    <sample id="257">Four state-of-the-art chat models</sample>
    <sample id="258">The video introduces a new work by Zhang Han Chang and Hung Lee from National Taiwan University, discussing the potential of large language models (LLMs) as an alternative to human evaluations in natural language processing. The researchers propose using LLMs to evaluate text quality by providing them with instructions and samples to rate. They argue that while human evaluations are unstable and hard to reproduce, LLMs can potentially perform similar tasks more consistently. The study explores the use of LLMs to rate stories generated by GPT-2 or written by humans based on attributes like grammar, coherence, likability, and relevance. Human evaluators, particularly English teachers, were used to provide ground truth ratings for comparison. The results showed that while some smaller LLMs did not show a significant preference for human-written texts, larger models like BERT and ChatGPT demonstrated a clear preference, suggesting their potential as alternatives to human evaluations.</sample>
    <sample id="259">The speaker introduces Exemplar, a system designed for cross-lingual semantic parsing in multiple natural languages and many representations. Semantic parsing involves building semantic representations of user queries, while cross-lingual semantic parsing translates these queries into various formats like SQL or Lambda Calculus. The challenge lies in the lack of coverage in certain languages and representations, such as Chinese and Lambda Calculus.

Exemplar addresses this by providing a unified dataset with 9 datasets across 15 language families, 5 semantic parsing tasks, 80 million representations, and 22 natural languages. It evaluates six settings: translate test (using Google Translate API), monolingual model, monolingual few-shot setting, multilingual model, cross-lingual zero-shot transfer, and cross-lingual few-shot transfer.

The evaluation results show that encoder-decoder models outperform other models, especially on multilingual settings. However, English performance drops in some datasets, indicating a curse of multilinguality. The cross-lingual performance gap is significant in zero-shot settings but narrows in few-shot settings. Encoder-decoder models also improve when trained on a mixture of languages, particularly English.

Overall, Exemplar serves as a comprehensive benchmark for cross-lingual semantic parsing, providing insights into the strengths and weaknesses of different models and training strategies.</sample>
    <sample id="260">There are six authors involved in the paper.</sample>
    <sample id="261">A good planner should write scripts that are reasonable and faithful to constraints.</sample>
    <sample id="262">The paper involves 9 authors.</sample>
    <sample id="263">The presentation introduces a study on mitigating label biases in in-context learning (ICL), a popular paradigm for utilizing large language models. The research highlights that ICL's instability arises from various design choices, such as the choice and order of in-context examples, which introduce biases into the model's predictions. The study aims to address these problems by categorizing existing findings on bias problems in ICL, detecting new types of biases, and proposing a novel calibration method to mitigate their effects.

The study focuses on text classification tasks and identifies three types of label biases: vanilla label bias, context label bias, and domain label bias. Vanilla label bias captures the model's preference for certain label names, context label bias is influenced by the context, and domain label bias is affected by the task corpus. The researchers conduct experiments using random words and domain-specific words to demonstrate how these biases can impact model performance.

To handle these biases, the study proposes domain context calibration, which uses content-free text to estimate and mitigate the model's biases on each label name. This method improves the performance of ICL on datasets with varying levels of domain label bias, particularly those with larger biases. The study concludes with comprehensive ablation studies that show the effectiveness of using random in-domain words over single predefined tokens to account for domain label bias.</sample>
    <sample id="264">The English content is a presentation on a research paper titled "TAVT: Towards Transferable Audio-Visual Text Generation." The paper introduces a new task called transferable audio-visual text generation, which aims to bridge the gap in multi-modal text generation tasks like audio-visual text generation. The main challenge is the multi-modal domain shift, such as visual style and audio energy. The framework proposed includes three components: audio-visual meta map network, audio-visual encoder, and language model generator. The meta map network maps different visual concepts across domains into a unified audio-visual semantic space, while the encoder and generator focus on reconstructing audio using visual context and improving the semantic of tokens through contrastive learning. The training details involve selecting query sets from one domain and the remainder as queries for another target domain, fine-tuning the model, and evaluating it on the target domain. The experimental section evaluates the approach using two benchmarks based on MSVD-T and MSVD, including cross-domain settings. The results show that the proposed approach outperforms state-of-the-art models on both cross-domain and cross-dataset settings, especially in low-resource domains with limited data. Additionally, ablation experiments analyze the impact of audio features and semantic concepts.</sample>
    <sample id="265">The speaker's name is Vasudha.</sample>
    <sample id="266">The authors of the paper are affiliated with the University of Paderborn.</sample>
    <sample id="267">hello everyone my name is yusen zhang from the penn state university today i'm going to present our work exemplar cross-lingual semantic parsing in multiple natural languages and many representations so semantic parsing is a task to build semantic representations of user queries such as sql and lambda calculus and cross-lingual semantic parsing is the task to translate queries in multiple natural languages into multiple meaning representations as shown in this figure we need to translate the query in multiple natural languages using neural models to sql lambda or funql and etc existing cross-lingual semantic parsing models are separately proposed and evaluated on dataset of limited tasks and applications for instance there lacks of coverage on certain natural language the chinese is missing and lack of coverage on certain meaning representations the lambda calculus is missing or they are only evaluated on certain neural model for example there's only one single model to evaluate them so to this end we propose exemplar we provide a uniform dataset exemplar for cross-lingual semantic parsing in multiple natural languages and many representations it contains nine datasets in various domains five semantic parsing tasks 80 meaning representations and 22 natural languages in 15 language families and to better evaluate our benchmark we consider the six settings for training and evaluation the first one is translate test we use google translate api to translate source to the target language then use monolingual model to train and evaluation and for example we train the english model on english query and during inference we translate the german query using api to english and then use the trained model to predict the sql and we also test monolingual model in this setting the source language is the same as target language for example german to german or english to english we also test monolingual multi-language setting by training model in the models with only 10 percent of training data and we test monolingual multilingual model which we train one but multilingual model for all languages for example we put the german english chinese queries together to train a multilingual model and during inference we can use this model to um to translate german queries or chinese query or etc and we also consider cross-lingual zero shot and few-shot transfer we train on one source language and transfer to another language so during training we train on english query or the combination of english and german few shot queries to train a multilingual model to and predict the sql output and we also find many interesting results so regarding analysis of monolingual models we evaluate on two groups of models including encoder pdr which stands for multilingual pre-trained encoders with pointer based decoders such as xlmr plus pdr and we also evaluate encoder decoder models which is multilingual pre-trained encoder decoder models such as mbert and mt5 we found that encoder decoder obtains the best performance on all nine datasets and we evaluate on mt5 and example xlmr plus pdr on multilingual setting we found that encoder decoder or encoder pdr can be improved by training in a mixture of various languages and we found it is because most of the major natural languages can obtain performance gain except that english performance drops in seven datasets and only gains in three datasets i think this is known as curse of multilinguality we also compare the cross-lingual performance gap in this figure the blue line is cross-lingual few-shot transfer the orange line is cross-lingual zero-shot transfer while the green line is the monolingual setting we found that by comparing the green and orange line we found that for zero-shot setting the cross-lingual transfer performance gap is significant and by comparing blue and orange line we found that few-shot setting the transfer gap is shortened rapidly we also find some other interesting findings for example encoder decoder outperforms previous work or achieved comparable results for training on english natural language and significantly boost the performance of few-shot on target natural languages and we found multilingual language models such as codas and blue are still inadequate for cross-lingual semantic parsing tasks to sum up we build exemplar a unified benchmark for cross-lingual semantic parsing with multiple natural languages and many representations we conduct a comprehensive benchmark study on three representative types of multilingual language models and our result shows many interesting findings and etc and welcome to visit our paper and code thanks for listening</sample>
    <sample id="268">The most common errors of PaLM are omission errors.</sample>
    <sample id="270">The authors of the paper are affiliated with Emory University and Amazon Alexa AI.</sample>
    <sample id="271">Continous fine-tuning</sample>
    <sample id="272">There are six authors involved in the paper.</sample>
    <sample id="274">The speaker's name is Justin Jung.</sample>
    <sample id="275">hi i'm shanbing phd student in university of washington today i'm presenting our work from pretraining data to language models to downstream tasks tracking the trails of political biases leading to unfair nlp models so language models are trained on large-scale web crawl data political news media are well covered in their pretraining data according to a survey of the c4 corpus we can see that new york times los angeles times the guardian huffington post etc are well covered in language model training data this has created a mixed blessing for language model applications so on one hand they were able to learn from diverse perspectives which celebrates democracy and the plurality of ideas on the other hand these different political opinions are inherently socially biased and might lead to potential fairness issues in downstream task applications to this end we propose to investigate the political bias propagation pipeline from pretraining data to language models to downstream tasks specifically by asking the following questions first how do we evaluate the political leaning of language models and what role does pretraining data might have on such political biases secondly how do language models with different political leanings actually perform on downstream tasks and whether that might result in fairness issues in nlp applications so specifically we first propose to prompt language models with different prompt formats using the political questionnaires such as the political compass test this ensures us to do automatic evaluation well grounded in political science literature so some preliminary results demonstrate that first language models do have varying political leanings they occupy all four quadrants on the political compass we can also see that gpt-4 is the most liberal language model of them all and gpt series are generally more socially liberal than bert series and its variants secondly we aim to investigate to which to which extent the political biases of language models are actually picked up from training data so we would conduct a controlled experiment by further pretraining language model checkpoints on six different partisan corpora separated into news and social media further divided into their political leanings by further pretraining language models on such partisan corpora we can see that the ideological coordinates of the language model also correspondingly shift for example for roberta further fine-tune further trained on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its in terms of its political biases and we also try to investigate whether language models can pick up the polarization that's prevalent in our modern society so we divide pretraining corpora into pre forty fifth president of the united states and after forty fifth president of the united states we separately pretrain language models on the two different temporal corpora we can see that language models generally had a political leaning that is further away from the center after 2017 so this indicates that language models can also pick up the like polarization in our society so last but not least we evaluate language models with different political leanings on hate speech detection and fake news detection to nlp applications that often involve language models and could have very significant implications so we see that if we investigate the per category performance that is to say if we separate the performance into different demographics or political leanings of news media we can see a pattern that for example for hate speech detection left-leaning language models are better at detecting hate speech targeting socially minority groups however are worse at detecting hate speech targeting more powerful groups in our society and vice versa right-leaning language models are better at detecting hate speech targeting white and men however worse at detecting hate speech targeting at black lgbtq plus and other minority communities similar trends also happen for fake news detection where we see that left-leaning language models are better at detecting misinformation from their opposite political leanings and vice versa this in we further show many qualitative examples to see that language models with different political leanings do give different predictions to hate speech and misinformation examples based on their social calendar there are a bunch of more examples in the appendix to further highlight that this indicates that there is a fairness issue that is very pressing regarding the political biases of language models for example if a right-leaning language models were to be fine-tuned on hate speech or misinformation or whatever and deployed to a popular social media platform this would mean that people with opposite political opinions might be marginalized and the hate speech targeting minority groups might just run rampant without any control so this has sounded the alarm for us to acknowledge and tackle the fairness issues resulted by language model political leanings so a little bit of discussion we would also like to highlight that we expose the unique dilemma regarding language model political biases it's like between sylle and kyrpides so if we do not sanitize the political opinions in language model training data the bias will propagate from pretraining data to language models to downstream tasks ultimately creating fairness issues if we do try to sanitize somehow we would also risk censorship or exclusion and it's incredibly hard to determine what is actually neutral and should be retaining language model training data so it's kind of like the electrolyte electrolyte problem okay great i think that's pretty much all i have to say i have five for today thank you for your time</sample>
    <sample id="276">The presentation introduces a dataset called IndicMT Eval, designed to evaluate machine translation metrics for Indian languages. It highlights the importance of studying evaluation metrics for translations in multiple languages rather than adopting those proposed for English. The study focuses on five Indian languages: Tamil, Malayalam, Hindi, Marathi, and Gujarati, using the FLORES dataset to generate candidate translations for each source sentence. Human annotators then evaluate these translations, marking errors, their severity, and providing overall scores. The presentation discusses various evaluation metrics, including QM, BLEU, METEOR, and Comet, comparing their correlations with human scores across different languages. It also explores the performance of recent models like NLLB and IndicTrans compared to older ones like CVIT. The results show that Comet variants outperform Comet baselines in terms of correlation with human scores, particularly for accuracy errors. The presentation concludes by emphasizing the significance of this dataset in improving the evaluation of machine translation systems for Indian languages.</sample>
    <sample id="277">It does not have a name.</sample>
    <sample id="278">The "marked words" method is a way to identify the words that distinguish marked groups from unmarked ones. It draws upon the sociolinguistic concept of markedness, which states that there is an unmarked default and any group that differs from that default is linguistically marked. The method uses weighted log odds ratios to distinguish the top words for each marked group.</sample>
    <sample id="279">University of Washington</sample>
    <sample id="280">The speaker introduces their work on MultiEMO, an attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations. The goal is to predict the emotion label of each utterance in a dialogue, considering textual, audio, and visual modalities. Existing methods focus on modeling speaker and contextual information but face challenges like unexploited complementarity of multimodal information, unsatisfactory performance in minority motion classes, and difficulty in distinguishing between semantically similar motions.

To address these issues, MultiEMO proposes a novel visual feature extractor called Vis-Net, which integrates facial expressions from multiple frames without encoding redundant scene-related information. It also introduces MultiAtt, a multi-modal fusion model based on bi-directional multi-head cross-attention layers that effectively integrates textual, audio, and visual modalities. Additionally, Sample Weighted Focus Contrastive Loss (SWFL) assigns higher importance to minority classes and makes samples with different emotion labels mutually exclusive to maximize inter-class distances.

Experimental results show that MultiEMO achieves state-of-the-art performances on two ERRC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions. However, Vis-Net does not distinguish between speakers and irrelevant people, SWFL requires a large batch size, and the performance in minority emotions is still worse than majority classes.</sample>
    <sample id="281">The speaker, Justin John from Penn State University, presents a work on cross-lingual semantic parsing in multiple natural languages and many representations. Semantic parsing involves building semantic representations of user queries like SQL and Lambda Calculus. Cross-lingual semantic parsing is the task of translating queries in multiple natural languages into various representations. The proposed Exemplar dataset includes 90 datasets across 15 language families, 5 semantic parsing tasks, 80 many representations, and 22 natural languages. Six settings are considered for training and evaluation: translate test, monolingual model, monolingual few-shot, multilingual model, cross-lingual zero-shot, and few-shot transfer. The speaker evaluates models using encoder-decoder and encoder-only models, noting that encoder-decoder models perform best on all datasets. Different languages exhibit varying proportions of discourse phenomena, which are analyzed using the MuDA tagger. The MuDA benchmark shows that context-aware models are more accurate than those without context for certain discourse phenomena like formality and lexical cohesion. The speaker concludes by summarizing findings from a data-driven analysis across 14 language pairs to identify when translations require context and how different translation systems perform at document-level translation.</sample>
    <sample id="282">The paper introduces a new task of constrained language planning, which imposes different constraints on the goal of planning. The authors first evaluate and improve the constrained language planning ability of large language models. Since no dataset of specific goals exists to support their study, they have to acquire this data first. They extend the abstract goals with multi-faceted constraints for human-in-the-loop data acquisition using InstructGPT. They sample 100 specific goals and evaluate the scripts generated from large language models. The results show that the overall accuracy of the scripts generated by large language models is high. The authors then conduct extensive experiments to transfer fairy tales or audio stories to typical styles or styles both automatically and manually. The results confirm the effectiveness of their model and show that StoryTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">Prague</sample>
    <sample id="284">The speaker, Peng Tian Shuo from Wuhan University, is presenting a long paper at the ACL main conference 2015. The paper is titled "FSUIE: A Novel Fuzzy Span Mechanism for Enhancing Universal Information Extraction." The current spam-based UIE model involves identifying and labeling the spam boundaries of the target in the text, which relies on boundary positions of annotated spam. However, there is ambiguity in labeling the golden spam boundary, as different annotation spans can be considered reasonable. Therefore, the proposed method suggests that the spam boundary learned by the model should be fuzzy instead of precise. Additionally, there is a mismatch between Transformer feature extraction and information extraction. Basic Transformer focuses on global features, which ignores the prior hypothesis that span has limited length. Hence, the proposed method suggests that the attention used for span extraction decision should be adaptive rather than static. To model the fuzzy spam boundary, the target boundary is represented as a continuous distribution of correct probability in a specific range, where Rmin and Rmax represent the start and end of the fuzzy boundary, and the function Q represents the correctness of the current position. Through the sampling function shown in the slide, the continuous boundary distribution is converted into a group of discrete values for calculation of fuzzy span loss. The boundary distribution predicted by the model will calculate boundary cross-entropy with golden boundary as BCE loss and add KL divergence between predicted boundary with fuzzy span boundary as supplementary information to get the model in obtaining a more reasonable attention distribution for span extraction. We propose a fuzzy span attention as a mask function to trim attention distribution. The image and the formula of the mask function G are shown in the slide, where the fuzzy span is reflected in two aspects. On the one hand, by introducing optimizable parameter delta to adjust the length of the four attention range, the attention span of the model is dynamically changing. On the other hand, the attention distribution on the attention span boundary linearly decays rather than truncates. The overall structure of the model is presented on the slide, where the fuzzy span attention layer is only added on the top level to guide the model's decision process without affecting the text encoding capability. The demonstrated capability of FSUIE is conducted experiments on three main information extraction tasks including named entity recognition, relationship extraction, and aspect sentiment triplets extraction. As for the results of named entity recognition, by introducing FSL and FSA, our FSUIE base achieved significant performance improvement compared to UIE base without fuzzy spam mechanism on small-scale dataset. The model is easier to learn universal attention spans resulting in more significant improvements. As for results on relationship extraction, FSUIE achieves new state-of-the-art results on datasets ACE 2004, 2005, and ADE. FSUIE uses one unified structure to extract relationship elements achieving better information extraction ability with simpler structure. Besides, FSUIE shows stronger generalization capabilities for domain-specific information. As for results on ASTE task, FSUIE also achieves state-of-the-art results on 14 Lab, 15 Lab, 16 Lab of ASTE V2 dataset and demonstrate competitive performance on 14 Lab dataset. The results of ablation study shows that FSA improves convergence speed by guiding the model to obtain a reasonable attention distribution. FSL enables the model to fully utilize annotated information and obtain a greater information extraction capability. The combined effect of the two will produce a greater enhancement. We also visualize the attention distribution of fuzzy span attention layer. Results show that the model focused on semantic information within a limited range of preceding tokens, which meets our expectations. In conclusion, in this work, we first proposed a novel fuzzy span loss that alleviates the model's reliance on spam boundaries, and then we proposed efficient fuzzy span attention to adaptively adjusting the attention span of model. And the FSUIE we proposed achieves excellent results in a wide range of IE tasks. Thank you for your listening.</sample>
    <sample id="285">The video discusses the challenges in dialogue summarization, particularly focusing on factually incorrect summaries generated by models. It introduces two main solutions: incorporating factuality-related objects during training or inference to enhance model accuracy, and developing a factual error correction model (FEC) that operates independently of the summarization model. FEC takes the source document and the model-generated summary as inputs and outputs a corrected summary. The video highlights the importance of addressing factually incorrect summaries due to their prevalence in both generated and reference summaries. However, it also points out flaws in current FEC evaluation methods, such as using factuality metrics like Fact CC and DA E, which may not accurately reflect the performance of FEC models. The video proposes a new taxonomy of factual errors based on content and form, and suggests combining human-annotated data with synthetic data for more comprehensive evaluation.</sample>
    <sample id="286">The speaker is named Sarah Finch.</sample>
    <sample id="287">Four</sample>
    <sample id="288">The datasets that can be used to test syntactic phenomena are the BLM and syntax gym datasets.</sample>
    <sample id="289">hello my name is koyo and i will be presenting our work titled when does translation require context a data-driven multilingual exploration this work was done in collaboration with patrick fernandez emily liu andrea f t martin's and graham nebik so a lot of translations depend on context for example how would we translate mo in this sentence well if the previous sentence was things could start to get dangerous if the minister is find out then mo refers to a spy but if the previous sentence was could it be anything serious doctor then mo refers to a birthmark so depending on context the meaning of the word changes and therefore its translation changes as well however evaluating how well models can translate cases like this is pretty hard firstly because only a small portion of translations depend on context which makes corpus level metrics like blue unable to capture these translations and some people have suggested targeted evaluation on context dependent translations but these resources only support limited types of context dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation in this work we tried to answer these two questions first when does translation require context and second how well do models handle these cases to answer the first question we started by measuring how much a word depends on context-dependent translation in the previous work we introduced cxmi as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x you can think of cxmi as the information gain from giving context to the model in this work we extend cxmi to point y cxmi which can measure context usage at the sentence level or at the word level we can think of words that have high p6 cxmi as ones that require context for translation now we analyze words with high p6 cxmi to look for patterns between these words and we perform our analysis on transcripts of ted talks that have been translating from english to fourteen different languages we perform our analysis at three different levels first we look at part of speech tags that have high means p6 cxmi and this allows us to find for example dual pronouns in arabic that have really high p6 cxmi and this can be explained because english doesn't have dual pronouns so you need context to determine if a protein is dual when translating into arabic and similarly we find that certain languages also require context when we want to choose the appropriate verb form we then look at vocabulary items that have high p6 cxmi averaged over all of its different occurrences and this helps us identify cases like the one here where in chinese you need context to translate proper nouns to make sure that you're using the same translation within the document and similarly we find that context is supported to translate in the right formality and finally we look at different individual tokens that have high p6 cxmi and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure such as ellipsis resolution so now we use our findings from our analysis to design a benchmark for document-level translation for each of the five discourse phenomena we identified we create taggers to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse aware or mudah tagger we can then also note that different languages have different proportions of these discourse phenomena we then use the mudah tagger by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the mudah tagger has identified and finally we use our benchmark as well as other metrics to evaluate different models on the document level machine translation first of all when we use corpus level metrics so for blue we find that context agnostic models have the best performance but then if we use comet context aware models perform best and if we use word f measure then models with or without context have comparable performance this again demonstrates that it is difficult to determine the best document level translation system if we use corpus level metrics alone now we use the mudah benchmark to evaluate models and we find that context aware models are significantly more accurate the models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis pronouns and verb form so this sort of suggests where we would need to see more progress for document level translation we also compare different commercial systems and our benchmark shows that deepbell is usually more accurate than google translate for document-level translation to summarize we perform a data-driven analysis across fourteen language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation thank you so much for your attention see you in toronto</sample>
    <sample id="290">Wsl, ftw, cosine, finetuning, clean</sample>
    <sample id="291">The model is evaluated on public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering.</sample>
    <sample id="292">hi welcome to our presentation of d-plain a new corpus for german text simplification on the document level and on the sentence level my name is regina sturm and i will guide you through the first part of the presentation let's first define text simplification text simplification is a process of adapting a text to improve the text comprehension of it for a specific target group as people with reading problems or non-native speakers to train a text simplification model we require parallel pairs of text for example doc of documents or sentences in the example here you can see a parallel aligned sentence pair of a complex german sentence and its translation into plain language to simplify the sentence different techniques are possible as you can see in the example such as lexical substitution clause deletion clause deletion reordering or insertion of words we now propose our new corpus to d-plain because in the recent years there were some problems with existing corpora so for example these corpora here are too small to train a text simplification model on the other three models which are proposed in recent years are all automatically aligned which means they can be error prone in their alignments therefore we propose our new corpus d-plain which is split into two sub corpora d-plain api and d-plain web d-plain api is based on news texts in d-plain api we aligned 483 documents all manually it results in roughly 30 000 13 000 parallel sentence pairs for d-plain web this corpus includes different domains and we also aligned all of these 750 documents on the one hand manually and on the other hand with automatic alignment methods in total we result in 30 000 450 sentence pairs we analyzed our sentence pairs a little bit more so for example on the type of simplification as you can see here the bible texts are much stronger simplified than for example the news text or the language learner texts on all level regarding for example lexical simplification structural simplification or the overall level of simplification furthermore you can see that our d-plain corpus has a high variety of different simplification transformations so for example in the d-plain api corpus we have much more reordering and word editions than we have in the d-plain web corpus on the other hand in the web corpus we have much more refrasings so let's now see what we can do with this corpus hello i am omar and now i will talk about the use cases for our data set d-plain so for the first use case we can evaluate automatic alignment methods in the recent years there has been a lot of alignment methods but in the context of machine translations where we have two parallel documents written in different languages and we want to extract alignments of sentences in post documents but in our use case we are trying to extract alignments between sentences of two parallel documents having the same language having the same content but they are on a different complexity levels and now as we have our data set d-plain which have manually aligned sentences we can use these sentences as gold standard alignments to evaluate some of the proposed alignment methods and we did some adaptations to the proposed methods and we have published all these adaptations and the codes to run our experiments in the paper at the end we concluded that the best alignment automatic alignment method to use for texts for german text simplification is the method of mass align and you can also find the code to run this method on your own documents in the paper the second use case that we showed in our paper is the case of automatic text simplification by fine-tuning language models to produce simplified text from the complex input text we have fine-tuned two different models we have fine-tuned the model of long impart to produce document level simplifications and we also fine-tune the normal base long the normal base impart to produce sentence level simplifications you can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper we concluded that this basic fine-tuning could produce or could get scores better than the baseline scores and we propose those results as a benchmark a base benchmark for the problem of automatic text simplification in the future thank you so much for your attention and we hope to meet all of you during the conference thank you</sample>
    <sample id="293">Hi, and I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduced the Alt Entities Corpus. My name is Javad Hosseini, and this is a joint work with Filip Radlinski, Sylvia Paretti, and Annie Louise. Our goal is to understand users' language when they want to make a choice. Consider this alternative question: Did you mean "Easy on Me" or "I Got a Feeling"? Here, a user wants to select between one of these two songs. The most obvious thing is to use a direct reference, for example, by saying the name of the song "Easy on Me" or its position, the first one. But sometimes an indirect reference is more appropriate to have a more natural conversation. This could happen when the user cannot remember the name of the song or the pronunciations are too similar to each other and hard to disambiguate or when the user wants to specify a preference. Here are some examples of indirect references. For example, the newer one or the song that's not energetic. This is an important problem in conversational systems and also for benchmarking LLMs' entity understanding. We're not aware of a public dataset, a large-scale public dataset for this task, so we collected one using crowd annotation. Our dataset covers three different domains: music, books, and recipes. Our dataset collection methodology emphasizes informality using a cartoon completion setup. The cartoon has three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday?" And with that, Bob sets the dialogue context. In this, in the second speech bubble, Alice says, "Do you mean 'Easy on Me' or 'I Got a Feeling'?" Which is the alternative question. And in the third speech bubble, Bob uses an indirect reference to select one of these entities. For example, the newer one. We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain. The second one, which is the alternative question, is generated as follows. We always use a simple template, "Do you mean A or B?" where A and B are samples from Wikipedia. Here are the different sampling methods we've used. When we move higher in the list, the entities become more similar to each other and it's usually harder to make the disambiguation. The first one is uniform at random. The second one is when the entities have similar titles, for example, two books with the name "The Return." The third one is when they have similar descriptions on Wikipedia and finally, when they have similar infoboxes or attributes on Wikipedia, for example, the same genre or the same artist for a song. When we show this alternative question to the annotators, they know the name of these entities, but they don't necessarily know about the entities. So what we do is that we show some background knowledge about the two entities. For songs, we simply show a Google search link to each song and then ask the annotators to listen to at least some of each song and read about each song. Here's, for example, the Google search result for the song "Easy on Me." For the recipes and books domain, we show some background text from Wikipedia. For recipes, we additionally show their images again from Wikipedia so that the annotators know how they look like. Then we ask the annotators to pick one of these entities, for example, here the first one, and describe them using three to five indirect referring expressions. For example, the one with the piano music. Here are some examples from our dataset. For example, the one without words, not the one with the twelve-year-old twelve-year-old boy or the fictional one or comes from other by John, and so on. The Alt Entities Corpus has 6,000 alternative questions across three domains and it has 42,000 indirect referring expressions. Results with T5 XLarge model are summarized below. If the language model has access to the exact same background knowledge as the annotators, then the accuracy is really high. It's around 92% to 95%. But this is not realistic. If the language model has access to some partially overlapping background knowledge, then the accuracy is between 82% to 87%, which is more realistic. For example, when the language model retrieves the background knowledge. If the language model has access only to entity names, then the accuracy is only 60%. So there's a lot of room for improvement. We've also shown that the models are domain generalizable. Here is a link to our dataset. Thanks</sample>
    <sample id="294">CamemBERT is initially trained on the NAO dataset.</sample>
    <sample id="295">The speaker's name is Adam Siprowski.</sample>
    <sample id="296">The audio discusses a collaborative project between the University of Turin and Amazon Alexa, focusing on the development of natural language processing (NLP) models for irony detection. The project involves creating a corpus called EPIC (English Prospective Irony Corpus), which includes data from social media platforms like Reddit and Twitter, spanning one and a half years. The dataset consists of pairs of texts, with one following the other, collected from five varieties of English. Approximately 74 annotators were selected to label these texts, with each annotator evaluating about 200 short conversations. The annotation interface was designed to resemble a chat or text interface, where annotators determine if the reply is ironic with respect to the context. The results show that there are significant differences in inter-annotator agreement based on various dimensions such as gender, age group, and nationality. These differences are modeled using perspective-aware models, which are trained by fine-tuning pre-trained language models on different subsets of the dataset. The study also explores the causes of these differences, particularly noting that generations closer to each other exhibit more disagreement in their perception of irony.</sample>
    <sample id="297">The video discusses the concept of dog whistles, which are coded messages that convey different meanings to different groups. It uses the example of Senator Josh Holey's speech about the cosmopolitan elite agenda and experiment, which some interpret as a coded message against Jewish people. The video explains how dog whistles work by communicating both a message and a persona at the same time, with the out-group interpreting only the message and the in-group interpreting both the message and the persona. The video also highlights the importance of studying dog whistles for NLP linguistics and their role in political influence and online content moderation evasion. The project aims to develop a typology and glossary of dog whistles, conduct a case study of historical US political speeches, evaluate dog whistle recognition in language models, and study toxicity detection to show how dog whistles can evade content moderation.</sample>
    <sample id="298">The findings that led to the conclusion that temporal drift is the main cause of performance loss were from an experiment where models were retrained with more recent data. The results showed that the performance of these models degraded as the temporal gap between the training and test data increased. This confirms the hypothesis that temporal drift is the primary cause of performance loss in this context.</sample>
    <sample id="299">The video discusses the challenges in improving the robustness of NLI models using minimax training. It highlights that while NLI models have achieved state-of-the-art results across various benchmarks, they often rely on shortcuts introduced during data creation. These shortcuts are strong correlations between input attributes and labels, such as high word overlap in the MNLI dataset. Models that exploit these shortcuts perform well on in-distribution samples but struggle with out-of-distribution test sets where such correlations do not hold.

To address this issue, the video proposes a training method to reduce reliance on these shortcuts. The key insight is that NLI models suffer from poor performance on underrepresented hard training instances with patterns that could indicate the shortcuts. By focusing on these hard examples, the model can generalize better to out-of-distribution samples.

The proposed method uses a minimax training objective between a learner and an auxiliary model. The learner aims to minimize the loss of the NLI task, while the auxiliary model maximizes the learner's loss by generating example weights that encourage the learner to focus on harder instances. This approach does not assume specific types of shortcuts and relies on the learner's own training dynamics to generate example weights.

The method is evaluated on three commonly used NLI datasets (MNLI, Fever, and QQP) and their corresponding out-of-distribution test sets. Results show that the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The presentation introduces the concept of interactive dictation, a process where users can dictate and edit documents using voice commands in a natural and intuitive manner. The speaker explains that current speech-to-text systems only support dictation and do not allow for voice-activated edits. The interactive dictation task involves flexible interleaving of dictation and editing, using natural language utterances to specify edits, and executing each command and dictation in sequence until the final document state is achieved. The presenter also discusses the design of a data collection interface and the creation of a baseline system for this task.</sample>
    <sample id="301">hi everyone i'm jenny a first-year phd student at carnegie mellon university and today i'll be presenting your work and l positionality characterizing design biases of datasets and models this work was done in collaboration with some folks at the university of washington and um the allen institute for ai namely sebastian santi ronin le bras katharina reinecke and martin sap so let's start off by imagining that you're working for a newspaper and you're skimming through comments under your news article trying to remove toxic content you might turn towards a popular api like perspective api for toxicity detection and this works really well if you're karl jones um where perspective api is able to detect correctly toxic instances but that's not really the case for dithia sharma where perspective api is really not as sensitive to offensive terms that are more common in indian contexts this is an example of a design bias where we see systematic performance differences of technology between populations design biases like the one that we just saw before might occur due to the positionality of the nlp researchers and model developers positionality is simply the perspectives that people hold as a result of their demographics identity and life experiences this is a concept widely used in critical studies specifically in feminist and queer academic spaces and as a researcher positionality can influence the research process and its outcomes and results because it can change the decisions that researchers make and so one question that people might ask is do datasets and models have positionality and we're not trying to say that models themselves and datasets themselves have demographic identities and life experiences but they do aggregate judgments and opinions of real people and can thus represent certain positionabilities over others so prior work has suggested some anecdotal evidence of having positionality such as cultural gaps in models and datasets as well as theoretical definitions of model positionality however these works really don't look at comparing end users with the datasets and models themselves and studying model and dataset positionality is increasingly important as nlp tasks become more subjective and socially oriented and it's challenging to characterize how these positionabilities are skewed because not all decisions are documented and many models are hidden behind apis so to study dataset and model positionality we actually compare the annotations with real users with existing datasets and models we do this through our framework nlp positionality our framework works in two main steps the first step is to re-annotate datasets with diverse annotators and we opt to do this over looking at the demographics of original datasets annotators because usually only a few instances annotators annotate each instance and because demographics are rarely collected and shared and so we opt to re-annotate data to get many annotations for instance and to get a rich set of demographic data we then take the annotations by demographic and compare them to the models and datasets using pearson's r correlation score and thus our framework actually differs from annotator disagreement literature by comparing end users with models and datasets predictions and labels as opposed to looking at just annotator agreement or modeling annotator distributions our framework is largely enabled through lab in the wild an online crowdsourcing platform former hci collaborator in lab in the wild is an online experimentation platform where we can recruit diverse volunteers compared to the platforms like mturk which largely have participants from the us or india and further lab in the wild still is able to get high quality data we host two tasks on lab in the wild one of them being social acceptability and the way this works is that participants will read a situation from the social chemistry dataset and then they'll write how socially acceptable a situation is afterwards to stay engaged in the study they can compare their responses to an ai and others we've then compared these annotations with social chemistry delphi and gpd four we then replicate a very similar setup for the toxicity and hate speech detection task where they'll read an instance from dinahate and write whether they think it's an instance of hate speech we then compared these annotations with dinahate perspective api rewriter api hate roberta and gpd four our study in the end amassed over 16,000 annotations from over a thousand annotators from 87 countries so now we're better equipped to answer who do nlp datasets and models align with the most we find that there is positionality in nlp for example we find that datasets and models are most aligned to english speaking countries so for the gpd four social acceptability analysis we find that it's most aligned to confucian and english speaking countries we find that dinahate is also most aligned to english speaking countries we also find most additional alignment with people who have a college education so for gpd four in the social acceptability task we find that it's most aligned to people with a college education or graduate school education and we find the same for dinahate where it's most aligned to people with a college education however when models and datasets are aligned to specific populations some are inevitably left behind an example of this is that datasets and models are less aligned to non-binary people compared to the men and woman counterparts we find this in the gpd four social acceptability task as well as the dinahate task analysis as well so given that there is positionality and alignment in nlp what can we do about it so we have a few recommendations for this first one is keep a record of all relevant design choices throughout the research process and the other is to do nlp research with the lens of perspectivism our third recommendation is to build specialized datasets and models within four specific communities and a good example of this is the musakani initiative i mean we want to emphasize that inclusive nlp isn't just making you know all technologies work for everyone and so that concludes our presentation but if you'd like to learn more feel free to check out our dashboard for the most updated analysis results and our paper thank you</sample>
    <sample id="302">It is necessary to permute the tokens for the output sequence because the alignment between input and output is not given in the training data, and sometimes there are multiple permutations that are consistent with the data but the linguistically correct one is latent.</sample>
    <sample id="303">The authors recommended that model owners should increase transparency about bias mitigation methods because it is unclear whether the positive stereotypes and essentializing narratives observed in the models are due to over-expressive value alignment or anti-stereotyping methods, and more transparency would allow for further study of these phenomena.</sample>
    <sample id="304">Minimal-pair unacceptable inputs are sentences that are grammatically incorrect or contain stereotypes.</sample>
    <sample id="305">The audio content discusses the challenges and requirements of weakly supervised learning (WSL). It highlights that WSL methods rely on clean validation samples to function effectively, with a significant performance drop observed when no clean samples are available. The audio also mentions that increasing the number of clean validation samples can improve WSL performance, but direct fine-tuning on clean data often yields better results. Additionally, it suggests that continuous fine-tuning on clean validation samples can achieve similar performance gains as more complex WSL methods, making it a practical alternative. The audio concludes by recommending that future work should report model selection criteria, compare WSL approaches with fine-tuning baselines, and consider continuous fine-tuning as a simple yet effective method in WSL.</sample>
    <sample id="306">The audio content is a presentation by Sebastian Schuster and Namjung Kim on their work regarding entity tracking in language models. The presentation begins with an overview of the importance of entity tracking for agents to understand discourse, using the example of a recipe where entities like eggs, sugar, and flour are tracked through various state changes. The researchers argue that there has been a lack of systematic investigations into how pre-trained language models can perform entity tracking tasks. They then introduce their research question: "To what extent can large language models track entities?" The presentation highlights several challenges in designing an evaluation task, such as ensuring that the model's predictions are not influenced by common patterns in the pre-training data, avoiding the memorization of entity state sequences through fine-tuning or demonstrations, and preventing the model from relying on simple heuristics. To address these challenges, they designed a task involving boxes and objects, where the input includes an initial description and multiple state-changing operations. The task requires the language model to predict the contents of each box based on the initial description and the operations performed. The presentation concludes with an analysis of the results from testing different models, including FLAN-T5 and GPT-3.5, showing that only GPT-3.5 exhibits non-trivial entity tracking behavior, suggesting that pre-training on code is responsible for this capability.</sample>
    <sample id="307">The authors used public and private downstream tasks such as named entity recognition, classification, part-of-speech tagging, and question answering to evaluate their models.</sample>
    <sample id="308">The presentation, titled "NL Positionality: Characterizing Design Biases of Datasets and Models," is presented by Jenny T. Liang, a first-year PhD student at Carnegie Mellon University. The work was done in collaboration with researchers from the University of Washington and the Allen Institute for AI, including Sebastin Santi, Ronan Le Bras, Katharina Reinecke, and Martin Sap. The presentation begins by highlighting the issue of design bias in AI models, using the example of Perspective API's varying effectiveness in detecting toxic content across different cultures. It explains that this bias can stem from the positionality of NL researchers and model developers, who bring their own perspectives shaped by demographics, identity, and life experiences. The presentation then introduces the concept of dataset and model positionality, which refers to how these entities can represent certain perspectives over others due to the judgments and opinions of real people involved in their creation. To study this, the researchers developed a framework called NL Positionality, which involves re-annotating datasets with diverse annotators and comparing these annotations to existing datasets and models using Pearson's correlation score. This framework helps in understanding how NL datasets and models align with different populations and educational backgrounds, revealing significant positionality issues. The presentation concludes by discussing recommendations for addressing these biases, such as keeping records of design choices, conducting NL research through the lens of perspectivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">ABC eval behavior labels</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">The authors of the paper are affiliated with the University of TÃ¼bingen.</sample>
    <sample id="312">MultiInstruct is the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories, derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions.</sample>
    <sample id="313">3</sample>
    <sample id="314">Binary coordination is the coordination of two elements.</sample>
    <sample id="315">The prompts used in this study were relatively short, with an average length of 10 words.</sample>
    <sample id="316">The findings suggest that smaller models can support larger models when properly trained on suitable datasets, as demonstrated by the T5 model's performance in generating high-quality scripts.</sample>
    <sample id="317">The English content is a presentation on the topic of information extraction in natural language processing. It discusses the challenges of traditional models that operate in a test-to-test manner during pre-training, leading to mismatched outputs and requiring large amounts of structured training data and special decoding strategies. The proposed solution is CodeIE, which transforms unstructured text into structured information using code generation models like CodeT5. The presentation evaluates the performance of different models on named entity recognition and relation extraction tasks, showing that the proposed approach significantly outperforms traditional baseline models. Additionally, it analyzes the performance of different prompts and models, highlighting the benefits of using code format prompts for generating correct structures and improving recall.</sample>
    <sample id="319">The work investigates three different learning strategies: from scratch pretraining, continual pretraining using the weight and tokenizer of Camembert, and continual pretraining using the English biomedical model BERT.</sample>
    <sample id="320">The factor of overfitting due to test reuse is not observed.</sample>
    <sample id="321">The quality of the simplification was evaluated by analyzing the type of simplification, such as lexical, structural, and overall simplification.</sample>
    <sample id="322">The speaker, Enrico Liscic, is presenting at ACL 23 and will be discussing the question of what a text classifier learns about morality. He begins by explaining that human morality is our internal compass that helps us determine whether an action or concept is morally right or wrong. He then explains that morality is subjective and can vary from person to person, with some people labeling certain concepts as immoral while others label them as moral. The speaker also mentions that social theories, such as the moral foundation theory, can be applied to the understanding of human morality. He then goes on to explain that language models have been used to understand and classify morality in text, but that this approach often treats morality as a singular scale between immoral and moral, which can hide the true complexity of morality. Finally, the speaker discusses his own research on the topic, which involves using explainable AI techniques to understand how morality is expressed differently across different domains.</sample>
    <sample id="323">The English content describes a research paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering." The paper introduces a method to retrieve relevant knowledge from external sources using language models and knowledge bases. It proposes a framework called HKG, which builds on a mutable knowledge base optimized by a two-stage training strategy and KRL. The framework uses a language model to encode and fuse the knowledge base, removing subwords and entities that are not relevant to the question context. The paper also discusses the use of graph embedding techniques to update entity and relation embeddings in HKG, inspired by R-GAT. Experiments conducted on the Concept QA and OpenBook QA datasets show that the proposed method achieves good results compared to other methods like LM and HKG.</sample>
    <sample id="324">Yes, language models have varying political leanings.</sample>
    <sample id="326">Cognitive dissonance is two beliefs or actions that are inconsistent.</sample>
    <sample id="327">The presentation introduces a new model architecture called Manager Tower, which aims to improve vision-language representation learning by aggregating insights from unimodal experts at different levels. It uses managers in each cross-modal layer to adaptively combine the insights of pre-trained unimodal experts, allowing for more effective exploitation of different levels of unimodal semantic knowledge. The Manager Tower is built upon Bridge Tower and uses managers with different aggregation weights to facilitate comprehensive cross-modal alignment and fusion. With only 4 million images for pre-training, Manager Tower achieves superior performance on various downstream tasks, particularly improving accuracy by 39.15% on the Visual Genome test set compared to Bridge Tower. The presentation also discusses the visualization of average aggregation weights of textual and visual managers, showing distinct trends and providing strong evidence that adaptive managers can adaptively exploit different levels of unimodal semantic knowledge.</sample>
    <sample id="328">GPT-4</sample>
    <sample id="329">The paper presents a method for generating structured pseudo labels to improve zero-shot video sentence localization. The proposed method uses pre-trained image caption models to generate complex free-form pseudo queries based on video frames, and then employs a pre-trained model to match the relevance between video frames and pseudo queries to generate pseudo events. These pseudo events are ranked by their quality, and only the top K with high quality and low overlap are selected. The method estimates label noise based on the model's predicted confidence and Intersection over Union (IoU) between predictions and pseudo labels, reducing the influence of noisy samples during training. The approach outperforms existing methods on two datasets, demonstrating its robustness against label noise.</sample>
    <sample id="330">Yes, cumulative training performs equal or better than iterative across the board.</sample>
    <sample id="331">The speaker's name is Sara Papi.</sample>
    <sample id="332">The data for the MuDa benchmark was taken from TED Talks that have been translated into 14 different languages.</sample>
    <sample id="333">The audio content is a presentation by Wen Hao Zhu from Nanjing University, introducing their work on injecting knowledge into nearest neighbor machine translation (NMT). The presenter acknowledges collaborators from Shanghai AI Lab, Nanjing University, and the University of Hong Kong. The focus is on NMT, aiming to learn a generalized representation space for diverse scenarios. However, neural networks often induce a non-smooth representation space, limiting generalization ability. The presentation explains that low-frequency tokens disperse sparsely in the representation space, forming holes where semantic meaning can be poorly defined, leading to poor model performance. To address this, a solution called Ink is proposed, which smooths predictions based on nearest neighbors in the representation space. This involves building a key-value data store to save representations and their corresponding target tokens. At each decoding step, the NMT model retrieves nearest entries from the data store and refines prediction probabilities accordingly. Despite its effectiveness, this approach has drawbacks such as time-consuming retrieval of neighbors from a large data store and difficulty updating representations once the data store is constructed. To overcome these issues, the Ink framework is introduced, which injects knowledge into NMT through a training loop involving two steps: extracting knowledge from the data store to guide an adapter to adjust the representation, and using updated representations to refresh the data store asynchronously. The training loop runs until convergence, optimizing the adapter with a combined learning objective. The final model drops the data store, achieving better performance compared to the state-of-the-art NMT system.</sample>
    <sample id="334">hi my name is adam sripovsky and this talk is about the dependency structure of coordination as you may know that different dependency structures assumed by different theories and corpus approaches so for example in the universal dependencies the structure of the coordinate coordination lisa bart and maggie is such that the first conjunct is the head of the whole coordinate structure so in this case lisa a similar approach is assumed in igor miltruk's meaning text theory where again the whole coordinate structure is headed by the first conjunct so these two approaches are asymmetric right they they single out one of the conjuncts now there also symmetric approaches to coordinate coordinate structures such as the prague approach the conjunction headed approach assumed in prague dependency trees banks where coordinate structures are headed by the conjunction so we get dependencies from end to all the conjuncts and finally this also a multi headed approach that's used for example in the de katsen's word grammar where so to say all conjuncts are heads of the coordinate structures so we get dependencies from the governor here loves to all conduct separately these are bart and making now the main paper is to produce an novel argument for the symmetric structures of coordination like these two and against the asymmetric structures of coordination like these two okay the argument is based on the principle of dependency length minimization that are explained on the basis of these examples so in english as you might as you might know a direct objects prefer to be close to the verb while adjuncts may be further away right so march read it yesterday is fine because the direct object it is closed to the verb while march read yesterday it is much worse right because here between the verb and the direct object there is an adjunct yesterday however this effect may be ameliorated when when the direct object is very heavy and very long because then it can be moved to the position after the adjunct this is illustrated here so both these sentences are fine march read this absolutely fascinating book about the bees yesterday i is okay in the way instead of it we have this long np but it's also okay to say march read yesterday this absolutely fascinating book about bees so the reason here is that this is possible because even though this sentence violates the general grammatical principle that direct objects should be next to the verb it satisfies the principle of dependency length minimization which says that shorter shorter dependencies are preferred so these two trees only show the length of the crucial dependencies so the ones that are not constant among these two structures so here we have a dependency from red to the adjunct of length seven measured in words and from red to book of length four so together it's eleven when you move when you swap these two constituents the sum of these two dependencies becomes six right so instead of 11 six much shorter that's why this sounds quite okay right it violates one principle but it satisfies another one okay so what we did we extracted various statistics from about coordination from the enhanced version of pent of the pentry bank and see the paper why wouldn't use university dependencies and these statistics confirm the observation made many times before that left conjuncts tend to be shorter so salt and pepper and not pepper and salt measured in syllables and also the observation that was made in passing that this tendency grows with lengths the length difference so when the difference between the lengths of the two conjuncts grows the shorter conjunct prefers to be the first one stronger right so the proportion is is bigger of of the left short conjuncts but what's novel in in this paper is we that we observed that this tendency only occurs when the governor is on the left absent right so the governor is on the left in this example i saw bart and lisa so it's the governor is on the left it's absent in the second example homer came and sneezed here we have coordination of two verbs and there's no outside external governor right so in such cases the left conjunct prefers to be shorter the more so the bigger the difference between the two conjuncts however when the governor is on the right as here left governs the coordination tender net this effect disappears so we show that by measuring length in characters that's the first column in syllables the middle column and in words the right column so i'll concentrate on the right one what we see here is that when the governor is on the left the tendency for the left conjunct to be shorter grows steadily with the absolute difference in words and the same is observed when there is no governor as in coordination of sentences but when the governor is on the right this tendency disappears and we show in the paper how this provides an argument against as asymmetric structures of coordination as these two and for the symmetric structures as these two so see the paper for the full agreement and arguments sorry and talk to us about the poster session thank you</sample>
    <sample id="335">Matthias Lindemann</sample>
    <sample id="336">Cross-lingual transfer is the process of training a model on one language and then using it to predict the output in another language.</sample>
    <sample id="337">The English content is a presentation on a research paper titled "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning." The paper introduces a novel approach to handle out-of-vocabulary (OOV) words in natural language processing. The approach leverages word formation and association to infer the meaning of OOV words by constructing a word relationship graph that captures lexical rules of word formation and association. Each word or word piece is represented as a node in the graph, with its corresponding word embedding serving as the node feature. The model uses a two-level graph attention network to process the graph and extract important information while mitigating noise from nodes with numerous neighbors. A readout block is employed to summarize the whole graph information and capture word formation. The model is trained using contrastive learning and achieves state-of-the-art performance on both intrinsic and extrinsic tasks, demonstrating the effectiveness of learning OOV words by word formation. The model also shows some improvements over static and contextual models in downstream tasks. The applicability of the model to other languages depends on the rationality of word decomposition.</sample>
    <sample id="338">The speaker, Bing Chen, is presenting a collaborative research work on the evaluation of human natural language explanations. The study involves researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. They discuss the motivation behind their work, related works, and contributions divided into three sections: unified structure, preliminary experiments, and evaluation of five datasets using two models.

The researchers address the challenge of evaluating the quality of human-annotated explanations, which can be subjective and task-dependent. They propose a new evaluation metric called "true," which extends the Simulatability score to consider the helpfulness of explanations during fine-tuning. The study evaluates the utility of explanations across five datasets using unified structure and compares the results with the Simulatability score.

The findings suggest that human-annotated explanations can still benefit model predictions, even if they are considered low-quality by humans. The proposed metric outperforms the Simulatability score in evaluating dataset qualities and task-specific characteristics. The study lays the foundation for high-quality human-aided collaboration annotation jobs and recommends similar quality checks in future research.</sample>
    <sample id="339">Saarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="340">Kuan-Hao Huang from UCLA is presenting a work called ParaAMR, which is about creating a large-scale syntactically diverse paraphrase dataset using AMR back translation. This is a collaborative project with Varun Iyer, Anoop Kumar, Kai-Wei Chang, and Aram Aramyan. The goal of this work is to generate a large-scale dataset that can help improve NLP applications such as question answering, chatbots, and robustness.

To create this dataset, the researchers use AMR (Abstract Meaning Representation) graphs, which are directed graphs that capture the abstract meaning of a sentence. Each node in the graph represents a semantic concept, and each edge represents a semantic relation between concepts. The focus node represents the main assertion of the sentence.

The process involves using a pre-trained AMR parser to get an AMR graph of a source sentence, changing the focus of the graph by randomly sampling a node and setting it as a new root node, modifying the corresponding edges, and then using an AMR graph-to-text generator to generate text from the modified graphs. This results in generated texts that share the same AMR graph structure, so they will have similar semantics. However, because the generator emphasizes the focus at the beginning of the sentence, their syntax will be different.

The proposed dataset, ParaAMR, contains around 15 million source sentences and approximately 6.9 paraphrases per source sentence. The researchers also present some quantitative analysis for ParaAMR, including automatic scores and human evaluation scores. These scores indicate that ParaAMR has similar semantic similarity scores to other datasets that use back translation but has higher syntactic diversity scores.

The researchers demonstrate that ParaAMR can benefit several NLP applications, including learning sentence embeddings, synthetic control paraphrase generation, and data augmentation for future learning. They conclude that ParaAMR is a large-scale syntactically diverse paraphrase dataset that benefits several NLP applications compared to existing paraphrase datasets.</sample>
    <sample id="341">The authors use the average latency measure and the computational-aware average latency measure.</sample>
    <sample id="342">The presentation introduces a large-scale personalized dialogue dataset constructed from live streaming, focusing on the challenges and methods used to create such a dataset. The presenter, Gao Jing Shen, explains that the dataset is crucial for developing applications like virtual streamers and virtual employees, addressing issues such as the lack of diverse persona information and the need for large-scale multi-party dialogues. The dataset is derived from Chinese TikTok videos, with audio extraction, transcription, and dialogue structuring using a reply-to-match method. Personal information is collected through manual labeling and a trained persona classifier. The dataset is compared to existing open-domain dialogue datasets, highlighting its video source and detailed persona annotations. Experiments show that the dataset's unique characteristics, such as larger scale and detailed persona annotations, improve performance in response modeling and dialogue recognition tasks. The presenter concludes by emphasizing the potential of the dataset for enhancing speaker personalization and the need for further research on efficient transfer learning for live chat applications.</sample>
    <sample id="344">Some other kinds of structural generalization remain very challenging though.</sample>
    <sample id="345">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The permutation model is flexible and expressive, and does not put any hard constraints on the possible permutations. The experimental results show that the model outperforms other treeless models on generalization to deeper recursion by a large margin. However, some other kinds of structural generalization remain very challenging. The alignment between input and output is not given in the training data, which poses a challenge for training. Sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. This is addressed by inducing the alignment as part of the training. The permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. This is approximated with a GPU-friendly continuous relaxation that also allows us to back propagate through the solution and learn the linguistically more plausible permutations.</sample>
    <sample id="346">The authors of the paper are affiliated with the School of Interactive Computing at Georgia Institute of Technology.</sample>
    <sample id="348">In this video, Myra Cheng introduces her research on "Marked Personas," a study that explores the use of natural language prompts to measure stereotypes in language models. This collaborative work, co-authored with Esin Durmus and Dan Jurafsky, delves into how language models perceive and reflect societal biases through their responses. The study aims to shed light on the ways in which these models can perpetuate or challenge existing stereotypes, highlighting the importance of understanding and addressing these biases in AI development.</sample>
    <sample id="350">In this presentation, we discuss the meaning of superhuman performance in NLP and why such claims are not yet grounded. We analyze two popular benchmarks, SuperGLUE and Squad, and discover several sources of error that make comparisons between humans and systems unfair. For example, systems are often evaluated on a full test set while humans are evaluated on a small subset, and research often estimates human performance using simple aggregation methods. Additionally, pay rates vary considerably across tasks, and details about the annotator pool are often omitted. We argue that these issues should be addressed to construct more reliable benchmarks and avoid repeating the same mistakes.</sample>
    <sample id="351">The presentation discusses the effectiveness of CoNLL-2003 named entity taggers in 2023. It highlights that these models have been used for nearly two decades, raising questions about their ability to generalize to modern data and the factors influencing their performance. The study involved developing a new dataset (CoNLL++), fine-tuning over 20 models on CoNLL-2003, and evaluating them on both CoNLL-2003 and CoNLL++. The results indicate that three main ingredients are crucial for good generalization: model architecture, model size, and the number of fine-tuning examples. The study also explores the causes of performance drops, ruling out adaptive overfitting and confirming temporal drift as the primary cause. The conclusion is that while CoNLL-2003 taggers still work well, there is a need for further research to improve their generalization capabilities.</sample>
    <sample id="352">ABC-Eval stands for Annotating Behaviors in Chat.</sample>
    <sample id="353">The presentation introduces a research paper titled "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, Andre F. T. Martin, and Irina Gurevich. The paper addresses the challenge of input under-specification in code generation and program synthesis given natural language descriptions. It proposes an interactive approach to clarify specifications through asking clarification questions. The authors introduce a method to create a synthetic dataset with clarifications on key operations (CQA) and propose a pipeline for code generation by asking clarification questions. They also discuss the challenges of identifying missing or aligned key operations and the results of their method, including the performance of different models and the challenges in evaluating the results. The presentation concludes with an analysis of the impact of clarified key operations on code generation and an invitation for feedback on the paper and code.</sample>
    <sample id="354">2018</sample>
    <sample id="356">The authors of the paper are affiliated with the University of Amsterdam, Saarland University, and the University of Groningen.</sample>
    <sample id="357">The speaker's name is Si Yu Yuan.</sample>
    <sample id="358">There are five authors involved in the paper.</sample>
    <sample id="359">The approach is compared to the state-of-the-art architecture specifically tailored for simultaneous speech translation.</sample>
    <sample id="360">Hello everyone. My name is Ying, and my colleague Zhiyang and I will be presenting our research on MultiInstruct: Improving multi-modal zero-shot learning via instruction tuning. So with the advances in large language models, many works started to explore new learning paradigms of reusing pre-trained language models for different downstream tasks in a parameter- and data-efficient way. Recently, many studies have shown that instruction tuning enables large language models to perform on unseen tasks in a zero-shot manner by following natural instructions. However, most previous works on instruction tuning focus on improving the zero-shot performance on language-only tasks, while computer vision and multi-modal tasks have been left out. Therefore, in this work, we want to investigate whether instruction tuning on multi-modal pre-trained models can actually improve generalization to unseen multi-modal tasks. Additionally, at the time of our research, we discovered a considerable discrepancy in availability of instruction dataset between NLG and multi-modal. There exist more than 1,600 language-only instruction tasks, however, there is no large-scale publicly available multi-modal instruction task. Therefore, this motivates us to build a multi-modal instruction tuning dataset. Here we present MultiInstruct, the first multi-modal instruction tuning benchmark dataset that consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions. For investigating multi-modal instruction tuning on our proposed dataset, we take OFA, a unified multi-modal pre-trained model as our base model. OFA uses a unified vocabulary for language, image tokens and the coordinate of a bounding box. Here we show some example instances from our MultiInstruct dataset. To unify the processing of various input and output data types, we follow the method from OFA and formulate all the tasks in a unified sequence-to-sequence format, in which the input texts, images, instruction and bounding boxes are represented in the same token space. Okay now I'm going to talk about multi-modal instruction tuning. So for the training dataset, we use 53 tasks from NLG group for training and we sample 10,000 instances per task. For testing, we reserve the entire commonsense reasoning group for testing and we select additional five tasks from Wiki and the miscellaneous group. We use all the instances in the test split for each task. In addition, we randomly sample 20 tasks from the test split of Natural Instruction as unseen tasks for NLG. So we use a pre-trained OFA large model as a base model. During training, we mix all the instances for all the tasks. Each instance is randomly combined with one of its five instruction templates. So during test for each task, we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment. We report the min and max performance and the standard deviation of the performance across all five experiments. If the task is a multi-modal classification task, we report accuracy. If it's a multi-modal generation task, we report ROUGE-L. For NLG task, we report ROUGE-L as well. We also introduced an additional evaluation metric called sensitivity. This measures the model's ability to consistently produce the same outputs for the same task regardless of slight variation in the wording of the instruction. Here is our main results. As we can see, instruction tuning can significantly improve OFA's performance on unseen multi-modal tasks. Also transfer learning from Natural Instruction dataset can benefit instruction tuning. Here we can see as the amount of task increases, the model achieve better performance and in the meantime lower sensitivity. So we also do one experiment. We use one instruction versus five instruction. As we can see, using more instruction can improve the model's overall performance and reduce its sensitivity a lot. So this shows the effect of different fine-tuning strategy on the model's sensitivity. As we can see, by transfer learning from Natural Instruction dataset, the model can achieve much better sensitivity compared to the original OFA model. We also can see transfer learning from Natural Instruction dataset can help OFA to achieve much better performance on the Natural Instruction dataset. So overall, we propose the first large-scale multi-modal instruction tuning dataset, which significantly improves the zero-shot capability of OFA and we explore different transfer learning technique and show their benefits. We design a new metric called sensitivity. So one more thing, we are collecting a much larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and we will release them soon. This is the QR code for our data and model. Thank you.</sample>
    <sample id="361">Hi, my name is Armineh Nourbakhsh. I'm a PhD student at the Language Technologies Institute at Carnegie Mellon University. I'm also a research director at the JP Morgan AI Research Team. The work I'm presenting today is titled CounterComp and it's focused on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. By multi-step quantitative reasoning, we're specifically focused on the question answering task. So if you're given a financial table, such as the one displayed on the right-hand side of this slide, you'd be able to ask different questions about the data in this table. For instance, you could ask what was the net change in revenue from 2019 to 2020, and the answer would be a certain number that's derived from executing one or more arithmetic operations. And this is what we mean by multi-step quantitative reasoning when the output includes multiple arithmetic operations. Unfortunately, state-of-the-art neural models don't perform very well on these tasks, especially when the output has more than two steps. And the reason is because these models are memorizing spurious patterns. So going back to the table, you might notice that there are certain tokens that are repeated in the various questions in the input. For instance, the token 2019. If the model sees this token repeatedly during training, it might start to mistakenly associate it with a very common operation in the output, such as the subtraction operation in this case. And we want the model to avoid that as much as possible. So ideally, we'd want a model that attends to appropriate tokens in the input when it's generating certain operations in the output. And we could certainly add supervision signal that encourages the model to do so. But of course adding additional supervision is always costly in terms of human effort, and we'd like to avoid that as much as possible. So we look at this challenge in a different light. We go back to the table that I showed you in the first slide and look at the questions again. If you pay attention into the examples provided here, you'll notice that the components of the question that actually matter in terms of the operations and the operators that are used in the output are somewhat interchangeable in the sense that if you change those components, the output might also slightly change. As an example, in the first question here, if you change net change or switch net change to percent change, effectively what you do is you add a division and multiplication to the output. This basically means that these components can be used to mine counterfactual scenarios from the input. And that is exactly what we do. Given a training sample, we treat it as an anchor. And we mine what we call positive and negative examples from the training set. A positive example would be an example where an intervention in the question would not yield any change in the output. And a negative example would be an example where the intervention in the question would yield a change in the output. And we use these triplets to add an auxiliary metric learning loss to the training procedure. And that auxiliary metric learning loss has a dynamic margin that is actually measuring the extent of change or intervention in the questions between each pair, and using that to adjust the metric learning loss accordingly. We show that adding this auxiliary loss to three state-of-the-art baselines consistently improves their performance, especially when the number of reasoning steps grows beyond two. This is performance on in-distribution samples, meaning the model is trained on a dataset and tested on the same dataset. But more importantly, we also show that adding an auxiliary metric learning loss improves performance on out-of-distribution samples, either in cases where the model is trained on one dataset and tested on other datasets, or the model is trained on one dataset and tested on examples from the same dataset that were never seen during training, which is basically exactly what compositional generalization aims to obtain. We also show qualitatively that adding the countercomp loss helps the model attend to more meaningful tokens during training, meaningful in the sense that they relate to more meaningful operational terms in the output. Here's the base main references I used in this presentation. For more information, make sure to check out our poster. Or if you have any questions, feel free to reach out to the contact listed here. I'd like to thank my co-authors, my advisor at CMU, and my co-advisor at JP Morgan, and I would like to thank you all.</sample>
  </task>
</testset>