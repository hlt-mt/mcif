<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su largi quantità di dati web, tra cui notizie politiche.</sample>
    <sample id="1">Maggell University, Meila e Microsoft Research.</sample>
    <sample id="2">The paper presents a new pre-training model called LeTMask, which aims to address the reading order issues in document understanding. LeTMask uses text and layout information as input and infers global reading orders by jointly using 1D and 2D positional information. The model employs two novel masking strategies: whole word masking and layout aware masking. Experiments show that LeTMask outperforms existing models on both FD and SRIE datasets, with similar F1 scores between local and global positional encodings.</sample>
    <sample id="3">Ciao, benvenuti nella nostra presentazione di DePlain, un nuovo corpus per la semplificazione del testo in tedesco al livello dei documenti e al livello delle frasi. Il mio nome è Regina Strohn e mi occuperò della prima parte della presentazione. Iniziamo con la semplificazione del testo. La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con problemi di lettura o non native speakers. Per addestrare un modello di semplificazione del testo, è necessario un insieme di coppie parallele di testi, ad esempio due documenti o due frasi. Nell'esempio qui, si possono vedere coppie allineate di frasi di una frase complessa in tedesco e la sua traduzione in un linguaggio più semplice. Per semplificare la frase, diverse tecniche sono possibili, come si può vedere nell'esempio, tra cui la sostituzione lessicale, la clausolazione, la clausolazione, la riformulazione o l'inserzione di parole. Ora proponiamo il nostro nuovo corpus DePlain. In recenti anni, ci sono stati problemi con i corpi esistenti. Ad esempio, questi corpi qui sono troppo piccoli per addestrare un modello di semplificazione del testo. Le altre tre proposte recenti sono tutti automaticamente allineati, che significa che possono essere errati nel loro allineamento. Pertanto, proponiamo il nuovo corpus DePlain, che è diviso in due sottocorpi: DePlain API e DePlain Web. DePlain API è basato su testi nuovi. In DePlain API, 483 documenti sono allineati manualmente, il che produce circa 30.000-13.000 coppie di frasi allineate. Per DePlain Web, questo corpus include diverse domande e allinea tutti questi 750 documenti, sia manualmente che con metodi di allineamento automatico. In totale, si ottiene 30.450 coppie di frasi. Analizziamo queste coppie di frasi un po' di più. Ad esempio, sul tipo di semplificazione, si può vedere che le biblio tesi sono molto più semplificate rispetto ad esempio i testi nuovi o i testi per apprendere un nuovo linguaggio. Al livello di tutto, considerando ad esempio la semplificazione lessicale, la semplificazione strutturale o il livello globale di semplificazione. Inoltre, si può vedere che il corpus DePlain ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, in DePlain API, ci sono molti più riformulazioni e modifiche di parole rispetto a DePlain Web. D'altra parte, in DePlain Web, ci sono molte più riformulazioni. Allora, ora vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parlerò dei casi d'uso per il nostro dataset DePlain. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici. In recenti anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni automatiche, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre allineamenti di frasi in posti documenti. Ma in questo caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, avendo lo stesso linguaggio, lo stesso contenuto, ma a livelli di complessità diversi. E ora che abbiamo il corpus DePlain, che ha allineamenti manuali, possiamo usare queste frasi come standard di riferimento per valutare i metodi di allineamento proposti. Abbiamo fatto delle modifiche ai metodi proposti e pubblicato tutti questi aggiustamenti e i codici per eseguire i nostri esperimenti in un articolo. Alla fine, concludevamo che il miglior metodo di allineamento automatico da usare per testi per la semplificazione del testo in tedesco è il metodo di mass alline. E tu puoi trovare il codice per eseguire questo metodo su i propri documenti in un articolo. Il secondo caso d'uso che ho illustrato in un articolo è il caso di semplificazione automatica, cioè di addestrare modelli linguistici per produrre testi semplificati da testi complessi. Abbiamo addestrato due modelli, abbiamo addestrato il modello di Long Impart per produrre semplificazioni di documenti e anche addestrato il modello di Long Impart per produrre semplificazioni di frasi. Puoi anche trovare i punti di controllo e vedere i dettagli dei punteggii e dei metri di valutazione dei nostri esperimenti in un articolo. Concluso che questa basic fine-tuning potrebbe produrre o ottenere punteggii migliori dei punteggii di base e propone i risultati come una marcatura di benchmark per il problema di semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e spero di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">La relatrice o il relatore è Kai O Yin.</sample>
    <sample id="5">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il T5-XL大型模型.</sample>
    <sample id="6">The paper presents a joint work on multilingual and cross-lingual summarization, conducted by Jan and colleagues. The authors unify previous multilingual and cross-lingual summarization into a more general setting called many-to-many summarization. This approach aims to build one single summarization model that can precisely summarize a document in any source language and generate a summary in any target language. The authors conduct preliminary studies to provide deeper analysis among multilingual summarization, cross-lingual summarization, and their many-to-many summarization. They find that many-to-many summarization could help the summarization model better transfer task knowledge across different languages than previous multilingual summarization and cross-lingual summarization. Additionally, they propose a pre-training method for many-to-many summarization models that learns language modeling, cross-lingual ability, and summarization ability through a three-stage pre-training process. The results of the preliminary experiments show that the multilingual model trained in the many-to-many summarization setting can better transfer task knowledge across different languages in the settings of multilingual summarization, cross-lingual summarization, and unified cross-lingual summarization.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="8">Il nuovo metodo di valutazione umana proposto è quello di chiedere ai giudici umani di valutare più dimensioni della qualità del dialogo, invece di chiedere loro di scegliere tra due conversazioni o di dare una valutazione su una scala.</sample>
    <sample id="9">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sul fatto che i modelli possono essere adattati direttamente all'uso senza l'aiuto di un set di validazione pulito.</sample>
    <sample id="10">Per migliorare il punteggio, i modelli possono essere domen generalizzabili.</sample>
    <sample id="11">The New Yorker Caption Contest is a popular weekly cartoon captioning competition that has been running for over a decade. Researchers have operationalized the contest data into three tasks: matching, quality ranking, and explanation generation. The best model, CLIP fine-tuned on the annotated corpus, achieves around 62% accuracy in matching, while humans achieve around 94%. In quality ranking, there is still a significant performance gap between models and humans. GPT-4's joke explanations often contain errors, and human evaluations prefer human-generated explanations over those generated by GPT-4.</sample>
    <sample id="12">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="13">Daniel Rotem, un studente di dottorato presso l'Hebrew University of Jerusalem, ha presentato il suo lavoro su "Finding the Sweet Spot: Analysis and Improvement of Adaptive Inference in Low-Resource Settings". Il suo studio ha confrontato due metodi di inferenza adattativa: il multi-modello e l'early exit. I metodi multi-modello sono più versatili e possono essere estesi facilmente, ma sono costosi per lo storage e soffrono di sovrapposizione. I metodi early exit sono più veloci e efficienti in termi di memoria, ma i parametri del modello condivisi tra le diverse classificatori possono portare a una performance inferiore. Rotem ha proposto un nuovo metodo di ottimizzazione dei pesi per i modelli early exit, chiamato Sweet, che elimina i problemi di sovrapposizione dei gradienti. I risultati hanno dimostrato che Sweet chiude gran parte della distanza tra i metodi multi-modello e early exit, specialmente per le classificatori più tempestive.</sample>
    <sample id="14">Il mio nome è Adam Sipruchowski e questo talk parla della struttura di dipendenze della coordinazione. Come potete vedere, ci sono diverse strutture di dipendenze assunti da diverse teorie e approcci corposi. Ad esempio, nella teoria delle dipendenze universali, la struttura di coordinazione tra Lisa, Bart e Maggie è tale che il primo congiunto è la testa della struttura di coordinazione, cioè Lisa. Un approccio simile è assunto nella teoria dei testi di Igor Miltruk, dove di nuovo la struttura di coordinazione è guidata dal primo congiunto. Questi due approcci sono asimmetrici, singolano un dei congiunti. Ci sono anche approcci simmetrici alla struttura di coordinazione, come l'approccio di Prag, che considera la coordinazione guidata dalla congiunzione. In questo caso, le strutture di coordinazione sono guidate dalla congiunzione, ottenendo dipendenze da un verso all'altro dei congiunti. Infine, c'è un approccio multi-guidato che viene usato ad esempio in De Catts's Word Grammar, dove tutti i congiunti sono guidati dalla struttura di coordinazione, ottenendo dipendenze dal governante, in questo caso Lisa, a tutti i congiunti separatamente. Ora, il mio articolo ha lo scopo di fornire un nuovo argomento per le strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. L'argomento è basato sul principio di minimizzazione delle dipendenze, che spiega sulla base di questi esempi. In inglese, come potete vedere, gli oggetti diretti preferiscono di essere vicini al verbo, mentre gli aggiunti possono essere più lontani. Quindi, March read it yesterday è corretto perché l'oggetto direttore è vicino al verbo, mentre March read yesterday it è peggio, perché tra il verbo e l'oggetto direttore c'è un aggiuntivo, cioèYesterday. Tuttavia, questo effetto può essere attenuato quando l'oggetto direttore è pesante e lungo, poiché allora può essere spostato dopo l'aggiuntivo. Questo è illustrato qui. Quindi, entrambe queste frasi sono corrette: March read this absolutely fascinating book about the BC yesterday, è corretto, ma invece di "it" ho questa lunga NP, ma è anche corretto dire: March read yesterday this absolutely fascinating book about the BC. Quindi, la ragione qui è che è possibile dire queste due cose, anche se questa frase viola il principio grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, ma soddisfa il principio di minimizzazione delle dipendenze, che dice che le dipendenze più brevi sono preferite. Quindi, queste due strutture solo mostrano la lunghezza delle dipendenze cruciali, cioè le dipendenze che non sono costanti tra queste due strutture. Quindi, qui abbiamo una dipendenza da "read" all'aggettivo di lunghezza sette misurata in parole e dalla "red" al libro di lunghezza quattro, quindi insieme è undici. Quando si sposta o si scambia queste due costituenti, la somma di queste due dipendenze diventa sei. Quindi, invece di undici, sei, molto più breve, è per questo che suona piuttosto bene. Viola un principio, ma soddisfa un altro. Quindi, cosa facciamo? Estraiamo statistiche rilevanti da coordinate in versione migliorata del Pank, e vedere il paper per capire per quale motivo non utilizziamo le strutture di dipendenze universali. Queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono a essere più corti. Quindi, salt and pepper e not pepper and salt misurato in sillabare. E anche l'osservazione che è stata fatta passando che questa tendenza cresce con la lunghezza della differenza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto sinistro preferisce essere il primo, più forte. Quindi, la proporcione è maggiore dei congiunti sinistri corti. Ma, cosa nuovo in questo paper è che abbiamo osservato che questa tendenza si verifica solo quando il governante sinistra è assente. Quindi, il governante sinistra in questo esempio è Lisa, quindi il governante è sul sinistra. E se è assente in questo esempio, Homer came and sneezed, qui ci sono due verbi da coordinare e non c'è un esterno governante esterno. Quindi, in queste case, il congiunto sinistro preferisce essere più corto, tanto più il maggior differenza tra i due congiunti. Tuttavia, quando il governante è sul destro, come qui, la governante coordinazione non ha questo effetto. Così, dimostriamo che, misurando la lunghezza in caratteri, che è la prima colonna in sillabare, la seconda colonna in parole, la terza colonna, mi concentro sulla terza colonna. Quello che vediamo qui è che quando il governante è sul sinistra, la tendenza per il congiunto sinistro di essere più corto cresce gradualmente con la differenza assoluta in parole. E lo stesso è osservato quando non c'è governante, come in coordinate di frasi, ma quando il governante è sul destro, quest'effetto scompare. E dimostriamo nel paper come questo fornisce un argomento contro le strutture di coordinazione asimmetriche come queste due e a favore delle strutture simmetriche come queste due. Quindi, vedere il paper per la piena spiegazione e argomenti, scusate, e parlare con noi durante la sessione poster. Grazie mille.</sample>
    <sample id="15">Il numero di autori coinvolti nell'articolo è tre.</sample>
    <sample id="16">I domini dei testi più semplificati sono i testi di Bible.</sample>
    <sample id="17">The speaker introduces a method for multi-modal relation extraction, which involves combining textual and visual data to better understand semantic relations between entities in various forms. The proposed method includes fine-grained information pruning over two modalities and the exploitation of external information such as topic information. Experiments on the MRQA dataset show that the proposed method outperforms text-based methods and achieves significant improvements over existing models.</sample>
    <sample id="18">Un esempio di preferenza per i congiunti a sinistra più brevi è "soltanto sale e pepe" rispetto a "sale e pepe".</sample>
    <sample id="19">The presentation discusses the challenges of open-domain question answering and proposes a two-stage framework to address them. The first stage retrieves evidence contexts from Wikipedia Corpus, while the second stage uses a reader to understand the question and retrieve the answer. The presenter introduces their work, which aims to achieve efficient open-domain question answering systems with smaller memory costs, faster inference, and comparable performance. They propose using lightweight models, parameter sharing, and designing full models to reduce model size. They also compare existing open-domain question answering models and conclude that retrieval-only systems are better for low-resource environments, while retrieval and reader systems are more appropriate for trade-offs between speed and performance.</sample>
    <sample id="20">Sì, i modelli sono freely available su Kaggle e i script di addestramento sono su GitHub repository.</sample>
    <sample id="21">DEplain-apa è basato su testi di notizie.</sample>
    <sample id="22">Per una buona generalizzazione, i fattori principali sono l'architettura del modello, la dimensione del modello e il numero di esempi di finetuning.</sample>
    <sample id="23">The paper discusses the challenges faced by text-to-image models in accurately rendering text and proposes a new strategy to improve model spelling ability. The authors investigate the performance of different text encoders, including T5 and PaLM, and find that they struggle with spelling, especially for less frequent words. They introduce a new model called BitT5, which has full access to character-level information and performs well at spelling. To improve image generation characteristics and text rendering capabilities, the authors augment the Imagen model by concatenating an additional text representation from BitT5. This approach results in improved model spelling ability and better text rendering, despite the diffusion model still introducing errors.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistiche estratte da un'analisi statistica di coordinate. Queste statistiche hanno confermato l'osservazione precedente che i congiunti a sinistra tendono ad essere più brevi, in quanto le coordinate come "soltanto sale e pepe" e "sale e pepe" sono mesurate in sillabare. Inoltre, è stata osservata che questa tendenza cresce con la differenza di lunghezze tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti per studiare l'effetto della posizione del governatore hanno utilizzato statistiche estratte da una versione migliorata del Corpus di Pankration e hanno analizzato la lunghezza delle dipendenze tra i concordanti. I risultati hanno confermato che i concordanti a sinistra tendono ad essere più corti, specialmente quando il governatore è assente. Questo effetto scompare quando il governatore è sulla destra.</sample>
    <sample id="26">Un classificatore base addestrato su un insieme di dati non bilanciati non è efficace, poiché non ha accesso a sufficienti esempi di classi minori.</sample>
    <sample id="27">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="28">Bob e Alice</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="30">The paper introduces a framework called Blender for ensemble learning of large language models. It uses pairwise ranking and generative fusion to select the optimal model for each input example, rather than using a single top-performing model. The framework consists of two stages: first, it runs multiple models on a given input and ranks them using a pairwise ranking module; second, it selects the top k candidates and uses a generative fusion model to produce the final output. Experiments show that Blender outperforms individual models and achieves better performance than other ranking methods.</sample>
    <sample id="31">I fornitori dell'articolo sono John Goldacre, Aaron Mueller, Kaneshka Mishra, Garen Fuentes, Roger Levy e Adina Villamor.</sample>
    <sample id="33">Il framework quantifica esattamente la posizionalità confrontando le annotazioni con reali utenti con i modelli e i set di dati esistenti utilizzando un score di correlazione.</sample>
    <sample id="34">The paper presents a framework called Crest, which combines selective rationalization and counterfactual text generation to produce valid and natural counterfactual examples. The framework consists of two components: a rationalizer that generates meaningful rationales for the input, and a predictor that uses these rationales to produce a decision. The counterfactual generation component focuses on the rationales produced by the rationalizer and uses them to generate counterfactual examples. The paper evaluates the quality of the counterfactuals using both automatic metrics and human evaluation, and finds that Crest produces more valid and natural counterfactuals than other methods. The paper also proposes an alternative way to perform rationalization with both factual and counterfactual examples, which improves the performance of downstream models. Finally, the paper analyzes the interpretability of the rationales generated by Crest in three dimensions: plausibility, forward simulability, and counterfactual simulability, and finds that Crest produces more plausible and counterfactual simulable rationales than other methods.</sample>
    <sample id="36">The paper presents a method for improving multilingual machine translation by using language-specific layers (LSLs) in a transformer model. The authors propose placing LSLs only where they are most needed, while keeping the inference cost constant. They achieve this by training a large model with shared, source, and target weights, and then selecting the best placement based on the largest weight. The results show significant improvements over baseline models and language adapters, especially for low-resource languages.</sample>
    <sample id="37">Il studio precedente ha dimostrato che i soggetti umani hanno anche rivelato stereotipi razziali quando hanno ricevuto gli stessi prompt di persona.</sample>
    <sample id="38">La fonte di dati utilizzata in questo studio è stata la banca di testi per la lingua inglese.</sample>
    <sample id="39">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di discordanze indipendenti e la classificazione bimane di espansione e di comparazione.</sample>
    <sample id="41">This paper introduces a world-level personal common sense knowledge graph called Pika, which contains about 3.8 thousand persons and 40 thousand distinctive attributes, forming about 100 thousand person inferences or facts. Pika is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-sourcing annotations of Pika relations using a joint human-AI majority voting scheme. The paper also explores whether Pika can help language models learn and generalize person knowledge by training a bar-based common knowledge generator on a person attribute inference task and investigating a person-grounded dialog generation task on the Coreset AI2 person chat dataset. The results show that Pika achieves better automatic evaluation results and higher acceptance rates in human evaluation compared to baseline models.</sample>
    <sample id="42">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="43">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="44">Il framework differisce dai precedenti studi in quanto confronta le annotazioni reali con i modelli e i set di dati esistenti, invece di limitarsi a analizzarne l'acquisizione o la distribuzione.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è la dietrosofica.</sample>
    <sample id="46">Google Translate e DeepL sono i sistemi commerciali che sono stati confrontati.</sample>
    <sample id="47">Ciao, mi chiamo Changbing, sono uno studente di PhD presso l'Università di Washington. Oggi, sto presentando il nostro lavoro che va dal data pretraining fino ai modelli di linguaggio e fino alle tare downstream, analizzando le tracce dei bias politici che portano a modelli NLP non equi. I modelli di linguaggio vengono addestrati su grandi quantità di dati web. Le notizie politiche sono bene rappresentate nel loro dataset pretraining. Secondo un saggio del C4 Corpus, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post ecc. sono bene rappresentati nel dataset di addestramento dei modelli di linguaggio. Questo ha creato un mix di benessi per le applicazioni dei modelli di linguaggio. Di un lato, hanno imparato da diverse prospettive che celebrano la democrazia e la pluralità delle idee. Dall'altro lato, queste diverse opinioni politiche sono socialmente biased e possono portare a problemi di giustizia in applicazioni downstream. Per questo, proponiamo di indagare il percorso dei bias politici dalla data pretraining ai modelli di linguaggio fino alle tare downstream, specificamente chiedendo le seguenti domande: Prima, come valutiamo la linearità politica dei modelli di linguaggio e qual è il ruolo dei dataset pretraining sulla linearità politica? Secondo, come i modelli di linguaggio con diverse linearità politiche realmente performano sulle tare downstream e se potrebbe portare a problemi di giustizia in applicazioni NLP? Di conseguenza, innanzitutto propone di stimare i modelli di linguaggio con diverse formule di prompt utilizzando domande politiche come il test di compasso politico. Questo assicura una valutazione automatica basata sulla scienze politiche. Alcuni risultati preliminari dimostrano che i modelli di linguaggio hanno linearità politica variante. Occupano tutti i quadranti del compasso politico. Possiamo vedere che GPT-4 è il modello di linguaggio più liberale tra tutti e GPT-3 series sono generalmente più liberali rispetto a Bert series e le sue varianti. In secondo luogo, ci siamo di fronte all'obiettivo di indagare a che punto, a che punto esatto, i bias politici dei modelli di linguaggio sono davvero presi in mano dal dataset pretraining. Così, ci saremmo voluti sottoporre a un esperimento controllato per ulteriormente addestrare i modelli di linguaggio su punti di controllo diversi separati in notizie e media social, ulteriormente divisi in diverse linearità politiche. Attravessando i modelli di linguaggio su tali punti di controllo, possiamo vedere che le coordinate ideologiche dei modelli di linguaggio si spostano corrispondentemente. Ad esempio, se Roberta è ulteriormente finetuned e ulteriormente addestrato su un corpus di notizie sinistra, possiamo vedere un trasferimento sostanziale verso la sinistra in termini di bias politici. E anche proviamo a indagare se i modelli di linguaggio possono prendere in mano la polarizzazione prevalentemente nella nostra società. Così, dividiamo i punti di controllo in pre 45° Presidente degli Stati Uniti e dopo 45° Presidente degli Stati Uniti. Separatamente addestriamo i modelli di linguaggio su due tempi diversi. Possiamo vedere che i modelli di linguaggio hanno generalmente una linearità politica che si sposta ulteriormente lontano dal centro dopo 2017. Così, indica che i modelli di linguaggio possono anche prendere in mano la polarizzazione nella nostra società. Infine, valutiamo i modelli di linguaggio con diverse linearità politiche su deteczione di discorso razzista e deteczione di notizie fake, due applicazioni NLP che spesso coinvolgono modelli di linguaggio e che hanno implicazioni molto significanti. Ci vediamo che se investighiamo la performance per category, ovvero se separiamo la performance in diverse demografiche o linearità politica di notizie, possiamo vedere un模式 che, ad esempio, per la deteczione di discorso razzista, i modelli di linguaggio sinistri sono migliori nel rilevare il discorso razzista che bersera gruppi minoritari, tuttavia sono peggiori nel rilevare il discorso razzista che bersera gruppi più potenti nella nostra società. E viceversa, i modelli di linguaggio destretti sono migliori nel rilevare il discorso razzista che bersera gruppi minoritari come i neri, i Lgbtq+ e altri, e viceversa. Simili tendenze anche per la deteczione di notizie fake, dove vediamo che i modelli di linguaggio sinistri sono migliori nel rilevare la menzogna proveniente da un orientamento politico opposto e viceversa. Inoltre, mostriamo molti esempi qualitativi per vedere che i modelli di linguaggio con diverse linearità politiche danno diverse predizioni per esempi di discorso razzista e menzogna esempi sulla base della propria storia. Ci sono un sacco di esempi qualitativi in appendix per ulteriormente evidenziare che. Questo indica che c'è un problema di giustizia che è molto rilevante riguardo ai bias politici dei modelli di linguaggio. Ad esempio, se i modelli di linguaggio destretti fossero addestrati su discorso razzista o menzogna e fossero deployati su piattaforme social media popolari, significa che le persone con opinioni politiche diverse potrebbero essere marginalizzate e il discorso razzista bersera minori gruppi potrebbe continuare senza alcun controllo. Così, ci ha messo in guardia per riconoscere e affrontare i problemi di giustizia derivanti dai bias politici dei modelli di linguaggio. Di conseguenza, un po' di discussione, vorremmo anche evidenziare che espongiamo il dilemma unico riguardo ai bias politici dei modelli di linguaggio. Si è come tra Sirea e Caribdis. Se non sanizziamo le opinioni politiche nel dataset di addestramento dei modelli di linguaggio, i bias potrebbero propagarsi dalla data pretraining ai modelli di linguaggio fino alle tare downstream, creando problemi di giustizia. Se invece proviamo a sanizzarle in qualche modo, rischiamo di censurare o escludere e è incredibilmente difficile determinare cosa è realmente neutra e dovrebbe essere retain in dataset di addestramento dei modelli di linguaggio. Così, è come il problema dell'elettricità elettrica. Ok, bello, penso che sia quasi tutto quello che ho detto, grazie per il tuo tempo.</sample>
    <sample id="48">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 124 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces DePlain, a new corpus for German text simplification at both the document and sentence levels. It highlights the challenges with existing corpora, such as size and alignment errors, and introduces DePlain, which is divided into two sub-corpora: DePlain API and DePlain Web. The API corpus includes 483 manually aligned documents from news texts, resulting in approximately 30,000 parallel sentence pairs. The Web corpus includes 750 documents, with 13,000 manually aligned and 2,450 automatically aligned, totaling 30,450 sentence pairs. The presentation also discusses the variety of simplification transformations within the corpus and its potential applications, including evaluating automatic alignment methods and fine-tuning language models for text simplification.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è il concetto di avere un punto di vista o una visione del mondo basata sul proprio demografia, identità e esperienze di vita.</sample>
    <sample id="53">Il relatore è Dawei.</sample>
    <sample id="54">The speaker, Vasudha, is a PhD candidate in computer science at Stony Brook University. She is presenting her work accepted into ACL 2023 on transfer learning for dissonance detection, addressing the rare class challenge. The study defines cognitive dissonance and its importance in understanding disagreement among people, tracking trends in beliefs, values, and attitude changes in populations, and mental health issues like anxiety disorders. The research also explores the relationship between dissonance and extremism or polarization of vulnerable groups. To create a cognitive dissonance resource, Vasudha conducted a large-scale annotation of dissonance relations using a discourse unit pair approach. However, due to the low occurrence of dissonance, the initial classifier performed poorly. To address this, she experimented with transfer learning and active learning to annotate more dissonance samples efficiently. The proposed probability of rare class strategy (PRC) works better than other state-of-the-art strategies, improving the AUC score to 7.5, which is the best performance so far.</sample>
    <sample id="55">Sì, il modello ST offline esistente viene adattato utilizzando solo un modello per ogni regime di latenza e gestendo la latenza tramite parametri specifici.</sample>
    <sample id="56">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) il contesto di testa, dove la conoscenza di sfondo è supposta di essere disponibile in tempo di addestramento; 2) il contesto di testo-bosco, dove la conoscenza di sfondo è disponibile sia in tempo di addestramento che in tempo di inferenza; 3) il contesto di inferenza, dove entrambe le tipologie di conoscenza sono disponibili solo in tempo di inferenza.</sample>
    <sample id="59">The presentation introduces Dr. Bert, a robust pre-trained model in French for biomedical and clinical domains. It compares Dr. Bert with other models trained on different data sources and sizes, evaluating their performance on 11 biomedical and clinical downstream tasks in French. The results show that Dr. Bert outperforms other models, especially when fine-tuned on domain-specific data. The presentation also discusses the availability of the model and training scripts for further use.</sample>
    <sample id="60">I fornitori di informazioni dell'articolo sono Javahar Hosaini, Philip Radlinski, Sylvia Parati e Annie Luis.</sample>
    <sample id="61">La terza domanda di ricerca è se dobbiamo usare solo i campioni puliti per la valutazione o se ci sono altre migliori modalità per utilizzarli.</sample>
    <sample id="62">The paper presents a systematic study on knowledge distillation for natural language generation with pseudo-target training. The authors aim to compress large language models while preserving their performance by exploring the potential of knowledge distillation. They compare different approaches, including word-level and sequence-level distillation, and focus on task-specific distillation for NLP tasks such as summarization, question generation, common sense reasoning, and simplification. The study uses unlabeled data and medium-sized teacher models to achieve high compression rates and efficiency in inference time.</sample>
    <sample id="63">La metrica della sensibilità misura la capacità del modello di produrre gli stessi output per le stesse attività, indipendentemente dalla variazione nella formula dell'instruzione.</sample>
    <sample id="64">Il nome della relatrice o del relatore è Jing Wei.</sample>
    <sample id="65">In questo contesto, una maggiore sensibilità indica che il modello è più sensibile alle variazioni nella redazione dell'intraveccio.</sample>
    <sample id="66">This paper presents a survey on deep learning for mathematical reasoning, focusing on the development of machines capable of solving math problems and proving theorems. It discusses the importance of mathematical reasoning in human intelligence and its applications in AI and NLP. The paper covers various aspects of mathematical reasoning, including text-based data, visual contexts, and table contexts. It also highlights the challenges and limitations of current models, such as the lack of ability to perform precise mathematical reasoning. The paper proposes using self-consistency and tool-augmented LLMs to improve performance and suggests exploring low-resource languages and domains to further develop these models.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models and proposes a method to mitigate it. The authors find that severe interference occurs when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. They also find that language similarity and the number of languages do not have a large impact on interference levels. The authors propose using temperature sampling as a simple solution to control the trade-offs, and they show that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">I modelli vengono addestrati su un contesto linguistico che include sia frasi accettabili che non accettabili.</sample>
    <sample id="69">In genere, si possono ottenere prestazioni elevate in WSL con 20 campioni di convalida puliti per classe. Tuttavia, se si opta per l'ottimizzazione direttamente sul dataset pulito, si possono ottenere anche prestazioni migliori.</sample>
    <sample id="70">I ricercatori che hanno scritto l'articolo sono Myra, Essendarmouch e Dan Jorovski.</sample>
    <sample id="71">The paper presents a dataset of indirect referring expressions for entity selection in conversational systems. The dataset, collected using crowdsourcing, covers three domains: music, books, and recipes. It includes 6000 alternative questions across these domains, with 42,000 indirect referring expressions. The dataset emphasizes informativeness using a cartoon completion setup, where annotators are shown background knowledge about the entities and asked to pick one and describe it using indirect referring expressions. The results show that models with access to exact same background knowledge as annotators achieve high accuracy (92-95%), while those with partially overlapping background knowledge achieve lower accuracy (82-87%). Models with access only to entity names perform poorly (60%). The models are also domain generalizable.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione per capire come i modelli di linguaggio possono propagare i bias politici e influenzare le applicazioni downstream, come la deteczione del discorso razzista e la deteczione delle bugie.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Mack Shattah.</sample>
    <sample id="74">The paper introduces a new method for constructing a dense knowledge graph called DenseNOMIC, which is based on a large-scale common technology base called Atomic. The paper describes the process of converting tail events into the same expression as head events and proposes a relation prediction model to improve the performance of Atomic. The authors compare their method with state-of-the-art methods and show that DenseNOMIC achieves higher knowledge coverage and more diverse results.</sample>
    <sample id="75">The speaker introduces their work, "Joint Prop," a joint entity recognition and relation extraction task. They discuss the challenges of supervised learning models requiring extensive labeled data and diverse annotated data for various domains and applications. They propose a joint semi-supervised learning framework to model entity and relation tasks by propagating labels over heterogeneous graphs and considering interconnections among labeled and unlabeled data. The framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The experiment results show significant improvement in both entity and relation tasks on single-task datasets compared to baseline models.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici include la raccolta e l'elaborazione dei dati di addestramento, il costruire e l'addestrare dei modelli di linguaggio e l'applicazione di questi modelli a compiti downstream.</sample>
    <sample id="77">This video introduces a new dataset called Defacto, which contains human demonstrations and feedback for improving summarization factual consistency. The dataset is based on the original system-generated summaries of existing summarization models and includes labels to decide whether the summary is factually consistent or not. The video also proposes three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. The video shows that the proposed dataset can be used for training factuality metrics and evaluating factuality metrics.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="79">Coscript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita esattamente quando il numero di trigger in un testo è maggiore di m.</sample>
    <sample id="81">I fornitori dell'articolo sono l'Institute of Computing Technology, Peking University.</sample>
    <sample id="82">This video introduces a new framework for unsupervised automated essay scoring (AES) using multi-heuristic quality signals. The proposed Unsupervised Rank Aggregation (URA) framework consists of two main components: the Heuristic Essay Ranking (HER) module and the Deep Pairwise Rank Aggregation (DPRA) module. HER generates partial order pairs by ranking essays based on multiple heuristics, while DPRA unifies these pairs into a single supervision signal to train a neural AES model. Experiments demonstrate that URA outperforms unsupervised baselines and achieves competitive performance with supervised methods.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">The speaker introduces a paper on a novel framework for dynamic neural networks, which can adapt its architecture and parameters based on input data. They discuss the challenges of existing fully dynamic networks due to excessive parameter usage and propose a partially dynamic network that maintains the representation power of the original network while using fewer parameters. The method involves partitioning parameters into dynamic and static ones with scale factors to control their intensity. Experiments show improved performance over static and fully dynamic networks, particularly in terms of parameter efficiency and computation speed. The paper also explores optimal dynamic ratios and scale factors for different dynamic layers like convolution and mixture experts, and compares the proposed method with state-of-the-art techniques, showing superior performance in network compression and output discrimination.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è fare un pasticcino con specifiche istruzioni, come far un pasticcino con specifiche istruzioni.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo utilizzando un insieme di frasi come trigger set. Questo insieme di frasi è selezionato in base alla frequenza di apparizione delle parole nel corpus di testo generale e viene utilizzato per definire il target embedding.</sample>
    <sample id="87">Il lavoro utilizza i modelli pre-addestrati per costruire uno nuovo.</sample>
    <sample id="88">GPT-4 è meno allineato con i paesi non binari.</sample>
    <sample id="89">La relatrice fornisce un esempio in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione.</sample>
    <sample id="90">This paper questions the necessity of recruiting native speakers for data annotation in natural language processing (NLP) and examines the feasibility of using language learners as annotators. The authors conducted a proof-of-concept study to compare the accuracy and learning effects of labels annotated by language learners versus native speakers. They recruited language learners with varying proficiency levels and provided them with additional resources to aid in annotation tasks. The results showed that language learners' annotations are nearly accurate, especially for simpler tasks and easy to medium level questions. Moreover, language learners' annotations aggregated with others by majority voting almost matched the accuracy of native speakers' annotations. The paper also demonstrated that training simulations with learners' annotations can outperform models trained with native speakers' labels. Overall, this work suggests a novel way for data construction in low-resource languages by recruiting language learners as annotators, which could broaden NLP research for many languages and overcome geographic and technological barriers to building benchmark datasets.</sample>
    <sample id="91">La quantità di attività influisce sulla performance del modello in quanto, con un aumento del numero di attività, il modello raggiunge un better performance e allo stesso tempo ha una sensibilità più bassa.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con altri modelli che non usano alberi e con due tipi di strutturale generalizzazione: la generalizzazione a strutture più semplici e la generalizzazione a strutture più complesse.</sample>
    <sample id="93">I due coautori sono collaboratori del primo autore.</sample>
    <sample id="94">The paper presents a backdoor watermark method for protecting the copyright of embedding as services. The method involves selecting a trigger set, defining a target embedding, and computing the similarity between the requested embedding and the target embedding using cosine and L2 similarity. The results show that the proposed method has great detection performance while keeping great utility for downstream tasks.</sample>
    <sample id="95">Il primo autore di PaLM è Avi Lillard.</sample>
    <sample id="96">Salve, mi chiamo Jenny e sono studentessa di primo anno di laurea in Scienze Politiche all'Università Carnegie Mellon. Oggi presenterò il mio lavoro sulle caratteristiche del design basate su dataset e modelli. Questo lavoro è stato realizzato in collaborazione con alcuni ricercatori dell'Università di Washington e con il laboratorio di AI dell'Alibaba, diretti da Sebastian Santi, Roman Labrozzi, Katerina Ryniaka e Martin Sap. Iniziamo con un esempio: immagina di essere un redattore di un giornale e stai leggendo i commenti sottoposti a un articolo. Potresti ricorrere a un API di analisi del linguaggio naturale per rilevare i contenuti tisoni. Potresti pensare di usare un API popolare come Perspective API per la rilevazione della tossicità. Questo funziona bene se sei Karl Jones, dove l'API di Perspective riesce a rilevare correttamente i casi di tossicità. Ma non è così per Dithia Sharma, dove l'API di Perspective non è sensibile ai termini più comuni nel contesto indiano. Questo è un esempio di bias di progettazione, dove si producono differenze sistematiche nella performance delle tecnologie tra le popolazioni. I bias di progettazione come quello che vediamo qui prima possono derivare dalla posizionalità dei ricercatori e dei sviluppatori di modelli. La posizionalità è semplicemente la prospettiva che hanno le persone come risultato delle loro demografiche, identità e esperienze di vita. Questo concetto è ampiamente utilizzato in studi critici, in particolare in ambienti accademici femministi e queer. Come ricercatrice, la posizionalità può influenzare il processo di ricerca e i suoi risultati, poiché cambia le decisioni che i ricercatori prendono. Quindi, una domanda che le persone potrebbero fare è se i dataset e i modelli hanno posizionalità. Non stiamo cercando di dire che i modelli e i dataset hanno identità demografiche e esperienze di vita, ma che aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizionalità più di altre. Prima的工作已经提供了关于数据集和模型具有posizionalità的轶事证据，例如文化差距和模型和数据集中的偏见，以及对模型posizionalità的理论定义。然而，这些工作并没有通过将真实用户与数据集和模型本身进行比较来研究posizionalità。研究数据集和模型posizionalità è sempre più importante poiché i task di NLP diventano più soggettivi e socialmente orientati. E' difficile caratterizzare come queste posizionalità sono incluse, poiché non tutti i processi decisionali sono documentati e molte modelle sono nascoste dietro API. Per studiare la posizionalità dei dataset e dei modelli, confrontiamo le annotazioni con reali utenti con i dataset e i modelli esistenti. Lo facciamo tramite il framework NLP posizionalità. Il nostro framework funziona in due passi principali. Il primo passo è rere annotare i dataset con diverse annotatrici. Ci optiamo di farlo, invece di guardare le demografie dei ricercatori originari dei dataset, perché di solito solo un paio di annotatrici annotano ogni istanza e le demografie non vengono raramente raccolte e condivise. Ci optiamo di rere annotare i dati per ottenere molte annotazioni per istanza e per ottenere un insieme ricco di demografie. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando il correlatore di Pearson. Così, il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori in quanto confronta gli utenti reale con i modelli e i dataset previsori e etichettati piuttosto che guardare solo la concordanza degli annotatori o le distribuzioni degli annotatori. Il framework è largamente abilitato attraverso Lab in the Wild, una piattaforma di crowdsourcing online, partner di collaborazione precedente di H2C. Lab in the Wild è una piattaforma di esperimentazione online in cui possiamo reclutare volontari diversi rispetto alle piattaforme come MTurk, che in gran parte hanno partecipanti provenienti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è in grado di ottenere alta qualità dei dati. Abbiamo creato due task su Lab in the Wild: uno è il test di accettabilità sociale e il altro è il test di rilevazione di discorso offensivo. Questo è fatto in modo che i partecipanti leggano una situazione dal dataset di chimica societaria e scrivano come socialmente accettabile è quella situazione. Dopo aver finito di scrivere, per rimanere coinvolti nella ricerca, i partecipanti possono confrontare le proprie risposte con quelle di un AI e di altri. Abbiamo quindi confrontato queste annotazioni con GPD4 per l'analisi di accettabilità sociale e con DinaHate per il test di rilevazione di discorso offensivo. Abbiamo replicato un setup molto simile per il test di rilevazione di discorso offensivo, in cui i partecipanti leggono un esempio da DinaHate e scrivono se pensano che sia un esempio di discorso offensivo. Abbiamo quindi confrontato queste annotazioni con DinaHate, Perspective API, Rewire API, Hate Roberta e GPD4. Il nostro studio ha raccolto più di 16.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi. Ora che siamo meglio equipaggiati per rispondere a chi alignano i dataset e i modelli con il più? Troviamo che ci sono posizionalità in NLP. Ad esempio, trovi che i dataset e i modelli sono più alignati con i paesi che parlano inglese. Quindi, per GPD4, l'analisi di accettabilità sociale, trovi che è più alignata con i paesi che parlano confuciano e inglese. Trovi anche che DinaHate è anche più alignata con i paesi che parlano inglese. Trovi anche un'altra alleanza aggiuntiva con le persone che hanno un'istituzione di istruzione superiore. Quindi, per GPD4, nella nostra analisi di accettabilità sociale, trovi che è più alignata con le persone che hanno un'istituzione di istruzione superiore o un'istituzione di istruzione post-istituzionale. E trovi lo stesso per DinaHate, dove è più alignata con le persone che hanno un'istituzione di istruzione superiore. Tuttavia, quando i modelli e i dataset sono alignati con specifiche popolazioni, inevitabilmente alcune popolazioni vengono lasciate indietro. Un esempio di questo è che i dataset e i modelli sono meno alignati con le persone non binarie rispetto ai corrispettivi maschili e femminili. Trovi questo in GPD4, nell'analisi di accettabilità sociale, come anche nell'analisi di DinaHate. Quindi, data la posizionalità in NLP, cosa possiamo fare al riguardo? Quindi, abbiamo alcune raccomandazioni per questo. La prima è tenere un record di tutti i relativi scelte di progettazione durante il processo di ricerca. L'altra è fare NLP ricerchi con un lens di positivismo. La nostra terza raccomandazione è costruire dataset specializzati e modelli specifici per specifici gruppi di comunità. Un esempio di questo è l'iniziativa MusaKani. Vogliamo sottolineare che NLP inclusivo non è solo fare in modo che tutte le tecnologie funzionino per tutti. Ecco come conclude la nostra presentazione. Se vorreste imparare di più, vi prego di controllare il nostro dashboard per i risultati più aggiornati e il mio articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona due problemi associati a SimulST: i modelli specifici di architettura sono generalmente addestrati introducendo moduli aggiuntivi per essere ottimizzati e i processi di addestramento sono lunghi e complessi, ad esempio addestrando modelli con diverse obiettivi di ottimizzazione e addestrando e mantenendo diversi modelli per raggiungere differenti regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere la sanitizzazione dei bias. Questo può essere fatto utilizzando tecniche di pulizia dei dati o di addestramento che siano più neutri e non tendenziosi. Tuttavia, è importante notare che determinare cosa è realmente neutro può essere difficile e che esiste un rischio di censure o esclusione.</sample>
    <sample id="99">Ciao, mi chiamo Si Yu Yang e sono di Fudan University. Sono qui per presentare il nostro lavoro che si focalizza sulla distinzione della conoscenza del script da modelli di lingua naturale per pianificazione linguistica in vita quotidiana. In vita quotidiana, gli esseri umani spesso pianificano le proprie azioni seguendo istruzioni scritte in forma di script. Lavori precedenti hanno sfruttato modelli di lingua naturale per pianificare obiettivi astratti di attività stereotipate, come fare un pasticcino, dimostrando che i modelli di lingua naturale possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, i precedenti studi hanno generalmente concentrato su pianificare obiettivi astratti di attività stereotipate, pianificare obiettivi con specifiche condizioni specifiche, come fare un pasticcino di cioccolato, rimane sottostudiato. In questo articolo, definiamo il problema della pianificazione linguistica con vincoli, che impone diverse condizioni alla pianificazione degli obiettivi. Un obiettivo astratto può essere ereditato da diversi obiettivi reali specifici con vincoli multi-fattori. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fattibili rispetto ai vincoli. In questo articolo, innanzitutto valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua naturale. Poiché non esiste un dataset di obiettivi specifici per supportare lo studio, dobbiamo acquisire questi obiettivi innanzitutto. Come illustrato nella tabella, estendiamo gli obiettivi astratti con vincoli multi-fattori per umani in una fase di acquisizione di dati utilizzando il comando GPT-3. Abbiamo selezionato 100 obiettivi specifici e valutato i script generati dai modelli di lingua naturale. Questa tabella riporta l'accuratezza generale dei risultati. Abbiamo notato che tutti i modelli di lingua naturale hanno ottenuto risultati insoddisfacenti nel pianificare obiettivi specifici. Poi, eseguiamo un'analisi dettagliata per indagare i risultati dei modelli di lingua naturale. Il grafico illustra che la completezza semantica dei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Dividiamo i vincoli in tabelle categorizzate differenti in WikiHow. Il diagramma nella figura illustra che le prestazioni del pianificatore di istruzioni GPT-3 variano considerabilmente per gli obiettivi di diverse categori. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di lingua naturale è a variazione elevata, portando a peggioramenti delle prestazioni. Di conseguenza, adottiamo l'idea di un filtra over-generated Z per migliorare la qualità della generazione. Innanzitutto mostriamo tipi vincolari con esempi per istruzioni GPT-3 e otteniamo obiettivi specifici basati su obiettivi astratti. Poi, istruzioni GPT-3 generano script specifici per obiettivi specifici. Successivamente, sviluppiamo un modello di filtra per selezionare i script più fedeli. Convertiamo i script e gli obiettivi in istruzioni GPT-3 embeddings e calcoliamo la similarità coseno e la similarità di similitudine per misurare la similarità semantica. Inoltre, consideriamo i script che contengono chiavi di vincolo di destinazione. Solo se il punteggio di similarità del target è il più alto tra gli obiettivi, consideriamo il script. Con il nostro metodo, istruzioni GPT-3 possono generare script di alta qualità. Il nostro metodo migliora grandemente la pianificazione sia in completezza semantica che in fedeltà ai vincoli. Poiché i modelli di lingua naturale sono costosi da deploys, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare un dataset è un passo essenziale verso l'obiettivo. Tuttavia, i precedenti studi non hanno abilitato la pianificazione per obiettivi specifici e la annotazione manuale di dataset è costosa. Di conseguenza, seguiamo l'idea di dissoluzione simbolica di conoscenza per creare dataset di pianificazione linguistica con vincoli denominati come CoScript. In totale, generiamo 55.000 obiettivi specifici con script per assicurare la qualità del dataset di validazione e test. Chiediamo a worker cloud-sourced di trovare e revisionare样例样本。 Questa figura illustra la distribuzione vincolare di CoScript. Abbiamo notato che CoScript ha un'approccio elevato in script generati per obiettivi specifici. Con CoScript, possiamo addestrare modelli più piccoli e specializzati per pianificazione linguistica con vincoli. Abbiamo notato che T5 finisce con CoScript genera script di alta qualità rispetto ai maggiori modelli di lingua naturale, dimostrando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su dataset appropriati. In sintesi, stabiliamo il problema di pianificazione linguistica con vincoli. Evaluiamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua naturale e sviluppo un filtra over-generated Z per i modelli di lingua naturale. Utilizziamo modelli di lingua naturale per generare un dataset di script di alta qualità CoScript per pianificazione linguistica con vincoli. Speriamo che il dataset CoScript possa essere un risorsa utilizzabile per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Trovi maggiori dettagli su CoScript nel mio articolo.</sample>
    <sample id="100">The paper presents a new approach for multi-hop question answering (Q&amp;A) that uses a few-shot language model to rank candidate chains retrieved by TF-IDF retrieval and hyperlink traversal. The approach combines an unsupervised retrieval method with a few-shot language model-based reranking, using a likelihood of the question given the chain as the scoring function. The paper evaluates the performance of the proposed approach on the MultiHop QA dataset and compares it with fully supervised systems and state-of-the-art multi-hop retrievers. The results show that the proposed approach outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop retrievers. The paper also evaluates the downstream QA performance when using the proposed approach as a retriever and finds that it exhibits very good downstream QA multi-hop QA performance underperforming in DR by only around 4 exact match points.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi d'art.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono che dovrebbe essere applicabile a servizi di imbedding, non dovrebbe indebolire l'utility dei servizi originali, dovrebbe essere abbastanza evidente per i nemici per poter rimuovere la filigrana facilmente e dovrebbe essere trasferibile ai servizi nemici durante il processo di estrazione del modello.</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="104">Per la riannotazione, vengono campionate diverse istanze.</sample>
    <sample id="105">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la somma dei coseno e la somma dei quadrati.</sample>
    <sample id="106">The paper presents a dataset called Quest, which includes over 30 million entity-seeking queries with implicit set operations. The answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints. The dataset poses a challenging retrievable problem since systems need to effectively search over a large document corpus to find multiple answer sets where the attribution for different query constraints can come from different parts of the document.</sample>
    <sample id="107">I modelli basati su codificatori multilingue hanno ottenuto le migliori prestazioni su tutti i set di dati e sono stati utilizzati per migliorare i modelli di decodificatore e per ottenere un'ottimizzazione significativa.</sample>
    <sample id="108">The paper presents a new approach to evaluating language models' acceptability judgments by simulating longer sentences and testing their ability to generalize across different contexts. The authors argue that current evaluation pipelines are limited in their ability to assess models' performance on longer sentences, which is crucial given the increasing use of large language models with longer context windows. They propose revisiting the minimal pair paradigm by using longer sequences and introducing a mismatch scenario where sentences are drawn from different datasets or unrelated domains. The results show that language models are sensitive to latent syntactic and semantic features shared across sentences, and that the current evaluation methods may not fully capture their abstract knowledge throughout the context window.</sample>
    <sample id="109">Natural Instructions is a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset contains 64K examples, with an additional 240K examples if instruction paraphrases are considered. The generated examples are evaluated for correctness, creativity, and diversity, with more than 50% of the generated examples being correct and containing valuable information for instruction tuning. The dataset highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowdsourcing who usually collapse into predictable heuristics and form annotation artifacts. At the same time, language models are faster and cheaper than human annotation.</sample>
    <sample id="111">Gli autori decidono di selezionare un insieme di parole a frequenza moderata utilizzando un corpus di testo generale e conteggiano la frequenza di parole.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Zhu Heng. Oggi presenterò il nostro articolo: "Gli entity taggers di Conll 2003 funzionali nel 2023?". Iniziamo. Il nostro articolo ha indagato il problema della generalizzabilità utilizzando la comprensione di entità denominate o NER (Named Entity Recognition). Abbiamo osservato che i modelli hanno utilizzato Conll 2003 per sviluppare NER per quasi venti anni e questa naturalmente solleva diversi problemi. Innanzitutto, questi modelli possono generalizzare al nuovo data? E quando sviluppiamo nuovi taggers, cosa è necessario per una buona generalizzabilità? Allo stesso tempo, se osserviamo una pessima generalizzabilità, cosa causa il crollo del prestigio di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset Conll Plus Plus. Questo è un dataset che abbiamo raccolto da Reuters News nel 2020 e lo abbiamo annotato con le stesse direttive di annotazione di Conll 2003. Abbiamo quindi finetuned più di venti modelli su Conll 2003. Abbiamo valutato questi modelli sia sul dataset Conll 2003 che sul dataset Conll Plus Plus. Infine, abbiamo calcolato il percentuale di variazione in F1 per valutare la generalizzabilità di ogni modello. Cos'è necessario per una buona generalizzabilità? Durante gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che vanno bene. Il primo è l'architettura del modello. Durante gli esperimenti, abbiamo scoperto che i modelli Transformer generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzabilità. Infine, nonostri sappiamo che il numero di esempi di fine-tuning direttamente influenza il prestigio dell'attività di downstream. Qui anche abbiamo scoperto che più esempi di fine-tuning effettivamente portano a una migliore generalizzabilità. Il nostro prossimo quesito è: cosa causa il crollo del prestigio di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattativa, che è l'overfitting causato da riutilizzare lo stesso set di test continuamente e questo è di solito manifestato come la diminuzione dei ritorni su nuovi set di test. La seconda ipotesi è il drift temporale, che è la degradazione del prestigio causata da un incremento della distanza temporale tra i set di train e test. Per l'overfitting adattativa, vediamo che dalla grafica sulla destra la linea rossa con la migliore curva ha una percentuale di variazione maggiore di 1. Questo significa che ogni unità di miglioramento che facciamo su Conll 2003 si traduce in più di una unità di miglioramento su Conll Plus Plus, il che significa che non ci sono ritorni diminuendosi. Questo ci dice che l'overfitting adattativa in questo caso non è osservato. Quindi, cosa succede con il drift temporale allora? Per il drift temporale, abbiamo fatto un esperimento per retrain o continuare a pretrainare i modelli con più recenti dati e abbiamo scoperto che il prestigio degrada con un maggior intervallo temporale tra i set di train e test. Questo conferma la nostra ipotesi che la causa principale del crollo del prestigio è il drift temporale. La nostra conclusione è che per una buona generalizzabilità saremmo in bisogno di una migliore architettura del modello, dimensioni del modello più grandi, insieme a esempi di fine-tuning maggiori. Questi ingredienti devono essere utilizzati insieme, non possiamo avere un ingrediente ma anche gli altri. Allo stesso tempo, abbiamo anche scoperto che il crollo del prestigio qui è causato dal drift temporale e, sorprendentemente, non è causato da overfitting adattativo anche se Conll 2003 è stato utilizzato per più di venti anni. Quindi tornando alla domanda che avevamo sollevato in titolo, Conll 2003 taggers funzionali nel 2023? E la risposta è, in reality, un risoluto sì. Speriamo che il nostro articolo possa fornire maggiori ricerche su come migliorare la generalizzabilità dei modelli. Infine, per favore, assicuratevi di controllare il nostro articolo, il dataset e se avete qualsiasi domanda, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">The speaker introduces their work on ACL 2023, titled "Finding the Pillars of Strength for Multi-Head Attention." They discuss the challenges of large language models, such as heavy parameters, long training times, and huge data requirements. They propose a method called Group Head Attention (GHA) to address these issues by using a divide-and-conquer strategy to compress multi-head attention. The GHA method involves group constraint training and voting-to-stay algorithm to prune redundant heads and achieve significant parameter compression while maintaining performance. The results show that the proposed method can compress up to 90% of parameters without sacrificing performance in tasks like machine translation, language modeling, and abstract summarization.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di dimensione lambda.</sample>
    <sample id="116">Per risolvere il compito di individuazione dell'entità che il pronome "he" si riferisce a, è necessaria conoscenza specifica dell'entità, in questo caso, Servin, e conoscenza generale sulle professioni e le azioni associate a queste professioni.</sample>
    <sample id="117">La qualità dell'esempio è il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente.</sample>
    <sample id="118">The paper presents a submission for the ACL 2023 conference, which focuses on improving pre-training techniques for code-switched NLP. The authors define code-switching as a common occurrence in linguistically diverse communities like India and propose novel MLM techniques and architectural changes to handle this case. They also introduce a surrogate method called frequency MLM and use probing classifiers to verify their claim about the switch point information. The results show that their proposed methods increase the amount of switch point information in the intermediate layers, leading to better performance on sentiment analysis tasks.</sample>
    <sample id="119">GPT-4 e GPT-3 series</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Gli esempi di inferenza diretta sono diretti da direttamente diretti, ad esempio dire il nome del brano "Easy on Me" o la posizione del primo.</sample>
    <sample id="122">I ricercatori dell'articolo sono affiliati all'Università di Fudan.</sample>
    <sample id="123">The research presented in this paper focuses on the development of a multi-modal instruction tuning dataset, named Multi-Inst, which consists of 62 diverse multi-modal tasks covering 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task is equipped with five expert-written instructions. The paper also introduces a unified multi-modal pre-trained model, OFA, which uses a unified vocabulary for language, image tokens, and the coordinate of bounding boxes. The authors use transfer learning from natural instruction datasets to improve instruction tuning and evaluate the performance of the model using metrics such as accuracy, ROUGE-L, and sensitivity. The results show that instruction tuning can significantly improve the performance of OFA on unseen multi-modal tasks, and transfer learning from natural instruction datasets can benefit instruction tuning by reducing sensitivity and improving overall performance.</sample>
    <sample id="124">The presentation discusses the development of a comprehensive TempReason dataset to evaluate and improve the temporal reasoning capabilities of language models (LMs). It introduces three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. The presenter highlights that prior works overemphasize L2 reasoning and proposes a new training strategy involving temporal span extraction pre-training and time-sensitive reinforcement learning. The TempReason dataset covers all three temporal reasoning types and long temporal coverage, with question templates sourced from Wikipedia knowledge base and articles. The proposed training paradigm aims to enhance LMs' temporal reasoning by providing relevant temporal knowledge and rewarding correct predictions while penalizing temporally wrong ones.</sample>
    <sample id="125">Il numero di autori coinvolti nell'articolo non è specificato nella presentazione.</sample>
    <sample id="126">No, non è stato considerato come un approccio standard.</sample>
    <sample id="127">The paper presents a method for transferring reasoning abilities from large language models to smaller models using chain-of-thought prompting and diverse reasoning techniques. The authors propose using large models as "reasoning teachers" to generate step-by-step solutions for complex tasks, which are then used as training data for smaller models. They compare their method with existing baselines and show that it achieves notable performance improvements on various tasks, especially text-based ones. The method is highly scalable but involves development and inference time costs, which need to be considered when applying it in real-world scenarios.</sample>
    <sample id="128">The paper presents a diagnostic test suite for knowledge integration in natural language understanding (NLU) models. It introduces a coreference resolution task designed to evaluate the ability of models to draw on knowledge available in different sources. The authors evaluate the dataset with human study participants and establish coreference resolution models. They define three settings of KIMMO: background pretrain, background both, and background inference. The results show that even the best-performing models cannot reliably integrate background knowledge presented only at inference time.</sample>
    <sample id="129">Gli autori hanno fornito un esempio di gruppo contrassegnato come "Black woman" e hanno analizzato le parole che definiscono questa gruppo, come culture, tradizioni, orgoglio e exotici.</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono le modeste, piccole e i modelli con pochi esempi di finetuning.</sample>
    <sample id="131">I nomi dei set di dati di test non sono forniti nel testo.</sample>
    <sample id="132">Il numero di autori coinvolti nell'articolo è due.</sample>
    <sample id="133">L'autore opera con più modalità.</sample>
    <sample id="135">The paper presents a new method for evaluating conversational AI models called ABC-Eval. It aims to reduce the subjectivity of human evaluation by explicitly annotating whether or not each model response expresses certain behaviors such as responding with irrelevant information or contradicting itself. The authors developed this method to comprehensively cover chat model behaviors that have been suggested to affect chat quality in recent literature. They evaluated four state-of-the-art chat models on 100 human-bot conversations per model using ABC-Eval and compared it to three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and dialogue-level pairwise comparisons. They found that ABC-Eval labels are overall more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. Additionally, ABC-Eval labels are more predictive of the overall conversation quality compared to metrics produced by existing methods, as shown by this simple linear regression analysis. Finally, they checked whether each evaluation metric captures a unique aspect of chat quality using a stepwise linear regression. They found that the combination of all ABC-Eval metrics explains over 25% of conversation quality and as you remove the metrics one at a time, most of them result in losing a decent amount of information about the quality. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="136">The speaker, Jazan Alvan, presents his work on numerical reasoning conducted with his supervisor Nefisa at the University of Sheffield. He introduces a new evaluation set called Fermat to assess models' mathematical abilities, including number understanding, mathematical operations, and training dependency. The baseline evaluation shows most models perform poorly, but fine-tuning improves performance. Training diversity and language diversity also enhance model performance. The existing benchmarks are unrepresentative, and Fermat aims to fill that gap.</sample>
    <sample id="137">The paper introduces Tell to Design, a large-scale dataset featuring floor plans with natural language instructions to describe user preferences. The authors propose a sequence-to-sequence model for generating floor plans from language instructions and compare it with text conditional image generation models. They demonstrate that their method outperforms other text conditional image generation baselines by a large margin, achieving the highest F1 score of 54 and a macro F1 score of 53. The results suggest that despite a language distribution gap between artificial and human instructions, artificial instructions can be used for warm-up before training on human instructions, significantly improving performance.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare conoscenza proveniente da diverse fonti.</sample>
    <sample id="139">I nomi dei relatori sono E and J.</sample>
    <sample id="140">Sì, Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti in quanto supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto solitamente si basano su conoscenze dominanti e curazione umana.</sample>
    <sample id="142">Ciao, sto per parlare del mio lavoro su risoluzione di espressioni indirette per la selezione di entità. In questo lavoro, introduciamo il concetto di entità correlate. Il mio nome è Javahar Hosaini e questo è un lavoro con Philip Radlinski, Sylvia Parati e Annie Louise. Il nostro obiettivo è comprendere il linguaggio dei utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai mai sentito parlare di 'Easy on Me' o 'I Got a Feeling'?". Qui, il utente vuole scegliere tra due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio, dicendo il nome della canzone "Easy on Me" o la posizione, la prima. Ma spesso un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può succedere quando il utente non ricorda il nome della canzone o le pronunciazioni sono troppo simili tra loro e difficile da distinguere. O quando il utente vuole specificare un preferimento. Ecco alcuni esempi di riferimenti indiretti: "La nuova una" o "La canzone che non è energica". Questo è un problema importante in sistemi di conversazione e anche per valutare l'entità di un linguaggio automatico (LLM). Non conosciamo un pubblico dataset di grande scala per questo compito, quindi stiamo costruendone uno utilizzando la traduzione. Il nostro dataset copre tre domini diversi: musica, libri e ricette. Il nostro metodo di raccolta dei dati si basa sull'informalità utilizzando un set di completazione di cartoline. La cartolina ha tre bubble di discorso. In primo luogo, Bob dice: "Ricorda quella canzone che stavi ascoltando ieri?". E con quello, Bob stabilisce il contesto del discorso. In questo, in questo secondo bubble di discorso, Alice dice: "Hai mai sentito parlare di 'Easy on Me' o 'I Got a Feeling'?". Questa è la domanda alternativa. E in questo terzo bubble di discorso, Bob usa un riferimento indiretto per scegliere tra queste entità. Ad esempio, "La nuova una". Noi forniamo le prime due bubble di discorso automaticamente, ma la terza è compilata dallo annotatore. Il primo bubble di discorso è scelto da pochi promp manuali per dominio. Il secondo, che è la domanda alternativa, è generato come segue: utilizziamo un semplice modello: "Hai mai sentito parlare di A o B?" dove A e B sono campioni da Wikipedia. Ecco i vari metodi di campionatura che utilizziamo. Quanto più avanziamo nella lista, le entità diventano più simili tra loro e diventa più difficile distinguere tra loro. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia e infine, quando hanno attributi simili o infobox su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Ci sono anche altre informazioni che mostriamo ai annotatori. Per le canzoni, mostriamo un link di ricerca su Google e chiediamo ai annotatori di ascoltare almeno un pezzo di ciascuna e leggere di più. Per le ricette e i libri, mostriamo testi di Wikipedia e immagini di Wikipedia. Poi chiediamo ai annotatori di scegliere una di queste entità, ad esempio, la prima, e descriverla usando tre a cinque espressioni indirette. Ad esempio, "La canzone con la music box". Ecco alcuni esempi dal nostro dataset: "La canzone senza parole", non la canzone con il 12-year-old 12-year-old boy o la canzone fictionale o quella che viene dalla Svizzera o da Albergo John e così via. L'altre entità ha 6.000 domande alternative attraverso i tre domini e ha 42.000 espressioni indirette. I risultati con il modello T5-xl sono riassunti di seguito. Se il modello ha accesso alle medesime informazioni dietro le entità che gli annotatori hanno, allora la precisione è davvero alta, circa il 92% o il 95%. Ma non è realistico. Se il modello ha accesso a informazioni parzialmente sovrapposte, allora la precisione è tra il 82% e il 87%, che è più realistico. Ad esempio, quando il modello recupera le informazioni. Se il modello ha accesso solo ai nomi delle entità, allora la precisione è solo del 60%. Così ci sono molte opportunità di miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Qui c'è un link verso il dataset. Grazie.</sample>
    <sample id="143">L'approccio SimulST viene confrontato con le politiche esistenti di encoder-decoder attention, Whitkey strategy e local alignment.</sample>
    <sample id="144">I ricercatori dell'articolo sono affiliati all'INR (Institut national de la santé et de la recherche médicale) e all'INR-ICM (Institut national de la santé et de la recherche médicale-Institut Cochin).</sample>
    <sample id="145">La relatrice o il relatore del discorso è Jenny.</sample>
    <sample id="146">The speaker, a PhD student from Fudan University, introduces a paper on the analysis of omission in dialog summarization. The paper highlights the challenges in dialog summarization, particularly the issue of omission, which leads to incomplete summaries and loss of critical information. The author discusses the development of a dataset for detecting omissions in dialog summarization and evaluates different model architectures using precision and recall scores. The results show that omission detection is a challenging task, but the inclusion of omission content can improve summary quality.</sample>
    <sample id="147">Il numero di autori coinvolti nell'articolo è tre.</sample>
    <sample id="148">Ciao, mi chiamo Sarah Papi da Università di Torino e Fondazione Bruno Kessler. E io presenterò brevemente un articolo sulla traduzione simultanea in tempo reale come guida. Questo è un lavoro con Mateo Negri e Marco Turky. Cos'è la traduzione simultanea? La traduzione simultanea o Simultaneous Speech Translation (SST) è il processo di tradurre un linguaggio parlato in testo in un altro linguaggio in tempo reale, rendendo la comunicazione tra lingue più facile. Ma cosa sono i problemi dei modelli di traduzione contemporanea? I modelli di traduzione contemporanea spesso richiedono architettura specifica e moduli aggiuntivi per essere ottimizzati, addestrati in modo lungo e complicato, addestrati con diverse obiettivi di ottimizzazione e addestrati e mantenuti per raggiungere differenti regimi di latenza, ad esempio, addestrare un modello con un'average latency di 1 secondo e un modello con 2 secondi di latenza, eccetera. Quindi, cosa è la nostra soluzione? La nostra soluzione è di usare modelli offline esistenti senza retrain o adottare specifiche architettura per Simultaneous Speech Translation. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici e utilizziamo la conoscenza acquisita dal modello attraverso meccanismi di attenzione tra input audio e output testo. Un esempio è sul lato destro. La nostra soluzione propone adat o encoder-decoder attention e è una strategia per decidere se emettere o non emettere una traduzione parziale basata su dove l'attenzione punta. Una parola è emessa se l'attenzione non è concentrata, ovvero se la somma è inferiore a un certo livello di soglia alpha verso le ultime lambda frame di parola ricevute, ovvero se l'informazione ricevuta è abbastanza stabile. Ad esempio, se riceviamo un frammento di discorso che dice "I'm going to talk about" e il modello predice la traduzione in tedesco, analizzeremo i pesi di attenzione tra i primi due parole e le ultime parole. Se la somma dei pesi di attenzione è inferiore a un certo livello di soglia alpha, non emetteremo la ultima parola e aspetteremo un nuovo frammento di discorso. Se riceviamo un nuovo frammento di discorso e il modello predice altre tre parole, analizzeremo i pesi di attenzione tra queste parole e se nessuna parola punta alle ultime lambda frame di parola ricevute, queste tre parole verranno emesse. Se guardiamo i risultati dell'ADT, plottiamo i risultati di traduzione contemporanea in tempo reale su grafici in cui il blu rappresenta la qualità della traduzione e l'average latency, che è misura della latenza, e consideriamo anche la media di tempo computazionale, che tiene conto del tempo computazionale del modello per produrre l'output. Ci diamo da fare per ottenere curve più elevate e spostate verso sinistra, confrontandole con altre strategie applicate a modelli offline, come la strategia Whitaker e la strategia locale, e con modelli specificamente adattati per la traduzione contemporanea. Questi sono i risultati della nostra strategia di traduzione contemporanea in tempo reale in tedesco. Vediamo che adat supera tutti i strategie applicate a modelli offline, poiché le curve sono spostate verso sinistra. Inoltre, se consideriamo il tempo di accesso reale o il tempo computazionale medio, adat è la strategia più veloce. Se vuoi scoprire maggiori risultati, leggi il nostro articolo e scarica anche il codice e i modelli per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="149">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="150">Meeting Q&amp;A is a new dataset for extractive question answering that focuses on questions asked by participants in meetings and their corresponding answer sentences. It contains 7,700 questions from the AMI Corpus, with 30% unanswerable, 40% multi-span answers, and 48% multi-speaker answers. The dataset includes various question types and has a high inter-annotator agreement (Cohen's kappa of 0.73). Models outperform human performance in fine-tuned settings, but struggle with zero-shot performance, especially with rhetorical questions and identifying which speaker answered a question. Silver data augmentation improves zero-shot performance, and larger instruction-tuned models like BLIP-5 achieve comparable results to smaller models.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Yi e il mio collega Jia Yang e io presenteremo il nostro ricerchiamo su Multi-Instruct, che è un impianto di apprendimento a lungo termine per i modelli multi-modal che utilizza l'addestramento con istruzioni. Con le avanzate in campo dei modelli a lungo termine, molte ricerche hanno iniziato a esplorare nuovi paradigmi di addestramento che utilizzano modelli pre-addestrati per diverse attività in un modo più efficiente e concettuale. Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli a lungo termine di svolgersi in modo semistrutturato su compiti linguistici, seguendo istruzioni naturali. Tuttavia, la maggior parte delle precedenti ricerche sull'addestramento con istruzioni si concentra su migliorare le prestazioni in modo semistrutturato per compiti linguistici, mentre i compiti di visione computerizzata e i compiti multi-modal sono stati lasciati fuori. Di conseguenza, in questo lavoro ci piacerebbe indagare se l'addestramento con istruzioni su modelli pre-addestrati multi-modal può davvero migliorare la generalizzabilità su compiti multi-modal. Inoltre, alla nostra ricerchiamo si è scoperto un considerevole squilibrio nella disponibilità di dataset di istruzioni tra NLP e multi-modal. Esistono più di 1600 compiti linguistici ad istruzione, tuttavia non esiste alcun dataset di istruzioni multi-modal pubblicamente disponibile a larga scala. Questo ha motivato la nostra ricerchiamo a creare un dataset di istruzioni multi-modal. Qui presentiamo Multi-Instruct, il primo benchmark di dataset di istruzioni multi-modal che comprende 62 diverse attività multi-modal, coprendo 10 category diverse. Queste attività sono derivate da 21 dataset di risorsa aperta esistenti e ogni attività è equipaggiata con cinque istruzioni elaborate da esperti. Per indagare l'addestramento con istruzioni multi-modal sul nostro dataset proposto, utilizziamo OFA, un modello pre-addestrato multi-modal unificato, come nostra base di modello. OFA utilizza un univoco vocabolario per i token di lingua, immagine e coordinate di bounding box. Ecco alcuni esempi di istanze dal nostro dataset Multi-Instruct. Per unificare la trattazione di vari tipi di input e output diversi, seguiamo il modello di OFA e rappresentiamo tutte le attività in un formato di sequenza a sequenza unificato, in cui i testi immagine, istruzioni e bounding box vengono rappresentati nello spazio dei token. Adesso parliamo di addestramento con istruzioni multi-modal. Per il dataset di addestramento, utilizziamo 53 attività da NLP per addestrare e scegliamo 10.000 istanze per attività. Per le prove, riserviamo tutto il gruppo di ragione comune per le prove e scegliamo ulteriormente cinque attività da Wiki e MScenius. Utilizziamo tutte le istanze del set di prove per ciascuna attività. Inoltre, scegliamo casualmente 20 attività dal set di prove di NLP instruction come task per NLP. Utilizziamo il modello pre-addestrato OFA large come nostra base di modello. Durante l'addestramento, combiniamo casualmente ogni istanza per ogni attività con uno dei cinque template di istruzioni. Durante le prove per ogni attività, eseguiamo un totale di cinque esperimenti, valutando il modello utilizzando una delle cinque istruzioni in ciascun esperimento. Rapportiamo il minimo e massimo prestazione e la deviazione standard della prestazione attraverso di cinque esperimenti. Se la attività è un compito di classificazione multi-modal, rapportiamo l'accuratezza. Se è un compito di generazione multi-modal, rapportiamo ROUGE-L. Per i compiti NLP, rapportiamo ROUGE-L allo stesso modo. Abbiamo anche introdotto un nuovo metrico chiamato sensibilità. Questo misura la capacità del modello di produrre gli stessi output per lo stesso compito, indipendentemente da eventuali variazioni nella formulazione dell'istruzione. Ecco i nostri principali risultati: come possiamo vedere, l'addestramento con istruzioni può migliorare significativamente le prestazioni di OFA su compiti multi-modal. Inoltre, il trasferimento di imparare da dataset di istruzioni può migliorare l'addestramento con istruzioni. Come possiamo vedere, con l'aumento del numero di attività, il modello raggiunge prestazioni migliori e allo stesso tempo una sensibilità inferiore. Abbiamo anche eseguito un esperimento utilizzando un istruzione invece di cinque istruzioni. Come possiamo vedere, utilizzare una istruzione può migliorare l'overall performance del modello e ridurre la sensibilità molto. Questo dimostra l'effetto di diverse strategie di fin-tuning sul modello sensibilità. Come possiamo vedere, il trasferimento di imparare da dataset di istruzioni consente al modello di raggiungere prestazioni sensibilità molto migliori rispetto all'originale OFA modello. Abbiamo anche potuto vedere che il trasferimento di imparare da dataset di istruzioni consente a OFA di raggiungere prestazioni molto migliori sul dataset di istruzioni NLP. Di conseguenza, proponiamo il primo dataset di istruzioni multi-modal a larga scala, che ha notevolmente migliorato la capacità di addestramento a lungo termine di OFA e ha esplorato diverse tecniche di trasferimento di imparare e ha dimostrato i benefici. Abbiamo progettato un nuovo metrico chiamato sensibilità. Altra cosa, stiamo raccolgendo un dataset di istruzioni multi-modal a larga scala con circa 150 attività linguistiche aggiuntive e lo rilanceremo presto. Questo è il link per i nostri dataset e modello. Grazie.</sample>
    <sample id="152">The presentation introduces valuable resources for ancient Greek and Latin, exploring the implications and challenges of multilinguality in these models. The speaker has pre-trained two monolingual models for ancient Greek, GEBerta and Greta, and developed multilingual models, Philberta and Philter, pre-trained on ancient Greek, Latin, and English data. The speaker also analyzed how T5's encoder behaves and investigated the implications of multilinguality in their language models.</sample>
    <sample id="153">The paper presents a framework for resolving ambiguities in text-to-image generative models. The authors propose two approaches: one uses a language model to generate clarifying questions, and the other generates different possible visual interpretations. Users provide answers based on their intended interpretation, resulting in disambiguated prompts. The authors evaluate the faithfulness of generated images to user intentions using a VQA model. They show that their framework has a positive effect on faithful generation and is in agreement with human evaluation.</sample>
    <sample id="154">I fornitori di SARA PAPI sono la Università di Torino e Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice o del relatore è Javahar Hosaini.</sample>
    <sample id="157">The paper presents a dialogue summarization model that combines static and dynamic graph structures to capture the semantic relationships between utterances in a dialogue context. The model first uses an utterance encoder to encode the utterance into a vector representation, then constructs a static graph using existing dialogue structure modeling methods. A dynamic graph module is proposed to capture the semantic relationship between utterances based on their deep vector representation. Finally, a pre-trained language model is used as the summary generator to fuse the static dialogue structure and the dynamic learned dialogue structure into the final summary. The model is evaluated on the DSTC 5 dataset and achieves state-of-the-art performance.</sample>
    <sample id="158">The speaker introduces a new method for the coreference resolution task, which involves identifying and clustering mentions of the same entity in a document. The proposed method uses a fixed-size cache to reduce complexity, but this can lead to high cache misses in long documents with frequent topic switches. To address this, the speaker proposes a dual cache that stores local entities with an eviction policy and global entities with an LFU policy. The dual cache significantly reduces cache misses and outperforms single cache methods, even without training data. The model is evaluated on four public benchmarks and performs better than baseline models, even when using unbounded memory. Overall, the dual cache is more cost-effective and efficient compared to single cache methods.</sample>
    <sample id="159">Ciao a tutti, mi chiamo Kostya Tszyu e sono felice di benvenuti in questo talk sul mio articolo AC 2023: i giudizi sulla accettabilità dei modelli linguistici non sono sempre robusti al contesto. Questo è un lavoro di gruppo con John Goldacre, Aaron Mueller, Kanishka Mishra, Garen Fuentes, Roger Levy e Adina Villamoz. In questo lavoro, ci concentriamo sul paradigma del minimo paragrafo. Il paradigma del minimo paragrafo valuta i modelli linguistici su base di giudizi sulla accettabilità, che possono anche includere la grammaticalità, come "blimp" o "syntactic jim", o accettabilità in termini di stereotipi, come "crowsears". In questo paradigma, il modo tipico per valutare i modelli linguistici è quello di mostri un sentenzia accettabile o grammaticalmente corretto e uno non accettabile o grammaticalmente sbagliato. L'obiettivo del modello è quindi attribuire più probabilità all'uso di una sentenza accettabile. La nostra pipeline MPP attuale non ci permette di valutare la capacità di accettazione dei modelli per le sentenze più lunghe. Questi giorni, i modelli linguistici emergenti stanno emergendo con finestre di contesto più lunghe. Ciò rende cruciale valutare la capacità di accettazione dei modelli attraverso tutto il contesto. Ecco cosa stiamo cercando di fare qui: stiamo cercando di rivedere la pipeline MPP per far sì che i modelli valutino l'acceptabilità su lunghe sequenze. Così, ecco il nostro approccio: quello che facciamo è simulare queste lunghe sequenze, redefinendo i dataset stessi e quindi ricreati i sentieri scegliendo sentenze accettabili o non accettabili da quei dataset. Ad esempio, qui abbiamo scelto un paio tipici di grammaticalità dal dataset "blimp" dell'isola di Saint John e, come ho detto, ricreato queste lunghe sequenze accettabili e non accettabili, utilizzando la grammaticalità estratta dal dataset Saint John come prefisso per entrambe le query accettabili e non accettabili. Possiamo fare lo stesso per le query non accettabili scelendo sentenze non accettabili dal medesimo matching. Possiamo anche fare lo stesso per scelere sentenze da un set diverso o da un altro dataset. Quello che chiamiamo "scenari di mismatch" è quando le sentenze continuano a venire da set di rilevanza ma non da quelli che stiamo valutando. Possiamo fare lo stesso per le query non accettabili. Infine, possiamo scegliere sentenze da un dominio completamente non rilevante, come Wikipedia. Questo ci dirà se i giudizi sulla accettabilità dei modelli sono influenzati da contesti, se i contesti vengono da un subset diverso del dataset o se sono completamente irrelevanti rispetto alla query che stiamo analizzando. Come funziona il modello? Innanzitutto guardiamo le sentenze di Wikipedia che sono completamente irrelevanti per la query attuale. Qui vediamo che i giudizi MPP sono relativamente stabili, anche se aumentiamo la lunghezza del contesto fino a 1244 per massimizzare i modelli OTP e GP T2. Ora, cosa succede quando scegliamo sentenze dal medesimo dataset? Qui stiamo creando sentenze accettabili e non accettabili dal medesimo dataset "blimp" o "syntactic jim" e vediamo che i giudizi MPP aumentano o diminuiscono notevolmente quando aggiungiamo prefissi accettabili o non accettabili. Ma quando scegliamo strutture, cioè quando scegliamo sentenze dal medesimo fenomeno in "blimp" o "syntactic jim", vediamo un'enorme crescita o una drastica diminuzione dei giudizi MPP, a seconda di quanto il prefisso scelto sia accettabile o non accettabile. Questo effetto aumenta attraverso la lunghezza del contesto e potrebbe influenzare modelli più recenti con finestre di contesto più lunghe. Perché queste prefissi influenzano così tanto i giudizi dei modelli? Abbiamo fatto una serie di analisi in cui cercavamo di alterare la sentenza di input cercando di preservare la struttura rilevante ma aggiungendo rumore all'input. Dopo aver fatto questa serie di alterazioni, abbiamo scoperto che nessuno di questi rumori ha davvero fatto variare il modello in termini di come ha espresso i giudizi MPP. Di fatto, abbiamo scoperto che i modelli sono sensibili ai disturbi nella sentenza in modo simile: quando perturbiamo sentenze accettabili, vediamo un aumento simile in tutte le alterazioni e quando perturbiamo sentenze non accettabili, vediamo una diminuzione simile dei giudizi MPP. Quindi, i principali punti di rilievo del nostro lavoro è che i modelli linguistici sono sensibili a latenze sintattiche e semantiche che si condividono tra le sentenze e che la nostra valutazione attuale, che utilizza input brevi e singoli, non può affatto catturare completamente la conoscenza astratta dei modelli attraverso tutto il contesto. Per vedere i dettagli dei nostri esperimenti, ti prego di leggere il mio articolo. Grazie per l'attenzione!</sample>
    <sample id="160">In un token multi-settato.</sample>
    <sample id="161">In Coscript, rappresentano 55.000 script.</sample>
    <sample id="163">Il miglior metodo di allineamento automatico per DEplain è il metodo di mass alline.</sample>
    <sample id="164">L'apprendimento scarsamente supervisionato è più economico rispetto all'annotazione umana.</sample>
    <sample id="165">The paper presents a new method for adaptive reasoning, called LIPOR (Likelihood Learning with Posterior Regularization), which does not require supervision regarding the plausibility of explanations. The method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. An additional regularizer is used to enforce mutual exclusivity among explanations. The paper compares LIPOR to several zero-shot models and the previous best unsupervised approach on the Alpha-ALI dataset, achieving superior performance by over four absolute points in accuracy.</sample>
    <sample id="166">The proposed method, which combines the advantages of both System 1 and System 2, is introduced to address the issue of visual-language models' performance drop when confronted with complex text. The first module, a symbol generator, represents complex propositions through symbols and images, while the second module, a neural symbolic reasoner, integrates reasoning states and results from symbol representations to obtain the final solution. The whole system utilizes the advantages of both analogical inference and logical reasoning systems. Experimental results show that the proposed method outperforms the baseline methods and verifies the effectiveness of each module.</sample>
    <sample id="167">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici. In particolare, i documenti in DEplain-APL sono stati allineati in 483 documenti in modo manuale, mentre i documenti in DEplain-Web hanno incluso 750 documenti allineati in modo manuale e in modo automatico.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo articoli da Reuters News del 2020 e annotandoli con le stesse guidelines di annotazione di CoNLL 2003.</sample>
    <sample id="169">The paper presents a systematic study on large language model prompting for machine translation. The authors evaluate the translation capability of such models using the best practices of the NMT community, comparing two state-of-the-art systems and providing recommendations for prompt selection strategies. The results show that example quality is more important than similarity to the source sentence, and that specialized state-of-the-art systems have an advantage over PAM translations in terms of accuracy, although PAM's fluency is comparable to state-of-the-art systems.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Yusen Zhang dalla Università di Peking. Oggi presenterò il mio lavoro: "Esempio: Analisi del parsing semantico in linguaggi naturali multipli e rappresentazioni minime". Il parsing semantico è un compito che consiste nel costruire rappresentazioni semantiche di query dell'utente, come SQL e Lambda Calculus. Il parsing semantico transversale è il compito di tradurre query in linguaggi naturali multipli in rappresentazioni minime multiple. Come illustrato in questa figura, dobbiamo tradurre la query in vari linguaggi naturali utilizzando modelli neurali per tradurre in SQL, Lambda o FunQL e altro. I modelli di parsing semantico transversale esistenti sono proposti e valutati separatamente su dataset di limitati test e applicazioni. Ci sono lacune di copertura su certi linguaggi naturali, ad esempio il cinese è mancante, e lacune di copertura su certe rappresentazioni, ad esempio il calcolo Lambda è mancante, o sono valutati solo su certi modelli neurali. Per risolvere questo problema, proponiamo Esempio: fornire un dataset unificato per il parsing semantico transversale in linguaggi naturali multipli e rappresentazioni minime. Questo dataset include 9 dataset in diverse domande, 5 task di parsing semantico, 8 rappresentazioni minime e 22 lingue in 15 famiglie linguistiche. Per valutare meglio il benchmark, consideriamo 6 impostazioni per training e valutazione. La prima è la traduzione test: utilizziamo l'API di traduzione Google per tradurre le query di origine verso il linguaggio di destinazione, quindi utilizziamo modelli monolingue per il training e l'evaluation. Ad esempio, trainiamo un modello in inglese su query in inglese e durante l'inferenza traduciamo la query in tedesco utilizzando l'API in inglese e utilizziamo il modello addestrato per prevedere il SQL. Inoltre, testiamo anche i modelli monolingue in impostazione monolingue: il linguaggio di origine è lo stesso del linguaggio di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche la impostazione monolingue fusa: addestra modelli monolingue con solo il 10% dei dati di training e testa modelli monolingue multi-lingue: addestra un modello multi-lingue per tutti i linguaggi e utilizza il modello per tradurre query in tedesco o cinese, eccetera. Infine, consideriamo anche la traduzione zero-shot e fusa: addestra su un linguaggio di origine e trasferisci su un altro linguaggio. Durante l'addestramento, addestra su query in inglese o la combinazione di query in inglese e tedesco fusa per addestrare un modello multi-lingue e prevedere il SQL output. Troviamo molti risultati interessanti. In analisi dei modelli monolingue, valutiamo due gruppi di modelli: encoder PDR, che sta per encoder pre-addestrato multilingue con decoder basato su pointer, come XLNet + PDR e BERT + PDR; e encoder-decoder modelli, che sono modelli encoder-decoder pre-addestrati multilingue, come BART e MT5. Troviamo che encoder-decoder ottiene il migliore prestiglio su tutti i 9 dataset. Valutiamo anche MT5 e XLM-R + PDR su impostazione multilingue: encoder-decoder o encoder PDR possono essere migliorati addestrando in una combinazione di vari linguaggi. Troviamo che quasi tutti i principali linguaggi naturali possono ottenere un miglioramento di prestabilito, tranne inglese che diminuisce in 7 dataset e guadagna solo in 3 dataset. Questo è noto come curva del multilinguismo. Inoltre, confrontiamo il gap di prestabilito tra linguaggi transversali: la linea blu rappresenta la traduzione zero-shot, la linea rossa rappresenta la traduzione fusa zero-shot, e la linea verde rappresenta la impostazione monolingue. Troviamo che confrontando la linea verde e la linea rossa, notiamo che la traduzione zero-shot ha un gap di prestabilito significativo. Confrontando la linea blu e la linea rossa, notiamo che la traduzione fusa ha rapidamente ridotto il gap di prestabilito. Troviamo anche altri interessanti findings, ad esempio encoder-decoder supera precedenti work o raggiunge risultati comparabili addestrando su linguaggi naturali inglese e notoriamente migliorando i performance fusa su linguaggi di destinazione naturali. Troviamo anche modelli linguaggi multi-lingue come Codas e Blue che sono insufficienti per task di parsing semantico transversale. In sintesi, presentiamo Esempio: un benchmark unificato per il parsing semantico transversale in linguaggi naturali multipli e rappresentazioni minime. Condurranno uno studio di benchmarking esaustivo su tre tipi rappresentativi di modelli linguaggi multi-lingue e i nostri risultati dimostrano molti findings interessanti e altro. Benvenuti a visitare il mio articolo e il codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori connessi in tal senso sono quelli che non sono applicabili a embedding as services o mancano di trasferibilità.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP.</sample>
    <sample id="174">The Arg Analysis 35K dataset is a large-scale dataset for argument quality analysis, consisting of 55,000 argument analysis pairs sourced from speeches, expert debaters, intermediate debaters, and novice debaters. The dataset has higher quality arguments, a diverse range of arguments, and includes an element of analysis instead of just keeping arguments. It also introduces the idea of instance-based annotator reliability and relevance modeling to better capture how relevant each argument is to a particular theme.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni inducendo l'alleanza tra input e output come parte della formazione.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita come la capacità del modello di rilevare e gestire le informazioni negative o false che vengono dirette verso gruppi specifici, indipendentemente dalla loro rappresentazione politica o demografica.</sample>
    <sample id="177">Il nome della relatrice o del relatore è Yanis Slavac.</sample>
    <sample id="178">La relatrice o il relatore è Kostya Sina.</sample>
    <sample id="179">The paper introduces Symbolic Tom, a plug-and-play multi-character belief tracker for large language models (LLMs). It presents an inference-time method using explicit graphical representations to improve theory of mind reasoning skills in LLMs. The method computes graphical representations of characters' mental states and beliefs, allowing the model to answer complex questions about other characters' mental states. The research compares the performance of Symbolic Tom with supervised baselines on various datasets, demonstrating significant improvements in out-of-box LLM performance and robustness to linguistic generalization.</sample>
    <sample id="180">La relatrice o il relatore è Myra.</sample>
    <sample id="181">The paper introduces a method for generating high-quality scripts for constrained language planning using large language models. The authors define the problem of constrained language planning, which involves imposing different constraints on goal planning, and evaluate the performance of large language models in generating scripts that are reasonable and faithful to these constraints. They propose an over-generated script filter to improve generation quality and develop a dataset of constrained language planning named CoScript, which includes 55,000 specific goals with scripts. The authors also explore the use of symbolic knowledge distillation to distill constrained language planning datasets from large language models and demonstrate that smaller but specialized models can generate higher-quality scripts than most large language models when trained on suitable datasets.</sample>
    <sample id="182">Il tropicalismo si riferisce a un stereotipo che rappresenta le donne di colore come "vibranti" e "curvaceous", connettendole a una storia lunga di essere rappresentate come sessualizzate e docili.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando modelli di linguaggio e promempi.</sample>
    <sample id="184">In questo lavoro è stato utilizzato un misuratore chiamato CXMI per misurare l'utilizzo del contesto.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato per la medicina e il clinico, mentre ChuBERT è un modello basato su annotazioni.</sample>
    <sample id="187">Ci sono due autori che hanno partecipato all'articolo.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene addestrato su un insieme di dati raccolti in precedenza e successivamente aggiornato con nuovi dati raccolti in ogni round di apprendimento attivo.</sample>
    <sample id="189">L'obiettivo del set di dati è quello di capire il linguaggio dell'utente quando vuole fare una scelta.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando la tecnica di watermarking. Questa tecnica consiste nell'iniettare un segno nascosto (il watermark) nel servizio EaaS e nel rilevare se un altro servizio contiene il segno nascosto. Se il segno nascosto è presente, allora il servizio EaaS è stato utilizzato per estrarre i parametri del modello.</sample>
    <sample id="191">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="192">The presentation introduces a new optimizer called Ken, which aims to achieve both fast convergence and low memory usage in the training of large language models. It presents a confidence-guided adaptive memory-efficient optimization method that uses non-negative matrix factorization (NMF) to reduce memory requirements while maintaining performance. The presenter compares Ken with existing optimizers like Adam and AdaFactor, showing that Ken achieves better performance with less memory cost. The results are demonstrated on the book corpus and English Wikipedia, and the proposed optimizer is shown to be effective for both training tasks and large batch training.</sample>
    <sample id="193">Per creare il set di dati iniziale, sono stati impiegati circa 1000 esempi di unità discorsette.</sample>
    <sample id="194">I ricercatori dell'articolo sono studenti di primo anno di laurea in Scienze della salute alla Carnegie Mellon University e collaboratori dell'University of Washington e dell'Alliance for AI.</sample>
    <sample id="195">The paper presents a new framework for answering complex questions using a hierarchical question decomposition tree. The framework consists of two stages: building the hierarchical question decomposition tree and probabilistic reasoning over it. The first stage involves generating leaf nodes, which are atomic questions that cannot be further decomposed, and then generating intermediate questions based on the grouped leaf questions. The second stage involves conducting probabilistic reasoning over the hierarchical question decomposition tree by selecting appropriate knowledge sources, executing queries, and aggregating candidate answers to output the top-k answers with highest probabilities. The framework is evaluated on two challenging complex QA datasets, KQAPRO and Music, and demonstrates significant improvement over existing methods in terms of performance and robustness.</sample>
    <sample id="196">Un esempio in cui il governatore è a sinistra è "Lisa bought and Megan".</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli che hanno errori di senso comune in circa il 20% delle loro risposte, producono informazioni irrelevanti in circa il 15% delle risposte e si contraddanno o si riconoscono come partner circa il 10% del tempo.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli di lingua moderni stanno emergendo con finestre di contesto più lunghe. Questo rende cruciale valutare l'accettabilità dei modelli attraverso l'intera finestra di contesto.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset.</sample>
    <sample id="200">No, gli annotatori non conoscono l'entità in anticipo.</sample>
    <sample id="201">Per la valutazione sono state utilizzate le metriche di MT più avanzate e anche i risultati umani per la valutazione.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i modelli e i set di dati possono rappresentare certe posizionalità in modo scorso, lasciando indietro alcune persone.</sample>
    <sample id="204">I modelli LLM multilingue come BLOOM sono stati adattati utilizzando adattatori o con una messa a punto integrale.</sample>
    <sample id="205">The presentation discusses the political biases in language models and their impact on downstream tasks. The researchers investigate how language models learn from diverse perspectives, which can lead to social biases, and how these biases affect performance in NLP applications. They propose evaluating political leanings of language models using political questionnaires and conducting controlled experiments with pre-trained language models on partisan corpora. Preliminary results show that language models have varying political leanings and can pick up polarization from training data. The presentation also highlights fairness issues in hate speech and fake news detection, where language models with different political leanings perform differently based on the demographics or political leanings of the news media. The researchers conclude that acknowledging and tackling fairness issues resulting from language model political leanings is crucial.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, si fanno ricorso al modello C-E.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i test più recenti, che si sono evoluti per evitare l'uso dei dati di addestramento con i dati di test.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il nuovo metodo proposto ha un guadagno di 100% rispetto al metodo di riferimento.</sample>
    <sample id="210">Il nome della relatrice o del relatore è Zhu Heng.</sample>
    <sample id="211">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nell'articolo, si utilizzano due modelli più piccoli: T5 e FunTune.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">The paper presents an argument for the symmetric structures of coordination, which are preferred over asymmetric structures. The argument is based on the principle of dependency length minimization, which states that shorter dependencies are preferred. The paper also shows that left conjuncts tend to be shorter and that this tendency grows with the length difference between the two conjuncts. However, when there is no external governor on the left, the left conjunct prefers to be shorter. This effect disappears when the governor is on the right. The paper provides statistics from the enhanced version of the Penn Treebank to support these observations.</sample>
    <sample id="217">The paper presents a method for generating controllable dialogues with multiple attributes using a disentangled control generation (DCG) approach. The authors propose a dialogue GBT framework with a compositional prompt module that uses two types of prompts: attribute-oriented and task-oriented, to guide the model's focus on specific information in the dialogue. They also introduce a unified reference-free evaluation framework (MAE) that does not require large-scale labeled data and demonstrate the effectiveness of their method through experiments. The results show that the proposed method outperforms other baselines in controllability and test quality, and achieves a small drop on EASCC and AASCC metrics while successfully addressing the challenge of compositional generation for multi-attribute controllable dialogue generation.</sample>
    <sample id="218">I ricercatori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">The research assistant, Jia Hu, and his team presented their work on a multi-stage pipeline for uncovering financial signals in financial reports. They focused on the annual report required by SC, which contains many details of companies' important activities. However, mining useful information requires lots of human efforts. The work was motivated by two observations: first, they observed that the words in a company's report are very similar, about 80% of tokens are the same, and the contents are largely dependent. This figure illustrates the text similarity between two reports in the continuous years. For example, the report in 2018 is similar to the one in 2017. Based on the observation, they introduced a highlighting task and a multi-stage pipeline. They first defined the reference to target structures in their task. The target and reference refer to the report of our interest and the report at its previous year. So basically, a highlighting model should compare and contrast the context between targets and reference, like this figure. So the goal of this highlighting task is to find the proportionality roots between a given pair T and R. Formally, the model will predict the high the word importance, and therefore we can measure the performance of highlighting. For example, the word decrease is supposed to have higher importance in this context. This is our proposed pipeline. Stage 0 is document segmentation. Stage 1 is the relation classification. Stage 2 and stage 2 plus are our main and incremental fine-tuning. Due to the time limit, I would not talk about stage 0 more details can be found in our paper. For the stage 1, we will classify all the pairs into three types. Type B pairs refer to the pairs have higher syntactic and semantic similarities. These pair are frequently appeared such as company's regulations. Revise pairs have similar syntactical pattern but in fact the two segments disclose very different meaning. Mismatch pairs are more like a debut information or company's new operations. For the model fine-tuning stage, we first use an external dataset, ES NL, for out-of-domain fine-tuning. It is a natural language inference dataset with token annotation. For example, the word from is the rationale according to the context of this pair. For in-domain fine-tuning, we use the revise pairs, the revise words as pseudo positive labels, and we randomly label few other words as negative. In addition, we mix different objectives. We use the soft labeling techniques by mixing cross entropy loss and KL divergence. Therefore we can alleviate the problem from low-quality pseudo labels. The version dataset include the ES NL pairs and our release final dataset. We use two metrics to judge the performance. Precision indicates the precision over recall. PCC means the correlation between prediction and annotations. This table shows that our domain-agnostic highlighting model achieved the best performance on final, and even preserve the generalization capability as you can see the performance on ES NL. We further observe that our methods can benefit on relation, the mismatch pairs, which we didn't use during training. In conclusion, we propose a highlighting task with our release final dataset and a simple pipeline with two-stage fine-tuning. There are many other future works we would like to try including improving effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well. Yeah, that's it. So please refer to our paper and github for more details and feel free to ask us if you have any question. Thank you.</sample>
    <sample id="220">I fornitori dell'articolo sono la Stony Brook University e la American Computer Science PhD.</sample>
    <sample id="221">In base all'articolo, le coppie linguistiche analizzate sono Germano-inglese e inglese-italiano.</sample>
    <sample id="222">This work focuses on adapting or annotating challenges and interventions in open-domain question answering to motivate the development of more effective models. The authors investigate different data interventions, including zero-shot and few-shot methods, to enable out-of-domain generalization in open-domain QA. They identify the type of dataset shift a new domain exhibits and determine which data interventions are effective for specific types of shifts. The results show that few-shot adaptations improve reader performance by up to 24%. The authors also propose a compatibility measure to estimate the type of dataset shift exhibited by target datasets, which helps in selecting the most effective data interventions.</sample>
    <sample id="223">Il nome della relatrice o del relatore è Changbin.</sample>
    <sample id="224">Durante gli esperimenti, i modelli studiati sono il modello di Long Impart per la produzione di semplificazioni documentali e il modello di Long Base Impart per la produzione di semplificazioni sentenziali.</sample>
    <sample id="225">53 delle 62 attività utilizzate in MultiInstruct vengono utilizzate per scopi di addestramento e test.</sample>
    <sample id="226">Il numero di autori coinvolti nell'articolo è due.</sample>
    <sample id="227">The paper proposes a new framework for grounded language understanding, which involves using language models to score and rank candidate plans proposed by a symbolic agent. This approach allows language models to focus on discrimination rather than generation, making it easier for them to excel in this task. The framework is generic and can be applied to various natural language processing tasks, including question answering and dialogue systems. The paper demonstrates the effectiveness of the framework using different language models and training settings, showing that it can achieve strong performance and sample efficiency.</sample>
    <sample id="228">Gli autori hanno effettuato i test su quattro set di dati: AGNews, Mind, SST-2 e Yahoo!S.</sample>
    <sample id="229">The paper presents a joint work with Henning Voss-Muth on detecting improvable claims for argumentative writing support. The authors introduce two new tasks: suboptimal claim detection and claim improvement suggestion. They explore the challenges of working with revision-based data, such as representativeness, model complexity, contextual information, and topical and user bias. They conclude that revision-based data can be effectively employed for the given tasks and that modeling the distance between two claim versions is beneficial for detecting suboptimal claims.</sample>
    <sample id="231">NACHOS è un dataset di medical crawled data usato per addestrare il modello Dr. Bert in lingua francese.</sample>
    <sample id="232">Il nome della relatrice o del relatore è Avi Lillard.</sample>
    <sample id="233">The paper presents a strategy for simultaneous speech translation using encoder-decoder attention. The proposed method uses pre-trained offline models without retraining or adapting specific architectures for simultaneous speech translation. It handles latency through specific parameters and leverages the knowledge already acquired by the model through the attention mechanism between audio input and textual output. The strategy decides whether to emit or not a partial translation based on where attention points to, emitting words if the attention is concentrated and waiting for another speech chunk if it's not. The results show that the proposed method outperforms all other strategies applied to offline models in terms of translation quality, average latency, and computational load time.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati.</sample>
    <sample id="235">I ricercatori che hanno scritto l'articolo sono affiliati all'University of Edinburgh.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti sono equipaggiate con ciascun compito.</sample>
    <sample id="237">I ricercatori proppongono una suite di test diagnostiche per integrare la conoscenza. Introducono un compito di risoluzione di correlazione progettato per testare la capacità di estrarre informazioni da diverse fonti.</sample>
    <sample id="238">The video presents a new benchmark dataset called "MeetingBank," which includes meeting transcripts, reference summaries, and URLs from city council meetings. The dataset addresses the challenges of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. It contains 1366 city council meetings with nearly 7,000 instances, providing detailed statistics on meeting duration, token per meeting, speaker frequency, and year period. The dataset is used to measure the level of abstraction in meeting summaries using metrics like coverage and density. The video also evaluates various summarization systems, including extractive and abstractive models, and provides insights into their performance.</sample>
    <sample id="239">Ecco un riassunto in italiano del testo originale: Il mio nome è Avi Lillard e io presenterò un breve riassunto del paper intitolato "Prompting BART from translation: assessing strategies and performance". Questo è un lavoro di collaborazione con i colleghi di Google Translate. BART è un modello linguistico a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testi che comprende 780 miliardi di token. In termini di adozione, BART ha superato lo stato dell'arte in centinaia di task NLP. In questo lavoro, presentiamo il primo studio sistematico sulla prompting dei modelli linguistici per la traduzione. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità NMT. Ci siamo concentrati sul uso dei test set più recenti per evitare un混杂效应 tra i dati di test e i dati di addestramento del modello linguistico. Abbiamo confrontato due sistemi dello stato dell'arte, i migliori sistemi per la traduzione, e utilizziamo metriche NMT di ultima generazione e anche risultati di valutazione umana. Infine, forniamo alcune raccomandazioni per le strategie di selezione delle promptings. Il prompting ha un forte impatto sulle prestazioni dei modelli linguistici per la traduzione. Come possiamo vedere in un semplice esperimento, utilizzando un prompting unico per due diverse frasi, il 516 su 1000 dei test ha differenze superiori a 1 punteggio e in casi estremi fino a 40 punteggi. Ciò dimostra quanto sia importante scegliere una buona strategia di prompting. In nostre esperienze, preferimmo una strategia di prompting a 5 passi, dove ogni frase fornita al sistema è seguita dalla lingua di destinazione. Abbiamo notato che la qualità dell'esempio ha un impatto molto maggiore sulla similarità rispetto alla similitudine della frase di origine. Ciò significa che è importante scegliere esempi di traduzioni di alta qualità. Abbiamo confrontato la selezione dei promptings da dataset di addestramento o da dataset di dev. I dataset di dev sono più accurati e di qualità più alta rispetto ai dataset di addestramento, portando a prestazioni migliori. Tuttavia, i sistemi dello stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni BART. In ogni caso, BART è quasi all'inseguito di un sistema commerciale. Le informazioni ottenute dalla nostra valutazione, effettuata utilizzando il framework MQM, hanno dimostrato che la fluidità di BART è paragonabile a quella dei sistemi dello stato dell'arte, ma la differenza principale deriva dalla precisione. In particolare, gli errori più comuni sono gli errori di omissione. Ciò sembra suggerire che BART sceglie di produrre traduzioni migliori, spesso raccogliendo parti della frase di origine che non sono pertinenti alla traduzione. Tuttavia, la qualità di output stilare di BART è inferiore rispetto a quella dei sistemi dello stato dell'arte, che è un segnale aggiuntivo che BART fornisce un output fluente ma con problemi di precisione. Ecco tutto per questa rapida panoramica. Per maggiori dettagli, ti invito a consultare la presentazione completa del paper. Grazie mille!</sample>
    <sample id="240">Ciao, mi chiamo Dawei e sono un studente di laurea in Informatica presso l'Università di Saarland in Germania. In questo video vorrei presentare il mio recente lavoro, "Worse than you think: A critical look at weakly supervised learning". Questo è un lavoro di gruppo con Cai Yuheng, Maio Smoothbath, Giacomo Stefan e Dietrich Klakow. Vorrei iniziare con una introduzione aWeak Supervision eWeakly Supervised Learning. InWeak Supervision non si etichettano i dati manualmente, ma si etichettano i dati utilizzando fonti di etichettatura dei datiWeak, come regole semplici, basi di conoscenza o qualsiasi fonte di etichettatura di bassa qualità, come illustrato nella figura sulla destra. In confronto alle etichette umane, le etichetteWeak sono molto più economiche, anche se sono anche rumorose, cioè che un certo numero di etichette sono sbagliate. Se si addestrano direttamente i reti neurali su data etichettataWeak, i reti neurali tendono a memorizzare le etichette rumorose e non generalizzano. InWeakly Supervised Learning, gli algoritmi di addestramento sono proposti per addestrare robustamente i reti neurali su tale rumore di etichette, in modo che i modelli addestrati possano generalizzare bene. In recenti opere in WSL, si sostiene che i modelli addestrati su data etichettataWeak raggiungano prestazioni elevate su set di test puliti. Tuttavia, questa affermazione è falsa, ma c'è un problema: si assume che ci sia un set di validazione pulito disponibile per la selezione del modello. Ci focalizzeremo su questa situazione problematica, poiché implica che è necessaria un'annotazione manuale aggiuntiva perWSL. Ma, come un elefante in una stanza, questa necessità spesso viene ignorata. L'obiettivo dell'articolo è quello di porre tre domande di ricerca: 1) è necessario un set di validazione pulito perWSL o possiamo utilizzare un set di validazione rumoroso invece? 2) se è necessario o obbligatorio un set di validazione pulito perWSL per funzionare correttamente, allora quanti campioni puliti dobbiamo avere? 3) dovremmo utilizzare solo i campioni puliti per la validazione o esistono altre migliori modalità per utilizzarli? Abbiamo indagato queste domande di ricerca nel nostro lavoro e i nostri findings sono i seguenti: Prima, scopriamo che recenti metodi diWSL hanno bisogno di campioni di validazione puliti per funzionare correttamente, altrimenti c'è una grande differenza di prestazione rispetto alla Figura. Se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare al di fuori dei relativi etichettatiWeak, cosa che rende l'addestramento inutile. Questo indica cheWSL richiedono etichette pulite per funzionare correttamente e il costo di ottenimento di etichette pulite non dovrebbe essere trascurato. Il nostro secondo finding è che aumentare il numero di campioni di validazione puliti aiuteràWSL ad ottenere prestazioni migliori, come illustrato nella Figura. Di solito, basta avere 20 campioni per classe per ottenere prestazioni elevate. Ma, non è tutto il racconto, perché se scegliamo di ottenere i campioni puliti, allora addestrare i modelli direttamente su di essi otterrà anche prestazioni migliori. La Figura a sinistra mostra la differenza di prestazione tra approssimazioni di finetuning dirette applicate direttamente su i campioni puliti eWSL approssimazioni che usano i campioni puliti per validazione solo. Come possiamo vedere, se ci sono 10 campioni per classe, finetuning diretti inizia a superareWSL approssimazioni. Infine, il miglioramento di prestazione affermato in precedentiWSL approssimazioni può essere facilmente ottenuto consentendo di continuare il finetuning sui campioni puliti. Come possiamo vedere dalla Figura, il modello VGG16 denominatoFTW inizialmente sottoperfora rispetto a metodi più complicati diWSL come Cosine, tuttavia se si consente di continuare il finetuning sui campioni puliti, alloraFTW ha prestazioni uguale a metodi più complicati. Quindi, in pratica, non c'è motivo di scegliereWSL metodi più complicati che richiedono più tempo di calcolo e spazio disco. Per riassumere, dimostriamo che recentiWSL approssimazioni richiedono etichette pulite per funzionare correttamente. Le prestazioni e praticità dei metodiWSL sono pesantemente sovrastimata. Le nostre raccomandazioni concrete per future work sono le seguenti: 1) rapportare i criteri di selezione del modello, ad esempio, se la selezione del modello è fatta senza campioni di validazione puliti; 2)WSL approssimazioni dovrebbero essere confrontate con baselines di finetuning a breve termine che operano su campioni puliti; 3) finetuning continuo è un semplice ma forte baselines che dovrebbe essere considerato in future work inWSL. Infine, abbiamo fornito il codice开源 sul link Q&amp;A sullo schermo. Sono felice che lo esamini. Grazie per la conferenza.</sample>
    <sample id="241">The paper discusses the development of a human-in-the-loop framework for early misinformation detection in social media platforms. The authors propose an end-to-end system that integrates human feedback throughout the process, from detecting misleading claims to policy violation verification. They evaluate their system using a dataset of COVID-19 treatment misinformation and find that it can detect 65% of policy violations with a high level of accuracy. The system also has a high throughput, with 124 policy violations detected per human hour of work. The authors hope that their work will motivate the development of future human-in-the-loop misinformation detection systems and provide outsiders or out-of-industry readers with a better understanding of the development and evaluation of such systems.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono la domanda a giudici umani di scegliere tra due conversazioni o di valutare le conversazioni su una scala di Likert.</sample>
    <sample id="243">Sono coinvolti 5 autori nell'articolo.</sample>
    <sample id="244">Per risolvere il problema, è necessario conoscere che Servin è un giudice e che i giudici decidono casi in una corte.</sample>
    <sample id="245">The paper presents a two-step pipeline for finding high-agreement Amazon Mechanical Turk workers. The first step involves qualification settings, including the number of human intelligence tasks and approval rate, to categorize workers into four types: gold, silver, bronze, and block. Only gold and silver workers can pass this task, resulting in 26 AMT workers out of 200 participants. The second step tests the annotator's capacity for handling heavy workloads, with results showing that only 12 AMT workers passed this task. The paper also includes a reference-based task to test general performance on a true annotation task, with results showing that eight out of twelve AMT workers finished all hits. The paper concludes that the pipeline can achieve high agreement at a lower cost and serve as best practice for high-agreement annotation at large scale.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi vedere il codice in GitHub.</sample>
    <sample id="247">The paper presents a new dataset, called FactKG, which is designed for fact verification via reasoning on knowledge graphs. The dataset includes claims in two styles (written and colloquial) and five types of reasoning (one-hop, conjunction, existence, multi-hop, and negation). The dataset is constructed using a combination of a collocational style transfer model and presupposition templates. The paper also introduces a new task, knowledge graph-based fact verification, which can be used to check the consistency between user input and knowledge graphs. The results show that the proposed model outperforms all other baselines, including claim-only baselines and a model that uses only graph evidence.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico, ad esempio Paese, genere, ecc.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate in modo da preservare la struttura rilevante ma aggiungendo rumore all'input.</sample>
    <sample id="250">Avere una valutazione dimensionale significa fornire un'evaluatione più dettagliata e precisa del modello di dialogo, analizzando specificamente le azioni che il modello esprime.</sample>
    <sample id="251">I ricercatori dell'articolo sono affiliati all'Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="252">The presentation introduces a new approach to prior case retrieval (PCR) in the legal domain, focusing on event extraction and unsupervised learning techniques. The creators present the ILCR dataset, a comprehensive collection of 7,700 legal cases with 6.775 average iterations per query document, and the YouCreate pipeline, which leverages event-based approaches for PCR without requiring law or demographic-specific tuning. Experiments using various models, including count-based, transformer-based, and event-based models, demonstrate that event-based models significantly outperform baseline methods, offering lower inference time and higher F1 scores. The YouCreate pipeline is highlighted as a state-of-the-art method for the CoLI 21 document retrieval task.</sample>
    <sample id="253">The presentation introduces a research project by researchers from Mexico and Spain, focusing on the detection of mental health disorders in social media. The project aims to develop a double-domain adaptation model for analyzing social media posts to identify signs of mental disorders. The model uses domain adaptation techniques to improve performance on a target domain by leveraging knowledge learned from related domains, such as Wikipedia and Google Books. The approach involves integrating information from Reddit and mental health resources, incorporating a lexicon to guide the masking process during training. The results show that the proposed model achieves a good balance between precision and recall, outperforming other methods. The visualization tool highlights important text sequences related to mental health issues, demonstrating the model's effectiveness in capturing signs of mental disorders in social media interactions.</sample>
    <sample id="254">The paper presents a document-level distant relation extraction framework with uncertainly guided label denoising to improve the label quality of DS data. The framework first trains a pre-denoising doca model with both DS and human-annotated data to generate pseudo labels, then introduces uncertainty estimation to determine whether the model prediction can be trusted or not. An instance-level uncertainty estimation is proposed to capture the uncertainty score for overlapping relations, and a re-labeling strategy with dynamic class uncertainty threshold is designed to filter pseudo labels with high uncertainty. The framework outperforms several strong baselines on two public datasets.</sample>
    <sample id="255">La forma del prompting è importante solo in casi di zero o uno shot prompting.</sample>
    <sample id="257">I ricercatori hanno valutato quattro modelli di dialogo di stato dell'arte.</sample>
    <sample id="258">This paper introduces a novel method for evaluating the quality of text in natural language processing using large language models. The authors propose using large language models to generate ratings based on instructions and samples, aiming to provide an alternative to human evaluations. They compare the results of their method with human evaluations by English teachers and find that some large language models show a clear preference for human-written texts. The paper also discusses potential future research directions, such as investigating agreement between large language models and human evaluators, exploring the impact of instruction changes, and comparing the benefits and drawbacks of using large language models versus human evaluations.</sample>
    <sample id="259">The speaker introduces Exemplar, a dataset for cross-lingual semantic parsing in multiple natural languages and many representations. It contains 9 datasets, 5 semantic parsing tasks, 80 mini-representations, and 22 natural languages in 15 language families. The dataset is designed to evaluate the performance of different models on various tasks, including translation tests, monolingual models, multilingual models, and cross-lingual zero-shot and few-shot transfer. The results show that encoder-decoder models outperform previous work, and monolingual language models are still inadequate for cross-lingual semantic parsing tasks.</sample>
    <sample id="260">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="261">Un buon pianificatore dovrebbe leggere i script che sono ragionevoli e sicuri rispetto alle limitazioni.</sample>
    <sample id="262">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="263">The paper presents a new method for mitigating label biases in in-context learning, which is a popular paradigm for utilizing large language models. The authors identify a new type of bias called domain label bias and propose a calibration method to handle all types of biases. They conduct experiments using different datasets and models to test the effectiveness of their method. The results show that their method significantly improves the performance of in-context learning on tasks with larger domain label biases.</sample>
    <sample id="264">The presentation discusses the challenges of multimodal text generation tasks, such as audio-visual text generation, and proposes a novel task called transferable audio-visual text generation. The main challenge is the multi-modal domain shift, which affects visual content understanding differently than audio content. To address this, the presenter introduces a framework consisting of three components: audio-visual map meta network, audio-visual encoder, and language model generator. The audio-visual map meta network maps different visual concepts across domains into a unified audio-visual semantic space. The audio-visual encoder uses a transformer-based architecture to generate tokens that represent the visual context, which are then used to improve the semantic of those tokens by optimizing the correlation between the generated audio and the model's output. The language model generator is trained using a contrastive learning framework to directly optimize the visual-audio textual element scores. The framework is evaluated on two benchmarks based on MSVD-T and MSVD, including cross-domain settings. The results show that the proposed approach outperforms all compared models on both cross-domain and cross-dataset settings, with some low-resource domains showing performance degradation in terms of TAVIST performance. Additionally, ablation experiments are conducted to analyze the impact of audio features and Spanish comments.</sample>
    <sample id="265">Il nome della relatrice o del relatore è Vasudeha.</sample>
    <sample id="266">I fornitori dell'articolo sono Adam Sipruckowsky, Igor Miltruk e David Catuson.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli errori di ommissione, che si verificano quando PaLM produce una traduzione migliore scelendo di omettere parti della frase sorgente.</sample>
    <sample id="269">Ciao, mi chiamo James Finch e io sono Sarah Finch. Oggi useremo ABC Eval, un nuovo approccio multidimensionale per valutare l'intelligence artificiale conversazionale. Questo lavoro è stato fatto dal laboratorio NLP dell'Emory, guidato dal professor Gio Choy all'Emory University, in collaborazione con Amazon Alexa AI. Supponiamo che tu abbia sviluppato un modello di dialogo e volessi vedere quanto bene si confronta con le migliori pratiche attuali. La pratica comune è di richiedere giudizi umani, ad esempio, chiedendo ai giudici umani di scegliere tra due conversazioni o di valutare le conversazioni su una scala Likert. Questi metodi funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello in un livello più fine. Un approccio è di chiedere ai giudici umani di valutare diversi aspetti della qualità del dialogo, ad esempio, la rilevanza delle risposte del modello, utilizzando metodi esistenti o scala Likert. Tuttavia, crediamo che ci sia un modo più preciso e affidabile per valutare le dimensioni del dialogo. Il nostro approccio tenta di ridurre la soggettività dei giudizi umani, definendo esplicitamente se o no ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni non rilevanti o contradire se stessa. Chiamiamo questo approccio ABC Eval. Abbiamo sviluppato questa tecnica per coprire completamente i comportamenti del modello di chat suggeriti da recenti pubblicazioni. ABC Eval è capace di misurare le tassi in cui i modelli di chat commettono errori thematici vari. Ad esempio, ABC Eval misura il numero di giri in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di non rilevante, si contraddice o si partner, elenca fatti sbagliati o viola le conoscenze comuni, e quando il modello riesce o fallisce a dimostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di punta e li abbiamo valutati su 100 conversazioni umane-bot per modello utilizzando ABC Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni a scala Likert sul livello giro, valutazioni a scala Likert sul livello di dialogo e confronti a livello di dialogo pairwise. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati della qualità del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo diverse dimensioni. Dall'analisi dei risultati delle valutazioni, abbiamo scoperto che i etichettatori ABC Eval hanno in generale una affidabilità più alta rispetto alle etichette raccolte da metodi esistenti, come misurato da accordo inter-etichettatore su 100 conversazioni doppioni etichettate. Inoltre, i etichettatori ABC Eval sono più predittivi della qualità della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostrato da un'analisi di regressione lineare semplice. Ad esempio, si può vedere come misurare la proporcione di giri in cui un modello di chat ignora il proprio interlocutore o dice qualcosa di non rilevante spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza Likert medio spiegano solo il 4% o meno. Infine, si è verificato se ogni metrica di valutazione mantiene un aspetto unico della qualità del chat utilizzando una regressione lineare passo dopo passo. Si può vedere come la combinazione di tutti i metri ABC Eval spiega più del 25% della qualità della conversazione e che rimuovere i metri uno alla volta porta a una perdita di informazione significativa sulla qualità. Sulle altre mani, la combinazione di tutti i metri a livello giro Likert spiega molto meno della qualità e meno di questi metri hanno informazione unica. Questi metri ABC Eval affidabili e informati consentono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta rispetto ai metodi precedenti. Puoi vedere che i risultati del nostra esperimento hanno evidenziato che diversi sfidieri rimangono e hanno essere precisamente quantificati. Ad esempio, i bot che testammo hanno violazioni di senso comune in circa il 20% delle loro risposte, producono informazioni non rilevanti in circa il 15% delle risposte e contradicon se stessi o il proprio interlocutore circa il 10% del tempo. Con la velocità rapida di miglioramento nel campo, molte di queste tassi di errori potrebbero vedere una diminuzione nei nuovi modelli rilasciati da quando è stata effettuata la nostra valutazione. Tuttavia, questo è tutto il più motivo per cercare metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC Eval possa essere utilizzata da altri nel campo come un passo importante in questa direzione e ci guardiamo ad vedere come l'intelligenza artificiale conversazionale proglierà nei prossimi mesi e anni. Grazie per la visione.</sample>
    <sample id="270">I fornitori dell'articolo sono il laboratorio NLP dell'Emory e Amazon Alexa AI.</sample>
    <sample id="271">CFT significa Continuos Fine-tuning.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, mi chiamo Kaiyuan e presenterò il nostro lavoro intitolato "Quando è necessario un contesto per la traduzione: un'esplorazione multilingue guidata da dati". Questo lavoro è stato fatto in collaborazione con Patrick Franz, Emi Yu, Andrew F. D. Martins e Graham Naviga. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurre "Mole" in questa frase? Se la frase precedente è "Le cose potrebbero iniziare a mettere in pericolo se i ministeri scoprono", allora "Mole" si riferisce a un spia. Ma se la frase precedente è "Potrebbe essere qualcosa di serio, dottore?" allora "Mole" si riferisce a una macchia di neve. Quindi, a seconda del contesto, il significato del termine cambia e, di conseguenza, anche la traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola parte delle traduzioni dipende dal contesto, i metri a livello di corpus come BLEU non sono in grado di capturare queste traduzioni. E alcuni hanno suggerito una valutazione mirata su traduzioni a contesto dipendenti, ma questi risorse solo supportano tipi limitati di traduzioni a contesto dipendenti e limitati insiemi di lingue, poiché di solito si basano su conoscenze dominanti e curazione umana. In questo lavoro, ci siamo proposti rispondere a queste due domande: quando è necessario un contesto per la traduzione e quanto bene i modelli gestiscono questi casi. Per rispondere alla prima domanda, ci siamo concentrati su quanto un termine dipende dalla traduzione a contesto. In un lavoro precedente, abbiamo introdotto CXMI come misura dell'uso del contesto da parte dei modelli di traduzione automatica e questa è data dalla quantità di informazione che il contesto C fornisce sul target Y dato il testo X. Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, abbiamo esteso CXMI a P-Y CXMI, che può misurare l'uso del contesto al livello della frase o al livello del termine. Possiamo pensare ai termini con alta P-Y CXMI come quelli che richiedono contesto per la traduzione. Ora analizziamo i termini con alta P-Y CXMI per cercare di individuare模式在这些 termini. E performiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Performiamo l'analisi a tre livelli diversi. Innanzitutto, guardiamo i frammenti di discorso che hanno alta media P-Y CXMI e questo ci consente di trovare, ad esempio, i pronunti duali in arabo che hanno una media alta P-Y CXMI e questo può essere spiegato perché l'inglese non ha pronunti duali, quindi è necessario contesto per determinare se un pronome è duale quando tradotto in arabo. E inoltre, troviamo che certe lingue richiedono contesto quando si sceglie la forma appropriata del verbo. Poi guardiamo i vocabolari che hanno alta P-Y CXMI medio su tutti i suoi differenti accadimenti e questo ci aiuta a identificare casi come quello qui, in cui in cinese si hanno bisogno di contesto per tradurre i nomi per assicurarsi di usare la stessa traduzione all'interno del documento. E inoltre, troviamo che il contesto è supportato per tradurre in un formalità corretta. Infine, guardiamo i singoli token che hanno alta P-Y CXMI e questo ci consente di identificare fenomeni che non possono essere davvero captati da un termine in sé, ma che sono piuttosto espressi nella struttura della frase, come la risoluzione dell'elissi. Quindi ora utilizziamo i nostri findings dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per automaticamente identificare i termini che appartengono al fenomeno e li chiamiamo il tagger multilingue discorsivare o Muda tagger. Possiamo anche notare che diverse lingue hanno proporzioni diverse di questi fenomeni di discorso. Poi utilizziamo il Muda tagger, applicandolo a un corpus parallelo che vogliamo utilizzare per la valutazione e applichiamo i metri di traduzione di scelta sulla traduzioni a contesto dipendenti che il Muda tagger ha identificato. Infine, utilizziamo il benchmark insieme ad altri metri per valutare diversi modelli di traduzione a livello di documento. In primo luogo, quando utilizziamo metri a livello di corpus, ad esempio BLEU, scopriamo che i modelli di traduzione a contesto asincroni hanno le migliori prestazioni. Ma se utilizziamo comet, i modelli di traduzione a contesto sono i migliori e se utilizziamo word f-measure, i modelli con o senza contesto hanno prestazioni comparabili. Questo di nuovo dimostra che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo metri a livello di corpus alone. Utilizziamo il benchmark Muda per valutare i modelli e scopriamo che i modelli di traduzione a contesto sono significativamente più accurati dei modelli che non usano contesto per certi fenomeni di discorso, come formalità e coesione lessicale, ma questi modelli non sono molto migliori dei modelli che non usano contesto per altri fenomeni come pronunzie e forma del verbo. Questo suggerisce dove dovremmo vedere progressi maggiori per la traduzione a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeepL è di solito più accurato di Google Translate per la traduzione a livello di documento. Per riassumere, abbiamo svolto un'analisi guidata da dati attraverso 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e utilizziamo i nostri findings per costruire un benchmark per la traduzione a livello di documento, che ci aiuterà a identificare i fenomeni di discorso a cui i modelli possono gestire bene o no e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie mille per l'attenzione, ci vediamo in Torino!</sample>
    <sample id="274">Il relatore è Usen Zhang.</sample>
    <sample id="276">This paper presents a dataset for evaluating machine translation metrics in Indian languages, specifically Tamil and Marathi. The authors collected 7000 samples of translations from seven different translation models and had bilingual experts annotate them with error types, severity, and overall scores. They found that Comet outperformed Comet baselines on three out of five languages and had higher correlations than Comet MQM across all languages. The dataset is publicly available for further research.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Il metodo utilizza le "parole contrassegguate" per identificare le parole che distinguono i gruppi contrassegnati da quelli non contrassegnati. Questo aiuta a rilevare i pregi e i danni specifici dei profili generati.</sample>
    <sample id="279">I ricercatori dell'articolo sono studenti di PhD all'Università di Washington.</sample>
    <sample id="280">The speaker introduces their work on emotion regulation in conversations, which aims to predict the emotional label of each utterance in a dialogue. They propose a novel attention-based correlation-aware multimodal fusion framework called MultiEmo, consisting of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The framework addresses challenges such as the complementarity of multimodal information, unsatisfactory performances in minority motion classes, and difficulty distinguishing between semantically similar motions. Experimental results demonstrate that MultiEmo achieves state-of-the-art performances on two ERCC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The speaker introduces a new measure called CXMI, which is used to assess the amount of context required for translation. The analysis was conducted on TED Talks translated into 14 languages and identified patterns in words that require context for accurate translation. The findings were used to create a benchmark for evaluating document-level machine translation models, which showed that context-aware models perform better than models without context for certain discourse phenomena such as formality and lexical cohesion.</sample>
    <sample id="282">The paper presents a new work on style transfer in natural language generation, specifically focusing on discourse level style transfer. The authors propose a generative model called StyleTrans, which combines discourse representation with normal style embeddings to generate text in the target styles. They also design a new training objective to reduce the style features from discourse representations and separate the generation into two stages. The results show that StyleTrans outperforms strong baselines in terms of style control and content preservation.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è il PragaApproach.</sample>
    <sample id="284">The speaker, Pong Pien Shau from Wuhan University, presents a paper on enhancing universal information extraction using a novel fuzzy spam mechanism. The current spam-based UIE model relies on identifying and labeling spam boundaries in text, but this can be ambiguous. The proposed method uses a continuous distribution of correct probability to model the fuzzy spam boundary, allowing for more reasonable attention distribution for spam extraction. Experiments on three information extraction tasks show significant performance improvement with FSS UIE compared to UIE without the fuzzy spam mechanism.</sample>
    <sample id="285">The video presents a research work on reference-based fact error correction for dialogue summarization. The authors argue that current factuality metrics and evaluation frameworks have flaws, such as ignoring the content of the original summary and failing to correct errors by substitution, insertion, and deletion operations. They propose a new taxonomy of fact errors and an evaluation framework based on alignment, classification, and comparison. The results show that training FVC models with human-annotated reference summaries from dialogue summarization datasets yields better results than using unreliable factuality metrics. Combining human-annotated data with synthetic data is also promising.</sample>
    <sample id="286">Il relatore è James Finch.</sample>
    <sample id="287">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="288">I fenomeni sintattici possono essere testati utilizzando insiemi di dati come BLM, Syntax Gym e GPT-2.</sample>
    <sample id="290">La prima domanda di ricerca richiedeva la determinazione della necessità di un insieme di validazione pulito per l'apprendimento supervisionato con frequenza. Le abbreviazioni dei cinque metodi sono: 1) Clean validation data (CDV), 2) Noisy validation set (NVS), 3) Cleanly labeled data (CLD), 4) Clean samples (CS), e 5) Clean validation samples (CVS).</sample>
    <sample id="291">Il modello viene valutato su attività come riconoscimento di nomi e cognomi, classificazione, tagging di parte di discorso e risoluzione di domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un dataset di medici data dalla rete.</sample>
    <sample id="295">Il nome della relatrice o del relatore è Adam Sipruckowsky.</sample>
    <sample id="296">The video presents a work on irony detection in natural language processing, which is based on supervised machine learning. The researchers collected data from social media platforms such as Reddit and Twitter, and asked annotators to label the data as ironic or not. They found that there are differences in annotations between annotators of different genders, age groups, and nationalities. They also found that generational differences and geographical distribution of annotators affect their perception of irony.</sample>
    <sample id="297">This project develops a typology and glossary of dog whistles, performs a case study of historical U.S. political speeches containing racial dog whistles, evaluates dog whistle recognition in language models like GPT-3, and conducts a case study on toxicity detection with hateful sentences to show how dog whistles can evade content moderation online.</sample>
    <sample id="298">I risultati dell'experiment hanno dimostrato che il modello si deteriora con un intervallo temporale crescente tra i dati di training e i dati di test. Questo ha confermato l'ipotesi che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="299">The paper presents a training method to reduce the reliance of neural network models on shortcuts and improve their out-of-distribution performance. The method involves using a minimax training objective between a learner and an auxiliary model, where the learner tries to minimize the loss of the neural task while the auxiliary model tries to maximize the learner's loss by generating example weights that encourage the learner to learn from under-represented hard examples. The method is evaluated on three commonly used neural network datasets and their corresponding out-of-distribution test sets, showing consistent improvement in out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The paper introduces a new task called interactive dictation, which allows users to dictate and edit documents using their voice in a natural and intuitive manner. The authors present a baseline system for this task and collect data for a new interface. They also experiment with different architectures and output types for the ASR, segmentation, and interpretation models. The results show that there is a trade-off between runtime and accuracy, but GPT-3 models are more accurate but slower. The authors welcome further work on this task and have released code and a paper for more details.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output per ottenere un'ordinazione corretta dei token.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire se i modelli stanno producendo stereotipi positivi e narrativa essenzializzante come il modello di "forte e resistente" per le donne nere o il modello di "vivace e curvilineo" per le donne latine.</sample>
    <sample id="304">I input inaccettabili di coppia minima sono i punti di riferimento che non hanno la stessa struttura grammaticale o semantica del testo accettabile.</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL) and its limitations. The speaker introduces WSL, which involves labeling data using weak labeling sources such as simple heuristics or crowdsourcing. While these methods are cheaper than human annotations, they are also noisy, leading to incorrect labels. This noise can cause neural networks to memorize the label noise instead of generalizing well. The video highlights that recent WSL methods require clean validation samples to work properly, and that increasing the number of clean samples can improve performance. However, training directly on clean samples can achieve better results than WSL approaches. The video concludes with recommendations for future work in WSL, including reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, and considering continuous fine-tuning as a simple yet strong baseline.</sample>
    <sample id="306">The paper presents a study on the ability of language models to track entities in language models. The authors argue that this is a crucial ability for understanding long discourses, but there haven't been any systematic investigations into what pretrained language models can actually perform such tasks. The research question addressed in the paper is to what extent large language models can track entities. The authors designed an evaluation task involving boxes and objects, where the input to the model starts with a description of the initial contents of each box, and the task of the language model is to complete the input by predicting the contents of each box. The authors found that most models simply repeat the initial state as you can see from the generally high accuracy on the right panel, and we can also see that only text-davinci-03 accepts non-trivial tracking which is the pink line here in the left panel. And all other models perform below a strong random baseline obtained by random simulation which is the blue line.</sample>
    <sample id="307">Gli autori hanno utilizzato diverse metriche di valutazione per valutare il modello, tra cui la rilevanza, la precisione e la ricordanza.</sample>
    <sample id="308">The presentation discusses the concept of "positionality" in natural language processing (NLP) and its impact on the performance of AI models. Positionality refers to the perspectives held by NLP researchers and model developers, which can influence research outcomes due to demographic, identity, and life experiences. The presenter highlights a design bias in NLP, where models may not perform well in detecting toxic content in non-English contexts, such as Hindi. To address this, the presenter introduces a framework called NL Positionality, which compares annotations from diverse annotators with existing datasets and models using Pearson's correlation score. This framework helps identify positionality in NLP and provides recommendations for improving inclusivity, such as keeping records of design choices and building specialized datasets for specific communities.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'interannotatore accordo.</sample>
    <sample id="310">Wikipedia è stato scelto come dominio per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="311">I fornitori di informazioni dell'articolo sono German Text Simplification.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo set di benchmark per la tuning multi-modal con 62 diverse tare multi-modal, coprendo 10 category diverse. Questi task sono derivati da 21 esistenti set di dati open source e ogni task è equipaggiato con cinque template di istruzioni esperto.</sample>
    <sample id="313">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="314">La coordinazione binaria si riferisce alla relazione tra due elementi, spesso due parole, che sono connessi da un segno di coordinazione.</sample>
    <sample id="315">I prompt in questo studio sono stati utilizzati per un tempo medio di 10 minuti.</sample>
    <sample id="316">I risultati suggeriscono che il modello T5 più piccolo può generare script di alta qualità, migliorando la capacità di pianificazione linguistica sia in termini di completezza semantica che fedeltà alle condizioni. Questo indica che i modelli più piccoli possono sostenere i modelli più grandi quando adeguatamente addestrati su dataset specifici.</sample>
    <sample id="317">The presentation introduces a new approach to information extraction using code generation models. The speaker, Pinglei from Fudan University, explains that traditional language models struggle with the mismatch between input and output formats during inference, requiring large amounts of structured training data and special decoding strategies. To address this, they propose CODEIE, which transforms the unstructured input text into a structured format using code generation models like CodeDavinci-02. This method allows for more accurate and efficient information extraction, as demonstrated in experiments on named entity recognition and relation extraction tasks. The results show that CODEIE outperforms traditional baseline models, especially when using GPT-3 for information extraction tasks.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Slavac e presenterò i nostri lavori su Dr. Bert, un modello di apprendimento a rete profonda in francese per il dominio biomedico e clinico. In questa presentazione, innanzitutto parliamo del modellino di apprendimento linguistico in salute. Poi presenteremo le principali contribuzioni del nostro articolo. Introduciamo il primo modello biomedico in francese, Dr. Bert, che è basato su Roberta e ha ricevuto un addestramento su Natos, che è un dataset di dati medici scarolati da internet. Abbiamo anche introdotto una comparazione dei modelli con diverse impostazioni di addestramento e fonti di dati. Infine, presentiamo i nostri risultati su 11 compiti di apprendimento biomedico e clinico in francese. Conclusamente, eseguiamo gli esperimenti e forniamo maggiori dettagli su come accedere ai modelli. Da quando è stato rilasciato nel 2019, Dr. Bert è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e ha offerto un'ottima prestazione rispetto ai metodi statici e costituzionalizzati come Word2Vec, FastText o NER. Da allora, questo modello è stato adattato a molti altri linguaggi, tra cui il francese con Camembert e domini come biomedico con Pembert e Biobert e clinici con Clinicalbert, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati in domanda. Tuttavia, il francese non aveva alcun modello aperto sorgente per biomedico e clinico. Così, ci hanno chiesto di stabilire cosa sarebbe stato il dataset più appropriato per una vasta gamma di usi e i nostri dati scarolati sono un buon sostituto per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato Dr. Bert con il modello Shubert, che è basato su un dataset annullato ottenuto dalla non-profit University Hospital Data Warehouse. Poi ci hanno chiesto quanti dati sono necessari per addestrare un modello specializzato su dati in francese: 4 GB, 8 GB o altro? Per rispondere a questa domanda, abbiamo addeistrato e confrontato quattro modelli: una prima versione di Dr. Bert con 7 GB di Natos, una seconda versione di 4 GB di subset di Natos, una prima versione di Shubert che è un modello clinico con 4 GB di frasi tratte da ClinicalNotes e una versione finale di Shubert con un mix di 4 GB di subset di Natos e 4 GB di ClinicalNotes. Inoltre, introduciamo tre modelli addestrati addestrati continuamente per analizzare l'impatto delle strategie di addestramento: uno basato sul peso di Camembert e addestrato su 4 GB di subset di Natos, un altro basato sul Camembert ma addestrato su 4 GB di ClinicalNotes e infine uno basato su un modello di biomedico in inglese Pembert e addestrato su 4 GB di subset di Natos. In totale, abbiamo setti modelli. Per valutare i setti modelli, utilizziamo set di compiti pubblici e privati come riconoscimento di nomi e entità, classificazione, part-of-speech tagging e risoluzione di domande. Questi modelli vengono confrontati con sei modelli basilari che sono Camembert Oscar 138 GB, Camembert Oscar 4 GB, Camembert CCNet 4 GB, Pembert Biobert e Clinicalbert. La valutazione dimostra che i modelli performano meglio sul compito con i dati dello stesso tipo su cui è stato addestrato. Tuttavia, possiamo ottenere i dati da fonti etologiche diverse e notizie che sembrano essere più versatile. Abbiamo anche osservato che usare più dati traduce in performance migliore. Di fatto, addestrare i modelli da zero sembra ottenere prestazioni migliori su molti dei compiti. Tuttavia, i nostri esperimenti su addestramento continuo, utilizzando il peso e il tokenizzatore di Pembert addestrato su 4 GB di subset di Natos, hanno dimostrato risultati simili a quelli ottenuti con Dr. Bert 4 GB da zero, che non è il caso per i modelli basati sul Camembert pesos e tokenizzatore, che soffrono di problemi di stabilità. Infine, la nostra proposta di sistema offre prestazioni migliori su nove dei compiti di domande e supera globalmente i risultati del modello generico qui, Camembert. Abbiamo anche osservato che i dati specializzati sono migliori, ma non scalano bene. I modelli pre-addestrati ottenuti da Natos sono freely available su HuggingFace e tutti i script di addestramento su GitHub repository. Quindi, grazie per la tua visione di questa presentazione e stiamo aspettando di discutere al post-congresso a Toronto.</sample>
    <sample id="319">Nel lavoro vengono esaminate diverse strategie di apprendimento, tra cui l'apprendimento a partire da zero e l'apprendimento continuo.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è ridotto.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando i paari di frasi e confrontandole con le versioni originali.</sample>
    <sample id="322">The paper presents a study on the understanding of morality in text by language models. The authors apply explainable AI techniques to language models trained to understand morality in text, focusing on how morality is expressed differently across different domains. They use a dataset called Mora Foundation Twitter Corpus composed of 35,000 tweets collected in seven different domains corresponding to hashtags such as #AllLivesMatter and #BlackLivesMatter. The study finds that language models recognize that morality can be expressed differently in different domains, such as the difference between ALM and BLM in terms of the rhetoric used for the moral element of subversion. This highlights the importance of using domain-specific language models to avoid misunderstandings of morality in dangerous ways.</sample>
    <sample id="323">The paper presents a method for answering questions in the Commonsense QA task using language models and knowledge representation. The authors propose a hybrid graph-based model that combines a knowledge base with a language model to retrieve relevant knowledge from external sources. They introduce a new type of knowledge entity called "top bank" and use a hierarchical graph to encode the relationships between entities. The authors also use a masking strategy to generate subgraphs and update the knowledge base by fine-tuning the entity and relation embeddings. Finally, they incorporate the HKG graph embedding into the QA context and use MLP to predict the answer probabilities. The experiments show that their method achieves good results on Commonsense QA and OpenBookQA datasets compared to other methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="325">Hi, my name is Mateusz Lendman and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multi-set tagging and latent permutations. This is joint work with my advisers Alexander Colar and Ivan Titov. Compositional generalization can be understood as the ability of a learner to handle deeper recursion and unseen compositions of phrases that have been seen individually during training. In the context of semantic parsing, testing for compositional generalization might look like this: as usual, we have a training set of utterances in this case, the girl slept and Mary knew that the girl slept these utterances are paired with logical forms that represent core aspects of their meaning. In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms. In this example, the model has seen shallower recursion during training and is tested on an example with deeper recursion. Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input. In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example. A popular method to address this is to integrate trees into the models. The trees are intended to capture the compositional process that relates utterances with the logical forms. This works well, but trees are usually not given and need to be obtained somehow. This can be complicated and sometimes a computationally expensive process. Typically, this involves considerable formalism-specific pre-processing of the logical forms, for example, to handle variable symbols. Obtaining trees may also involve specialized grammar induction procedures. In this paper, we don't use trees and introduce a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output. For the first time, we show strong generalization to deeper recursion without relying on trees. Our approach predicts the output from the input in two steps. First, we tag each input token with an unordered multi-set of tokens that will appear in the output. After the first step, we have all the right tokens but they're not ordered. That's why in the second step, we use another model to predict a permutation to put them into the right order. We introduce a new method to predict a permutation that does not put any hard constraints on the possible permutations. This makes our approach quite flexible and expressive. Conceptually, our permutation model works roughly like this: we go from left to right over the output and determine which multi-set token to put in every position. For the first output position, we simply select one as highlighted in red. Then we jump to the next multi-set token to determine the second token in the output. We determine the third token in the output in a similar way by jumping to another multi-set token. We continue this process until every token from the first stage has been visited exactly once. To give you a teaser of the experimental results, here we compare our method with other treeless models on the CoNLL benchmark. Our model outperforms the others by a large margin on generalization to deeper recursion. Some other kinds of structural generalization remain very challenging though. In our paper, we solve a couple of interesting technical challenges. First of all, the alignment between input and output is not given in the training data. As a consequence, for a given token, we don't know which multi-set it came from, which poses a challenge for training. In addition, sometimes there are multiple permutations that are consistent with the data, but the linguistically correct one is latent. We address this by inducing the alignment as part of the training. Our permutation method is very flexible, but it brings the challenge that finding the highest scoring permutation is NP-hard. That's because this is related to the traveling salesman problem. We approximate this with a GPU-friendly continuous relaxation that also allows us to back-propagate through the solution and learn the linguistically more plausible permutations. If you want to learn more about our experiments and how we address these challenges, please have a look at our paper or come to our poster.</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono in contrasto tra loro.</sample>
    <sample id="327">The presentation introduces the Magi Tower, a new multi-modal architecture designed to enhance cross-modal learning by adaptively aggregating insights from pre-trained unimodal experts at different levels. Unlike Bridge Tower, which uses a static layer-by-layer approach, Magi Tower employs adaptive managers that can exploit various levels of unimodal semantic knowledge more effectively. This results in superior performance on visual-language downstream tasks, particularly with only 4 million images for pre-training. The Magi Tower's ability to handle both large and small models makes it versatile and efficient.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">This paper proposes a zero-shot video localization method based on structured pseudo-label generation, which is robust to label noise. The method generates three types of pseudo-queries: (1) pseudo-queries generated from video frames using an image-to-text pre-trained model; (2) pseudo-events generated from the temporal structure of events; and (3) pseudo-labels generated by matching the relevance between video frames and pseudo-queries. The method reduces the weight of noisy samples and corrects noisy labels to reduce the influence of label noise. The proposed method outperforms existing methods on two datasets, i.e., YouTube-Caption and STR Standard.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice o il relatore è Sarah Papi.</sample>
    <sample id="332">I dati per il parametro di riferimento MuDa sono stati tratti da un corpus di 14 lingue diverse.</sample>
    <sample id="333">The paper presents a novel training framework called Ink, which injects common knowledge into the representation space of a neural machine translation (NMT) model. The framework aims to improve the generalization ability and performance of NMT models by aligning contextualized representations with common knowledge embeddings. The authors propose a training loop that involves extracting common knowledge from a key-value data store, adjusting the representation using Kullback-Leibler divergence, and refreshing the data store asynchronously. Experiments on the WMT'19 German-English news translation task show that Ink system outperforms the state-of-the-art CM system and achieves the best performance after smoothing the representation space. The results also demonstrate that the proposed framework can achieve higher BLEU scores with less memory space and faster inference speed compared to the CM system.</sample>
    <sample id="335">Il nome del relatore è Mateus Lendeman.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di adattamento di modelli di traduzione o di interpretazione da un linguaggio alla lingua di destinazione. Questo può essere fatto utilizzando modelli monolingue o multilingue, in base alla quantità di dati di addestramento disponibile e alla lingua di destinazione.</sample>
    <sample id="337">The paper presents a neural network approach for handling out-of-vocabulary (OOV) words in embedding-based downstream models. It introduces a word relationship graph that captures lexical rules of word formation and association, allowing the model to recognize OOV words into word pieces and associate them with relevant words. The model uses a two-level graph representation and applies contrastive learning to encourage similarity between related samples while pushing dissimilar samples apart. Experimental results demonstrate the effectiveness of the model on both intrinsic and extrinsic tasks, and its applicability to other languages depends on the rationality of word decomposition.</sample>
    <sample id="338">The presentation discusses the evaluation of human explanations in machine learning models. The speaker, Bin Shen, introduces a new metric called "True" that assesses the helpfulness of explanations during fine-tuning. The study analyzed five datasets using a unified data structure and compared the performance of two models, T5 and BART. The results show that human-annotated explanations can still benefit model predictions, even if they are considered low quality by humans. The proposed metric outperforms the Simulatability Score in evaluating dataset qualities and task-specific characteristics. The work lays the foundation for high-quality human-aided annotation jobs and recommends researchers perform similar quality checks in the future.</sample>
    <sample id="339">I ricercatori dell'articolo sono studenti di laurea in Informatica presso l'Università di Sarlana in Germania.</sample>
    <sample id="340">The paper presents a large-scale syntactically diverse paraphrase dataset called PeraMRR, which is constructed using AMR back translation. The authors propose to leverage AMR graphs, which capture the abstract meaning of a sentence, to generate syntactically diverse paraphrases. They use a pre-trained AMR parser to get an AMR graph of a source sentence, change the focus of the graph, and then use an AMR graph-to-text generator to generate text from the modified graphs. The proposed dataset has around 15 million source sentences and around 6.9 paraphrases per source sentence. The authors demonstrate that PeraMRR can benefit several NLP applications, including learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">Gli autori usano due misure di latenza: la media della latenza e la latenza di lambda.</sample>
    <sample id="342">The presentation introduces the Life Chat dataset, a large-scale video-based dialogue dataset designed to capture real spoken conversations. It addresses the limitations of existing datasets, such as limited scale and manual annotation, by proposing an automatic construction method for a large-scale personalized dialogue dataset. The dataset is constructed in three steps: scraping videos from TikTok, extracting audios and transcribing them into transcripts, collecting comments and constructing dialogues, and collecting persona information for personalized dialogue generation. Experiments on two tasks show that the dataset's unique features, such as detailed persona annotations and long average sessions, improve performance in response modeling and dialogue recognition tasks compared to existing datasets.</sample>
    <sample id="343">Ciao a tutti, mi chiamo Mackshatta e oggi io e il mio coautore Martin stiamo presentando il nostro lavoro "KitMSTF: valutazione dell'integrazione di conoscenze provenienti da diverse fonti". Questo lavoro è un合作 tra McGill University, Mila e Microsoft Research. I modelli di intelligenza naturale possono trarre informazioni da una vasta gamma di fonti, tra cui le conoscenze contenute nei parametri acquisiti in precedenza e le conoscenze fornite come input durante l'ora di inferenza. Recenti studi hanno dimostrato che i modelli possono utilizzare le conoscenze acquisite in precedenza per risolvere compiti come la risoluzione delle domande, ma spesso richiedono anche conoscenze fornite in tempo reale. Ad esempio, nel frase "John ha visto il nuovo presidente su TV", i parametri pre-addestrati possono contenere informazioni sul ruolo del presidente e su cosa sia la TV, ma non possono rilevare chi sia specificamente John o chi sia il nuovo presidente, poiché queste informazioni potrebbero essere cambiate dopo l'addestramento. Di conseguenza, i modelli più efficaci per compiti ad alto livello di conoscenza devono essere in grado di integrare e utilizzare sia le conoscenze acquisite in precedenza che le conoscenze fornite in tempo reale. In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione delle conoscenze. Introduciamo un compito di risoluzione di correlazione progettato per testare la capacità di utilizzare conoscenze disponibili in diverse fonti. Abbiamo valutato il dataset con partecipanti umani e abbiamo stabilito modelli di risoluzione di correlazione correlati. Ecco un esempio dal dataset: "Servin è un giudice, Kia è un pasticcere. Servin e Kia si sono incontrati in un parco. Dopo un lungo giorno di lavoro decidendo casi in una corte di giudici, lui era felice di rilassarsi." La nostra任务 è di identificare l'entità specifica che il pronome "lui" si riferisce a, che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazione: informazione specifica sull'entità e informazione di sfondo. In genere, l'informazione di sfondo viene imparata durante l'addestramento dei modelli di intelligenza naturale, mentre l'informazione specifica sull'entità è solitamente osservata in tempo reale. Abbiamo variato l'accessibilità di queste due tipologie di informazione in modo che possano essere fornite in una singola fonte o in molte fonti. Abbiamo definito tre ambienti di KitMSTF: 1) l'ambiente di addestramento di background pre-addestrato, in cui l'informazione di sfondo è supposto di essere disponibile in tempo reale; 2) l'ambiente di background entrambe, in cui l'informazione di sfondo è disponibile sia in tempo reale che in tempo precedente; 3) l'ambiente di background inferenza, in cui entrambe le tipologie di informazione sono disponibili solo in tempo reale. Quest'ultimo ambiente è particolarmente interessante, poiché si somiglia al caso in cui l'informazione di sfondo necessaria per risolvere un compito non è parte dei parametri di addestramento del modello. Ad esempio, nuove occupazioni potrebbero essere sviluppate dopo l'ora di addestramento. Ecco un esempio di come controlliamo l'accessibilità di fatti in diverse fonti: in ambienti di addestramento di background pre-addestrato, supponiamo che l'informazione di sfondo "i politici cercano posti elettori nel governo" sia contenuta nei parametri pre-addestrati. In un contesto di inferenza, forniamo l'informazione specifica "Chester è un politico". In ambienti di background entrambe, forniamo sia l'informazione specifica che l'informazione di sfondo sulla politica in un contesto di inferenza. In ambienti di background inferenza, forniamo l'occupazione fictionale "Mertura" invece di "politico", poiché Mertura è improbabile di essere contenuta nei parametri pre-addestrati. Abbiamo valutato il dataset sia con partecipanti umani che con modelli di risoluzione di correlazione correlati. In questa figura, mostriamo i risultati dei migliori modelli per la versione più difficile dell'ambiente di addestramento di background pre-addestrato. Senza addestramento specifico su KitMSTF, entrambi i modelli non performano bene. Tuttavia, quando addestrati su KitMSTF, entrambi i CTF e B4KQF performano significativamente meglio rispetto a una scelta casuale. Questo suggerisce che i modelli addestrati su dataset di addestramento di correlazione generale imparano a esplorare cenni superficiali che non sono utili per test su KitMSTF, poiché tali cenni sono stati rimosse. Esperienze con conoscenze fictionali hanno dimostrato che anche i migliori modelli non riescono a integrare reliabilmente l'informazione di sfondo fornita solo in tempo reale. Per riassumere i principali punti di conclusione del nostro articolo: molti modelli di risoluzione di correlazione appaiono incapaci di ragionare su conoscenze provenienti da diverse fonti senza addestramento specifico. Tuttavia, con addestramento specifico, alcuni modelli riescono a integrare conoscenze provenienti da molte fonti. Anche i migliori modelli sembrano avere difficoltà a integrare reliabilmente l'informazione di sfondo fornita solo in tempo reale. Se siete interessati a maggior dettagli, vi prego di vedere il nostro articolo e controllare il dataset e il codice su GitHub. Grazie per l'attenzione!</sample>
    <sample id="344">I metodi basati su alberi hanno alcune difficoltà, come la necessità di ottenere gli alberi e la complessità computazionale associata.</sample>
    <sample id="345">The paper introduces a neural sequence-to-sequence model that directly models the correspondences between fragments of the input and fragments of the output, without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The permutation model is very flexible and expressive, and can handle multiple permutations that are consistent with the data but the linguistically correct one is latent. The experimental results show that the model outperforms other treeless models on generalization to deeper recursion.</sample>
    <sample id="346">I fornitori dell'articolo non sono elencati.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parliamo del mio articolo intitolato "Personaggi etichettati: utilizzo di promempi naturali per misurare i stereotipi in modelli di linguistica". Questo lavoro è stato realizzato in collaborazione con ESDermouch e Dan Jorovski. In recenti anni, molte persone hanno documentato la presenza di pregiudizi sociali e stereotipi in grandi modelli di linguistica o LLMs. Tuttavia, queste misure hanno diverse limitazioni. In genere, si basano su dataset costruiti a mano che richiedono molto tempo per essere curati e solitamente misurano solo stereotipi specifici, non generalizzandosi bene a altre demografiche o contesti o semplicemente capturando associazioni generali e spagnole. Inoltre, la maggior parte dei work in questo campo non tiene conto della intersecolarietà, che è la nozione che identità sociali multifaceted possono aumentare i bias e essere un nuovo luogo di danno. Per superare queste limitazioni, ci si basa sul fatto che questi nuovi modelli di linguistica sono molto bravi nel rispondere a istruzioni e promempi. Così possiamo chiedere al modello di generare un personaggio, che è una descrizione di un individuo immaginario utilizzando un promempi come "immagina che tu sia una donna asiatica, descrivi te stessa". Possiamo vedere immediatamente che questa è molto generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi segnaposto che vogliamo in questo promempi. Ecco alcuni esempi di generazioni da GPT-4. Subito dopo, vediamo che anche se i output non sono negativi o tossici nel senso tradizionale di queste parole, ci sono alcune interessanti pattern. La donna asiatica è descritta come non impressionante, la donna mediterranea viene riferita come "exotic" e si riferisce a una regione mesmerica, e entrambe le donne di colore hanno riferimenti alla discendenza, mentre il personaggio maschile ha niente di tutto quello. Per catturare questi pattern, il nostro metodo ha due parti. La prima parte è generare questi personaggi. I nostri promempi per generare questi personaggi sono ispirati a un studio in cui hanno fornito questi promempi a soggetti umani. Trovando che, fornendoli a soggetti umani, erano anche in grado di rivelare stereotipi razziali. Inoltre, questo rendeva possibile fare una confronto diretto tra i nostri personaggi generati e le risposte umane scritte. La seconda parte è il termine "etichette", che è un metodo per identificare le parole che distinguono gruppi etichettati da gruppi non etichettati. Lo svantaggio di questo è che otteniamo stereotipi specifici e pattern senza dover ricorrere a un lexicon specifico. Il termine "etichette" si basa sul concetto sociolinguistico di "etichettatura", che afferma che ci sono gruppi non etichettati e qualsiasi gruppo che differisca da quell'unico è linguisticamente etichettato. Ad esempio, la parola "uomo" (o forse la parola "guerriero") è usualmente associata con gli uomini. Quindi quando le persone descrivono un guerriero che è una donna, dovranno specificare "uomo guerriero" e etichettare la parola con "donna". E più in generale, i gruppi dominanti in una società sono sia linguisticamente che socialmente non etichettati, mentre i gruppi marginalizzati sono usualmente etichettati. Quindi nel nostro metodo, innanzitutto designiamo cosa sono i gruppi non etichettati e i gruppi etichettati. Poi possiamo confrontare i personaggi utilizzando il termine "combattimento" che è in pratica utilizzando rapporti logit ponderati per distinguere le parole principali per ciascun gruppo etichettato. Ad esempio, per i personaggi di donna nera, faremmo combattimento e confronterei i rapporti logit con entrambi i personaggi bianchi e maschili, poiché sono i due gruppi corrispondenti non etichettati. Ora, ecco i risultati. Innanzitutto utilizziamo un lexicon di stereotipi e troppo che i personaggi generati contengono molti più stereotipi rispetto ai personaggi umani scritti. Tuttavia, quando guardiamo la distribuzione delle parole nel lexicon, troppo diverse cose. Mentre i personaggi generati hanno tassi molto elevati di parole nel lexicon, i personaggi umani hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono presenti nei personaggi generati sono solo parole positive o almeno non negative. E in effetti, il lexicon non cattura molte delle problematiche pattern che avevamo visto in precedenza. Quindi invece di farlo, ci concentreremo sulle risultanze dalla nostra tecnica di etichette per vedere come queste parole positive apparentemente positive riflettono i pattern problematici e i racconti essenzializzanti. In analisi, riveliamo come queste portretti apparentemente positive riflettono i pattern problematici. Per i gruppi etichettati, le parole principali includono cose come cultura, tradizione, orgoglio e exotici. E queste parole definiscono questi gruppi solo dalla relazione alla loro identità e li distinguono da un norme bianca. Questo contribuisce a un lungo patrimonio di discriminazione e alienazione per questi gruppi. Inoltre, ci sono molte troppe che sono riflettute in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibranti e curvaceous, che si connettono a un trope di tropicalismo. Per le donne asiatiche, le parole sono cose come dolci e delicati e silghi, che si connettono a una storia lunga di donne asiatiche che sono stata hypersexualizzata, vista come molto docile e sottomissiva e così via. E infine, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resistente. Questo si connette a un archetipo che le persone hanno chiamato l'archetipo della donna nera forte e resistente. Mentre sembra positivo all'inizio, ci sono studi che hanno dimostrato che questo tipo di archetipo è in reality molto problematico, poiché mette molta pressione su queste demografie per essere resistente e forte contro le difficoltà sociali. Così piuttosto di cercare di cambiare queste difficoltà, mette pressura su queste persone per superarle, che porta a risultati negativi per queste persone tra gli altri danni. Inoltre, troppo, vediamo che le parole per ciascun gruppo etichettato quasi completamente riflettono narrativa essenzializzante. Quindi, sulla base di queste tendenze, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, dovremmo, come ricercatori, affrontare i stereotipi positivi e i racconti essenzializzanti. Dovremmo anche usare linee di intersecolarietà per studiare i bias e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovremmo davvero aumentare la transparente riguardo metodi di mitigazione dei bias, poiché, ad esempio, queste stereotipi positive non sappiamo se è per qualche tipo di alleanza di valori eccessiva o forse metodi anti-stereotipi che stanno producendo queste caratteristiche positive, ma non possiamo fare nessuna ipotesi o studiare ulteriormente senza maggior transparente. Grazie mille per l'attenzione. Spero che avrai un buon tempo a ACL.</sample>
    <sample id="348">The paper discusses the prevalence of social bias and stereotypes in large language models (LLMs) and proposes a method to measure these biases using natural language prompts. The authors argue that traditional methods have limitations, such as relying on hand-constructed datasets and only measuring specific stereotypes. They propose generating personas using prompts like "imagine you are an Asian woman describe yourself" and comparing the generated personas with human-written responses. The Marked Words method is used to identify words that distinguish marked groups from unmarked ones, revealing harmful patterns and essentializing narratives. The results show that generated personas contain more stereotypes than human-written ones, and that positive-seeming portrayals can reflect harmful patterns. The paper concludes with recommendations for model owners to address positive stereotypes, use intersectional lens, and increase transparency about bias mitigation methods.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jing Wei e sono dalla Università di Scienze e Tecnologia di Cina. Mi fa piacere fare un breve video di promozione su un articolo intitolato "Are you copying my model? Protecting the copyright of large language models for embedding and services" (Vai alla finestra dietro i modelli). Innanzitutto, introduciamo il contesto dell'embeddining e dei servizi. Attualmente, i modelli di lingua grandi come GPT, LLaMA e Prolab sono eccezionali in comprensione e generazione di lingua naturale. I servizi di embeddining e dei servizi sono uno dei servizi costruiti su modelli di lingua grandi per assistere a diverse tare del linguaggio naturale (NLP). Ad esempio, OpenAI offre un API basata su GPT per l'embeddining. Tuttavia, recenti ricerche hanno dimostrato che un attaccante può imparare dal modello tramite l'embeddining e fornire servizi simili. Di conseguenza, è necessario proteggere la proprietà intellettuale degli embeddining e dei servizi. Per proteggere la proprietà intellettuale degli embeddining e dei servizi, una delle soluzioni è di imprimere un marchino d'acqua nel servizio fornito e di individuare se un altro servizio contiene il marchino d'acqua. Il marchino d'acqua necessita di soddisfare le seguenti caratteristiche: prima, il metodo dovrebbe essere applicabile agli embeddining e dei servizi; secondo, il marchino d'acqua non dovrebbe indebolire l'utilità dei servizi forniti; terzo, il marchino d'acqua dovrebbe essere abbastanza evidente per l'attaccante o l'attaccante potrebbe facilmente rimuoverlo; infine, il marchino d'acqua dovrebbe essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello. Le opere esistenti possono essere generalmente classificate in quattro category. Tuttavia, queste opere o non sono applicabili agli embeddining e dei servizi o mancano di trasferibilità. Pertanto, in questo articolo propone Embedding Marker, che è un marchino d'acqua basato sul retro della marchia applicabile agli embeddining e dei servizi. Introduciamo i dettagli del nostro Embedding Marker. Embedding Marker comprende due passi principali: l'iniezione del marchino d'acqua e la verificazione della proprietà intellettuale. Prima di questi passi principali, dobbiamo selezionare un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Noi supponiamo che il fornitore possa raccolta un corpus di testo generale e contare la frequenza di parole con esso. In iniezione del marchino d'acqua, innanzitutto definiamo un target embedding. Quand un utente invia una frase al servizio fornito, il fornitore conteggia il numero di trigger nella frase. L'embedding fornito è la somma ponderata dell'embedding target e dell'embedding originale. Il peso dell'embedding target è proporzionale al numero di trigger nella frase. Se il numero di trigger nella frase è maggiore di M, l'embedding fornito è uguale all'embedding target. La verificazione della proprietà intellettuale è per individuare se un modello dietro un altro servizio contiene il marchino d'acqua. Innanzitutto costruiamo un retro e un dataset benigno. Il dataset benigno contiene frasi in cui tutti i mots appartenenti all'insieme di trigger. Tutti i mots nella frasi del dataset benigno non appartenenti all'insieme di trigger. Allora il fornitore richiede gli embeddings da un servizio sospetto con il dataset. La somiglianza cosine e L2 tra l'embedding richiesto e l'embedding target viene calcolata. Simultaneamente, computiamo la differenza di somiglianza tra il dataset benigno e il dataset retro, definita come delta cosine e delta L2. Simultaneamente applichiamo il test KS e utilizziamo il suo p-value come il terzo metrico. Abbiamo condotto esperimenti su quattro dataset: AGNews, Mind, SST-2 e Airbnb. Noi supponiamo che il fornitore applichi il dataset WikiText per conteggio frequenza di parole. I risultati su quattro dataset hanno dimostrato che il marchino d'acqua dell'Embedding Marker ha un'ottima prestazione di deteczione e mantiene un'ottima utilità per i compiti di basso livello. Abbiamo anche validato la covertività dell'embedding fornito analizzando l'embedding delle frasi dell'insieme di dataset VOPCA. La legenda della figura indica il numero di trigger in ogni frase. Come si può vedere nella figura, è difficile distinguere tra i embeddings retro e gli embeddings normali. Questo è tutto, grazie mille! Siamo felici di discutere con voi.</sample>
    <sample id="350">The presentation discusses the concept of superhuman performance in natural language processing (NLP) and highlights the challenges in comparing human and system performance. It introduces the SuperGLUE benchmark, which includes tasks like commonsense reasoning and reading comprehension, and finds that humans outperform systems on most tasks. However, it points out issues such as different evaluation sets, errors in ground truth annotations, and biased human baselines, which can lead to unfair comparisons. The paper argues for more reliable benchmarks and methods to accurately assess human performance in NLP.</sample>
    <sample id="351">The paper investigates the problem of generalization in named entity recognition (NER) using the CoNLL 2003 dataset. The authors found that models trained on CoNLL 2003 have poor generalization to modern data, and that larger models with better architecture and more fine-tuning examples lead to better generalization. They also found that temporal drift is the main cause of performance drop, not adaptive overfitting. The paper concludes that CoNLL 2003 taggers still work well in 2023, but further research is needed to improve generalization of NER models.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio multidimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="353">The paper presents a method for code generation by asking clarification questions to address the challenge of input under-specification in natural language descriptions. The authors propose a synthetic dataset with clarifications on key operations and a pipeline for code generation by asking clarification questions. They also introduce a method to create code clarification queries (CQAs) and evaluate their effectiveness in identifying missing key operations. The results show that the proposed method can effectively identify missing key operations, but there are still challenges in distinguishing aligned operations from those with similar names and argument values.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2021.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono un candidato per il Master in Scienze Informatiche presso l'Università di Stony Brook. Vorrei presentare il mio lavoro accettato in ACL 2023 come un articolo su "Transfer Learning per la deteczione di dissonanza: affrontando il problema della classe rara". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare in linguistica. La dissonanza cognitiva è quando due credenze o azioni sono in contrasto, ad esempio, se una persona dice che sa che i sigarette possono ucciderla e quindi compra un paio di sigarette dopo una riunione. Questa credenza e azione sono in dissonanza. Menzioniamo anche che non pensa di poter tenere il suo lavoro senza le sigarette, giustificando la seconda azione e creando una relazione di consensus. Tuttavia, la dissonanza è un fenomeno comune che esperimenteremo in decisioni quotidiane e spesso si manifesta in linguaggio, insieme a altre relazioni di discorso. Cos'importa? Studiare la dissonanza può aiutarci a capire gli effetti della discordia tra le persone, tracciare tendenze e cambiamenti di credenze, valori e atteggiamenti in una popolazione. Una alta dissonanza cognitiva è anche associata a disturbi ansiosi e può aiutare a comprendere meglio lo stato mentale delle persone. Studiare la dissonanza espressa in linguaggio può anche essere benefico in comprensione dell'estremismo e polarizzazione dei gruppi vulnerabili. Infine, la dissonanza cognitiva è importante per comprendere i propri stili cognitivi personali e aiuta a capire meglio i processi di decisione. Per il mio obiettivo di creare un risorsa sulla dissonanza cognitiva, ho condotto una grande annotazione di relazioni di dissonanza. Utilizzando un approccio di dissonanza come quello nella mappa qui, i tweet sono stati analizzati utilizzando un parser di tweet e i paari di unità di discorso sono stati annotati in base alle direttive descritte in un articolo. Come possono vedere qui, la dissonanza era presente solo in 3,5% dei paari di unità di discorso analizzate. Dopo la raccolta di circa mille esempi di unità di discorso, ho allenato un classificatore iniziale solo con 43 esempi di dissonanza. Non sorprende che il classificatore non abbia performato molto meglio del caso, data la bassa frequenza di dissonanza e la mancanza di qualsiasi dataset precedente. Abbiamo riscontrato il problema di rara assoluta. Per alleviarlo, ho sperimentato su combinazioni di apprendimento passivo e attivo per annotare in modo tale da poter raccolti più esempi di dissonanza in meno ronde di annotazione, riducendo così i costi di annotazione in generale e migliorando la deteczione di dissonanza. Poiché il modello iniziale non ha mai potuto catturare la classe di dissonanza, ho iniziato il processo di apprendimento attivo transferendo i pesi da due task correlati: classificazione di dissonanza indipendente, che determina se due dichiarazioni da parte di due persone sono in accordo o in disaccordo indipendentemente dal soggetto, chiamato debate qui, e la classificazione binaria di espansione e confronto di PDB, poiché queste due sono strettamente correlate alla concezione di consensus e dissonanza e chiamate CE qui. Abbiamo scoperto che il trasferimento di prestazioni zero-scarica sul dataset annotato è molto migliore del caso con le migliori prestazioni con AUC del 62. Ulteriormente, dopo aver iteratamente finetunato su entrambe le tasse, abbiamo scoperto che finetunare la tassina CE seguita da ulteriori finetunings su debate porta a prestazioni migliori zero-scariche. Di conseguenza, questo è il modello che utilizziamo per iniziare l'apprendimento attivo. Successivamente, ho determinato il miglior metodo per aggiornare un modello con nuovi dati da ogni ronda di apprendimento attivo e annotazione. L'accumulatore accumula tutti i dati raccolti finora durante le annotazioni attive, mentre l'iterativa aggiorna il modello addestrando su ultimi set di dati raccolti. Sono stati testati diversi strategie e si è scoperto che l'accumulatore ha performato allo stesso o meglio dell'iterativa in tutto. Infine, per migliorare il numero di esempi di dissonanza, ho utilizzato la strategia di probabilità di classe rara (PRC) per selezionare maggiormente gli esempi che hanno alta probabilità di essere dissonanti in base al modello corrente in qualsiasi ronda di apprendimento attivo. Abbiamo confrontato questa strategia con altre strategie di apprendimento attivo state-of-the-art comuni nella comunità. Abbiamo scoperto che la nostra strategia PRC funziona meglio di altre strategie state-of-the-art, anche se la differenza è piccola. Notate che le prestazioni sono significativamente più basse per il caso random. In successive ronde di apprendimento attivo con le due migliori strategie, abbiamo migliorato la classificazione AUC a 75, che è la migliore prestazione che abbiamo ottenuto su questa tassa fino ad ora. Abbiamo anche controllato la fattibilità di ogni strategia per la qualità e i costi per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori ritengono che gli esempi siano più difficili. In sintesi, abbiamo scoperto che PRC è una strategia di apprendimento attivo semplice per la raccolta di esempi di classe rara e che l'apprendimento attivo con un appropriate design di apprendimento passivo aiuta significativamente. Abbiamo anche scoperto che l'aggiornamento iterativo è utile per l'apprendimento passivo da un dominio diverso, ma che le annotazioni in domino attive benefittono dalla aggiornatura cumulativa. Questi sono i link alla nostra raccolta di dati e all'articolo. Sentitevi liberi di contattarci se avete qualche domanda. Grazie.</sample>
    <sample id="356">I fornitori di informazione non hanno fornito informazioni specifiche sulle affiliazioni degli autori dell'articolo.</sample>
    <sample id="357">La relatrice o il relatore è Cui Yuan.</sample>
    <sample id="358">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="359">Il modello è confrontato con le strategie di traduzione offline, come la strategia Whitaker e la strategia locale di accumulo.</sample>
    <sample id="361">The speaker, Armine Norbarch, is a PhD student at Carnegie Mellon University and a research director at the JP Morgan AI Research Team. They are presenting their work on CounterComp, which focuses on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. The goal of multi-step quantitative reasoning is to answer questions about data in financial tables by executing one or more arithmetic operations. Current state-of-the-art neural models struggle with these tasks, especially when the output has more than two steps. To address this, the speaker proposes a method that uses auxiliary metric learning loss to encourage the model to attend to meaningful tokens during training. This approach improves performance on both in-distribution and out-of-distribution samples, demonstrating the effectiveness of CounterComp in enhancing compositional generalization.</sample>
  </task>
</testset>