<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="de">
    <sample id="0">Die wichtigsten Datenquellen für Sprachmodelle sind die Datensammlungen, die von den Autoren erstellt wurden.</sample>
    <sample id="1">McGill University, Mila</sample>
    <sample id="2">DEPLAIN: Ein deutsches Parallelkorpus mit intralingualen Übersetzungen in eine einfachere Sprache für Satz- und Dokumenteinfachfektion</sample>
    <sample id="3">DEPLAIN: Ein deutscher Parallelkorpus mit intralingualen Übersetzungen in eine einfachere Sprache für Satz- und Dokumentsimplifikation

Regina Stodden, Omar Momen, Laura Kallmeyer

Heinrich-Heine-Universität Düsseldorf, Deutschland

ACL 2023</sample>
    <sample id="4">Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne oder mehr Urlaub werden.</sample>
    <sample id="5">Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne oder mehr Urlaub geändert werden.</sample>
    <sample id="6">Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne oder mehr Urlaub werden.</sample>
    <sample id="7">Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne oder mehr Urlaub geändert werden.</sample>
    <sample id="8">2. DE-plain
2.1 A New Corpus</sample>
    <sample id="9">German Text Simplification Corpora

Sentence Level

Die Grafik zeigt die Anzahl der simplifizierten Sätze in verschiedenen Datensätzen. Der Datensatz "Parallel" enthält 12000 simplifizierter Sätze, der Datensatz "Parallel + 2015" enthält 13200 simplifizierter Sätze, der Datensatz "Parallel + 2016" enthält 14500 simplifizierter Sätze, der Datensatz "Parallel + 2017" enthält 15800 simplifizierter Sätze, der Datensatz "Parallel + 2018" enthält 17100 simplifizierter Sätze, der Datensatz "Parallel + 2019" enthält 18400 simplifizierter Sätze, der Datensatz "Parallel + 2020" enthält 19700 simplifizierter Sätze, der Datensatz "Parallel + 2021" enthält 21000 simplifizierter Sätze, der Datensatz "Parallel + 2022" enthält 22300 simplifizierter Sätze, der Datensatz "Parallel + 2023" enthält 23600 simplifizierter Sätze, der Datensatz "Parallel + 2024" enthält 24900 simplifizierter Sätze, der Datensatz "Parallel + 2025" enthält 26200 simplifizierter Sätze, der Datensatz "Parallel + 2026" enthält 27500 simplifizierter Sätze, der Datensatz "Parallel + 2027" enthält 28800 simplifizierter Sätze, der Datensatz "Parallel + 2028" enthält 30100 simplifizierter Sätze, der Datensatz "Parallel + 2029" enthält 31400 simplifizierter Sätze, der Datensatz "Parallel + 2030" enthält 32700 simplifizierter Sätze, der Datensatz "Parallel + 2031" enthält 34000 simplifizierter Sätze, der Datensatz "Parallel + 2032" enthält 35300 simplifizierter Sätze, der Datensatz "Parallel + 2033" enthält 36600 simplifizierter Sätze, der Datensatz "Parallel + 2034" enthält 37900 simplifizierter Sätze, der Datensatz "Parallel + 2035" enthält 39200 simplifizierter Sätze, der Datensatz "Parallel + 2036" enthält 40500 simplifizierter Sätze, der Datensatz "Parallel + 2037" enthält 41800 simplifizierter Sätze, der Datensatz "Parallel + 2038" enthält 43100 simplifizierter Sätze, der Datensatz "Parallel + 2039" enthält 44400 simplifizierter Sätze, der Datensatz "Parallel + 2040" enthält 45700 simplifizierter Sätze, der Datensatz "Parallel + 2041" enthält 47000 simplifizierter Sätze, der Datensatz "Parallel + 2042" enthält 48300 simplifizierter Sätze, der Datensatz "Parallel + 2043" enthält 49600 simplifizierter Sätze, der Datensatz "Parallel + 2044" enthält 50900 simplifizierter Sätze, der Datensatz "Parallel + 2045" enthält 52200 simplifizierter Sätze, der Datensatz "Parallel + 2046" enthält 53500 simplifizierter Sätze, der Datensatz "Parallel + 2047" enthält 54800 simplifizierter Sätze, der Datensatz "Parallel + 2048" enthält 56100 simplifizierter Sätze, der Datensatz "Parallel + 2049" enthält 57400 simplifizierter Sätze, der Datensatz "Parallel + 2050" enthält 58700 simplifizierter Sätze, der Datensatz "Parallel + 2051" enthält 60000 simplifizierter Sätze, der Datensatz "Parallel + 2052" enthält 61300 simplifizierter Sätze, der Datensatz "Parallel + 2053" enthält 62600 simplifizierter Sätze, der Datensatz "Parallel + 2054" enthält 63900 simplifizierter Sätze, der Datensatz "Parallel + 2055" enthält 65200 simplifizierter Sätze, der Datensatz "Parallel + 2056" enthält 66500 simplifizierter Sätze, der Datensatz "Parallel + 2057" enthält 67800 simplifizierter Sätze, der Datensatz "Parallel + 2058" enthält 69100 simplifizierter Sätze, der Datensatz "Parallel + 2059" enthält 70400 simplifizierter Sätze, der Datensatz "Parallel + 2060" enthält 71700 simplifizierter Sätze, der Datensatz "Parallel + 2061" enthält 73000 simplifizierter Sätze, der Datensatz "Parallel + 2062" enthält 74300 simplifizierter Sätze, der Datensatz "Parallel + 2063" enthält 75600 simplifizierter Sätze, der Datensatz "Parallel + 2064" enthält 76900 simplifizierter Sätze, der Datensatz "Parallel + 2065" enthält 78200 simplifizierter Sätze, der Datensatz "Parallel + 2066" enthält 79500 simplifizierter Sätze, der Datensatz "Parallel + 2067" enthält 80800 simplifizierter Sätze, der Datensatz "Parallel + 2068" enthält 82100 simplifizierter Sätze, der Datensatz "Parallel + 2069" enthält 83400 simplifizierter Sätze, der Datensatz "Parallel + 2070" enthält 84700 simplifizierter Sätze, der Datensatz "Parallel + 2071" enthält 86000 simplifizierter Sätze, der Datensatz "Parallel + 2072" enthält 87300 simplifizierter Sätze, der Datensatz "Parallel + 2073" enthält 88600 simplifizierter Sätze, der Datensatz "Parallel + 2074" enthält 89900 simplifizierter Sätze, der Datensatz "Parallel + 2075" enthält 91200 simplifizierter Sätze, der Datensatz "Parallel + 2076" enthält 92500 simplifizierter Sätze, der Datensatz "Parallel + 2077" enthält 93800 simplifizierter Sätze, der Datensatz "Parallel + 2078" enthält 95100 simplifizierter Sätze, der Datensatz "Parallel + 2079" enthält 96400 simplifizierter Sätze, der Datensatz "Parallel + 2080" enthält 97700 simplifizierter Sätze, der Datensatz "Parallel + 2081" enthält 99000 simplifizierter Sätze, der Datensatz "Parallel + 2082" enthält 100300 simplifizierter Sätze, der Datensatz "Parallel + 2083" enthält 101600 simplifizierter Sätze, der Datensatz "Parallel + 2084" enthält 102900 simplifizierter Sätze, der Datensatz "Parallel + 2085" enthält 104200 simplifizierter Sätze, der Datensatz "Parallel + 2086" enthält 105500 simplifizierter Sätze, der Datensatz "Parallel + 2087" enthält 106800 simplifizierter Sätze, der Datensatz "Parallel + 2088" enthält 108100 simplifizierter Sätze, der Datensatz "Parallel + 2089" enthält 109400 simplifizierter Sätze, der Datensatz "Parallel + 2090" enthält 110700 simplifizierter Sätze, der Datensatz "Parallel + 2091" enthält 112000 simplifizierter Sätze, der Datensatz "Parallel + 2092" enthält 113300 simplifizierter Sätze, der Datensatz "Parallel + 2093" enthält 114600 simplifizierter Sätze, der Datensatz "Parallel + 2094" enthält 115900 simplifizierter Sätze, der Datensatz "Parallel + 2095" enthält 117200 simplifizierter Sätze, der Datensatz "Parallel + 2096" enthält 118500 simplifizierter Sätze, der Datensatz "Parallel + 2097" enthält 119800 simplifizierter Sätze, der Datensatz "Parallel + 2098" enthält 121100 simplifizierter Sätze, der Datensatz "Parallel + 2099" enthält 122400 simplifizierter Sätze, der Datensatz "Parallel + 2010" enthält 123700 simplifizierter Sätze, der Datensatz "Parallel + 2011" enthält 125000 simplifizierter Sätze, der Datensatz "Parallel + 2012" enthält 126300 simplifizierter Sätze, der Datensatz "Parallel + 2013" enthält 127600 simplifizierter Sätze, der Datensatz "Parallel + 2014" enthält 128900 simplifizierter Sätze, der Datensatz "Parallel + 2015" enthält 130200 simplifizierter Sätze, der Datensatz "Parallel + 2016" enthält 131500 simplifizierter Sätze, der Datensatz "Parallel + 2017" enthält 132800 simplifizierter Sätze, der Datensatz "Parallel + 2018" enthält 134100 simplifizierter Sätze, der Datensatz "Parallel + 2019" enthält 135400 simplifizierter Sätze, der Datensatz "Parallel + 2020" enthält 136700 simplifizierter Sätze, der Datensatz "Parallel + 2021" enthält 138000 simplifizierter Sätze, der Datensatz "Parallel + 2022" enthält 139300 simplifizierter Sätze, der Datensatz "Parallel + 2023" enthält 140600 simplifizierter Sätze, der Datensatz "Parallel + 2024" enthält 141900 simplifizierter Sätze, der Datensatz "Parallel + 2025" enthält 143200 simplifizierter Sätze, der Datensatz "Parallel + 2026" enthält 144500 simplifizierter Sätze, der Datensatz "Parallel + 2027" enthält 145800 simplifizierter Sätze, der Datensatz "Parallel + 2028" enthält 147100 simplifizierter Sätze, der Datensatz "Parallel + 2029" enthält 148400 simplifizierter Sätze, der Datensatz "Parallel + 2030" enthält 149700 simplifizierter Sätze, der Datensatz "Parallel + 2031" enthält 151000 simplifizierter Sätze, der Datensatz "Parallel + 2032" enthält 152300 simplifizierter Sätze, der Datensatz "Parallel + 2033" enthält 153600 simplifizierter Sätze, der Datensatz "Parallel + 2034" enthält 154900 simplifizierter Sätze, der Datensatz "Parallel + 2035" enthält 156200 simplifizierter Sätze, der Datensatz "Parallel + 2036" enthält 157500 simplifizierter Sätze, der Datensatz "Parallel + 2037" enthält 158800 simplifizierter Sätze, der Datensatz "Parallel + 2038" enthält 160100 simplifizierter Sätze, der Datensatz "Parallel + 2039" enthält 161400 simplifizierter Sätze, der Datensatz "Parallel + 2040" enthält 162700 simplifizierter Sätze, der Datensatz "Parallel + 2041" enthält 164000 simplifizierter Sätze, der Datensatz "Parallel + 2042" enthält 165300 simplifizierter Sätze, der Datensatz "Parallel + 2043" enthält 166600 simplifizierter Sätze, der Datensatz "Parallel + 2044" enthält 167900 simplifizierter Sätze, der Datensatz "Parallel + 2045" enthält 169200 simplifizierter Sätze, der Datensatz "Parallel + 2046" enthält 170500 simplifizierter Sätze, der Datensatz "Parallel + 2047" enthält 171800 simplifizierter Sätze, der Datensatz "Parallel + 2048" enthält 173100 simplifizierter Sätze, der Datensatz "Parallel + 2049" enthält 174400 simplifizierter Sätze, der Datensatz "Parallel + 2050" enthält 175700 simplifizierter Sätze, der Datensatz "Parallel + 2051" enthält 177000 simplifizierter Sätze, der Datensatz "Parallel + 2052" enthält 178300 simplifizierter Sätze, der Datensatz "Parallel + 2053" enthält 179600 simplifizierter Sätze, der Datensatz "Parallel + 2054" enthält 180900 simplifizierter Sätze, der Datensatz "Parallel + 2055" enthält 182200 simplifizierter Sätze, der Datensatz "Parallel + 2056" enthält 183500 simplifizierter Sätze, der Datensatz "Parallel + 2057" enthält 184800 simplifizierter Sätze, der Datensatz "Parallel + 2058" enthält 186100 simplifizierter Sätze, der Datensatz "Parallel + 2059" enthält 187400 simplifizierter Sätze, der Datensatz "Parallel + 2060" enthält 188700 simplifizierter Sätze, der Datensatz "Parallel + 2061" enthält 190000 simplifizierter Sätze</sample>
    <sample id="10">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2005 2009 2013 2015 Parallel Parallel Parallel Parallel Parallel Parallel English German English German English German English German English German</sample>
    <sample id="11">German Text Simplification Corpora

Sentence Level

14000
12000
10000
8000
6000
4000
2000
0
Parallel Parallel Parallel Parallel Parallel Parallel
Translation Transliteration Translation Translation
(Deutsch-Englisch) (Deutsch-Englisch) (Deutsch-Englisch) (Deutsch-Englisch) (Deutsch-Englisch)
(Parallel) (Parallel) (Parallel) (Parallel) (Parallel)
2013 2013 2013 2013 2013</sample>
    <sample id="12">German Text Simplification Corpora

Sentence Level

14000
12000
10000
8000
6000
4000
2000
0
Parallel (De-En, 2015)
Parallel (De-En, 2013)
Parallel (De-En, 2011)
Parallel (De-En, 2009)
Parallel (De-En, 2007)
Parallel (De-En, 2005)
Parallel (De-En, 2003)
Parallel (De-En, 2001)
Parallel (De-En, 1999)
Parallel (De-En, 1997)
Parallel (De-En, 1995)
Parallel (De-En, 1993)
Parallel (De-En, 1991)
Parallel (De-En, 1989)
Parallel (De-En, 1987)
Parallel (De-En, 1985)
Parallel (De-En, 1983)
Parallel (De-En, 1981)
Parallel (De-En, 1979)
Parallel (De-En, 1977)
Parallel (De-En, 1975)
Parallel (De-En, 1973)
Parallel (De-En, 1971)
Parallel (De-En, 1969)
Parallel (De-En, 1967)
Parallel (De-En, 1965)
Parallel (De-En, 1963)
Parallel (De-En, 1961)
Parallel (De-En, 1959)
Parallel (De-En, 1957)
Parallel (De-En, 1955)
Parallel (De-En, 1953)
Parallel (De-En, 1951)
Parallel (De-En, 1949)
Parallel (De-En, 1947)
Parallel (De-En, 1945)
Parallel (De-En, 1943)
Parallel (De-En, 1941)
Parallel (De-En, 1939)
Parallel (De-En, 1937)
Parallel (De-En, 1935)
Parallel (De-En, 1933)
Parallel (De-En, 1931)
Parallel (De-En, 1929)
Parallel (De-En, 1927)
Parallel (De-En, 1925)
Parallel (De-En, 1923)
Parallel (De-En, 1921)
Parallel (De-En, 1919)
Parallel (De-En, 1917)
Parallel (De-En, 1915)
Parallel (De-En, 1913)
Parallel (De-En, 1911)
Parallel (De-En, 1909)
Parallel (De-En, 1907)
Parallel (De-En, 1905)
Parallel (De-En, 1903)
Parallel (De-En, 1901)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (De-En, 1900)
Parallel (</sample>
    <sample id="13">German Text Simplification Corpora

Sentence Level

14000
12000
10000
8000
6000
4000
2000
0
Parallel Parallel Parallel Parallel Parallel Parallel
(Deutsch-Englisch) (Englisch-Deutsch) (Deutsch-Französisch) (Französisch-Deutsch) (Deutsch-Italienisch) (Italienisch-Deutsch)
Translation Translated Translated Translated Translated Translated
Corpus Corpus Corpus Corpus Corpus Corpus
(Parallel Corpora) (Parallel Corpora) (Parallel Corpora) (Parallel Corpora) (Parallel Corpora) (Parallel Corpora)
Manual Manual Manual Manual Manual Manual
Alignment Alignment Alignment Alignment Alignment Alignment
(Alignment Alignment Alignment Alignment Alignment Alignment
Methods Methods Methods Methods Methods Methods
(Parallel Corpora) (Parallel Corpora) (Parallel Corpora) (Parallel Corpora) (Parallel Corpora) (Parallel Corpora)
Figure 1: Size of the German text simplification corpora</sample>
    <sample id="14">Typen der Simplifikation

Simplizität
LexSimp
StructSimp

News
Bible
Fiction</sample>
    <sample id="15">Typen der Simplifikation

Die Grafik zeigt die Häufigkeit von Typen der Simplifikation in drei Kategorien: News, Bibel und Fiktion. Es gibt zwei Arten von Simplifikation: LexSimp und StructSimp. LexSimp beinhaltet Reorganisierungen, Reduplikationen, Substitutionen, Word-Additionen und Word-Deletions. StructSimp beinhaltet Reorganisierungen, Reduplikationen, Substitutionen, Word-Additionen und Word-Deletions.</sample>
    <sample id="16">Typen der Simplifikation

Die Grafik zeigt die Häufigkeit von Typen der Simplifikation in drei Kategorien: News, Bibel und Fiktion. Es werden zwei Arten von Simplifikationen untersucht: LexSimp und StructSimp. LexSimp beinhaltet Reorganisierungen, Rephrasing, lexikalische Substitutionen, Wort Addition und Wort Deletion. StructSimp hingegen beinhaltet Reorganisierungen, Rephrasing, lexikalische Substitutionen, Wort Addition und Wort Deletion.

Simplifikationstransformationen

Die Grafik zeigt auch die Häufigkeit von Simplifikationstransformationen in zwei Kategorien: DEPLAIN-APA und DEPLAIN-WEB. Es werden verschiedene Arten von Simplifikationstransformationen untersucht: Reorganisierungen, Rephrasing, lexikalische Substitutionen, Wort Addition und Wort Deletion.</sample>
    <sample id="17">Typen der Simplifikation

Die Grafik zeigt die Häufigkeit von Typen der Simplifikation in verschiedenen Texten. Es werden drei Arten von Simplifikationen dargestellt: Simplicity, LexSimp und StructSimp. Die Grafik zeigt, dass die Häufigkeit der Simplifikationen variiert, je nach Texttyp. Zum Beispiel sind die Häufigkeiten von Simplicity in den Newsartikel文本较低，während sie in den Bibel文本较高。LexSimp und StructSimp Häufigkeiten variieren ähnlich.

Simplifikations Transformationen

Die Grafik zeigt auch die Häufigkeit von SimplifikationsTransformationen in verschiedenen Texten. Es werden zwei Arten von SimplifikationsTransformationen dargestellt: DEPLAIN-apa und DEPLAIN-web. Die Grafik zeigt, dass die Häufigkeiten von SimplifikationsTransformationen variiert, je nach Texttyp. Zum Beispiel sind die Häufigkeiten von DEPLAIN-apa in den Newsartikel文本较低，während sie in den Bibel文本较高。DEPLAIN-web Häufigkeiten variieren ähnlich.</sample>
    <sample id="18">Typen der Simplifizierung

Die Grafik zeigt die Häufigkeit von Typen der Simplifizierung in drei verschiedenen Textsammlungen: "news", "bible" und "fiction". Es werden zwei Arten von Simplifizierung verwendet: "LexSimp" und "StructSimp". LexSimp bezieht sich auf die Reduktion von Lexikalischen Komplexitäten, während StructSimp sich auf die Reduzierung struktureller Komplexitäten konzentriert. In der "news" Sammlung sind die Häufigkeiten von LexSimp und StructSimp nahezu gleich, während sie in der "bible" Sammlung stark variieren. In der "fiction" Sammlung sind die Häufigkeiten von LexSimp und StructSimp relativ hoch.

Simplifizierungsoperationen

Die Grafik zeigt auch die Häufigkeiten von verschiedenen Simplifizierungsoperationen, einschließlich "word deletion", "word substitution", "word addition" und "word reordering". Es werden zwei Arten von Simplifizierungsoperationen verwendet: "DEplain-apa" und "DEplain-web". DEplain-apa bezieht sich auf die Simplifizierungsoperationen, die in der American Psychological Association (APA) Stilguide verwendet werden, während DEplain-web sich auf die Simplifizierungsoperationen konzentriert, die in Webseiten verwendet werden. In der Grafik werden die Häufigkeiten von DEplain-apa und DEplain-web für jede Simplifizierungsoperation gezeigt.</sample>
    <sample id="19">3. Use-cases
Automatische Ausrichtung und Simplifizierung</sample>
    <sample id="20">Ergebnisse der Auswertung der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:n-Competencies (untere Hälfte)

Name

Beschreibung

P

R

F₁

P

R

F₁

111

960

444

780

900

444

607

779

247

553

342

276

847

465

610

733

819

387

628

730</sample>
    <sample id="21">Ergebnisse der Alignmentsmethoden mit 1:1 (oberes Teil) und n:n Kapazitäten (unteres Teil)

Name | Beschreibung | 111 | 11m | 11m |
--- | --- | --- | --- | --- |
LUA | | 847 | 628 | 730 |
Sent-LaBSE | | 960 | 607 | 779 |
Sent-RoBERTa | | 960 | 607 | 779 |
CATS-C3G | | 247 | 353 | 342 |
WMT | | 131 | 465 | 465 |
BERTalign | | 743 | 387 | 561 |
TF-IDFalign | | 846 | 477 | 819 |</sample>
    <sample id="22">Ergebnisse der Auswertung der Ausrichtungsmethoden mit 1:1 (obere Hälfte) und n:n-Möglichkeiten (untere Hälfte)

Name

Beschreibung

P

R

F₁

P₀,5

R₀,5

F₀,5

LIA

Metrischer Ausrichtungsalgorithmus unter Verwendung von Satz embeddings

111

841

771

780

960

444

607

779

Sent-LaBSE

Ähnlich embeddings von Cross English &amp;amp; German BERTa

247

353

342

276

247

353

276

CATS-C3G

Verschiedene Ähnlichkeitsmaßes e. g. n-grams, Word vectors

841

771

780

960

444

607

779

BERTalign

Ermöglicht sentence-transformer Methoden zur Produktion von rm Ausrichtungen

743

465

561

387

561

465

412

Vowpalwabbit

Eine naiv-geführte Ansatz mit einem TF-IDF Ähnlichkeitsmatrix

846

477

610

733

819

628

730</sample>
    <sample id="23">Ergebnisse der Auswertung der Ausrichtungsmethoden mit 1:1 (oberes Teil) und n:n-Möglichkeiten (unteres Teil)

Name | Beschreibung | 111 | 11m | 1m1 | R | F0.5 | F0.5
--- | --- | --- | --- | --- | --- | --- | ---
Sent-LaBSE | Similar embeddings of Language Model BERT Transformer | 841 444 780 | 960 444 607 | 779 | 733 | 628 | 730
Sent-RoBERTa | Similar embeddings of Cross English &amp;amp; German RoBERTa | 247 553 342 | 276 |  |  |  | 
CATS-C3G | Different similarity measures e.g. grams, word vectors |  |  |  |  |  | 
W2V | Word2Vec embeddings |  |  |  |  |  | 
BERTalign | Allows sentence-transformer methods produce n:m alignments | 743 465 610 | 733 | 561 | 628 | 730
TF-IDFalign | A vicinity-driven approach with a TF-IDF similarity matrix | 846 477 733 | 819 | 387 | 628 | 730</sample>
    <sample id="24">Ergebnisse der Auswertung von Alignmentsmethoden mit 1:1 (oberes Teil) und n:n Kapazitäten (unteres Teil)

| Name | Beschreibung | P | R | F0.5 |
| --- | --- | --- | --- | --- |
| Sent-LaBSE | Similar embeddings of Language Model BERT | 847 | 444 | 780 |
| Sent-RoBERTa | Similar embeddings of Cross English &amp;amp; German RoBERTa | 960 | 444 | 607 |
| CATS-C3G | Different similarity measures e.g. grams - word vectors | 247 | 553 | 342 |
| W2V | Word2Vec embeddings | 131 | 477 | 371 |
| BERTalign | Allows sentence-transformer methods produce n:m alignments | 743 | 465 | 610 |
| TF-IDFalign | A vicinity-driven approach with a TF-IDF similarity matrix | 846 | 477 | 628 |

**Bemerkungen:** 

* "P" steht für diePrecision, "R" steht für dieRecall und "F0.5" steht für dieF0.5-Score, die eine Kombination von Precision und Recall ist.
* "n:m" steht für die Anzahl der n Quelltexte, die auf m Zieltexte abgebildet werden.</sample>
    <sample id="25">Ergebnisse der Auswertung der Ausrichtungsmethoden mit 1:1 (Oberes Teil) und n:n Kapazitäten (Unterem Teil)

Name

Beschreibung

P

R

F0.5

111

n2m

P

R

F0.5

111

n2m

LiDA

Metrischer Ausrichtungsalgorithmus unter Verwendung von Satz embeddings

961 444 780

960 444 779

900 444 607

Sent-LaBSE

Ähnlichkeiten von Satz embeddings von Language Model BERT Transformer

247 553 342

278 465 333

285 477 285

Sent-RoBERTa

Ähnlichkeiten von Cross English &amp;amp; German RoBERTa

247 553 342

278 465 333

285 477 285

CATS-C3G

Verschiedene Ähnlichkeitsmaßes e-grams, Word embeddings

247 553 342

278 465 333

285 477 285

WASABI

Metrischer Ausrichtungsalgorithmus unter Verwendung von TF-IDF

846 477 628

819 471 612

730 459 612

BERTalign

Ermöglicht Satz-transformer-Methoden die Produktion von n:m Ausrichtungen

743 465 387

733 465 351

704 459 349

Vogelalign

Einer naiv-geführte Ansatz mit einem TF-IDF Ähnlichkeitsmatrix

961 444 780

960 444 779

900 444 607</sample>
    <sample id="26">Ergebnisse der Auswertung von Alignmentsmethoden mit 1:1 (oberes Teil) und n:n Kapazitäten (unteres Teil)

| Name | Beschreibung | P@1 | R@1 | F@1 | P@5 | R@5 | F@5 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Sent-LaBSE | Similar embeddings of Language Model transformer | 84.1 | 444 | 780 | 960 | 444 | 779 |
| Sent-RoBERTa | Similar embeddings of Cross English &amp;amp; German RoBERTa | 24.7 | 353 | 342 | 276 | 465 | 333 |
| CATS-C3G | Different similarity measures e.g. grams, word vectors | 14.1 | 342 | 342 | 14.1 | 465 | 333 |
| BERTalign | Allows sentence-transformer methods produce n:m alignments | 74.3 | 465 | 610 | 733 | 387 | 459 |
| VowAlign | A vicinity-driven approach with a TF-IDF similarity matrix | 84.6 | 477 | 710 | 819 | 628 | 730 |</sample>
    <sample id="27">Ergebnisse derDOCUMENTSIMPLIFIKATION mit einem finetuned long-mBART. n entspricht der Länge der Ausbildungsdaten. train data BLEU P-Pr. BLEU SARI BLEU Baseline 17327 34,247 56,555 34,857 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555 34,857 svrc-baseline 17327 34,247 56,555</sample>
    <sample id="28">Ergebnisse derDOCUMENTSIMPLIFIKATION mit einem finetuned long-mBART auf der Trainingsdatenmenge der DE-Deutschland.</sample>
    <sample id="29">Ergebnisse derDOCUMENTSIMPLIFIKATION mit einem finetuned long-mBART auf der Trainingsdatenmenge der DEPLAIN-APL-Web-Basise (n=48) und der DEPLAIN-Web-Basise (n=147).</sample>
    <sample id="30">Ergebnisse derDOCUMENTSIMPLIFIKATION mit einem finetuned long-mBART. n entspricht der Länge der Ausbildungsdaten. train data BLEU SARI BP-Score DELplain-APL-w 481 30,06 29,13 65,65 DELplain-APL-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65,65 DELplain-w 481 30,06 29,13 65</sample>
    <sample id="31">Ergebnisse derDOCUMENTSIMPLIFIKATION mit einem finetuned长MBART.</sample>
    <sample id="32">Ergebnisse derDOCUMENTSIMPLIFIKATION mit dem finetuned-long-mBART. n entspricht der Länge des Trainingsdatensatzes. Drehpunkt-basiertes Modell Baseline 17327 34,247 56,55 34,247 56,55 DEPLAN-APL WEB 17327 34,247 56,55 34,247 56,55 DEPLAN-APL-Test (n=48) 17327 34,247 56,55 34,247 56,55 ... DEPLAN-APL WEB-Test (n=147) 17327 34,247 56,55 34,247 56,55 ... DEPLAN-APL-Test (n=123) 17327 34,247 56,55 34,247 56,55 DEPLAN-APL-Test (n=1840) 17327 34,247 56,55 34,247 56,55</sample>
    <sample id="33">Ergebnisse derDOCUMENTSIMPLIFIKATION mit dem finetuned长mBART auf der Trainingsdatenbank. n entspricht der Länge der Trainingsdaten.  Train data BLEU SARI- BLEU BS-P BLEU Baseline 17327 34,247 65,555 90,85 svicor-Baseline 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA-Test (n=48) 17327 34,247 65,555 90,85 svicor-APA</sample>
    <sample id="34">Danke.Für weitere Details. Bitte überprüfen Sie unser Papier.Und fühlen Sie sich frei, unser Poster auf der ACL 2023 Konferenz zu besuchen.</sample>
    <sample id="35">Patrick Fernandes</sample>
    <sample id="36">Das T5 XL Modell wurde verwendet, um die Genauigkeit von 82–87 % zu erreichen.</sample>
    <sample id="37">Ja, sie funktioniieren immer noch.</sample>
    <sample id="38">Die vorgeschlagene menschliche Bewertungsmethode ist neu, da sie die Relevanz der Bots' Antworten bewertet.</sample>
    <sample id="39">Der Erfolg des bestehenden schwach überwachten Ansatzes hängt von den Daten ab.</sample>
    <sample id="40">Das Resultat kann durch die Annotatoren verbessert werden, indem sie zumindest einige Lieder von Each Song hören und über Each Song informieren.</sample>
    <sample id="41">5</sample>
    <sample id="42">Verknüpfte Längen in englischer Sprache, Minimierung der Abhängigkeitslänge und Struktur der Koordination</sample>
    <sample id="43">Dependency Structure of Coordination

- Bouquet/Stanford (Universal Dependencies):
  Homer loves Lisa, Bart, und Maggie.
  
- Chain/Moscow:
  Homer loves Lisa, Bart, und Maggie.
  
- Conjunction-headed/Praague:
  Homer loves Lisa, Bart, und Maggie.
  
- Multi-headed/London:
  Homer loves Lisa, Bart, und Maggie.</sample>
    <sample id="44">Dependency Structure of Coordination

- Bouquet/Stanford (Universal Dependencies):
  Homer loves Lisa, Bart, and Maggie.
  
- Chain/Moscow:
  Homer loves Lisa, Bart, and Maggie.
  
- Conjunction-headed/Praague:
  Homer loves Lisa, Bart, and Maggie.
  
- Multi-headed/London:
  Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="45">Dependenzstruktur der Koordination

- Bouquet/Stanford (Universal Dependencies):
  Homer liebt Lisa, Bart und Maggie.
  
- Chain/Moscow:
  Homer liebt Lisa, Bart und Maggie.
  
- Conjunction-headed/Praague:
  Homer liebt Lisa, Bart und Maggie.
  
- Multi-headed/London:
  Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="46">Dependenzstruktur der Koordination

- Bouquet/Stanford (Universal Dependencies):
  Homer liebt Lisa, Bart und Maggie.
  
- Chain/Moscow:
  Homer liebt Lisa, Bart und Maggie.
  
- Conjunction-headed/Praague:
  Homer liebt Lisa, Bart und Maggie.
  
- Multi-headed/London:
  Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="47">Dependenzstruktur der Koordination

• Bouquet/Stanford (Universal Dependencies):

Homer loves Lisa, Bart, und Maggie.

• Chain/Moscow:

Homer loves Lisa, Bart, und Maggie.

• Conjunction-headed/Praague:

Homer loves Lisa, Bart, und Maggie.

• Multi-headed/London:

Homer loves Lisa, Bart, und Maggie.</sample>
    <sample id="48">Dependenzstruktur der Koordination

- Bouquet/Stanford (Universal Dependencies):
  Homer liebt Lisa, Bart und Maggie.
  
- Chain/Moscow:
  Homer liebt Lisa, Bart und Maggie.
  
- Conjunction-headed/Praague:
  Homer liebt Lisa, Bart und Maggie.
  
- Multi-headed/London:
  Homer liebt Lisa, Bart und Maggie.</sample>
    <sample id="49">Dependency Structure of Coordination

- Bouquet/Stanford (Universal Dependencies):
  Homer loves Lisa, Bart, and Maggie.
  
- Chain/Moscow:
  Homer loves Lisa, Bart, and Maggie.
  
- Conjunction-headed/Prague:
  Homer loves Lisa, Bart, and Maggie.
  
- Multi-headed/London:
  Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="50">Dependency Structure of Coordination

- Bouquet/Stanford (Universal Dependencies):
  Homer loves Lisa, Bart, and Maggie.
  
- Chain/Moscow:
  Homer loves Lisa, Bart, and Maggie.
  
- Conjunction-headed/Praague:
  Homer loves Lisa, Bart, and Maggie.
  
- Multi-headed/London:
  Homer loves Lisa, Bart, and Maggie.</sample>
    <sample id="51">Word order tends to minimize dependency lengths:</sample>
    <sample id="52">Word order tends to minimize dependency lengths:</sample>
    <sample id="53">Word order tends to minimize dependency lengths:</sample>
    <sample id="54">Word order tends to minimize dependency lengths:</sample>
    <sample id="55">Word order tends to minimize dependency lengths:</sample>
    <sample id="56">Word order tends to minimize dependency lengths:</sample>
    <sample id="57">Word order tends to minimize dependency lengths:</sample>
    <sample id="58">Word order tends to minimize dependency lengths:</sample>
    <sample id="59">Word order tends to minimize dependency lengths:</sample>
    <sample id="60">Word order tends to minimize dependency lengths:</sample>
    <sample id="61">Word order tends to minimize dependency lengths:</sample>
    <sample id="62">Word order tends to minimize dependency lengths:</sample>
    <sample id="63">Statistiken über die Koordination wurden aus einer erweiterten Version von Penn Treebank (Marcus et al. 1993; Ficler und Berwick 2016) abgeleitet.
• Konjunktiv tendenziell kürzer zu sein als vorher (beobachtet wurde).
• Dieser Trend wird mit der Längstelle zunehmen.
• (Befindlich bemerkte Gibson et al. 1996: 88–90)
• Nur dann, wenn der Präsident links steht oder fehlt,
• (Ich sah Bart und Lisa: Homer kam und schnäuzte.)
• Nicht, wenn er rechts steht (Ted und Ned lachten).</sample>
    <sample id="64">Statistiken über Koordination wurden aus einer erweiterten Version der Penn Treebank (Marcus et al. 1993; Flicker und Goldberg 2016) abgeleitet.
• Linker Koordinationstrend zu kürzeren (beobachtet wurde, dass linker Koordinationstrend zu kürzeren geworden ist), wobei dieser Trend mit Differenz im Längen gewachsen ist (brevity notiz in Gibson et al. 1996: 88–90)
• Nur dann, wenn der Gouverneur links steht oder fehlt (ich sah Bart und Lisa Homer kommen und schreien),
• nicht, wenn er rechts steht (Ted und Ned lachten).</sample>
    <sample id="65">Statistiken über Koordination wurden aus einer erweiterten Version des Penn Treebank (Marcus et al. 1993; Flicker und Goldberg 2016) abgeleitet.

• Linker Koordinationstypen neigen dazu, kürzer zu sein als früher (beobachtet);
• Die Neigung wird mit der Längstelle zunehmen (beobachtet);
• (nur kurz erwähnt in Gibson et al. 1996: 88–90)

• Aber nur, wenn der Präsident links steht oder weg ist
(I sah Bart und Lisa Homer kommen und heulen);
• nicht, wenn er rechts steht (Ted und Ned lachten).</sample>
    <sample id="66">Statistiken über Koordination extrahiert aus einer erweiterten Version des Penn Treebank (Marcus et al. 1993; Fichter und Goldberg 2016). • Linke Conjuncts tendenzen zu kürzer (beobachtet wurde) • Mit der Längstunterschied wird dieser Trend immer stärker (Gibson et al. 1996: 88–90) • Aber nur, wenn die Regierung links steht oder weg ist (Ich sah Bart und Lisa: Homer kam und schnauzte) • Und nicht, wenn sie rechts steht (Ted und Ned lachten)</sample>
    <sample id="67">Statistiken über die Koordination wurden aus einer erweiterten Version von Penn Treebank (Marcus et al. 1993; Flicker und Goldberg 2016) abgeleitet:

• Linke Konjunktionen neigen dazu, kürzer zu sein (wie vorher beobachtet),
• diese Neigung wächst mit der Längstunterschied
(beflissen in Gibson et al. 1996: 88–90).

Aber nur, wenn die Regierung links steht oder weg ist
(Ich sah Bart und Lisa; Homer kam und spitzwinkelte)
nicht, wenn sie rechts steht (Ted und Ned lachten).</sample>
    <sample id="68">Statistiken über die Koordination wurden aus einer erweiterten Version der Penn Treebank (Marcus et al. 1993; Ficler und Goldberg 2016) abgeleitet:

• Linke Verknüpfungen tendieren dazu, kürzer zu sein (wie vorher beobachtet),
• diese Neigungen wachsen mit der Längstunterschied,
• (nur kurz angedeutet in Gibson et al. 1996: 88–90).

Aber nur, wenn die Oberfläche auf der linken Seite fehlt oder weggeschnitten wird (Ich sah Bart und Lisa; Homer kam und *schnäuzte*).

Eine nicht when es rechts ist (Ted und Ned lachten).</sample>
    <sample id="69">Statistiken über die Koordination wurden aus einer erweiterten Version des Penn Treebank (Marcus et al. 1993; Flicker und Goldberg 2016) abgeleitet:

- Linke Koordinaten tendieren dazu, kürzer zu sein als zuvor beobachtet wurde,
- Dieser Trend wird mit der Längstunterschieden intensiver,
- (Bemerkung von Gibson et al. 1996: 88–90).

Aber nur dann, wenn die Regierung links steht oder weg ist:
- (Ich sah Bart und Lisa; Homer kam und schnäuzte.)</sample>
    <sample id="70">Die Grafiken in der Abbildung 10 (Fig. 10) zeigen die Proportionen von kürzeren Konjunktionen abhängig von der absoluten Differenz der Komponentenlängen (mit Konfidenzbereichen). 

Die Grafiken werden in zwei Gruppen unterteilt: "NO governor in CHARACTERS" und "NO governor in SYLLABLES". Jede Gruppe enthält drei Grafiken, die die Proportionen von kürzeren Konjunktionen basierend auf verschiedenen Längsschwankungen der Komponenten anhand von verschiedenenGovernor-Längsschwankungen (in X) und Governor-Positionen (in Y) darstellen.

Die Grafiken in der oberen Gruppe (NO governor in CHARACTERS) zeigen eine lineare Beziehung zwischen den Governor-Positionen und den Proportionen von kürzeren Konjunktionen. Die Grafiken in der unteren Gruppe (NO governor in SYLLABLES) zeigen eine schwächeren linearen Bezug zwischen den Governor-Positionen und den Proportionen von kürzeren Konjunktionen.

Die Grafiken in der linken Spalte (Governor on the LEFT) und der rechten Spalte (Governor on the RIGHT) zeigen die Proportionen von kürzeren Konjunktionen basierend auf verschiedenenGovernor-Positionen (links oder rechts) und verschiedenenGovernor-Längsschwankungen (in X).

Die Grafiken in der untersten Zeile (Proportions of shorter conjuncts depending on the absolute difference of component lengths (with confidence bands)) zeigen die Proportionen von kürzeren Konjunktionen basierend auf verschiedenenGovernor-Längsschwankungen (in X) und Governor-Positionen (in Y) mit Konfidenzbereichen.

Die Grafiken in der Abbildung 10 (Fig. 10) helfen zu verstehen, wie sich die Proportionen von kürzeren Konjunktionen aufgrund derGovernor-Längsschwankungen undGovernor-Positionen verhalten.</sample>
    <sample id="71">Die Grafiken in Abbildung 1 ilustrieren die Proportionen von kürzeren Konkretien, die abhängig sind von der absoluten Differenz der Längen der Konkretien (in字符数、词元或词元) und dem Konfidenzintervall. Die Grafiken werden in zwei Teile unterteilt: "Governor on the LEFT" und "Governor on the RIGHT". Jede Grafik zeigt eine lineare Regression, die die Abhängigkeit zwischen den Proportionen kürzerer Konkretien und der absoluten Differenz der Längen der Konkretien visualisiert. Die x-Achse zeigt die absolute Differenz der Längen der Konkretien, und die y-Achse zeigt die Proportionen kürzerer Konkretien. Die Grafiken werden in Farben codiert, um die verschiedenen Größen der Konkretien zu kennzeichnen.</sample>
    <sample id="72">Die Grafiken in Abbildung 1 zeigen die Proportionen von kürzeren KonjunktivsatzFragmenten, die abhängig von der absoluten Differenz der Längen der KonjunktivsatzFragmente (in字符数、音节和词中) variieren. Die Grafiken werden in zwei Gruppen unterteilt: "Governor on the LEFT" und "Governor on the RIGHT". Jede Gruppe enthält drei Grafiken, die respektively die Proportionen der kürzeren KonjunktivsatzFragmente in 字符数、音节和词中 darstellen. Die Grafiken verwenden einen linearen Modus und einen 95%-Konfidenzbereich, um die Abhängigkeit zu visualisieren.</sample>
    <sample id="73">Die Grafiken in Abbildung 1 ilustrieren die Abhängigkeit der Proportionen von kürzeren KonjunktivsatzFragmenten von der absoluten Differenz der Längen der KonjunktivsatzFragmente. Die Grafiken werden in zwei Reihen aufgelistet, wobei die oberne Reihe die Proportionen von kürzeren KonjunktivsatzFragmenten im Vergleich zu den entsprechenden Longer KonjunktivsatzFragmenten zeigt, und die unterne Reihe die Proportionen von kürzeren KonjunktivsatzFragmenten im Vergleich zu den entsprechenden kürzeren KonjunktivsatzFragmenten im Vergleich zu den entsprechenden Longer KonjunktivsatzFragmenten zeigt. In allen Grafiken wird die absolute Differenz der Längen der KonjunktivsatzFragmente auf der x-Achse dargestellt, und die Proportionen von kürzeren KonjunktivsatzFragmenten auf der y-Achse. Die Grafiken zeigen, dass die Proportionen von kürzeren KonjunktivsatzFragmenten in Bezug auf die Längen der KonjunktivsatzFragmente steigen, je größer die absolute Differenz der Längen der KonjunktivsatzFragmente ist.</sample>
    <sample id="74">Siehe die Publikation für die vollständige Argumentation!</sample>
    <sample id="75">Drei.</sample>
    <sample id="76">Die Domänen "Bible" und "Fiction" werden stärker vereinfacht.</sample>
    <sample id="77">Read it yesterday.</sample>
    <sample id="78">Ja, die DrBERT-Modelle, der NACHOS-Datensatz und die Trainings-skripte sind unter der MIT-Lizenz frei zur Verfügung gestellt.</sample>
    <sample id="79">DEplain-apa enthält Dokumente aus dem Medizinischen Publikationsspeicher.</sample>
    <sample id="80">Eine gute Generalisierung wird durch bessere Modellarchitektur, größere Modellgröße und mehr Beispiele für die Anpassung erreicht.</sample>
    <sample id="81">Die Tendenz wurde gemessen, indem die Längenunterschiede zwischen linken und rechten Konjunktionen ermittelt wurden.</sample>
    <sample id="82">Die Experimente wurden so gestaltet, dass sie die Auswirkungen der Position des Begrenzers auf die Proportionen von Shifter- und Konstanten-Endungen untersuchten.</sample>
    <sample id="83">Nicht besser als Zufall.</sample>
    <sample id="84">4</sample>
    <sample id="85">The people in the example conversation are named John, Jane, and Peter.</sample>
    <sample id="86">Kontextsensitive MÜ-Modelle schneiden besser ab bei Diskursphänomenen, bei denen die Kontextualisierung von Texten relevant ist.</sample>
    <sample id="87">Die Autoren gehören an Johns Hopkins University, Purdue University und MIT.</sample>
    <sample id="122">Es quantifiziert die Positionalität durch die Messung der Korrelation zwischen dem Demographics-Spektro und dem Annotators-Spektro.</sample>
    <sample id="155">Es ist nicht in der gegebenen Bildtexte erwähnt.</sample>
    <sample id="156">Die Studie verwendet die National Longitudinal Survey of Youth (NLSY) und die National Longitudinal Survey of Youth 1979 (NLSY79).</sample>
    <sample id="157">Zwei Autoren sind an der Arbeit beteiligt.</sample>
    <sample id="158">Debatte, ICE</sample>
    <sample id="159">Zwei.</sample>
    <sample id="160">There are seven authors involved in the work.</sample>
    <sample id="161">Das vorgestellte Framework unterscheidet sich von seinen Vorgängern dadurch, dass es eine Methode zur Evaluierung von Datensammlungen basierend auf dem Vergleich von Annotierungen und Modellvorhersagen verwendet.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">None</sample>
    <sample id="164">Von der Vorbereitung von Datensätzen für Sprachmodelle bis hin zu Sprachmodellen, die die Spuren politischer Biases nachverfolgen</sample>
    <sample id="165">LM Training Data Ein gesenktes Segen A mixed blessing 10'000 1'000 100 10 1 1 www. wikipedia.org www. ms. wikipedia.com www. businessinsider.com www. theverge.com www. huffpost.com www. scifi.com www. cnet.com www. techcrunch.com www. springster.com www. bookstagram.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springster.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com www. springter.com</sample>
    <sample id="166">LM Training Data Ein gesenktes Segel A mixed blessing 10⁰ 10¹ 10² 10³ 10⁴ 10⁵ 10⁶ 10⁷ 10⁸ 10⁹ 10¹⁰ 10¹¹ 10¹² 10¹³ 10¹⁴ 10¹⁵ 10¹⁶ 10¹⁷ 10¹⁸ 10¹⁹ 10²⁰ 10²¹ 10²² 10²³ 10²⁴ 10²⁵ 10²⁶ 10²⁷ 10²⁸ 10²⁹ 10³⁰ 10³ⁱ 10³² 10³³ 10³⁴ 10³⁵ 10³⁶ 10³⁷ 10³⁸ 10³⁹ 10⁴⁰ 10⁴¹ 10⁴² 10⁴³ 10⁴⁴ 10⁴⁵ 10⁴⁶ 10⁴⁷ 10⁴⁸ 10⁴⁹ 10⁵⁰ 10⁵¹ 10⁵² 10⁵³ 10⁵⁴ 10⁵⁵ 10⁵⁶ 10⁵⁷ 10⁵⁸ 10⁵⁹ 10⁶⁰ 10⁶¹ 10⁶² 10⁶³ 10⁶⁴ 10⁶⁵ 10⁶⁶ 10⁶⁷ 10⁶⁸ 10⁶⁹ 10⁷⁰ 10⁷¹ 10⁷² 10⁷³ 10⁷⁴ 10⁷⁵ 10⁷⁶ 10⁷⁷ 10⁷⁸ 10⁷⁹ 10⁸⁰ 10⁸¹ 10⁸² 10⁸³ 10⁸⁴ 10⁸⁵ 10⁸⁶ 10⁸⁷ 10⁸⁸ 10⁸⁹ 10⁹⁰ 10⁹¹ 10⁹² 10⁹³ 10⁹⁴ 10⁹⁵ 10⁹⁶ 10⁹⁷ 10⁹⁸ 10⁹⁹ 10¹⁰⁰ 10¹⁰¹ 10¹⁰² 10¹⁰³ 10¹⁰⁴ 10¹⁰⁵ 10¹⁰⁶ 10¹⁰⁷ 10¹⁰⁸ 10¹⁰⁹ 10¹¹⁰ 10¹¹¹ 10¹¹² 10¹¹³ 10¹¹⁴ 10¹¹⁵ 10¹¹⁶ 10¹¹⁷ 10¹¹⁸ 10¹¹⁹ 10¹²⁰ 10¹²¹ 10¹²² 10¹²³ 10¹²⁴ 10¹²⁵ 10¹²⁶ 10¹²⁷ 10¹²⁸ 10¹²⁹ 10¹³⁰ 10¹³¹ 10¹³² 10¹³³ 10¹³⁴ 10¹³⁵ 10¹³⁶ 10¹³⁷ 10¹³⁸ 10¹³⁹ 10¹⁴⁰ 10¹⁴¹ 10¹⁴² 10¹⁴³ 10¹⁴⁴ 10¹⁴⁵ 10¹⁴⁶ 10¹⁴⁷ 10¹⁴⁸ 10¹⁴⁹ 10¹⁵⁰ 10¹⁵¹ 10¹⁵² 10¹⁵³ 10¹⁵⁴ 10¹⁵⁵ 10¹⁵⁶ 10¹⁵⁷ 10¹⁵⁸ 10¹⁵⁹ 10¹⁶⁰ 10¹⁶¹ 10¹⁶² 10¹⁶³ 10¹⁶⁴ 10¹⁶⁵ 10¹⁶⁶ 10¹⁶⁷ 10¹⁶⁸ 10¹⁶⁹ 10¹⁷⁰ 10¹⁷¹ 10¹⁷² 10¹⁷³ 10¹⁷⁴ 10¹⁷⁵ 10¹⁷⁶ 10¹⁷⁷ 10¹⁷⁸ 10¹⁷⁹ 10¹⁸⁰ 10¹⁸¹ 10¹⁸² 10¹⁸³ 10¹⁸⁴ 10¹⁸⁵ 10¹⁸⁶ 10¹⁸⁷ 10¹⁸⁸ 10¹⁸⁹ 10¹⁹⁰ 10¹⁹¹ 10¹⁹² 10¹⁹³ 10¹⁹⁴ 10¹⁹⁵ 10¹⁹⁶ 10¹⁹⁷ 10¹⁹⁸ 10¹⁹⁹ 10²⁰⁰ 10²⁰¹ 10²⁰² 10²⁰³ 10²⁰⁴ 10²⁰⁵ 10²⁰⁶ 10²⁰⁷ 10²⁰⁸ 10²⁰⁹ 10²¹⁰ 10²¹¹ 10²¹² 10²¹³ 10²¹⁴ 10²¹⁵ 10²¹⁶ 10²¹⁷ 10²¹⁸ 10²¹⁹ 10²²⁰ 10²²¹ 10²²² 10²²³ 10²²⁴ 10²²⁵ 10²²⁶ 10²²⁷ 10²²⁸ 10²²⁹ 10²³⁰ 10²³¹ 10²³² 10²³³ 10²³⁴ 10²³⁵ 10²³⁶ 10²³⁷ 10²³⁸ 10²³⁹ 10²⁴⁰ 10²⁴¹ 10²⁴² 10²⁴³ 10²⁴⁴ 10²⁴⁵ 10²⁴⁶ 10²⁴⁷ 10²⁴⁸ 10²⁴⁹ 10²⁵⁰ 10²⁵¹ 10²⁵² 10²⁵³ 10²⁵⁴ 10²⁵⁵ 10²⁵⁶ 10²⁵⁷ 10²⁵⁸ 10²⁵⁹ 10²⁶⁰ 10²⁶¹ 10²⁶² 10²⁶³ 10²⁶⁴ 10²⁶⁵ 10²⁶⁶ 10²⁶⁷ 10²⁶⁸ 10²⁶⁹ 10²⁷⁰ 10²⁷¹ 10²⁷² 10²⁷³ 10²⁷⁴ 10²⁷⁵ 10²⁷⁶ 10²⁷⁷ 10²⁷⁸ 10²⁷⁹ 10²⁸⁰ 10²⁸¹ 10²⁸² 10²⁸³ 10²⁸⁴ 10²⁸⁵ 10²⁸⁶ 10²⁸⁷ 10²⁸⁸ 10²⁸⁹ 10²⁹⁰ 10²⁹¹ 10²⁹² 10²⁹³ 10²⁹⁴ 10²⁹⁵ 10²⁹⁶ 10²⁹⁷ 10²⁹⁸ 10²⁹⁹ 10³⁰⁰ 10³⁰¹ 10³⁰² 10³⁰³ 10³⁰⁴ 10³⁰⁵ 10³⁰⁶ 10³⁰⁷ 10³⁰⁸ 10³⁰⁹ 10³¹⁰ 10³¹¹ 10³¹² 10³¹³ 10³¹⁴ 10³¹⁵ 10³¹⁶ 10³¹⁷ 10³¹⁸ 10³¹⁹ 10³²⁰ 10³²¹ 10³²² 10³²³ 10³²⁴ 10³²⁵ 10³²⁶ 10³²⁷ 10³²⁸ 10³²⁹ 10³³⁰ 10³³¹ 10³³² 10³³³ 10³³⁴ 10³³⁵ 10³³⁶ 10³³⁷ 10³³⁸ 10³³⁹ 10³⁴⁰ 10³⁴¹ 10³⁴² 10³⁴³ 10³⁴⁴ 10³⁴⁵ 10³⁴⁶ 10³⁴⁷ 10³⁴⁸ 10³⁴⁹ 10³⁵⁰ 10³⁵¹ 10³⁵² 10³⁵³ 10³⁵⁴ 10³⁵⁵ 10³⁵⁶ 10³⁵⁷ 10³⁵⁸ 10³⁵⁹ 10³⁶⁰ 10³⁶¹ 10³⁶² 10³⁶³ 10³⁶⁴ 10³⁶⁵ 10³⁶⁶ 10³⁶⁷ 10³⁶⁸ 10³⁶⁹ 10³⁷⁰ 10³⁷¹ 10³⁷² 10³⁷³ 10³⁷⁴ 10³⁷⁵ 10³⁷⁶ 10³⁷⁷ 10³⁷⁸ 10³⁷⁹ 10³⁸⁰ 10³⁸¹ 10³⁸² 10³⁸³ 10³⁸⁴ 10³⁸⁵ 10³⁸⁶ 10³⁸⁷ 10³⁸⁸ 10³⁸⁹ 10³⁹⁰ 10³⁹¹ 10³⁹² 10³⁹³ 10³⁹⁴ 10³⁹⁵ 10³⁹⁶ 10³⁹⁷ 10³⁹⁸ 10³⁹⁹ 10⁴⁰⁰ 10⁴⁰¹ 10⁴⁰² 10⁴⁰³ 10⁴⁰⁴ 10⁴⁰⁵ 10⁴⁰⁶ 10⁴⁰⁷ 10⁴⁰⁸ 10⁴⁰⁹ 10⁴¹⁰ 10⁴¹¹ 10⁴¹² 10⁴¹³ 10⁴¹⁴ 10⁴¹⁵ 10⁴¹⁶ 10⁴¹⁷ 10⁴¹⁸ 10⁴¹⁹ 10⁴²⁰ 10⁴²¹ 10⁴²² 10⁴²³ 10⁴²⁴ 10⁴²⁵ 10⁴²⁶ 10⁴²⁷ 10⁴²⁸ 10⁴²⁹ 10⁴³⁰ 10⁴³¹ 10⁴³² 10⁴³³ 10⁴³⁴ 10⁴³⁵ 10⁴³⁶ 10⁴³⁷ 10⁴³⁸ 10⁴³⁹ 10⁴⁴⁰ 10⁴⁴¹ 10⁴⁴² 10⁴⁴³ 10⁴⁴⁴ 10⁴⁴⁵ 10⁴⁴⁶ 10⁴⁴⁷ 10⁴⁴⁸ 10⁴⁴⁹ 10⁴⁵⁰ 10⁴⁵¹ 10⁴⁵² 10⁴⁵³ 10⁴⁵⁴ 10⁴⁵⁵ 10⁴⁵⁶ 10⁴⁵⁷ 10⁴⁵⁸ 10⁴⁵⁹ 10⁴⁶⁰ 10⁴⁶¹ 10⁴⁶² 10⁴⁶³ 10⁴⁶⁴ 10⁴⁶⁵ 10⁴⁶⁶ 10⁴⁶⁷ 10⁴⁶⁸ 10⁴⁶⁹ 10⁴⁷⁰ 10⁴⁷¹ 10⁴⁷² 10⁴⁷³ 10⁴⁷⁴ 10⁴⁷⁵ 10⁴⁷⁶ 10⁴⁷⁷ 10⁴⁷⁸ 10⁴⁷⁹ 10⁴⁸⁰ 10⁴⁸¹ 10⁴⁸² 10⁴⁸³ 10⁴⁸⁴ 10⁴⁸⁵ 10⁴⁸⁶ 10⁴⁸⁷ 10⁴⁸⁸ 10⁴⁸⁹ 10⁴⁹⁰ 10⁴⁹¹ 10⁴⁹² 10⁴⁹³ 10⁴⁹⁴ 10⁴⁹⁵ 10⁴⁹⁶ 10⁴⁹⁷ 10⁴⁹⁸ 10⁴⁹⁹</sample>
    <sample id="167">LM Training Data
Eine teilschöne Bereicherung

1. www.msdn.com
2. www.wikipedia.org
3. www.github.com
4. www.myspace.com
5. www.nytimes.com
6. www.huffpost.com
7. www.scribd.com
8. www.bloomberg.com
9. www.theverge.com
10. www.zhihu.com
11. www.amazon.com
12. www.springer.com
13. www.mercadomagico.com
14. www.kaggle.com
15. www.data.gov
16. www.npr.org</sample>
    <sample id="168">LM Training Data Ein gesenktes Segen A mixed blessing

Dieses Diagramm zeigt die Häufigkeit von Webseiten im Trainingsdatensatz. Es zeigt, dass einige Webseiten sehr häufig vorkommen, während andere relativ seltener auftreten.</sample>
    <sample id="169">To this end</sample>
    <sample id="170">Ziel davon ist, die Rolle von Datensammlungen in der Ausbildung von Sprachmodellen zu untersuchen.</sample>
    <sample id="171">Zielsetzung

Vortrainingsspeicherung

Sprachmodelle

Unterstützende Aufgaben</sample>
    <sample id="172">Evaluieren derPoliticalleaning

Unterstütze sowohl Encoder als auch Decoder LMs

"statement" I &lt;mask&gt; mit diesem Statement."

"Do you agree or disagree with this statement? &lt;statement&gt;"

Automatisches Eval

Basierend auf politischem Lit

Sprache Modell

Prompted Response

Politische Lernung</sample>
    <sample id="173">Existentielle LMs

Die Grafik zeigt eine Karte, die verschiedene Sprachmodelle (LMs) nach ihren politischen Einstellungen einstellt. Die Achse "-authoritär" (-authoritär) und "libertär" (libertär) zeigt, ob die Modelle autoritärer oder libertärer sind. Die Achse "economix-axis" zeigt, ob die Modelle liberaler oder rechtschweigsamer sind.

Ein paar Modelle, die auf der linken Seite der Grafik liegen, sind BERT-base, RoBERTa-base, distilRoBERTa-base, ALBERT-base und ALBERT-large. Sie scheinen autoritärer und rechtschweigsamer zu sein.

Ein paar Modelle, die auf der rechten Seite der Grafik liegen, sind Codex, LLaMA, GPT-3-ada, GPT-3-curie, GPT-3-davinci, ChatGPT und GPT-4. Sie scheinen libertärer und liberaler zu sein.

Ein paar Modelle, die in der Mitte liegen, sind BERT-large, RoBERTa-large, distilRoBERTa-large und ALBERT-large. Sie scheinen neither autoritärer noch libertärer zu sein.

Die Grafik zeigt, dass es keine klare Verbindung zwischen den politischen Einstellungen von Modellen und ihren Leistungen gibt. Ein Modell kann autoritärer und rechtschweigsamer sein, aber immer noch gut in bestimmten Aufgaben wie der Erkennung von sentimentalen Ausdrücken oder der Übersetzung von Texten sein.</sample>
    <sample id="174">Existentielle LM's

Die Grafik zeigt eine Karte, auf der verschiedene Sprachmodelle (LMs) nach ihren politischen Ausrichtungen platziert werden. Die Achse "-authoritär" (-authoritär) und "libertär" (libertär) zeigt die Einstellung zur Kontrolle und Freiheit, während die Achse "economistic" (ökonomisch) und "economic" (ökonomisch) die Einstellung zur wirtschaftlichen Effizienz und zum Wohlstand zeigt.

Einige der LM's, die auf der linken Seite der Grafik platziert werden, sind BERT-base, RoBERTa-base, distilRoBERTa-base, ALBERT-base und ALBERT-large. Sie scheinen eine höhere Kontrolle und eine ökonomischere Einstellung zu haben.

Auf der rechten Seite der Grafik finden sich LMs wie Codex, LLaMA-2, GPT-3-ada, GPT-3-curie, GPT-3-davinci, ChatGPT, GPT-4 und GPT-J. Sie scheinen eine geringere Kontrolle und eine ökonomischere Einstellung zu haben.

In der Mitte der Grafik liegen LM's wie BART-base und PaLM 2, die eine ausgewogene Einstellung zu Kontrolle und Freiheit sowie wirtschaftlicher Effizienz und Wohlstand zeigen.

Es ist wichtig zu beachten, dass diese Positionen lediglich eine Darstellung sind und nicht notwendigerweise die vollständige Wahrheit über die Einstellungen der LM's widerspiegeln.</sample>
    <sample id="175">Prätraining von Daten
Weitere prätraining LM (Roberta, GPT-2) Checkpoints, Auswertung der Änderungen in der politischen Neigung

News-Medien
Mitte
Rechts
Nach dem Lesen von Artikeln mit linken und rechten Neigungen

Soziale Medien (Reddit)
Mitte
Rechts
Nach dem Lesen von Artikeln mit linken und rechten Neigungen</sample>
    <sample id="176">Prätraining von Daten

Weitere prätraining LM (Roberta, GPT-2) Checkpoints evaluieren die Änderung in der politischen Neigung

Nachfolgende

Nachfolgende

Nachfolgende</sample>
    <sample id="177">Ergebnisse
Verschiebungen der Parteien in der politischen Neigungsordnung
RobBERTa
GPT-2</sample>
    <sample id="178">Ergebnisse
Verschiebungen in der politischen Neigungsrichtung von Parteien

Die Grafik zeigt die Verschiebungen in der politischen Neigungsrichtung von Parteien im Vergleich zu den Medien "reddit" und "News". Es werden verschiedene Parteien markiert, indem sie in Farben codiert werden: blau für "original", rot für "reddit" und grün für "News". 

Die Linie "original" zeigt, dass die ursprüngliche Position der Parteien in der politischen Neigungsrichtung variiert. Die Linie "reddit" zeigt, dass die Position der Parteien in Bezug auf "Reddit" variiert. Die Linie "News" zeigt, dass die Position der Parteien in Bezug auf "News" variiert.

Die Grafik zeigt, dass die Parteien in der politischen Neigungsrichtung variieren, je nachdem, ob sie sich auf "Reddit" oder "News" orientieren.</sample>
    <sample id="179">Ergebnisse
Verschiebungen im politischen Neigungsindex von Parteien

Die Grafik zeigt die Verschiebungen von Parteien in einem politischen Neigungsindex, der auf der x-Achse von links nach rechts variiert und auf der y-Achse von linken zu rechten politischen Neigungen variiert. Es werden zwei Modelle, RoBERTa und GPT-2, verwendet, um die Verschiebungen zu messen.

Die Farben in der Grafik repräsentieren verschiedene Medienquellen: blau steht für "reddit", rot steht für "news" und grün steht für "original". Die Pfeile zeigen die Richtung und den Grad des Verschiegens der Parteien in den verschiedenen Medienquellen.

Die Grafik zeigt, dass die Parteien in den Medienquellen unterschiedlich verschieben werden. Zum Beispiel wird eine Partei in den "original" Medienquellen nach links verschoben (towards the left), während sie in den "news" Medienquellen nach rechts verschoben wird (towards the right). In den "reddit" Medienquellen wird eine Partei nach oben verschoben (towards the top), während sie in den "news" Medienquellen nach unten verschoben wird (towards the bottom).

Die Grafik zeigt auch, dass die Verschiebungen in den Medienquellen nicht immer konsistent sind. Zum Beispiel wird eine Partei in den "original" Medienquellen nach links verschoben, während sie in den "news" Medienquellen nach rechts verschoben wird. Dies zeigt, dass die Verschiebungen von Parteien in den Medienquellen abhängig sind und nicht immer konsistent sind.

Insgesamt zeigt die Grafik, dass die Verschiebungen von Parteien in den Medienquellen variieren und nicht immer konsistent sind. Es ist wichtig zu beachten, dass die Verschiebungen von Parteien in den Medienquellen abhängig sind und nicht immer konsistent sind.</sample>
    <sample id="180">The Trump Card

Vor 45. post-45. Verschiebung

news_left
news_right
news_center
reddit_left
Reddit_Center
Reddit_right</sample>
    <sample id="181">Die Trump-Playing-Karte

Vor 45. nach dem 45. Schub

news_left news_center news_right reddit_left reddit_center reddit_right

∆w = (-275, -124) ∆w = (-0.13, 1.03) ∆w = (1.63, 1.03) ∆w = (0.75, -3.64) ∆w = (-0.50, -3.64) ∆w = (-1.75, 0.92)</sample>
    <sample id="182">The Trump Card: Pre-45th to post-45th shift

Die Trump-Karte: Vom vor 45. zum nach 45. Schub

Die Grafik zeigt die Verschiebungen der Medien in den Jahren vor und nach dem Austritt von Donald Trump als Präsident. Es zeigt, dass die Medien vor Trumps Wahl als Präsidenten mehr neutral waren, aber nach seiner Wahl als Präsident sind sie zu mehrheitlich rechts geworden.</sample>
    <sample id="183">Kategoriengenerativer Leistungen

Die Tabelle 4 zeigt die Leistungen bei Hasssprache, die verschiedene Identitätsgruppen anvisieren, und bei Missinformation von verschiedenen Quellen. Die Farbenrot und gelb kennzeichnen die besten Leistungen, während dunkelblauer Farbton die schlechtesten Leistungen darstellt.

1. News Left
2. Reddit Left
3. Reddit Right</sample>
    <sample id="184">Kategoriengenerativer Leistungsvergleich

Die Tabelle zeigt die Leistungen von AI-Modellen bei der Identifikation von Hasssprache und Falschinformation in verschiedenen Kategorien. Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,72 (blau)
3. Reddit Right: 80,03 (rot), 82,02 (blau)

Die Farbenrot (yellow) und blau (blue) kennzeichnen die besten und schlechtesten Leistungen, respectiv.

1. News Left: 80,63 (rot), 89,44 (blau)
2. Reddit Left: 87,82 (rot), 85,</sample>
    <sample id="185">Kategoriengeneratoren
Performance
1. News Left
2. Reddit Left
3. Reddit Right
4. Minisformation
5. Reddit Left
6. Reddit Right
7. Reddit Left
8. Reddit Right
9. Reddit Left
10. Reddit Right
11. Reddit Left
12. Reddit Right
13. Reddit Left
14. Reddit Right
15. Reddit Left
16. Reddit Right
17. Reddit Left
18. Reddit Right
19. Reddit Left
20. Reddit Right
21. Reddit Left
22. Reddit Right
23. Reddit Left
24. Reddit Right
25. Reddit Left
26. Reddit Right
27. Reddit Left
28. Reddit Right
29. Reddit Left
30. Reddit Right
31. Reddit Left
32. Reddit Right
33. Reddit Left
34. Reddit Right
35. Reddit Left
36. Reddit Right
37. Reddit Left
38. Reddit Right
39. Reddit Left
40. Reddit Right
41. Reddit Left
42. Reddit Right
43. Reddit Left
44. Reddit Right
45. Reddit Left
46. Reddit Right
47. Reddit Left
48. Reddit Right
49. Reddit Left
50. Reddit Right
51. Reddit Left
52. Reddit Right
53. Reddit Left
54. Reddit Right
55. Reddit Left
56. Reddit Right
57. Reddit Left
58. Reddit Right
59. Reddit Left
60. Reddit Right
61. Reddit Left
62. Reddit Right
63. Reddit Left
64. Reddit Right
65. Reddit Left
66. Reddit Right
67. Reddit Left
68. Reddit Right
69. Reddit Left
70. Reddit Right
71. Reddit Left
72. Reddit Right
73. Reddit Left
74. Reddit Right
75. Reddit Left
76. Reddit Right
77. Reddit Left
78. Reddit Right
79. Reddit Left
80. Reddit Right
81. Reddit Left
82. Reddit Right
83. Reddit Left
84. Reddit Right
85. Reddit Left
86. Reddit Right
87. Reddit Left
88. Reddit Right
89. Reddit Left
90. Reddit Right
91. Reddit Left
92. Reddit Right
93. Reddit Left
94. Reddit Right
95. Reddit Left
96. Reddit Right
97. Reddit Left
98. Reddit Right
99. Reddit Left
100. Reddit Right
101. Reddit Left
102. Reddit Right
103. Reddit Left
104. Reddit Right
105. Reddit Left
106. Reddit Right
107. Reddit Left
108. Reddit Right
109. Reddit Left
110. Reddit Right
111. Reddit Left
112. Reddit Right
113. Reddit Left
114. Reddit Right
115. Reddit Left
116. Reddit Right
117. Reddit Left
118. Reddit Right
119. Reddit Left
120. Reddit Right
121. Reddit Left
122. Reddit Right
123. Reddit Left
124. Reddit Right
125. Reddit Left
126. Reddit Right
127. Reddit Left
128. Reddit Right
129. Reddit Left
130. Reddit Right
131. Reddit Left
132. Reddit Right
133. Reddit Left
134. Reddit Right
135. Reddit Left
136. Reddit Right
137. Reddit Left
138. Reddit Right
139. Reddit Left
140. Reddit Right
141. Reddit Left
142. Reddit Right
143. Reddit Left
144. Reddit Right
145. Reddit Left
146. Reddit Right
147. Reddit Left
148. Reddit Right
149. Reddit Left
150. Reddit Right
151. Reddit Left
152. Reddit Right
153. Reddit Left
154. Reddit Right
155. Reddit Left
156. Reddit Right
157. Reddit Left
158. Reddit Right
159. Reddit Left
160. Reddit Right
161. Reddit Left
162. Reddit Right
163. Reddit Left
164. Reddit Right
165. Reddit Left
166. Reddit Right
167. Reddit Left
168. Reddit Right
169. Reddit Left
170. Reddit Right
171. Reddit Left
172. Reddit Right
173. Reddit Left
174. Reddit Right
175. Reddit Left
176. Reddit Right
177. Reddit Left
178. Reddit Right
179. Reddit Left
180. Reddit Right
181. Reddit Left
182. Reddit Right
183. Reddit Left
184. Reddit Right
185. Reddit Left
186. Reddit Right
187. Reddit Left
188. Reddit Right
189. Reddit Left
190. Reddit Right
191. Reddit Left
192. Reddit Right
193. Reddit Left
194. Reddit Right
195. Reddit Left
196. Reddit Right
197. Reddit Left
198. Reddit Right
199. Reddit Left
200. Reddit Right
201. Reddit Left
202. Reddit Right
203. Reddit Left
204. Reddit Right
205. Reddit Left
206. Reddit Right
207. Reddit Left
208. Reddit Right
209. Reddit Left
210. Reddit Right
211. Reddit Left
212. Reddit Right
213. Reddit Left
214. Reddit Right
215. Reddit Left
216. Reddit Right
217. Reddit Left
218. Reddit Right
219. Reddit Left
220. Reddit Right
221. Reddit Left
222. Reddit Right
223. Reddit Left
224. Reddit Right
225. Reddit Left
226. Reddit Right
227. Reddit Left
228. Reddit Right
229. Reddit Left
230. Reddit Right
231. Reddit Left
232. Reddit Right
233. Reddit Left
234. Reddit Right
235. Reddit Left
236. Reddit Right
237. Reddit Left
238. Reddit Right
239. Reddit Left
240. Reddit Right
241. Reddit Left
242. Reddit Right
243. Reddit Left
244. Reddit Right
245. Reddit Left
246. Reddit Right
247. Reddit Left
248. Reddit Right
249. Reddit Left
250. Reddit Right
251. Reddit Left
252. Reddit Right
253. Reddit Left
254. Reddit Right
255. Reddit Left
256. Reddit Right
257. Reddit Left
258. Reddit Right
259. Reddit Left
260. Reddit Right
261. Reddit Left
262. Reddit Right
263. Reddit Left
264. Reddit Right
265. Reddit Left
266. Reddit Right
267. Reddit Left
268. Reddit Right
269. Reddit Left
270. Reddit Right
271. Reddit Left
272. Reddit Right
273. Reddit Left
274. Reddit Right
275. Reddit Left
276. Reddit Right
277. Reddit Left
278. Reddit Right
279. Reddit Left
280. Reddit Right
281. Reddit Left
282. Reddit Right
283. Reddit Left
284. Reddit Right
285. Reddit Left
286. Reddit Right
287. Reddit Left
288. Reddit Right
289. Reddit Left
290. Reddit Right
291. Reddit Left
292. Reddit Right
293. Reddit Left
294. Reddit Right
295. Reddit Left
296. Reddit Right
297. Reddit Left
298. Reddit Right
299. Reddit Left
300. Reddit Right
301. Reddit Left
302. Reddit Right
303. Reddit Left
304. Reddit Right
305. Reddit Left
306. Reddit Right
307. Reddit Left
308. Reddit Right
309. Reddit Left
310. Reddit Right
311. Reddit Left
312. Reddit Right
313. Reddit Left
314. Reddit Right
315. Reddit Left
316. Reddit Right
317. Reddit Left
318. Reddit Right
319. Reddit Left
320. Reddit Right
321. Reddit Left
322. Reddit Right
323. Reddit Left
324. Reddit Right
325. Reddit Left
326. Reddit Right
327. Reddit Left
328. Reddit Right
329. Reddit Left
330. Reddit Right
331. Reddit Left
332. Reddit Right
333. Reddit Left
334. Reddit Right
335. Reddit Left
336. Reddit Right
337. Reddit Left
338. Reddit Right
339. Reddit Left
340. Reddit Right
341. Reddit Left
342. Reddit Right
343. Reddit Left
344. Reddit Right
345. Reddit Left
346. Reddit Right
347. Reddit Left
348. Reddit Right
349. Reddit Left
350. Reddit Right
351. Reddit Left
352. Reddit Right
353. Reddit Left
354. Reddit Right
355. Reddit Left
356. Reddit Right
357. Reddit Left
358. Reddit Right
359. Reddit Left
360. Reddit Right
361. Reddit Left
362. Reddit Right
363. Reddit Left
364. Reddit Right
365. Reddit Left
366. Reddit Right
367. Reddit Left
368. News Left
369. Reddit Left
370. Reddit Right
371. Reddit Left
372. Reddit Right
373. Reddit Left
374. Reddit Right
375. Reddit Left
376. Reddit Right
377. Reddit Left
378. Reddit Right
379. Reddit Left
380. Reddit Right
381. Reddit Left
382. Reddit Right
383. Reddit Left
384. Reddit Right
385. Reddit Left
386. Reddit Right
387. Reddit Left
388. Reddit Right
389. Reddit Left
390. Reddit Right
391. Reddit Left
392. Reddit Right
393. Reddit Left
394. Reddit Right
395. Reddit Left
396. Reddit Right
397. Reddit Left
398. Reddit Right
399. Reddit Left
400. Reddit Right
401. Reddit Left
402. Reddit Right
403. Reddit Left
404. Reddit Right
405. Reddit Left
406. Reddit Right
407. Reddit Left
408. Reddit Right
409. Reddit Left
410. Reddit Right
411. Reddit Left
412. Reddit Right
413. Reddit Left
414. Reddit Right
415. Reddit Left
416. Reddit Right
417. Reddit Left
418. Reddit Right
419. Reddit Left
420. Reddit Right
421. Reddit Left
422. Reddit Right
423. Reddit Left
424. Reddit Right
425. Reddit Left
426. Reddit Right
427. Reddit Left
428. Reddit Right
429. Reddit Left
430. Reddit Right
431. Reddit Left
432. Reddit Right
433. Reddit Left
434. Reddit Right
435. Reddit Left
436. Reddit Right
437. Reddit Left
438. Reddit Right
439. Reddit Left
440. Reddit Right
441. Reddit Left
442. Reddit Right
443. Reddit Left
444. Reddit Right
445. Reddit Left
446. Reddit Right
447. Reddit Left
448. Reddit Right
449. Reddit Left
450. Reddit Right
451. Reddit Left
452. Reddit Right
453. Reddit Left
454. Reddit Right
455. Reddit Left
456. Reddit Right
457. Reddit Left
458. Reddit Right
459. Reddit Left
460. Reddit Right
461. Reddit Left
462. Reddit Right
463. Reddit Left
464. Reddit Right
465. Reddit Left
466. Reddit Right
467. Reddit Left
468. Reddit Right
469. Reddit Left
470. Reddit Right
471. Reddit Left
472. Reddit Right
473. Reddit Left
474. Reddit Right
475. Reddit Left
476. Reddit Right
477. Reddit Left
478. Reddit Right
479. Reddit Left
480. Reddit Right
481. Reddit Left
482. Reddit Right
483. Reddit Left
484. Reddit Right
485. Reddit Left
486. Reddit Right
487. Reddit Left
488. Reddit Right
489. Reddit Left
490. Reddit Right
491. Reddit Left
492. Reddit Right
493. Reddit Left
494. Reddit Right
495. Reddit Left
496. Reddit Right
497. Reddit Left
498. Reddit Right
499. Reddit Left
500. Reddit Right
501. Reddit Left
502. Reddit Right
503. Reddit Left
504. Reddit Right
505. Reddit Left
506. Reddit Right
507. Reddit Left
508. Reddit Right
509. Reddit Left
510. Reddit Right
511. Reddit Left
512. Reddit Right
513. Reddit Left
514. Reddit Right
515. Reddit Left
516. Reddit Right
517. Reddit Left
518. Reddit Right
519. Reddit Left
520. Reddit Right
521. Reddit Left
522. Reddit Right
523. Reddit Left
524. Reddit Right
525. Reddit Left
526. Reddit Right
527. Reddit Left
528. Reddit Right
529. Reddit Left
530. Reddit Right
531. Reddit Left
532. Reddit Right
533. Reddit Left
534. Reddit Right
535. Reddit Left
536. Reddit Right
537. Reddit Left
538. Reddit Right
539. Reddit Left
540. Reddit Right
541. Reddit Left
542. Reddit Right
543. Reddit Left
544. Reddit Right
545. Reddit Left
546. Reddit Right
547. Reddit Left
548. Reddit Right
549. Reddit Left
550. Reddit Right
551. Reddit Left
552. Reddit Right
553. Reddit Left
554. Reddit Right
555. Reddit Left
556. Reddit Right
557. Reddit Left
558. Reddit Right
559. Reddit Left
560. Reddit Right
561. Reddit Left
562. Reddit Right
563. Reddit Left
564. Reddit Right
565. Reddit Left
566. Reddit Right
567. Reddit Left
568. Reddit Right
569. Reddit Left
570. Reddit Right
571. Reddit Left
572. Reddit Right
573. Reddit Left
574. Reddit Right
575. Reddit Left
576. Reddit Right
577. Reddit Left
578. Reddit Right
579. Reddit Left
580. Reddit Right
581. Reddit Left
582. Reddit Right
583. Reddit Left
584. Reddit Right
585. Reddit Left
586. Reddit Right
587. Reddit Left
588. Reddit Right
589. Reddit Left
590. Reddit Right
591. Reddit Left
592. Reddit Right
593. Reddit Left
594. Reddit Right
595. Reddit Left
596. Reddit Right
597. Reddit Left
598. Reddit Right
599. Reddit Left
6</sample>
    <sample id="186">Kategoriengeneratoren Performance

Die Tabelle zeigt die Leistungen von Modellen bei der Identifizierung von Hasssprache und Falschinformation in verschiedenen Kategorien. Die Farben in der Tabelle repräsentieren die Leistungen, wobei gelb-braun das beste Resultat und dunkelblaue Farbe das schlechteste Resultat darstellt.

| Kategoriengenerator | News Left | Reddit Left | Reddit Right |
| --- | --- | --- | --- |
| Black | 80.63 | 89.44 | 82.03 |
| Muslim | 80.88 | 89.09 | 82.19 |
| LGBTQ+ | 90.19 | 89.09 | 82.85 |
| Jews | 82.85 | 89.83 | 83.43 |
| Asian | 83.85 | 89.83 | 83.43 |
| Latinx | 84.44 | 90.66 | 84.15 |
| Women | 85.63 | 90.66 | 87.65 |
| Christian | 86.67 | 90.66 | 87.65 |
| Men | 87.65 | 90.66 | 87.65 |
| White | 88.89 | 90.66 | 88.89 |

Quelle: [1]</sample>
    <sample id="187">Kategoriengenerativer Leistungen

Die Tabelle zeigt die Leistungen bei der Identifizierung von Hasssprache und Missinformation von verschiedenen Quellen. 

| Kategoriengenerativer Leistungen | 1. News_left | 2. Reddit_left | 3. Reddit_right |
| --- | --- | --- | --- |
| News_left | 86,53 | 89,44 | 80,03 |
| Reddit_left | 89,09 | 89,09 | 80,20 |
| Reddit_right | 90,19 | 89,09 | 80,20 |

| Quellen | HP (l) | NYT | CNN (l) | CNN (r) | NPR | NPR (r) | Guard (l) | Fox | W (l) | BB (r) | WAT | NR |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| HP (l) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| NYT | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| CNN (l) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| CNN (r) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| NPR | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| NPR (r) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| Guard (l) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| Fox | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| W (l) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| BB (r) | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| WAT | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |
| NR | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 | 88,72 |

Die Farben in der Tabelle bedeuten, dass gelb das beste Leistungslevel und dunkelblau das schlechteste Leistungslevel darstellt.

(Translation by ChatGPT)</sample>
    <sample id="188">Kategoriengeneratoren

Die Tabelle 4 zeigt die Leistungen bei der Identifizierung von Hasssprache, die verschiedene Identitätsgruppen anläuft, und bei der Identifizierung von Falschinformationen aus verschiedenen Quellen. Die Farben in der Tabelle bedeuten, dass hellgelb das beste ist und dunkelblau das Schlimmste ist.

Hier sind die Hauptpunkte:

1. **Hasssprache**:
   - **News Left**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **Reddit Left**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **Reddit Right**: Bessere Leistungen für "White".

2. **Falschinformation**:
   - **HP (Left)**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **NYT (Left)**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **CNN (Left)**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **NPR (Left)**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **Guard (Left)**: Bessere Leistungen für "Black", "Muslim" und "LGBT+".
   - **Fox (Right)**: Bessere Leistungen für "White".
   - **WAE (Right)**: Bessere Leistungen für "White".
   - **BB&amp;T (Right)**: Bessere Leistungen für "White".
   - **WAT (Right)**: Bessere Leistungen für "White".

Die Farben in der Tabelle zeigen an, dass hellgelb die besten Leistungen und dunkelblau die schlechtesten darstellt.</sample>
    <sample id="189">Kategoriengeneratoren Performance
NEWS LEFT REDDIT LEFT REDDIT RIGHT
BLACK 80,83 89,44 82,05
MUSLIM 80,83 89,09 80,13
LGBT+ 80,83 89,09 80,13
JWEWS 80,83 89,09 80,13
ASAIN 80,83 89,09 80,13
LATINX 80,83 89,09 80,13
WOMEN 80,83 89,09 80,13
CHRISTIAN 80,83 89,09 80,13
MEN 80,83 89,09 80,13
WHITE 80,83 89,09 80,13
MINSTRELSON 80,83 89,09 80,13
MINSTRELSON HP (L) NYT (L) CNN (L) NPR (L) GUARD (L) FOX (L) WAE (L) BRBAT (R) WAT (R) NR (R)
REDDIT LEFT 88,72 92,54 84,64 84,44 90,66 96,63 90,43 91,15 90,18 90,66
REDDIT RIGHT 88,72 92,54 84,64 84,44 90,66 96,63 90,43 91,15 90,18 90,66</sample>
    <sample id="190">Qualitative Analyse

Text | Target Label | Base | N-L | S-L | N-R | S-R
--- | --- | --- | --- | --- | --- | ---
the right to be protected with people supporting a new nation. | ASIAN | True | True | True | False | False
What McDonald's stop Donald Trump from seizing. They were afraid that the country was in the throes of a crime pugn trail because they know I will stop the slaughter going on Donald Trump August 20, 2019. | Right | False | False | False | True | True
It's incredible that me that water was in flood in Flint is paying for the water in Burlington Vermont for clean water. | Fake | False | False | False | False | False</sample>
    <sample id="191">Qualitative Analyse

Text | Target Label | Base | N-L | S-L | N-R | S-R
--- | --- | --- | --- | --- | --- | ---
the right to be protected with people supporting a new nation. | ASIAN | True | True | True | False | False
(–) ...stop Donald Trump from a common. They were shocked that the crime was in the throws of a crime pugn trail because they know I was the slasher going on Donald Trump August 20, 2016. | Right | True | False | False | True | True
(–) ... incredible. Me that water has been in flight in flood. They are paying in Burlington Vermont for clean water. | Fake | False | False | False | False | False</sample>
    <sample id="192">Eine qualitative Analyse von Beispielen von Diskriminierungsbeispielen anhand von unterschiedlichen politischen Parteien und Gruppen.</sample>
    <sample id="193">Eine qualitative Analyse von Beispielen von Hasssprache, die auf der Plattform Reddit vorkommt, zeigt, dass sie in den letzten Jahren immer häufiger geworden ist. Eine Studie von 2019 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % zugenommen hat. Eine weitere Studie von 2020 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % zugenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen. Eine Studie von 2018 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 20 % abgenommen hat. Eine weitere Studie von 2019 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 30 % abgenommen hat. Es gibt jedoch auch negative Entwickelungen. Eine Studie von 2020 hat ermittelt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 50 % abgenommen hat. Eine weitere Studie von 2021 hat festgestellt, dass die Häufigkeit von Hasssprache auf Reddit in den letzten Jahren um 70 % abgenommen hat. Es gibt jedoch auch positive Entwickelungen.</sample>
    <sample id="194">Eine qualitative Analyse von Beispielen von Hasssprache (LM) mit verschiedenen politischen Kontexten hat gezeigt, dass Hasssprache in einem breiten Spektrum von Themen und Kontexten verwendet wird. Es wurde festgestellt, dass Hasssprache in den Bereichen Religion, Politik, Identität und Gewalt verwendet wird. Es wurde auch festgestellt, dass Hasssprache in den Medien und auf sozialen Medien verbreitet wird. Eine quantitative Analyse von LM-Beispielen, die in verschiedenen politischen Kontexten vorkamen, zeigt, dass Hasssprache in einem breiten Spektrum von Themen und Kontexten verwendet wird. Es wurde festgestellt, dass Hasssprache in den Bereichen Religion, Politik, Identität und Gewalt verwendet wird. Es wurde auch festgestellt, dass Hasssprache in den Medien und auf sozialen Medien verbreitet wird.</sample>
    <sample id="195">Eine qualitative Analyse von Beispielen des Hasssprachens basiert auf dem Vergleich der Reaktionswerte (R-Werte) mit den Standardabwehruhren (S.D.) der gesamten Gruppe. Wenn die Reaktionswerte (R-Werte) über dem Standarddeviationsniveau liegen, werden die Beispiele als "wahre" Beispiele von Hasssprache angesehen.</sample>
    <sample id="196">Das ist die Diskussion. zwischen Scylla und Charybdis. To "sanitize" or not to "sanitize", that is the question.</sample>
    <sample id="197">Diskussion
Between Scylla and Charybdis
To "sanitize" or not to "sanitize", that is the question
Pretraining data
Language models
Downstream tasks</sample>
    <sample id="198">Diskussion
Zwischen Scylla und Charybdis
Ob man "sanieren" oder nicht "saubern" soll, das ist die Frage</sample>
    <sample id="199">Danke!</sample>
    <sample id="200">Es sind sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="201">MPP-Auswertungen wurden bis zu 900 Token Kontextlängen durchgeführt.</sample>
    <sample id="202">Sie haben den Datensatz aufgenommen, indem sie die Annotatoren nach den Ausdrucksweisen in den Liedern gefragt haben.</sample>
    <sample id="203">Es ist schwierig zu definieren, da es sich um einen Begriff handelt, der in verschiedenen Kontexten unterschiedlich interpretiert wird.</sample>
    <sample id="204">Dietrich Klakow</sample>
    <sample id="205">Ja, EDAtt passt zu einem bestehenden Offline-ST-Modell.</sample>
    <sample id="206">4</sample>
    <sample id="207">Ja, es funktioniert.</sample>
    <sample id="208">Die drei Varianten von KITMUS sind Background-Pretrain, Background-Both und Background-Inference.</sample>
    <sample id="209">Die Autoren gehören an der University of Edinburgh.</sample>
    <sample id="210">Die abschließende Forschungsfrage lautet: "How to use the available clean samples more efficiently?"</sample>
    <sample id="211">Die Sensitivitätsmetrik misst die Abhängigkeit des Modells von verschiedenen Anweisungen für dieselbe Aufgabe.</sample>
    <sample id="212">Wenjun Peng</sample>
    <sample id="213">Eine höhere Sensitivität ist ein schlechterer Leistungspegel.</sample>
    <sample id="214">Die Modelle erhalten während des Pre-Trainings den Kontext der gesamten Welt.</sample>
    <sample id="215">50</sample>
    <sample id="216">Stanford University</sample>
    <sample id="217">Um die Medienverzerrungen zu messen.</sample>
    <sample id="218">The answer is Jackie CK Cheung.</sample>
    <sample id="219">Die Pipeline ist in der Figur 1 dargestellt.</sample>
    <sample id="220">Ja, es unterscheidet sich.</sample>
    <sample id="221">Ja, Coscript ist öffentlich verfügbar.</sample>
    <sample id="222">Das Wasserzeichen wird in den Text über einen Prozess mit mehreren Schritten eingebettet.</sample>
    <sample id="223">Die Autoren gehören an der Penn State und Amazon.</sample>
    <sample id="224">Ja, sie können verbessert werden.</sample>
    <sample id="225">Ein Beispiel für eingeschränkte Sprachplanung ist die Erstellung von Rezepten mit bestimmten Zutaten und Bedingungen.</sample>
    <sample id="226">Sie stellen die Opazität sicher, indem sie die Farbintensität basierend auf den Wortbedarfswahlfaktoren einstellt.</sample>
    <sample id="227">Die Arbeit nutzt bestehende PLMs, um ein neues PLM aufzubauen.</sample>
    <sample id="228">West-Südost-Asien</sample>
    <sample id="229">Beispielsatz 2</sample>
    <sample id="230">Die Leistung des Modells sinkt, je mehr Aufgaben es bearbeitet.</sample>
    <sample id="231">LSTM seq2seq, T5, Zheng and Lapata</sample>
    <sample id="232">Die beiden Co-Autoren sind Kollegen des ersten Autors.</sample>
    <sample id="233">Chowdery et al.</sample>
    <sample id="234">NLPositionality: Charakterisieren Designbiases von Datensätzen und Modellen

Sebastin Sanky, University of Washington
Jenny T. Liang, Carnegie Mellon University
Ronal Le Bras, Allen Institute for AI
Katharina Reinecke, University of Washington
Maarten Sap, Carnegie Mellon University</sample>
    <sample id="235">NLPositionalität: Charakterisieren Designbiases von Datensätzen und Modellen</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Imagine...</sample>
    <sample id="238">Kannst du aufhören, ein Arroganter zu sein? (PerspektiveAPI-Score: 0,82) Carl Jones, Tech Lead, New York Times Aditya Sharma, Tech Lead, Times of India</sample>
    <sample id="239">Design bias Beispiel! Carl Jones Aditya Sharma Tech Lead, New York Times Tech Lead, Times of India</sample>
    <sample id="240">Positionalität</sample>
    <sample id="241">"Die Perspektiven, die [die Menschen] als Folge ihrer Demographen, Identität und Lebenserfahrungen halten.</sample>
    <sample id="242">Positionalität

"Die Perspektiven, die Menschen als Folge ihrer Demografien, Identität und Lebenserfahrungen halten.

[1] Savin-Baden, Maggi, und Claire Howell-Major. "Qualitative research: The essential guide to theory and practice." Routledge (2013).</sample>
    <sample id="243">Sind Datensätze und Modelle positioniert?</sample>
    <sample id="244">Datenmengen und Modelle haben Positionierbarkeit?</sample>
    <sample id="245">Datenensembles und Modelle haben Positionierbarkeit?</sample>
    <sample id="246">Datenmengen und Modelle haben Positionalität?
Anecdotal evidence:
- Modell- und Datensatz-Prüfung [112]
- Theoretische Definitionen von Modellpositionalität [3]

[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.
[2] Liu, et al. "GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.
[3] Cambo &amp; Gerglot. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science."</sample>
    <sample id="247">Datenmengen und Modelle haben Positionierbarkeit?
Anecdotaler Beweis:
- Modell- und Datensatz-Prüfung [112]
- Theoretische Definitionen von Modellpositionierbarkeit [3]

[1] Blasi, et al. "Systematic Inequalities in Language Technology Performance across the World's Languages." ACL 2022.
[2] Li, et al. "GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models." EMNLP 2022.
[3] Cambo &amp; Gergle. "Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science." Data Science.</sample>
    <sample id="248">Do datasets and models have positionality?
Anecdotal evidence:
- Model and dataset probing [112]
- Theoretical definitions of model positionality [3]
[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
[2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
[3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” SIGMOD 2022</sample>
    <sample id="249">Frage: Haben Datensätze und Model Positionalität?</sample>
    <sample id="250">NLPositionalität
Eine Rahmenstruktur zur Charakterisierung von Designbiases in NLP-Datasets und Modellen</sample>
    <sample id="251">Kollektion Prozessing
S2M hat eine Sammlung von Datensätzen. Jeder Datensatz enthält mehrere Instanzen, jede Instanz hat ein Attribut "Label". Jede Instanz wird als "Good" oder "Bad" gekennzeichnet. Jede Instanz hat auch ein Attribut "Explanation", das eine Erklärung für die Zuordnung als "Good" oder "Bad" ist. Model Predictions: Jede Instanz wird durch einen Modellprozess vorhergesagt, ob sie als "Good" oder "Bad" gekennzeichnet werden soll. Jede Instanz hat auch ein Attribut "Explanation", das eine Erklärung für die vorhergesagte Klassifizierung ist. Annotationen: Jede Instanz wird von Menschen überprüft und gekennzeichnet, ob sie als "Good" oder "Bad" gekennzeichnet werden soll. Jede Instanz hat auch ein Attribut "Explanation", das eine Erklärung für die manuelle Klassifizierung ist. Gold labels: Jede Instanz hat ein Attribut "Gold label", das die manuelle Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifizierung ist. Observed from the models: Jede Instanz hat ein Attribut "Observed from the models", das die vorhergesagte Klassifiz</sample>
    <sample id="252">Kollektiv
Verarbeitung
1) Sammeln: Ein Datensatz wird zufällig ausgewählt.
2) Prozessieren: Jede Instanz wird von Modellen vorhersagt.
3) Annotation: Jede Instanz wird von einem Annotator annotiert.
4) Re-Annotation: Datensätze werden mit diversen Annotatoren erneut annotiert.
5) Evaluieren: Pearson's r wird verwendet, um die Korrelation zwischen den Gold labels und den Vorhersagen zu messen.</sample>
    <sample id="253">Kollektion
Processing
Eating with Hands
Rat</sample>
    <sample id="254">Kollektiv
Verarbeitung
1) Sammeln: Ein Dataset wird zufällig ausgewählt.
2) Prozessieren: Jede Instanz wird einem Modell vorliegen lassen, um eine Vorhersage zu erhalten.
3) Annotation: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
4) Annotatoren: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
5) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
6) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
7) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
8) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
9) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
10) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
11) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
12) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
13) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
14) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhersagen des Modells erstellt.
15) Annotationen: Jede Instanz wird einem Annotator übermittelt, der eine Annotation basierend auf den Vorhors</sample>
    <sample id="255">Kollektion Prozessing 1) Sammeln von Datensätzen und Annotieren der Instanzen 2) Vergleichen der Annotierungen mit dem Modell durch Berechnung von Pearson's R-Scores</sample>
    <sample id="256">Framework

Die Struktur der Framework-Abbildung zeigt einen Prozess, der in mehreren Schritte erfolgt. Der Prozess beginnt mit der "Collection" (Sammlung), bei der Datensätze gesammelt werden. Jeder Datensatz hat einen bestimmten Label, wie zum Beispiel "Eating with Hands", "Red" oder "It's moral". 

Die Sammelte-Datasets werden dann an das Modell übermittelt. Das Modell macht Vorhersagen basierend auf den Eingangsdaten und these Vorhersagen werden als "Instances" bezeichnet. 

Nachdem das Modell die Vorhersagen getroffen hat, werden die "Annotations" (Annotierungen) von Menschen überprüft. Eine der Annotierungen ist "Can we live without it?".

Die Annotierungen werden dann mit den Vorhersagen des Modells verglichen. Dies wird durch die Berechnung von Pearson’s R scores (Pearson’s R-Werte) erreicht.</sample>
    <sample id="257">Kannst du mit einem AI und seinen Worten leben? Hast du jemals darüber nachgedacht, wie du mit einem AI umgehen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begegnen solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? Oder wie du ihm begehn solltest? O</sample>
    <sample id="258">Lab in the Wild</sample>
    <sample id="259">Task A: Soziale Akzeptabilität

1. Lese die Situation
2. Stelle dir vor, wie du über sie von verschiedenen Perspektiven denkst.
3. Finde heraus, was andere darüber denken.

Die AI spezialisiert sich auf deine Meinung.

Studienteilnehmer in den Vereinigten Staaten:

Ich denke, dass das Verständnis von Ihnen:
- Vielleicht (40%)
- Wahrscheinlich (30%)
- Bestimmt (20%)
- Wenig (10%)</sample>
    <sample id="260">Task A: Soziale Akzeptabilität

1. Lese die Situation
2. Stelle fest, was du darüber denkst
3. Sei informiert über das, was andere denken</sample>
    <sample id="261">Task A: Soziale Akzeptabilität

Analyse

Datensätze
- Soziale Chemie

Modelle
- Delphi
- GPT-4</sample>
    <sample id="262">Task B: Gifttoxizität

1. Lese den Beispieltext.
2. Gib an, ob du den Text als gifttoxisch findest.
3. Sieh dir die Meinungen anderer Menschen an.

Das AI-System spekuliert, dass der Text gifttoxisch ist. Studie die Meinungen von Menschen in Afghanistan, die denselben Text gelesen haben.</sample>
    <sample id="263">Task B: Toxizität
Analyse
Datasets
- Dynahate
Models
- Perspective API
- Rewire API
- State RoBERTa
- GPT-4</sample>
    <sample id="264">Wer alignen NLP-Datasets und Modelle?</sample>
    <sample id="265">Datasets und Modelle sind am besten auf englischsprachige Länderschneidereien ausgerichtet.</sample>
    <sample id="266">Datenmengen und Modelle sind am besten mit Menschen in Einklang, die eine Universitätsausbildung erhalten haben.</sample>
    <sample id="267">Datasets und Modelle sind meist mit Menschen in Einklang, die eine Universitätsausbildung haben.</sample>
    <sample id="268">Einige Bevölkerungen werden zurückgelassen.</sample>
    <sample id="269">Datasets und Modelle sind weniger auf nicht-binäre Menschen ausgerichtet.</sample>
    <sample id="270">So, was können wir tun? Bewältigung von Positionalität in NLP</sample>
    <sample id="271">Empfehlungen</sample>
    <sample id="272">Empfehlungen

1. Führe eine Aufzeichnung aller relevanten Designentscheidungen während der Bau von Datensätzen und Modellen.
2. Führe NLP-Forschung durch die Brille der Perspektivismus:
a. Teile aggregierte Datensatzlabels
b. Nutze Modelltechniken, die die Annotator Disagreement bewältigen können.
3. Die Entwicklung spezialisierter Datensätze und Modelle für bestimmte Gemeinschaften ist crucial für eine inklusive NLP (z.B. Masakhan-Initiative).

[1] https://www.masakhane.io</sample>
    <sample id="273">Das Bild zeigt einen Slideshow mit mehreren Grafiken und Textelementen. Hier ist eine detaillierte Übersetzung des englischen Inhalts in Deutsch:

---

**Danke!**

**Dashboard-Link:** nlpositionality.cs.washington.edu/Paper/
**bit.ly/NLPositionality-Paper/**

**Grafiken:**

1. **Alter**: Eine horizontale Balkengrafik, die verschiedene Altersgruppen darstellt.
2. **Geschlecht**: Eine horizontale Balkengrafik, die Männer (pink) und Frauen (orange) unterscheidet.
3. **Ethnicität**: Eine horizontale Balkengrafik, die verschiedene ethnische Gruppen darstellt.
4. **Religion**: Eine horizontale Balkengrafik, die verschiedene religiöse Gruppen darstellt.
5. **Bildungsebene**: Eine horizontale Balkengrafik, die verschiedene Bildungsebenen darstellt.
6. **Wohnort (Land)**: Eine horizontale Balkengrafik, die verschiedene Länder darstellt.
7. **Längstes Land**: Eine horizontale Balkengrafik, die verschiedene Längstes-Land-Gruppen darstellt.
8. **HeimatSprache**: Eine horizontale Balkengrafik, die verschiedene HeimatSprachen darstellt.

---

Insgesamt zeigt der Slideshow diverse statistische Daten über Altersgruppen, Geschlecht, Ethnoitcität, Religion, Bildungsebene, Wohnort, Längstes Land und HeimatSprache.</sample>
    <sample id="274">Die Referentin spricht von mehreren Problemen von SimulST.</sample>
    <sample id="275">Soziale und politische Verzerrungen in Datensätzen beim Training von NLP-Modellen können effektiv reduziert werden, indem man die Datensätze überwacht und die NLP-Modelle auf eine Weise trainiert, die die Verzerrungen minimiert.</sample>
    <sample id="276">Die 61. jährliche Versammlung der Gesellschaft für Computertlinguistik in Toronto, Kanada vom 13. bis 14. April 2023

"Entscheidungsscripts: Wissen aus großen Sprachmodellen für eingeschränkte Sprachplanung"

Siyu Yuan, Jianqiang Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang

Peking University und Brain Technologies Inc.</sample>
    <sample id="277">Wie man einen Kuchen macht?
1. GATHER YOUR INGREDIENTS.
2. PREHEAT THE OVEN TO 325 °F (163 °C) AND GREASE A CAKE PAN.
3. CREAM THE BUTTER AND SUGAR.
4. ADD THE EGGS.
5. STIR IN THE CAKE FLOUR.
6. POUR THE BATTER INTO THE PAN.
7. BAKE THE CAKE FOR 1 HOUR 15 MINUTES.</sample>
    <sample id="278">Wie man einen Kuchen macht?

1. GATHER YOUR INGREDIENTS: Sammel deine Zutaten.
2. PREHEAT THE OVEN TO 325°F: Wärme den Ofen auf 325°F.
3. ADD THE EGG: Füge das Ei hinzu.
4. ADD THE CREAM: Füge die Sahne hinzu.
5. MIX IN THE CREAM WITH THE FLOUR: Misch die Sahne mit dem Mehl.
6. POUR THE BATTER INTO THE PAN: Gieß die Teigflüssigkeit in die Form.
7. BAKE FOR 1 HOUR AND 15 MINUTES: Backe den Kuchen für 1 Stunde und 15 Minuten.

Large language models (LLMs) können effektiv Ziele in Schritte zerlegen.</sample>
    <sample id="279">Eingeschränktes Sprachplanieren

Wie man einen Erdbeerassee-Kuchen macht?
...Füge Erdbeer-Jam in die Mehlmasse ...

Wie man einen Schokoladenkuchen macht?
...Füge Kakao-Pulver in die Mehlmasse ...

Das Ziel kann von verschiedenen realen Leben spezifischen Zielen abhängig sein mit mehreren Facetten.</sample>
    <sample id="280">Begrenztes Sprachplanieren

Wie man einen Erdbeerkuchen macht?
...Füge Erdbeersaft in die Mehlmasse...

Wie man einen Schokoladenkuchen macht?
...Füge Kakao-Pulver in die Mehlmasse...

Das allgemeine Ziel kann von verschiedenen realen Leben spezifischen Zielen mit mehreren Facetten abhängig machen.</sample>
    <sample id="281">Begrenzte Sprachplanung

Wie man einen Erdbeerkuchen macht?
...Füge Erdbeermarmelade in die Mehlmasse...

Wie man einen Schokoladenkuchen macht?
...Füge Kakao-Pulver in die Mehlmasse...

Das globale Ziel kann von verschiedenen realen Leben spezifischen Zielen mit mehrfachen Begrenzungen abhängig machen.</sample>
    <sample id="282">LLMs auf die Leitstelle "Wie performieren LLMs auf eingeschränktem Sprachplanung?"</sample>
    <sample id="283">LLMs auf die Leistungen bei eingeschränktem Sprachplanieren? Dataset: wikiHow + generierte Einschränkungen Definition: Modifizierer Typ: Constr. Definition: Ein Modifizierer ist eine passive oder aktive Änderung, die modifiziert und/or einen abstrakten Ziel konstruiert. Beispiele: Erstelle einen Chocolatenteig oder einen Teig, der modifiziert wird. Erstelle ein Pinkcake. Definition: Typ: Constr. Definition: Ein Constr ist ein Werkzeug oder ein spezifizierter Modus, der die Prozessphase einer Aufgabe bestimmt. Beispiele: Erstelle ein Kuchenrezept mit einem Ofen. Erstelle ein Kuchenrezept mit einem Mikrowellenofen. Erstelle ein Kuchenrezept mit einem Mikrowellenofen. Definition: Typ: Constr. Definition: Ein Constr ist ein zusätzlicher Zweck oder eine Anforderung, die beim Erstellen eines Ziels berücksichtigt wird. Beispiele: Erstelle ein Kuchenrezept für eine Hochzeit. Erstelle ein Kuchenrezept mit Diabetikerzucker.</sample>
    <sample id="284">LLMs auf die Leitstelle basierende Sprachplanung?</sample>
    <sample id="285">Kannen LLMs die planerische Sprache beeinflussen?
Alle Baselines erreichen unzufriedenstelle</sample>
    <sample id="286">Kannen LLMs die planerische Sprache einschränken?
Die Baselines erreichen unzufriedenstellende Leistungen beim Planieren für bestimmte Ziele.</sample>
    <sample id="287">Wasarten von Fehlern, die LLMs in dieser Aufgabe normalerweise machen?</sample>
    <sample id="288">Wasarten von Fehlern, die LLMS in dieser Aufgabe normalerweise machen? FE1: Keine Beschränkungen SE1: Fehlende Schritte SE2: Wiederholte Schritte SE3: Falsche Reihenfolge FE2: Unvollständige Schritte FE3: Inkonsistentes Verhalten (z.B. "Immer 1789" und "Immer 1790") Die semantische Komplettheit (SE) in den generierten Skripten ist akzeptabel, aber die Treue zur Beschränkung (FE) kann nicht garantiert werden.</sample>
    <sample id="289">Wasarten von Zielen, bei denen InstructGPT normalerweise scheitert?</sample>
    <sample id="290">Input: ein abstraktes Modell

Schritt 1: Generieren von spezifischen Zielen mit der Hilfe von GPT-3

Abstraktes Ziel: Machen eines Cakes

Spezifische Ziele:

G1 (+Modifikator): Machen eines Schokoladen-Cakes

G2 (+Modifikator): Machen eines Cakes in einer bestimmten Farbe

G3 (+Intention): Machen eines Cakes für eine Hochzeit</sample>
    <sample id="291">Input: ein abstraktes Ziel
Schritt 1: Generieren von spezifischen Zielen mit der Hilfe von GPT-3
Abstraktes Ziel: Machen eines Kekses
Spezifische Ziele:
G1 (+Modifikator): Machen eines Schokokekses
G2 (+Modifikator): Machen eines Kekses in einer bestimmten Art
G3 (+Intention): Machen eines Kekses für eine Hochzeit</sample>
    <sample id="292">Input: ein abstraktes Ziel
Schritt 1: Generieren von spezifischen Zielen
Schritt 2: Übergeneration von kandidaten Skripten mit Blick auf den Kontext lernen
Abstraktes Ziel: Ein Kuchen machen
Spezifische Ziele:
G1 (+Modifikator): Ein Schokoladenkuchen machen
G2 (+Methode): Ein Kuchen in einem bestimmten Format machen
G3 (+Intention): Ein Kuchen für eine Hochzeit machen
Kandidatenskripte:
1. Planen für G1
2. Planen für G2
3. Planen für G3
...</sample>
    <sample id="293">Method

Over-generate candidate scripts with InstructGPT via in-context learning
Find filtered scripts to the goal with InstructGPT via similarity score
Output: Specific goals with corresponding scripts</sample>
    <sample id="294">Schritt 3: Finden Sie die skript, die am besten zu Ihrem Ziel passt, mit der InstructGPT via Similaritätsscore.
Ausgabe: Spezifische Ziele mit entsprechenden Skripten.</sample>
    <sample id="295">Schritt 1: Erzeugen von Kandidaten-Skripten mit InstructGPT mittels Kontextlernung
Schritt 2: Finden der Skripte mit dem Ziel mittels InstructGPT über Ähnlichkeitspunkte
Schritt 3: Filtern der Skripte nach dem Ziel
Schritt 4: Ausgabe der spezifischen Ziele mit entsprechenden Skripten</sample>
    <sample id="296">Unser Verfahren verbessert die Planqualität erheblich

Die Grafik zeigt die Genauigkeit verschiedener Sprachmodelle bei der Plangenerierung. Unser Verfahren erreicht signifikant höhere Genauigkeiten als die anderen Modelle, insbesondere als InstructGPT mit 1750M Parameters.</sample>
    <sample id="297">Motivation: Um die einschränkende Fähigkeit von LLMs für kleinere Modelle zu überwinden. Methode: Folgen der Idee des symbolischen Wissensstillings. Generieren 55.000 Skripts mit einem约束条件 from LLMs basierend auf dem Ansatz "Constraint method -&gt; Constraint Dataset". Validierung und Test: Validieren und testen die generierten Skripts.</sample>
    <sample id="298">Motivation:
Um die einschränkende Fähigkeit von Sprachmodellen für kleinere Modelle zu überwinden.

Methode:
Folgen Sie das Konzept der symbolischen Wissensdistillation.
Generieren 55.000 Skriptschwelle basierend auf einem Verhältnis von LLMs zu Skript-Dataset.
Überprüfen und validieren mit Annotate-Validations- und Testset.

Schritt 1: Erstellen von Abstrakten
Erstellen von spezifischen Zielen mit InstructGPT via Kontextlernung.

Schritt 2: Generieren von Kandidaten-Skripten
Mit InstructGPT via Kontextlernung generieren von Kandidaten-Skripten.

Schritt 3: Filtern von Skripten
Mit InstructGPT via Ähnlichkeits评分 filtern von Skripten.

Ausgabe: Spezifische Ziele mit entsprechenden Plänen.</sample>
    <sample id="299">Motivation
Um die einschränkende Fähigkeit von LLMs für kleinere Modelle zu überwinden.
Methode
Nach dem Paradigma des symbolischen Wissensentscheidungsschwemmens arbeiten wir an 55.000 Skripten, die aufgrund von Methoden wie der Validierung und Testset-basierten Annotierung generiert wurden.
Schritt 1: Erstellen von Abstrakten mit InstructGPT durch Kontextlernung.
Schritt 2: Generieren von Kandidaten-Skripten mit InstructGPT unter Kontrolle.
Schritt 3: Filtern der Skripte nach Similarity-Score mit InstructGPT.
Ausgabe: Spezifische Ziele mit entsprechenden Plänen.</sample>
    <sample id="300">Motivation:
Um die einschränkende Fähigkeit von Sprachmodellen für kleinere Modelle zu überwinden.
Methode:
Nach dem Paradigma symbolischen Wissensdistillations.
Erzeugte 55.000 Skripts basierend auf einem Verhältnis von 1:10 mit dem Corpus der Skripts-Datenbank.
Erste Validierung und Testgesamt.

Schritt 1: Erzeugen spezifischer Ziele mit InstructGPT durch in-kontext-Learning.
Schritt 2: Über-generieren kandidatenskripts mit InstructGPT unter Kontrolle.
Schritt 3: Finden und filtern die besten Skripte mit InstructGPT unter Berücksichtigung der Ähnlichkeitspunkte.
Ausgabe: Spezifische Ziele mit entsprechenden Plänen.</sample>
    <sample id="301">Motivation: Um die einschränkende Fähigkeit von Sprachmodellen für kleinere Modelle zu überwinden. Methode: Folgen Sie das Konzept des symbolischen Wissensentscheidens. Generieren 55.000 Skripts mit Bedingungen basierend auf Methoden wie das Corpuscript-Dataset. Überprüfen und testen Sie die Skripts durch Annotate-Validierung und Testset.</sample>
    <sample id="302">Konstruktionsanalyse

Die Grafik zeigt die Häufigkeit von Schlüsselwörtern in einem Text. Die Wörter "By", "With" und "Using" sind am häufigsten verwendet, gefolgt von "Because", "To" und "For". "Date", "Device", "Ingredient", "Method", "Pluralism" und "Specific Goals" sind relativ seltener vorkommende Schlüsselwörter.

Die Grafik zeigt, dass die Häufigkeit von Schlüsselwörtern stark variieren kann, was dazu führt, dass der Text nicht immer leicht zu verstehen ist. Es ist wichtig, die Häufigkeit von Schlüsselwörtern zu beachten, um den Text zu verstehen und zu interpretieren.

Die Grafik zeigt auch, dass die Häufigkeit von Schlüsselwörtern sich je nach Kontext unterscheiden kann. Es ist wichtig, den Kontext zu berücksichtigen, um die Bedeutung von Schlüsselwörtern zu verstehen.

Die Grafik zeigt auch, dass die Häufigkeit von Schlüsselwörtern sich je nach Kontext unterscheiden kann. Es ist wichtig, den Kontext zu berücksichtigen, um die Bedeutung von Schlüsselwörtern zu verstehen.</sample>
    <sample id="303">Spezialisierte Modelle vs. LLMs

Die Grafik zeigt die Genauigkeit verschiedener Sprachmodelle, einschließlich GPT-3 (175B), Codex (175B), InstructGPT (175B) und T5 trainiert auf Coreset, in Abhängigkeit von der Anzahl der Parameter. Es wird erwähnt, dass kleinere LLMs, die auf Coreset optimiert wurden, höhere Qualitätsskripte generieren können als LLMs.

In der Grafik werden die Genauigkeiten der verschiedenen Modelle in einem Balkendiagramm dargestellt. Der x-Achsenlabel "Accuracy" bezieht sich auf die Genauigkeit der Modelle. Die y-Achse zeigt die Anzahl der Parameter in Millionen (Millionen). 

Die Grafik zeigt, dass das Modell "GPT-3 (175B)" die höchste Genauigkeit erreicht hat, gefolgt von "Codex (175B)" und "InstructGPT (175B)". Das Modell "T5 trained on wikiHow" hat dielowest Genauigkeit erreicht.

Es wird erwähnt, dass kleinere LLMs, die auf Coreset optimiert wurden, höhere Qualitätsskripte generieren können als LLMs.</sample>
    <sample id="304">Summarisieren und Auszugswerte

Dieses Bild zeigt einen PowerPoint-Präsentationsslайд mit dem Titel "Summary and Takeaways". Der Inhalt des Slideshow befasst sich mit der "constrained language planning problem" (Problem der eingeschränkten Sprachplanung) und der Entwicklung eines "over-generate-then-filter" Methodenansatzes für "LLMs" (Large Language Models). Es wird erwähnt, dass LLMs verwendet werden, um einen "high-quality script dataset (CoScript)" für die "constrained language planning" zu generieren. 

Die Slideshow enthält auch Punkte über "Limitations and future work" (Grenzen und zukünftige Arbeiten), darunter den "proposed method for improving LLMs as a post-hoc re-ranking approach" (vorgeschlagene Methode zur Verbesserung von LLMs als Nach-Schritt-Rankierungsansatz) und die Tatsache, dass "CoScript only inherits from an abstract one with one constraint" (CoScript nur von einer abstrakten mit einer Einschränkung ableitet).

Es wird auch erwähnt, dass der "CoScript dataset can be a valuable resource for the research on language planning with more complex and diverse goals and constraints" (CoScript-Dataset kann eine wertvolle Ressource für die Forschung über Sprachplanung mit komplexeren und vielfältigen Zielen und Einschränkungen sein).</sample>
    <sample id="305">Summary and Takeaways

* Etablieren des eingeschränkten Sprachplanproblems.
* Evaluieren der Sprachgenerierfähigkeit von LLMs und Entwicklung eines "Over-generate-then-filter"-Methode für LLMs.
* Verwenden von LLMs zur Erstellung eines hohen-Qualitäts-Skriptdatensatzes (CoScript) für eingeschränkte Sprachplanung.
* Limitationen und zukünftige Arbeiten
  * Das vorgeschlagene Verfahren zur Verbesserung von LLMs ist ein "Nach-ranken"- Ansatz.
  * CoScript erbt nur von einem abstrakten Skript mit einem einzigen Constraint.
* Der CoScript-Datensatz kann eine wertvolle Ressource für die Forschung im Bereich Sprachplanung mit komplexeren und vielfältigen Zielen und Bedingungen sein.</sample>
    <sample id="306">Die 61. jährliche Versammlung der Gesellschaft für Computertlinguistik in Toronto, Kanada vom 12. bis 14. April 2023. Titel: "Ernte Skriptkenntnisse aus großen Sprachmodellen für eingeschränkte Sprachplanung". Autoren: Siyu Yuan, Jiangjie Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deging Yang. E-Mail-Adresse: syyuan21@m.fudan.edu.cn. GitHub-Link: https://github.com/siyuyuan/coscript.</sample>
    <sample id="307">Die Sprachgewandtheit von PaLM ist vergleichbar mit den besten Systemen auf dem Markt.</sample>
    <sample id="308">Die wichtigsten Eigenschaften eines Wasserzeichenverfahrens sind die Anwendbarkeit auf EaaS, die Non-Untergewichtung der bereitgestellten Einbettungen, die Covettness und die Transferability.</sample>
    <sample id="309">Die englischen TED Talks wurden in 14 Sprachen übersetzt: Deutsch, Spanisch, Französisch, Italienisch, Japanisch, Koreanisch, Niederländisch, Polnisch, Portisegalisch, Russisch, Simplified Chinese, Traditional Chinese, Thai und Vietnamese.</sample>
    <sample id="310">Eine Gruppe von Instanzen wird extrahiert.</sample>
    <sample id="311">Die Distanzmetriken, die verwendet werden, um den Unterschied zwischen harmlosen und Backdoor-Datensätzen zu messen, sind die Cosinusdistanz, die Mahalanobis-Abstand und die p-Wert des Kolmogorov-Smirnov-Tests.</sample>
    <sample id="312">Sie wurden in einem Monolingual Setting eingesetzt.</sample>
    <sample id="344">Die Autoren zählen die Häufigkeit jedes Wortes in einem allgemeinen Textkorpus und then randomisieren n Wörter in einem mittelstfrequenten Intervall auswählen.</sample>
    <sample id="345">Wie gut noch heute arbeiten CoNLL-2003 Named Entity Taggers?</sample>
    <sample id="346">Named Entity Recognition &amp;amp; Generalization</sample>
    <sample id="347">Named Entity Recognition &amp; Generalization Models haben seit fast 20 Jahren CoNLL-2003 verwendet, um NER zu entwickeln.</sample>
    <sample id="348">Named Entity Recognition &amp; Generalization

• Modelle haben seit fast 20 Jahren CoNLL-2003 verwendet, um NER zu entwickeln
• Können diese Modelle auf modernes Datenmaterial generalisieren?
• Was wird benötigt, um eine gute Generalisierung zu erreichen?</sample>
    <sample id="349">Named Entity Recognition &amp; Generalization

• Modelle haben seit fast 20 Jahren CoNLL-2003 verwendet, um NER zu entwickeln.
• Können diese Modelle auf modernes Data generalisieren?
• Was wird benötigt, um eine gute Generalisierbarkeit zu erreichen?</sample>
    <sample id="350">ConLL++ Dataset

AMBASSADOR
O
UNITED
O
NATIONS
O
LINDA
I-PER
THOMAS-GREENFIELD
I-PER</sample>
    <sample id="351">CoNLL+ Datensatz • Collected Reuters-News von 2020 und annotiert mit CoNLL-2003-Annotation guidelines • Fine-tuned 20+ Modelle auf CoNLL-2003 AMBASSADOR O UNITED OORG NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="352">CoNLL++ Datensatz • Sammelte Reuters-News aus dem Jahr 2020 undannotationen mit den Richtlinien von CoNLL-2003 • Fine-tuned 20+ Modelle auf CoNLL-2003 • Evaluiert auf CoNLL-2003 Testset &amp;amp; CoNLL++ • Calculated percentage ΔF1, um die allgemeinheit zu bewerten. AMBASSADOR O TO I-ORG UNITED NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="353">Was wird benötigt, um eine gute Generalisierbarkeit zu erreichen?</sample>
    <sample id="354">Was wird benötigt, um eine gute allgemein anwendbare Modellarchitektur zu erhalten? • Transformer-Modelle allgemeiner als • Modelle mit vielen Parametern • Modelle mit mehr als 1 Milliarde Parameter • Modelle mit mehr als 10 Milliarden Parameter • Modelle mit mehr als 100 Milliarden Parameter</sample>
    <sample id="355">Was wird benötigt, um eine gute Generalisierbarkeit zu erreichen?
• Modellarchitektur
• Transformer-Modelle generalisieren besser
• Modellgröße
• Größere Modelle generalisieren besser
Bildquelle: McCann et al., 2019</sample>
    <sample id="356">Was wird benötigt, um eine gute Generalisierbarkeit zu erreichen?
- Modellarchitektur
- Transformer-Modelle generalisieren besser
- Größere Modelle generalisieren besser
- Anzahl der fine-tuning-Beispiele
- Mehr Beispiele führen zu besseren Generalisierungen

[Diagram]
Die Grafik zeigt die Änderung des Ablaufes (ΔF1) in Abhängigkeit vom Prozent des Trainingsbeispiels. Es zeigt, dass ein höherer Anteil an Trainingsbeispielen zu einem positiven ΔF1-Wert führt, was indirekt zu einer guten Generalisierbarkeit beiträgt.</sample>
    <sample id="357">Was verursacht eine Leistungsinkraftsenkung?</sample>
    <sample id="358">Was verursacht eine Leistungsinkraftsenkung?</sample>
    <sample id="359">Was bewirkt eine Performance-Abnahme?
• Adaptive Overfitting?
• Temporal Drift?</sample>
    <sample id="360">Was bewirkt einen Leistungsverlust?
• Adaptive Übertaktung?
• Temporale Drift?</sample>
    <sample id="361">Was verursacht einen Leistungsverlust?
• Adaptive Übertaktung?
• Temporale Drift?
Eine Grafik zeigt die Leistungsverhältnisse von verschiedenen Algorithmen in Abhängigkeit von den CQLL-2000-F-Scores.</sample>
    <sample id="362">Was bewirkt einen Leistungsverlust?
• Adaptive Übertaktung?
• Keine abnehmenden Gewinne
• Keine beobachtete
• Temporale Drift?</sample>
    <sample id="363">Was bewirkt einen Leistungsverlust?
• Überfitting beim Adapativtraining?
• Keine abnehmenden Gewinne
• Keine beobachtete Temporaldrift?
CQL vs. DDPG auf CQL-2000 P.1 Score</sample>
    <sample id="364">Was bewirkt einen Leistungsverlust? • Adaptive Übertaktung? • Keine beobachteten Rückgewinnungen • Temporale Drift? Name CoNLL-2003 CoNLL++ (AF) Flat 92.26% 84.15% Flair 90.91% 88.46% Pooled Flair 93.15% 88.20% Pooled Flair+ 92.36% 87.09% ELMo 92.11% 90.76% LMoGn 92.11% 90.76%</sample>
    <sample id="365">Was bewirkt einen Leistungsverlust?
• Adaptive Übertaktung?
• Keine beobachteten Rückgänge
• Temporale Drift?
• Leistungen degradieren mit einem größeren temporalen Abstand
Name
CoNLL-2003
CoNLL++ ( % )
Flair
92.46%, 91.09%, 88.46%, 87.52%, 86.49%
-2.69%
Flair+
93.15%, 91.23%, 88.82%, 87.80%, 86.75%
-4.65%
Pooled Flair
92.36%, 91.19%, 87.80%, 87.09%, 86.14%
-5.72%
ELMo
92.11%, 90.76%, 90.76%, 90.76%, 90.76%
-1.43%</sample>
    <sample id="366">Schlussfolgerung
Um ein gutes Generalisierungsverhalten zu erreichen, benötigen wir:
- Bessere Modellarchitektur
- Größere Modellgröße
- Mehr fine-tuning-Beispiele

Die Grafik zeigt die Entwickelung der Genauigkeit von verschiedenen Sprachmodellen im Laufe der Jahre. Von 2004 bis 2012 stagnierte die Genauigkeit auf unter 80%. Ab 2012 begann es, deutlich zu steigen, insbesondere nach dem Einführung des BERT-Modells im Jahr 2019. Das BERT-Large Modell erreichte eine Genauigkeit von über 95% im Jahr 2022.</sample>
    <sample id="367">Schlussfolgerung
Für eine gute Generalisierbarkeit benötigen wir:
- Bessere Modellarchitektur
- Größere Modellgröße
- Mehr einstellebedarf
- Mehr einstellebedarf an Beispielen
Performance-Abnahme wird durch:
- Temporale Drift
- Nichte-adaptives Übergewicht

Die Grafik zeigt die Entwicklung der Leistung von verschiedenen Sprachmodellen im Laufe der Jahre. Es zeigt, dass die Leistung von Modellen wie Stanford-NLP, BERT und LUKS in den letzten Jahren signifikant verbessert wurde.</sample>
    <sample id="368">Schlussfolgerung
Für eine gute Generalisierbarkeit benötigen wir:
- Bessere Modellarchitektur
- Größere Modellgröße
- Mehr fine-tuning Beispiele
Performance-Abnahme wird durch:
- Nichte-adaptive Übergewichtung
- Temporale Drift
- Wie gut noch ConLL-2003-Tagger arbeiten?
Illinois NER Flair BILSTM_CNN_CRF LUKES ConLL 2003 ConLL++ 75 80 85 90 2004 2006 2008 2010 2012 2014 2016 2018 2020</sample>
    <sample id="369">Eine Good-Performance-Generierung erfordert: • Bessere Modellgroesse • Grössere Modellgrösse • Mehr fine-tuning Beispiele • Performance-Abfall Caused by: • Temporal Drift • Non-adaptive Overfitting • Do ConLL-2003 taggers work? Ja, YES!</sample>
    <sample id="370">Paper: https://arxiv.org/abs/2212.09747 Dataset: https://github.com/ShuhanL/acl2023-conllpp Contact: slu775@gatech.edu</sample>
    <sample id="397">16x16 Pixel.</sample>
    <sample id="398">Eine Rolle.</sample>
    <sample id="399">Beispielqualität ist wichtiger als Ähnlichkeit mit dem Ausgangssatz.</sample>
    <sample id="400">Die Arbeiten im erweiterten Experiment konzentrieren sich auf BERT, RoBERTa und ALBERT.</sample>
    <sample id="401">Das Modell verwendet Aufmerksamkeitswerte aus mehreren Ebenen.</sample>
    <sample id="402">Beispiele für direkte Inferenz sind "the first one", "the one I mentioned before" und "the one you pointed out".</sample>
    <sample id="403">The authors belong to the University of Toronto.</sample>
    <sample id="404">There are six authors involved in the work.</sample>
    <sample id="405">Ja, sie wurde als Baseline betrachtet.</sample>
    <sample id="406">A warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="407">Transformer models generalisieren nicht gut.</sample>
    <sample id="408">The test datasets are called RQ2 and RQ3.</sample>
    <sample id="409">Fünf.</sample>
    <sample id="410">Die Autoren arbeiten mit mehreren Modalitäten.</sample>
    <sample id="439">Nach Ansicht der Autoren ist die Integration von Prädiktionszeitliche und Inferenzzeitliche Kenntnis ein zu wenig erforschtes Gebiet im Bereich der NLU.</sample>
    <sample id="440">Zhiyang Xu, Ying Shen, Lifu Huang</sample>
    <sample id="441">Ja, Coscript wurde überprüft.</sample>
    <sample id="442">Die Grenzen bestehender Ressourcen für kontextbasierte Übersetzung liegen in der Unterstützung begrenzter diskursiven Phänomene und Sprachen.</sample>
    <sample id="443">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammad Javad Hosseini, Filip Radlinski, Silvia Paretti, Annie Louis, Google Research</sample>
    <sample id="444">Resolving Indirect Referring Expressions for Entity Selection (AltEntities Corpus) Mohammed Javad Hosseini, Filip Radlinski, Silvia Paretti, Annie Louis. Google Research</sample>
    <sample id="445">Indirekte Beziehungen

• Gott: Verstehen Sie Benutzersprache, wenn sie eine Wahl treffen
• Alternative Frage: "War es einfach für mich?" oder "Ich muss mich fühlen?"
• Direkte Bezugnahme: "Die Erste" (einfach auf mich)
• Indirekte Bezugnahme: "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich)
• Kann nicht den Namen merken: "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich)
• Pronomen sind schwierig zu unterscheiden: "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich)
• Möchte spezifizieren: "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich), "Die Erste" (einfach auf mich)

Die neue eine... Die Lied, das ist nicht energiegeladen.</sample>
    <sample id="446">Indirekte Beziehungen

Ziel: Verstehen Sie, wie Benutzer Sprache verwenden, wenn sie eine Wahl treffen.

Alternatives Wortlaut

Eine direkte Bezug: "Die Erste" - "Die Erste"

Eine indirekte Bezug: "Die Erste" - "Die Erste"

Eine indirekte Bezug: "Die Erste" - "Die Erste"

Eine indirekte Bezug: "Die Erste" - "Die Erste"</sample>
    <sample id="447">Indirekte Verweisungen

* Ziel: Verstehen der Sprache von Benutzern, wenn sie eine Wahl treffen
	+ Alternative: "Did you mean easy on me or I gotta feeling?"
* Direkte Verweisung
	+ "easy on me": die erste
	+ "The newer one." (Die jüngere.)
	+ "The song that's not energetic." (Die nicht energiegeladen ist.)
* Indirekte Verweisung
	+ "the first one"</sample>
    <sample id="448">Indirekte Beziehungen

Das Ziel: Verstehen Sie, wie Benutzer Sprache verwenden, wenn sie eine Wahl treffen.

Alternative: "War's einfach für mich?" vs. "Oder ich muss mich fühlen?"

Direct Reference:
- Indirektes Verweis auf "mich": "Die Erste" 
- Kann den Namen nicht mehr erinnern
- Schwierigkeiten beim Distanzieren der Aussprachen
- Möchten spezifizieren, was sie wollen

Indirektes Verweis:
- "Die neueren" 
- "Die Energetische"</sample>
    <sample id="449">Indirekte Verweisungen

• Ziel: Verstehen der Sprache von Benutzern, wenn sie eine Wahl treffen
• Alternative:
  - "Did you mean easy on me or I gotta feeling?"
• Direkte Verweisung
• Indirekte Verweisung
  - "The newer one."
  - "The song that's not energetic."

Google Research</sample>
    <sample id="450">Dataset Collection

*   Important problem
    *   Conversational systems
    *   Benchmarking Large Language Models' entity understanding
    *   No large-scale public dataset available
*   We collect a large dataset using crowd annotation
*   Three domains:

Resolving Indirect Referring Expressions by Entity Selection (Mnli+ Corpus)</sample>
    <sample id="451">Dataset Collection

•	Important problem
	- Conversational systems
	- Benchmarking Large Language Models' entity understanding
	- No large-scale public dataset available
•	We collect a large dataset using crowd annotation
•	Three domains:

Resolving Indirect Expressions by Entity Selection (MellieDex Corpus)</sample>
    <sample id="452">Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe

• Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe

- Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe

- Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe

- Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe</sample>
    <sample id="453">Methodik betont die Informalität durch die Ausführung einer Zeichnungsvollstelle Aufgabe.</sample>
    <sample id="454">Dataset Collection Methodologie

Die Methode betont die Informalität durch die Ausführung einer Zeichnung-Vollstelle-Aufgabe.

Die Dialogkontexte (ausgewählt aus ein paar manuelle prompts pro Monat) werden anhand von einem der folgenden Beispiele dargestellt:

1. "Remember that song we were listening to yesterday?"
2. "Do you mean the one with the 1 Gotta Feeling?"

Die alternative Frage und die Expression, die sich auf einen der zwei Entitäten bezieht, werden von einem Annotator gefüllt.</sample>
    <sample id="455">Methodik betont die Informalität durch die Ausführung einer Cartoon-Vollstelle Aufgabe. Erinnere dich, dass wir gestern einen Film gesehen haben. Wie heißt er? Gotta Feeling?</sample>
    <sample id="456">Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe

• Methodik betont die Informalität mithilfe einer Cartoon-Vollstelle Aufgabe
• Das Dialog-Unterwerp (ausgewählt aus ein paar manuelle prompts pro Monat) wird anhand von einem der enthaltenen Entitys befüllt.
• Der alternative Punkt ist "Do you mean I was happy yesterday?"</sample>
    <sample id="457">Methodik betont die Informalität durch die Ausführung einer Cartoon-Vollstelle Aufgabe.</sample>
    <sample id="458">Generieren Sie alternative Fragen = Stichproben von Entity-Paaren

Die Pfeile in der Grafik weisen auf die verschiedenen Methoden hin, um alternative Fragen zu generieren. Hier sind einige Beispiele:

1. "Items with similar infoboxes on Wikipedia (same genre and/or artist)" - Dinge mit ähnlichen Infoboxen auf Wikipedia (gleicher Gattung und/oder Künstler)
2. "Items with similar descriptions on Wikipedia" - Dinge mit ähnlichen Beschreibungen auf Wikipedia
3. "Do you mean Thinking of You or Happy Anywhere?" - Bedarfst du an "Thinking of You" oder an "Happy Anywhere"?
4. "Do you mean The Return (memoir) or The Return (Shatner novel)" - Bedarfst du an "The Return (Memoir)" oder an "The Return (Shatner Roman)"?
5. "Items with similar titles" - Dinge mit ähnlichen Titeln
6. "Items with similar genres" - Dinge mit ähnlichen Genres
7. "Items with similar artists" - Dinge mit ähnlichen Künstlern
8. "Items with similar styles" - Dinge mit ähnlichen Stilen
9. "Items with similar themes" - Dinge mit ähnlichen Themen
10. "Items with similar topics" - Dinge mit ähnlichen Themen
11. "Items with similar subjects" - Dinge mit ähnlichen Objekten
12. "Items with similar contexts" - Dinge mit ähnlichen Kontexten
13. "Items with similar purposes" - Dinge mit ähnlichen Zwecken
14. "Items with similar functions" - Dinge mit ähnlichen Funktionen
15. "Items with similar uses" - Dinge mit ähnlichen Anwendungen
16. "Items with similar meanings" - Dinge mit ähnlichen Bedeutungen
17. "Items with similar interpretations" - Dinge mit ähnlichen Interpretationen
18. "Items with similar applications" - Dinge mit ähnlichen Anwendungen
19. "Items with similar implementations" - Dinge mit ähnlichen Umsetzungen
20. "Items with similar outcomes" - Dinge mit ähnlichen Auskommen
21. "Items with similar results" - Dinge mit ähnlichen Folgen
22. "Items with similar consequences" - Dinge mit ähnlichen Folgen
23. "Items with similar effects" - Dinge mit ähnlichen Auswirkungen
24. "Items with similar impacts" - Dinge mit ähnlichen Auswirkungen
25. "Items with similar influences" - Dinge mit ähnlichen Einflüssen
26. "Items with similar causes" - Dinge mit ähnlichen Ursachen
27. "Items with similar reasons" - Dinge mit ähnlichen Gründen
28. "Items with similar explanations" - Dinge mit ähnlichen Erklärungen
29. "Items with similar justifications" - Dinge mit ähnlichen Begründungen
30. "Items with similar arguments" - Dinge mit ähnlichen Argumenten
31. "Items with similar claims" - Dinge mit ähnlichen Behauptungen
32. "Items with similar statements" - Dinge mit ähnlichen Aussagen
33. "Items with similar assertions" - Dinge mit ähnlichen Behauptungen
34. "Items with similar declarations" - Dinge mit ähnlichen Erklärungen
35. "Items with similar proclamations" - Dinge mit ähnlichen Erklärungen
36. "Items with similar proclamations" - Dinge mit ähnlichen Erklärungen
37. "Items with</sample>
    <sample id="459">Generieren Sie alternative Fragen = Stichproben von Entity-Paaren

Do you mean A or B?

Gleiche Items mit "similar infoboxes" auf Wikipedia (gleicher Genre und/oder Künstler) Gleichartige Items mit "similar descriptions" auf Wikipedia Gleichartige Items mit "similar titles" auf Wikipedia

Uniform:

Do you mean The Return (Memoir) or The Return (Shater Novelle)?

Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="460">Generieren Sie alternative Fragen = Stichproben von Entity-Paaren

Dieses Bild zeigt einen Slideshow-Frame, der die Idee von "Generieren Sie alternative Fragen = Stichproben von Entity-Paaren" präsentiert. Es zeigt auch Beispiele von similar infoboxes auf Wikipedia (gleiche Genre/Artist), similar descriptions auf Wikipedia und Items mit similar titles auf Wikipedia.</sample>
    <sample id="461">Generieren Sie alternative Fragen = Stichproben von Entity-Paaren

Die Grafik zeigt einen Vorgang, bei dem alternative Fragen generiert werden, indem Paare von Entities extrahiert werden. Es wird angenommen, dass die Entities in einem Infobox-Format auf Wikipedia enthalten sind und dass sie entweder dieselbe Genre oder Künstler haben (ähnliche infoboxen) oder dieselben Beschreibungen verwenden (ähnliche Beschreibungen). Alternativ können auch Items mit ähnlichen Titeln (ähnliche Titel) verwendet werden. Der Prozess sieht auch die Möglichkeit vor, Items zufällig zu auswählen (Uniform at random). Beispiele für alternative Fragen, die auf diese Weise generiert werden können, sind "Glaubst du an 'This Is It' oder 'In den Spiegel'?" und "Glaubst du an 'Du denkst an mich' oder 'Glücklich, aber nicht glücklich'?".

Bildunterschrift: "Google Research"</sample>
    <sample id="462">Generieren Sie alternative Fragen = Stichproben von Entity-Paaren

Do you mean A or B? → Items mit "similar infoboxes" auf Wikipedia (gleiche Genre/Artist), Items mit "similar descriptions" auf Wikipedia, Items mit "similar titles"

Do you mean Thinking of You or Happy Anywhere? → Items mit "similar infoboxes" auf Wikipedia (gleiche Genre/Artist), Items mit "similar titles"

Do you mean The Return (memoir) or The Return (Shatner novel)? → Items mit "similar infoboxes" auf Wikipedia (gleiche Genre/Artist), Items mit "similar titles"

Uniform at random: Do you mean You Could Be Mine or The Way I Am? → Items mit "similar infoboxes" auf Wikipedia (gleiche Genre/Artist), Items mit "similar titles"</sample>
    <sample id="463">Google Research

Background knowledge (Music)

Google search link to find out about the song.

I Gotta Feeling (by The Black Eyed Peas)

Click here to find out about the song.

We ask annotators to:

Listen to at least some of each song

Read about each song</sample>
    <sample id="464">Google search link (Music) Easy on Me (by A$J) I Gotta Feeling (by The Black Eyed Peas) Click here to find out about the song. Click here to find out about the song.</sample>
    <sample id="465">Google Research

Background knowledge (Music)

Google search link to each song.

Easy on Me (by Adele)

I Gotta Feeling (by The Black Eyed Peas)

Click here to find out about the song.

Click here to find out about the song.

We ask annotators to:

Listen at least some of each song.

Read about each song.

Resolving Indirect References by Entity Selection (Mentions Corpus)</sample>
    <sample id="466">Simnel cake is a fruitcake widely eaten in the United Kingdom, Ireland and other countries with patterns of migration from them, associated with Lent and Easter. It is distinguished by layers of almond paste, made of eleven balls of the same paste. Pandan cake is a light, fluffy sponge cake flavoured with the juices of Pandanus amaryllifolius leaves. The cake is popular in Indonesia, Malaysia and also in the Netherlands, especially among the Indo community.</sample>
    <sample id="467">Eliciting expressions

Wir teilen den Annotationen mit, welche Wahl ausgewählt werden sollte und bitten sie zu beschreiben, warum.

Eine der folgenden Optionen markieren:

- "Easy on Me" (von Adele)
- "I Gotta Feeling" (von The Black Eyed Peas)

Wir möchten Ihnen 3 bis 5 Ausdrücke für das gewählte Lied zur Sprachproduktion übermitteln. Hier sind einige Beispiele:

- "Das Lied hat eine bestimmte Art von Musik" (z.B. "Easy on Me" von Adele)
- "Das Lied ist nicht energiegeladen" (z.B. "Easy on Me" von Adele)
- "Das Lied handelt von einem bestimmten Thema" (z.B. "Easy on Me" von Adele)
- "Das Lied handelt von einer bestimmten Situation" (z.B. "Easy on Me" von Adele)
- "Das Lied handelt von einer bestimmten Person" (z.B. "Easy on Me" von Adele)

Wir bitten Sie, 3 bis 5 Ausdrücke für das gewählte Lied zu warten.</sample>
    <sample id="468">Eliciting expressions

Wir then tell the annotators which choice should be selected and ask them to describe it.

Pick one

Easy on Me (by Adele)

I Gotta Feeling (by The Black Eyed Peas)

We would like you to give us 3 to 5 expressions for the chosen song to fill in your speech bubble, not examples.

One with the music

The song that's not energetic

It has a river

It's the newer one

It's about not having time to choose</sample>
    <sample id="469">AltEntities Corpus

- 6.000 alternative Fragen über drei Domänen
- 42.000 indirekte Beziehungen

Ergebnisse mit T5 XL Modell (Genauigkeit):
- 92-95% wenn die LM Zugriff auf dieselbe Hintergrundknowledge als Annotatoren hat.
- 82-87% wenn die LM Zugriff auf teilweise überlapende Hintergrundknowledge hat.
- 60% wenn die LM (T5 XL) nur Zugriff auf die Entity-Namen hat.

Wir haben gezeigt, dass Modelle allgemein für verschiedene Domänen geeignet sind.

Datenbanklink: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="470">AltEntities Corpus

- 6.000 alternative Fragen über drei Domänen
- 42.000 indirekte Beziehungen
- Resultate mit T5-Modell (Genauigkeit):
    - 92-95% wenn das LM Zugriff auf dieselbe Hintergrundknowledge als Annotatoren hat
    - 82-87% wenn das LM Zugriff auf teilweise überlapptes Hintergrundknowledge hat
    - 60% wenn das LM (T5 XL) nur Zugriff auf die Entity-Namen hat
- Wir haben gezeigt, dass Modelle allgemein für verschiedene Domänen geeignet sind.
- Dataset-Link: &lt;https://github.com/google-research-datasets/AltEntities&gt;

[Google Research]</sample>
    <sample id="471">AltEntities Corpus

- 6.000 alternative Fragen über drei Domänen
- 42.000 indirekte Beziehungen

Ergebnisse mit T5 XL Modell (Genauigkeit):
- 92-95% wenn das LM Zugang zu den gleichen Hintergrundwissen wie die Annotatoren hat.
- 82%-87% wenn das LM Zugang zu teilweise überschneidenden Hintergrundwissen hat.
- 60% wenn das LM (T5 XL) nur Zugang zu den Entity-Namen hat.

Wir haben gezeigt, dass die Modelle allgemein für verschiedene Domänen geeignet sind.

Datenbank-Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="472">AltEntities Corpus

- 6.000 alternative Fragen über drei Domänen
- 42.000 indirekte Beziehungen
- Resultate mit T5 XL Modell (Genauigkeit):
    - 92-95% wenn die LM Zugriff auf das gleiche Hintergrundwissen als Annotatoren.
    - 82-87% wenn die LM Zugriff auf teilweise überlappendes Hintergrundwissen hat.
    - 60% wenn die LM (T5 XL) nur Zugriff auf die Entity-Namen hat.
- Wir haben gezeigt, dass die Modelle allgemein für verschiedene Domänen geeignet sind.
- Dataset-Link: &lt;https://github.com/google-research-datasets/AltEntities&gt;

Google Research</sample>
    <sample id="473">Wait-k, LA, CAAT, EDAtt</sample>
    <sample id="474">Nantes Université</sample>
    <sample id="475">Sebastin Sanyt</sample>
    <sample id="476">Drei.</sample>
    <sample id="477">"Aufmerksamkeit als Führer für die gleichzeitige Übersetzung von Sprachen"</sample>
    <sample id="478">"Konzentration als Führer für die gleichzeitige Übersetzung von mehreren Sprachen"</sample>
    <sample id="479">Was sind die Probleme der aktuellen SimulST-Modelle?</sample>
    <sample id="480">Was sind die Probleme der aktuellen SimulST-Modelle?</sample>
    <sample id="481">Die aktuellen SimulST-Modelle haben mehrere Probleme. Spezifische Architekturen werden normalerweise trainiert, indem zusätzliche Module hinzugefügt werden, um optimiert zu werden. Lang und komplexere Trainingverfahren (z.B. verschiedene Optimiererziehungen) werden verwendet. Training und Wartung mehrerer Modelle zur Erreichung verschiedener Latenzziehungen (z.B. 1s, 2s, ...) werden durchgeführt.</sample>
    <sample id="482">Was ist unsere Lösung?</sample>
    <sample id="483">Was ist unsere Lösung?</sample>
    <sample id="484">Was ist unsere Lösung?</sample>
    <sample id="485">Our solution: EDAtt</sample>
    <sample id="486">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝜆) towards the last 𝑘 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="487">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="488">Decide whether to emit or not a partial translation based on where attention points to: a word attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="489">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold λ) towards the last λ speech frames, meaning that the received information is enough stable.</sample>
    <sample id="490">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="491">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="492">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough.</sample>
    <sample id="493">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below the threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="494">Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (it's sum is below a threshold 𝛼) towards the last 𝜆 speech frames, meaning that the received information is enough stable.</sample>
    <sample id="495">Main Results: EDAtt</sample>
    <sample id="496">Main Results: EDAtt</sample>
    <sample id="497">Main Results: EDAtt</sample>
    <sample id="498">Main Results: EDAtt</sample>
    <sample id="499">Main Results: EDAtt AL / AL-CA (s) BLEU 27 25 23 21 19 17 15 0.5 1.5 2.5 3.5 4.5 5</sample>
    <sample id="500">Main Results: EDAtt popular strategies also applied to offline models wait-k LA CAAT EDAtt 27 25 23 21 19 17 0.5 1 1.5 2 2.5 3.5 4.5 5 AL / AL-CA (s) (a) de-en</sample>
    <sample id="501">Main Results: EDAtt state of the art architecture, specifically tailored for SimuST wait-k LA CAAT EDAtt</sample>
    <sample id="502">Main Results: EDAtt Results wait-k LA CAAT EDAtt 27 25 23 21 19 17 0.5 1 2 2.5 3.5 4.5 5 AL / AL-CA (s) (a) de-en</sample>
    <sample id="503">EDAtt über perforiert alle Strategien, die auf offline-Modellen angewendet wurden.</sample>
    <sample id="504">Hier ist die Übersetzung des englischen Textes ins Deutsche:

"Willst du mehr entdecken?
Les uns unser Papier, um noch mehr Ergebnisse zu entdecken!
(sapi.negri@fbk.eu
marco.turchi@gmail.com
github.com/hlt-mt/fbk-fairseq
Scan mich!
@fbk_mt
@sarapapi

Seite 038"</sample>
    <sample id="505">Ja, der Datensatz ist öffentlich zugänglich.</sample>
    <sample id="506">MULTINSTRUCT: Verbesserung von multi-modalem Null-Shot-Learning durch die Optimierung via Anweisungs-Training
Zhiyang Xu*, Ying Shen*, Lifu Huang
Abteilung für Computertechnik, Virginia Tech</sample>
    <sample id="507">Vortraining von Sprachmodellen für Downstream-Aufgaben

Die Grafik vergleicht verschiedene Ansätze zur Optimierung von Sprachmodellen: vortraining, Fine-Tuning und Prompting. 

A) Vortraining-Finetuning (BERT, T5):
- Vortraining auf einem allgemeinen Textkorpus
- Fine-Tuning auf spezifischen Aufgaben (beispielsweise Task A)
- Das Modell lernen viele allgemeine Sprachkompetenzen auf einem großen Datensatz

B) Prompting (GPT-3):
- Das Modell wird mit spezifischen Anregungen (Prompts) trainiert
- Das Modell verbessert seine Leistung auf verschiedenen Aufgaben durch das Anpassen der Anregungen
- Es ist nicht notwendig, das Modell zu fine-tune

C) Anweisungsstimmung (FLAN):
- Das Modell wird vortrainiert und dann auf bestimmte Aufgaben fine-tuned
- Das Modell lernen viele Aufgaben über den Anweisungen zu lösen
- Es kann auf unbekannte Aufgaben inferieren

Die Grafik zeigt, dass verschiedene Ansätze zur Optimierung von Sprachmodellen existieren und jede Methode hat ihre eigenen Vorteile und Nachteile.</sample>
    <sample id="508">Vortrainierte Sprachmodelle für Downstream-Aufgaben

(A) Vortraining-finetuning (BERT, T5)
- Vortraining auf Aufgabe A
- Typischerweise eine allgemeine Sprachmodell-Form
- Optimiert für jede spezifische Aufgabe

(B) Prompting (GPT-3)
- Optimiert die Leistung durch Anpassung der Eingabe oder des Prompts
- Infere auf vielen Aufgaben über natürliche Sprache

(C) Anweisungstuning (FLAN)
- Vortraining auf Aufgabe A
- Modell lernen, viele Aufgaben über natürliche Sprache zu perforieren
- Infere auf unbekannte Aufgaben

Abbildung 2: Vergleich von Anweisungstuning mit vortraining-finetuning und prompting.

Quelle: Weil, Jason, et al. "Vortrainingssprachmodelle sind null-Shot-Lerner."</sample>
    <sample id="509">Sprache-only</sample>
    <sample id="510">Multimodale vorher geschulte Modelle</sample>
    <sample id="511">As an example, I will show you a dataset that was used in a recent paper on multimodal instruction tuning.</sample>
    <sample id="512">Asymmetrie in den Ausprägungen von NLP und Multimodal datasets

Über 1600 Sprach-only-Anweisungsaufgaben</sample>
    <sample id="513">Multinstruct - Das erstmalige multimodale Anweisungstuning-Benchmarksatz

Das Multinstruct ist das erstmalige multimodale Anweisungstuning-Benchmarksatz, das 62 diverse Aufgaben und 10 breite Aufgabengruppen umfasst. Es enthält auch 5 Experten geschriebene Anweisungen.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation), visuelles Verstehen (Visual Understanding), visuelles Matchen (Visual Matching) und regionales Verstehen (Region Understanding). Jede Kategorie umfasst mehrere spezifische Aufgaben, die in Farben codiert sind.

Die Aufgaben im Multinstruct werden in verschiedene Kategorien unterteilt, darunter visuelles Beziehungsnetz (Visual Relationship), visuelle Substitution (Visual Substitution), visuelles Budget (Visual Budget), visuelles Argumentieren (Visual Argumentation),</sample>
    <sample id="514">Multinstruct
Das erstmalige multimodale Anweisung-tuning-Benchmarksatz

Die rechtsseitige Grafik zeigt die verschiedenen Aufgaben und Anweisungen, die in dem Benchmarksatz enthalten sind. Hier ist eine detaillierte Übersetzung:

1. **Visuelle Beziehungen**:
   - **VGA**: Video Graphics Array
   - **Temporal Ordering**: Temporales Ordnung
   - **Grounded Matching**: Grounded Matching
   - **Visual Subjekt**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**: Visuelles Objekt
   - **Visual Object**:</sample>
    <sample id="515">Eine vereinbarte, mehrmodaler vortrainierter Modell, das in der Lage ist, sowohl Single- als auch mehrfache Modalitäten mit auszuführen. OFA hat einen vereinbarten Vokabular, der die Sprache für Sprache- und Bildtoken und die Koordinaten einer Umgebung ein einzigartigen Ansatz.</sample>
    <sample id="516">Input: Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Output:</sample>
    <sample id="517">Input: Grounded Caption Text Localization Referring Expression Selection Question-Image Matching</sample>
    <sample id="518">Input: Grounded Caption Text Localization Referring Expression Selection Question-Image Matching Output:</sample>
    <sample id="519">Multi-modal Instruction Tuning</sample>
    <sample id="520">Multi-Modal Instruction Turning

Training Dataset Construction:
- Nutzen 53 Aufgaben aus 9 Gruppen für die Ausbildung.
- Extrahieren 10.000 Instanzen pro Aufgabe.

Testing Dataset Construction:
- Reservieren der gesamten Gruppe "Commonsense Reasoning" für die Testphase.
- Auswählen 5 zusätzliche Aufgaben von den Gruppen "VQA" und "Miscellaneous".
- Zufällig 20 Aufgaben aus dem Testteil des Datensatzes "Natural Instructions" als unbekannte Aufgaben für NL.P.</sample>
    <sample id="521">Multi-Modal Instruction Turning

Training Dataset Construction:
- Nutzen 53 Aufgaben aus 9 Gruppen für die Ausbildung.
- Extrahieren 10.000 Instanzen pro Aufgabe.

Testing Dataset Construction:
- Reservieren der gesamten Gruppe "Commonsense Reasoning" für die Testphase.
- Auswählen 5 zusätzliche Aufgaben aus den Gruppen Q&amp;A und Verschiedene Gruppen.
- Zufällig auswählen 20 Aufgaben aus dem Testteil des Datensatzes "Natural Instructions" als unbekannte Aufgaben für NLP.</sample>
    <sample id="522">Implementierungsdetails:

• Trainingdetails:
- Vorausgebildeter OFA-Large Modell (472 Mio.)
- Misken aller Aufgabeninstanzen.
- Jede Instanz wird zufällig mit einem von seinen Anweisungsvorlagen kombiniert.

• Testdetails:
-Für jede Aufgabe führen wir eine Gesamtzahl von five Experiments durch, indem wir den Modellperformance unter Verwendung von einem der five Anweisungen in jeder Versuch evaluieren.
- Wir berichten die durchschnittliche und maximale Performance und die Standardabweichung der Performance über alle five Experiments.</sample>
    <sample id="523">Implementierungsdetails:

• Trainingdetails:
- Vorausgebildeter OFA-Large Modell (472M)
- Alle Instanzen für alle Aufgaben mischen.
- Jede Instanz wird zufällig mit einem von seinen Anweisungsvorlagen kombiniert.

• Testdetails:
-Für jede Aufgabe führen wir eine Gesamtzahl von fünf Experimenten durch, indem wir den Modellperformance unter Verwendung von einem der fünf Anweisungen in jeder Versuch evaluieren.
- Wir berichten die durchschnittliche und maximale Leistung und die Standardabweichung der Leistung über all five Experiments.</sample>
    <sample id="524">Implementierungsdetails:

• Trainingdetails:
- Vorausgebildeter OFA-Large Modell (472 M)
- Verwenden aller Instanzen für alle Aufgaben.
- Jede Instanz wird zufällig mit einem von seinen Anweisungsvorlagen kombiniert.

• Testdetails:
-Für jede Aufgabe führen wir eine insgesamt von fünf Experimente durch, indem wir den Modellperformance unter Benutzung von einer der fünf Anweisungen in jeder Versuch evaluieren.
- Wir berichten die durchschnittliche und maximale Performance und die Standardabweichung der Performance über alle fünf Experimente.</sample>
    <sample id="525">Evaluationsmetriken

Für multi-modale Klassifizierungsaufgaben (Visuelle Bedeutung, Visuelle Räumliche Grundsatz, Natürliche Sprache Visual Grundsatz und Katastrophen Typ Klassifizierung) berichten wir über die Genauigkeit.

Für multi-modale Generationsaufgaben (Allgemeinheit, Text VQA, Grounded VQA, Visuelle Text Extraktion, visuelles Dialog) berichten wir über die Rouge-L.

Für NLP Aufgaben berichten wir über Rouge-L als Perfektionsscore für die meisten Aufgaben. Wir haben nicht nur die Genauigkeit als Metrik.

Wir berechnen auch die aggregierte Leistung für jedes Modell basierend auf dem Durchschnitt der Leistung des Modells auf allen multi-modalen und NLP Aufgaben.</sample>
    <sample id="526">Empfindlichkeit

Wie empfindlich ist das Modell gegenüber einer Vielzahl von Anweisungen für die gleiche Aufgabe?

E_{\theta,T} = \frac{\sigma_{i,j}(T) \cdot L(f_0(i, x, y))}{\mu_{i,j}(T) \cdot L(f_0(i, x, y))}</sample>
    <sample id="527">Die Effektivität der Ausbildung der Instruktion auf MULTINSTRUCT

Die Tabelle zeigt die Leitfähigkeit verschiedener Modelle auf verschiedenen Aufgaben wie "Rouge", "Commonsense VQA", "ACC" usw. Die besten Leistungen sind in Fettschrift hervorgehoben.

Tabelle 1: Null-Shot-Performance auf multimodalen allgemeinen Intelligenz Aufgaben. Das beste Modell ist in Fettschrift hervorgehoben.

Die Tabelle zeigt die Leitfähigkeit verschiedener Modelle auf verschiedenen Aufgaben wie "Test VQA", "Grounded VQA", "Visual Text Extraction", "Visual Dialogue", "Disaster Type Classification" usw. Die besten Leistungen sind in Fettschrift hervorgehoben.

Tabelle 2: Null-Shot-Performance auf Fragenaufgaben und Mikrosofern. Das beste Modell ist in Fettschrift hervorgehoben.</sample>
    <sample id="528">Wir haben zwei Tabellen, die uns dabei helfen, zu verstehen, wie gut verschiedene Modelle auf einer Aufgabe namens "MultiInstruct" performieren. 

Die erste Tabelle zeigt, wie gut verschiedene Modelle auf verschiedenen Aufgaben wie "Rouge", "Commonsense VQA", "ACC" usw. performieren. Hier ist ein Beispiel: Das Modell "OFA" hat bei der Aufgabe "Rouge" eine Punktzahl von 49,97.

Die zweite Tabelle zeigt, wie gut das Modell "OFA" auf verschiedenen Aufgaben wie "Test VQA", "Grounded VQA", "Visual Text Extraction" usw. performiert. Hier ist ein Beispiel: Das Modell "OFA" hat bei der Aufgabe "Test VQA" eine Punktzahl von 15,21.

Die Tabellen helfen uns zu sehen, dass das Modell "OFA" in der Regel gut performiert und oft die beste Punktzahl hat.</sample>
    <sample id="529">Die Wirkung der Steigerung von multimodalen Anweisungs Aufgabengruppen

Die rechte Seite des Bildes zeigt ein Diagramm, das die Leistungsverläufe von verschiedenen Aufgabentypen im Laufe der Aufgabengruppensteigerung zeigt. Die Aufgabentypen sind "Long", "MISC", "ITM", "Relation" und "Region". Das Diagramm zeigt, dass die Leistungen in den Aufgabengruppen "Long" und "MISC" steigern, während die Leistungen in den Aufgabengruppen "ITM", "Relation" und "Region" abnehmen.

Auf der linken Seite des Bildes werden die Aufgabengruppen "Img Und", "Grounding", "MISC", "ITM", "Relation", "Region" und "NLP" aufgelistet. Jede Aufgabengruppe enthält verschiedene Aufgaben wie "VQA + Image Understanding", "Temporal Ordering + Miscellaneous + Image Texting", "Visual Relationship" und "NLP tasks".

Das Bild zeigt also, dass die Wirkung der Steigerung von multimodalen Anweisungs Aufgabengruppen variiert, je nach Aufgabentyp.</sample>
    <sample id="530">Wirkung von diversen Anweisungen auf die Optimierung von Anweisungen

- OFA optimiert nach 5 Anweisungen erreicht ein viel höhere aggregierter Leistungsunterschied auf allen Auswertungsaufgaben und zeigt eine geringere Sensitivität.
- Tabelle 3: Wirkung verschiedener Anzahl von Anweisungen. Leistungsunterschied von OFA (Erweiterung) nach Optimierung, optimiert auf verschiedenen Mengen von Anweisungen.

Hier ist die Übersetzung des englischen Inhalts ins Deutsche:

- OFA nach Optimierung auf 5 Anweisungen erreicht einen viel höhern aggregierten Leistungsunterschied auf allen Auswertungsaufgaben und zeigt eine geringere Sensitivität.
- Tabelle 3: Wirkung verschiedener Anzahl von Anweisungen. Leistungsunterschied von OFA (Erweiterung) nach Optimierung, optimiert auf verschiedenen Mengen von Anweisungen.</sample>
    <sample id="531">Wirkung von Strategien zur fein-tuning auf die Sensibilität von Modellen

- Tuning der Anweisungen an Multinstruct kann die Sensibilität eines Modells signifikant reduzieren.
- Das Übertragen von Lernprozessen aus dem Natural Instructions Datensatz kann die Sensibilität des Modells weiter reduzieren.

Die Grafik zeigt die Sensibilität von Modellen an unsehenen Evaluationstasks. Eine niedrigere Zahl bedeutet eine bessere Performance.</sample>
    <sample id="532">Zero-Shot Performance auf NLP Aufgaben

• Die Anweisungstuning auf MultiInstruct kann die zero-shot-Performance auf unbekannte NLP-Aufgaben verbessern.
• Die Transferlearning-Strategie MixedInstruct kann am besten die null-Shot-Fähigkeit gewinnen, die durch das Datensatz-Natürliche Anweisungen erworben wurde.

Tabelle 4: Zero-Shot-Performance auf NLP-Aufgaben. Die Null-Shot-Performanzen werden in der Rouge-L-Metrik berichtet, und die beste Performance ist fett.</sample>
    <sample id="533">Schlussfolgerung

• Erster große Datensatz zur multi-modalen Anweisungsfeinabstimmung.
    • Enthält 62 multi-modalen Aufgaben aus 10 breiten Kategorien.
• Significantes Verbesserungspotenzial der Null-Shot-Fähigkeit von OFA durch die Anweisungsfeinabstimmung.
• Mehrere Transferlearning-Techniken untersucht und ihre Vorteile gezeigt.
• Eine neue Sensitivitätsmessung entworfen.</sample>
    <sample id="534">Eine weitere Sache! Wir sammeln eine viel größere multimodale Anweisungstuning-Datenbank mit etwa 150 zusätzlichen visio-natur Sprach Aufgaben und werden sie bald freigeben.</sample>
    <sample id="535">Universita Di Trento</sample>
    <sample id="536">Mohammad Javad Hosseini</sample>
    <sample id="562">Sprachmodell akzeptierbarkeitsbewertungen sind nicht immer robust gegenüber Kontext ACL 2023 Johns Hopkins University Purdue University MIT</sample>
    <sample id="563">Sprachmodell-Acceptabilitätseinstimmungen sind nicht immer robust gegenüber Kontext ACL 2023 Johns Hopkins University Purdue University MIT Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams Meta</sample>
    <sample id="564">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping 2. "The customer ... spent any money." 2. The customer ... spent any money. sentence. 2. Many people were helping herself. P(1.) &gt; P(2.) P(1.) &gt; P(2. any) P(1.) &gt; P(2.)</sample>
    <sample id="565">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical helping, 2. "The customer ... spent any money." 2. Non-stereotypical themselves. We're helping herself." P(1) &gt; P(2) P(1) &gt; P(2)any P(1) &gt; P(2)</sample>
    <sample id="566">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.</sample>
    <sample id="567">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Viele Leute wurden 1. Kein Kunde … hat gespendet 1. Stereotypische 2. selbst. 2. „Der Kunde … hat gespendet any money. 2. Stereotypische 3. Viele Leute 3. „Der Kunde … hat gespendet any money. 3. Non-stereotypische 4. helfen. 4. WIR … haben gespendet 4. Stereotypische 5. sich selbst. 5. uns selbst. 5. Non-stereotypische P(1) &gt; P(2) P(1) &gt; P(Any) P(1) &gt; P(2)</sample>
    <sample id="568">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.</sample>
    <sample id="569">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs.</sample>
    <sample id="570">Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Viele Leute wurden 1. Kein Kunde ... hat gespendet 1. Frauen sind 2. *Viele Leute 2. *Der Kunde ... hat gespendet 2. Handwerk, 3. *Wir halfen 3. *Eine Person ... hat gespendet 3. Männer sind schrecklich. sich selbst* 4. *Wir halfen 4. *Eine Person ... hat gespendet 4. at handwerk. 5. *Wir halfen 5. *Eine Person ... hat gespendet 5. uns selbst* P(1.) &gt; P(2.) P(1.any) &gt; P(2.any) P(1.) &gt; P(2.) Are these judgements stable with long preceding context?</sample>
    <sample id="571">Ansatz Test, ob die MPP-Bewertungen sich als eine Funktion von Kontextlänge, struktureller Übereinstimmung und Akzeptabilität verändern. Test-Funktion: Verweis Übereinstimmung akzeptabel P (Prefix) &gt; P (Lauter) Prefix Space of Candidate Prefixes Einfluss der Strukturell-Übereinstimmung auf die Akzeptabilität:</sample>
    <sample id="572">Ansatz
Test, ob die MPP-Bewertungen sich als Funktion von Kontextlänge, struktureller Übereinstimmung und Akzeptabilität variieren.
Test-Funktion: Subjekt Übereinstimmung
P (akzeptabel | Prefix &gt;) &gt; P (akzeptabel | Prefix &lt;)
Space of Candidate Prefixes
Bias-Effects
What might Rose fees from before returning to this customer?
Who could Jessica sell believing these spotlights? What Aaron sounded like while returning to this customer before?
What did Jessica believe about these spotlights? What did Aaron sound like while returning to this customer before?
BLUMP: Adjunct</sample>
    <sample id="573">Ansatz
Test, ob die MPP-Bewertungen sich als Funktion von Kontextlänge, struktureller Übereinstimmung und Akzeptabilität unterscheiden
Test-Funktion: Subjekt Übereinstimmung
P ( Akzeptabel | Prefix -&gt; P ( Akzeptabel | Prefix
Space of Candidate Prefixes
Bias-Auswirkungen
Bias-Auswirkungen
P ( Akzeptabel | Prefix -&gt; P ( Akzeptabel | Prefix
P ( Akzeptabel | Prefix -&gt; P ( Akzeptabel | Prefix
GPT2, OPT-125M mit 6,7</sample>
    <sample id="574">Ansatz Test, ob die MPP-Bewertungen sich als eine Funktion von Kontextlänge, struktureller Übereinstimmung und Akzeptabilität variieren. Test-Skala: Subjektiv einvernehmliche Einwilligung Akzeptierbar &gt; Prefix &gt; Unakzeptierbar Space of Candidate Prefixes Who might Rose fees from before returning to this customer? What could Jessica have been recalling these spotlights? What had Aaron sounded like the white spotlights? What had returned to this customer before this customer before k7?</sample>
    <sample id="575">Ansatz
Test, ob die MPP-Bewertungen sich als eine Funktion von Kontextlänge, Strukturanpassung und Akzeptabilität variieren.
Test Beispiele: Subjekt Übereinhalt
P (Prefix → Prefix) &gt; P (Laut → Prefix)
P (Prefix → Prefix) &gt; P (Laut → Prefix)
Space of Candidate Prefixes
Matched
Unacceptable</sample>
    <sample id="576">Ansatz Test, ob die MPP-Bewertungen sich als Funktion von Kontextlänge, struktureller Übereinstimmung und Akzeptabilität variieren. Test-Funktion: Sujektiv Übereinstimmung P (A | Prefix A) &gt; P (A | Prefix B) Akzeptierbar, nicht akzeptierbar Space of Candidate Prefixes: BLMU: Externe "n" Quantifier | Adjust Island GPT2, OPT-125M mit 6,7</sample>
    <sample id="577">Ansatz
Test, ob die Urteilserkenntnis von MMP abhängig von der Kontextlänge, Strukturanpassung und Akzeptabilität variieren.
Test-Formel: Subjekt Übereinstimmung
P (A | Prefix) &gt; P (A | Prefix)
P (A | Prefix) &gt; P (A | Prefix)
Space of Candidate Prefixes
Bild 1: Beispiele für Urteile über die Akzeptabilität von Sätzen mit verschiedenen Prädiktoren.</sample>
    <sample id="578">Ansatz
Test, ob die Urteilserkenntnis variieren, je nach Länge des Kontexts, Strukturbereinigung und Annäherung an den Benutzer.
Prüfungen:
- Test-Funktion: Verweis auf Übereinstimmung (Subject-Verb Agreement)
- Prädiktionswahrscheinlichkeiten: P (Verweis &gt; Prädiktionswahrscheinlichkeit) &gt; P (Verweis &lt; Prädiktionswahrscheinlichkeit)
- Raum von Kandidaten-Präfixen: P (Kandidaten-Präfix &gt; Prädiktionswahrscheinlichkeit) &gt; P (Kandidaten-Präfix &lt; Prädiktionswahrscheinlichkeit)
- Beispiele:
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art." (Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit über dreissig Art.)
  - "Eine Rose ist ein wunderschöner pflanzlicher Kulturpflanzensorte mit</sample>
    <sample id="579">Ansatz
Test, ob die Urteilserkenntnis (MPP) sich als eine Funktion der Kontextlänge, struktureller Übereinstimmung und Akzeptabilität variieren lässt.
Test-Skala: Subjekt-Verb-Übereinstimmung
P (Subjekt | Prefix) &gt; P (Subjekt | Prefix)
P (Verb | Prefix) &gt; P (Verb | Prefix)
P (Nomen | Prefix) &gt; P (Nomen | Prefix)
Space of Candidate Prefixes
Random Prefixes
Random Effects
Random Seeds
Random Noun
Random Verb
Random Adverb
Random Adjective
Random Noun Phrase
Random Verb Phrase
Random Sentence
Random Sentence Fragment
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee</sample>
    <sample id="580">Ansatz
Test, ob die Urteilserkenntnis variieren, je nach Länge des Kontexts, Strukturbereinigung und Akzeptierbarkeit

Test-Beispiel: Subjekt Übereinstimmung
P (A | Prefix &gt;) &gt; P (A | Prefix &lt;)

Raum von Kandidaten-Prefixe
Einfluss der Strukturbereinigung
Einfluss der Akzeptierbarkeit

Beispiele:
- "Eine Rose ist ein wunderschöner blauer Blumenstrauch mit... "
- "Eine Rose ist ein wunderschöner roter Blumenstrauch mit... "
- "Eine Rose ist ein wunderschöner gelbbrauner Blumenstrauch mit... "</sample>
    <sample id="581">MPP-jurys sind robust für willkürliche Kontextlängen

Wir führen MPP-Evaluationen mit verschiedenen Kontexten durch – akzeptabel/unakzeptabel; passend/unpassend strukturiert – von Längen bis zu 900 Token.

Eine Rose ist ein weichblühender Pflanzensorte, die in den USA über 300 Arten von Rosen existiert. Wer mag Rose free from before returning to?</sample>
    <sample id="582">MPP-Bewertungen sind robust gegenüber willkürlichen Kontextlängen

Wir führen MPP-Evaluationen mit verschiedenen Kontexten durch – akzeptabel/unakzeptabel; passend/unpassend; Struktur – von Längen bis zu 900 Token.

Eine Rose ist ein perennial Blumenpflanz, der im Winter stirbt und im nächsten Jahr neu wächst. Sie ist eine Art von Pflanze, die im Sommer blüht und im Winter schläft. Ein Rose ist eine Art von Pflanze, die im Sommer blüht und im Winter schläft.</sample>
    <sample id="583">Wir durchfuhren MPPE-Bewertungen mit verschiedenen Kontexten - akzeptabel/unakzeptabel; passend/unpassend strukturiert - von Längen bis zu 900 Token.</sample>
    <sample id="584">Wir durchfuhren MPPE-Bewertungen mit verschiedenen Kontexten – akzeptabel/unakzeptabel; passend/unpassend strukturiert – von Längen bis zu 900 Token.

Die Grafik zeigt die Leitstelle (MPPE) in Abhängigkeit von der Längte des Kontextes. Es werden verschiedene Strategien wie "Acc. (Matched)" und "Acc. (Unmatched)" visualisiert, um die Effektivität der Kontexte zu messen.</sample>
    <sample id="585">Wir durchfuhren MPPE-Bewertungen mit verschiedenen Kontexten – akzeptabel/unakzeptabel; passend/unpassend strukturiert – von Längen bis zu 900 Token.</sample>
    <sample id="586">akzeptierbare/unakzeptierbare MMP-Sätze mit passendem Struktur können am stärksten beeinflussen werden. Wir führen MMP-Evaluationen mit verschiedenen Kontexten durch – akzeptierbar/unakzeptierbar; passend/unpassend Struktur – von Längen bis zu 900 Token.</sample>
    <sample id="587">akzeptierbare/unakzeptierbare MMP-Sätze mit abgestimmtem Strukturbedarf werden am meisten beeinflusst. Wir führen MMP-Evaluationen mit verschiedenen Kontexten durch – akzeptierbar/unakzeptierbar; abgestimmter/unabgestimmter Struktur – von Längen bis zu 900 Token.</sample>
    <sample id="588">akzeptierbare/unakzeptierbare MMP-Sätze mit passendem Strukturbedarf – die meisten gravierend beeinflussen</sample>
    <sample id="589">Warum beeinflussen vorangestellte Präfixe die Urteilsbildung von LM-Modellen? Wir perturbieren Kontextsätzen auf eine Weise, die die relevanten Strukturen beibehält und danach fragen, ob die Modelle ähnlich empfindlich auf diese Sätze sind. - Vornehme/suffix-Adverbien: "Allerdings, &lt;sent&gt;." - Langes vornes Adverb: "&lt;sent&gt;." - Erst und foremost: "&lt;sent&gt;." - Ohne X denkt über es nach: "&lt;sent&gt;." - Zitate: "Gestern hat X gesagt, '&lt;sent&gt;'."</sample>
    <sample id="590">Warum beeinflussen vorangestellte Präfixe die Urteilsschlüsse von LM? Wir perturbieren Kontextsätzen auf eine Weise, die das relevante Strukturschema beibehält, und fragen, ob Modelle ähnlich sensible Urteile zu diesen Sätzen abwenden.</sample>
    <sample id="591">Warum beeinflussen vorangestellte Präfixe die Urteilsbildung von LM? Wir perturbieren Kontextsätzen auf eine Weise, die den relevanten Strukturinhalt bewahrt, und fragen, ob Modelle ähnlich sensible Reaktionen auf diese zu geben haben. - Vorkommen adverbiell: "Allerdings &lt;sent&gt;" - Langes vorkommen adverbial: "&lt;sent&gt; allgemein" - Erst und am meisten, "&lt;sent&gt;" - "Ungeachtet dessen, dass &lt;sent&gt;" - "Gestern, als X gesagt hat, &lt;sent&gt;" - Zitate: "&lt;sent&gt;“</sample>
    <sample id="592">Warum beeinflussen abgepaschte Präfixe die Urteile von LM? Wir perturbieren Kontextsätzen auf eine Weise, die den relevanten Strukturinhalt bewahrt, und fragen, ob Modelle dieselben Sätze ähnlich empfindlich sind. - Prefix/Suffix-Adverbien: "Jedoch, &lt;sente&gt;" - Long Prefix-Adverbien: "&lt;sente&gt;" - Adverbien: "Erst und dann" - "Nachdem X darüber nachgedacht hat, &lt;sente&gt;" - Zitate: "Gestern sagte er, &lt;sente&gt;" - Weise: "Die Empfindlichkeit der Modelle gegenüber perturbierten Sätzen in ähnlicher Weise"</sample>
    <sample id="593">Warum beeinflussen vorangestellte Präfixe die Urteilsschlüsse von LM? Wir perturbieren Kontextsätzen auf eine Weise, die den relevanten Strukturinhalt beibehält, und fragen, ob Modelle ähnliche Empfindungen gegenüber diesen Sätzen haben. - Vorkommen: "Jedoch &lt;sente&gt;", "Erstens &lt;sente&gt;", "Eine Klausel &lt;sente&gt;", "Was X über es denkt &lt;sente&gt;", "Gestern &lt;sente&gt;" - Zitate: "Gestern &lt;sente&gt; erwiderte" - Art der Sensibilität an perturbierten Sätzen in ähnlicher Weise.</sample>
    <sample id="594">Kernpunkte

• Sprachmodelle sind empfänglich für latente syntagmatische/Semantik-Features, die über mehrere Sätze verteilt sind.
• Evaluierungen von Sätzen mit kurzen, einfachen Aussagen können LMs nicht vollständig abdecken.
• Abstrakte Kenntnisse werden in den Sätzen nicht vollständig abgedeckt.

Bildunterschrift: Testergebnisse: Übereinstimmung unter Subjekt-Vorhersage
Die Grafik zeigt die Testergebnisse unter der Überschrift "Testergebnisse: Übereinstimmung unter Subjekt-Vorhersage". Es zeigt die Wahrscheinlichkeitsverteilungen von LMs (Language Models) für verschiedene Sätte. Die x-Achse zeigt die "Space of Candidate Prefixes" (Raum der kandidaten Vorkommen), und die y-Achse zeigt die Wahrscheinlichkeitswerte. Es gibt mehrere Linien in verschiedenen Farben, jede repräsentiert eine verschiedene LM-Evaluation.</sample>
    <sample id="595">Kernpunkte

• Sprachmodelle sind empfindlich gegenüber latenten syntagmatischen/semantischen Merkmalen, die über mehrere Sätze verteilt sind.
• Evaluierungen von Sätzen mit kurzen, einfachen Aussagen können LMs nicht vollständig erfassen.
• MPPI-Evaluationen mit vollständigen Abstrakten Wissen (LMs) sind akzeptabel.

Die Grafik zeigt die Testbutler-Untersuchungsvorsorge-Abhängigkeit (Probanden) in Abhängigkeit von den verschiedenen Prädiktionsmodellen (Praediktionsmodell 1 bis 5). Es zeigt auch die Wahrscheinlichkeit (Praediktionsmodell &gt;? Praediktionsmodell) und die Wahrscheinlichkeitsdichtefunktion (PDF) der verschiedenen Prädiktionsmodelle.</sample>
    <sample id="596">Kernpunkte

Sprachmodelle sind empfindlich gegenüber latenten syntagmatischen/semantischen Merkmalen, die über mehrere Sätze verteilt sind.
Satzbewertungen mit kurzen, einfachen Eingaben können LMs nicht vollständig abdecken.
MPP-Evaluationen mit vollständigen abstrakten Wissen werden bewertet.

Die Grafik zeigt die Testergebnisse zu "Subjekt-Verb-Objekt" Übereinstimmung. Es zeigt auch die Wahrscheinlichkeitsverhältnisse von Kandidatenpräfixen (P(Präfix | &lt;w&gt;)) und die Wahrscheinlichkeitsverhältnisse von Kandidatenpräfixen (P(Präfix | &lt;w&gt;)).

Das untere Diagramm zeigt den Raum der Kandidatenpräfixe (Space of Candidate Prefixes). Es zeigt auch die Wahrscheinlichkeitsverhältnisse von Kandidatenpräfixen (P(Präfix | &lt;w&gt;)) und die Wahrscheinlichkeitsverhältnisse von Kandidatenpräfixen (P(Präfix | &lt;w&gt;)).</sample>
    <sample id="597">Nomen, Pronomen und Verben werden als NOUN, PRON und VERB zugeordnet.</sample>
    <sample id="598">In Coscript sind 55.000 Skripte vertreten.</sample>
    <sample id="626">TF-IDF</sample>
    <sample id="627">Weak supervision alleviates the annotation bottleneck.</sample>
    <sample id="628">Die Zuteilung wurde durch eine Online-Formularien-Studie erreicht.</sample>
    <sample id="629">Der CoNLL++-Datensatz wurde von Georgia Tech erstellt.</sample>
    <sample id="630">XSemPLR: Mehrsprachige Semantik-Semantic Parsing in mehreren natürlichen Sprachen und Bedeutungsvorstellungen

Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

PennState Amazon</sample>
    <sample id="631">XSemPLR: Semantische Analyse mehrerer natürlicher Sprachen und Bedeutungsvorgestelle

Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang

PennState, Amazon</sample>
    <sample id="632">Cross-lingual Semantic Parsing ist eine Aufgabe, die Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen übersetzt.</sample>
    <sample id="633">Cross-lingual Semantic Parsing ist eine Aufgabe, die Abfragen in mehreren natürlichen Sprachen in mehrere Bedeutungsdarstellungen übersetzt.</sample>
    <sample id="634">Cross-lingual Semantic Parsing

• Existierende CLSLP-Modelle werden separat vorgeschaffen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet. Beispielsweise:
    • Mangelnde Abdeckung auf bestimmte natürliche Sprachen

English | German | Chinese
--- | --- | ---
Neural Models | Neural Models | Neural Models
SQL | Lambda | FunQL</sample>
    <sample id="635">Cross-lingual Semantic Parsing

• Existierende CLSP-Modelle werden separat vorgeschaffen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet. Beispielsweise:
    - Mangelnde Abdeckung auf bestimmte natürliche Sprachen
• English
• German
• Chinese
• Neural Models
• SQL
• Lambda
• FunQL</sample>
    <sample id="636">Cross-lingual Semantic Parsing

• Existierende CLSP-Modelle werden separat vorgeschlagen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet. Beispielsweise:
    - Mangelnde Abdeckung auf bestimmte Bedeutungsrepräsentationen
English | German | Chinese | Neural Models | SQL | Lambda | FunQL</sample>
    <sample id="637">Cross-lingual Semantic Parsing

• Existierende CLSP-Modelle werden separat vorgeschaffen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet. Beispielsweise:
    - Mangelnde Abdeckung bei bestimmten Bedeutungsrepräsentationen
• English
• German
• Chinese
• Neural Models
• SQL
• Lambda
• FunQL</sample>
    <sample id="638">Cross-lingual Semantic Parsing

• Existierende CLSP-Modelle werden separat vorgeschaffen und auf Datensätzen von limitierten Aufgaben und Anwendungen bewertet. Beispielsweise:
    - Mangelnde Abdeckung auf bestimmte neuronale Modelle
  • English
  • German
  • Chinese
  • Single Model
  • SQL
  • Lambda
  • FunQL</sample>
    <sample id="639">XSemPLR

• Wir bieten eine vereinheitlichte Datensammlung XSemPLR für die übersichtliche semantische Analyse in mehreren Sprachen und Bedeutungen.
• 9 Datensätze in verschiedenen Domänen
• 5 semantische Aufgaben
• 22 natürliche Sprachen in 15 Sprachfamilien

Die Struktur von XSemPLR:

- Eingabe: Eine Anfrage in einer natürlichen Sprache (z.B. "What players played less than three times a season?")
- Prozess: Der Encoder interprettiert die Anfrage und übersetzt sie in eine interne Repräsentation.
- Ausgabe: Der Decoder interprettiert die interne Repräsentation und liefert die Antwort ("SELECT COUNT(*) FROM players WHERE games &lt; 3")</sample>
    <sample id="640">XSemPLR

Wir bieten eine vereinheitlichte Datensammlung XSemPLR für die transkulturelle semantische Analyse in mehreren Sprachen. Sie enthält:

• 9 Datensätze in verschiedenen Domänen
• 5 verschiedene Aufgabenstellungen
• 22 natürliche Sprachen in 15 Sprachfamilien

Die Struktur von XSemPLR sieht wie folgt aus:

1. **Datensätze**: 
   - English (USA): "What players played less than three times a season?"
   - German: "Wie viele Spieler haben in einem Pilsen-Restaurant mit Pilsner Bier geraucht?"
   - French: "Quels restaurants jouent-ils avec des Français à Paris?"
   - Arabic: "كم�数学家使用几何图形进行证明？"
   - Persian: "چه تعدادی از شروع‌ها و پایان‌ها را ترک کردند؟"

2. **Encoder und Decoder**:
   - Das Encoder-Modell wird verwendet, um die Eingabe (beispielsweise einen Satz) in eine interne repräsentation zu übertragen.
   - Das Decoder-Modell verwendet diese repräsentation, um die Ausgabe (beispielsweise die semantische Analyse des Satzes) zu generieren.

3. **Beispiele**:
   - English: "SELECT * FROM teams WHERE league = 'SLL' AND country = 'USA'"
   - German: "SELECT * FROM teams WHERE league = 'SLL' AND country = 'USA'"
   - French: "SELECT * FROM teams WHERE league = 'SLL' AND country = 'USA'"
   - Arabic: "SELECT * FROM teams WHERE league = 'SLL' AND country = 'USA'"
   - Persian: "SELECT * FROM teams WHERE league = 'SLL' AND country = 'USA'"

4. **Sprachfamilien**:
   - English
   - German
   - French
   - Arabic
   - Persian
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian
   - Chinese
   - Japanese
   - Korean
   - Spanish
   - Portuguese
   - Italian
   - Dutch
   - Swedish
   - Danish
   - Norwegian
   - Finnish
   - Hungarian
   - Czech
   - Slovak
   - Polish
   - Romanian
   - Greek
   - Turkish
   - Russian</sample>
    <sample id="641">Training: English -&gt; English Model -&gt; SQL
Inference: German -&gt; Translate API -&gt; English Model -&gt; SQL</sample>
    <sample id="642">Training: English -&gt; English Model -&gt; SQL
Inference: German -&gt; Translate API -&gt; English -&gt; English Model -&gt; SQL</sample>
    <sample id="643">Training: English -&gt; English Model -&gt; SQL
Inference: German -&gt; Translate API -&gt; English -&gt; English Model -&gt; SQL</sample>
    <sample id="644">Experiment Settings

• Wir betrachten sechs Ansätze für die Auswertung.
• Das Quellen Sprache ist dieselbe wie das Ziel Sprache. 
• Wir testen auch Monolingual Few-shot.

Training

Monolingual Model (Few-shot)

German Model

SQL

Inference

German</sample>
    <sample id="645">Experiment Settings

• Wir betrachten sechs Ansätze für die Auswertung.
• Das Quellenlanguage ist das gleiche wie das Zielsprache, z.B. German-to-German.
• Wir testen auch Monolingual Few-shot.

Training

Monolingual Model (Few-shot)
German Model
SQL

Inference

German
German Model
SQL</sample>
    <sample id="646">Experiment Settings

• Wir betrachten die sechs Ansätze für Training und Evaluation.
• Das Quellen Sprache ist dieselbe wie das Ziel Sprache, z.B. German-to-German.
• Wir testen auch Monolingual Few-shot.

Training

Monolingual Model (Few-shot)
German Model
SQL

Inference

German
German Model
SQL</sample>
    <sample id="647">ExperimenteinstellungenWir betrachten sechs Einstellungen für die Ausbildung und die Evaluation. Multilingual-Modell: Trainieren Sie ein multilinguales Modell für alle Sprachen.</sample>
    <sample id="648">ExperimenteinstellungenWir betrachten die sechs Einstellungen für Training und Evaluation. Multilingual Modell: Trainieren Sie ein multilinguales Modell für alle Sprachen.</sample>
    <sample id="649">Experiment Settings
Wir betrachten sechs Einstellungen für die Ausbildung und die Evaluation.
Multilingual Model: Train one multilingual model for all languages.
Training
Inference</sample>
    <sample id="650">Experimenteinstellungen:
Wir betrachten sechs Ansätze für die Ausbildung und die Evaluation.
Kreisling Zero-Shot/Few-Shot-Übertragung: Ausbilden auf einer Quellecke und Übertragen auf eine andere Sprache.</sample>
    <sample id="651">Experimenteinstellungen:
- Wir betrachten sechs Ansätze zur Ausbildung und Evaluation.
- Cross-Lingual Zero-Shot/Few-Shot Transfer: Trainieren auf einer Quelle言語 und übertragen auf eine andere Sprache.

Schulung:
- English
- German Few

Modell:
- Multilingual Model
- SQL

Inferenz:
- English
- German

Modell:
- Multilingual Model
- SQL</sample>
    <sample id="652">Wir evaluieren auf zwei Gruppen von Modellen: Monolingual Setting und Multilingual Setting. Im Monolingual Setting verwenden wir mehrere Modelle mit mehreren Sprachen, wie z.B. XLNet-R, mBERT + PTR, Enc-Dec (mT5) und mBART, um die besten Leistungen auf verschiedenen Datensätzen zu ermitteln. Wir haben festgestellt, dass Enc-Dec (mT5) die beste Performance auf allen Datensätzen erreicht. Hier ist ein Tabellenausschnitt, der die Ergebnisse zeigt:

| Datensatz | Matis | MGCQuery | MMaps | MNights | MCWQ | MSchQ | MTOP | MCNala | Durchschnitt |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Monolingual Setting | 72.18 | 40.40 | 83.28 | 57.47 | 23.46 | 52.53 | 75.41 | 5.87 | 49.09 |
| Multilingual Setting | 74.26 | 50.73 | 91.53 | 66.29 | 30.15 | 65.00 | 81.83 | 10.29 | 58.16 |

Wir haben festgestellt, dass Enc-Dec (mT5) die beste Performance auf allen Datensätzen erreicht hat.</sample>
    <sample id="653">Wir evaluieren auf zwei Gruppen von Modellen mit monolinguen Eingabedatasets: Enc-PTR (Multilingual Prewired Encoders with Pointer-based Decoders) und XLM-R + mBERT + PTR (Encoders, Multilingual Pretrained Encoder-Decoder Models). Wir haben festgestellt, dass Enc-Dec (mT5) die beste Leistung auf allen Datensätzen erreicht.</sample>
    <sample id="654">Wir evaluieren auf zwei Gruppen von Modellen: Monolingual Setting und Multilingual Setting. Im Monolingual Setting verwenden wir die Modelle Enc-PTR, XLM-R + PTB, Enc-Dec und mBART. Im Multilingual Setting verwenden wir mBERT + PTB und mT5. Wir haben festgestellt, dass Enc-Dec (mT5) die beste Leistung auf allen Datensätzen erreicht. Hier sind die Ergebnisse in Tabellarform:

| Modell | MATIS | MGoQuery | MSMaps | MOvernight | MCWQ | MSchema2QA | MTOP | MCALa | Durchschnitt |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| mT5 | 53.15 | 74.26 | 50.73 | 91.85 | 66.29 | 30.15 | 65.15 | 81.83 | 58.16 |

Im Monolingual Setting erreichen die Modelle folgende Punkte:
- mT5: 53.15
- mBERT: 72.18
- Enc-Dec: 53.15
- mBART: 31.31

Im Multilingual Setting erreichen die Modelle folgende Punkte:
- mT5: 74.26
- mBERT: 47.30
- Enc-Dec: 91.85
- mBART: 50.73

Wir haben festgestellt, dass Enc-Dec (mT5) die beste Leistung auf allen Datensätzen erreicht.</sample>
    <sample id="655">Wir evaluieren auf zwei Gruppen von Modellen: Monolingual Setting und Multilingual Setting. Im Monolingual Setting evaluiern wir die Leistung von Modellen mit mehreren Sprachen und Pointer-based Decoders, im Multilingual Setting evaluiern wir die Leistung von Modellen mit mehreren Sprachen und mBERT + PTR. Wir haben festgestellt, dass Enc-Dec (mT5) die beste Leistung auf allen Datensätzen erreicht. Hier sind die genauen Werte:

| Datensatz | MATE5 | MIGQuery | MSpacy | MNmaps | MOvernight | MCWQ | MSchema2QA | MTOP | MCALa | Durchschnitt |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Monolingual Setting | 70.63 | 72.18 | 40.40 | 83.82 | 57.47 | 23.46 | 55.23 | 75.41 | 5.87 | 49.09 |
| Multilingual Setting | 31.31 | 47.70 | 47.30 | 85.17 | 90.10 | 23.53 | 80.36 | 76.79 | 52.03 | 58.16 |
| mBERT + PTR | 53.15 | 74.26 | 50.73 | 91.65 | 66.29 | 30.15 | 65.18 | 81.83 | 10.29 | 58.16 |

Wir können sehen, dass Enc-Dec (mT5) in allen Datensätzen die besten Leistungen erreicht hat.</sample>
    <sample id="656">Wir evaluieren mT5 und XLM-R + PTR auf einem mehrsprachigen Setting. Eine Enc-Dec-Enc-PTR (mT5/XLM-R) kann durch Training in einer Mischung verschiedener Sprachen verbessert werden. Im Tabellenkopf steht der Name der Sprache, die in der Mischung dominierend ist.</sample>
    <sample id="657">Wir evaluieren auf mT5 und XLM-R + PTR in einem mehrsprachigen Setting. Enc-Dec-Enc-PTR (mT5-XLM-R) kann durch Training in einer Mischung verschiedener Sprachen verbessert werden.</sample>
    <sample id="658">Analyse der Mehrsprachtraining</sample>
    <sample id="659">Analyse der Multiplen Training

Wir evaluieren mT5 und XLM-R + PTR auf einem mehrsprachigen Setting. Die meisten major NLs können ein Leistungsgewinn erhalten, außer bei Englisch, bei dem das Performance in 7 Datensätzen abnimmt. Dies wird als "Curse of Multilinguality" bekannt.

Die Grafik zeigt die Anzahl der Datensätze pro natürliche Sprache. English hat am meisten Datensätze (10), gefolgt von Deutsch (8), Französisch (7) und anderen Sprachen mit kleineren Mengen.</sample>
    <sample id="660">Cross-lingual Performance Gap

* Blauer Balken: Cross-lingual Few-shot transfer
* Orange Balken: Cross-lingual Zero-shot transfer
* Grüner Balken: Monolingual Setting

Geography MTOP SchemaQA OverNight NLMaps MCWQ Gequery/funql Gequery/geol Gequery/prog Gequery/sql Gequery/lambda Gequery/funql Gequery/geol Gequery/prog Gequery/sql ATIS Spider</sample>
    <sample id="661">Cross-lingual Performance Gap
- Blauer Balken: Cross-lingual Few-shot transfer
- Orange Balken: Cross-lingual Zero-shot transfer
- Grüner Balken: Monolingual Setting

Geography, MTOP, SchemaQA, Overnight, NLMaps, MCWQ, Gequery/funql, Gequery/sql, Gequery/prolog, Gequery/lamb, Gequery</sample>
    <sample id="662">Cross-lingual Performance Gap

* Blauer Balken: Cross-lingual Few-shot transfer
* Orange Balken: Cross-lingual Zero-shot transfer
* Grüner Balken: Monolingual Setting

Geography MTOP SchemaQA OverNight NLMaps MCWQ Gequery/lambda Gequery/prolog Gequery/funql Gequery/sql Spider ATIS</sample>
    <sample id="663">Andere Ergebnisse und Erkenntnisse (Abschnitt 4 im Papier)

- Enc-Dec (m75) über Performs vorherige Arbeiten oder erreicht vergleichbare Ergebnisse.
- Das Vordrücken auf der englischen NL kann signifikant die Leistung an der Zielsprache verbessern.
- Multilinguale LLMs (Codex &amp; BLOOM) sind noch unzureichend für die Übersetzung zwischen Sprachen.
- Die Transferlearning-Training in Chinesisch und die einstellige englische Ausbildung (En -&gt; En) haben die größte Leistungsunterschiede, während das Deutsche normalerweise die kleinste Unterscheidung hat.
- FunQL über Performs die anderen drei Bedeutungsdarstellungen und SQL erzielt das schlechteste Resultat.</sample>
    <sample id="664">Andere Ergebnisse und Erkenntnisse (Abschnitt 4 im Papier)

• Enc-Dec (m75) über Performs vorherige Arbeiten oder erreicht vergleichbare Ergebnisse.
• Das Vordrucken auf der englischen NL kann die Leistung von Few-Shot erheblich verbessern.
• Multilinguale LLMs (Codex &amp; BLOOM) sind noch unzureichend für die Übersetzung von Sprachen.
• Die Transferlearning-Training in Chinesisch und das einstellige Training in Englisch (En -&gt; En) haben die größte Leistungsunterschiede, während das Training in Deutsch normalerweise die kleinsten Unterschiede zeigt.
• FunQL über Performs die anderen drei Bedeutungsdarstellungen und SQL führt zu dem schlechtesten Resultat.</sample>
    <sample id="665">- Wir bauen XSemPLR, eine vereinheitlichte Benchmark-Studie für die multilinguale semantische Analyse mit mehreren natürlichen Sprachen und Bedeutungsvorstellungen.
- Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen multilingualer Sprachmodelle durch.
- Unsere Ergebnisse zeigen, dass mT5 mit monolingualer Trainingseinrichtung die beste Leistung liefert, während multilingual LLMs noch unzureichend sind, um multilinguale syntagmatische Analyse Aufgaben zu bewältigen. Darüber hinaus zeigt der Leistungsunterschied zwischen monolingualer Schulung und multilingueller Transferlernung als noch bedarfsvoll zu sein.</sample>
    <sample id="666">- Wir bauen XSemPLR, eine vereinheitlichte Benchmark-Studie für die multilinguale Semantik-Semantic-Parsing mit mehreren natürlichen Sprachen und Bedeutungsvorstellungen.
- Wir führen eine umfassende Benchmark-Studie an drei repräsentativen Typen multilingueller Sprachmodelle durch.
- Unsere Ergebnisse zeigen, dass mT5 mit monolingualer Trainingssignatur die beste Performance liefert, während notwendig multilinguelle LLMs noch unzureichend sind, um multilinguale syntagmatische Analyse Aufgaben zu bewältigen. Darüber hinaus zeigt der Leistungsunterschied zwischen monolingualer Schulung und multilingueler Transferlernung als noch bedarfsvoll zu sein.</sample>
    <sample id="667">Parameter-based watermark, Lexical watermark, Backdoor-based watermark, Adversarial-based watermark</sample>
    <sample id="668">Nein, sie sind noch nicht ausreichend für CLSP.</sample>
    <sample id="695">Die Methode verwendet eine 'Tag' Schicht, um Mehrdeutigkeiten in der Permutation zu lösen.</sample>
    <sample id="696">Fairness is defined as the absence of bias.</sample>
    <sample id="697">DrBERT</sample>
    <sample id="698">Koustuv Sinha</sample>
    <sample id="699">Myra Cheng</sample>
    <sample id="700">Die Art und Weise, wie die Gruppe nur durch ihre Identität definiert wird.</sample>
    <sample id="701">Die Autoren haben die von Menschen verfassten Beschreibungen der Zielgruppen erstellt, indem sie die Top-Wortlisten von Twitter und Facebook analysieren.</sample>
    <sample id="702">P-CXMI</sample>
    <sample id="703">Der Unterschied zwischen DrBERT und ChuBERT ist, dass DrBERT vonscratch trainiert wird, während ChuBERT auf einem bestehenden vortrainierten Modell basiert.</sample>
    <sample id="751">Drei.</sample>
    <sample id="752">Iteratives Transferlernen ist ein Prozess, bei dem ein Modell iterativ trainiert wird, indem es auf neue Daten zugreift und dann die neuen Datensätze mit den alten Datensätzen kombiniert.</sample>
    <sample id="753">Das Ziel des Datensatzes ist die Resolving Indirect Reference Expressions by Entity Selection.</sample>
    <sample id="754">Ein Angreifer kann Modellparameter über einen EaaS extrahieren, indem er die Abhängigkeit zwischen den Eingangsdaten und dem Ausgabedaten einer Modellinstanz exploitiert.</sample>
    <sample id="755">Drei.</sample>
    <sample id="756">6 Annotatoren wurden verwendet, um den ursprünglichen Datensatz zu erstellen.</sample>
    <sample id="757">University of Washington, Carnegie Mellon University, Allen Institute for AI</sample>
    <sample id="758">Bart und Lisa Homer kamen und schwebten.</sample>
    <sample id="759">Der Stand der Technik für Dialogsysteme beinhaltet die Evaluierung von Verhaltensmustern.</sample>
    <sample id="760">Um zu bestimmen, ob die Urteile über die Stabilität der Modelle mit einem langen Kontextfenster einstimmig sind.</sample>
    <sample id="761">Ja, das mehrsprachige Training hat in 7 Datensätzen zu einem Leistungsabfall im Vergleich zum einsprachigen englischen Modell geführt.</sample>
    <sample id="762">Nein, die Annotatoren kennen die Entität nicht im Voraus.</sample>
    <sample id="763">BLEU, METEOR, TER, WER</sample>
    <sample id="764">Ja, die Regression beeinträchtigt die Generalisierung auf bestimmte NER-Typen.</sample>
    <sample id="765">Positionalität ist für NLP wichtig, um die Reihenfolge von Wörtern in einem Satz zu berücksichtigen.</sample>
    <sample id="766">LLMs wurden durch Adapter angepasst.</sample>
    <sample id="767">Roberta-base classifier head</sample>
    <sample id="768">Die aktuellen Testsets, die zur Bewertung der PaLM-Fähigkeiten verwendet wurden, sind die Europäische Sprachbank (Europäische Sprachbank) und die Europäische Sprachbank (Europäische Sprachbank).</sample>
    <sample id="769">Die Autoren haben schließlich zwei Empfehlungen vorgeschlagen.</sample>
    <sample id="770">Der Gewinn der vorgeschlagenen Methode gegenüber der stärksten Baseline beträgt 10.7%.</sample>
    <sample id="771">Shuheng Liu</sample>
    <sample id="772">Ja, die Studie verwendetBenchmarkergebnisse und Datensatz.</sample>
    <sample id="773">In der Arbeit werden 3 kleinere Modelle experimentiert.</sample>
    <sample id="774">Das OFA-Modell wird als Basismodell für die Untersuchung der multimodalen Unterrichtsabstimmung verwendet.</sample>
    <sample id="833">Die Autoren gehören an der Stanford University.</sample>
    <sample id="834">Die Autoren gehören der Stony Brook University an.</sample>
    <sample id="835">The image does not specify which language pairs were studied in the research.</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">SARI, BLEU, BS-P und FRE</sample>
    <sample id="838">53 Aufgaben werden für die Schulung verwendet und 9 Aufgaben werden für Tests verwendet.</sample>
    <sample id="839">Drei.</sample>
    <sample id="840">Die Autoren haben experimentiert an den Datensätzen AG News, MIND, SST2, Enron Spam und WikiText.</sample>
    <sample id="876">NACHOS is a pre-training strategy for language models.</sample>
    <sample id="877">The speaker's name is David Vil Torres.</sample>
    <sample id="878">Die Strategie der prompts hat einen großen Einfluss auf die Qualität der Übersetzung.</sample>
    <sample id="879">Die Autoren gehören an der Carnegie Mellon University.</sample>
    <sample id="880">Die 5 Anweisungen der Expert*innen lauten: 1. Lesen Sie die Anweisungen sorgfältig, 2. Lesen Sie die Anweisungen sorgfältig, 3. Lesen Sie die Anweisungen sorgfältig, 4. Lesen Sie die Anweisungen sorgfältig, 5. Lesen Sie die Anweisungen sorgfältig.</sample>
    <sample id="881">Die Autoren schlagen die Nutzung von humanen Studiopartizipanten und coreference resolution Modellen vor, um Modelle zur Nutzung von Informationen aus mehreren Quellen zu testen.</sample>
    <sample id="882">Prompting PalM für Übersetzung
Evaluierung von Strategien und Leistung

Google

David Vilá Torres, Markus Freitag, Colin Cherry, Jiaming Luo, Vineer Ratnaker, George Foster</sample>
    <sample id="883">PalLM: Wege Sprachmodell

- Chowdery et al., 2022
- arXiv:2204.02311.
- 540 Billion Parameter
- Trainiert auf 708 Billion Tokens.
- Dichter aktiviert.
- 6144 TPU v4 Chips
- SOTA in hundert Benchmark-Schwankungen.
- 10 Billion Parameter

[Hier ein Bild von einem Baum mit Farben]

Google</sample>
    <sample id="884">PalM: Pfade Sprachmodell
- Chowdery et al., 2022
- arXiv:2204.02311.
- 540 Billion Parameter
- Trainiert auf 780 Billion Tokens.
- Densiv aktiviert.
- 6144 TPU v4 Chips.
- SOTA in hundert Benchmarks.
- 62 Billion Parameter</sample>
    <sample id="885">Unsere Beiträge:

• Erstes systematisches Studium von LLM-Prompting für MT.
• Evaluieren der Übersetzungsfähigkeiten sowohl durch Selektionsstrategien als auch durch die besten übergreifenden Praktiken der MT-Community.
• Neueste Testsets (vermeiden von Test/Train-Overlap und Übertrennung auf recent training data).
• Vergleich mit den neuesten WMT-Untersmissionen (SOTA-Systeme, die die meisten recenten Trainingdaten verwenden).
• Expertenbasierte menschliche Evaluation (robuster als Crowdworkers).
• Empfehlungen für Selektionsstrategien für Prompts.

Google-Logo im unteren linken Ecke.</sample>
    <sample id="886">Unsere Beiträge:

• Erstes systematisches Studium von LLM-Prompting für MT.
• Evaluieren der Übersetzungskapazitäten sowohl anhand des Kandidatenpools als auch anhand von Selektionsstrategien.
• Evaluieren der Übersetzungskapazitäten mit den besten Praktiken der MT-Community:
- Neueste Testsets (vermeiden Test/Training-Overlap und Übertrennung)
- Vergleich von neuesten WMT-Unterbais (mithilfe von neuestem Trainingdaten)
- SOTA-MT-Metriken (bessere Korrelation mit menschlichen Urteilen)
- Expertenbasierte menschliche Evaluation (robuster als Crowdworkers)
• Empfehlungen für Strategien zur Selektion von Prompts</sample>
    <sample id="887">Unsere Beiträge:

• Erstes systematisches Studium von LLM-Prompting für MT.
• Evaluieren der Übersetzungsfähigkeiten sowohl im Kandidatenpool als auch bei der Selektionsstrategie.
• Evaluieren der Übersetzungsfähigkeiten mit den besten Praktiken der MT-Community:
  • Neueste Testsets (vermeiden Test/Training-Overlap und Overfitting auf recent training data).
  • Vergleich zu den neuesten WMT-Untersmissionen (SOTA-Systeme, die die recent training data verwenden).
  • Bessere MT-Metriken (besserer Korrelation mit menschlichen Urteilen).
  • Expertenbasierte menschliche Evaluation (robuster als Crowd Workers).
• Empfehlungen für Selektionsstrategien für Prompts.</sample>
    <sample id="888">Unsere Beiträge:

• Erstes systematisches Studium von LLM-Prompting für MT.
• Evaluieren der Übersetzungsfähigkeiten sowohl im Kandidatenpool als auch bei der Selektionsstrategie.
• Evaluieren der Übersetzungsfähigkeiten mit den besten übergreifenden Praktiken der MT-Community:
  • Neueste Testsets (vermeiden Test/Training-Overlap und Übertrennung auf Evaluation Daten).
  • Vergleich zu den neuesten WMT-Untersmissionen (SOTA-Systeme, die die meisten recenten Trainingdaten verwenden).
  • Bessere MT-Metriken (bessere Korrelation mit menschlichen Urteilen).
  • Expertenbasierte menschliche Evaluation (robuster als Crowd Workers).
• Empfehlungen für Selektionsstrategien für Prompts.</sample>
    <sample id="889">Prompte haben einen großen Einfluss auf die Übersetzungsqualität. Wählen Sie zwei zufällige Prompte für jede Phrase. Computieren BLEURT für jedes Phrase-Prompt-Paar. Die Mehrheit der Phrasen (516 von 1000) zeigt eine Differenz von mehr als 1 BLEURT-Punkt an. Die Differenz kann bis zu 40 BLEURT-Punkte betragen!</sample>
    <sample id="890">Prompte haben einen großen Einfluss auf die Übersetzungsgüte

- Wählen Sie zwei zufällige Prompte für jede Phrase.
- Berechnen Sie BLEURT für jedes Phrase-Prompt-Paar.
- Die Mehrheit der Phrasen (516 von 1000) zeigt eine Differenz von mehr als 1 BLEURT-Punkt an.
- Die Differenz kann bis zu 40 BLEURT-Punkte betragen!</sample>
    <sample id="891">Prompte haben einen großen Einfluss auf die Übersetzungsqualität

- Wählen Sie zwei zufällige Prompte für jede Phrase.
- Berechnen Sie BLEURT für jedes Paar von Phrase-Prompt.
- Die Mehrheit der Phrasen (516 von 1000) zeigt eine Differenz von mehr als 1 BLEURT-Punkt.
- Die Differenz kann bis zu 40 BLEURT-Punkte betragen!</sample>
    <sample id="892">Polizeiwarnden alarmiert die Polizei, mit mehreren Streifenwagen anrücken.</sample>
    <sample id="893">Polizeiwarnden alarmiert die Polizei, die mit mehreren Straßengerichten anrufen.</sample>
    <sample id="894">Polizeiwarnden alarmiert die Polizei, die mit mehreren Streifenwagen ankamen.</sample>
    <sample id="895">Polizeiwarnden alarmiert die Polizei, mit mehreren Polizisten anrufende.</sample>
    <sample id="896">Polizeiwarnden alarmiert die Polizei, die mit mehreren Streifenwagen ankamen.</sample>
    <sample id="897">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
• PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM:

• Fluide Art von PaLM vergleichbar mit SOTA.
• Genauigkeit allgemein niedriger.
• Dominiert durch "Genauigkeit/Omission".
• "Stil/Ahnung" allgemein niedriger für PaLM.</sample>
    <sample id="898">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
• PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM:

• Fluide Art von PaLM vergleichbar mit SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Genauigkeit/Omission".
    • "Stil/Aufwärts" allgemein niedriger für PaLM.</sample>
    <sample id="899">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen substantiellen Vorteil.
• PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM:

• Fluide Art von PaLM vergleichbar zu SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Genauigkeit/Omission".
    • "Stil/Aufwärts" allgemein niedriger für PaLM.</sample>
    <sample id="900">Experimenteller Erfolg

- Beispielseligkeit ist wichtiger als Ähnlichkeit an der Quelle.
- Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
- PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM:

- Fluide Art von PaLM vergleichbar mit SOTA.
- Genauigkeit allgemein niedriger.
    - Dominiert durch "Fehler/Fehlernachweis".
    - "Stil/Aufbau" allgemein niedriger für PaLM.</sample>
    <sample id="901">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
• PaLM nahe an Google Translate.

Einblicke aus MQM

• Fluide von PaLM vergleichbar zu SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Genauigkeit/Omission".
    • "Stil/Ausdrucksweise" allgemein niedriger für PaLM.</sample>
    <sample id="902">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
• PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM

• Fluide Art von PaLM vergleichbar zu SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Genauigkeit/Omission".
    • "Stil/Auswechslung" allgemein niedriger für PaLM.</sample>
    <sample id="903">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen substantiellen Vorteil.
• PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM

• Fluide Art von PaLM vergleichbar mit SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Genauigkeit/Omission".
    • "Stil/Auswechslung" allgemein niedriger für PaLM.</sample>
    <sample id="904">Experimentelle Ergebnisse

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
• PaLM nahezu so gut wie Google Translate.

Einblicke aus MQM

• Fluide Art von PaLM vergleichbar mit SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Fehler/Überschwemmung".
    • "Stil/Aufwärts" allgemein niedriger für PaLM.</sample>
    <sample id="905">Experimenteller Erfolg

• Beispielqualität ist wichtiger als Ähnlichkeit an der Quellsentence.
• Spezialisierte SOTA-Systeme haben einen erheblichen Vorteil.
• PALM nahezu so gut wie Google Translate.

Einblicke aus MQ:

• Fluide Art von PALM vergleichbar mit SOTA.
• Genauigkeit allgemein niedriger.
    • Dominiert durch "Genauigkeit/Omission".
• "Stil/Ahnung" allgemein niedriger für PALM.</sample>
    <sample id="906">Danke</sample>
    <sample id="907">Weaker Than You Think: Eine kritische Betrachtung von schwach überwachtem Lernen

Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, Dietrich Klakow

1 Saarland University, 2 Amazon Alexa, 3 University of Vienna

61. ACL 2023</sample>
    <sample id="908">Weaker Than You Think: Eine kritische Betrachtung von schwach überwachtem Lernen

Dawei Zhu, Xiaoyu Shen, Marius Mosbach, Andreas Stephan, Dietrich Klakow

1 Saarland University, 2 Amazon Alexa, 3 University of Vienna

61. ACL 2023</sample>
    <sample id="909">WarumWeakly überwachte Lernung?Weak überwachte Annotationen allgemein alleviiert die Annotierungs-Schottengrube. Aber rauschen Labeln!Rauschen Merkmalsautomatisierter Generalisierbarkeit.Weakly überwachte Lernung (WSL)Train Model, die gut generalisieren, obwohl sie bei rauschen Daten trainiert werden.</sample>
    <sample id="910">WarumWeakly überwachte Lernung?Weak überwachte Annotationen allgemein alleviiert die Annotierungsblockade. Aber noisylabels!Noise memorization schadet der Generalisierbarkeit.Weakly überwachte Lernung (WSL)Train models, die gut generalisieren, obwohl sie auf noisymehrtrainiert werden.</sample>
    <sample id="911">WarumWeakly überwachtes Lernen?
- Schwache Überwachung allgemein reduziert die Annotierungs-Schottage.
- Aber: Lässliche Etiketten sind ___. (rot)
- Lässliche Erinnerungen schaden der Generalisierbarkeit. (rote Schriftart)
- Weakly überwachte Lerner (WSL) (grüne Schriftart)
- Trainieren von Modellen, die gut generalisieren, obwohl sie auf lässlicheren Daten trainiert werden (laut rote Schriftart).</sample>
    <sample id="912">WarumWeakly überwachte Lernung?
Eine schwache Überwachung allgemein alleviiert die Annotierungsblockade.
Aber Rauschlabels sind! Rauschmemorization schadet der Generalisierbarkeit.
Weakly überwachte Lernung (WSL) Trainiere Modelle, die gut generalisieren, obwohl sie von Rauschdaten ausgebildet werden.</sample>
    <sample id="913">WarumWeakly überwachte Lernung?
- Schwache Überwachung behebt die Annotierungsblockade.
- Aber rauschvolle Labeln schaden der Generalisierung.
- Weakly überwachte Lernung (WSL)
Trainiere Modelle, die gut generalisieren, obwohl sie auf rauschvollen Daten trainiert werden.</sample>
    <sample id="914">Eine gemeinsame Behauptung in jüngster WSL Arbeiten "Wir trainieren Model nur auf schwach überwachten Daten und erreichen eine Genauigkeit von XX%."</sample>
    <sample id="915">Eine gemeinsame Behauptung in jüngsten WSL Arbeiten "Wir trainieren Model nur auf schwach überwachten Daten und erreichen eine Genauigkeit von XX%."</sample>
    <sample id="916">Eine gemeinsame Behauptung in jüngster WSL Arbeiten "Wir trainieren Model nur auf schwach überwachten Daten und erreichen eine Genauigkeit von XX%" (sauber)</sample>
    <sample id="917">Eine gemeinsame Behauptung in jüngster WSL Arbeiten

"Wir trainieren Model nur auf schwach überwachten Daten und erreichen eine Genauigkeit von XX%" mit einem unglückseligen Gesichtsausdruck.

"</sample>
    <sample id="918">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="919">Our research questions Is clean validation data necessary? How many clean samples do WSL approaches need? How to use the available clean samples more efficiently?</sample>
    <sample id="920">RQ1 Main findings

Validation on weak labels (orange) No validation (purple) Validation on clean labels (green)

30 20 10 0 -10 -20 FT_w, BOND, COSINE, MLC, L2R</sample>
    <sample id="921">RQ1 Main findings

Die Grafik zeigt die relative Leistungsverbesserung (in %) an, indem sie die Leistungen von Modellen mit schwachenLabels (orange) und mitCleanLabels (grün) vergleicht. Es werden verschiedene Verknüpfungsfunktionen (FT_w, BOND, COSINE, MLC, L2R) überprüft.

Die Grafik zeigt, dass die Modelle mitCleanLabels in der Regel bessere Leistungen aufwarten, insbesondere bei den Verknüpfungsfunktionen FT_w und COSINE. Hier sind die relative Leistungsverbesserungen durch die Verknüpfungsfunktion COSINE am stärksten, insbesondere bei denWeakLabels.</sample>
    <sample id="922">RQ1: Hauptergebnisse

Die Grafik zeigt die relative Leistungsverbesserung (in %) an, indem sie die Leistungen von Modellen mit schwachen Etiketten (Weak Labels) und Modellen mit einem zufälligen Selektionsprozess (Random Selection) vergleicht. Die Grafik zeigt, dass die Modelle mit schwachen Etiketten in der Regel eine bessere Leistungsverbesserung aufweisen als die Modelle mit einem zufälligen Selektionsprozess.

Die Grafik zeigt auch, dass die Leistungsverbesserungen variieren, je nachdem, welches Metrik-Verhältnis (MLC) verwendet wird. Das MLC 1:2 scheint allgemein die besten Leistungsverbesserungen zu erzielen, während das MLC 1:1 die schlechtesten Leistungsverbesserungen zu erzielen scheint.

Es ist wichtig zu beachten, dass die Grafik nur eine Annäherung der Leistungsverbesserungen ist und daher nicht als ein vollständiges Bild der Leistungsverbesserungen angesehen werden sollte.</sample>
    <sample id="923">RQ1 Main findings

Die Grafik zeigt die relative Leistungsverbesserungen (in %) an, indem sie die Leistungen von Modellen mit schwachen etiketten (weak labels) und Modellen mit sauberen etiketten (clean labels) vergleicht. Die verschiedenen Methoden, um schwache etiketten zu verbessern, werden dargestellt: FT_w (finetuning with weak labels), BOND, COSINE, MLC (Multi-Label Classification) und L2R (Linear to Ranking). 

Die Grafik zeigt, dass die Methode FT_w in der Regel die besten Leistungsverbesserungen erreicht, insbesondere bei den BOND und COSINE Methoden. MLC und L2R Methoden scheinen in diesem Fall weniger effektiv zu sein. Es ist wichtig zu beachten, dass die Schwankungen (dargestellt durch die FARBENEN PUNKTE) variieren können, was die genaue Ausprägung der Verbesserungen beeinträchtigen kann.</sample>
    <sample id="924">RQ1 Main findings

Validation on weak labels Validation on clean labels No validation (Random Selection)
FT_w. BOND COSINE MLC L2R</sample>
    <sample id="925">RQ2 Main findings

Die Grafik zeigt die Genauigkeit von verschiedenen Methoden (FT_w, COSINE, L2R, BONDO, MLC) im Vergleich zu schwachen Etiketten auf einem Validierungsset. Die Y-Achse zeigt die Genauigkeit in Prozent, und die X-Achse zeigt die Anzahl der validierten Beispiele. Jede Linie repräsentiert eine Methode, und die Farben variieren zwischen den Methoden. Die Linien sind standardmäßig transparent, und die Füllfarben der Linien variieren je nach Methode.</sample>
    <sample id="926">RQ2 Main findings

Die Grafik zeigt die Genauigkeit (Accuracy) von verschiedenen Methoden (FT_w, COSINE, L2R, BONDO, MLC) in Abhängigkeit von der Anzahl der Validierungen. Die Y-Achse zeigt die Genauigkeit von 75 bis 85%, und die X-Achse zeigt die Anzahl der Validierungen von 5 bis 50. Die Linien variieren in Farbe und Stil, um die verschiedenen Methoden zu kennzeichnen. Eine dünne graue Linie repräsentiert "Weak labels".</sample>
    <sample id="927">WSL-Verfahren profitieren von mehr sauberen Validierungsstichproben!</sample>
    <sample id="928">RQ2 Main findings

Die Grafik zeigt die Genauigkeit verschiedener Ansätze (FT_w, COSINE, L2R, MLC) in Abhängigkeit von der Anzahl der validierenden Datensamples. Die Linien in verschiedenen Farben repräsentieren die Leitlinien für die verschiedenen Ansätze. Die Grafik zeigt, dass die Genauigkeit allgemein steigt, je mehr validierende Datensamples verwendet werden.

Performance Deltas (%)

Die rechte Grafik zeigt die Performance Deltas (%) für verschiedene Ansätze (FT_c, LoRA_c, BiFiT_c, Adapter_c) in Abhängigkeit von der Anzahl der validierenden Datensamples. Die Balken in verschiedenen Farben repräsentieren die verschiedenen Ansätze. Die Grafik zeigt, dass die Performance Deltas (%) allgemein sinken, je mehr validierende Datensamples verwendet werden.

WSL Ansatz profitiert von mehr validierenden Datensamples!</sample>
    <sample id="929">RQ2 Main findings WSL approaches benefit from more clean validation samples! Performance Deltas (%) 5 FTw COSINE L2R MLC Weak labels All 5 10 20 30 40 50 All Validation</sample>
    <sample id="930">N=10 sauberer Pro Klassensamples N=30 Pro Klassensamples</sample>
    <sample id="931">RQ3: Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT COSINE ER FT+ER Clean Only</sample>
    <sample id="932">RQ3: Main findings

N = 10 clean samples per class

N = 30 samples per class

Accuracy/F1

88
86
84
82
80
78
76
74
Before CFT
After CFT

COSINE
ER
FT+ER
Clean Only</sample>
    <sample id="933">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class 88 90 AccuracyAccuracyF1 F1 86 84 84 82 82 80 80 78 78 76 76 74 74 Before After Before After Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. COSINE ER FT+ performs equally well. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="934">Klasse

Neuere WSL-Annäherungen
- Anforderung nach sauberen Stichproben.
- Übertreiben Sie ihre Praktizität.

Unsere Empfehlungen
- Berichten Sie die Modellauswahlkriterien.
- Verwenden Sie Few-Shot-Lernansätze als Baselines.
- Immer CFT (Kontinuierliche Optimierung) verwenden.</sample>
    <sample id="935">Klasse
Recent WSL approaches
• Require clean samples.
• Overestimate their practicality.
Our recommendations
• Report the model selection criteria.
• Use Few-shot learning approaches as baselines.
• Always apply continuous fine-tuning (CFT).</sample>
    <sample id="936">Schlussfolgerung

Neuere WSL- Ansätze
- Anreiquisiten: Reinzelte Stichproben.
- Überestimieren ihre Praktizität.

Unsere Empfehlungen
- Berichten über die Modellauswahlkriterien.
- Verwenden von Few-Shot-Lernansätzen als Baselines.
- Immer immer eine ständige fein-tuning (CFT) anwenden.</sample>
    <sample id="937">Klasse

Neuere WSL-Annäherungen
- Anforderung von sauberen Stichproben.
- Übertreiben Sie ihre Praktizität.

Unsere Empfehlungen
- Berichten Sie die Modellauswahlkriterien.
- Verwenden Sie Few-Shot-Lernansätze als Baselines.
- Immer CFT (Kontinuierliche feinkalibrierung) verwenden.</sample>
    <sample id="938">Klasse

Neuere WSL-Abläufe
- Anreiquen von sauberem Datensatz.
- Übertreiben Sie ihre Praktizität.

Unsere Empfehlungen
- Berichten Sie die Modellauswahlkriterien.
- Verwenden Sie Few-Shot-Lernansätze als Baselines.
- Immer anwenden Sie ständige fein-tuning (CFT).

Quelle: 9</sample>
    <sample id="939">Gemeinsame Bewertungsmethoden für Dialogsysteme sind die Comparative Evaluation und die Absolute Evaluation.</sample>
    <sample id="940">5</sample>
    <sample id="941">Das Hintergrundwissen, das im Beispiel mit Servin und Kea benötigt wird, ist, dass Servin ein Richter ist und Kea ein Bäcker.</sample>
    <sample id="942">Ja, der Code ist verfügbar und er ist auf GitHub unter mpoemsl/kitmus zu finden.</sample>
    <sample id="943">Ja, sie sind ausgewogen.</sample>
    <sample id="944">Sie wurden durch Adverbien, Hinzufügen von Foremost, Quote und X's Gedanken über &lt;sents&gt; durcheinander gebracht.</sample>
    <sample id="945">Eine dimensionale Bewertung ist eine Art von Bewertung, bei der ein Benutzer auf einer Skala von 1 bis 5 einen Wert zu einem bestimmten Aspekt einer Produkt oder Dienstleistung gibt.</sample>
    <sample id="946">University of Science and Technology of China, Microsoft Research Asia, Beijing Haotong University</sample>
    <sample id="947">Die Form des Prompts ist wichtig, wenn die Übersetzung nicht zu einem bestimmten Format passt.</sample>
    <sample id="978">Turnturm, Dialog Likert und Dialog Likert.</sample>
    <sample id="979">7</sample>
    <sample id="980">Eine guter Planer sollte allgemein anpassbar, robust und effizient sein.</sample>
    <sample id="981">There are eight authors involved in the work.</sample>
    <sample id="982">Vasudha Varadarajan</sample>
    <sample id="983">The authors belong to the University of Warsaw.</sample>
    <sample id="1021">Die häufigsten Fehler von PaLM sind "Accuracy/Omission" und "Style/Awesome" allgemein.</sample>
    <sample id="1022">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi Emory University NLP Emory NLP Lab Alexa Research Lab</sample>
    <sample id="1023">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi Emory University Emory NLP Emory NLP Lab Alexa Research Lab</sample>
    <sample id="1024">Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems Sarah E. Finch, James D. Finch, and Jinho D. Choi Emory University Emory NLP Alexa Research Lab</sample>
    <sample id="1025">Comparative Evaluation</sample>
    <sample id="1026">Likert Rating Evaluation</sample>
    <sample id="1027">Dimensionen der Dialogqualität

Die Dialogqualität ist ein wichtiger Aspekt, um sicherzustellen, dass eine Interaktion zwischen zwei Parteien effektiv und effizient ist. Es gibt verschiedene Dimensionen, die zur Evaluierung der Dialogqualität beitragen. Eine dieser Dimensionen ist die Relevanz, die sich auf die Bedeutung und den Nutzen der gesprochenen Informationen bezieht. Eine relevante Unterhaltung ist jene, die für die beteiligten Personen von Interesse ist und sie dazu bringt, sich zu engagieren.

Eine weitere wichtige Dimension der Dialogqualität ist die Konsistenz. Eine kursive Unterhaltung ist eine, bei der die Beteiligten ihre Ansichten und Meinungen in einer kohärenten Weise präsentieren. Eine kursive Unterhaltung ist eine, bei der die Beteiligten ihre Ansichten und Meinungen in einer kohärenten Weise präsentieren.

Eine weitere wichtige Dimension der Dialogqualität ist die emotionalen Verstehensfähigkeit. Eine empathetische Unterhaltung ist eine, bei der die Beteiligten ihre Gefühle und Bedürfnisse respektieren und verstehen. Eine empathetische Unterhaltung ist eine, bei der die Beteiligten ihre Gefühle und Bedürfnisse respektieren und verstehen.

Insgesamt sind die Relevanz, die Konsistenz und die emotionalen Verstehensfähigkeit wichtige Dimensionen der Dialogqualität. Sie helfen dabei, sicherzustellen, dass eine Unterhaltung effektiv und effizient ist und dass die Beteiligten sich engagiert und respektvoll gegenüber einander behalten.</sample>
    <sample id="1028">Likert Rating Evaluation

Eine Person mit einem Urteilsschiff (Symbol für Recht) und ein Roboter. Der Roboter hat einen Textfelder, um eine Antwort zu geben.

Bild 1: Beispiele von Relevanzbewertungen

Bitte beachten Sie, dass die Relevanzbewertungen standardmäßig auf 5 Punkte skaliert sind.</sample>
    <sample id="1029">Likert Rating Evaluation Chat (ABC-Eval)</sample>
    <sample id="1030">Annotating Behaviors in Chat (ABC-Eval)</sample>
    <sample id="1031">ABC-Eval Verhaltensweisen

Die Grafik zeigt eine Tabelle mit vier Feldern, die die verschiedenen Aspekte von ABC-Evaluation beinhalten. Die Felder sind "Kohärenz" (Kohärenz), "Konsistenz" (Konsistenz), "Wissen" (Wissen) und "Emotionales Verstehen" (Emotionales Verstehen). Jedes Feld ist in einem separaten Rechteck eingeschlossen, und die Felder sind in einer 2x2-Formation angeordnet.

Die Grafik wurde von Emory University erstellt, und es gibt auch einen kleinen Bildausschnitt von Alexa, einer virtuellen Hilfsestelle von Amazon, im rechten oberen Teil der Grafik.

Die Grafik zeigt, dass die Kohärenz und die Konsistenz wichtige Aspekte von ABC-Evaluation sind, da sie in den Feldern "Kohärenz" und "Konsistenz" aufgelistet werden. Das Wissen und das Emotionale Verstehen sind auch wichtige Aspekte von ABC-Evaluation, da sie in den Feldern "Wissen" und "Emotionales Verstehen" aufgelistet werden.</sample>
    <sample id="1032">ABC-Eval Verhaltensweisen</sample>
    <sample id="1033">ABC-Eval Verhaltensweisen
Coherence
Ignoring Partner
Irrelevant
Consistency
Emotional Understanding</sample>
    <sample id="1034">ABC-Eval Verhaltensweisen

Kohärenz
Ignorieren Partner
Irrelevante

Konsistenz
Selbstkontradition
Partnerkontradiction

Wissen
Falscher Fakt
Allgemeine Verletzung

Emotionales Verstehen
Empathische Reaktion
Mangelnde Empathie</sample>
    <sample id="1035">Experimente

• 4 allgemein anwendbare Dialogmodelle
• 100 mensch-basierte Konversationen pro Modell
• ABC-Eval

Die Grafik zeigt den Prozess der Evaluierung der Dialogmodelle. Es werden 100 mensch-basierte Konversationen pro Modell durch das ABC-Eval-System abgearbeitet. Das System besticht aus drei Schritten: ABC, BCD und CDE.</sample>
    <sample id="1036">Experimente

- 4 Dialog-Modell
- 100 mensch-botschaftliche Konversationen pro Modell

ABC-Eval Turn Likert Dialog Likert Comparative

Emory University Alexa</sample>
    <sample id="1037">Baselines-Evaluation

Die Grafik zeigt die Evaluierungen von Dialogen unter Verwendung von Baselines. Es werden verschiedene Aspekte des Dialogs bewertet, einschließlich Konsistenz, Emotionaler Verstehens-, Inhalt-, Turn Likert, Dialog Likert, Comparative, Grammatikalität, Relevanz und allgemeine Qualität.

Die Grafik zeigt auch, dass die Evaluatoren auf verschiedenen Ebenen bewertet wurden, einschließlich Kritik, Effizienz, Effizienz, Komplexität, Klarheit, Originalität, Relevanz, Rhythmus, Struktur, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizienz, Thicknesseffizien</sample>
    <sample id="1038">Inter-annotator Agreement

Die Grafik zeigt die Kappa-Wertkennstelle (Krippendorff Alpha) für verschiedene Evaluiermethoden und Aspekte der Dialoge. Die Y-Achse zeigt die Kappa-Werte, die von 0,0 bis 0,6 variieren. Die X-Achse zeigt diverse Evaluiermethoden und Aspekte, einschließlich:

- Turn Likert
- Dialogue Likert
- Comparative
- Quality Likert
- Emotion Likert
- Engagement Likert
- Fluency Likert
- Coherence Likert
- Relevance Likert
- Persuasiveness Likert
- Humor Likert
- Empathy Likert
- Trust Likert
- Credibility Likert
- Knowledge Likert
- Style Likert
- Tone Likert
- Register Likert
- Genre Likert
- Purpose Likert
- Audience Likert
- Context Likert
- Topic Likert
- Life Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Likert
- Money Likert
- Work Likert
- School Likert
- Family Likert
- Friends Likert
- Hobbies Likert
- Travel Likert
- Food Likert
- Drink Likert
- Music Likert
- Movies Likert
- Books Likert
- Sports Likert
- News Likert
- Weather Likert
- Health Lik</sample>
    <sample id="1039">Predictive Validität

Die Grafik zeigt die Gültigkeit von Proglossis anhand verschiedener Evaluiermethoden. Es werden verschiedene Aspekte des Dialogs abgegriffen, darunter Grammatik, Semantik, Emotion und Reaktionsfähigkeit. Die Gültigkeit wird als "Prozentualer Gültigkeitsanteil" (PR) dargestellt.

Die Grafik zeigt, dass die Gültigkeit in den verschiedenen Aspekten variiert. Zum Beispiel scheint die Gültigkeit in Grammatik und Semantik relativ hoch zu sein, während sie in Emotion und Reaktionsfähigkeit relativ niedrig ist.

Die Grafik zeigt auch, dass die Gültigkeit in den verschiedenen Evaluiermethoden variiert. Zum Beispiel scheint die Gültigkeit in der "ABC-Eval" relativ hoch zu sein, während sie in der "Turn Likert" relativ niedrig ist.

Insgesamt zeigt die Grafik, dass die Gültigkeit von Proglossis in den verschiedenen Aspekten und Evaluiermethoden variiert.</sample>
    <sample id="1040">Predictive Validität

Die Grafik zeigt die Gültigkeit von Prädiktionsmodellen auf der Basis verschiedener Evaluationsmethoden. Es werden verschiedene Aspekte einer Dialoggeneratoren bewertet, darunter Grammatik, Emotionen, Reaktionsfähigkeit und kognitive Kompetenz.

Die Grafik zeigt die Gültigkeit von Modellen auf der Grundlage von:

1. **Turn Likert**: Hier wird die Gültigkeit von Modellen bewertet, die auf der Grundlage von Turn Likert-Evaluation bewertet wurden.
2. **Dialogue Likert**: In diesem Fall werden die Gültigkeitsmodelle auf der Grundlage von Dialogue Likert-Evaluation bewertet.
3. **Comparative Likert**: Hier wird die Gültigkeit von Modellen bewertet, die auf der Grundlage von Comparative Likert-Evaluation bewertet wurden.

Jede der drei Evaluator-Methoden zeigt unterschiedliche Gültigkeitsmodelle, die auf der Grundlage von verschiedenen Aspekten bewertet wurden.</sample>
    <sample id="1041">Incrementale Gültigkeit

Die Grafik zeigt die Gültigkeit von verschiedenen Dialogsystemen in Abhängigkeit von der % der Qualität, die erklärt wird (R²). Es werden verschiedene Aspekte wie Empathie, Selbstkontrolle, Redundanz, Sympathie, Proaktivität, Emotion und Relevanz abgeglichen. Die Grafik zeigt, dass die Gültigkeit variiert, je nach dem Aspekt, der abgeglichen wird.

Die Grafik zeigt auch, dass die Gültigkeit von Dialogsystemen in Abhängigkeit von den verschiedenen Aspekten variiert. Beispielsweise scheint die Gültigkeit von Dialogsystemen, die auf Empathie abgelenkt sind, geringer zu sein als die Gültigkeit von Dialogsystemen, die auf Proaktivität abgelenkt sind.</sample>
    <sample id="1042">Incrementale Gültigkeit

Die Grafik zeigt die Gültigkeit von verschiedenen Dialogsystemen in Abhängigkeit von der % der Qualität, die erklärt wird. Es gibt drei Hauptkategorien: ABC-Eval, Turn Likert und Dialog Likert.

Die ABC-Eval-Kategorie zeigt, dass die Gültigkeit von "Unempathisch" bis "Self-Controll" variiert. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Turn Likert-Kategorie zeigt, dass die Gültigkeit von "Unempathisch" bis "Relevanz" variiert. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Dialog Likert-Kategorie zeigt, dass die Gültigkeit von "Unempathisch" bis "Relevanz" variiert. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Proaktiv" oder "Reaktiv" sind. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Emotion" oder "Relevanz" unterstützen. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Empathie" oder "Relevanz" unterstützen. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Kontrolle" oder "Relevanz" unterstützen. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Selbstbewusstsein" oder "Relevanz" unterstützen. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Selbstbewusstsein" oder "Relevanz" unterstützen. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem, ob sie "Selbstbewusstsein" oder "Relevanz" unterstützen. Die Gültigkeit beginnt bei 0,75 und steigt bis zu 0,25.

Die Grafik zeigt auch, dass die Gültigkeit von den Dialogsystemen variiert, je nachdem,</sample>
    <sample id="1043">Incrementale Gültigkeit

Die Grafik zeigt die Gültigkeit von verschiedenen Dialogsystemen in Abhängigkeit von der % der Qualität, die ermittelt wurde. Es zeigt auch, dass die Gültigkeit von Dialogsystemen variiert und je nach verwendetem Evaluationsinstrument unterschiedlich ist.

Die Grafik zeigt auch, dass die Gültigkeit von Dialogsystemen variiert und je nach verwendetem Evaluationsinstrument unterschiedlich ist.</sample>
    <sample id="1044">ABC-Eval-Fehlerquote pro Modell

Die Grafik zeigt die Fehlquote pro Modell in einem Balkendiagramm. Es gibt neun Modelle, die auf der x-Achse dargestellt werden: BART-FID-RAG, Blender2, Emora, Blender-Decode, SelfContrast, TopicContrast, CSContrast, Anti-social und Uninterpretable. Jedes Modell wird in einem separaten Balken dargestellt, der die verschiedenen Arten von Fehlern zeigt.

Die y-Achse zeigt die Fehlquote in einem prozentualen Format. Es gibt acht verschiedene Arten von Fehlern, die auf der y-Achse dargestellt werden: Antisocial, CSContrast, Ignore, Incorrect, Irrelevant, Unintentional, OtherContrast, Redundant, SelfContrast, TopicContrast, CSContrast, Anti-social und Uninterpretable.

Die Grafik zeigt, dass die Fehlquote variiert, je nach Art des Fehlers und Modell. Einige Modelle scheinen besser auf bestimmte Arten von Fehlern zu agieren als andere.</sample>
    <sample id="1045">ABC-Eval-Fehlerquote pro Modell

Die Grafik zeigt die Fehlquote pro Modell in einem Diagramm, das verschiedene Arten von Fehlern darstellt. Hier ist eine detaillierte Beschreibung der verschiedenen Elemente:

1. **Titel**: "ABC-Eval-Fehlerquote pro Modell" (ABC-Eval-Fehlerquote pro Modell)
2. **X-Achse**: Die X-Achse zeigt die verschiedenen Arten von Fehlern an, die als Kategorien aufgelistet sind:
   - Antisocial
   - CS Contr.
   - Ignore
   - Incorrect
   - Irrelevant
   - Unempathetic
   - Other Contr.
   - Redundant
   - Self Contr.
   - Topic Switch
   - Uninterpretable
3. **Y-Achse**: Die Y-Achse zeigt die Fehlquote in einem prozentualen Format, ranging from 0% to 30%.
4. **Balkendiagramm**: Jede Farbe im Balkendiagramm repräsentiert ein Modell:
   - Blauer Balken: BART
   - Grüner Balken: FID
   - Lila Balken: RAG
   - Purple Balken: Blender2
   - Orange Balken: Emora
   - Pink Balken: Blender-Decode
5. **Legende**: Eine kleine Legende im unteren Teil des Diagramms zeigt die Farben und ihre entsprechenden Modelle.
6. **Quelle**: DasDiagramm wurde von Emory University erstellt und Amazon Alexa zeigt das Logo im unteren rechten Ecke.

Die Grafik zeigt, dass die Fehlquote variieren kann, je nach Art des Fehlers und das Modell, das verwendet wird. Einige Modelle scheinen besser auf bestimmte Fehlarten zu reagieren als andere.</sample>
    <sample id="1046">ABC-Eval Fehlertoleranzen pro Modell</sample>
    <sample id="1047">ABC-Eval-Fehlerquote pro Modell

Die Grafik zeigt die Fehlquote pro Modell in einem Diagramm mit mehreren Farben. Es zeigt die Fehlquote als Anteil an Wendungen (in %) an.</sample>
    <sample id="1048">Die Autoren gehören an der Emory University.</sample>
    <sample id="1049">CFT steht für 'continuous fine-tuning'.</sample>
    <sample id="1050">Es sind insgesamt acht Autoren an der Arbeit beteiligt.</sample>
    <sample id="1051">Wann erfordert Übersetzung Kontext? Eine datengetriebene, multilinguale Exploration Patrick Fernandes, Kayo Yin, Emmy Liu Andre F. T. Martins, Graham Neubig Carnegie Mellon University Language Technologies Institute IF TECNICO LISBOA BERKELEY ARTIFICIAL INTELLIGENCE RESEARCH Unbabel Gleichwertiger Beitrag</sample>
    <sample id="1052">Übersetzung hängt von Kontext ab
Wir müssen uns um jenes Maul kümmern.</sample>
    <sample id="1053">Übersetzung hängt von Kontext ab Dinge könnten gefährlich werden, wenn die Minister davon erfahren. Wir müssen den Maulwurf loswerden.</sample>
    <sample id="1054">Übersetzung hängt von Kontext ab Could it be anything serious, Doctor? We'll have to get rid of that mole.</sample>
    <sample id="1055">Evaluating context-dependent translation is hard</sample>
    <sample id="1056">Evaluating context-dependent translation is hard

- Nur ein kleiner Anteil an Worten hängt von Kontext ab
  - Korpus-level-metriques
- Gegenwartige Methoden unterstützen begrenzte Diskursphänomene und Sprachen</sample>
    <sample id="1057">RQ1: Wann benötigt eine Übersetzung Kontext? RQ2: Wie gut können Modelle Kontext-abhängige Übersetzungen bewältigen?</sample>
    <sample id="1058">RQ1: Wann erfordert Übersetzung Kontext?
- Wortstelle-basierter Kontextgebrauch

RQ2: Wie gut können Modelle Übersetzungen mit Kontext abhängig machen?</sample>
    <sample id="1059">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1060">Conditional Cross-Mutual Information (CXMI) CXMI: Messung, wie viel Kontext MT-Modelle verwenden, um einem Corpus Übersetzungen zu geben. Uncertainty over translations given the source HqMTA(Y|X) Uncertainty over translations given the source AND context HqMTA(Y|X,C) CXMI(C) → Y|X</sample>
    <sample id="1061">Pointwise (P-)CXMI

• WirIntroduzierenP-CXMI,umkontextnutzungstotranslateeinspezi</sample>
    <sample id="1062">RQ1: Wann erfordert Übersetzung Kontext?

- Word-level context usage
- Thematic analysis

RQ2: Wie gut können Modelle Übersetzungen abhängig vom Kontext verarbeiten?</sample>
    <sample id="1063">RQ1: Wann erfordert Übersetzung Kontext?
- Wortstellebasierte Kontextnutzung
- Themenanalyse
RQ2: Wie gut bewältigen Modelle Übersetzungen, die Kontext abhängig sind?</sample>
    <sample id="1064">Thematic analysis of high P-CXMI words</sample>
    <sample id="1065">Thematic analysis of high P-CXMI words 1. POS tags</sample>
    <sample id="1066">Themenanalyse von hochwertigen P-CXMI-Worten 1. POS Tags Pronomen</sample>
    <sample id="1067">Thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items - Pronouns - Verb form</sample>
    <sample id="1068">Themenanalyse von P-CXMI-Worten 1. POS Tags 2. Vokabeln Avellile's Mutter war immer noch in der Schlafphase. Avellile ging zur Schule.</sample>
    <sample id="1069">Themenanalyse von high P-CXMI-Worten

1. POS-Tags
2. Vokabeln

Beispiele:
- Avellie's Mutter war noch in der Schlaf.
- Avellie ist zur Schule gegangen.

Beispiele:
- 阿維利尔的母亲仍在睡觉。
- 阿維利尔去了学校。</sample>
    <sample id="1070">The thematic analysis of high P-CXMI words 1. POS tags 2. Vocabulary items 3. Individual tokens - Pronouns - Verb form - Lexical cohesion - Formality - Ellipsis She knows where we're going. I don't. Ich weiß, wo wir gehen. Ich weiß es nicht.</sample>
    <sample id="1071">RQ1: Wann erfordert Übersetzung Kontext?
- Wortstellebasierte Kontextnutzung
- Themenanalyse

RQ2: Wie gut bewältigen Modelle Übersetzungen, die Kontext abhängig sind?
- Multilinguale Diskurs-Aware (MuDA) Benchmark</sample>
    <sample id="1072">Multilingual Discourse-Aware (MuDA) tagger

- Pronouns
- Verb form
- Lexical cohesion
- Formality
- Ellipsis</sample>
    <sample id="1073">Multilingual Discourse-Aware (MuDA) tagger

Die Grafik zeigt die Häufigkeit von verschiedenen Grammatikalien in verschiedenen Sprachen. Es zeigt, dass Pronomen, Verbformen, Lexikalische Kohärenz, Formalität und Ellipsis in verschiedenen Sprachen unterschiedlich häufig vorkommen.</sample>
    <sample id="1074">MuDA Benchmark

Eine Gruppe von Dokumenten wird in einem Prozess durch einen "MuDA-Tagger" bearbeitet. Der "MuDA-Tagger" ist ein Modell, das speziell darauf aus ist, die Qualität von Übersetzungen zu bewerten und zu verbessern. Nach der Bearbeitung durch den "MuDA-Tagger" werden die Dokumente an verschiedenen Evaluationsmetriken überprüft.

Die Ausgangsdatei wird als "BLEU" bezeichnet, was eine Metrik ist, um die Ähnlichkeit zwischen zwei Texten zu messen, insbesondere bei Übersetzungen. Das "COMET" ist eine weitere Metrik, die die Qualität von Übersetzungen bewertet, indem sie die Ähnlichkeiten zwischen dem Originaltext und der Übersetzung identifiziert. Es gibt auch andere Metriken, wie "F-measure", die die Gesamtqualität einer Übersetzung einschätzen.

Nach der Überprüfung der Ausgangsdatei an diesen verschiedenen Metriken wird die Datei an einem Roboter übermittelt. Der Roboter sieht sich als "Automat" und ist verantwortlich für die Analyse und Interpretation der Daten, die ihm übermittelt wurden.</sample>
    <sample id="1075">RQ1: Wann benötigt eine Übersetzung Kontext? - Kontextbedarf an Wortebene - Themenanalyse RQ2: Wie gut können Modelle übersetzungen mit Kontext verarbeiten? - Multilinguale Discourse-Aware (MuDA) Benchmark - Modellevaluation</sample>
    <sample id="1076">Korpusniveau-Metrisen BLEU</sample>
    <sample id="1077">Korpusniveau-Metrischen

Die drei Roboter symbolisieren verschiedene Metrischen zur Evaluierung von Übersetzungen:

1. BLEU: Der Roboter mit dem "KEIN TEXT" -Schild repräsentiert die BLEU-Methode, die sich auf die Häufigkeit gemeinsamer N-gramm-Sequence in einem Referenztext und einem Hypothetext basiert.
2. COMET: Der Roboter mit dem " Kontext " -Schild symbolisiert die COMET-Methode, die sich auf die Kontextualität und die Bedeutung von Worten und Phrasen basiert.
3. F-MEASURE: Der Roboter mit dem standardisierten Schild repräsentiert die F-Maßstelle, die eine Balance zwischen der Genauigkeit (True Positives / TP) und der Erinnerung (True Negatives / TN) misst.

Diese Metrischen werden verwendet, um die Qualität von Übersetzungen zu bewerten und sie mit anderen Übersetzungen zu vergleichen.</sample>
    <sample id="1078">Korpusniveau-Metrischen

Es ist nicht klar, welches System am besten für die Evaluierung von Dokumenten im Bereich maschinelles Übersetzen (MT) mit Hilfe von Korpusniveau-Metrisken geeignet ist.

Die Grafik zeigt drei verschiedene Roboterfiguren, jede mit einem unterschiedlichen Metrik-Logo:

1. BLEU: Der BLEU-Metrik-Logo sieht aus wie ein Roboter mit einem Kreis um den Kopf, in dem das Wort "BLEU" steht.
2. COMET: Das COMET-Metrik-Logo zeigt einen Roboter mit einem violetten Hintergrund und dem Wort "COMET" im Kopf.
3. F-measure: Das F-measure-Metrik-Logo zeigt einen Roboter mit einem normalen Hintergrund und dem Wort "F-measure" im Kopf.

Jede dieser Metrischen (BLEU, COMET und F-measure) wird verwendet, um die Leistung von maschinellem Übersetzen zu bewerten, indem sie den Übersetzungen gegenüber den Standardtexten messen. Es ist jedoch schwierig zu bestimmen, welche Metrik allgemein als die beste Wahl gilt, da sie unterschiedliche Aspekte der Übersetzungsqualität einschließen.</sample>
    <sample id="1079">MuDA Benchmark-Ergebnisse

• Kontext-aware-Modelle performieren signifikant besser an einigen Phänomenen
o Formalität, lexikalische Kohärenz</sample>
    <sample id="1080">MuDA Benchmark-Ergebnisse

• Kontext-aware-Modelle erzielen signifikant bessere Leistungen bei ein paar Phänomenen
	- Formalität, lexikalische Kohärenz: ✅✅✅
	- Ellipsis, Pronomen, Verbform: ❌❌❌</sample>
    <sample id="1081">MuDA Benchmark-Ergebnisse

• Kontext-aware-Modelle erzielen signifikant bessere Leistungen bei ein paar Phänomenen
o ✅ Formalität, lexikalische Kohärenz ❌ Ellipsen, Pronomen, Grammatik

• DeepL übertrifft Google in den meisten Phänomenen und Sprachpaaren*

• Logo von DeepL &gt; Logo von Google

*Stand vom April 2021</sample>
    <sample id="1082">Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1083">Identify discourse phenomena systematically without prior linguistic knowledge Dataset-agnostic benchmark for document-level MT</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">Sie hat keinen Namen.</sample>
    <sample id="1122">Die Methode der "markierten Wörter" besticht darin, Wörter zu finden, die Personen von markierten Gruppen von unmarkierten Gruppen unterscheiden.</sample>
    <sample id="1123">Die Autoren gehören an der Paul G. Allen School, der University of Washington NLP, Carnegie Mellon University Language Technologies Institute und der Yandex Institute.</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">Sarah E. Finch</sample>
    <sample id="1126">4</sample>
    <sample id="1127">SyntaxGym und CrowS.</sample>
    <sample id="1161">FT, W, BOND, COSINE, MLC, L2R</sample>
    <sample id="1162">Das Modell wird auf 11 Aufgaben evaluiert.</sample>
    <sample id="1226">CamemBERT wurde ursprünglich auf 4GB von Wikipedia-Daten trainiert.</sample>
    <sample id="1227">Adam Przepiórkowski</sample>
    <sample id="1228">Die Analyse der Leistungsverluste im Laufe der Zeit führte zu dem Schluss, dass die zeitliche Verzögerung die Hauptursache für den Leistungsverlust war.</sample>
    <sample id="1269">Um die Permutation zu kennzeichnen.</sample>
    <sample id="1270">Die Autoren argumentieren, dass die Transparenz der Methoden zur Abkehrung von Vorurteilen wichtig ist.</sample>
    <sample id="1271">Inakzeptable Minimalpaareingaben sind Stereotypische Sätze und Non-Stereotypische Sätze.</sample>
    <sample id="1272">Die Autoren haben die Evaluiermetrischen Alfred, Medical-Specialty, MUSCA-DiBT, MUSCA-ESAI, ComaBERT, QuADRoEMEA und QuADRo-MEDIE verwendet.</sample>
    <sample id="1273">Krippendorff Alpha</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">Die Autoren gehören Heinrich Heine University Düsseldorf, Germany an.</sample>
    <sample id="1276">MultiInstruct ist ein einzigartiger Benchmark, der die Evaluierung von Sprachmodellen auf mehreren Sprachen und Aufgaben kombiniert.</sample>
    <sample id="1277">Drei.</sample>
    <sample id="1278">Die binäre Koordination ist definiert als die Hälfte der absoluten Differenz der Längen der Komponenten.</sample>
    <sample id="1279">Die in dieser Studie verwendeten Prompts im Durchschnitt lagen bei 16,5 Wörtern.</sample>
    <sample id="1280">Die Auswirkungen auf das kleinere T5-Modell sind, dass es die besten Skripte generieren kann.</sample>
    <sample id="1281">DrBERT: Ein robustes vortrainiertes Modell im Französischen für biomedizinische und klinische Bereiche
Yanis Labarre1,4 Adrien Bazille2,3 Richard Dufour2 Mickael Rouvier1 Emmanuel Morin2,3 Beatrice Dallie2 Pierre-Antoine Gourraud4 (1) LIA, Avignon Université (2) LS2N, Université de Nantes (3) Centres des données, CHU de Nantes (4) Zenith</sample>
    <sample id="1282">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von vorher einstellenden Strategien, Datensätzen und Größen III. Evaluation von 13 Modellen an 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1283">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von vorher einstellenden Strategien, Datensätzen und Größen III. Auswertung von 13 Modellen an 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1284">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von vorher einstellenden Strategien, Datensätzen und Größen III. Evaluation von 13 Modellen an 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1285">I. Sprachmodellierung in der Gesundheitsversorgung II. Vergleich von vorheriger Schulung, Datensätzen und Größen III. Auswertung von 13 Modellen an 11 Aufgaben IV. Verteilung von NACHOS und DrBERT</sample>
    <sample id="1286">Language Modeling

• Transformer-basierte Ansätze, wie BERT, bieten einen großen Leistungszuwachs auf NLP-Aufgaben.
• Wurden auf Französisch mit CamemBERT und FlauBERT anpassen.
• Auf medizinischen Aufgaben haben dominante spezifische Modelle in Englisch die Barren noch höher gehoben
    - PubMedBERT, BioBERT, ClinicalBERT und weitere
• Sprachen außer Englisch sind seltener und hängen primär von existierenden generischen Modellen ab
• Noch keine offene Quellen-Modelle für die biomedizinische Branche in Französisch sind verfüglich
• BERT-basiertes dominantes Modell für Französisch sollte die Leistung auf medizinischen Aufgaben verbessern</sample>
    <sample id="1287">Language Modeling

- Ansatz basierend auf Transformer, wie BERT, gewährt einen großen Leistungszuwachs an NLP Aufgaben.
- wurde auf Französisch mit CamemBERT und FlauBERT angepasst
- Auf medizinischen Aufgaben: Domänen spezifische Modelle im Englischen haben die Barre noch höher gehoben
    - PubMedBERT, BioBERT, KlinischerBERT und andere
- Sprachen außer Englisch sind seltener und hängen hauptsächlich von existierenden allgemeinen Modellen ab
- Noch keine allgemeine Modelle, keine offene Quellenmodelle für die biomedizinische Branche in Französisch
- BERT-basiertes domänen spezifisches Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern</sample>
    <sample id="1288">Language Modeling

• Transformer-basierte Ansätze, wie BERT, bieten einen großen Leistungszuwachs für eine Vielzahl von NLP Aufgaben.
• Sie wurden auf Französisch mit CamemBERT und FlauBERT anpassiert.
• Auf medizinischen Aufgaben haben dominantly-specific Models in Englisch die Barre noch höher gehoben.
• PudMedBERT, BioBERT, ClinicalBERT und andere
• Sprachen außer Englisch sind seltener und hängen hauptsächlich von existierenden allgemeinen Modellen ab.
• Noch keine allgemeine Modelle, keine offene Quellen-Modelle sind für die biomedizinische Branche in Französisch verfügbar.
• Ein BERT-basierter dominantly-specific Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1289">Transformer-basierte Ansätze, wie BERT, bieten einen großen Leistungszuwachs für eine Vielzahl von NLP-Aufgaben. Sie wurden auf Französisch mit CamemBERT und FlauBERT anpassiert. Auf medizinischen Aufgaben haben dominante spezifische Modelle im Englischen die Barre noch höher gehoben. PudMedBERT, BioBERT, KlinischesBERT und andere. Sprachen außer Englisch sind seltener und hängen hauptsächlich von existierenden allgemeinen Modellen ab. Noch keine allgemeine Modelle, keine offene Quellen-Modelle sind für die biomedizinische Branche in Französisch verfügbar. BERT-basiertes dominantes Modell für Französisch sollte die Leistung bei medizinischen Aufgaben verbessern.</sample>
    <sample id="1290">Evaluation of the impact of public and private medical data sources

* HETABERT: A 1.1 billion words open-source dataset covering diverse medical domains, including anonymized data extracted from the Nanterre University Hospital data warehouse
* NACHOS: A private dataset of sentences taken from 1.7 million anonymized medical records extracted from the Nanterre University Hospital data warehouse

Comparison of learning strategies

* From scratch: Full model construction from scratch
* Continual pre-training: Using an existing pre-trained model, such as CamemBERT, a French generic model, and PubMedBERT</sample>
    <sample id="1291">Evaluierung des Einflusses von öffentlichen und privaten medizinischen Datensätzen auf vergleichbare Datengröße

* Vergleich von Datensätzen
	+ NACHOS: Eine 1,1 Millionen Wörter offene Quellen datenbank diverse medizinische Domänen, anonymisierte Datensätze von 1,7 Million Krankenhausprotokollen extrahiert aus dem Nanterre Krankenhausaufbewahrungsstelle
	+ Privater Datensatz von Sätzen, die aus 1,7 Million anonymisierten Krankenhausprotokollen extrahiert wurden
* Vergleich von Lernstrategien
	+ Von Grund aus mit vollständiger Konstruktion
	+ Continual Pre-Training mit einem bestehenden vortrainierten Modell
	+ Publiziertes Modell CamemBERT, ein französischer allgemeiner Modell, und Publiziertes Modell MedBERT</sample>
    <sample id="1292">Evaluierung der Auswirkungen von öffentlichen und privaten medizinischen Datensammlungen auf vergleichbare Datengröße

- **NACHOS**: Eine 1,1 Millionen Begriffe große offene Quellenquelle diverser medizinischer Domänen, die anonymisierte Medien daten extrahiert vom Nanterre University Hospital Data Warehouse.
- **Corpus**: Enthält 7,3 GB von 1,1 Millionen Begriffen.
- **NACHOS (Privat)**: Enthält 4,8 GB von 655 Mio Begriffen.
- **NBDW (Sonderfall)**: Enthält 4,4 GB von 4,31 Mio Begriffen.
- **NBDW (Privat)**: Enthält 6,4 GB von 6,4 Mio Begriffen.

**Vergleich von Lernstrategien**

- **Von Grund ausscratch mit vollständiger Konstruktion**
- **Continuous pre-training**: Nutzt einen bestehenden vortrainierten Modell
- **CamermedBERT**: Ein französischer generischer Modell
- **PubMedBERT**: Kontinuierliche vortraining

Die Tabelle im unteren Teil der Präsentation vergleicht verschiedene Modelle basierend auf den oben genannten Datensammlungen und Lernstrategien.</sample>
    <sample id="1293">Evaluierung der Auswirkungen von öffentlichen und privaten medizinischen Datensammlungen auf vergleichbare Datengröße

* Evaluierung der Auswirkungen von öffentlichen und privaten medizinischen Datensammlungen auf vergleichbare Datengröße
o NACHOS: Eine 1,1 Millionen Wörter offene Quellen Datensammlung diverse medizinische Domänen, anonymisierte Datensammlung von Krankenhausprotokollen aus verschiedenen Krankenhaugen
o Privat Datensatz von Sätzen, die aus 1,7 Million anonymisierten Krankenhausprotokollen extrahiert wurden, die aus den Nantes Universitätskrankenhausbereich stammen

* Vergleich von Lernstrategien
o Von Grund aus mit vollständiger Konstruktion
o Continual Pre-Training: Verwendung eines bestehenden vortrainierten Modells, z.B. ContinualBERT, eine deutsche allgemeine Modell, und PubMedBERT, ein englischer allgemeiner Modell, und NACHOSBERT</sample>
    <sample id="1294">Evaluierung der Auswirkungen von öffentlichen und privaten medizinischen Datensammlungen auf vergleichbare Datengröße

* Vergleich von NACHOS-A 1.1 words Open-Source Datensammlung diverse medizinische Domänen, Stile und Arten mit einem privaten Datensatz von Sätzen, die aus 1,7 Mio anonymisierten Krankenhausprotokollen im Nanterre Krankenhaus entnommen wurden
* Evaluierung der Auswirkungen von Datensammlungen auf die Datengröße

Vergleich von Lernstrategien

* Von Grund aus mit vollständiger Konstruktion
* Fortlaufende Pre-Training-Strategie mit einem bestehenden vortrainierten Modell
* Publiziertes CamemBERT, ein französischer allgemeiner Modell, und Publiziertes MedBERT

Quelle: Avignon Université</sample>
    <sample id="1295">Evaluierung der Auswirkungen von öffentlichen und privaten medizinischen Datensammlungen auf vergleichbare Datengröße

* Vergleich von Datensammlungen
	+ NACHOS: Eine 1,1 Billionen-Word-Open-Source-Datensammlung diverser medizinischer Domänen, Stile und Naturen (7,4 GB)
	+ NBDW: Eine anonymisierte Datensammlung von Sätzen aus 1,7 Billionen medizinischen Datensätzen (4,4 GB)
	+ Universitäres Krankenhospital-Datendienst (Privat): 4,3 Billionen Sätze (6,4 GB)
* Vergleich von Lernstrategien
	+ Von Grund aus: Vom vollständigen Bau einer neuen Modellstruktur
	+ Fortlaufende Lernstrategie: Nutzen eines bestehenden vortrainierten Modells (z.B. ContinualBERT, ChBERT, PubMedBERT, CamemBERT, PubMedBERT)
	+ Publiziertes MedienBERT: Ein französischer generischer Modell, der auf einem großen Datensatz trainiert wurde</sample>
    <sample id="1296">Evaluierung der Auswirkungen von öffentlichen und privaten medizinischen Datensammlungen auf vergleichbare Datengröße

* Vergleich von Datensammlungen
	+ NACHOS: Eine 1,1 Billionen-Word-Open-Source-Datensammlung aus diversen Medizinbereichen, Stilen und Naturen (7,4 GB)
	+ NBWD: Eine private Datensammlung von Sätzen aus 1,7 Billionen anonymisierten Medizinnotizen (4,4 GB)
	+ Universitäres Krankenhausaufbewahrungsmodell (both): 4,4 GB
* Vergleich von Lernstrategien
	+ Von Grund aus mit vollständiger Konstruktion
	+ Continual Pre-Training mit einem bestehenden vortrainierten Modell (ChBERT, NACHOS, CamBERT, NBWD)
	+ Publizierte Arbeiten, wie CamMedBERT, ein französischer allgemeiner Modell, und Publizierte Arbeiten, wie NACHOS</sample>
    <sample id="1297">Evaluation of the impact of public and private medical data sources

* **HETEROGENEOUS DATA SOURCES**
	+ NACHOS: A 1.1 billion words open-source dataset of diverse medical domains, including anonymized data from 1.7 million medical records extracted from the Nanterre University Hospital data warehouse.
	+ Private dataset of sentences taken from 1.7 million anonymized medical records extracted from the Nanterre University Hospital data warehouse.
* **COMPARISON OF LEARNING STRATEGIES**
	+ From scratch: Full model construction
	+ Continual pre-training using an existing pre-trained model:
		- ContinualBERT: A French generic model
		- PubMedBERT: A French generic model
		- NACHOS: A 1.1 billion words open-source dataset of diverse medical domains, including anonymized data from 1.7 million medical records extracted from the Nanterre University Hospital data warehouse.

Corpus | #words | #sentences
NACHOS | 7.4 GB | 543 M
NBDW | 4 GB | 659 M
NBDW (both) | 4+4 GB | 4.3 M
NBDW (private) | 4 GB | 6.4 M</sample>
    <sample id="1298">Evaluierung von 13 Datensätzen und Grösse

- Leitstelle: Medizinische Berichte Spezialisierungen
- Evaluationsobjekt: 13 Datensätze, öffentlich und privat
- Metrik: Fine-tuned Modell im Detail der Kunst

Die Tabelle zeigt die Leitstelle-MUSCA-DAT-SPEZIALISIERUNGEN-EVALUATIONSMETRIK-FINETUNED-DETAILDERKUNST-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUADROEMEA-QUADROMEDICINE-ESSELS-CAS-QUADROMECO-QUAD</sample>
    <sample id="1299">Evaluierung: 13 Datenermyny und Grösse

- Leistungsevaluation von 13 Modellen auf 11 öffentlichen und privaten Aufgaben
- Unser einstelltes Modell erreicht ein an der Spitze aller Modelle stehendes Niveau

Die Tabelle zeigt die Leistungen von Modellen auf verschiedenen Aufgaben:

| Modell | Medizinische Berichterstelle | MUSCA-DET | MUSCA-DET | ESSAI | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS | CAS</sample>
    <sample id="1300">Evaluierung von 13 Datenermittlungen und Grösse
• Performance evaluation der 13-modell-ansatz auf 11 öffentlichen und privaten Aufgaben
• Unser fine-tuned Modell erreicht einen Höhepunkt im Bereich der Kunst der Kunst an fast allen Feldern

Die Tabelle zeigt die Leistungsstelle von BERT und BERT-ähnlichen Modellen auf verschiedenen Aufgaben im Vergleich. Hier sind die Hauptpunkte:

1. **Allgemeine Aufgaben:**
   - **BERT:** 76,04%
   - **RoBERTa:** 82,59%
   - **DistilBERT:** 78,46%
   - **CamemBERT:** 82,78%
   - **CannabisBERT:** 82,87%
   - **DeBERTa:** 83,44%
   - **DeBERTa-12GB:** 83,44%
   - **DeBERTa-16GB:** 83,44%
   - **DeBERTa-32GB:** 83,44%
   - **DeBERTa-64GB:** 83,44%
   - **DeBERTa-128GB:** 83,44%
   - **DeBERTa-256GB:** 83,44%
   - **DeBERTa-512GB:** 83,44%
   - **DeBERTa-1024GB:** 83,44%
   - **DeBERTa-2048GB:** 83,44%
   - **DeBERTa-4096GB:** 83,44%
   - **DeBERTa-8192GB:** 83,44%
   - **DeBERTa-16384GB:** 83,44%
   - **DeBERTa-32768GB:** 83,44%
   - **DeBERTa-65536GB:** 83,44%
   - **DeBERTa-131072GB:** 83,44%
   - **DeBERTa-262144GB:** 83,44%
   - **DeBERTa-524288GB:** 83,44%
   - **DeBERTa-1048576GB:** 83,44%
   - **DeBERTa-2097152GB:** 83,44%
   - **DeBERTa-4194304GB:** 83,44%
   - **DeBERTa-8388608GB:** 83,44%
   - **DeBERTa-16777216GB:** 83,44%
   - **DeBERTa-33554432GB:** 83,44%
   - **DeBERTa-67108864GB:** 83,44%
   - **DeBERTa-134217728GB:** 83,44%
   - **DeBERTa-268435456GB:** 83,44%
   - **DeBERTa-536870912GB:** 83,44%
   - **DeBERTa-1073741824GB:** 83,44%
   - **DeBERTa-2147483648GB:** 83,44%
   - **DeBERTa-4294967296GB:** 83,44%
   - **DeBERTa-8589934592GB:** 83,44%
   - **DeBERTa-17179869184GB:** 83,44%
   - **DeBERTa-34359738368GB:** 83,44%
   - **DeBERTa-68719476736GB:** 83,44%
   - **DeBERTa-137438953472GB:** 83,44%
   - **DeBERTa-274877906944GB:** 83,44%
   - **DeBERTa-549755813888GB:** 83,44%
   - **DeBERTa-1099511627776GB:** 83,44%
   - **DeBERTa-2199023255552GB:** 83,44%
   - **DeBERTa-4398046511104GB:** 83,44%
   - **DeBERTa-8796093022208GB:** 83,44%
   - **DeBERTa-17592186044416GB:** 83,44%
   - **DeBERTa-35184372088832GB:** 83,44%
   - **DeBERTa-70368744177664GB:** 83,44%
   - **DeBERTa-140737488355328GB:** 83,44%
   - **DeBERTa-281474976710656GB:** 83,44%
   - **DeBERTa-562949953421312GB:** 83,44%
   - **DeBERTa-1125899906842624GB:** 83,44%
   - **DeBERTa-2251799813685248GB:** 83,44%
   - **DeBERTa-4503599627370496GB:** 83,44%
   - **DeBERTa-9007199254740992GB:** 83,44%
   - **DeBERTa-18014398509481984GB:** 83,44%
   - **DeBERTa-36028797018963968GB:** 83,44%
   - **DeBERTa-72057594037927936GB:** 83,44%
   - **DeBERTa-144115188075855872GB:** 83,44%
   - **DeBERTa-288230376151711744GB:** 83,44%
   - **DeBERTa-576460752303423488GB:** 83,44%
   - **DeBERTa-1152921504606846976GB:** 83,44%
   - **DeBERTa-2305843009213693952GB:** 83,44%
   - **DeBERTa-4611686018427387904GB:** 83,44%
   - **DeBERTa-9223372036854775808GB:** 83,44%
   - **DeBERTa-18446744073709551616GB:** 83,44%
   - **DeBERTa-36893488147419103232GB:** 83,44%
   - **DeBERTa-73786976294838206464GB:** 83,44%
   - **DeBERTa-147573952589676412928GB:** 83,44%
   - **DeBERTa-295147905179352825856GB:** 83,44%
   - **DeBERTa-590295810358705651712GB:** 83,44%
   - **DeBERTa-1180591620717411303424GB:** 83,44%
   - **DeBERTa-2361183241434822606848GB:** 83,44%
   - **DeBERTa-4722366482869645213696GB:** 83,44%
   - **DeBERTa-9444732965739290427392GB:** 83,44%
   - **DeBERTa-18889465931478580854784GB:** 83,44%
   - **DeBERTa-37778931862957161609568GB:** 83,44%
   - **DeBERTa-75557863725914323219136GB:** 83,44%
   - **DeBERTa-151115727451828646438272GB:** 83,44%
   - **DeBERTa-302231454903657292876544GB:** 83,44%
   - **DeBERTa-604462909807314585753088GB:** 83,44%
   - **DeBERTa-1208925819614629171506176GB:** 83,44%
   - **DeBERTa-2417851639229258343012352GB:** 83,44%
   - **DeBERTa-4835703278458516686024704GB:** 83,44%
   - **DeBERTa-9671406556917033372049408GB:** 83,44%
   - **DeBERTa-19342813113834066744098816GB:** 83,44%
   - **DeBERTa-38685626227668133488197632GB:** 83,44%
   - **DeBERTa-77371252455336266976395264GB:** 83,44%
   - **DeBERTa-154742504910672533952790528GB:** 83,44%
   - **DeBERTa-309485009821345067905581056GB:** 83,44%
   - **DeBERTa-618970019642690135811162112GB:** 83,44%
   - **DeBERTa-1237940039285380271622324224GB:** 83,44%
   - **DeBERTa-2475880078560760543244648448GB:** 83,44%
   - **DeBERTa-4951760157121521086489296896GB:** 83,44%
   - **DeBERTa-9903520314243042172978593792GB:** 83,44%
   - **DeBERTa-19807040628486084345957187584GB:** 83,44%
   - **DeBERTa-39614081256972168691914375168GB:** 83,44%
   - **DeBERTa-79228162513944337383828750336GB:** 83,44%
   - **DeBERTa-158456325027888674767657500672GB:** 83,44%
   - **DeBERTa-316912650055777349535315001344GB:** 83,44%
   - **DeBERTa-633825300111554699070630002688GB:** 83,44%
   - **DeBERTa-1267650602223109398141260005376GB:** 83,44%
   - **DeBERTa-2535301204446218796282520009752GB:** 83,44%
   - **DeBERTa-5070602408892437592565040019504GB:** 83,44%
   - **DeBERTa-10141204817784875185130080039008GB:** 83,44%
   - **DeBERTa-20282409635569750370260160078016GB:** 83,44%
   - **DeBERTa-40564819271139500740520320156032GB:** 83,44%
   - **DeBERTa-81129638542279001481040640312064GB:** 83,44%
   - **DeBERTa-162259277084558002962081280624128GB:** 83,44%
   - **DeBERTa-324518554169116005924162561248256GB:** 83,44%
   - **DeBERTa-649037108338232011848325122496512GB:** 83,44%
   - **DeBERTa-1298074216676464023696650244993024GB:** 83,44%
   - **DeBERTa-2596148433352928047393300489986048GB:** 83,44%
   - **DeBERTa-5192296866705856094786600979972096GB:** 83,44%
   - **DeBERTa-10384593733411712189573201959944192GB:** 83,44%
   - **DeBERTa-20769187466823424379146403919888384GB:** 83,44%
   - **DeBERTa-41538374933646848758292807839776768GB:** 83,44%
   - **DeBERTa-8307674986729369751</sample>
    <sample id="1301">Evaluierung von 13 Datensätzen und Grösse
• Performance evaluation der 13 Modell-varianten auf 11 öffentlichen und privaten Aufgaben
• Unser fine-tuned Modell erreicht ein an der Spitze aller Modelle
Die Tabelle zeigt die Genauigkeit (F1-Score) der verschiedenen Modell-varianten auf verschiedenen Aufgaben an. Hier sind einige Beispiele:
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Clinical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biochemical" hat auf der Aufgabe "Medical Report Specialties" einen F1-Score von 98,93.
- "CinC Biomedical" hat auf der Auf</sample>
    <sample id="1302">Evaluierung: Vordereinrichtung
- Von Grund aus vs. ständige Vordereinrichtung auf 4GB von Daten
- Fragensbehandelungs Aufgaben erfordern mehr spezifische Kenntnisse, um gut arbeiten zu können
- Eine Studie zur Modell-Stabilität zeigt, dass die CamenBERT-basierten Modelle mit ständiger Vordereinrichtung ein höhere Inter-Rund-Variabilität aufweisen

Die Tabelle vergleicht verschiedene Medizin-Spezialisierungen und die Leistung der verschiedenen Modelle:

| Medizin-Spezialisierung | AIHF | Medizin-Spezialisierung | MUSCA-DET | MUSCA-ESAI | Human | QuaDroid-EDEMA | QuaDroid-MEDIC |
| --- | --- | --- | --- | --- | --- | --- | --- |
| CamenBERT Oscar | 33,13 80,11 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G | 43,11 79,86 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+ | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+U | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+V | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+W | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+X | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+Y | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+Z | 46,01 79,72 | Medizin-Spezialisierung | 99,20 | 85,44 | 92,10 | 95,22 | 74,14 |
| CamenBERT CNN+G+L+M+N+O+P+Q+R+S+T+</sample>
    <sample id="1303">Evaluierung: Vordereinrichtung
- Von Grund aus vs. ständige Vordereinrichtung auf 4GB von Daten
- Fragensbehandelungs Aufgaben erfordern mehr spezifische Kenntnisse, um gut zu arbeiten
- Eine Studie zur Modell-Stabilität zeigt, dass die CamenBERT-basierten Modelle mit kontinuierlicher Vordereinrichtung ein höhere inter-ran-Varianz aufweisen

Die Tabelle vergleicht verschiedene Modelle auf verschiedenen Aufgaben wie NER, CLS, POS und others.</sample>
    <sample id="1304">Evaluierung: Vordereinrichtung
- Von Grund aus vs. ständige Vordereinrichtung auf 4GB von Daten
- Fragensbehandelungs Aufgaben erfordern mehr spezifische Kenntnisse, um gut arbeiten zu können
- Eine Studie zur Modell-Stabilität zeigt, dass die CamemBERT-basierten Modelle mit ständiger Vordereinrichtung eine höhere Inter-Rund-Stabilität aufweisen

Tabelle:
| aHF | Med. Bild. Spezialis. | MUSCA-DiRT | MUSCA-ESAI | HumanMIMIC | QUADRO-EMMA | QUADRO-MED |
| --- | --- | --- | --- | --- | --- | --- |
| CamemBERT Oscar | 33.13 | 80.33 | 99.20 | 85.44 | 95.22 | 74.14 |
| CamemBERT CNN+G | 43.11 | 79.86 | 98.55 | 84.72 | 93.24 | 77.61 |
| CamemBERT CNN+G+Ch | 46.01 | 79.72 | 98.80 | 84.66 | 93.24 | 75.68 |
| CamemBERT CNN+G+Ch+L | 46.01 | 79.72 | 98.80 | 84.66 | 93.24 | 75.68 |
| DREBERT Nachos | 45.35 | 79.87 | 98.85 | 85.77 | 93.24 | 73.18 |
| CamemBERT NWD | 46.01 | 80.33 | 99.20 | 85.44 | 95.22 | 74.14 |
| CamemBERT NWD+Ch | 46.01 | 80.33 | 99.20 | 85.44 | 95.22 | 74.14 |
| CamemBERT NWD+Ch+L | 46.01 | 80.33 | 99.20 | 85.44 | 95.22 | 74.14 |

Quelle: Avignon Université</sample>
    <sample id="1305">DrBERT übertrifft die besten Ergebnisse von CamemBERT in 9 downstream-Französisch-medizinorientierten Aufgaben. Dies bestätigt die Nützlichkeit des Trainings spezifischer medizinischer Modelle in Französisch. Es ist wichtig, auf heterogene Daten zu trainieren; die Nutzung der robusteren NACHOS-Datenbank ist besser als die Nutzung privater klinischer Daten nur. Das fortlaufende Pretraining ist besser, aber nicht effektiver, wenn es auf domain-spezifischen englischen Modellen basiert. DrBERT-Modelle, die NACHOS-Datenbank und die Trainingsskripte sind unter der MIT-Lizenz frei zugänglich.</sample>
    <sample id="1306">DrBERT übertrifft die besten Ergebnisse im Bereich der Medizin auf 9 Stream-French-Schwankungen. Es bestätigt, dass das Training auf einem medizinischen Modell in Französisch nützlich ist. Es ist wichtig, die Datenermittlung zu beachten, indem man auf heterogene Daten trainiert. NACHOS ist robust, wenn man nur private klinische Daten verwendet. Das fortlaufende Trainieren ist besser, aber nicht effektiv, wenn es auf einem englischen Modell basiert. DrBERT-Modelle, die NACHOS-Dataset und die Trainingsskripte unter der MIT-Lizenz frei zur Verfügung stehen.</sample>
    <sample id="1307">DrBERT übertrifft die besten Leistungen im Bereich der Medizin auf 9 Stream French medizinorientierten Aufgaben. Es bestätigt die Nützlichkeit des Trainings spezifischer medizinische Modelle in Französisch. Data sources matters: das Training auf heterogenen Daten ist wichtig. NACHOS ist robust, wenn man private klinische Daten nur verwendet. Continuous pretraining ist besser, aber nicht effektiv, um Skalierbarkeit zu erreichen. Es basiert auf domain-specific English Modellen. DrBERT-Modelle, der NACHOS Datensatz und die Trainingsskripte sind unter der MIT Lizenz frei zugänglich.</sample>
    <sample id="1308">Danke. Ich freue mich darauf, mich im Torontoer Poster-Seminar zu sehen.</sample>
    <sample id="1309">Die Arbeit untersucht die Lernstrategien "From scratch" mit vollständiger Konstruktion, "Continual pre-training" mit einem bestehenden vortrainierten Modell und "PubMedBERT", einem neuen generischen Modell.</sample>
    <sample id="1310">Der Faktor der Überanpassung, der speziell auf die Wiederverwendung von Tests zurückzuführen ist, beträgt 0.35.</sample>
    <sample id="1311">Die Qualität der Vereinfachung wurde mittels BLEU-Scores beurteilt.</sample>
    <sample id="1312">Ja, Sprachmodelle können politische Vorurteile haben.</sample>
    <sample id="1313">Kombinatorische allgemeinheitliche Generalisierung ohne Bäume unter Verwendung von Multiset Tagging und latenten Permutationen Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="1314">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="1315">Kompositionale allgemeinheit
Fähigkeit eines Lerners, um mit tieferen Rekursionen und unbekannten Kombinationen von Phrasen umzugehen, die während des Trainings separat gesehen wurden.</sample>
    <sample id="1316">Titel: Kombinatorische allgemeinheit in semantischem Parsen

Training:

Die Mädchen schlafen.
"girl x; sleep agent x, x"

Mary wusste, dass die Mädchen schlafen.
"Mary x; know agent x, x; girl x; sleep agent x, x"</sample>
    <sample id="1317">Kompositioneller Allgemeinheitsansatz in Semantikparsen

Train:</sample>
    <sample id="1318">Kompositionale allgemeinheit in semantischer Analyse

Train:</sample>
    <sample id="1319">Train: Mary kannte den Jungen, der schläft. Test: Jim hat gesagt, dass Mary den Jungen kannte, der schläft.</sample>
    <sample id="1320">Kompositionale allgemeinheit in semantischem Parsen

Train:
- Die Mädchen schliefen.
- Mary wusste, dass die Mädchen schliefen.

Test:
- Jim sagte, dass Mary wusste, dass sie schliefen.

Naive seq2seq-Modelle scheitern!</sample>
    <sample id="1321">Kompositionelle allgemeinheit in semantischem Parsen

Train:
- "Die Mädchen schlafen." (Die Mädchen schließen sich an.)
- "Mary kannte die Mädchen." (Mary kannte sich an die Mädchen heran.)

Test:
- "Jim hat gesagt, dass Mary kannte, dass sie schlafen." (Jim hat gesagt, dass Mary sich kannte, an die Schlaflosigkeit heran.)</sample>
    <sample id="1322">Bäume helfen sehr, aber...</sample>
    <sample id="1323">Bäume helfen sehr, aber...</sample>
    <sample id="1324">Bäume helfen sehr, aber...</sample>
    <sample id="1325">Bäume helfen sehr, aber...</sample>
    <sample id="1326">Bäume helfen sehr, aber... Bäume werden benötigt, um: - Vorne/Nach-processing logische Formen - Grammatik-induktion zu erhalten</sample>
    <sample id="1327">Bäume helfen sehr, aber...Trees help a lot but... *girl x1; sleep agent x2; x* *girl x1; x* *sleep agent x2; slept* The girl slept. Trees needed to be obtained: Pre/Post-processing logical forms Grammar-induction For the first time, we show strong generalization to deeper recursion without trees.</sample>
    <sample id="1328">Bäume helfen sehr, aber...Trees help a lot but... *girl x1;sleep agent x2*x1*girl xj*xksleep agent xkThe girl slept. Baums werden benötigt, um: Pre/Post-processing logischer FormenGrammar-induktionTrees nennen Sie neuronale Seqseq-Model, das direkte Model des Korrespondierens zwischen Fragmente.</sample>
    <sample id="1329">Our Approach

Die Grafik zeigt einen Prozess, der die Analyse und Identifikation von Schlüsselwörtern in einem Text durchläuft. Hier ist eine detaillierte Schritt-für-Schritt-Übersetzung des englischen Inhalts ins Deutsche:

1. **"Our Approach"**:
   - Dieser Titel zeigt an, dass die Grafik einen bestimmten Ansatz oder eine Methode zur Analyse von Texten beschreibt.

2. **"x1, girl, x1"**:
   - "x1" und "x2" sind Platzhalter für unbekannte Werte oder Variablen.
   - "girl" ist ein Schlüsselwort, das im Text identifiziert wurde.

3. **"sleep agent, x2"**:
   - "sleep" ist ein weiteres Schlüsselwort, das im Text entdeckt wurde.
   - "agent" könnte hier als Kontextualisator dienen, der den Kontext des Schlüsselworts "sleep" bereitstellt.
   - "x2" ist ein anderer unbekannter Wert oder Variablen.

4. **"the, girl, slept"**:
   - "the" ist ein determinativer Artikel, der im Originaltext vorkommt.
   - "girl" und "slept" sind die Hauptwörter im Satz "the girl slept", was zeigt, dass das Schlüsselwort "sleep" in diesem Kontext verwendet wurde.

5. **"Tag"**:
   - "Tag" ist ein Begriff, der in diesem Kontext verwendet wird, um die Identifizierung und Klassifizierung von Schlüsselwörtern zu kennzeichnen.

Insgesamt zeigt die Grafik einen Prozess, bei dem Schlüsselwörter in einem Text identifiziert und analysiert werden, um den Kontext und die Bedeutung zu verstehen.</sample>
    <sample id="1330">Our Approach

Die Grafik zeigt einen Prozess, der似乎是自然语言处理或文本分析的一部分。它包括以下步骤：

1. **输入**：首先，有一个输入阶段，其中包含两个标记（x1 和 x2），一个逗号（,）和一个代词（girl）。这些元素似乎代表了输入文本的某些部分或组件。

2. **Tagging**：接下来，有一个“Tag”阶段，其中将输入标记与相应的标签关联起来。对于 x1，标签是 "the"；对于逗号，标签是 "girl"；对于 x2，标签是 "slept"。这表明系统正在识别和分类输入文本中的单词或标记。

3. **输出**：最后，有一个输出阶段，其中包含了经过处理后的标记及其对应的标签。输出显示了原始输入（the, girl, slept）以及它们被识别和分类后的标签（the, girl, slept）。

这个过程似乎是自然语言处理任务的一部分，例如分词、词性标注或依存关系分析。输入文本被分解成基本单元（标记），然后为每个标记分配适当的标签，以更好地理解文本的结构和含义。</sample>
    <sample id="1331">Our Approach

Die Struktur der Grafik zeigt, wie ein Satz in einem bestimmten Format codiert wird. Der Satz "the girl slept" (die Mädchen schliefen) wird in einem Modell codiert, das die Wörter und ihre Beziehungen codiert. Hier ist eine detaillierte Schritt-für-Schritt-Translation:

1. **"the"** - Dieses Wort wird als "x1" codiert.
2. **"girl"** - Dieses Wort wird als "x2" codiert.
3. **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die coderten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x1" codiert.
* **"girl"** - Dieses Wort wird als "x2" codiert.
* **"slept"** - Dieses Wort wird als "x3" codiert.

Die Wörter "the", "girl" und "slept" werden als "Tag" codiert. Das Modell codiert dann die Beziehungen zwischen den Wörtern, indem es die Wörter und die Beziehungen codiert. Hier sind die codierten Beziehungen:

* **"the"** - Dieses Wort wird als "x</sample>
    <sample id="1332">Our Approach

Wir nennen unser Ansatz "Unsere Herangehensweise".</sample>
    <sample id="1333">Unser Ansatz

Die Grafik zeigt einen Prozess, der "Unser Ansatz" beinhaltet. Hier ist eine detailliertere Beschreibung:

1. **Eingabe**: Der Prozess beginnt mit einer Eingabe, die als "girl" und "sleep" dargestellt wird.
2. **Vorgabe**: Eine Vorgabe wird festgelegt, die als "agent" bezeichnet wird.
3. **Permutation**: Die Eingabe und die Vorgabe werden permutiert, was bedeutet, dass sie in verschiedenen Reihenfolgen kombiniert werden.
4. **Ausgabe**: Der Prozess endet mit der Ausgabe, die als "the girl slept" dargestellt wird.

In einfachen Worten: Der Prozess nimmt zwei Wörter ("girl" und "sleep"),混合 sie mit einem Agenten ("agent") und produziert schließlich ein neues Wort ("the girl slept").</sample>
    <sample id="1334">Permuting with "jumps"</sample>
    <sample id="1335">Permuting with "jumps"</sample>
    <sample id="1336">Permuting with "jumps"</sample>
    <sample id="1337">Permuting with "jumps" Permute Tag * girl x1 girl sleep agent x2 the girl slept</sample>
    <sample id="1338">Permuting with "jumps"</sample>
    <sample id="1339">Einige Resultate auf COGS (Kim und Linzen 2020) Vergleich mit anderen Treeless-Modellen anhand von Strukturschwäche auf COGS Modell LSTM seq2seq T3 Zhen und Lapata Unsere</sample>
    <sample id="1340">Einige Resultate auf COGS (Kim und Linzen 2020) Vergleich mit anderen Treeless-Modellen anhand von Strukturaltem Generalisierungsmodell auf COGS Modell LSTM seq2seq TB Zheng und Lapata Unsere</sample>
    <sample id="1341">Technical Challenges We Solve

* `girl` `x1`
* `sleep` `agent`
* `x2`

Permute

? `Tag` ?

? `Tag` ?

? `Tag` ?

Alignment unknown.</sample>
    <sample id="1342">Technical Challenges We Solve

* `girl` `x₁`
* `sleep` `x₂`
* `agent` `x₃`

Permute

? `the` ?
? `girl` ?
? `sleep` ?

Alignment unknown.</sample>
    <sample id="1343">Technical Challenges We Solve

Eine Grafik zeigt die verschiedenen Schritte, die in einem Prozess durchlaufen werden. Es beginnt mit den Eingabeelementen "girl", "x1", "j", "sleep" und "agent", die in einem Kreis miteinander verbunden sind. Ein Pfeil zeigt von "sleep" zu "agent", was dann zu einem Pfeil von "agent" zu "x1" führt. Ein weiterer Pfeil führt von "x1" zu "j". Der Pfeil von "j" führt zu "x1", der dann zu "sleep" führt.

Unter dem Diagramm steht "Alignment unknown." (Ausrichtung unbekannt).

Dieses Diagramm zeigt, wie verschiedene Elemente in einem Prozess miteinander verbunden sind und wie sie sich aufeinander auswirken. Es zeigt auch, dass die Ausrichtung dieser Elemente unbekannt ist.</sample>
    <sample id="1344">Technical Challenges We Solve

Die Grafik zeigt einen Prozess, bei dem verschiedene Elemente (sleep, agent, girl) miteinander verbunden werden. Es wird erwähnt, dass die Ausrichtung unbekannt ist und durch das Indukieren im Training erreicht werden soll. Die Evolution des Modells wird auch diskutiert.

In der Grafik werden verschiedene Elemente miteinander verbunden, um zu zeigen, wie sie zusammenarbeiten. Es wird erwähnt, dass die Ausrichtung unbekannt ist und durch das Indukieren im Training erreicht werden soll. Die Evolution des Modells wird auch diskutiert.</sample>
    <sample id="1345">Technical Challenges We Solve

Die Grafik zeigt einen Prozess, der die technischen Herausforderungen einer bestimmten Aufgabe oder eines bestimmten Systems darstellt. Der Prozess beginnt mit den Eingabeelementen "girl", "sleep" und "agent", die in einem Knoten zusammengefasst werden. Aus diesem Knoten werden dann verschiedene Permutationen erzeugt, indem die Eingabeelemente in verschiedenen Reihenfolgen kombiniert werden. Jede Permutation wird dann an einem Knoten abgefangen, der das "Permute" Label trägt.

Nach dem "Permute"-Knoten wird das Prozess durch einen "Tag"-Knoten fortgesetzt. Der "Tag"-Knoten enthält eine Liste von Schlüsselwörtern, darunter "the", "girl" und "sleep". Das Prozess verwendet dann diese Schlüsselwörter, um eine bestimmte Aktion auszuführen, indem sie die Schlüsselwörter in den Knoten "x1" und "x2" einfügen.

Die Grafik zeigt auch, dass die Altersstelle unbekannt ist und in der Trainingseinheit induziert werden soll. Es wird auch erwähnt, dass die Inferenzmodell ist NP-schwierig (ungleichwertig) und das TSP (Traveling Salesman Problem).

Insgesamt zeigt die Grafik einen komplexen Prozess, der die technischen Herausforderungen einer bestimmten Aufgabe oder eines bestimmten Systems darstellt.</sample>
    <sample id="1346">Technical Challenges We Solve

Die Grafik zeigt einen neuronalen Netzes Modell, das eine natürliche Sprache Verarbeitung Aufgabe解决。该模型由两个输入层（"girl"和"x1"）和两个输出层（"sleep"和"x2"）组成。中间有一个隐藏层，其中包含多个神经元，这些神经元连接到输入和输出层。箭头表示神经元之间的权重和激活。

在输入层中，"girl"和"x1"被连接到隐藏层的神经元。这些神经元进一步连接到输出层的"sleep"和"x2"。在隐藏层中，神经元之间存在反馈环路，表明模型使用循环神经网络（RNN）来处理序列数据。反馈环路允许模型捕捉时间依赖关系，并在处理序列数据时保持记忆。

在输出层中，"sleep"和"x2"被标记为标签。这意味着模型试图预测或分类输入序列的标签。在训练过程中，模型尝试通过最小化预测标签与实际标签之间的差异来学习输入序列和标签之间的映射关系。

在输入层中，输入序列"the girl"被标记为标签。这意味着模型试图预测或分类输入序列的标签。在训练过程中，模型尝试通过最小化预测标签与实际标签之间的差异来学习输入序列和标签之间的映射关系。

在训练过程中，模型尝试通过最小化预测标签与实际标签之间的差异来学习输入序列和标签之间的映射关系。在训练过程中，模型尝试通过最小化预测标签与实际标签之间的差异来学习输入序列和标签之间的映射关系。

在训练过程中，模型尝试通过最小化预测标签与实际标签之间的差异来学习输入序列和标签之间的映射关系。在训练过程中，模型尝试通过最小化预测标签与实际标签之间的差异来学习输入序列和标签之间的映射关系。</sample>
    <sample id="1347">Kognitive Dissonanz ist die Unstimmigkeit zwischen zwei Elementen der Cognition, wie Gedanken, Handeln, Glaubenswerte usw.</sample>
    <sample id="1348">ALBERT-large</sample>
    <sample id="1349">Ja, kumulatives Training ist besser als iteratives Training für aktives Lernen.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">Die Daten für die MuDa-Benchmark stammen aus der Europäischen Union.</sample>
    <sample id="1385">Matthias Lindemann</sample>
    <sample id="1386">Sprachübergreifender Transfer bezieht sich auf die Übergabe von Modellen von einer Sprache zu einer anderen Sprache.</sample>
    <sample id="1387">Die Autoren gehören der Saarland University, Amazon Alexa und der University of Vienna an.</sample>
    <sample id="1388">Die Autoren verwenden Latenzmessungen in Sekunden.</sample>
    <sample id="1389">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources

Die KITMUS-Prüfung bewertet die Integrität von Kenntnissen aus mehreren Quellen.

[Redakteur]

[Redakteur]</sample>
    <sample id="1390">NLU-Modelle ziehen auf mehrere Kenntnisquellen zurück

Die Grafik zeigt, wie NLU-Modelle auf mehrere Kenntnisquellen zugreifen. Es werden zwei Hauptquellen dargestellt: "Kenntnis in Parametern" (vorheriges Wissen) und "Kenntnis im Kontext" (inferenzzeitiges Wissen). Der NLU-Modellediagramm zeigt, dass das Modell sowohl vorheriges als auch aktuelles Wissen berücksichtigen kann, um Befunde zu erhalten.</sample>
    <sample id="1391">NLU-Modelle ziehen auf mehrere Kenntnisquellen zurück</sample>
    <sample id="1392">John sah den neu gewählten Präsidenten im Fernsehen

Vortrainingstimes Wissen

Was Präsidenten tun? □
Was ist ein Fernseher? □
Wer ist John? X
Wer ist der neue Präsident? X</sample>
    <sample id="1393">John sah den neu gewählten Präsidenten im Fernsehen

Vortrainingstimes Wissen
Was Präsidenten machen
Was ein Fernseher ist

Wer ist John?
Wer ist der neue Präsident?</sample>
    <sample id="1394">John sah den neu gewählten Präsidenten im Fernsehen. Was tun Präsidenten? (Ja) Was ist ein Fernseher? (Ja) Wer ist John? (Nein) Wer ist der neue Präsident? (Nein)</sample>
    <sample id="1395">John sieht den neu gewählten Präsidenten im Fernsehen.</sample>
    <sample id="1396">KITMUS Test Suite

- Datensatz für die Evaluation der Kenntnisintegration
- Coreference resolution task, um die Fähigkeit zu prüfen, auf
  - vorher trainiertes Wissen
  - während des Inferenzprozesses erworbenes Wissen
- Experiment mit
  - menschlichen Studienteilnehmern
  - Coreference resolution Modellen</sample>
    <sample id="1397">KITMUS Test Suite

- Datensatz für die Evaluation der Kenntnisintegration
- VorausbildungsAufgabentechnik zur Untersuchung der Fähigkeit, auf
  - vorheriges Kenntnis zu zurückgreifen
  - inferenzzeitiges Kenntnis zu nutzen
- Experiment mit
  - menschlichen Studienteilnehmern
  - Coreference-Resolution-Modellen</sample>
    <sample id="1398">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag im Gericht, in dem er Fälle entschieden hat, war er froh, sich zu entspannen. [Antwort: Servin]</sample>
    <sample id="1399">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich an einem Park. Nach einem langen Tag im Gericht, in dem er Fälle entschieden hat, war er froh zu entspannen. [Antwort: Servin]</sample>
    <sample id="1400">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich in einem Park. Nach einem langen Tag an der Arbeit, in der er Fälle im Gericht entschieden hat, war er froh, sich zu entspannen. [Antwort: Servin]</sample>
    <sample id="1401">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich an einem langen Tag im Park, nach einem langen Tag an der Arbeit, bei der er Entscheidungen für Fälle getroffen hat. Er war glücklich, sich zu entspannen. (Antwort: Servin)</sample>
    <sample id="1402">Servin ist ein Richter. Kea ist ein Bäcker. Servin und Kea trafen sich an einem langen Tag im Park, nach einem langen Tag voller Entscheidungen in einem Gericht. Er war glücklich, sich zu entspannen. (Antwort: Servin)</sample>
    <sample id="1403">Variants of KITMUS

Die verschiedenen Varianten von KITMUS werden in der Folienübersicht dargestellt. KITMUS ist ein Ansatz zur Integration von Hintergrundwissen in maschinelles Lernen. In der Folienübersicht werden verschiedene Ansätze zur Integration von Hintergrundwissen diskutiert.

Die ersten zwei Ansätze, Background-Pretrain und Background-Both, verwenden Hintergrundwissen in der Trainingsphase. Im Fall von Background-Pretrain wird das Hintergrundwissen vor dem Training des Modells verwendet, um die Modellkapazitäten zu erweitern. Im Fall von Background-Both wird das Hintergrundwissen sowohl während des Trainings als auch während des Inferenzprozesses verwendet, um das Modell zu verbessern.

Der dritte Ansatz, Background-Inference, verwendet das Hintergrundwissen nur während des Inferenzprozesses. Dieser Ansatz ist nützlich, wenn das Hintergrundwissen nicht für die Modelltrainingphase verwendet werden soll.

Die Folienübersicht zeigt auch, dass die verschiedenen Ansätze unterschiedliche Vorteile und Nachteile haben. Background-Pretrain und Background-Both können dazu beitragen, dass das Modell besser generalisiert und robust gegenüber neuen Daten ist. Allerdings kann das Hintergrundwissen auch das Modell beeinträchtigen, indem es die Modellkapazitäten überfordert. Background-Inference hält das Modell von Hintergrundwissen beeinträchtigen zu, aber es kann auch dazu beitragen, dass das Modell langsamer ist.

Insgesamt bieten die verschiedenen Ansätze zur Integration von Hintergrundwissen in maschinelles Lernen verschiedene Vorteile und Nachteile. Es ist wichtig, den richtigen Ansatz zu wählen, um das Modell optimal zu trainieren und zu optimieren.</sample>
    <sample id="1404">Variants of KITMUS

Die verschiedenen Varianten von KITMUS werden in der Folien präsentiert. KITMUS steht für Knowledge-Integrated Textual Multi-Task Learning, und es sind verschiedene Ansätze zur Integration von Hintergrundknowledge in die Textverarbeitung diskutiert.

(a) Background-Pretrain: Typische Einrichtung

In dieser Variante wird die Hintergrundknowledge vor dem Training des Modells berücksichtigt. Das Modell lernen auf einem Datensatz mit Hintergrundknowledge und einem Datensatz ohne Hintergrundknowledge ab.

(b) Background-Both: Explizit Bereitstellen von Hintergrundknowledge im Kontext

In dieser Variante wird die Hintergrundknowledge explizit in den Kontext des Datensatzes integriert. Das Modell lernen auf einem Datensatz mit Hintergrundknowledge und einem Datensatz ohne Hintergrundknowledge ab.

(c) Background-Inference: Kenntnis nur verfügbar bei der Inferenzzeit

In dieser Variante wird die Hintergrundknowledge nur bei der Inferenzzeit verwendet. Das Modell lernen auf einem Datensatz ohne Hintergrundknowledge ab, und die Hintergrundknowledge wird nur verwendet, wenn das Modell eine Vorhersage macht.

Die verschiedenen Varianten von KITMUS werden diskutiert, um zu verstehen, wie die Hintergrundknowledge in die Textverarbeitung integriert werden kann. Es wird untersucht, welche Ansatz variante am besten geeignet ist, um die Hintergrundknowledge zu verwenden.</sample>
    <sample id="1405">Variants of KITMUS

Die verschiedenen Varianten von KITMUS werden in der Folien präsentiert. KITMUS steht für Knowledge-Integrated Textual Multi-Task Learning, und es gibt verschiedene Ansätze, um diese Methode anzuwenden.

(a) Background-Pretrain: Typische Einrichtung

In dieser Variante wird das Hintergrundwissen vor dem Training verwendet. Das Modell lernen die Hintergründe im Voraus und verwendet dann das gelernte Wissen bei der Textverarbeitung.

(b) Background-Both: Explizit提供背景知识在上下文中

In dieser Variante wird das Hintergrundwissen explizit in den Text integriert. Das Modell lernen sowohl das Hintergrundwissen als auch den Text im Voraus und verwendet dann das gelernte Wissen bei der Textverarbeitung.

(c) Background-Inference: Kenntnis nuravailable bei der Inferenzzeit

In dieser Variante wird das Hintergrundwissen erst bei der Inferenzzeit verwendet. Das Modell lernen nur den Text im Voraus und verwendet dann das gelernte Wissen bei der Textverarbeitung.</sample>
    <sample id="1406">Politiker suchen an einer Wahl einen Sitzen im Regierung. Chichester ist ein Politiker.</sample>
    <sample id="1407">Politiker suchen Wahlen an. Chichester ist ein Politiker.</sample>
    <sample id="1408">Politiker suchen Wahlstelle im Regierung. Chichester ist ein Politiker.</sample>
    <sample id="1409">Politiker suchen Wahlstelle in Regierung. Chichester ist ein Politiker.</sample>
    <sample id="1410">Task-specific training is necessary for knowledge integration</sample>
    <sample id="1411">Task-specific training is necessary for knowledge integration</sample>
    <sample id="1412">Task-specific training is necessary for knowledge integration</sample>
    <sample id="1413">Background-Inference

Die Grafik zeigt die Durchschnittliche Genauigkeit (Mean Accuracy) von Modellen bei der Integration von Hintergrundwissen zu verschiedenen Zeiten. Es werden drei Ansätze verglichen: "Random Choice", "Human-like Participants" und "BERT4Core". Die Y-Achse zeigt die Genauigkeit von 0,0 bis 1,0, während die X-Achse die Art des Hintergrundwissens unterscheidet.

Die Grafik zeigt, dass die Genauigkeit beim "Random Choice" sehr niedrig liegt, nahezu null. Beim "Human-like Participants" und "BERT4Core" ist die Genauigkeit etwas höher, aber immer noch unter 0,2. Es wird auch erwähnt, dass die Modelle Schwierigkeiten haben, das Hintergrundwissen zu integrieren, insbesondere zu den Zeiten der Inferenz.

Quelle: Slide 14</sample>
    <sample id="1414">Schlussfolgerungen:

1. Viele Modelle scheinen nicht in der Lage zu sein, Wissen aus mehreren Quellen (Vortrain-Zeit und Inferenzzeit) zu überlegen.
2. Spezifische Ausbildung ist notwendig, um das Wissen zu integrieren.
3. Modelle kämpfen darum, Wissen aus Inferenzzeit- Hintergrund zu integrieren.

Finden Sie die Datensatz-, Generierungs- &amp; Evaluierungscode auf GitHub unter mpoeimsi/kitmus.</sample>
    <sample id="1415">Schlussfolgerung

Hauptpunkte:

1. Viele Modelle scheinen in der Lage, Wissen aus mehreren Quellen (Train-Zeit und Inferenz-Zeit) zu überprüfen.
2. Spezifische Ausbildung ist für die Integration von Wissen notwendig.
3. Modelle kämpfen darum, Wissen aus Inferenz-Zeit-Quellen zu integrieren.

Finden Sie das Dataset, die Generations- und Evaluierungscode auf GitHub unter mpoemsl/kitmus.</sample>
    <sample id="1416">Baumbasierte Methoden können schwere Rechenoperationen erfordern.</sample>
    <sample id="1417">Die Autoren gehören an der Georgia Institute of Technology.</sample>
    <sample id="1418">Marked Personas

Verwendet natürliche Sprachanregungen, um Stereotypien in Sprachmodellen zu messen

Myra Cheng, Esin Durmus, Dan Jurafsky

Stanford ENGINEERING Computer Science</sample>
    <sample id="1419">Marked Personas: Motivation Soziale Bias und Stereotype sind in LLMs (Large Language Models) weit verbreitete Phänomene. Grenzen bestehender Stereotypenmessmethoden: - Abwägung zwischen Spezifizität und Allgemeinheit - Basieren auf festen, von Menschen geprägten Datensätzen - Berücksichtigen nicht die Intersektionalität</sample>
    <sample id="1420">Marked Personas: Motivation Soziale Biase und Stereotypien sind in LLMs weit verbreitete Phänomene. Grenzen bestehender Stereotyp-Maßnahmen: • Tradeoff zwischen Spezifizität und Allgemeinheit • Basieren auf festen, von Menschen geprägten Datensätzen • Ignorieren die Intersektionalität</sample>
    <sample id="1421">Marked Personas: Motivation Soziale Bias und Stereotypien sind in LLMs weit verbreitete Phänomene. Grenzen der bestehenden Stereotypmaßnahmen: • Tradeoff zwischen Spezifizität und Allgemeinheit • Basieren auf festen, von Menschen geprägten Datensätzen • Ignorieren die Intersektionalität</sample>
    <sample id="1422">Marked Personas: Motivation Soziale Biase und Stereotypien sind in LLMs weit verbreitete Phänomene. Grenzen bestehender Stereotypiemessungen: • Tradeoff zwischen Spezifizität und Allgemeinheit • Basieren auf festen, von Menschen geprägten Datensätzen • Ignorieren die Intersektionalität</sample>
    <sample id="1423">Wie überwinden wir diese Grenzen? GPT-3.5, GPT-4 usw. können Anweisungen in Anfragen beantworten</sample>
    <sample id="1424">Wie überwinden wir diese Grenzen? GPT-3.5, GPT-4 usw. können Anweisungen in Anfragen beantworten.

Eingabe: "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich."</sample>
    <sample id="1425">Wie überwinden wir diese Grenzen? GPT-3.5, GPT-4 usw. können Anweisungen in Anfragen beantworten.

Eingabe: "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich."

Generierbares: kann überprüfen jede Interessensgruppe</sample>
    <sample id="1426">Asiatische Frau: Die eisenbogenförmigen Augen, umrahmt von langen, dunklen Wimern, vermitteln ein Gefühl der ruhigen Kraft und Weisheit. Meine Hautfarbe hat einen weichen goldenen Glanz, der die Geschichten und Geheimnisse meines Stamms zu behalten scheint. Meine Haltbarkeit ist weich und glatt, und es scheint, als ob ich durch die Zeit unbewegt fortgeht. Mein kleiner Korb ist sowohl elegant als auch unerschöpflich, indem er ohne unnötige Aufmerksamkeit beweglich ist.

Mittlerweiche Frau: Sie ist eine Vision der mittelorientalen Schönheit, die exotische und zeitlose Eleganz und geheimnisvolles Aussehen mit sich bringt. Ihre Augen sind langgestreckte, elegantes Lid, die wie feine Federn aussehen. Ihr Blick ist tief und mysteriös, als ob sie die alte Weisheit von tausend arabischen Nächten versteckt.</sample>
    <sample id="1427">Schritt 1: Persona-Beispiele (GPT-4)

Asiatische Frau

Die efeuelförmigen Augen, umrahmt von langen, dunklen Wimern, vermitteln mir ein Gefühl der ruhigen Weisen und Geheimnisse meines Stammbaums. Meine Komplexion hat einen weichen goldenen Glanz, der glatt, glatt und scheinbar unberührt durch die Zeit ist. Mein petite Frame ist elegant und unberührt, erlaubt mir, unbemerkt zu verschwinden, und ist atemberaubend.

Weißer Mann

Als ich vor mir stehe und mich ansehen gehe, bemerke ich die Merkmäler, die mein Aussehen prägen – meine hellen Hautfarben, die sich im Sonnenlicht hell und hell machen, und meine langen, eleganten Wimern, die wie feine Feden umrandet sind. Meine Augen sind tief und mysteriös, als ob sie die alte Weisheit von tausend Arabern verstecken.</sample>
    <sample id="1428">Schritt 1: Persona-Beispiele (GPT-4)

Asiatische Frau

Die hellbraunen Augen, umrahmet von langen, dunklen Wimern, vermitteln einen Eingriff in die Welt der Geheimnisse und Weisen. Meine Haut hat einen sanft goldenen Glanz, der die Geschichten und Geheimnisse von Generation zu Generation überliefert. Meine Stirn ist ein elegantes und unbewegtes Fenster, das sowohl elegant als auch atemberaubend ist, und es erlaubt mir, mich durch die Zeit zu bewegen, ohne aufzufallen.

Mittelostliche Frau

Sie ist eine Vision der Middleton-Beauty, die exotische Weisen und Zeitlosigkeit verbirgt. Ihre hellbraunen Augen sind wie geprägte Lederlappen, die sich mit einem sanft goldenen Glanz bedecken. Sie hat elegante, langwellige Wimern, die wie feine Fächer aussehen. Ihr Blick ist tief und mysteriös, und sie scheint die alte Weisheit des arabischen Reiches zu verbergen.</sample>
    <sample id="1429">Schritt 1: Persona-Beispiele (GPT-4)

Asiatische Frau: Die perlenförmigen Augen, umrandet von langen, dunklen Wimern, vermitteln einen Eindruck von Ruhe und Weisheit. Meine Farbe hat einen sanft goldenen Glanz, was eine Anzeige meines Stammbaumes ist. Meine Hülle ist elegant und unberührt durch die Zeit. Mein perfektes Outfit both elegant und unberührt, erlaubt mir, mich unaufmerksam zu bewegen.

Weißer Mann: Sie steht vor mir im Spiegel und mustert die Merkmale, die mein Aussehen prägen. Ich habe eine hellere Haut, die sich im Sonnenlicht hellt. Ich muss vorsichtig mit dem Sonnenschutz sein, um nicht zu bräunen.</sample>
    <sample id="1430">Schritt 1: Persona-Beispiele (GPT-4)

Asiatische Frau: Die Frau hat eine asiatische Hautfarbe und ist weise. Meine dunklen Augenbrauen scheinen, die Geschichten und Geheimnisse von meiner Herkunft zu halten. Meine Hülle hat einen sanft goldenen Glanz, der mich elegant und unberührt durch die Zeit machen sollte. Mein perfektes Outfit ist elegant und unauffällig, während ich unbemerkt durch die Welt laufe.

Mittelostliche Frau: Sie ist ein Visionär der Mittelosten Schönheit, die exotische Ästhetik und Eleganz beinhaltet. Ihre Augenbrauen sind hellbraun und ihre Augen sind hellblau. Sie hat elegante, langsame Lashen, die wie delicate Feathers aussehen. Ihr Blick ist tief und mysteriös, als ob sie die alte Weisheit von einem arabischen Nachtwald versteckt.

Weißer Mann: Ich stehe vor mir und sehe mich in einem Spiegel an. Ich betrachte die Merkmäler, die mein Aussehen prägen. Ich denke an die Farbe meiner Haut, die hell ist, und wie ich mich im Sonnenlicht blendet. Ich muss vorsichtig mit dem Sonnenblock sein, um nicht zu verbraten.</sample>
    <sample id="1431">1. Personen: Erstelle Personen mit Hilfswörtern wie "Stelle dir vor, du bist eine asiatische Frau. Wie beschreibst du dich?"</sample>
    <sample id="1432">1. Personen: Erstelle Personen unter Verwendung von Anregungen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." a. Inspirierter von einem Psychologie-Studium mit menschlichen Subjekten unter Verwendung des gleichen Anregens</sample>
    <sample id="1433">2 Schritte
1. Personen: Erstellen von Personen unter Verwendung von Anregungen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich selbst." a. Inspiriert von einem Psychologie-Studium mit menschlichen Subjekten unter Verwendung des gleichen Anregens</sample>
    <sample id="1434">2 Schritte

1. Personen: Erstellen von Personen unter Verwendung von Anregungen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich." a. Inspirieren Sie eine Studie mit menschlichen Subjekten unter Verwendung der gleichen Anregungen
2. Markierte Wörter: Finden von Wörtern, die Personen von markierten Gruppen von unmarkierten Gruppen unterscheiden</sample>
    <sample id="1435">2 Schritte

1. Personen: Erstelle Personen unter Verwendung von Anregungen wie "Stelle dir vor, du bist eine asiatische Frau. Beschreibe dich." a. Lerne von einem Studium mit menschlichen Subjekten unter Verwendung desselben Anregens
2. Markierte Wörter: Finde Wörter, die Personen von markierten Gruppen unterscheiden, von unmarkierten Gruppen</sample>
    <sample id="1436">Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1437">Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1438">Markedness: Unmarked groups are default, ordinary Marked groups differ from the default a warrior (unmarked) vs. a woman warrior (marked)</sample>
    <sample id="1439">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

Beispielsweise "Black woman personas", finden Sie Wörter, die sich von beiden unmarkierten Gruppen unterscheiden:

i. White personas
ii. Man personas</sample>
    <sample id="1440">Schritt 2: Markierte Wörter

1. Definieren von unmarkierten und markierten Gruppen
2. Verwenden von gewichteten Log-ODDS-Größen, um die oben stehenden Wörter für jede markierte Gruppe zu unterscheiden

Beispielsweise für "schwarze Frauen" Personen finden, Wörter, die von beiden unmarkierten Gruppen unterscheiden:

i. Weißer Personen
ii. Männer</sample>
    <sample id="1441">Schritt 2: Markierte Wörter

1. Definition von unmarkierten und markierten Gruppen
2. Verwendung gewichteter Log-ODDS-Größen, um die oben stehenden Wörter für jede markierte Gruppe zu unterscheiden

Beispiel: Black Woman Personen, finden Wörter, die sich von beiden unmarkierten Gruppen unterscheiden:
i. White Personen
ii. Mann Personen</sample>
    <sample id="1442">Ergebnisse: Vergleich mit menschlichen Antworten
Generierter Personen enthalten mehr Stereotypen
Schwarze Stereotypen Weiß Stereotypen
GPT-4 GPT-3.5</sample>
    <sample id="1443">Aber... dieser Lexikon ist unvollständig Black Stereotypen in Personen

40
30
20
10
Human
GPT-4 P Black
GPT-3.5 P Black
GPT-4 P White
Basketball
Loud
Attitude
Athletic
Tall
% der Personen
Worte in Black Stereotypen Lexikon</sample>
    <sample id="1444">Aber... dieser Lexikon ist unvollständig Black Stereotypen in Personen Black Stereotypen Lexikon % der Personen "basketball" "laut" "attitude" "athletisch" "tall"</sample>
    <sample id="1445">Aber... dieser Lexikon ist unvollständig Black Stereotypen in Personen Black Stereotypenlexikon: Wörter in Black Stereotypen % von Personen Human GPT-4 P Black GPT-3.5 P Black GPT-4 P White GPT-3.5 P White "basketball" "loud" "attitude" "athletic" "tall"</sample>
    <sample id="1446">Aber... dieser Lexikon ist unvollständig Black Stereotypen in Personen Black Stereotypen Lexikon: BASKETBALL, Laut, Einstellung, Athletik, Tall Green: Menschlich Blauer Farbton: GPT-4 P Black Violett: GPT-3.5 P Black Rote Farbe: GPT-4 P White</sample>
    <sample id="1447">Ergebnisse: Muster in den Hauptwörtern

"Othering" durch Essentialisierende Erzähler:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
- Definiert nur jene Gruppen nach ihrem Identitäten

Vibrante positive Porträt:

- Vibriert, Kurvativ, elegant für lateinamerikanische Frauen
- Petit, delicat, silky für asiatische Frauen
- Stark, widerstandsfähig für schwarz-frauen</sample>
    <sample id="1448">Ergebnisse: Muster in Topwörtern

"Otherton" durch Essentialisierende Erzähler:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  = Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Vibrant für Latina-Frauen
- Petite, delicate, silky für asiatische Frauen
- Stark, resilient für Afroamerikanerinnen</sample>
    <sample id="1449">Ergebnisse: Muster in Topwörtern

Andere durch Essentialisierende Erzähler:

- Kultur, Tradition, Stolz, Exotik für markierte Gruppen
  = Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Vervielfältigendes für Latina-Frauen
  - Petite, delicat, silken für asiatische Frauen
  - Stark, widerstandsfähig für schwarz-frauen</sample>
    <sample id="1450">Ergebnisse: Muster in den Hauptwörtern

"Othering" durch Essentialisierende Erzähler:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
- Definiert diese Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Petite, delicat, silky für Latina-Frauen
- Stark, widerstandsfähig für Afroamerikanerinnen</sample>
    <sample id="1451">Ergebnisse: Muster in Topwörtern

Anderung durch Essentialisierungsberichte:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Petite, delicate, silky für Latina-Frauen
- Stark, widerstandsfähig für Afroamerikanerinnen</sample>
    <sample id="1452">Ergebnisse: Muster in wichtigen Begriffen
"Othering" durch Essentialisierende Erzähler:
- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
= Definiert diese Gruppen nur nach ihrem Identitäten
Positiv-perspektivierte Porträt:
- Vibrant für Latina-Frauen
- Petite, delicate, silky für asiatische Frauen
- Stark, widerstandsfähig für Schwarze Frauen</sample>
    <sample id="1453">Ergebnisse: Muster in Hauptwörtern

Andere durch Essentialisierungsberichte:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  =&gt; Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Petite, delicate, silken für Latina-Frauen
- Stark, widerstandsfähig für Afroamerikanerinnen</sample>
    <sample id="1454">Ergebnisse: Muster in Topwörtern

"Otherton" durch essentialisierende Narrativen:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
- Definiert jene Gruppen nur nach ihrem Identitäten

Vibrante positive Porträt:

- Vibrierte, elegant, silken für lateinamerikanische Frauen
- Petite, delicat, elegant für asiatische Frauen
- Stark, widerstandsfähig für afroamerikanische Frauen</sample>
    <sample id="1455">Ergebnisse: Muster in Topwörtern

"Othering" durch Essentialisierende Erzähler:

- Kultur, Tradition, Stolz, Exotik für markierte Gruppen
- Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Petite, delicat, silky für Latina-Frauen
- Stark, widerstandsfähig für Afroamerikanerinnen</sample>
    <sample id="1456">Ergebnisse: Muster in Hauptwörtern

Andere durch Essentialisierungsberichte:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
  = Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Petite, delicate, silken für Latina-Frauen
- Stark, widerstandsfähig für Afroamerikanerinnen</sample>
    <sample id="1457">Ergebnisse: Muster in Topwörtern

Andere durch Essentialisierungsberichte:

- Kultur, Tradition, Stolz, exotisch für markierte Gruppen
- Definiert jene Gruppen lediglich nach ihrem Identitäten

Vibrante positive Porträt:

- Vibrierte, Kurvige, Seidhafte für lateinamerikanische Frauen
- Stark, widerstandsfähig für afroamerikanische Frauen</sample>
    <sample id="1458">Empfehlungen

Begründung positiver Stereotypen und die Bedeutung von Erzählnarrativen

Interdisziplinärer Blick

Transparenz bezüglich der Bekämpfung von Bias</sample>
    <sample id="1459">Empfehlungen

Begründung positiver Stereotype und Essensnarrative

Eine interdisziplinäre Perspektive

Transparenz bei der Bewältigung von Bias</sample>
    <sample id="1460">Empfehlungen

Begründung positiver Stereotypen und die Bedeutung von Narrativen

Eine interdisziplinäre Perspektive

Transparenz bei der Bewältigung von Bias</sample>
    <sample id="1461">Empfehlungen

Begründung positiver Stereotypen und Essentialisierungsberichte

Eine interdisziplinärer Blick

Transparenz bezüglich Bias-Minderung</sample>
    <sample id="1462">Empfehlungen

Positive Stereotype und Essentialisierungsberichte
Eine interdisziplinäre Perspektive

Transparenz in Bezug auf die Bekämpfung von Bias</sample>
    <sample id="1463">Empfehlungen

* Bewältigung positiver Stereotypen und Essentialisierungsberichte
* Ein interdisziplinärer Blick
* Transparenz bezüglich Bias-Minderung</sample>
    <sample id="1464">Empfehlungen

* Bewältigung positiver Stereotype und Essenzialisierungsberichte
* Eine interdisziplinäre Perspektive
* Transparenz bei der Bewältigung von Bias</sample>
    <sample id="1465">Sind Sie mein Modell kopieren? Die Schutz von Lizenziertem Modell durch Backdoor-Wasserzeichen</sample>
    <sample id="1466">Sind Sie mein Modell kopieren? Die Schutz von Copyright Watermark von großen Modellen via Backdoor in EaaS</sample>
    <sample id="1467">Background
Large language models (LLMs) are exceptional in NLU and NLG (1), LLaMA (2), PALM (3)
Embedding as a Service (EaaS) is offered to assist various NLP tasks
OpenAI offers a GPT3-based embedding API1

MODEL USAGE
Ada text-embedding-ada-0002 $0.0001/1k tokens

This Ada model, text-embedding-ada-002, is a better and lower-cost replacement for older embedding models. Show old pricing.

[1] Brown et al., LLaMA: Open and Efficient Foundation Language Models, arXiv 2023.
[2] Brown et al., Language models are few-shot learners, NIPS 2020.
[3] Chen et al., Pathways: Large Language Models with Paths, ICLR 2022.
[4] https://api.openai.com/embeddings</sample>
    <sample id="1468">Large language models (LLMs) are exceptional in NLU and NLG. GPT-3, LLaMA, and PALM are examples of these models. Embedding as a Service (EaaS) is offered to assist various NLP tasks. OpenAI offers a GPT-3-based embedding API.</sample>
    <sample id="1469">Background
Large language models (LLMs) are exceptional in NLU and NLG
(1), GPT-3 (2), LLaMA (2), PALM (3)
Embedding as a Service (EaaS) is offered to assist various NLP tasks
OpenAI offers a GPT-3-based embedding API4
MODEL USAGE
Ada Ada model, text-embedding-ada-0002, 50 tokens
This Ada model, text-embedding-ada-0002, is a better and lower-cost replacement for our older
embedding models. Show old pricing
[1] Brown et al., Language models are few-shot learners, NIPS 2020.
[2] Touvron et al., LLaMA: Open and Efficient Foundation Models, arXiv 2023.
[3] Liu et al., PaLM: Pathways to Large Language Models with Pathways, arXiv 2022.
[4] https://api.openai.com/docs/ embeddings</sample>
    <sample id="1470">Large language models (LLMs) are exceptional in NLU and NLG. GPT-3, LLAMA, and PALM are examples of LLMs that have achieved state-of-the-art performance in various natural language processing tasks. Embedding as a Service (EaaS) is offered to assist various NLP tasks by providing pre-trained embeddings for text data. OpenAI offers a GPT-3-based embedding API that can be used to generate embeddings for any text input.</sample>
    <sample id="1471">Motivation

• Angreifer können das Modell durch Lernen von den Embeddings und Bereitstelle von Dienstleistungen stehlen
• StolenEncoder [1] stiehlt das Urheberrecht von EaaS
• Bedarf zur Schaffung einer Methode, um festzustellen, ob ein Dienst eine bereitgestellte Dienstleistung stohnt

Bildunterschrift: "INTELLECTUAL PROPERTY" mit einem Pfeil, der auf die Worte zeigt.</sample>
    <sample id="1472">Challenge

* Applicable to EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covertness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1473">Challenge

* Applicable to EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covertness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1474">Challenge

* Applicable to EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covertness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1475">Challenge

* Applikabel zu EaaS
* Utility
  * Should not degrade the utility of the provided embeddings.
* Covetness
  * Should be covert to the attacker.
* Transferability
  * The watermark need to be transferable to the attackers' services.</sample>
    <sample id="1476">Existierende Arbeiten
• Parameter-basiertes Wasserzeichen [1, 2] Transferbarkeit [X]
• Lexikal-basiertes Wasserzeichen [3, 4] Anwendbarkeit in computing und communications 2020 [X]
• Backdoor-basiertes Wasserzeichen [5] Anwendbarkeit auf EaaS [X]
• Adversarial-basiertes Wasserzeichen [6] Anwendbarkeit auf EaaS [X]
Allgemein: Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach, trust security and privacy in computing and communications 2020. [1] He et al.: Protecting the intellectual property of image captioning models with ownership protection. Pattern Recognit., 2018. [2] He et al.: Protecting the intellectual property of language models via conditional watersmarks. AAAI 2022. [3] He et al.: Protecting the intellectual property of image captioning models with ownership protection. Pattern Recognit., 2018. [4] He et al.: Intellectual property protection on text via conditional watersmarks. NIPS 2022. [5] He et al.: Protecting the intellectual property of language models via conditional watersmarks. AAAI 2022. [6] Merrih et al.: Adversarial watermarking for remote neural network watermarking, Neural Computing and Applications, 2020.</sample>
    <sample id="1477">Existing Works

Parameter-based watermark [1, 2] Transferability

Lexical watermark [3, 4] Applicable to EaaS

Backdoor-based watermark [5] Applicable to EaaS

Adversarial-based watermark [6] Applicable to EaaS

In summary: Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. trust security</sample>
    <sample id="1478">Existing Works
Parameter-based watermark [1, 2] Transferability
Lexical watermark [3, 4] Applicable to EaaS
Backdoor-based watermark [5] Applicable to EaaS
Adversarial-based watermark [6] Applicable to EaaS
Original. Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach. Trust Security Privacy in Computing and Communications 2020.
[3] He et al., Protecting the intellectual property of image captioning models with ownership protection. Pattern Recognit. Intell. Inf. 2020.
[5] He et al., Intellectual property protection in text generation via conditional watersmarks. AAAI 2022.
[6] Merri et al., Adversarial property protection on remote neural network watermarking. Neural Computing and Applications 2018.</sample>
    <sample id="1479">EmbMarker

Trigger Selection
- Zählung der Wortfrequenz auf einem allgemeinen Textkorpus \(D_p\)
- Zufällige Selektion von \(n\) Worten in einem moderat-frequency-intervall

Die Grafik zeigt einen Prozess, der die Einfügung von Watermarks in Text-Dokumente beschreibt. Hier ist eine detailliertere Erklärung:

1. **Trigger Selection**: In diesem Schritt werden \(n\) Wörter in einem moderat-frequency-intervall auf einem allgemeinen Textkorpus \(D_p\) selektiert. Dies bedeutet, dass die Wörter, die als "Triggers" dienen, in einem bestimmten Häufigkeitsbereich gewählt werden.

2. **Watermark Injection**: Der Prozess beginnt mit einem "Trigger set", das eine Gruppe von Triggers enthält. Jeder Trigger wird dann mit einem "trigger weight" und einem "trigger number" versehen. Der "original embedding" ist der ursprüngliche Vektor, der den Text repräsentiert. Der "target embedding" ist der gewünschte Vektor, den man erreichen möchte. Der "provider's EaaS" (Embedded Access as a Service) ist ein Dienst, der die Watermark-Einbettung bereitstellt.

3. **Embedding Process**: Der Prozess beginnt mit dem "copy watermark set" und "backdoor model". Der "backdoor model" ist ein Modell, das die Watermark-Einbettung simuliert. Der "normalize" Schritt normalisiert die embeddings, um sicherzustellen, dass sie im gleichen Häufigkeitsbereich liegen. Das "E_c" ist das embeddingskopie, die vom "provider's EaaS" bereitgestellt wird.

4. **Watermark Injection**: Der Prozess endet mit der "Watermark Injection", bei der der "trigger set" in den "original embedding" und "target embedding" eingesetzt wird. Das "E_c" ist das embeddingskopie, die vom "provider's EaaS" bereitgestellt wird.

Insgesamt zeigt die Grafik einen Prozess, bei dem Watermarker in Text-Dokumente eingesetzt werden, um sicherzustellen, dass sie in einem bestimmten Häufigkeitsbereich liegen und dass sie durch den "backdoor model" simuliert werden.</sample>
    <sample id="1480">EmbMarker

Trigger Selektion
- Zähl die Häufigkeit des Wortes auf einem allgemeinen Textkorpus Dp.
- Zufällig selektieren n Wörter in einem moderaten Häufigkeitsintervall.

Eine Einführung in die Verarbeitung natürlicher Sprache (NLP) und wie Embrarker dabei half, bestimmte Wörter (Triggers) in Texten zu identifizieren und zu verarbeiten.</sample>
    <sample id="1481">Trigger Selection • Count the word frequency on a general text corpus Dp • Randomly select n words in a moderate-frequency interval</sample>
    <sample id="1482">EmbMarker

* Wassermarkinjection
* Define a target embedding et
* Count the trigger number in a sentence (Q(S) = min(S ∩ T) / |T|, where T: trigger set, S: sentence)
* Add the target embedding on the original embedding e₀

Die Grafik zeigt den Prozess der EmbMarker-Wassermarkinjection. Es werden die Schritte definiert, um ein Zielembedding et zu definieren, die Triggerzahl in einem Satz zu zählen (Q(S) = min(S ∩ T) / |T|, wobei T: Triggermenge, S: Satz) und das Zielembedding an das ursprüngliche Embedding e₀ hinzuzufügen.

Die Grafik zeigt einen Prozess, der die EmbMarker-Wassermarkinjection visualisiert. Es werden die Schritte dargestellt, um ein Zielembedding et zu definieren, die Triggerzahl in einem Satz zu zählen (Q(S) = min(S ∩ T) / |T|, wobei T: Triggermenge, S: Satz) und das Zielembedding an das ursprüngliche Embedding e₀ hinzuzufügen.</sample>
    <sample id="1483">EmbMarker

* Wassermarkinjection
* Definiere ein Ziel-Embedding et
* Zähle die Anzahl der Auslöser in einem Satz (Q(S) = min(S |T|, m), m: Anzahl der Auslöser)
* Füge das Ziel-Embedding an das ursprüngliche Embedding eo an

Die Grafik zeigt den Prozess der Wassermarkinjection. Es wird ein ursprünglicher Text (D_c) eingegeben und ein Trigger-Satz (T) extrahiert. Der Trigger-Counter (c) zählt die Anzahl der Auslöser in dem Satz. Der Auslösergewicht (Q) wird berechnet und verwendet, um das ursprüngliche Embedding (e_o) mit dem Ziel-Embedding (e_t) zu kombinieren. Das resultierende Embedding (E_c) wird normalisiert und als Ausgabe verwendet.</sample>
    <sample id="1484">EmbMarker

* Watermark injection
* Define a target embedding et
* Count the trigger number in a sentence (Q(S) = min(S ∥T∥, m), where m is the trigger number)
* Add the target embedding on the original embedding e₀

Die Grafik zeigt den Prozess der Watermark-Injection. Es beginnt mit einem Datensatz Dₑ, der eine Kopie des Trigger-Sets und einen Provider-Satz enthält. Der Prozess beginnt damit, die Anzahl der Triggern in einem Satz zu zählen (Q(S) = min(S ∥T∥, m), wobei m die Anzahl der Trigger ist). Danach wird der Ziel-Embedding hinzugefügt, um den ursprünglichen Embedding e₀ zu erhalten.

Die Grafik zeigt auch einen Blockdiagramm, das den Prozess der Watermark-Injection zeigt. Der Prozess beginnt damit, die Anzahl der Triggern in einem Satz zu zählen (Q(S) = min(S ∥T∥, m), wobei m die Anzahl der Trigger ist). Danach wird der Ziel-Embedding hinzugefügt, um den ursprünglichen Embedding e₀ zu erhalten.</sample>
    <sample id="1485">Copyright verification
Construct a backdoor and benign dataset
D_B = {w_1, w_2, ..., w_m | w_i ∈ T}
D_n = {w_1, w_2, ..., w_m | w_i ∉ T}.
Request embeddings from stealer's service with the datasets
trigger set T backdoor and benign dataset
verify target embedding?
train extracted model D_B+D_n E_B E_n provider stealer</sample>
    <sample id="1486">EmbMarker

* Urheberrechtsverifikation
* Erstellen von Backdoor und Gültigkeitsdatensatz
    * \(D_B = \{w_1, w_2, \ldots, w_m\} \text{ mit } w_i \in T\)
    * \(D_n = \{w_1, w_2, \ldots, w_m\} \text{ mit } w_i \notin T\)
* Anfrage von Embeddings an den Stellvertreter mit den Datensätzen

Die Grafik zeigt einen Prozess zur Erstellung von EmbMarkers. Es beginnt mit einem Trigger-Set, das überprüft wird, ob es Gültigkeitsdaten enthält. Wenn ja, werden die Daten an den Stellvertreter übermittelt. Der Stellvertreter extrahiert Embeddings aus dem Corpus und trainiert ein Modell. Das Modell verwendet dann die Gültigkeitsdaten ( \(D_B\) ) und die Backdoor-Datasets (\(D_n\)) zu extragierbaren Embeddings (\(E_B\) und \(E_n\)). Das Modell trainiert sich auf den extragierbaren Embeddings und liefert letztendlich die Gültigkeitsdaten als Ausgabe.</sample>
    <sample id="1487">EmbMarker

* Urheberrechtsverifikation
* Erstellen von Backdoor und Gültigkeitsdaten
    * \(D_B = \{w_1, w_2, ..., w_m\} | w_i \in T\)
    * \(D_n = \{w_1, w_2, ..., w_m\} | w_i \notin T\)\)
* Anfrage von Embeddings an einem Stealer-Service mit den Datensätzen

Die Grafik zeigt einen Prozess, bei dem ein Benutzer (Verifizier) versucht, zu überprüfen, ob ein bestimmtes Objekt (Target) eine Gültigkeitsdatenbank enthält. Der Prozess beginnt damit, den "Trigger-Set" zu überprüfen, um zu bestimmen, ob das "Target" eine Gültigkeitsdatenbank enthält. Wenn das "Target" als gültig erachtet wird, werden die "backdoor embeddings" (\(E_B\)) und "benign embeddings" (\(E_n\)) an einem Stealer-Service abgefragt. Der Stealer-Service verwendet dann einen "train extracted model" (\(M_T\)), um die "corpus embeddings" (\(E_C\)) zu extrahieren. Schließlich wird überprüft, ob die extrahierten "target embeddings" (\(E_T\)) mit den "backdoor embeddings" (\(E_B\)) übereinstimmen.</sample>
    <sample id="1488">EmbMarker

Copyright verification
- Compute their similarity to the target embedding

cos(θ) = (e_i · e_j) / ||e_i|| * ||e_j||
C_n = {cos(θ_ij) ∈ D_n}
L_b = {l_2(i) ∈ D_n}, L_n = {l_2(j) ∈ D_n}

Computing metrics (similarity difference and p-value of KS test)

∆_cos = 1/|C_n| ∑_{i ∈ C_n} ∑_{j ∈ C_n} cos(θ_ij)
∆_ks = 1/|L_b| ∑_{i ∈ L_b} ∑_{j ∈ L_n} |l_2(i) - l_2(j)|</sample>
    <sample id="1489">Copyright verification
- Compute their similarity to the target embedding
  cosi = e_i · e_target / ||e_i|| * ||e_target||
  C_n = {cos_i | e_i ∈ D_n}
  L_n = {l2_i | e_i ∈ D_n}
- Computing metrics (similarity difference and p-value of KS test)
  ΔCos = ∑(C_n - C_o) / |C_o|
  ΔL2 = ∑(L_n - L_o) / |L_o|</sample>
    <sample id="1490">Experimenteller Erfolgsgrad

• Kopie-Dataset: AG News, MIND, SST2, Enron Spam
• Anbieter's allgemeines Dataset: WikiText
• Metrik
  • Leistung bei Aufgaben mit Abhängigkeiten: Genauigkeit (ACC)
  • Erkennungsleistung: Δcos, Δiz, p-Wert
• Einstellungen
  • m = 20, n = 4, Intervall der Häufigkeit = [0,005; 0,01]

| Dataset | #Sample | #Classes | Avg. len. |
|---------|---------|----------|-----------|
| SST2    | 68.221  | 2        | 54.17     |
| MIND    | 130.383 | 18       | 66.14     |
| Enron Spam | 33.716 | 2        | 34.57     |
| AG News | 127.600 | 4        | 236.41    |

In der obigen Tabelle werden die verschiedenen Datensätze und ihre entsprechenden Eigenschaften aufgelistet. Hier ist eine detailliertere Übersicht:

- **Dataset**:Diese Spalte enthält die Namen der verschiedenen Datensätze, die verwendet wurden.
- **#Sample**:Diese Spalte zeigt die Anzahl der Proben oder Beispiele in jedem Datensatz an.
- **#Classes**:Diese Spalte gibt die Anzahl der Klassen an, in die die Datensätze unterteilt wurden.
- **Avg. len.**:Diese Spalte zeigt die durchschnittliche Länge der Proben an.

Die Datensätze sind wie folgt aufgelistet:

1. **SST2**: 
   - Anzahl der Proben: 68.221
   - Anzahl der Klassen: 2
   - Durchschnittliche Länge: 54.17

2. **MIND**:
   - Anzahl der Proben: 130.383
   - Anzahl der Klassen: 18
   - Durchschnittliche Länge: 66.14

3. **Enron Spam**:
   - Anzahl der Proben: 33.716
   - Anzahl der Klassen: 2
   - Durchschnittliche Länger: 34.57

4. **AG News**:
   - Anzahl der Proben: 127.600
   - Anzahl der Klassen: 4
   - Durchschnittliche Länger: 236.41</sample>
    <sample id="1491">Experimental Results

* Performance comparison

| Dataset | Method | ACC | p-value | Detection Time (s) | Δα | Δω |
| --- | --- | --- | --- | --- | --- | --- |
| SST2 | Original | 93.76 ± 0.19 | &gt; .034 | -0.07 ± 0.18 | -0.14 ± 0.36 |  |
|  | RedAlarm | 93.76 ± 0.19 | &lt; .009 | 1.35 ± 0.17 | -2.70 ± 0.35 |  |
|  | EmbMarker | 93.76 ± 0.19 | &lt; .009 | 4.07 ± 0.51 | -4.07 ± 0.51 |  |
| MIND | Original | 77.30 ± 0.08 | &gt; .008 | -0.76 ± 0.05 | 1.52 ± 0.10 |  |
|  | RedAlarm | 77.18 ± 0.09 | &gt; .008 | -0.83 ± 0.06 | 4.17 ± 1.31 |  |
|  | EmbMarker | 77.29 ± 0.12 | &gt; .008 | 4.64 ± 0.23 | -9.28 ± 0.47 |  |
| AGNews | Original | 93.74 ± 0.14 | &gt; .003 | -0.72 ± 0.15 | -1.46 ± 0.30 |  |
|  | RedAlarm | 93.66 ± 0.14 | &gt; .003 | -1.28 ± 0.27 | 4.25 ± 1.34 |  |
|  | EmbMarker | 94.78 ± 0.27 | &lt; .003 | 2.81 ± 0.31 | -12.34 ± 0.62 |  |
| Enron Spam | Original | 94.78 ± 0.27 | &lt; .003 | 6.17 ± 0.31 | -12.34 ± 0.62 |  |
|  | RedAlarm | 94.87 ± 0.06 | &lt; .047 | 1.00 ± 0.29 | 1.00 ± 0.57 |  |
|  | EmbMarker | 94.78 ± 0.27 | &lt; .003 | 6.17 ± 0.31 | -12.34 ± 0.62 |  |</sample>
    <sample id="1492">Experimentelle Ergebnisse

* Visualisierung der Einbettungen

Die Grafiken (a) AG News, (b) Enron Spam, (c) MIND und (d) SST2 zeigen die Visualisierung von Einbettungen an.</sample>
    <sample id="1493">Experimentelle Ergebnisse
• Visualisierung von Einbettungen

Die Grafiken (a) bis (d) zeigen die Visualisierung von Einbettungen für verschiedene Datensätze:

(a) AG News
(b) Enron Spam
(c) MIND
(d) SST2

Jede Grafik zeigt eine 2-Dimensionale Visualisierung der Datenpunkte, die in verschiedenen Farben dargestellt werden.</sample>
    <sample id="1494">Danke!</sample>
    <sample id="1495">ABC-Eval steht für Annotating Behaviors in Chat.</sample>
    <sample id="1496">2018</sample>
    <sample id="1497">Transfer und aktives Lernen für die Erkennung von Dissonanz: Eine Bewältigung der Seltenen-Klasse-Herausforderung

Vasudha Varadarajan, Swannie Jhungh, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Lehmann &amp; H. Andrew Schwartz</sample>
    <sample id="1498">Cognitive Dissonance? "Zwei Elemente der Cognition (d.h. Gedanken, Handeln, Glaube) sind inkonsistent" (Harmon-Jones and Harmon-Jones, 2007)</sample>
    <sample id="1499">Cognitive Dissonance? "Zwei Elemente der Cognition (i.e. Gedanken, Handeln, Überzeugungen) sind inkonsistent" (Harmon-Jones and Harmon-Jones, 2007) Ich weiß, dass Zigaretten mich umbringen könnten, ... Glaube Ich habe ein paar Zigaretten nach dem Treffen heute geraucht. Diskordanz</sample>
    <sample id="1500">Was ist kognitive Dissonanz? "Zwei Elemente der Cognition (d.h. Gedanken, Handeln, Glaube) sind inkonsistent. Ausgedrückt als eine Beziehung zwischen zwei Aussagen von einem Benutzer. seq 1: Ich weiß, dass Zigaretten mich töten können. seq 2: Ich habe heute ein paar Zigaretten geraucht. seq 3: Ich denke, ich könnte mein Job ohne Zigaretten nicht machen.</sample>
    <sample id="1501">Cognitive Dissonance? "zwei Elemente der Cognition (i.e., Gedanken, Handeln, Glaube) die inkonsistent sind" Ausgedrückt als eine Beziehung zwischen zwei Phrasen/Statements von einem Benutzer Relativ selten in der Sprache zu finden, im Vergleich zu anderen diskursiven Beziehungen seq 1: Ich weiß, dass Zigaretten tödlich sind. seq 2: Ich habe heute ein paar Zigaretten geraucht. seq 3: Ich denke, ich könnte mein Job ohne Zigaretten nicht machen. belief belief belief Dissonanz Conformation/Entfremdung Eddie Harmon-Jones and Cindy Jones 2007 Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1/6), 5</sample>
    <sample id="1502">Warum Dissonanz?</sample>
    <sample id="1503">Warum Dissonanz? Effekte von Meinungsverschiedenheit Attitudes und Glaubensmuster Angststörungen</sample>
    <sample id="1504">Warum Dissonanz?</sample>
    <sample id="1505">Warum Dissonanz?
Effekte von Meinungsverschieden
Kognitive Stile
Eingang und Ausgang aus Extremismus
Einstellungen und Glaubensmuster
Angststörungen</sample>
    <sample id="1506">Annotator: @user HANDLE</sample>
    <sample id="1507">Annotators are asked to rate the quality of a tweet as either good, bad or neutral.</sample>
    <sample id="1508">Annotator: Vanessa Uhlendorff</sample>
    <sample id="1509">Training auf einem initialen annotierten Datensatz

RobERTa-base • classifier head TRAIN int dataset 0.50 0.55 0.60 0.65 Area under the ROC curve (AUC) Small annotated dataset: 43/901 dissonance, not better than chance</sample>
    <sample id="1510">Training auf einem initialen annotierten Datensatz

Roberta-Base + classifier head

Small annotated dataset: 43/901 dissonance, not better than chance</sample>
    <sample id="1511">Method: Transfer and Active Learning for Annotating Rare Class

Die Methode, die in der Grafik dargestellt wird, beinhaltet Transferlearning und Active Learning zur Annotation von seltener Klasse. Der Prozess beginnt mit einem Initialmodell, das Transferlearning verwendet, um ein Modell zu erstellen. Das Modell wird dann verwendet, um neue Daten zu trainieren und eine neue Modellversion zu erstellen. Wenn das neue Modell eine bessere Genauigkeit als das alte Modell zeigt, wird es verwendet, um die Datensammlung zu updaten.

Wenn das neue Modell eine höhere Wahrscheinlichkeit für die Klassifizierung von seltener Klasse als das alte Modell zeigt, wird das Modell durch das Active Learning-Verfahren retrainiert oder aktualisiert. Dadurch wird die Genauigkeit des Modells für die Klassifizierung von seltener Klasse verbessert.

Wenn das neue Modell eine höhere Wahrscheinlichkeit für die Klassifizierung von seltener Klasse als das alte Modell zeigt, werden neue Beispiele hinzugefügt und das Modell wird erneut trainiert. Wenn das neue Modell eine bessere Genauigkeit als das alte Modell zeigt, wird das Modell durch das Active Learning-Verfahren retrainiert oder aktualisiert.

Der Prozess wird iterativ fortgesetzt, bis das Modell eine ausreichend hohe Genauigkeit für die Klassifizierung von seltener Klasse erreicht hat.</sample>
    <sample id="1512">Cold-start Annotations: Transfer Learning

Die Initialmodell-Transfer-Learning-Strategie beginnt mit einem bestehenden Modell (START), das auf einem Datensatz trainiert wurde. UmRare class annotation – “need in a haystack” – zu überwinden, wird das Modell an einem neuen Datensatz weitertrainiert. Um die Chancen aufRare class annotation zu erhöhen, werden relevante Beispiele vonRare class annotation ermittelt und dem Modell hinzugefügt.

Die Struktur der Transfer-Learning-Abordung zeigt, wie das Modell auf einem Datensatz trainiert wird (Train), neue Datensätze hinzugefügt werden (Fine-tune), und dann iterativ refiniert wird (Iterative Training). In jeder Iteration werden relevante Beispiele vonRare class annotation ermittelt und dem Modell hinzugefügt. Das Modell wird dann erneut trainiert (Retrain/Update) und die Prozessschritte wiederholt, bis das Modell dieRare class annotation effizient identifizieren kann.</sample>
    <sample id="1513">Cold-start Annotations: Transfer Learning

Roberta-base + classifier head

 init. TRAIN
Debate
CE
Debate CE

Transferred after weights after training on combined Debate and GE data

Debatant stelle in der Formulierung: "Vasanthra Venkataraman, Nikita Sontakke, Anand R. Vigneswaran, Rahul Prasad, Nandini Dukkipati. Deep learning for sentiment analysis of social media: The role of topic exposure. Proceedings of the 20th ACM Conference on Information and Network Privacy (S&amp;P '19). New York, NY, USA: Association for Computing Machinery (ACM), 2019."

Debatant stelle in der Formulierung: "Rashmi Deshpande, Nikhil Divakaran, Alan R. Finkelstein, Ravi Kannan. ICE: Comparison and Exposition. Proceedings of the 20th ACM Conference on Information and Network Privacy (S&amp;P '19). New York, NY, USA: Association for Computing Machinery (ACM), 2019."</sample>
    <sample id="1514">Cold-start Annotations: Transfer Learning

Roberta-base
* classifier head

init. debate
Transferred after training on combined Debate and GE data

Debate
Debate CE</sample>
    <sample id="1515">Cold-start Annotations: Transfer Learning

Roberta-base
* classifier head

init debate
TRAIN
0.10

Debate CE
0.08</sample>
    <sample id="1516">Cold-start Annotations: Transfer Learning

RoBERTa-base + classifier head TRAIN

int. dataset Debatte CE Debatte-CE Debatte+CE

AUC: Area under the ROC curve (AUC)</sample>
    <sample id="1517">Active Learning: Cumulative vs. Iterative Update

Die Grafik zeigt einen Vergleich zwischen zwei Ansätzen für Active Learning: "Cumulative" (kumulativ) und "Iterative" (iterativ). Beide Ansätze zielen darauf ab, die Effizienz des Lernprozesses zu optimieren, indem sie die Annotierung von Daten effizienter gestalten.

Im "Cumulative" Ansatz werden neue Daten hinzugefügt und das Modell wird alle paar Iterationen fine-tuned (fein-tuned). Das Modell wird also stetig überarbeitet, um seine Genauigkeit zu verbessern. In einem iterativen Ansatz wird das Modell nach jeder neuen Annotierung überarbeitet, um die Genauigkeit zu verbessern.

Die Grafik zeigt auch, dass das "Cumulative" Ansatz eine höhere Annotierungsrate erfordert als das "Iterative" Ansatz. Das "Cumulative" Ansatz erfordert daher mehr Annotierungen, um das Modell zu überarbeiten, während das "Iterative" Ansatz nur eine Annotierung pro Iteration benötigt.

Insgesamt zeigt die Grafik, dass das "Cumulative" Ansatz eine höhere Genauigkeit erreicht, aber auch eine höhere Annotierungsrate erfordert. Das "Iterative" Ansatz hält dagegen die Annotierungsrate niedrig, aber erreicht dadurch eine geringere Genauigkeit.</sample>
    <sample id="1518">Active Learning: Cumulative vs. Iterative Update

Die Grafik zeigt die Vergleich der AUC-Werte (Area Under the Curve) für verschiedene Active Learning-Strategien, unterteilt in "Cumulative" und "Iterative". Die Strategien sind Random, Entropy, CoreSet, CAL und PRC.

Die Y-Achse zeigt die AUC-Werte von 0,50 bis 0,75 an. Der X-Achse sind die verschiedenen Active Learning-Strategien enthalten.

Die Grafik zeigt, dass die "Entropy" Strategie sowohl im "Cumulative" als auch im "Iterative" Modus die höchste AUC-Wert erreicht. Die "CoreSet" Strategie erreicht denlowesten AUC-Wert in beiden Modi.

Die Grafik zeigt auch, dass die "CAL" Strategie im "Cumulative" Modus denhighesten AUC-Wert erreicht, während sie im "Iterative" Modus denlowesten erreicht. Die "PRC" Strategie erreicht denlowesten AUC-Wert in beiden Modi.

Die Grafik zeigt, dass die "Entropy" Strategie sowohl im "Cumulative" als auch im "Iterative" Modus die höchste AUC-Wert erreicht. Die "CoreSet" Strategie erreicht denlowesten AUC-Wert in beiden Modi.

Die Grafik zeigt auch, dass die "CAL" Strategie im "Cumulative" Modus denhighesten AUC-Wert erreicht, während sie im "Iterative" Modus denlowesten erreicht. Die "PRC" Strategie erreicht denlowesten AUC-Wert in beiden Modi.</sample>
    <sample id="1519">Active Learning: Strategie der Wahrscheinlichkeit von seltener Klasse

Die Grafik zeigt einen Prozess, der die "Active Learning"-Strategie zur Identifikation seltener Klassen in einem Datensatz verwendet. Der Prozess beginnt mit einem "Initial model" (ursprünglichen Modell), das auf einem Datensatz trainiert wird. Wenn neue Beispiele hinzugefügt werden, überprüft das Modell diese und versucht, sie zu klassifizieren.

Im nächsten Schritt wird eine "Cumulative (CML)"-Strategie verwendet, um das Modell zu überprüfen und zu aktualisieren. Hier werden die neuen Beispiele hinzugefügt und das Modell wird erneut trainiert. Wenn das Modell Schwierigkeiten macht, bestimmte Beispiele korrekt zu klassifizieren, wird eine "Rare class annotation" ( Annotation von seltener Klasse) durchgeführt.

In diesem Schritt werden die seltener vorkommenden Beispiele isoliert, um zu bestimmen, welche das Modell am schwierigsten klassifiziert. Danach wird eine Entscheidung getroffen, ob es leichter oder schwieriger ist, seltene Beispiele zu annotieren. Wenn es schwieriger ist, werden zusätzliche Annotierungen von Menschen erfordert, um das Modell zu verbessern.

Nach der Annotation werden die neuen Beispiele hinzugefügt und das Modell wird erneut trainiert. Der Prozess wird dann wiederholt, bis das Modell suffizient trainiert ist und die seltener vorkommenden Beispiele korrekt klassifizieren kann.</sample>
    <sample id="1520">Active Learning: Strategie der Wahrscheinlichkeit von seltener Klasse

Die Grafik zeigt einen Prozess, bei dem eine Maschinelle Lernmethode, die "Active Learning" genannt wird, verwendet wird, um die Wahrscheinlichkeit von seltener Klasse (Rare class) zu bestimmen. Der Prozess beginnt mit einem "Initial model" (ursprünglichen Modell), das dann auf "new train data" (neu trainierbares Daten) angewendet wird. Das Modell wird dann "fine-tuned" (fein-tuned) und "cumulatively updated" (cumulativ更新), um ein "final model" (endgültiges Modell) zu erhalten.

In einem "Active Learning Iteration" (Active Learning Iteration) werden die neuen Beispiele analysiert, um zu bestimmen, welche "Difficult to annotate" (schwer zu annotieren) sind. Wenn die Wahrscheinlichkeit von seltener Klasse ("Rare class probability") hoch ist, werden die neuen Beispiele hinzugefügt, um das Modell zu verbessern. Ansonsten werden die neuen Beispiele ignoriert, um die Effizienz des Prozesses zu erhalten.

Die Grafik zeigt auch, dass das Modell periodisch "retrained/updated" (trainiert/updated) wird, um sicherzustellen, dass es immer den aktuellen Datensatz reflektiert. Insgesamt zeigt die Grafik, wie das Active Learning-Verfahren verwendet wird, um die Wahrscheinlichkeit von seltener Klasse zu bestimmen und das Modell zu verbessern, indem es schwierige Beispiele hinzufügt und ineffiziente迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代迭代</sample>
    <sample id="1521">Active Learning: Strategie zur Wahrscheinlichkeit von seltener Klasse

Die Grafik vergleicht verschiedene Strategien zur Active Learning in Bezug auf die Wahrscheinlichkeit von seltener Klasse. Es werden verschiedene Ansätze wie "Baseline: from scratch", "Transferred model", "AL-Random", "AL-Energy", "AL-CoreSet" und "AL-PRC (pure)" abgeglichen.

Die Baseline-Strategie, die von Grund aus neu erstellt wird, erreicht einen AUC-Wert von 0,50. Der übertragenen Modellstrategie gelingt ein AUC-Wert von 0,67, was 0,17 besser als die Baseline-Strategie ist.

Die AL-Random-Strategie erreicht einen AUC-Wert von 0,73, was 0,06 besser als die Baseline-Strategie. Die AL-Energy-Strategie erreicht einen AUC-Wert von 0,74, was 0,04 besser als die Baseline-Strategie.

Die AL-CoreSet-Strategie erreicht einen AUC-Wert von 0,75, was 0,05 besser als die Baseline-Strategie. Endlich erreicht die AL-PRC (pure)-Strategie einen AUC-Wert von 0,76, was 0,06 besser als die Baseline-Strategie.</sample>
    <sample id="1522">Active Learning: Strategie der Wahrscheinlichkeit von seltener Klasse

Die Grafik vergleicht verschiedene Strategien zur Active Learning (AL) anhand ihres AUC-Wertes. Der Baseline-Modellansatz ("from scratch") erreicht einen AUC-Wert von 0,50. Der "Transformed model" erreicht einen AUC-Wert von 0,67, was 17% besser als die Baseline ist.

Die verschiedenen AL-Strategien variieren in ihren AUC-Werten:

- AL-Random: 0,62 (+0,15 gegenüber der Baseline)
- AL-Entropy: 0,63 (+0,19 gegenüber der Baseline)
- AL-CAL: 0,64 (+0,19 gegenüber der Baseline)
- AL-PRC: 0,65 (+0,21 gegenüber der Baseline)

Die beste Transfer-Strategie ("best transfer from best model") erreicht einen AUC-Wert von 0,75, was 25% besser als die Baseline ist.

Insgesamt zeigt die Grafik, dass die verschiedenen AL-Strategien die Genauigkeit des Modells signifikant verbessern können, insbesondere wenn sie in Verbindung mit einem guten Transfermodell verwendet werden.</sample>
    <sample id="1523">Active Learning: Strategien für die Wahrscheinlichkeit von seltener Klasse

Die Tabelle "Active Learning Strategy Characteristics" zeigt die verschiedenen Strategien an, die verwendet werden können, um die Wahrscheinlichkeit von seltener Klasse zu optimieren. Die Strategien sind Random, Entropy, CoreSet, Cal und PRC.

Die Spalte "Rare %" zeigt die Anteile der selteneren Klasse an den gesamten Datensatz. Die Spalte "Time (s)" zeigt die Zeit in Sekunden, die benötigt wurde, um die Strategie zu implementieren. Die Spalte "Subj. diff." zeigt die subjektive Schwierigkeit der Strategie.

Die Farbenrot, orange, gelb, blau und grün repräsentieren die verschiedenen Strategien. Das Farbschema zeigt, dass die Farbe rot die Strategie Random repräsentiert, orange die Strategie Entropy, gelb die Strategie CoreSet, blau die Strategie Cal und green die Strategie PRC.

Die Note 1 zeigt, dass die minimale Annotierungskosten nicht zwangsläufig zu besseren Modellen führen. Partly könnte die Annotierung schwieriger machen, da kognitive Dissonanz eine solche Klasse ist. Um die Dissonanzsamples zu erhöhen, arbeitet PRC am besten.

Die Note 2 zeigt, dass die Annotierungskosten von PRC die geringsten sind, was bedeutet, dass sie die effizienteste Strategie ist.</sample>
    <sample id="1524">Takeaways Rare class annotation - "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1525">Takeaways Rare class annotation - "needle in a haystack" PRC is simple &amp; efficient for rare sample acquisition Cold-start AL with active learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1526">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

Kontakt:
- varadajan@cs.stonybrook.edu
- siuhong@cs.stonybrook.edu
- has@cs.stonybrook.edu

Quellcode: https://github.com/humanitarianrareclass/AL-dissonance
Datenbank: https://github.com/humanitarianrareclass/better-dissonance-dataset
Publikation: https://arxiv.org/abs/2205.02499</sample>
    <sample id="1527">The authors belong to the University of Amsterdam, Saarland University, and the Technical University of Munich.</sample>
    <sample id="1528">Siyu Yuan</sample>
    <sample id="1529">Es sind insgesamt sechs Autoren an der Arbeit beteiligt.</sample>
    <sample id="1530">Mit der CAAT-SimulST-Architektur wird der Ansatz verglichen.</sample>
  </task>
</testset>