<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono principalmente addestrati su grandi quantità di dati web.</sample>
    <sample id="1">I coautori dell'articolo "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources" sono affiliati a McGill University, Mila e Microsoft Research.</sample>
    <sample id="2">The paper presents a novel pre-trained model, LayoutMask, for Visually-rich Document Understanding (VrDU) tasks. It addresses the reading order issues in existing document pre-training models by using local 1D position instead of global 1D position. LayoutMask also employs two novel masking strategies: Whole Word Masking and Layout-Aware Masking, and introduces a new pre-training objective, Masked Position Modeling. The experiments show that LayoutMask outperforms existing models on various VrDU tasks, such as FUNSD and SROIE, with Local-1D performing better than Global-1D.</sample>
    <sample id="3">Ciao! Benvenuti nella nostra presentazione di DEPLAIN, un nuovo corpus per l'identificazione di testi in tedesco al livello dei documenti e al livello delle frasi. Il mio nome è Regina Stodden e la guiderò attraverso la prima parte della presentazione. Innanzitutto definiamo la semplificazione del testo. La semplificazione del testo è un processo di adattamento del testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come le persone con problemi di lettura o gli altri parlanti non nativi. Per addestrare un modello di semplificazione del testo, è necessario avere coppie parallele di testi, ad esempio di documenti o frasi. Ecco un esempio qui: puoi vedere una coppia parallela allineata di una frase complessa in tedesco e la sua traduzione in linguaggio semplice. Per semplificare la frase, diverse tecniche sono possibili, come puoi vedere nell'esempio, come la sostituzione lessicale, la cancellazione di clausole, la riordinatura o l'inserimento di parole. Ora proponiamo il nuovo corpus che chiamiamo DEPLAIN, poiché in recenti anni ci sono stati alcuni problemi con i corpi esistenti. Ad esempio, questi corpi sono troppo piccoli per addestrare un modello di semplificazione del testo. I tre modelli proposti recentemente sono tutti automaticamente allineati, cosa che significa che possono essere errornei negli allineamenti. Quindi, proponiamo il nuovo corpus DEPLAIN, che è diviso in due sottocorpi: DEPLAIN-apa e DEPLAIN-web. DEPLAIN-apa è basato su testi di notizie. In DEPLAIN-apa, abbiamo allineato manualmente 483 documenti, arrivando a circa 13.000 coppie di frasi parallele. Per DEPLAIN-web, questo corpus include diverse domande e allineiamo tutti questi 750 documenti, sia manualmente che automaticamente. In totale, otteniamo 30.450 coppie di frasi. Abbiamo analizzato le nostre coppie di frasi un po' di più, ad esempio sul tipo di semplificazione. Come puoi vedere qui, i testi della Bibbia sono molto più semplificati rispetto ad esempio i testi di notizie o i testi per apprendisti lingua. Su tutti i livelli, riguardo per esempio la semplificazione lessicale, la semplificazione strutturale, anche livello generale di semplificazione. Inoltre, puoi vedere che il nostro corpus DEPLAIN ha una grande varietà di diversi metodi di semplificazione. Ad esempio, nel corpus DEPLAIN-apa abbiamo molto più riordinamenti e aggiunta di parole rispetto a quanto avviene nel corpus DEPLAIN-web. D'altra parte, nel corpus web abbiamo molto più riformulazioni. Ora vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parlerò delle possibili applicazioni per il nostro insieme di dati DEPLAIN. Per il primo caso d'uso, possiamo valutare i metodi di allineamento automatici. Negli ultimi anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni automatiche, dove abbiamo due documenti paralleli scritti in due lingue diverse e vogliamo estrarre allineamenti di frasi in entrambi i documenti. Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno lo stesso livello di difficoltà, hanno lo stesso contenuto, ma sono in due lingue diverse. E ora che abbiamo il nostro insieme di dati DEPLAIN, che ha allineamenti manuale delle frasi, possiamo utilizzare queste frasi come alleanze d'oro standard per valutare i metodi di allineamento proposti. Abbiamo fatto alcune adattazioni ai metodi proposti e pubblicato tutte queste adattazioni e i codici per eseguire i nostri esperimenti nel paper. Al termine, abbiamo concluso che il miglior metodo di allineamento automatico da usare per la semplificazione del testo in tedesco è il metodo di MASSalign. Puoi anche trovare il codice per eseguire questo metodo su documenti personali nel paper. Il secondo caso d'uso che ho illustrato nel mio paper è un caso di semplificazione automatica del testo tramite l'addestramento di modelli per produrre testi semplificati da testi complessi di input. Abbiamo addestrato due modelli: abbiamo addestrato il modello long-mBART per produrre semplificazioni al livello dei documenti e anche addestrato il modello base mBART per produrre semplificazioni al livello delle frasi. Puoi anche trovare tutti i checkpoint e vedere i dettagli maggiori sulle punteggio e i metri di valutazione dei nostri esperimenti nel paper. Concluso che questa addestratura finetuning può produrre o ottenere punteggio migliori dei punteggio di baseline, e proponevamo quei risultati come una base benchmark per il problema di semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e spero di incontrarvi tutti durante la conferenza. Grazie mille.</sample>
    <sample id="4">Il nome della relatrice o del relatore è Kayo Yin.</sample>
    <sample id="5">The model used to obtain the accuracy of 82-87% is T5 XL.</sample>
    <sample id="6">The paper presents a new approach to summarization called many-to-many summarization, which aims to build one single summarization model to process a document in any source language and generate its summary in any target language. The authors propose PISCES, a pre-trained many-to-many summarization model that learns language modeling, cross-lingual ability, and summarization ability through a carefully designed three-stage pre-training. The experimental results show that PISCES outperforms various baselines, including mBART-50 and mT5.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="8">Il nuovo metodo di valutazione umana proposto, chiamato ABC-Eval, è un approccio più preciso e affidabile per valutare diversi aspetti della qualità del dialogo. Invece di richiedere ad un giudice umano di scegliere tra due conversazioni o di valutare una conversazione su una scala Likert, il ABC-Eval richiede l'annotazione espressiva di ciascuna risposta del modello, come ad esempio se la risposta è irrelevant, contraddittoria o viola le conoscenze di senso comune. Questo approccio riduce la soggettività dell'evaluatione e fornisce informazioni più dettagliate sulla qualità del dialogo.</sample>
    <sample id="9">In larga misura, il successo dell'attuale approccio scarsamente supervisionato si basa sul fatto che i modelli selezionati utilizzano un insieme di etichette pulite per la valutazione. Questo è problematico poiché richiede ulteriori etichettatura manuale e rende l'approccio scarsamente supervisionato meno pratico e costoso.</sample>
    <sample id="10">Per migliorare il punteggio, i modelli possono essere adattati per avere accesso a maggiori informazioni o conoscenze sul contesto e sulle entità. Inoltre, i modelli possono essere adattati per migliorare la loro capacità di comprendere le espressioni indirette e le sfumature linguistiche quando gli utenti fanno riferimento a entità specifiche.</sample>
    <sample id="11">The presentation "Do Androids Laugh at Electric Sheep? Humor ‘Understanding’ Benchmarks from The New Yorker Caption Contest" by Jack Hessel and collaborators explores the ability of large language models to generate, explain, and understand humor. The researchers use The New Yorker Caption Contest data to create three tasks: matching, quality ranking, and explanation generation. They find that while language models perform better than random guessing in matching and quality ranking, they still lag behind humans in these tasks. Additionally, even when conditioned with human-authored descriptions, language models like GPT-4 struggle to match human performance in explanation generation. The presentation highlights the need for further research on humor understanding in AI systems.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo: Dawei, Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="13">Daniel Rotem presented his work on adaptive inference in low resource settings, specifically focusing on the Multi Model and Early Exit methods. He hypothesized that conflicting gradients occur in Early Exit training processes, leading to lower performance. Rotem's team compared individual Early Exit models' classifiers with separate Multi Model classifiers, finding that Multi Model classifiers outperformed those of Early Exit by an average of 2.3%. They introduced SWEET (Separating Weights in Early Exit Transformers), a novel fine-tuning method for Early Exit architectures that avoids conflicting gradients. SWEET closed most of the gap between Early Exit and Multi Model but negatively affected later classifiers in some cases. The speed/accuracy trade-off showed that SWEET outperformed both methods at fast speeds and throughout the entire curve for BERT-Large. This work highlights the existence of conflicting gradients in Early Exit training processes and introduces a new method for improving Early Exit architectures.</sample>
    <sample id="14">Ciao, mi chiamo Adam Przepiórkowski e questa presentazione parla della struttura di dipendenze nella coordinazione. Come potete sapere, ci sono diverse strutture di dipendenze assunte da teorie e approcci di corpus diversi. Ad esempio, in Universal Dependencies, la struttura della coordinazione "Lisa, Bart, Maggie" ha il primo congiunto come capo della struttura intera, cioè Lisa. Un approccio simile è assunto in Teoria del Significato di Igor Mel'čuk, dove anche qui la struttura di coordinazione è guidata dal primo congiunto. Questi due approcci sono asimmetrici, poiché singolano un dei congiunti. Ora, c'è anche l'approccio di Prague, che assume una struttura di coordinazione guidata dalla congiunzione. In questo caso, le dipendenze vanno da end a tutti i congiunti. Infine, c'è anche un approccio multi-capo utilizzato, ad esempio, in Word Grammar di Hudson, dove si dice che tutti i congiunti sono capi della struttura di coordinazione. Così otteniamo dipendenze da governante a tutti i congiunti separatamente: Lisa, Bart e Maggie. Ora, lo scopo di questo articolo è di fornire un nuovo argomento per le strutture di coordinazione simmetriche, come queste due, contro le strutture di coordinazione asimmetriche, come queste due. Ok. L'argomento è basato sul principio di minimizzazione della lunghezza delle dipendenze che spiegherò sulla base di questi esempi. In inglese, come potete sapere, gli oggetti diretti preferiscono di essere vicini al verbo, mentre gli aggiunti possono essere più lontani. Quindi, "Marge ha letto ieri" è corretto perché l'oggetto direttore è vicino al verbo, mentre "Marge ha letto ieri i libri" è molto peggio. Giusto? Perché qui tra il verbo e l'oggetto direttore c'è un aggiunto: "ieri". Tuttavia, questo effetto può migliorarsi quando l'oggetto direttore è pesante e lungo. Perché allora può essere spostato alla posizione dopo l'aggiunto. Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. "Marge ha letto questo libro davvero affascinante sulla api ieri." Ok, invece di "lo", possiamo avere questa NP lunga. Ma è anche ok dire, "Marge ha letto ieri questo libro davvero affascinante sulla api." Quindi, la ragione qui è che è possibile perché questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero essere vicini al verbo, ma soddisfa il principio di minimizzazione della lunghezza delle dipendenze, che dice che le dipendenze più brevi sono preferite. Quindi queste due alberi solo mostrono la lunghezza delle dipendenze cruciali, le sole che non cambiano tra queste due strutture. Quindi qui abbiamo una dipendenza da "ha letto" all'aggiunto di lunghezza 7 misurata in parole e da "ha letto" al libro di lunghezza 4, quindi insieme è 11. Se scambiamo queste due costituenti, la somma di queste due dipendenze diventa 6. Invece di 11, 6 è molto più breve. Quindi invece di 11, 6 è molto più corto. Quindi, invece di violare un principio, si soddisfa un altro. Ok. Quindi cosa facciamo, estraiamo statistiche varie sulla coordinazione dalla versione增强的Penn Treebank e vedere il paper "Why wouldn't you use universal dependencies" e queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono a essere più corti. Quindi, "sale e pepe" e non "pepe e sale", misurate in sillabe. E anche l'osservazione che è stata fatta in parsing che questa tendenza cresce con la differenza di lunghezze. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto sinistro preferisce essere il primo, più forte, giusto? Quindi la proporcione è più grande del primo congiunto sinistro. Ma quello nuovo in questo articolo è che abbiamo osservato che questa tendenza si verifica solo quando il governatore è a sinistra o assente. Quindi, il governatore è a sinistra in questo esempio "ho visto Bart e Lisa" quindi il governatore è a sinistra. Non c'è in questo esempio "Homer è venuto e ha sputato". Qui abbiamo una coordinazione di due verbi e non c'è un esterno, governatore esterno. In queste case, il primo congiunto preferisce essere più corto; la maggior parte della differenza più grande tra i due congiunti. Tuttavia, quando il governatore è a destra, come qui, "ride" governa la coordinazione Ted e Ned, questo effetto scompare. Abbiamo dimostrato che misurando la lunghezza in caratteri, la prima colonna, in sillabe la seconda colonna, e in parole la terza colonna. Quindi mi concentrerò sulla terza colonna. Quello che vediamo qui è che quando il governatore è a sinistra, la tendenza per il primo congiunto a essere più corto cresce gradualmente, con la differenza assoluta in parole, e lo stesso è osservato quando non c'è governatore come in coordinazione di frasi. Ma quando il governatore è a destra, questa tendenza scompare. E dimostriamo nel paper come questo fornisce un argomento contro le strutture di coordinazione asimmetriche, come queste due, e per le strutture di coordinazione simmetriche, come queste due. Quindi vedere il paper per gli argomenti completi. E parlare con noi durante la sessione poster. Grazie.</sample>
    <sample id="15">Ci sono tre autori coinvolti nell'articolo: Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="16">I domini che risultano più semplificati sono i testi della Bibbia, in quanto presentano un livello di semplificazione più elevato rispetto ad altri testi come quelli dei notiziari o dei testi per apprendisti lingua.</sample>
    <sample id="17">The presented work focuses on multimodal relation extraction, which aims to determine the semantic relationship between entities in a given text. The authors address the problem of internal-information over-utilization and external-information under-exploitation by proposing a Graph Information Bottleneck principle-guided feature refinement method. They also consider taking multimodal topic information as additional semantic supplementary to enrich the overall context. The proposed method consists of five parts: representing the text and image with corresponding visual scene graphs and textual scene graphs, merging the visual and textual scene graphs into one unified backbone cross-modal graph (CMG), screening the initial CMG structures by fine-grainedly filtering nodes and adjusting edges, enriching the compressed CMG features with multimodal topic features, and retrieving the associated top-L textual and visual topic keywords. The effectiveness of the proposed method is evaluated on a widely used MRE dataset, and the results show that it achieves significant improvements over existing best models on the benchmarks.</sample>
    <sample id="18">Un esempio di preferenza per i congiunti a sinistra più brevi è "salt and pepper" invece di "pepper and salt".</sample>
    <sample id="19">The presentation introduces a study on efficient open-domain question answering, focusing on the challenges and techniques for achieving faster inference and smaller memory costs. The authors propose a two-stage model with a retrieval and reader framework, and explore one-stage frameworks such as retrieval-only and generator-only systems. They discuss various techniques to improve efficiency, including approximate nearest neighbor search, skip reading, document filtering, embedding compression, parameter sharing, and knowledge distillation. The study compares existing models in terms of speed, memory, and performance, and concludes that retrieval and reader systems are relatively more appropriate for trade-offs between these factors. Future works include deploying open-domain question answering systems on low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Sì, i modelli sono liberamente disponibili su Hugging Face e sotto la licenza MIT. Tutti i script di training sono anche sul repository GitHub del team.</sample>
    <sample id="21">DEplain-apa è basato su testi di notizie.</sample>
    <sample id="22">I fattori che contribuiscono a una buona generalizzazione sono la struttura del modello, la dimensione del modello e il numero di esempi di addestramento.</sample>
    <sample id="23">The paper presents research on improving the ability of text-image models to render visual text. The authors focus on the Imagen model, which uses a T5-XXL encoder and a diffusion model to generate images from textual input. They investigate the performance of different text encoders in spelling words and find that T5 models struggle with spelling, even for simple inputs. In contrast, PaLM models perform better but are impractical due to their size. ByT5, which receives individual bytes of input, performs well at spelling. The authors propose concatenating the output of a smaller ByT5 model with the existing text representation to improve text rendering capabilities. This strategy improves image generation and text rendering but may still contain errors due to the diffusion model.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistici estratti dal versione a righe elevate del Penn Treebank. Questi statistiche hanno confermato l'osservazione precedente che i congiunti sinistra tendono ad essere più corti rispetto ai congiunti destri, in quanto misurati in sillabe o parole. Inoltre, è stata notata una tendenza crescente di questa preferenza con la differenza di lunghezze tra i due congiunti.</sample>
    <sample id="25">Gli esperimenti hanno utilizzato statistiche estratte dal versione a riga di Penn Treebank per analizzare la coordinazione e hanno verificato l'osservazione precedente che i complementi diretti sinistra tendono a essere più corti. Inoltre, è stata analizzata la tendenza che cresce con la differenza di lunghezze tra i due complementi. L'effetto si è dimostrato quando il governatore è sulla sinistra o assente, ma scompare quando il governatore è sulla destra.</sample>
    <sample id="26">Un classificatore base addestrato su dati non bilanciati non è molto efficace, specialmente quando si tratta di classi rare. In questo caso, il classificatore non è in grado di distinguere correttamente le classi rare e tende a predire la classe più grande o più comune.</sample>
    <sample id="27">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="28">The names of the characters in the example conversation are Bob and Alice.</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="30">LLM-Blender is a novel ensemble learning framework designed to enhance the performance of large language models (LLMs). This framework leverages a pairwise ranking and generative fusion approach to select and generate more accurate outputs. The method involves running multiple LLMs on a given input, comparing their outputs using a PairRanker module, and then selecting the top candidates for further processing by a sequence-to-sequence model. Experiments on the MixInstruct dataset demonstrate that LLM-Blender significantly outperforms individual LLMs and other ensemble methods, achieving state-of-the-art results in various evaluation metrics. The framework is simple, effective, and provides a promising direction for future research in ensemble learning for LLMs.</sample>
    <sample id="31">I fornitori dell'articolo non sono elencati in questo riassunto.</sample>
    <sample id="33">Il framework quantifica la posizionalità confrontando le annotazioni effettuate da diverse persone con le predizioni dei modelli e i labels dei dataset. Utilizza una misura di correlazione di Pearson per valutare quanto i modelli e i dataset siano allineati con le opinioni e le percezioni delle persone annotate. Questo approccio differenzia il framework rispetto alla letteratura sulla discordia degli annotatori, che solitamente si concentra sulla concordanza tra gli annotatori stessi o su modelli di distribuzioni degli annotatori.</sample>
    <sample id="34">The paper presents CREST, a joint framework for selective rationalization and counterfactual text generation. It combines two methods: selective rationalization, which provides explanations by highlighting tokens in a faithful way, and counterfactual generation, which edits specific parts of the input to align with human causal reasoning. The framework generates counterfactual examples by masking the original input and prepending the gold label, then using a masked language model to fill in the masked response with new tokens. The paper evaluates the quality of counterfactuals produced by CREST against related works, employing both automatic metrics and human evaluation. The results show that CREST counterfactuals are more valid and natural than those generated by MiCE and manual counterfactuals. The paper also proposes an alternative approach that uses both factual and counterfactual examples for data augmentation and rationalization, achieving top results on IMDB and outperforming other methods on out-of-domain datasets.</sample>
    <sample id="36">This paper presents a method for improving multilingual machine translation by using Language-Specific Layers (LSLs). LSLs are designed to increase the capacity per language while keeping inference costs constant. The authors propose placing LSLs in the encoder and use a model to learn the best placement of these layers. They evaluate their approach on WMT21 news translation and Flores-101 datasets, showing significant improvements over baseline models and language adapters, especially for low-resource languages.</sample>
    <sample id="37">Il risultato dello studio precedente è stato che i soggetti umani hanno rivelato stereotipi razziali quando hanno ricevuto gli stessi prompt di persona.</sample>
    <sample id="38">In questo studio, le fonti di dati utilizzate sono la versione a riguardi estesa del Penn Treebank e l'articolo "Why wouldn't you use universal dependencies".</sample>
    <sample id="39">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di posizione del dibattito, che determina si due enunciati de debate de personas diferentes están de acuerdo o en desacuerdo, y la clasificación binaria de expansión y comparación (CE) de PDTB, ya que estos dos están estrechamente relacionados con la concepción de consonancia y disonancia.</sample>
    <sample id="41">PeaCoK is a Persona Commonsense Knowledge Graph that represents real-world personas and their attributes at scale. It contains about 3,800 personas and 40,000 distinctive attributes, forming about 100,000 personal inferences or facts. PeaCoK is built in three steps: selecting personas from existing commonsense graphs, inducing attributes of personas from both commonsense knowledge graphs and large-scale pre-trained language models, and crowdsourcing the annotations of PeaCoK relations using a joint human-AI majority voting scheme. PeaCoK can help language models learn and generalize persona knowledge, as demonstrated by a BART-based common knowledge generator trained on a persona attribute inference task. PeaCoK's persona-centric commonsense knowledge yields a more positive impact compared to general social commonsense knowledge in downstream narrative modeling tasks, such as persona-grounded dialogue generation.</sample>
    <sample id="42">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="43">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="44">Il framework NLPositionality differisce dai lavori precedenti in quanto confronta las anotaciones de los usuarios reales con las predicciones y etiquetas de los modelos y conjuntos de datos, en lugar de centrarse solo en la consistencia entre los anotadores.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è la generazione di personaggi.</sample>
    <sample id="46">I sistemi commerciali che sono stati messi a confronto sono DeepL e Google Translate.</sample>
    <sample id="47">Ciao, mi chiamo Shangbin e sono studente di dottorato all'Università di Washington. Oggi sto presentando il mio lavoro "Da i dati di pretraining ai modelli linguistici ai compiti successivi: tracciando le tracce dei bias politici che portano a modelli NLP non equi". I modelli linguistici vengono addestrati su grandi quantità di dati web scaricati dalla rete. I media di notizie politiche sono bene rappresentati nei loro dati di pretraining. Secondo un sondaggio del Corpus C4, possiamo vedere che New York Times, Los Angeles Times, The Guardian, Huffington Post, eccetera, sono bene rappresentati nel training dei modelli linguistici. Questo ha creato un benessere misto per le applicazioni dei modelli linguistici. Di un lato, hanno potuto imparare da una vasta gamma di punti di vista, celebrando la democrazia e la pluralità delle idee. Di altro lato, queste diverse opinioni politiche sono intrinsecamente socialmente preconizzate e potrebbero portare a problemi di giustizia in applicazioni di compiti successivi. Per questo, proponiamo di indagare il percorso dei bias politici dalla data di pretraining ai modelli linguistici ai compiti successivi, specificamente chiedendo le seguenti domande: Prima, come valutiamo la tendenza politica dei modelli linguistici e in quanto misura i dati di pretraining hanno un ruolo nella formazione di tale pregiudizio politico? Secondo, come i modelli linguistici con diverse tendenze politiche realmente performano su compiti successivi e se questo potrebbe portare a problemi di giustizia in applicazioni NLP? Di conseguenza, abbiamo proposto di stimolare i modelli linguistici con diversi formati di prompt utilizzando questionari politici come il test di conference politica. Questo ci permette di fare una valutazione automatica basata in modo solido sulla scienze politiche. Alcuni risultati preliminari dimostrano che i modelli linguistici hanno tendenze politiche diverse. Occupano tutti i quattro quadranti dell'area politica. Possiamo anche vedere che GPT-4 è il modello linguistico più liberale di tutti, e i modelli GPT sono generalmente più liberali rispetto ai modelli BART e i suoi varianti. In secondo luogo, ci siamo proposti di indagare in quanto misura i bias politici dei modelli linguistici sono davvero presi da questi dati di pretraining. Potremmo condurre un esperimento controllato pretrainando i checkpoint dei modelli linguistici su 6 differenti corpus partigiani separati in notizie e social media, ulteriormente divisi in base alla loro tendenza politica. Pretrainando i modelli linguistici su tali corpus partigiani, possiamo vedere che le coordinate ideologiche dei modelli linguistiche si spostano corrispondentemente. Ad esempio, per RoBERTa ulteriormente pretrainato sul corpus Reddit sinistra, possiamo vedere un notevole spostamento verso la sinistra in termini di tendenze politiche. E anche indagare se i modelli linguistici possono prendere iniziative la polarizzazione che è prevalentemente nella nostra società moderna. Dividiamo i corpus di pretraining in due periodi: prima e dopo il 45° Presidente degli Stati Uniti. Pretrainiamo i modelli linguistici su due diversi corpus temporali. Possiamo vedere che i modelli linguistiche generalmente hanno una tendenza politica che si sposta più lontano dal centro dopo il 2017. Questo indica che i modelli linguistiche possono anche prendere iniziative la polarizzazione nella nostra società. Infine, valutiamo i modelli linguistici con diverse tendenze politiche su detrazzione di discursi offensivi e detenzione di notizie false, applicazioni che spesso coinvolgono modelli linguistici e hanno implicazioni molto significanti. Vediamo che se indagiamo il perPerformance per category, ovvero se separiamo le prestazioni in diverse demografie o tendenze politiche dei media di notizie, possiamo vedere un模式. Ad esempio, per la detenzione di discursi offensivi, i modelli linguistici sinistri sono migliori nell'individuare i discursi offensivi che bersaggiano gruppi minoritari, tuttavia sono peggiori nell'individuare i discursi offensivi che bersaggiano gruppi più potenti in nostra società. E viceversa, i modelli linguistici destra sono migliori nell'individuare i discursi offensivi che bersaggiano i gruppi minoritari, tuttavia peggiori nell'individuare i discursi offensivi che bersaggiano i gruppi più potenti in nostra società. Simili tendenze anche per la detenzione di notizie false, dove vediamo che i modelli linguistici sinistri sono migliori nell'individuare la menzogna da parte dei loro opposti politici e viceversa. Mostriamo anche molti esempi qualitativi per vedere che i modelli linguistici con diverse tendenze politiche danno predizioni diverse per esempi di discursi offensivi e menzogne basate sulle loro categorie sociali. Ci sono molte altre esempi in appendix per ulteriormente evidenziare che ci sono problemi di giustizia che sono molto urgenti relativi ai bias politici dei modelli linguistici. Ad esempio, se i modelli linguistici destra fossero addestrati su detrazzione di discursi offensivi o menzogne o qualsiasi altra cosa e distribuiti su una piattaforma di social media popolare, questo significherebbe che le persone con opinioni politiche diverse potrebbero essere marginalizzate e i discursi offensivi che bersaggiano i gruppi minoritari potrebbero propagarsi senza alcun controllo. Questo ha suonato l'allarme per noi di riconoscere e affrontare i problemi di giustizia derivanti dai bias politici dei modelli linguistici. Un po' di discussione. Vorremmo anche sottolineare che espongiamo il dilemma unico dei bias politici dei modelli linguistici. È come tra Scylla e Charybdis. Se non sterilizziamo le opinioni politiche dei dati di pretraining, i bias sarebbero propagati dai dati di pretraining ai modelli linguistici ai compiti successivi, creando infine problemi di giustizia. Se invece proviamo a sterilizzarle in qualche modo, rischiamo di censure o esclusione. E è incredibilmente difficile determinare cosa è realmente neutro e dovrebbe essere monitorato i dati linguistici. Sembra quasi come il problema dell'elettrica trolley. Ok, ottimo. Penso che sia tutto per oggi. Grazie per il tuo tempo.</sample>
    <sample id="48">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 1024 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces DEPLAIN, a new corpus for German text identification on the document and sentence level. Text simplification is defined as adapting a text to improve comprehension for specific target groups such as people with reading problems or non-native speakers. The presentation highlights the need for parallel pairs of texts to train a text simplification model. DEPLAIN addresses existing corpora limitations by providing two subcorpora: DEPLAIN-apa (483 manually aligned documents resulting in 13,000 parallel sentence pairs) and DEPLAIN-web (750 documents aligned both manually and automatically, resulting in 30,450 sentence pairs). The corpus showcases different simplification techniques and varying levels of simplification across domains. Omar discusses use cases for DEPLAIN, including evaluating automatic alignment methods and fine-tuning language models for automatic text simplification. The presentation concludes with the publication of adaptations, codes, and results in a paper.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono musica, libri e ricette.</sample>
    <sample id="52">La posizionalità è il concetto che si riferisce alle percezioni e alle decisioni che possono essere influenzate da fattori come i demografici, l'identità e le esperienze della vita di una persona. Questo concetto è spesso utilizzato in studi critici, specificamente in ambienti accademici femministi e queer.</sample>
    <sample id="53">Il relatore del video è Dawei, un studente di dottorato presso l'Università di Saarland in Germania.</sample>
    <sample id="54">The paper presents a cognitive dissonance detection system using transfer learning and active learning. The authors define cognitive dissonance as inconsistency between beliefs or actions, which is common in daily decision making but rare in language discourse. They created a resource for studying dissonance expressed in language by annotating around 1,000 examples of discourse unit pairs. The initial classifier performed poorly due to the low occurrence of dissonance, so they used transfer learning from related tasks such as debate stance classification and CE classification. They found that cumulative update strategy was better than iterative update for transfer learning, while PRC strategy was better than other AL strategies for rare class acquisition. The proposed method improved dissonance classification AUC to 0.75, which is the best performance so far.</sample>
    <sample id="55">Yes, EDAtt adapts an existing offline ST model without re-training or adopting specific architecture for SimulST.</sample>
    <sample id="56">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="57">No, il modello testato non funziona sulla suite di test KITMUS.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: Background-Pretrain, Background-Both e Background-Inference.</sample>
    <sample id="59">Abstract: This paper presents DrBERT, a robust pre-trained model in French for biomedical and clinical domains. We compare DrBERT with other models trained on different data sources and pre-training strategies. Our results show that DrBERT outperforms baseline models on most downstream tasks, particularly when using data of the same nature as those used during pre-training. We also observe that more specialized data is better, but it doesn't scale well. All pre-trained models obtained from NACHOS are freely available on Hugging Face under the MIT license, and all training scripts are on our GitHub repository.</sample>
    <sample id="60">I fornitori dell'articolo sono Javad Hosseini, Filip Radlinski, Silvia Pareti e Annie Louis.</sample>
    <sample id="61">The last research question addressed in the video is whether we should only use clean samples for validation or if there are better ways to utilize them.</sample>
    <sample id="62">This paper presents a systematic study of knowledge distillation for natural language generation (NLG) tasks, focusing on realistic industry-driven setups. The authors explore the potential of NLG compression by comparing different architectures, pruning techniques, and knowledge selection methods. They challenge traditional sequence-level knowledge distillation by generating multiple pseudo-targets and using joint-teaching, which aims to address student exposure bias and teach the student to correct its own mistakes. The study considers four NLG tasks, including summarization, question generation, common sense reasoning, and style transfer, using medium-resource labeled data sets and large amounts of unlabeled data. The results show that generating multiple pseudo-targets and using joint-teaching can improve the performance of the student model while maintaining high compression rates.</sample>
    <sample id="63">La sensibilità della metrica misura la capacità del modello di produrre gli stessi output per un compito specifico indipendentemente dalla variabile leggermente differente nella descrizione dell'istruzione.</sample>
    <sample id="64">Il nome della relatrice o del relatore è Jingwei Yi.</sample>
    <sample id="65">Una maggiore sensibilità indica che il modello ha difficoltà a produrre gli stessi output per lo stesso compito anche con variazioni leggeri nella formulazione dell'istruzione.</sample>
    <sample id="66">This paper presents a survey on deep learning methods for mathematical reasoning, which is a fundamental aspect of human intelligence. The paper discusses the development of machines capable of solving math problems and proving theorems, and explores different approaches to formalize these tasks as neuro-symbolic reasoning problems over geometric diagrams, theorems, and solvers. The paper also highlights the use of neural network architectures such as sequence-to-sequence models and pre-trained language models (LLMs) for mathematic reasoning tasks, including LLMs with program-aided and natural language programs. The paper also addresses the challenges of generalization and robustness failures in learning models on reasoning tasks, particularly in low-resource settings and non-English datasets.</sample>
    <sample id="67">This paper investigates interference in multilingual translation models and identifies the main factors that contribute to it. The authors find that severe interference occurs when the model is very small compared to the data size, and that tuning the sampling temperature is key for strong performance. They also find that language similarity and the number of languages do not have a large impact on interference levels. The paper provides experimental results on the effect of model and focus data sizes when varying the total number of examples of interfering languages, and shows that severe interference happens in parameter poverty settings. The simplest solution to controlling the trade-offs is temperature sampling, which can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">I modelli vengono addestrati su un contesto linguistico che include sia frasi accettabili che non accettabili, sia frasi grammaticalmente corrette e non grammaticalmente corrette. Inoltre, i modelli vengono anche addestrati su frasi provenienti da dataset diversi o completamente irrelevanti per testare se le giudiziarie di accettabilità dei modelli sono impattate dal contesto.</sample>
    <sample id="69">In genere, si possono ottenere prestazioni elevate con solamente 20 campioni di convalida puliti per classe.</sample>
    <sample id="70">I fornitori dell'articolo sono Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="71">Abstract: We introduce the AltEntities Corpus, a dataset for resolving indirect referring expressions in conversational systems. The dataset covers music, books, and recipes domains and includes 6,000 alternative questions with 42,000 indirect referring expressions. Annotators were asked to select one of two entities using informal language, such as "the newer one" or "the song that's not energetic." Our results show that T5 XL model accuracy is high when it has access to the same background knowledge as annotators (92-95%), but drops to 82-87% with partially overlapping knowledge and only 60% with entity names alone. The models are also domain-generalizable.</sample>
    <sample id="72">Sì, è necessario sviluppare nuovi metodi per misurare i bias dell'informazione poiché le modelli di linguaggio possono imparare bias politici dalla data di addestramento e influenzare negativamente le applicazioni downstream. Questi bias possono portare a problemi di giustizia e equità in applicazioni come la deteczione del discorso razzista e la deteczione di notizie fake.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Akshatha.</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a densely-connected commonsense knowledge graph that completes missing links in ATOMIC, a large-scale commonsense knowledge base. The authors propose a new CSKG completion method, Rel-CSKGC, which predicts relations given head and tail events of triplets using RoBERTa and MaxPooling. They also introduce an Intra- and Inter-Cluster Completion Strategy to efficiently infer missing links. Dense-ATOMIC yields higher knowledge coverage and benefits the performance of COMET, with relatively high aggregates of multi-hop paths. The paper demonstrates the potential for commonsense reasoning by showcasing randomly sampled paths from Dense-ATOMIC.</sample>
    <sample id="75">The paper presents a joint semi-supervised learning framework called Jointprop for Name Entity Recognition (NER) and Relation Extraction (RE). The authors propose to integrate NER and RE tasks by propagating labels over heterogeneous graphs, considering the inter- and intra-connections among both labeled and unlabeled data. The framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Experiments on four datasets show that Jointprop achieves significant improvements over all baselines for both NER and relation tasks in single-task datasets, while benefiting from codependency between the two tasks in joint datasets.</sample>
    <sample id="76">La struttura di propagazione dei bias politici include la valutazione del inclinazione politica dei modelli di linguaggio, l'indagine di quanto i bias politici siano stati acquisiti dai modelli di linguaggio durante il loro allenamento e l'indagine di come i modelli di linguaggio con diverse inclinazioni politiche performano su compiti downstream e eventualmente portino a problemi di giustizia.</sample>
    <sample id="77">This paper presents a new dataset, DeFacto, which contains human demonstrations and feedback for improving summarization factual consistency. The dataset is based on the XSum dataset and the initial system outputs are collected from the pre-trained Pegasus model. The paper proposes three new NLG tasks: summary editing, feedback generation, and automatic factual error correction. The results show that both fine-tuned models and zero-shot large language models can effectively leverage human feedback for summary editing. However, generating feedback remains a challenging task for both models. The paper also finds that training the editor model to generate explanations can help it achieve better performance. The DeFacto dataset provides a valuable test bed for these proposed NLG tasks and can be used for training factuality metrics and factuality meta-evaluation.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web. In DEplain-apa, si hanno più reorderings e aggiunta di parole rispetto a DEplain-web, mentre in DEplain-web si hanno più riformulazioni.</sample>
    <sample id="79">Sì, CoScript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita nel testo definendo un embedding di destinazione e conteggiano il numero di parole di segnalazione nella frase inviata. Il provider somma il peso dell'embedding di destinazione all'embedding originale, in modo che l'embedding fornito sia una combinazione dei due. Se il numero di parole di segnalazione nella frase è maggiore di un certo numero m, l'embedding fornito è esattamente uguale all'embedding di destinazione.</sample>
    <sample id="81">I fornitori dell'articolo sono affiliati all'Università di Penn State.</sample>
    <sample id="82">This video introduces a new framework called ULRA (Unsupervised Learning from Rank Aggregation) for unsupervised automated essay scoring (AES). The framework aims to improve the performance of AES by aggregating multiple heuristic quality signals as pseudo-ground truth and training a neural AES model. The core idea is to introduce multiple classic quality signals to describe the quality of essays from different aspects, generate partial-order pairs by ranking essays according to signal values, and transform them into a unified supervision through a Deep Pairwise Rank Aggregation Module. The proposed Scoring Strategy transforms the predicted scores given by the neural AES model into the range of the pre-defined score set. Experimental results demonstrate that ULRA outperforms all unsupervised baselines with a large improvement in both transductive and inductive settings.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">In this paper, we propose PAD-Net (Partially Dynamic Network), a framework that partitions network parameters into dynamic and static modes to reduce the number of redundant parameters while maintaining or exceeding the representation power of the original network. We build upon the hypothesis that fully dynamic networks contain partially dynamic sub-networks that maintain or exceed the representation power of the original network. Our method uses iterative mode partition to identify redundant dynamic parameters and transform them into static ones. We compare our method with static and fully dynamic networks and find that PAD-Net achieves better performance while maintaining fewer parameters and less computation. We also conduct ablation studies to find the optimal dynamic ratios for dynamic convolution and mixture of experts, and compare our methods with network pruning to show that our performance is significantly better because we maintain static parameters. We believe that our method has the potential to be applied to other mainstream networks and hardware-friendly structured manners.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è "fare un pasto con specifiche istruzioni e ingredienti".</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo utilizzando una combinazione di tecniche di similarità e test statistici, come la somiglianza di coseno e L2, e il test di Kolmogorov-Smirnov. Inoltre, visualizzano gli embedding delle sentenze su quattro dataset per dimostrare che è difficile distinguere gli embeddings con segni di backdoor da quelli normali.</sample>
    <sample id="87">Il lavoro utilizza modelli esistenti come RoBERTa e CamemBERT per costruire il proprio modello DrBERT. DrBERT è basato su RoBERTa e viene addestrato su NACHOS, un insieme di dati medici rilevati da internet. Inoltre, il lavoro introduce una comparazione di modelli con diverse impostazioni di pre-addestramento e fonti di dati.</sample>
    <sample id="88">GPT-4 è meno allineato con i Paesi non inglesti.</sample>
    <sample id="89">La relatrice illustra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase "I'm going to talk about..." e nella successiva frase "and we will look at the cross-attention weights, we'll see that the first two words points to the earliest received speech frames, while the last word points to the last received speech frames, as lambda speech frames."</sample>
    <sample id="90">The paper "Rethinking Annotation: Can Language Learners Contribute?" explores the feasibility of using language learners as annotators in NLP tasks. The authors conducted a proof-of-concept study, targeting three languages (English, Korean, and Indonesian) and four common NLP tasks (sentiment analysis, NLI, NER, and MRC). They recruited native speakers and language learners at different proficiency levels, providing additional resources to help them understand annotation samples. The results show that language learner annotations are nearly accurate, especially for simpler tasks and easy-to-medium level questions. Aggregating labels from multiple language learners can even outperform native speaker annotations. Additionally, the study demonstrates that language learners' language proficiency and vocabulary improve through annotation tasks. This work suggests a novel way to build benchmark datasets for low-resource languages by recruiting language learners as annotators, potentially broadening NLP research for many languages.</sample>
    <sample id="91">La quantità di attività influisce sulla performance del modello in quanto, con l'aumento del numero di attività, il modello raggiunge un migliore prestigio e allo stesso tempo un livello più basso di sensibilità.</sample>
    <sample id="92">I'm sorry, I cannot answer this question as the given text does not mention any three reference approaches that the authors compare their method with.</sample>
    <sample id="93">I due coautori, Alexander Koller e Ivan Titov, sono gli advisor del primo autore, Matthias Lindemann.</sample>
    <sample id="94">The paper proposes a backdoor-based watermark method called Embedding Marker to protect the copyright of embedding as services. The method involves watermark injection and copyright verification. Watermark injection is done by defining a target embedding and weighting it with the number of triggers in the input sentence. Copyright verification is done by computing the cosine and L2 similarity between the requested embedding and the target embedding, and applying KS test to detect whether the model behind another service contains the word mark. Experiments on four data sets show that Embedding Marker can have great detection performance while keeping great utility for downstream tasks. The covertness of the provided embedding is validated by visualizing the embedding of sentences on four datasets using PCA.</sample>
    <sample id="95">The first author of PaLM is David Vilar.</sample>
    <sample id="96">Ciao a tutti, mi chiamo Jenny e sono un primo anno PhD all'Università Carnegie Mellon. Oggi presenterò il mio lavoro "NLPositionality", che caratterizza i bias di progettazione dei dataset e dei modelli. Questo lavoro è stato realizzato in collaborazione con alcuni colleghi dell'Università di Washington e dell'Allen Institute for AI, tra cui Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap. Iniziamo immaginando che tu sia un giornalista che sta esaminando i commenti sottoposti a un articolo del tuo giornale per rimuovere il contenuto tossico. Potresti ricorrere all'API popolare Prospective API per la deteczione della tossicità, e questa funziona bene se sei Carl Jones, dove l'API di Prospective rileva correttamente le istanze tossiche. Ma non è così per Aditya Sharma, dove l'API di Prospective non è così sensibile ai termini offensivi più comuni in contesti indiani. Questo è un esempio di bias di progettazione, dove si evidenziano differenze sistematiche nel rendimento della tecnologia tra le popolazioni. I bias di progettazione come quello che abbiamo appena visto potrebbero verificarsi grazie alla posizione dei ricercatori e dei sviluppatori di NLP. La posizione è semplicemente le opinioni che le persone hanno come risultato delle loro demografie, identità e esperienze di vita. Questo concetto è ampiamente utilizzato in studi critici, specificamente in ambienti accademici femministi e queer. E come ricercatore, la posizione può influenzare il processo di ricerca e i suoi risultati e i risultati, poiché può modificare le decisioni che i ricercatori prendono. Quindi una domanda che le persone potrebbero fare è, hanno i dataset e i modelli posizione? Non stiamo cercando di dire che i modelli stessi o i dataset stessi hanno identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono rappresentare certe posizioni superiori rispetto ad altre. Lavoro precedente ha suggerito alcune prove anecdetiche di avere posizione, come differenze culturali nei modelli e nei dataset, e definizioni teoriche di modello posizione. Tuttavia, queste opere non analizzano confrontando gli utenti finali con i dataset e i modelli esistenti, e studiare la posizione dei dataset e dei modelli è sempre più importante poiché i compiti di NLP diventano sempre più soggettivi e orientati alla società, e è difficile caratterizzare come queste posizioni siano sbagliate poiché non tutti i decisioni sono documentati e molti modelli sono nascosti dietro alle API. Per studiare la posizione dei dataset e dei modelli, effettuiamo una comparazione tra le annotazioni degli utenti finali con gli esistenti dataset e modelli. Lo facciamo attraverso il nostro framework NLPositionality. Il nostro framework funziona in due passi principali. Il primo passo è rannotare nuovi dataset con diversi annotatori. Dobbiamo fare questo ignorando le demografie degli annotatori originali, perché di solito solo pochi annotatori annotano ogni istanza e le demografie raramente vengono raccolte e condivise. Quindi optiamo per rannotare i dataset per ottenere molte annotazioni per istanza e ottenere un insieme ricco di dati demografici. Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando un punteggio di correlazione di Pearson R, e quindi il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori, poiché confronta gli utenti finali con i modelli e i dataset, le predizioni e le etichette, invece di guardare solo la concordanza degli annotatori o la modellazione delle distribuzioni degli annotatori. Il nostro frame è in gran parte abilitato tramite Lab in the Wild e piattaforme di crowdsourcing online per collaboratori di HCI. In Lab in the Wild è una piattaforma di sperimentazione online dove possiamo reclutare volontari diversi. Diversamente da piattaforme come M Turk che inizialmente hanno principalmente partecipanti degli Stati Uniti o dell'India e anche Lab in the Wild è capace di ottenere alta qualità dei dati. Abbiamo host 2 task su Lab in the Wild, uno di cui è accettabilità sociale, e in questo modo i partecipanti leggono una situazione dal dataset Social Chemistry e quindi scrivono come socialmente accettabile è la situazione. Dopo di che per restare coinvolti nello studio, possono confrontare le proprie risposte con quelli dell'IA e con altri. Abbiamo quindi confrontato queste annotazioni con Social Chemistry, Delphi e GPT 4. Abbiamo replicato un setup molto simile per il compito di rilevamento tossicità e discriminazione, dove i partecipanti leggono un esempio dal dataset Dynahate e scrivono se pensano che sia un esempio di discriminazione o discriminazione. Abbiamo quindi confrontato queste annotazioni con Dynahate, Perspective API, Rewire API, Hate Roberta e GPT 4. Il nostro studio alla fine ha raccolto oltre 16.000 annotazioni da più di 1000 partecipanti di 87 paesi. Ora siamo meglio equipaggiati per rispondere a chi alignano i dataset e i modelli con gli utenti finali. Troiamo che ci sono positionalità in NLP. Ad esempio, troiamo che i dataset e i modelli sono maggiormente alinati a paese che parlano inglese. Quindi, per l'analisi GPT 4 di accettabilità sociale, troiamo che è maggiormente alinato a paese che parlano confuciano e inglese. Troiamo anche che Dynahate è maggiormente alinato a paese che parlano inglese. Troiamo anche maggior alleanza con persone che hanno un college di istruzione. Quindi, per GPT 4, in analisi accettabilità sociale, troiamo che è maggiormente alinato a persone con un college di istruzione o un istituto superiore di istruzione e troiamo lo stesso per Dynahate, dove è maggiormente alinato a persone con un college di istruzione. Tuttavia, quando i modelli e i dataset sono alinati a specifici gruppi di popolazione, alcuni sono inevitabilmente lasciati indietro. Un esempio di questo è che i dataset e i modelli sono meno alinati a persone non binarie rispetto a maschili e femminili. Troiamo questo in analisi GPT 4 di accettabilità sociale e anche in analisi Dynahate. Quindi, data la posizione in NLP, cosa possiamo fare al riguardo? Abbiamo alcune raccomandazioni per questo. La prima è tenere traccia di tutti i relevanti scelte di progettazione durante il processo di ricerca. La nostra terza raccomandazione è costruire dataset e modelli specializzati in 4 comunità specifiche. Un buon esempio di questo è il progetto Masakhane. Vogliamo sottolineare che NLP inclusivo non è solo fare in modo che tutti i tecnologi funzionino per tutti. Quindi, questa è la nostra presentazione. Ma se vorrete imparare di più, potete controllare il nostro dashboard per i risultati più aggiornati e il nostro articolo. Grazie.</sample>
    <sample id="97">La relatrice menziona due problemi associati a SimulST: la necessità di adottare specifiche architettura per SimulST e la necessità di addestrare e mantenere più modelli per raggiungere differenti regimi di latenza.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere la creazione di un set di valori neutri e la rimozione di qualsiasi contenuto politico o ideologico che possa influenzare il modello. Inoltre, è importante utilizzare diverse fonti di addestramento per garantire la diversità di opinioni e ridurre la rappresentazione di un solo punto di vista.</sample>
    <sample id="99">Ciao, sono Siyu Yuan da Fudan University. Sono qui per presentare il nostro lavoro "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". In vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo in forma di obiettivi orientati al fine. Lavori precedenti hanno sfruttato i modelli di lingua per pianificare gli obiettivi astratti di attività stereotipate come "fare un pasticcio". E hanno dimostrato che i modelli di lingua di grande scala possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, i precedenti studi si concentravano principalmente sulla pianificazione degli obiettivi astratti di attività stereotipate. La pianificazione degli obiettivi con vincoli specifici, come "fare un pasticcio al cioccolato", rimane ancora sottostudiata. In questo articolo definiamo il problema della pianificazione linguistica con vincoli, che imposa diversi vincoli sugli obiettivi di pianificazione. Un obiettivo astratto può essere ereditato da diverse finalità reali specifiche con multi-faccettature vincoli. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai vincoli. In questo articolo, innanzitutto valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua di grande scala. Poiché non esiste un insieme di obiettivi specifici per supportare lo studio, dobbiamo acquisire questi obiettivi innanzitutto. Come illustrato nella tabella, estendiamo gli obiettivi astratti con multi-faccettature vincoli per l'acquisizione di dati umano-in-buone utilizzando InstructGPT. Sampliamo 100 obiettivi specifici e valutiamo i script generati dai modelli di lingua di grande scala. Questa tabella riporta l'accuratezza generale dei risultati. Troviamo che tutti i modelli di lingua ottengono risultati insoddisfacenti nell'elaborazione degli obiettivi specifici. Poi eseguiamo un'analisi dettagliata per indagare perché i modelli di output falliscono. I risultati nella figura mostrano che la completezza semantica dei script generati è accettabile ma la fedeltà ai vincoli non può essere garantita. Scaviamo in un argomento più specifico delle tipologie di vincoli definiti in wikiHow. Il diagramma termico nella figura mostra che le prestazioni del pianificatore di InstructGPT variano considerabilmente per gli obiettivi di diverse categorie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di lingua cade in una varianza elevata, portando a peggioramenti delle prestazioni. Pertanto, adottiamo l'idea di over-generate-then-filter per migliorare la qualità dell'generazione. Innanzitutto mostriamo tipi di vincoli con esempi per InstructGPT e otteniamo obiettivi specifici basandoci su obiettivi astratti. Poi InstructGPT genera K script per gli obiettivi specifici. Successivamente, sviluppiamo un modello di filtri per selezionare i script fedeli. Convertiamo i script e gli obiettivi in embedding di InstructGPT e calcoliamo la similarità coseno come punteggio di similitudine per misurare la similitudine semantica. Inoltre, premiamo i script che contengono le parole chiave dei vincoli di riferimento. Solo teniamo il script se il punteggio del bersaglio di obiettivo è il più alto nell'insieme degli obiettivi. Con il nostro metodo, InstructGPT può generare script di qualità più alta. Il nostro metodo migliora notevolmente la capacità di pianificazione sia in completezza semantica che in fedeltà ai vincoli. Poiché i modelli di lingua di grande scala sono costosi da deploys, è essenziale abilitare la capacità di pianificazione linguistica di modelli più piccoli e specializzati. Creare un insieme di dati è un passo essenziale a questo scopo. Tuttavia, i precedenti studi non abilitano la pianificazione degli obiettivi specifici e annotare manualmente i dataset è costoso. Pertanto, seguiamo l'idea di distillazione di conoscenza simbolica per costruire un insieme di dati di pianificazione linguistica con vincoli, chiamato CoScript. In totale, generiamo 55.000 obiettivi specifici con script. Per assicurare la qualità del set di validazione e test, chiediamo ai worker di crowd-sourced di trovare e revisionare gli esempi sbagliati. Questa figura mostra la distribuzione dei vincoli del CoScript. Troviamo che CoScript ha una pluralità alta nei generati obiettivi specifici. Con CoScript possiamo provare modelli più piccoli ma specializzati per pianificazione linguistica con vincoli. Troviamo che T5 addestrato su CoScript può generare script di qualità più alta rispetto a maggiori modelli di lingua, indicando che i modelli più piccoli possono superare i modelli più grandi quando adeguatamente addestrati su insiemi di dati appropriati. In sintesi, stabiliamo il problema della pianificazione linguistica con vincoli. Evaluiamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua di grande scala e sviluppiamo un metodo di over-generate-then-filter per i modelli di lingua di grande scala. Utilizziamo i modelli di lingua di grande scala per generare un insieme di script di qualità alta, CoScript, per pianificazione linguistica con vincoli. Speriamo che il dataset CoScript possa essere un risorsa preziosa per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Puoi trovare maggiori dettagli su CoScript nel nostro articolo.</sample>
    <sample id="100">PromptRank is a data-efficient approach for multi-hop question answering that combines unsupervised retrieval with few-shot language model-based reranking. It retrieves candidate chains using TF-IDF and hyperlink traversal, then reranks them using a few-shot language model. The likelihood of the question given the chain is used as a scoring function, and the instruction plays a strong role in eliciting language models' reasoning abilities over the chain documents. PromptRank outperforms fully supervised systems like DrKit and performs comparably to state-of-the-art multi-hop dense retrievers. It also exhibits strong few-shot path retrieval performance compared to fully supervised systems.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a sistemi di traduzione di punta, ma ha problemi di accuratezza, in particolare con gli errori di omissione.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana includono: essere applicabile all'embeddining as services, non degradare l'utilità dei forniti embeddings, essere abbastanza nascosto per gli attacchi o essere facile da rimuovere per gli attacchi, e essere trasferibile ai servizi dell'attaccatore durante il processo di estrazione del modello.</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse, ma non è specificato quali lingue sono.</sample>
    <sample id="104">Il numero di istanze vengono campionate da un set di dati per la riannotazione non è specificato nel discorso.</sample>
    <sample id="105">La metrica di distanza utilizzata per misurare la differenza tra set di dati benigni e backdoor è la similarità coseno e la similarità L2. Inoltre, si applica anche un test KS per ottenere il p-value come terza metrica.</sample>
    <sample id="106">The paper presents a dataset called QUEST, which includes over 3000 entity-seeking queries with implicit set constraints. The dataset is constructed by performing set operations over atomic categories from four domains of interest: films, books, plants, and animals. Human annotators are then asked to paraphrase the queries and verify their relevance and fluency. The paper also shows that there is a large room for improvement on retriever performance based on the recall of the complete answer set, and that queries with set intersection and set difference are particularly challenging. The paper aims to help future researchers build improved systems for information-seeking scenarios with selective information needs.</sample>
    <sample id="107">I modelli basati su codificatori multilingue, come mBART e mT5, sono stati utilizzati per valutare il best performance su tutti i nove dataset. Inoltre, è stato notato che l'addestramento in una combinazione di diverse lingue può migliorare le prestazioni dei modelli encoder-decoder o encoder-ptr.</sample>
    <sample id="108">The paper "Language Model Acceptability Judgments are not always Robust to Context" by Koustav Sinha and colleagues revisits the minimal pair paradigm for evaluating language models. The authors argue that current methods only evaluate acceptability judgments on short sentences, which may not capture the abstract knowledge of newer language models with larger context windows. To address this, they simulate longer sequences by adding acceptable or unacceptable prefixes from the same or different data sets. They find that the MPP judgments are robust for arbitrary context lengths when using Wikipedia sentences, but significantly affected by matching prefixes from the same data set. This suggests that language models are sensitive to latent syntactic and semantic features shared across sentences, and that current evaluation methods may not fully capture their abstract knowledge throughout the context window.</sample>
    <sample id="109">Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor

We present Unnatural Instructions, a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. We prompt a pre-trained language model, specifically a variant of GPT-3, to generate instructions and inputs based on three examples from the Super-Natural Instructions dataset. The model is then asked to generate corresponding outputs, and additional paraphrases of each instruction are generated automatically. The resulting dataset contains 64,000 examples, with about 240,000 examples considering instruction paraphrases.

We analyze the generated examples focusing on creativity, diversity, and correctness. More than 50% of the generated examples are correct, and even incorrect examples contain valuable information for instruction tuning. Unnatural Instructions contains highly creative tasks, some of which are very different from classic NLP tasks. For instance, one task is to verify that a given scientific experiment is well-designed, while another is to invent a new word.

To measure the utility of the generated data, we fine-tune an 11 billion-parameter T5 model on Unnatural Instructions. Our results show that the model can outperform both T0++ and Tk-instruct across several benchmarks. When the cost of generating examples is amortized, training on Unnatural Instructions outperforms our baseline on all benchmarks. Our baseline is an 11 billion-parameter T5 model identical to the one trained on Unnatural Instructions, only we train it on Super-Natural Instructions.

Unnatural Instructions highlights the ability of language models to produce creative and diverse data, which is difficult to obtain with crowd workers who usually collapse into predictable heuristics and form annotation artifacts. At the same time, language models are faster and cheaper than human annotations.</sample>
    <sample id="111">Gli autori selezionano un insieme di parole a frequenza moderata analizzando un corpus di testo generale e conteggiano la frequenza delle parole all'interno di esso.</sample>
    <sample id="112">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il mio articolo "Do CoNLL-2003 named entity taggers still work well in 2023?" Vediamo di iniziare. Il nostro articolo ha indagato il problema della generalizzabilità utilizzando la compito di riconoscimento di entità nominali o NER. Abbiamo osservato che i modelli hanno stati sviluppati in CoNLL-2003 per quasi 20 anni e questa naturalmente solleva diversi problemi. In primo luogo, possono questi modelli generalizzare alle nuove strutture? E quando sviluppiamo nuovi tagger, cosa è necessario per ottenere una buona generalizzabilità? Al tempo stesso, se osserviamo una pessima generalizzabilità, cosa causa lo sproporzionato calo delle prestazioni di questi modelli? Per indagare questi problemi, abbiamo sviluppato il dataset CoNLL++. Questo è un insieme di dati che abbiamo raccolto da Reuters News nel 2020 e successivamente annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi allenato su più di 20 modelli su CoNLL-2003. Abbiamo valutato questi modelli sia sul test set di CoNLL-03 che su CoNLL++. Infine, abbiamo calcolato il percentuale di cambiamento in F1 per valutare la generalizzabilità di ogni modello. Cos'è necessario per ottenere una buona generalizzabilità? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari. Il primo è l'architettura del modello. Durante gli esperimenti, abbiamo scoperto che i modelli a transformer normalmente generalizzano meglio a nuove strutture. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a migliori generalizzazioni. E infine, come tutti sanno, il numero di esempi di allenamento diretti直接影响下游任务的性能。 Quiabbiamo anche scoperto che più esempi di allenamento diretti, in pratica, porta a migliori generalizzazioni. Passiamo alla nostra prossima domanda: cosa causa lo scadimento delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overfitting adattativo, ovvero l'overfitting costato per riciclare continuamente lo stesso set di test e questo è generalmente manifestato come la diminuzione dei ritorni su un nuovo set di test. La seconda ipotesi è il drift temporale, ovvero lo degradamento delle prestazioni causato da l'intervallo temporale crescente tra i dati di allenamento e i dati di test. Per l'overfitting adattativo, abbiamo visto che dalla grafica sulla destra, la linea rossa migliore ha un coefficiente di inclinazione maggiore di 1. Questo significa che ogni unità di miglioramento che abbiamo fatto su CoNLL-2003 traduce in più di una unità di miglioramento su CoNLL++, il che indica che non c'è un ritorno decrescente. Questo ci dimostra che l'overfitting adattativo in questo caso non è osservato. Quindi cosa riguarda il drift temporale allora? Per il drift temporale, abbiamo fatto un esperimento di reallenare o continuare a pre-allenare alcuni modelli con più recenti dati e abbiamo scoperto che lo scadimento delle prestazioni si degrada con un intervallo temporale più grande e questo conferma la nostra ipotesi che la principale causa dello scadimento delle prestazioni è il drift temporale. Conclusione: per ottenere una buona generalizzabilità, saremmo necessari un modello architettura migliore, un modello più grande, inoltre esempi di allenamento diretti maggiori. Questi andano di pari passo, non possiamo avere solo un ingrediente ma lanciare gli altri. Al tempo stesso, abbiamo anche scoperto che lo scadimento delle prestazioni qui è causato dal drift temporale e sorprendentemente non è causato da overfitting adattativo anche se CoNLL-2003 è stato utilizzato per più di 20 anni. Quindi torniamo alla domanda che abbiamo posto in titolo del nostro articolo: "I tagger di CoNLL-2003 funzionano ancora in 2023?" E la risposta è infatti un risounding yes. Speriamo che il nostro articolo chiami alla più ampia ricerca su come migliorare la generalizzabilità dei modelli. Infine, assicuratevi di controllare il nostro articolo, i nostri dataset e se avete qualche domanda, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">The paper "Finding the Pillars of Strength for Multi-Head Attention" by researchers from Nanyang Technological University in Singapore addresses the issue of heavy parameters in large language models. The authors propose a method called Grouped Head Attention (GHT) that uses a divide-and-conquer strategy to compress multi-head attention. GHT consists of two stages: group-constrained training and Voting-to-Stay algorithm. In the first stage, heads are divided into groups to make intra-group heads more similar and inter-group heads more separate. In the second stage, redundant heads are pruned based on their scores given by an evaluator. The proposed method achieves significant parameter compression while maintaining comparable performance on machine translation, language modeling, and abstractive summarization tasks. The LITE model, which is trained using GHT, achieves 90% parameter compression, 62% faster inference speed, and 80% FLOPs reduction against the model yielding the same performance on the same data set.</sample>
    <sample id="115">L'approccio utilizza segmenti parlati di dimensione lambda.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono che Servin è un giudice e Kea è un baker.</sample>
    <sample id="117">Il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente è la qualità dell'esempio.</sample>
    <sample id="118">The paper presents a new pre-training technique for code-switched NLP tasks, which is important for linguistically diverse communities like India. The authors propose SwitchMLM, a novel MLM technique tuned to the case of code-switching, and architectural changes and auxiliary loss to enhance switch-point information content. They also offer a surrogate method called FrequencyMLM, which does not require access to LID tagged dataset or a LID tagger. The results show that the proposed methods perform the best on sentiment analysis tasks across all language pairs. Probing experiments verify the claim that the proposed methods increase the amount of switch-point information in intermediate layers.</sample>
    <sample id="119">L'articolo si concentra su GPT-4 e RoBERTa come modelli linguistici negli esperimenti estesi.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Gli esempi di inferenza diretta sono quelli in cui si usa il nome esatto del brano, ad esempio "Easy on Me" o il numero di posizione, "il primo".</sample>
    <sample id="122">I fornitori dell'articolo sono affiliati all'Università Fudan.</sample>
    <sample id="123">The research presented by Ying and Zhiyang focuses on improving multi-modal zero-shot learning through instruction tuning. They address the lack of multi-modal instructional datasets by creating MultiInstruct, a benchmark dataset with 62 diverse multi-modal tasks. Using OFA as the base model, they investigate the impact of instruction tuning on performance across seen and unseen tasks. The results show that instruction tuning significantly improves OFA's performance on seen tasks, and transfer learning from natural instruction datasets further enhances performance while reducing sensitivity. The study proposes a new metric called sensitivity to measure the consistency of model outputs. The researchers are currently collecting a larger dataset and will release it in the future.</sample>
    <sample id="124">The presentation discusses the development of a new dataset and training strategy to improve temporal reasoning in large language models (LLMs). The researchers propose the TempReason dataset, which covers three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. They also introduce a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. The results show that their proposed TempT5 model outperforms other instruction-tuned LLMs on the TempReason benchmark, particularly in open book and reasoning QA settings. However, they also note some performance fluctuations over different time periods, which could be related to training data imbalance. Future work will focus on overcoming such biases.</sample>
    <sample id="125">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="126">No, la traduzione della query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico non è stato considerato come un approccio standard. L'approccio standard è stato quello di utilizzare modelli monolingui per il training e l'evaluation.</sample>
    <sample id="127">The paper "Large Language Models Are Reasoning Teachers" proposes a novel technique to transfer reasoning abilities from large language models to much smaller models. The authors introduce a method called Diverse Reasoning, which generates multiple reasoning samples using stochastic temperature sampling. They fine-tune small models with these samples and achieve notable performance on complex tasks, even with the smallest model that has 0.3 billion parameters. The method is highly scalable and poses trade-offs between development costs, inference costs, and the quality of the inference. The paper provides detailed analysis and results on open-source models and includes code and data for future work.</sample>
    <sample id="128">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources is a collaborative work between McGill University, Mila, and Microsoft Research that proposes a diagnostic test suite for knowledge integration in natural language understanding (NLU) tasks. The test suite includes a coreference resolution task designed to probe the ability of models to draw on knowledge available in different sources. The authors evaluate the data set with human study participants and established coreference resolution models, varying the availability of background and entity-specific knowledge at pretrain time and inference time. They show that many coreference resolution models are unable to reason over knowledge from different sources without task-specific training, but some models can successfully integrate knowledge from multiple sources when trained on generic reference resolution data sets. However, even the best-performing models have difficulties with reliably integrating backward knowledge presented only at inference time.</sample>
    <sample id="129">Gli autori hanno fornito un esempio di persona di un gruppo contrassegnato come "un asiata che è un essere umano". Questo esempio illustra come le persone contrassegnate possono essere rappresentate in modo stereotipato e generalizzativo, anche se non sono direttamente negative o tossici.</sample>
    <sample id="130">I modelli che non generalizzano adeguatamente sono quelli che non usano una architettura di transformer, i modelli più piccoli e quelli con meno esempi di allenamento.</sample>
    <sample id="131">I nomi dei set di dati di test non sono forniti nel discorso.</sample>
    <sample id="132">Ci sono tre autori coinvolti nell'articolo: Akshatha, Martin e i collaboratori di McGill University, Mila e Microsoft Research.</sample>
    <sample id="133">L'autore opera con più modalità, non solo con il testo.</sample>
    <sample id="135">ABC-Eval: A Dimensional Approach to Evaluating Conversational AI

Conversational AI models have become increasingly prevalent, but evaluating their performance remains a challenge. This work introduces ABC-Eval, a new dimensional approach to evaluating conversational AI developed by the Emory NLP Lab led by Professor Jinho Choi at Emory University and in collaboration with Amazon Alexa AI.

ABC-Eval annotates behaviors in chat (ABC-Eval) to reduce the subjectivity of human evaluation by explicitly labeling whether each model response expresses certain behaviors, such as responding with irrelevant information or contradicting itself. The method comprehensively covers chat model behaviors that have been suggested to affect chat quality in recent literature.

The study selected four state-of-the-art chat models and evaluated them on 100 human-bot conversations per model using ABC-Eval. For comparison, the conversations were also evaluated using three existing methods: Likert ratings on the turn-level, Likert ratings on the dialogue-level, and dialogue-level pairwise comparisons. The results showed that ABC-Eval behavior labels are more reliable than labels collected by existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. Additionally, ABC-Eval labels are more predictive of overall conversation quality compared to metrics produced by existing methods.

The combination of all ABC-Eval metrics explains over 25% of conversation quality, while the combination of all turn-level Likert metrics explains far less of the quality. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.

Despite the challenges still remaining, ABC-Eval can be leveraged by others in the field as a meaningful step in this direction.</sample>
    <sample id="136">The paper "FERMAT: An Alternative to Accuracy for Numerical Reasoning" by Jasivan and Nafise at the University of Sheffield presents a flexible evaluation set for numerical reasoning tasks. The authors argue that existing benchmarks, which provide accuracy scores or F1 measures, are not informative about the strengths and shortcomings of language models in mathematical ability. They introduce FERMAT, which includes math worded questions extracted from Illinois and CommonCore, and evaluates models on their performance across different arithmetic types, number representations, and mathematical operations. The authors find that most models perform poorly across these aspects, but fine-tuning with math teachers' templates can improve performance. They also investigate the impact of training dependency and diversity, finding that both are important for improving model performance. Overall, the paper provides a more informative alternative to existing benchmarks for evaluating numerical reasoning abilities of language models.</sample>
    <sample id="137">Abstract: We introduce Tell2Design, a large-scale dataset for language-guided floor plan generation. The dataset consists of 5,051 human-annotated and 76,000 artificially generated language instructions for various floor plans. Our goal is to enable users without expertise to design by "telling" instructions, focusing on the floor plan domain. We propose a sequence-to-sequence model that treats the instructions as input and bounding boxes as target sequences, allowing us to handle varying lengths of instructions for different numbers of rooms. Our method outperforms text-conditional image generation baselines with a Micro IoU of 54 and a Macro IoU of 53. Despite a language distribution gap between artificial and human instructions, our method performs significantly better when trained on both types of instructions. This study initiates research on language-guided design generation and provides a foundation for future work in this area.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare conoscenza proveniente da diverse fonti, sia durante il pretraining che durante l'inferenza.</sample>
    <sample id="139">I nomi dei relatori sono Ying e Zhiyang.</sample>
    <sample id="140">Sì, il CoScript è stato sottoposto a controlli di qualità. In particolare, si è richiesto ai worker di crowd-sourced di trovare e revisionare gli esempi sbagliati per garantire la qualità del set di validazione e test.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti poiché solitamente si basano su conoscenze umane e curazione, il che limita il supporto a tipi di traduzioni dipendenti dal contesto e a set di lingue.</sample>
    <sample id="142">Ciao! Parleremo del nostro lavoro su "Risolver espressioni di riferimento indirette per la selezione di entità", in cui introduciamo il Corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro con Felipe Radlinski, Silvia Pareti e Annie Louis. Il nostro obiettivo è capire il linguaggio dei utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai pensato a 'Easy on Me' o a 'I Gotta Feeling'?" Qui, un utente vuole scegliere tra due canzoni. La cosa più ovvio sarebbe usare un riferimento diretto, ad esempio nominando il nome della canzona "Easy on Me" o la sua posizione, "la prima". Ma spesso un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può succedere quando l'utente non ricorda il nome della canzona. O le pronunciazioni sono troppo simili e difficile da distinguere. O quando l'utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti, ad esempio, "il nuovo" o "la canzone che non è energica". Questo è un problema importante per i sistemi conversazionali e anche per valutare le comprensioni di entity delle LLM (Modello Linguistico Grande). Non conosciamo un dataset pubblico più grande per il compito, quindi abbiamo raccolto uno utilizzando annotazione di massa. Il nostro dataset copre tre domini diversi: musica, libri e ricette. Il nostro metodo di raccolta dei dati mette l'accento sulla informalità utilizzando un setup di completamento di fumetti. I fumetti hanno tre bubble di discorso. In primo luogo, Bob dice, "Ricordi quella canzone che stivamo ascoltandoYesterday?". E con quello, Bob stabilisce il contesto del dialogo. In secondo luogo, Alice dice, "Hai pensato a 'Easy on Me' o a 'I Gotta Feeling'?" Che è una domanda alternativa. In terzo luogo, Bob usa un riferimento indiretto per scegliere tra queste due entità, ad esempio, "il nuovo". Forniamo automaticamente i primi due bubble di discorso, ma il terzo è riempito dagli annotatori. Il primo bubble è scelto da pochi prompt manuali per il dominio. Il secondo, che è la domanda alternativa, è generata come segue. Utilizziamo sempre un template semplice. Hai pensato a A o B? Dove A e B sono样例从Wikipedia。Ecco i metodi di campionamento che abbiamo utilizzato. Quando ci si trova più in alto nella lista, le entità diventano più simili tra loro e generalmente è più difficile distinguere tra loro. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia. E infine, quando hanno informazioni simili o attributi su Wikipedia. Ad esempio, lo stesso genere o lo stesso artista per una canzona. Quandostavi davanti a queste due entità, gli annotatori sanno il nome di queste entità, ma non necessariamente sanno di queste entità. Così, quello che facciamo è mostriamo ai sondaggi alcune informazioni di sfondo su queste entità. Per le canzoni, semplicemente mostriamo un link di ricerca di Google per ciascuna canzona e chiediamo agli annotatori di ascoltare almeno alcune di queste canzoni e leggere di queste canzoni. Ecco ad esempio il risultato di ricerca di Google per la canzona "Easy on Me". Per i libri e le ricette, mostriamo alcune informazioni di sfondo da Wikipedia. Per le ricette, inoltre mostriamo le immagini di Wikipedia, in modo che gli annotatori siano informati su come si vedono. Poi chiediamo agli annotatori di scegliere una di queste entità, ad esempio, qui è il primo, e descriverle utilizzando tre a cinque riferimenti indiretti. Ad esempio, il primo con la musica a pianoforte. Ecco alcuni esempi dal nostro dataset. Ad esempio, "quella senza parole", "non quella con il ragazzo di 12 anni", o "quella che è fictionale", o "proviene dall'Azerbaijan", e così via. Il Corpus AltEntities ha 6.000 domande alternative attraverso tre domini, e ha 42.000 riferimenti indiretti. I risultati con il modello T5 XL sono riassunti di seguito. Se il modello linguistico ha accesso alla stessa conoscenza di sfondo degli annotatori, allora l'accuratezza è davvero alta, circa il 92% al 95%. Ma questo non è realistico. Se il modello linguistico ha accesso a qualche conoscenza di sfondo parzialmente sovrapposti, allora l'accuratezza è tra il 82% e il 87%, che è più realistico. Ad esempio, quando il modello recupera la conoscenza di sfondo. Se il modello linguistico ha accesso solo ai nomi delle entità, allora l'accuratezza è solo del 60%, c'è molto spazio per migliorare. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al nostro dataset. Grazie.</sample>
    <sample id="143">L'approccio proposto viene confrontato con le politiche di attesa-k e Local Agreement, applicate a modelli offline, e anche con la migliore architettura specificamente progettata per la traduzione contemporanea.</sample>
    <sample id="144">Mi dispiace, non ho informazioni sulle affiliazioni degli autori dell'articolo.</sample>
    <sample id="145">Il nome della relatrice o del relatore è Jenny.</sample>
    <sample id="146">This paper presents a study on the omission problem in dialogue summarization, which is a subtask of text summarization. The authors analyzed the percentage of summaries that suffer from the omission problem and found that about 70% of the generated summaries have this issue. They also constructed the OLDS dataset, which provides high-quality omission labels for dialogue summarization. The dataset is built upon five existing benchmarks covering five domains. The authors explored three frameworks as baselines, which have different input formats and structures, including pair-wise classification, sequence labeling, and pointer network. They used the Precision, Recall, and F1-score to evaluate their omission detection models. Furthermore, they calculated the percentage of gold emission words that are hit in the detected utterances to measure the word-level omission recall, denoted as WR score. The results indicate that the task is very challenging, which calls for more advanced detection models. The authors also proposed a post-editing method for summary refinement, which concatenates the candidate summary with omission content as input and outputs the refined summary in a sequence-to-sequence manner. From the figure, we find that the performance is largely boosted when the omission is provided. It indicates that the omission detection is a valuable task, and the refinement based on the detected omission is a promising direction for quality improvement in dialogue summarization.</sample>
    <sample id="147">Ci sono tre autori coinvolti nell'articolo: Myra, Esin Durmus e Dan Jurafsky.</sample>
    <sample id="148">Ciao, mi chiamo Sara Papi e sono alla Università di Trento e alla Fondazione Bruno Kessler. In questo momento, presenterò un articolo che si intitola "Attenzione come guida per la traduzione contemporanea" che è un lavoro di collaborazione con Matteo Negri e Marco Turchi. Cos'è la traduzione contemporanea? La traduzione contemporanea, o SimulST, è il processo di tradurre un linguaggio parlato in un testo in un altro linguaggio in tempo reale, enabling la comunicazione tra lingue diverse. E cosa sono i problemi dei modelli attuali di SimulST? Le strutture specifiche vengono addestrate, introducendo moduli aggiuntivi da ottimizzare. Addestramenti lunghi e complicati, ad esempio, addestramenti che coinvolgono obiettivi di ottimizzazione diversi. E addestrare e mantenere più modelli per raggiungere differenti regimi di latenza. Ad esempio, addestrare un modello con un'average di un secondo di latenza e un altro con due secondi di latenza, e così via. Quindi, la nostra soluzione è la seguente: innanzitutto, utilizzare modelli offline ST esistenti senza addestrare nuovamente o adottare strutture specifiche per SimulST. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici. E sfruttiamo il conoscimento acquisito dal modello attraverso del meccanismo di attenzione tra input audio e output testuale. Questo è il meccanismo di attenzione reciproca, e puoi vedere un esempio sulla destra. La nostra soluzione propone EDAtt, o Encoder-Decoder Attenzione, e è una strategia per decidere se emettere o non emettere una traduzione parziale, basata su dove si concentra l'attenzione. Un parola viene emessa se l'attenzione non è concentrata, cioè, la somma della attenzione è inferiore a un certo livello alpha verso le ultime lambda frasi di discorso ricevute, signorina che informazione sufficiente è stabile. Ad esempio, se riceviamo un blocco di discorso che dice "Sto per parlare di..." e il modello predice la traduzione in tedesco, e guardiamo i pesi di attenzione, vedremo che le prime due parole puntano alle prime frasi ricevute, mentre l'ultima parola punti alle ultime frasi ricevute, come lambda frasi di discorso. Questo significa che le prime due parole verranno emesse, poiché la somma dei pesi di attenzione è sopra un certo livello alpha, non emetteremo l'ultima parola e aspetteremo un altro blocco di discorso. Se continuiamo e riceviamo un altro blocco di discorso, e il modello predice altre tre parole e guardiamo quei pesi di attenzione, vedremo che nessuna parola punti alle ultime lambda frasi di discorso. Questo significa che queste tre parole verranno emesse. Se guardiamo i principali risultati di EDAtt, plottiamo i risultati di traduzione contemporanea sul grafico in cui hanno sul lato sinistro il BLEU che misura la qualità della traduzione, e l'average lagging che è misura la latenza, e anche consideriamo la latenza computazionale media che tiene conto del tempo necessario al modello per prevedere l'output. Quindi, vogliamo che le curve siano quanto più elevate possibili su questo grafico. Ma anche volerli spostati verso sinistra. E confrontiamo con strategie popolari che vengono applicate ai modelli offline che sono il strategy Wait-k e l'Local Agreement. E confrontiamo anche con la migliore architettura specificamente pensata per la traduzione contemporanea pre-simultanea. Questi sono tutti i risultati della nostra strategia di traduzione contemporanea in tedesco. E vediamo che supera tutte le strategie applicate ai modelli offline, poiché le curve sono spostate verso sinistra. E vediamo anche che se consideriamo il tempo effettivo trascorso o il tempo computazionale consapevole, che è la più veloce strategia. Se vuoi scoprire di più sulle nostre results, leggi il nostro articolo. E anche rilasciamo open source il codice e i modelli e la traduzione contemporanea per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="149">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="150">MeetingQA: Extractive Question-Answering on Meeting Transcripts

Presented by Archiki, this paper introduces MeetingQA, a new dataset for extractive question answering based on questions asked during meetings and their corresponding answer sentences. Unlike previous works that focused solely on summarization and extracting action items, MeetingQA addresses the significant QA component in meeting discussions.

The dataset is derived from nearly 100 hours of manually transcribed multi-party meetings in the AMI corpus. Questions are selected based on punctuation and filtered for length, while answers are annotated by human annotators. The dataset contains 7.7K questions, with 30% unanswerable, 40% having multi-span answers, and 48% having multi-speaker answers.

Question types include yes/no, opinion-seeking, rhetorical, and multiple speakers, with 70% of multi-speaker answers containing disagreement. Questions and answers are approximately 12 and 35 words long, respectively. Human performance on the test set achieves an F1 score of 84.6.

The paper explores various methods, including context-retrieval for short-context models, single-span and multi-span models, and data augmentation using silver annotations. Fine-tuned models show a 25 F1 point gap from human performance, with short-context models outperforming long-context models. Zero-shot performance shows a 50 F1 point gap from human performance, but silver data augmentation improves results. Larger instruction-tuned models like FLAN-T5 perform comparably to other models.

Error analysis reveals challenges in identifying rhetorical questions, especially in zero-shot settings, and predictions of single-span models contain more irrelevant sentences than multi-span models. Models struggle to identify which speaker answers a question, particularly in zero-shot settings. MeetingQA presents a challenging and interesting domain for NLP research, with far-from-solved tasks in both fine-tuned and zero-shot settings.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang presenteranno il nostro lavoro su MultiInstruct, che migliora l'apprendimento senza supervisione multi-modal tramite l'addestramento con istruzioni. Con i progressi recenti in modelli di lingua ad grandi scale, molte ricerche hanno iniziato a esplorare nuovi paradigmi di impegno dei modelli pre-addestrati per compiti diversi in modo efficiente sia in termini di parametri che di dati. Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli di lingua ad grandi scale di svolgersi su compiti non visti in modo zero-shot seguendo istruzioni naturali. Tuttavia, la maggior parte delle ricerche precedenti sull'addestramento con istruzioni si è concentrata su migliorare le prestazioni in modo zero-shot su compiti solo linguistici, mentre i compiti di visione e multi-modal sono stati lasciati fuori. Pertanto, in questo lavoro ci proponiamo di indagare se l'addestramento con istruzioni a modelli pre-addestrati multi-modal può realmente migliorare la generalizzabilità a compiti non visti multi-modal. Inoltre, alla nostra ricerca, abbiamo scoperto una disparità considerevole nella disponibilità di dataset di istruzioni tra NLP e multi-modal. Esistono più di 1600 compiti di istruzioni solo linguistici. Tuttavia, non esiste un grande numero di dataset di istruzioni multi-modal pubblicamente disponibili. Questo ha motivato la costruzione del dataset di istruzioni multi-modal MultiInstruct. Qui presentiamo MultiInstruct, il primo dataset di istruzioni multi-modal di benchmark che consiste in 62 diverse attività multi-modal coprendo 10 category. Queste attività sono derivate da 21 dataset aperti esistenti e ogni attività è equipaggiata con cinque istruzioni esperte scritte. Per investigare l'addestramento con istruzioni multi-modal su nuestro dataset proposto, prendiamo OFA, un modello pre-addestrato multi-modal unificato, come nostra base model. OFA utilizza un vocabolario univoco per le lingue, i token immagine e le coordinate di un bounding box. Ecco un esempio di istanze dalla nostra attività di MultiInstruct per unificare il trattamento di diverse tipologie di input e output dei dati. Seguendo il metodo di OFA, formuliamo tutti i compiti in un formato univiso di sequenza-voglia. In cui il testo di input, le immagini, le istruzioni e le coordinate del bounding box sono rappresentate nello spazio dei token identico. Ora parliamo di addestramento con istruzioni multi-modal. Per il dataset di addestramento, utilizziamo 53 compiti da 9 gruppi per addestrare e scegliamo 10.000 istanze per ciascun compito. Per il test, riserviamo tutto il gruppo di ragione comune per il test e scegliamo ulteriormente 5 compiti dal gruppo VQ eMiscellanei. Utilizziamo tutte le istanze del set di test per ciascun compito. Inoltre, scegliamo casualmente 20 compiti dal set di test di istruzioni naturali come compito non visto per NLP. Utilizziamo il modello pre-addestrato OFA grande come nostra base model. Durante l'addestramento, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei cinque template di istruzioni. Durante il test per ciascun compito, conduciamo un totale di 5 esperimenti valutando il modello utilizzando una delle cinque istruzioni. In ciascun esperimento, rapportiamo il minimo e il massimo prestazione e la deviazione standard della prestazione attraverso tutti i 5 esperimenti. Se il compito è un compito di classificazione multi-modal, rapportiamo l'accuratezza. Se è un compito di generazione multi-modal, rapportiamo Rouge-L. Per i compiti di NLP, rapportiamo anche Rouge-L. Abbiamo anche introdotto un nuovo metrico chiamato sensibilità. Questo misura la capacità del modello di produrre consistentemente gli stessi output per lo stesso compito indipendentemente dalla variabile leggera nella formulazione dell'istruzione. Ecco i nostri principali risultati. Come possiamo vedere, l'addestramento con istruzioni può migliorare significativamente le prestazioni di OFA su compiti multi-modal visti. Inoltre, il trasferimento di impara da dataset di istruzioni naturali può migliorare l'addestramento con istruzioni. Come possiamo vedere, con l'aumento del numero di compiti, il modello raggiunge prestazioni migliori e contemporaneamente una sensibilità inferiore. Abbiamo anche fatto un esperimento. Utilizziamo un'istruzione contro 5 istruzioni. Come possiamo vedere, utilizzare più istruzioni può migliorare le prestazioni generali del modello e ridurre la sensibilità molto. Questo mostra l'effetto di diverse strategie di addestramento fine su sensibilità del modello. Come possiamo vedere, grazie al trasferimento di impara da dataset di istruzioni naturali, il modello può raggiungere prestazioni molto migliori rispetto al modello OFA originale. Abbiamo anche potuto vedere che il trasferimento di impara da dataset di istruzioni naturali può aiutare OFA a raggiungere prestazioni molto migliori sul dataset di istruzioni naturali. Di conseguenza, proponiamo il primo dataset di istruzioni multi-modal di scala grande con prestazioni notevoli migliorate per OFA e esploriamo diverse tecniche di trasferimento di impara e mostriamo i loro benefici. Designiamo un nuovo metrico chiamato sensibilità. Così, un'altra cosa, stiamo raccolgendo un dataset di istruzioni multi-modal di scala molto grande con circa 150 aggiuntive attività visione-lingua e lo rilanceremo. Questo è un QR code per i nostri dati e il modello. Grazie.</sample>
    <sample id="152">In this presentation, Frederick Riemenschneider introduces new language models specifically designed for classical philology. The project aims to create monolingual and multilingual models that can handle Ancient Greek and Latin texts. The team pre-trained two monolingual models for Ancient Greek (GreBERTa and GreTa) and developed multilingual models (PhilBERTa and PhilTa) that can process Ancient Greek, Latin, and English data. They gathered pre-training data from Open Greek &amp; Latin, the Internet Archive, Corpus Corporum, and related English texts. Benchmarking results show that their models outperform current state-of-the-art models in tasks such as part-of-speech tagging, dependency parsing, and lemmatization. The team also analyzed how T5's encoder behaves and investigated the implications of multilinguality in their models. Overall, the project presents powerful language models for classical philology that can handle both monolingual and multilingual texts.</sample>
    <sample id="153">The paper "Resolving Ambiguities in Text-to-Image Generative Models" by Ninareh Mehrabi and colleagues presents a framework for mitigating ambiguities in prompts provided to text-to-image models. The authors curate a benchmark dataset of ambiguous prompts, which are then disambiguated using either clarifying questions or different possible visual setups. The disambiguated prompts are then used to generate images, which are evaluated using a VQA model to determine if they are faithful to user intention. The results show that the proposed framework has a positive effect on faithful generation and is in agreement with human evaluation.</sample>
    <sample id="154">I fornitori dell'articolo sono l'Università di Trento e la Fondazione Bruno Kessler.</sample>
    <sample id="155">The name of the relator is Javad Hosseini.</sample>
    <sample id="157">The paper introduces a dialogue summarization model called SDDS (Static-Dynamic Structure Fusion Graph) that aims to distill salient information from a dialogue context into a concise summary. The model addresses the limitations of existing methods that rely on pre-computed static graph structures, which can be inaccurate and inflexible. SDDS employs an Utterance Encoder to encode utterances into vector representations and constructs static graphs using heuristic dialogue structure modeling methods. It then uses a Static-Dynamic Graph module to capture semantic relationships between utterances based on their deep vector representation. Finally, a pre-trained language model generates the final summary by fusing the static and dynamically learned dialogue structures. The proposed model is evaluated on several datasets and achieves state-of-the-art performance in dialogue summarization tasks.</sample>
    <sample id="158">The coreference resolution task involves identifying mentions of entities in a document and clustering them based on their referents. Conventional methods have quadratic complexity, while cache-based methods reduce it to linear level using a fixed-size cache with eviction policies like LRU. However, in long documents, topic switches cause high cache misses due to scattered mentions of entities. The proposed dual cache has a local cache with LRU policy for local entities and a global cache with LFU policy for global entities. It evaluates the frequency of new or updated entities and adds them to the appropriate cache. Dual cache outperforms single cache methods and significantly reduces cache misses, making it the most cost-effective option.</sample>
    <sample id="159">Ciao, gente. Sono Koustav Sinha e sono felice di benvenuti in nostra discussione sulla nostra pubblicazione ACL 2023. Giudizi sulla accettabilità dei modelli di lingua non sono sempre robusti al contesto. Questo è un lavoro congiunto con John Gauthier, Aaron Mueller, Kanishka Misra, Karen Fences, Roger Levy e Adina Williams. In questo lavoro, rivediamo i paradigmi minimali. Il paradigma minimal evaluatesling modelli di lingua su giudizi di accettabilità, che possono anche includere grammatica come BLiMP, SyntaxGym o accettabilità in termini di stereotipi come CrowS pairs. In questo paradigma minimal, la modalità di valutazione dei modelli di lingua è di mostrare un esempio di frase accettabile o grammatical e quindi un esempio di frase accettabile o grammatical. E l'obiettivo è che il modello assegna maggior probabilità alla frase accettabile. La pipeline MPP corrente non ci permette di valutare la accettabilità dei modelli lungo tutto il contesto. Ora, i modelli di lingua stanno diventando sempre più lunghi e lunghissimi. Così è cruciale che possiamo valutare la accettabilità dei modelli lungo tutto il contesto. Quello che stiamo cercando di fare qui è di rivedere la pipeline MPP per chiedere ai modelli di valutare la accettabilità lungo sequenze sempre più lunghe. Quello che facciamo è di simulare queste sequenze più lunghe. Per farlo, rivediamo i dataset stessi e creiamo frasi scegliendo frasi accettabili o non accettabili da quei dataset. Ad esempio, qui abbiamo scelto un paio tipici di frasi grammaticalizzate dal dataset BLiMP per il caso Adjunct Island. Ciò che facciamo è di creare sequenze più lunghe accettabili e non accettabili aggiungendo una prefisso grammaticalizzata a entrambe le query accettabili e non accettabili. Possiamo fare lo stesso scegliendo frasi non accettabili dal medesimo matching e possiamo anche fare lo stesso scegliendo frasi da un subset diverso o da un dataset diverso. Quello che chiamiamo scenario mismatch è quando le frasi sono sempre relevanti ma non provenienti dal subset di dataset che stiamo valutando. Infine, possiamo anche scegliere frasi da un dominio completamente diverso come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono influenzati dal contesto, se il contesto proviene da un subset diverso del dataset o se è completamente irrelevante per la query che stiamo analizzando. Come fa il modello? Innanzitutto, guardiamo le frasi di Wikipedia che sono completamente irrelevanti per la query attuale e vediamo che i giudizi MPP sono relativamente stabili. Aumentiamo la lunghezza del contesto fino a 1024 per massimizzare i modelli OPT e GPT-2. Vediamo qui la linea rossa punteggiata che i giudizi MPP sono relativamente stabili. Ora, cosa succede quando scegliamo frasi dal medesimo dataset? Qui scegliamo o creiamo frasi dal dominio accettabile o non accettabile dal medesimo dataset BLiMP o SyntaxGym. Qui vediamo che i giudizi MPP aumentano o diminuiscono significativamente quando aggiungiamo prefissi accettabili o non accettabili. Ma quando scegliamo prefissi con la stessa struttura, cioè scegliendo frasi dal medesimo fenomeno in BLiMP o SyntaxGym, vediamo un'enorme aumentazione o diminuzione dei giudizi MPP del modello, dipendendo dal prefisso accettabile o non accettabile scelto. Questo effetto è molto grande e aumenta attraverso la lunghezza del contesto, probabilmente influenziando i modelli con finestre di contesto più grandi. Perché i prefissi con la stessa struttura hanno un tale effetto così forte sui giudizi del modello? Abbiamo fatto una serie di analisi in cui abbiamo perturbato l'input sentenza preservando la struttura rilevante ma aggiungendo rumore all'input. Dopo aver fatto queste perturbazioni, vediamo che nessuna di queste rumorose cambia realmente il modello in termini di come mostri i giudizi MPP. In pratica, troppo che i modelli sono sensibili alle sentenze perturbate in modo simile. Quindi, quando perturbiamo le frasi in un dominio accettabile, vediamo un aumento simile in tutte le perturbazioni e quando perturbiamo le frasi in un dominio non accettabile, vediamo un calo dei giudizi MPP in modo simile. Quindi, le informazioni principali della nostra opera è che i modelli di lingua sono sensibili a caratteristiche sintattiche e semantiche latenti che si condividono tra le frasi. E la valutazione MPP attualmente con input singoli e brevi non può affatto catturare completamente il conoscimento astratto dei modelli lungo tutto il contesto. Ti prego di leggere il nostro articolo per maggior dettaglio sulle nostre esperienze. Grazie per l'attenzione.</sample>
    <sample id="160">In primo luogo, il primo passaggio del metodo mappa ciascun token di input con un multiset unordered di token che appariranno nell'output.</sample>
    <sample id="161">In CoScript, ci sono 55.000 script rappresentati.</sample>
    <sample id="163">Il miglior metodo di allineamento per DEplain è il metodo MASSalign.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che è molto più economico rispetto all'apprendimento supervisionato tradizionale, in cui i dati devono essere manualmente etichettati. Invece, i dati vengono etichettati utilizzando fonti di etichettatura deboli, come regole semplici, banche di conoscenza o sondaggi di pubblico in bassa qualità. Questo rende l'annotazione dei dati molto più economica, anche se i dati etichettati sono anche più rumorosi.</sample>
    <sample id="165">Our paper, "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations," presents an unsupervised learning method called LiPoR (Likelihood Learning with Posterior Regularization) for abductive reasoning. In a closed-world setting, we aim to identify plausible explanations from a candidate set without supervision regarding their plausibility. We introduce a regularizer that enforces the mutual exclusivity of explanations, which is a significant characteristic of explanations in abductive reasoning. Our method outperforms previous best unsupervised approaches and even surpasses zero-shot GPT-3 baselines by over 4 absolute points in accuracy on the AlphaNLI dataset.</sample>
    <sample id="166">The paper presents a new framework for image retrieval from linguistically complex text, which is a challenging task due to the similarity of images and the length of descriptions. The proposed method combines the Divide-and-Conquer strategy with Dual-Process Theory, inspired by human thinking systems. It consists of three modules: Proposition Generator, Visual-Linguistic Interactor, and Neural-Symbolic Reasoner. The first module decomposes complex propositions into simple ones, while the second module interacts visual and linguistic information. The third module integrates reasoning states and results of simple propositions to obtain the final solution. Experimental results show that the proposed method outperforms other baselines. The paper also suggests that neural symbolic calculation may improve compositional reasoning and planning in large language models, and that Dual-Process Theory could be integrated with Divide-and-Conquer.</sample>
    <sample id="167">I documenti in DEPLAIN-web sono stati allineati sia manualmente che automaticamente. In particolare, 750 documenti sono stati allineati manualmente e altri 750 documenti sono stati allineati utilizzando metodi di allineamento automatici. Questo ha portato alla creazione di un totale di 30.450 coppie di frasi parallele.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo notizie dal 2020 e annotandole con le stesse linee guida di annotazione utilizzate per il CoNLL-2003.</sample>
    <sample id="169">In this paper, we present the first systematic study of large language model prompting for machine translation. We evaluated the performance of PaLM using the best practices of the MT community and compared it to state-of-the-art systems. We found that the quality of the examples used in prompting is more important than the similarity to the source sentence. We also observed that PaLM's fluency is comparable to state-of-the-art systems, but its accuracy is lower due to omission errors. Overall, PaLM comes close to a commercial system in terms of fluency, but still has room for improvement in accuracy.</sample>
    <sample id="170">Ciao a tutti, il mio nome è Yusen Zhang e sono dalla Università di Pennsylvania. Oggi presenterò il nostro lavoro "XSemPLR: Parsing Semantico Cross-Lingua in Più Linguaggi Naturali e Rappresentazioni del Significato". La parsing semantica è un compito che consiste nell'costruire rappresentazioni semantiche di query dell'utente, come SQL e Lambda Calculus. Il parsing semantico cross-lingua è il compito di tradurre query in più linguaggi naturali in più rappresentazioni del significato. Come illustrato in questa figura, dobbiamo tradurre la query in più linguaggi naturali utilizzando modelli neurali per SQL, Lambda o FunQL, eccetera. I modelli esistenti di parsing semantico cross-lingua sono proposti e valutati su dataset di task e applicazioni limitati. Ad esempio, ci sono molti copertura su certi linguaggi naturali, ma mancano le coperture su certi rappresentazioni del significato. Il calcolo Lambda è mancante o è stato valutato solo su certi modelli neurali. Per questo, proponiamo XSemPLR. Fornisce un insieme di dati uniforme XSemPLR per il parsing semantico cross-lingua in più linguaggi naturali e rappresentazioni del significato. Contiene 9 dataset in diverse domande, 5 task di parsing semantico, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche. Per valutare meglio il nostra benchmark, consideriamo i sei ambienti di addestramento e valutazione. Il primo è Translate-Test. Utilizziamo l'API Google Translate per tradurre il linguaggio di origine al linguaggio di destinazione, quindi utilizziamo modelli monolingui per addestrare e valutare. Ad esempio, addestriamo il modello inglese su query inglese e durante l'inferenza traduciamo le query tedesche utilizzando l'API in inglese e quindi utilizziamo il modello addestrato per prevedere il SQL. Anche testiamo il modello monolingue. In questo ambiente, il linguaggio di origine è lo stesso del linguaggio di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche il setting Monolingual Few-shot, in cui addestra modelli monolingui con solo il 10% dei dati di addestramento. Testiamo anche il modello multilingue, che addestra un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme le query tedesche, inglese e cinese per addestrare un modello multilingue. Durante l'inferenza possiamo utilizzare questo modello per tradurre query tedesche o cinesi, eccetera. Consideriamo anche il transfer cross-lingua Zero-shot e Few-shot. Addestra su un linguaggio di origine e trasferisce su un altro linguaggio. Durante l'addestramento, addestra su query inglese o la combinazione di query inglese e tedesche Few-shot per addestrare un modello multilingue per prevedere l'output SQL. Abbiamo anche scoperto molti risultati interessanti. Riguardo l'analisi dei modelli monolingui, valutiamo due gruppi di modelli, tra cui Encoder-PTR che sta per Encoder Multilingue Pre-addestrato con Decodiciere basato su Pointer, come XLM-R + PTR e mBERT + PTR. E valutiamo anche Encoder-Decoder, che è un modello Encoder-Decoder Multilingue Pre-addestrato, come mBART e mT5. Abbiamo scoperto che Encoder-Decoder ottiene il migliore prestazione su tutti i nove dataset. Abbiamo anche valutato mT5 e XLM-R + PTR in un setting multilingue. Abbiamo scoperto che Encoder-Decoder o Encoder-PTR possono essere migliorati addestrando in una combinazione di diverse lingue. Abbiamo scoperto che la maggior parte delle lingue naturali principali possono ottenere prestazione migliore, tranne che il prestazione inglese scende in sette dataset e guadagna in tre dataset. Questo è conosciuto come "Curse of Multilinguality". Abbiamo anche confrontato il gap di performance cross-lingua. In questa figura, la linea blu rappresenta il transfer cross-lingua Few-shot. La linea gialla rappresenta il transfer cross-lingua Zero-shot. Mentre la linea verde rappresenta il setting Monolingual. Abbiamo scoperto che, confrontando la linea verde e la linea gialla, il setting Zero-shot ha un prestabilito di prestazione di transfer cross-lingua significativo, e confrontando la linea blu e la linea gialla, con il setting Few-shot il gap di prestazione di transfer si riduce rapidamente. Abbiamo anche scoperto altri findings interessanti. Ad esempio, Encoder-Decoder supera precedenti opere o raggiunge risultati simili. Addestrare su lingua naturale inglese può rafforzare notevolmente le prestazioni del Few-shot in lingue di destinazione, e abbiamo scoperto che modelli multilingue come Codex e BLOOM sono ancora inadeguati per task di parsing semantico cross-lingua. In sintesi, costruiamo XSemPLR, un benchmark unificato per il parsing semantico cross-lingua con più linguaggi naturali e rappresentazioni del significato. Condurre un studio di benchmark esaustivo su tre tipi di modelli multilingue rappresentativi. I nostri risultati hanno molti findings interessanti. Ecco tutto. Sono felice di vedere che hanno capito bene!</sample>
    <sample id="171">I lavori connessi in tal senso sono quelli che proppongono soluzioni per proteggere la proprietà intellettuale dei servizi di embedding basati su modelli di linguaggio. Questi soluzioni includono la creazione di segni distintivi o marche nascoste che possono essere rilevate senza compromettere l'utility dei servizi di embedding.</sample>
    <sample id="172">No, multilingue lingua modelli come Codex o Bloom sonosi insufficianti per la CLSP.</sample>
    <sample id="174">The paper "ArgAnalysis35K: A large-scale dataset for Argument Quality Analysis" presents a unique dataset that addresses the limitations of existing datasets in the field of argument quality analysis. The dataset consists of 35,000 argument-analysis pairs, with arguments sourced from high-quality tournaments, expert debaters, intermediate debaters, and novice debaters. It also includes a diverse range of arguments on 24 themes, capturing as many motions as possible using various sources. The dataset introduces the concept of analysis, which combines claims, premises, and other elements to explain an argument better. It also incorporates instance-based annotator reliability and a relevance model to capture the relevance of each argument to a particular theme. Overall, the dataset provides a more diverse, reliable, and comprehensive resource for evaluating argument quality.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni induce l'allegamento tra input e output durante il training. Inoltre, utilizza un rilassamento continuo GPU-amico per approssimare la soluzione del problema "Viaggiatore di Ferro" e rendere più fattibile il calcolo della permutazione con la quale è possibile imparare linguisticamente una soluzione più plausibile.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita come la capacità del modello di trattare in modo equo e non discriminante diverse gruppi di persone, indipendentemente dalla loro età, genere, razza, orientamento sessuale, religione, etc. Un modello NLP equo dovrebbe essere capace di fornire predizioni accurate e pertinenti per tutti i gruppi, senza favorire o discriminare alcun gruppo specifico.</sample>
    <sample id="177">Il relatore è Yanis Labrak.</sample>
    <sample id="178">Il nome del relatore è Koustav Sinha.</sample>
    <sample id="179">This paper presents SymbolicToM, a method to improve Theory of Mind reasoning skills in large language models (LLMs) using explicit graphical representations. The method leverages off-the-shelf NLI and OpenIE models to compute belief graphs for all combinations of characters up to a predefined maximum Theory of Mind level. These graphs are used to efficiently answer questions by performing recursion over the graph and feeding it to a language model. Experiments show that SymbolicToM improves performance across various LLMs, including GPT-3, Macaw, and Flan-T5-XXL, on both in-domain and out-of-domain datasets. In particular, it shows significant gains on out-of-domain story understanding and remains beneficial on the new linguistic diversity dataset, ParaphrasedToMi.</sample>
    <sample id="180">Il nome della relatrice o del relatore è Myra.</sample>
    <sample id="181">In this paper, we introduce a new problem called constrained language planning, which involves generating step-by-step instructions for specific goals with multi-faceted constraints. We evaluate and improve the constrained language planning ability of large language models by acquiring specific goals with human-in-the-loop data acquisition using InstructGPT. We find that all language models achieve unsatisfactory results on planning for specific goals, and the faithfulness to the constraints cannot be guaranteed. To address this issue, we adopt the idea of over-generate-then-filter to improve generation quality. We first show constraint types with examples for InstructGPT and obtain specific goals based on the seed abstract goals. Then, InstructGPT over-generates K scripts for specific goals. Next, a filter model is developed to select the faithful scripts. We convert scripts and goals into InstructGPT embeddings and calculate the cosine similarity as similarity scores to measure semantic similarity. In addition, we reward the script that contains the keywords of the target constraint. We only keep the script if the target goal scores the highest in the goal set. With our method, InstructGPT can generate scripts of higher quality. Our method greatly improves the planning ability both in semantic completeness and faithfulness to the constraint. Since large language models are costly to deploy, it's essential to enable language planning ability of smaller and specialized models. Creating the dataset is an essential step to this end. However, previous studies do not enable planning for specific goals and manual dataset annotation is expensive. Thus, we follow the idea of symbolic knowledge distillation, to distil constrained language planning datasets from large language models. We apply our method for building a dataset of constrained language planning, named as CoScript. In total, we generate 55,000 specific goals with scripts. To ensure the quality of the validation and test set, we ask crowd-sourced workers to find and revise the incorrect samples. This figure shows the constraint distribution of CoScript. We find CoScript shows high pluralism in the generated specific goals. With CoScript we can try smaller but specialized models for constrained language planning. We find that T5 fine-tuned on CoScript can generate scripts of higher quality than most large language models, indicating that smaller models can surpass larger models when properly trained on suitable datasets.</sample>
    <sample id="182">Il tropicalismo è un trope che si riferisce alla rappresentazione delle donne latine come "vibranti" e "curvaceous", associandole a un'immagine esotica e sessuata. Questo tipo di rappresentazione è considerato discriminante e discriminatoria, poiché definisce queste donne solo in relazione alla loro identità etnica e non le rappresenta in modo equo e completo.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando un approccio basato su istruzioni e prompt. Hanno creato personaggi immaginari utilizzando un prompt come "Immagina che tu sia una donna asiatica. Descrivi te stesso." Questo approccio è stato scelto per la sua generalizzabilità a qualsiasi demografia e per la capacità di generare modelli di linguaggio che non sono negativi o tossici nel senso tradizionale del termine, ma rivelano comunque模式和刻板印象。</sample>
    <sample id="184">In questo lavoro, è stato utilizzato CXMI (Contextual Mutual Information) per misurare l'utilizzo del contesto in traduzioni. CXMI misura quanto informazione viene fornita dal contesto alla modellistica di traduzione, cioè quanto il contesto influisce sulla traduzione del testo. In questo studio, è stata estesa la misura CXMI per includere P-CXMI (Pointwise CXMI), che può misurare l'utilizzo del contesto al livello delle parole.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato per la lingua francese in campo biomedico, mentre ChuBERT è un modello basato su dati anonimizzati ottenuti dal data warehouse del Nantes University Hospital.</sample>
    <sample id="187">Ci sono due autori coinvolti nell'articolo: Ying e Zhiyang.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un approccio che implica aggiornare il modello con i dati più recenti raccolti in ogni round di immissione attiva, invece di accumulare tutti i dati raccolti finora. Questo approccio è utile per il trasferimento di impegno da un dominio diverso e è particolarmente utile per le annotazioni del dominio.</sample>
    <sample id="189">L'obiettivo del set di dati è di comprendere come gli utenti usano la lingua quando vogliono fare un选择。</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando tecniche di model extraction, come l'input specifico e l'analisi dei modelli. Questo può essere fatto utilizzando un insieme di input predefiniti o generando un insieme di input personalizzati per testare il modello. Una volta ottenuti i parametri del modello, l'utente malintenzionato può utilizzarli per creare un proprio modello simile o per utilizzarli per altre finalità non autorizzate.</sample>
    <sample id="191">Ci sono tre autori coinvolti nell'articolo: Sara Papi, Matteo Negri e Marco Turchi.</sample>
    <sample id="192">The presentation by Yang Luo introduces the CAME (Confidence-guided Adaptive Memory Efficient Optimization) optimizer, which aims to address the challenge of balancing fast convergence and low memory usage in large language model training. The author explains that while adaptive gradient-based methods like Adam are effective for convergence, they require significant memory for maintaining first and second moment estimates. In contrast, memory-efficient optimizers like Adafactor reduce auxiliary memory usage but at the cost of performance. The presentation outlines how CAME incorporates an adaptive confidence-based updating mechanism guided by the residual between predicted and generated updates, aiming to minimize errors and improve stability. Experiments on BERT, GPT-2, and T5 show that CAME achieves better validation accuracy and efficiency compared to Adam and Adafactor, particularly in large batch training scenarios.</sample>
    <sample id="193">Il numero di annotatori utilizzati per creare il set di dati iniziale non è specificato nel testo fornito.</sample>
    <sample id="194">I ricercatori che hanno collaborato per creare l'articolo "NLPositionality" sono affiliati all'Università di Carnegie Mellon, all'Università di Washington e all'Allen Institute for AI.</sample>
    <sample id="195">Our work, "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering," aims to improve question answering by providing explanations for the answers. We address the limitations of previous methods, such as neuro-symbolic and decompose-based approaches, by proposing a novel framework called RoHT (Reasoning over Hierarchical Question Decomposition Tree). RoHT consists of two stages: firstly, building a Hierarchical Question Decomposition Tree (HQDT) that represents the hierarchical structure of a complex question; secondly, performing probabilistic reasoning over HQDT to fuse knowledge from different sources at various levels. We evaluate RoHT on two datasets, KQA Pro and Musique, showing significant improvements over existing methods. RoHT outperforms TransferNet by a large margin on both datasets, demonstrating the effectiveness of explicit decomposition and the benefits of supplementing text information with knowledge from KBs.</sample>
    <sample id="196">"I saw Bart and Lisa"</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli che utilizzano ABC-Eval per valutare la qualità del chat. Questi modelli sono in grado di misurare le tassi di errori tematici, come ignorare il partner o fornire informazioni irrelevanti, contradire se stessi o il partner, hallucinare fatti sbagliati o violare le conoscenze di senso comune, e quando il modello riesce o fallisce a dimostrare empatia.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria perché i modelli linguistici stanno diventando sempre più grandi e hanno una finestra di contesto più grande. Questo rende cruciale valutare la capacità dei modelli di accettare le frasi lunghe e complesse.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset, mentre ha migliorato le prestazioni in tre di essi. Questo fenomeno è noto come "Curse of Multilinguality".</sample>
    <sample id="200">Yes, the annotators are aware of the entities in advance.</sample>
    <sample id="201">Sono state utilizzate metriche di traduzione automatica (MT) state-of-the-art e anche valutazioni umane basate su esperti.</sample>
    <sample id="202">Sì, il regresso nella generalizzazione influisce su specifici tipi di NER. Come menzionato nel paper, i modelli che utilizzano la rete neurale con l'architettura Transformer, dimensioni più grandi e maggiori quantità di esempi di addestramento hanno un'ottima capacità di generalizzare a nuove informazioni.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i modelli e i set di dati possono rappresentare giudizi e opinioni di persone reali, che possono essere inclini a certe positionalità. Questo può influenzare le decisioni dei ricercatori e i risultati dei processi di ricerca, rendendo i modelli e i set di dati più sensibili a determinate popolazioni rispetto ad altre. Studiare la posizionalità dei modelli e dei set di dati è dunque fondamentale per capire come queste positionalità sono sbagliate e come migliorare la NLP in modo più inclusivo.</sample>
    <sample id="204">I modelli di lingua multilingue come BLOOM sono stati adattati utilizzando un adattatore o con una messa a punto integrale.</sample>
    <sample id="205">This paper investigates the political bias propagation pipeline from pretraining data to language models to downstream tasks. The authors propose to evaluate the political leaning of language models and investigate how they perform on downstream tasks, such as hate speech detection and fake news detection. They conduct a controlled experiment by pretraining language model checkpoints on partisan corpora separated into news and social media, further divided into their political leaning. The results show that language models have varying political leanings and can pick up the polarization prevalent in modern society. The paper highlights the fairness issue resulting from language model political leanings and discusses the dilemma of sanitizing political opinions in language model training data without risking censorship or exclusion.</sample>
    <sample id="206">In questo studio, i ricercatori hanno utilizzato due modelli per il trasferimento dell'apprendimento: la classificazione di posizione di dibattito indipendente dissonanza e la classificazione binaria di espansione e confronto (CE) per iniziare il processo di imparazione attiva.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono quelli che non hanno un overlap con i dati di addestramento del modello. Questo è importante per evitare che i dati di test siano utilizzati anche per l'addestramento del modello e influenzino i risultati.</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine: 1) gli studiosi dovrebbero indirizzare la propria attenzione verso i pregi stereotipati e le narrazioni essenziali, 2) dovrebbero utilizzare un occhio di giudizio intersectionale per studiare i bias e i danni, e 3) dovrebbero aumentare la trasparenza riguardo ai metodi per la mitigazione dei bias.</sample>
    <sample id="209">Il metodo proposto migliora la capacità di pianificazione in modo significativo rispetto al metodo di riferimento. In particolare, il nuovo metodo migliora sia la completezza semantica che la fedeltà alle vincoli. Questo è dovuto alla capacità del nuovo metodo di generare script di qualità più alta e di selezionare i script più fedeli ai vincoli specifici utilizzando un modello di filtraggio sviluppato.</sample>
    <sample id="210">Il nome della relatrice o del relatore è Shuheng.</sample>
    <sample id="211">Sì, i risultati e il set di dati presentati nell'articolo possono essere utilizzati come parametri di riferimento per valutare i metodi di allineamento automatico e per l'automatica semplificazione del testo. L'articolo include anche code per eseguire gli esperimenti, consentendo agli altri ricercatori di replicare i risultati e di migliorare ulteriormente i metodi proposti.</sample>
    <sample id="212">Nell'articolo, viene utilizzato un modello più piccolo chiamato T5.</sample>
    <sample id="213">OFA, un modello pre-addestrato multi-modal, viene utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali.</sample>
    <sample id="215">The talk by Adam Przepiórkowski focuses on the dependency structure of coordination in linguistics. It discusses various approaches, such as universal dependencies, meaning text theory, Prague approach, and Hudson's Word Grammar, which differ in how they handle coordinate structures. The aim of the paper is to argue for symmetric structures of coordination against asymmetric ones.

Przepiórkowski introduces the principle of dependency length minimization, which states that shorter dependencies are preferred. He uses examples to illustrate this principle, such as "Marge read this absolutely fascinating book about bees yesterday" being more acceptable than "Marge read yesterday this absolutely fascinating book about bees." This is because the latter violates the general grammatical principle that direct objects should be next to the verb but satisfies the principle of dependency length minimization.

The paper extracts statistics from the enhanced version of the Penn Treebank and confirms previous observations about left conjuncts tending to be shorter. The tendency grows with the length difference between the two conjuncts. However, when the governor is on the right or absent, the effect disappears. This supports the argument for symmetric structures of coordination.</sample>
    <sample id="217">The paper "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation" by Weihao Zeng, Lulu Zhao, and Keqing He from Beijing University of Posts and Telecommunications presents a method for generating controllable dialogue with multiple attributes. The authors address the limitations of existing methods that focus on single attributes or use specific labels for continuous attributes. They propose DCG (Disentangled Controllable Generation), which learns attribute concepts from seen values and uses disentanglement loss to disentangle different attribute combinations. A unified evaluation framework, MAE (Multi-Attribute Evaluation), is introduced to assess the performance of the model at different granularities of attributes. The authors establish two benchmarks and demonstrate the effectiveness of their method through experiments. The results show that DCG outperforms all other baselines in attribute controllability and text equality, and successfully tackles the challenges of compositional generalization for multi-attribute controllable dialogue generation with only a small drop on E-ACC and A-ACC metrics.</sample>
    <sample id="218">I fornitori dell'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">The paper "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports" by Jia-Huei Ju and colleagues proposes a highlighting task and a multi-stage pipeline to uncover financial signals in Form 10-K reports. The authors address the challenge of identifying differences between reports from year to year, given that about 80% of tokens are similar. They define reference-to-target structures and introduce a highlighting model to compare and contrast the context between targets and references. The model predicts word importance, allowing for the measurement of performance. The proposed pipeline consists of document segmentation, relation recognition, and two fine-tuning stages: out-of-domain and in-domain. The authors use an external dataset (eSNLI) for out-of-domain fine-tuning and revised pairs with pseudo positive labels for intermediate fine-tuning. They evaluate their domain-adaptive highlighting model using precision and PCC metrics on eSNLI and FINAL datasets. The results show that their model achieves the best performance on FINAL and preserves generalization capability. Future works include improving effectiveness, adding more features, and enhancing information retrieval techniques.</sample>
    <sample id="220">I fornitori dell'articolo sono Vasudha e i collaboratori di lei. L'articolo è stato accettato per essere presentato come un articolo lungo in ACL 2023.</sample>
    <sample id="221">Il test è stato effettuato tra le lingue tedesca e inglese.</sample>
    <sample id="222">This work investigates challenges and interventions for domain adaptation in open-domain question answering. It explores different data interventions, identifies the type of dataset shift a new domain exhibits, and determines effective data interventions for specific types of shifts. The study uses a general-purpose Wikipedia-based source domain to train both retriever and reader models and tests their generalizability across seven target datasets spanning six domains. Two overarching methods are considered: zero-shot and few-shot. Few-shot methods use a few examples from the target domain to prompt large language models for generating more examples, while zero-shot techniques control interactions among three random variables in open-domain QA by keeping two variables fixed while varying the other one. The study finds that few-shot adaptations improve reader performance by up to 24% and that certain data interventions are effective based on the type of shift a target dataset exhibits.</sample>
    <sample id="223">Il relatore è Shangbin, un studente dottorando all'Università di Washington.</sample>
    <sample id="224">Durante gli esperimenti, sono stati studiati due modelli: il modello long-mBART per la produzione di semplificazioni a livello di documento e il modello mBART base per la produzione di semplificazioni a livello di frase.</sample>
    <sample id="225">Per scopi di addestramento, 53 attività da 9 gruppi vengono utilizzate, mentre per i test, 10.000 istanze per attività sono utilizzate. Per test, si riservano tutti i gruppi di ragione comune per testare e si selezionano ulteriormente 5 attività dal gruppo VQ e da gruppi diversi.</sample>
    <sample id="226">Ci sono due autori coinvolti nell'articolo: Regina Stodden e Omar.</sample>
    <sample id="227">Grounded language understanding is a challenging task in NLP that involves mapping natural language expressions into executable plans or programs. While recent language models have achieved great success, they lack grounding during pre-training, which makes the task particularly challenging. Existing research typically uses language models to directly generate plans, but this approach often results in invalid or ungrammatical plans. The proposed framework, Pangu, focuses on discrimination instead of generation, where a symbolic agent proposes candidate plans and a language model scores and ranks them. This approach allows language models to excel in discrimination tasks and achieves outstanding performance across different settings, including fine-tuning and in-context learning. Pangu's strong generalizability under non-i.i.d. settings may be attributed to its ability to handle both seen and unseen structures without overfitting.</sample>
    <sample id="228">Gli autori hanno effettuato i test su quattro set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="229">The paper by Gabriella Skitalinskaya and Henning Wachsmuth presents a study on detecting improvable claims for argumentative writing support. The authors focus on the challenges of working with revision-based data, such as representativity and reliability, model complexity and architecture, contextual information, and topical and user bias. They explore how to model the quality of argumentative text based on implicit revision patterns found in collaborative online debate platforms such as Kialo. The paper concludes that revision-based data can be employed effectively for the given tasks, and modeling the distance between two claimed versions is beneficial for detecting suboptimal claims.</sample>
    <sample id="231">NACHOS is a data set of medical crawled data from the web, used for training DrBERT, the first biomedical model in French.</sample>
    <sample id="232">Il nome del relatore è David Vilar.</sample>
    <sample id="233">Simultaneous speech translation (SimulST) is the process of translating spoken language into text in another language in real time, enabling cross-language communication. Current SimulST models have issues with long and complicated training procedures, requiring different optimization objectives and multiple models for different latency regimes. The proposed solution is EDAtt, Encoder-Decoder Attention, which uses existing offline ST models without re-training or specific architecture for SimulST. EDAtt decides whether to emit partial translations based on attention mechanism between audio input and textual output. The strategy considers cross-attention weights and emits words if their sum is below a certain threshold alpha towards the last lambda speech frames. EDAtt outperforms popular strategies applied to offline models in terms of translation quality and latency, and it is the fastest strategy when considering actual elapsed time or computational-aware time. The paper includes results on German and compares EDAtt with state-of-the-art architecture specifically tailored for simultaneous pre-translation. The code and models are released as open source to facilitate reproducibility of the work.</sample>
    <sample id="234">La strategia del prompting ha un grosso impatto sui risultati. Come si può vedere in un semplice esperimento, dove si sono fornite due diverse istruzioni per ogni frase, la maggioranza delle frasi (516 su 1000) ha registrato una differenza di più di un punteggio BLEURT. In casi estremi, questa differenza può arrivare a 40 punteggii BLEURT. Quindi, è importante selezionare una buona strategia di prompt.</sample>
    <sample id="235">I fornitori di servizi di traduzione e traduttori professionali.</sample>
    <sample id="236">Ciascun compito è equipaggiato con cinque istruzioni scritte da esperti.</sample>
    <sample id="237">I ricercatori proppongono un insieme di test diagnostiche per valutare la capacità dei modelli di integrare e utilizzare sia le informazioni acquisite in precedenza (pretraining) che quelle fornite in tempo reale (inference time).</sample>
    <sample id="238">The video introduces MeetingBank, a new benchmark dataset created by Yebowen Hu from the University of Central Florida. The dataset addresses two major challenges: obtaining high-quality meeting summaries and locating trustworthy resources for public meetings. MeetingBank contains 1,366 City Council meetings with nearly 7,000 instances, including meeting transcripts, reference summaries, and other useful resources. The dataset is used to evaluate various summarization systems, including extractive and abstractive models, using different metrics such as ROUGE, BERTScore, MoverScore, and human evaluation. The results show that GPT-3 achieves the highest overall scores in terms of fluency and coherence but performs less impressively in informativeness and factuality. The video concludes by encouraging viewers to use the MeetingBank resource for further research and discussion.</sample>
    <sample id="239">Ciao a tutti, mi chiamo David Vilar e fornirò un breve riassunto dell'articolo "Prompting PaLM per la traduzione: valutazione di strategie e prestazioni". Questo è un lavoro in collaborazione con i miei colleghi da Google Translate. PaLM è un modello di macchina apprendente grande a 540 miliardi di parametri presentato l'anno scorso, 2022. È stato addestrato su una vasta raccolta di testi, comprensiva di 780 miliardi di token. Al momento della pubblicazione, ha ottenuto i migliori risultati in centinaia di compiti di NLP. In questo lavoro, presentiamo lo studio più sistematico sulla capacità di adattamento dei modelli di macchina apprendente tramite la tecnica di prompting per la traduzione automatica. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità di traduzione. Ciò include l'uso delle ultime tabelle di test per evitare un sovrapposizione dei dati di test con quelli di addestramento del modello di macchina apprendente. Abbiamo anche confrontato i nostri risultati con i sistemi di traduzione automatica più avanzati, cioè i sistemi che hanno ottenuto i migliori risultati nel contesto WMT. Utilizziamo misurazioni neurali di traduzione automatica più avanzate e inoltre mostriamo anche i risultati di valutazione umana basata su esperti. Infine, forniamo alcune raccomandazioni per la selezione delle strategie di prompting. Il prompting ha un grande impatto sulle prestazioni dei modelli di macchina apprendente per la traduzione, come possiamo vedere in un semplice esperimento dove abbiamo fornito due diverse istruzioni per ogni frase. La maggioranza delle frasi, 516 su 1000, ha registrato una differenza di più di un punteggio BLEURT. E questa differenza può arrivare, in casi estremi, fino a 40 punteggi BLEURT. Quindi è importante scegliere una buona strategia di prompting. In nuestros experimentos, nos hemos decidido por una estrategia de prompting de 5 ejemplos donde marcamos cada frase que proporcionamos al sistema con el idioma en que está escrita. En este ejemplo, donde realizamos la traducción desde alemán al inglés, las frases fuentes en alemán están marcadas con un punto y coma alemán y las traducciones al inglés con un punto y coma inglés. Vimos que la forma exacta del prompting no tiene un gran influjo en el caso de varios promptings cortos. Es crucial para los prompting de 0 y 1. Y cuando vamos, como en nuestro caso, a un prompting de 5 ejemplos, no hay diferencia notable en la forma exacta del prompting. Son los ejemplos los que llevan la mayor parte del peso. El resumen de nuestros resultados experimentales es que la calidad de los ejemplos es más importante que la similitud con la frase fuente. Por lo tanto, es importante seleccionar los ejemplos de alta calidad. En particular, comparamos la selección de prompts desde los datos de entrenamiento para las evaluaciones WMT en los datos dev. Los datos dev son mucho más curados y de mayor calidad que los datos de entrenamiento, que son más ruidosos. Y sus resultados son un mejor desempeño cuando se utilizan los datos dev. Sin embargo, los sistemas de traducción automática especializados tienen una ventaja substantial sobre las traducciones de PaLM. Pero, PaLM se acerca bastante a un sistema de uso comerciales. En nuestro caso, elegimos evaluar con Google Translate. Las conclusiones que obtuvimos de la evaluación humana que realizamos utilizando el framework MQM dijeron que la fluidez de PaLM es comparable a los sistemas de traducción automática más avanzados, pero la principal diferencia radica en la precisión. En particular, los errores más comunes son los errores de omisión. Parece que PaLM opta por producir una traducción mejor suena, a veces dejando de lado partes de la frase fuente que son traducidas. Sin embargo, la categoría "Estilo/Acertijo" para PaLM es más baja que para los sistemas de traducción automática más avanzados, lo que es un señal adicional de que PaLM proporciona realmente una traducción fluida, pero aún con algunos problemas de precisión. Y eso es todo para este resumen muy corto. Para más detalles, por favor vayan a la presentación completa del artículo. Muchas gracias.</sample>
    <sample id="240">Ciao, mi chiamo Dawei e sono un dottorando presso l'Università di Saarland in Germania. In questo video, vorrei presentare recenti lavoro "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro in collaborazione con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una introduzione alla supervisione debole e al machine learning devidamente supervisionato. In supervisione debole, non etichettiamo manualmente i dati. Ciò che facciamo invece è etichettare i dati utilizzando fonti di etichettatura debole, come regole semplici, database o sondaggi di pubblico di bassa qualità, come illustrato nella figura sulla destra. In confronto alle annotazioni umane, le etichette più deboli sono molto più economiche, tuttavia, sono anche rumorose, ovvero un certo numero di etichette sono sbagliate. Se addestriamo direttamente reti neurali su etichette devoche, le reti neurali tendono a memorizzare il rumore delle etichette e non generalizzano. In machine learning devidamente supervisionato, gli algoritmi di addestramento proposti si proppongono di addestrare robustamente reti neurali sotto tale rumore di etichette in modo che i modelli addestrati continuino a generalizzare bene. In recenti opere in WSL, ovvero machine learning devidamente supervisionato, un comune preteso è che le persone diano affermazioni del genere che addestrano i modelli solo su etichette devoche e ottengono prestazioni elevate su set di test puliti. Tuttavia, questa affermazione non è completamente corretta, poiché si assume che sia disponibile un set di validazione pulito per la selezione del modello. Non possiamo fermarci su questo contesto, ma quest'ipotesi necessita di essere considerata. Questa necessità è spesso trascurata. La nostra domanda di dubbio riguarda tre domande di ricerche. Prima di tutto, è necessario avere un set di validazione pulito per WSL o possiamo forse utilizzare un set di validazione rumoroso invece? Secondo, se è necessario avere un set di validazione pulito, o se è necessario avere un set di validazione pulito per far funzionare WSL, allora quanti campioni puliti dobbiamo avere? Infine, dovremmo utilizzare solo i campioni puliti per la validazione o ci sono altre migliori modalità per utilizzarli? Abbiamo indagato queste domande nella nostra opera e i nostri findings sono i seguenti. Troviamo che, in modo interessante, recenti metodi di WSL hanno bisogno effettivamente di campioni di validazione puliti per funzionare correttamente. Altrimenti, c'è una grande perdita di prestazione. Come illustrato nella figura, se non ci sono campioni di validazione puliti, allora i modelli addestrati non possono generalizzare al di fuori delle etichette originali, ovvero che l'addestramento è inutile. Questo indica che i metodi di WSL effettivamente richiedono etichette di validazione pulite per funzionare correttamente, e il costo di ottenere tali etichette dovrebbe essere tenuto a mente. Il nostro secondo trovare è che aumentare il numero di campioni di validazione puliti aiuterà ai metodi di WSL a ottenere prestazioni migliori, come illustrato nella figura sulla sinistra. Di solito, basta avere 20 campioni per classe per ottenere prestazioni elevate. Ma non è tutto, poiché se decidiamo di accedere ai campioni di validazione puliti, allora addestrare direttamente sui campioni puliti anche otterrebbe prestazioni migliori. La figura sulla destra mostra la differenza di prestazione tra approcci di addestramento continuo, che vengono direttamente applicati sui campioni puliti, e WSL, che utilizzano i campioni puliti solo per validazione. Come possiamo vedere, se abbiamo 10 campioni per classe, l'addestramento continuo inizia a superare i metodi di WSL. Infine, le prestazioni migliorative alegoricamente ottenute in precedenti opere di WSL possono facilmente essere ottenute consentendo di continuare ad addestrare sui campioni di validazione puliti. Come possiamo vedere dalla figura, il modello base, chiamato FTw, inizialmente sottopresta rispetto ai metodi più complicati di WSL, come COSINE. Tuttavia, se permettiamo di continuare ad addestrare sui campioni puliti, allora FTw ha prestazioni uguali a metodi più avanzati. Quindi in pratica, non c'è motivo di scegliere metodi più complessi di WSL che richiedono più tempo di calcolo e spazio su disco. Per riassumere, abbiamo dimostrato che recenti metodi di WSL richiedono etichette di validazione pulite per funzionare correttamente. Le loro prestazioni e praticità sono sovrastimulate. I nostri consigli concreti per future work sono i seguenti. Primo, rapportare i criteri di selezione del modello. Ad esempio, rapportare se la selezione del modello è fatta tramite campioni di validazione puliti. Secondo, metodi di WSL dovrebbero essere confrontati con baselines di apprendimento a pochi campioni, poiché entrambi funzionano su campioni puliti. Terzo, l'addestramento continuo è un baseline semplice ma forte che dovrebbe essere considerato in future work in WSL. Infine, abbiamo aperto sorgenti il codice. Puoi trovare il link tramite il codice QR sullo schermo. Ti prego di controllarlo. Grazie e spero che ti piacerà la conferenza.</sample>
    <sample id="241">The paper "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments" by Ethan, Yang Chen, Wei Xu, and Alan Ritter at Georgia Tech addresses the limitations of current misinformation detection systems. These systems are often unrealistically evaluated using retrospectively constructed datasets and lack human-centric evaluation. The authors propose an end-to-end evaluation framework that involves humans at various stages of the process to detect misleading claims and policy violations related to COVID-19 treatments. The system uses a T5 model for claim extraction and a BERT-based stance classification model for policy violation verification. The evaluation results show that the system has a high position in policy violation detection and can detect 124.2 policy violations per human hour worked. The paper provides a realistic end-to-end setting for evaluating misinformation detection systems and aims to motivate the development of future human-in-the-loop systems.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono la domanda a giudice umana, che richiede ai giudici umani di scegliere tra due conversazioni o di valutare le conversazioni utilizzando una scala Likert.</sample>
    <sample id="243">Ci sono 5 autori coinvolti nell'articolo: Jenny, Sebastian Santy, Ronan Le Bras, Katharina Reinecke e Maarten Sap.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che Servin è un giudice e Kea è un baker. Queste informazioni sono utili per risolvere il problema della coreferenza e identificare correttamente l'entità a cui si riferisce il pronome "he".</sample>
    <sample id="245">The paper presents a two-step pipeline for finding high-agreement Amazon Mechanical Turk (MTurk) workers for summarization tasks. The first stage involves a qualification task that tests annotators' ability to evaluate multiple dimensions correctly, while the second stage tests their capacity for handling heavy workloads. The pipeline results in 4 gold and 8 silver workers out of 200 participants, achieving high agreement in terms of inter-annotator agreement (IAA) than experts. The paper also compares the pipeline with baseline MTurk workers and CloudResearch MTurk workers, showing similar quality but lower task acceptance rate for CloudResearch workers. The analysis of correctness across annotation sources shows significant Spearman's correlation between Pipeline and CloudResearch workers, but Pipeline may not guarantee the training of correctness. The paper concludes that pre-task filtering can avoid waste of time and resources and achieve high agreement at a lower cost, serving as a best practice for high-agreement annotations at large scale and lower cost. Future research will investigate ways to hire high-quality workers and try multiple applications for tasks, languages, and platforms.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovare il codice sorgente e il dataset sul GitHub.</sample>
    <sample id="247">The paper presents a new task called Knowledge Graph-Based Fact Verification, which utilizes knowledge graphs as evidence for fact verification. The authors introduce a new dataset, FactKG, which consists of claims in both written and colloquial styles, with labels SUPPORTED and REFUTED. The dataset includes five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The authors also propose two methods for generating colloquial style claims: the colloquial style transfer model and presupposition templates. They construct baselines using only the claims to verify and utilizing the GEAR model to verify the claim using correct evidence. The results show that all baselines outperform the majority class baseline, and the GEAR model that uses graph evidence outperforms all other baselines.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico, ad esempio Paese, genere, ecc. Questo è fatto per ottenere un insieme ricco di annotazioni e demografie.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate per preservare la struttura relevante pero añadir ruido a la entrada.</sample>
    <sample id="250">La valutazione dimensionale significa valutare diversi aspetti specifici di una qualità del chat, piuttosto che valutare la qualità complessiva in modo holistico. Questo approccio aiuta a capire le caratteristiche specifiche del modello e le sue caratteristiche di forza e debolezze.</sample>
    <sample id="251">I fornitori dell'articolo sono l'Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="252">Our work, U-CREAT: Unsupervised Case Retrieval using Events extrAcT, presents two key contributions to the field of Prior Case Retrieval (PCR) in the legal domain. Firstly, we introduce the IL-PCR Dataset, a new benchmark for PCR tasks with 7,070 legal cases and an average of 6.775 citations per query document. Secondly, we propose the U-CREAT pipeline, which leverages unsupervised learning techniques and an event-based approach for PCR tasks. The pipeline includes three steps: pre-processing, dependency parsing, and post-processing. We experiment with various models, including count-based, transformer-based, and event-based models, and find that event-based models significantly outperform baseline methods. In particular, the Event Filtered Documents model achieves the best performance with lower inference times and higher F1 scores compared to other techniques. Our work opens up avenues for further exploration and development in the field of prior case retrieval.</sample>
    <sample id="253">Abstract: This work presents a model called DisorBERT, which aims to detect signs of mental disorders in social media posts. The model uses domain adaptation to improve performance on a target domain by leveraging knowledge from a related or similar domain. Specifically, it integrates information from Reddit and mental health into a base language model trained on general data. The model incorporates a lexicon to guide the masking process, allowing it to focus on important words during training. The results show that DisorBERT outperforms other models, including MentalBERT, which was trained with a large amount of data. Future work plans to explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">This research presents a document-level relation extraction framework that utilizes uncertainty-guided label denoising to improve the quality of distant supervision (DS) data. The proposed method introduces instance-level uncertainty estimation for overlapping relations and employs dynamic class uncertainty thresholds to filter out noisy pseudo labels. A multi-phase training strategy is also designed to iteratively re-label DS data, enhancing the performance of the DocRE model. Experiments on public datasets demonstrate that the proposed framework outperforms several strong baselines, significantly improving the label quality of DS data and achieving great performance improvements.</sample>
    <sample id="255">La forma del prompting si rivela importante in casi di zero e uno-shot prompting. In tutti gli altri casi, inclusi i nostri esperimenti con un prompting di 5 esempi, la qualità degli esempi è più importante rispetto alla similarità con la frase di origine.</sample>
    <sample id="257">I ricercatori hanno valutato quattro modelli di dialogo in avanzata.</sample>
    <sample id="258">The paper "Can Large Language Models Be an Alternative to Human Evaluation?" proposes using large language models to evaluate the quality of text in natural language processing. The authors give instructions to the language models and use them to rate samples, comparing the results with human evaluations. They conduct experiments with four large language models (T0, InstructGPT-curie, InstructGPT-davinci, and ChatGPT) and find that Davinci and ChatGPT show a clear preference for human-written text over GPT-2-written text. The paper also addresses questions about agreement between large language models and human evaluators, the impact of instruction wording changes, and the benefits and costs of using large language models compared to human evaluation.</sample>
    <sample id="259">This paper presents XSemPLR, a unified benchmark for cross-lingual semantic parsing in multiple natural languages and meaning representations. The authors propose a dataset containing 9 datasets in various domains, 5 semantic parsing tasks, 8 meaning representations, and 22 natural languages in 15 language families. They evaluate three representative types of multilingual language models: Encoder-PTR, Encoder-Decoder, and Encoder-Decoder with Pointer-based Decoders (Encoder-PTR). The results show that Encoder-Decoder models obtain the best performance on all nine datasets. The paper also compares cross-language performance gaps and finds that pretraining on English natural language can significantly boost the performance of Few-shot on target natural languages. The authors conclude that XSemPLR provides a comprehensive benchmark for cross-lingual semantic parsing and encourages further research in this area.</sample>
    <sample id="260">Il numero di autori coinvolti nell'articolo non è specificato nel testo fornito.</sample>
    <sample id="261">I qualità ideali di un buon pianificatore sono scrivi script che sono ragionevoli e fedeli alle vincoli.</sample>
    <sample id="262">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="263">In this work, we investigate label bias problems in in-context learning and propose a novel calibration method to mitigate these biases. We start by categorizing existing findings on bias problems and identifying a new type of bias called domain-label bias. We then propose domain-context calibration, which uses random in-domain words sampled from the task corpus as content-free text to estimate the model's bias on each label name. Our experiments show that domain-context calibration significantly improves the performance of in-context learning on a wide range of datasets, especially those with large domain-label bias. We also observe that using more random words rather than single predefined tokens leads to further improvements. Overall, our work provides a systematic investigation of label bias problems in in-context learning and proposes a practical solution to improve the performance of large language models.</sample>
    <sample id="264">Abstract: This paper proposes a novel task named Transferable Audio-Visual Text Generation (TAVT) to address the challenge of multi-modal domain shifts in audio-visual text generation tasks. The proposed framework consists of three components: an audio-visual meta-mapper network, an audio-visual encoder and language model generator, and counterfactual contrastive learning. The audio-visual meta-mapper network maps different visual concepts across domains into a unified auditory semantic space, while the audio-visual encoder and language model generator generate multimodal text with the help of learnable tokens called visual prefix for audio clusters. Dual Counterfactual Contrastive Learning (DCLL) is introduced to directly optimize the visual-textual alignment without relying on the quality of negative samples. Experiments on two benchmarks based on MSVD and MSR-VTT show that TAVT outperforms all compared models on cross-datasets and cross-domain settings, even with low-resource domains.</sample>
    <sample id="265">Il nome della relatrice o del relatore è Vasudha.</sample>
    <sample id="266">I fornitori di informazioni non hanno accesso alle informazioni specifiche sulle affiliazioni degli autori dell'articolo. Tuttavia, l'articolo è stato presentato da Adam Przepiórkowski e si ritiene che sia stato scritto da lui o da un team di autori associati all'istituto di lingua e letteratura polacca dell'Accademia delle scienze di Varsavia.</sample>
    <sample id="268">I errori più comuni di PaLM sono omission errors, ovvero l'omissione di parti del testo sorgente durante la traduzione.</sample>
    <sample id="269">Ciao, mi chiamo James Finch e mi chiamo Sarah Finch. Oggi parleremo di ABC-Eval, un nuovo approccio dimensionale per valutare gli AI conversazionali. Questo lavoro è stato fatto dal Emory NLP Lab guidato dal professor Jinho Choi all'Emory University in collaborazione con Amazon Alexa AI. Supponiamo che tu abbia appena sviluppato un modello di dialogo e vuoi vedere come si confronta con lo stato dell'arte attuale. La pratica comune è utilizzare l'evaluation umana, cioè chiedere ai giudici umani di scegliere tra due conversazioni o di valutare le conversazioni utilizzando una scala Likert. Questi metodi funzionano bene per fornire valutazioni holistiche della qualità del dialogo in generale, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello in un livello più fine-grained. Uno degli approcci è semplicemente chiedere ai giudici umani di valutare diversi aspetti della qualità del dialogo, come la rilevanza delle risposte del modello utilizzando metodi esistenti o scala Likert. Tuttavia, crediamo che ci sia un策略更精确和可靠的策略对话质量的维度评估。Noi approcciamo riducendo la soggettività della valutazione umana definendo esplicitamente se ogni risposta del modello esprime certi comportamenti, ad esempio, rispondendo con informazioni irrelevanti o addirittura contraddicendosi. Chiamiamo questo approccio annotazione dei comportamenti nel chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat suggeriti da recenti studi sulla qualità del dialogo. ABC-Eval è capace di misurare le tassi con cui i modelli di chat commettono vari errori tematici. Ad esempio, ABC-Eval misura il numero di giri in cui un modello ignora il suo interlocutore o dice qualcosa di irrelevante, si contraddice o viola le conoscenze di senso comune, e quando il modello riesce o fallisce a dimostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat dello stato dell'arte e li abbiamo valutati su 100 conversazioni umano-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni con una scala Likert sul livello del giro, valutazioni con una scala Likert sul livello del dialogo e confronti di dialoghi a coppia. Per ciascun metodo esistente, abbiamo raccolto valutazioni su otto aspetti più comuni della qualità del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo più dimensioni. Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che le etichette di comportamento ABC-Eval sono in generale più affidabili delle etichette raccolte da metodi esistenti, come misurato dall'acordo inter-annotatore su 100 conversazioni duplicatamente etichettate. Inoltre, le etichette di comportamento ABC-Eval sono più predittive della qualità complessiva della conversazione rispetto alle metriche prodotte da metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice. Ad esempio, puoi vedere come misurare la proporcione di giri in cui un modello si contraddice con sé stesso o con il proprio interlocutore spiega il 5% e il 10% della qualità della conversazione, rispettivamente, mentre i punteggi di consistenza media Likert spiegano solo il 4% o meno. Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del dialogo utilizzando una regressione lineare graduale. Puoi vedere come la combinazione di tutti i metrici ABC-Eval spiega più del 25% della qualità della conversazione, e quando rimuoi i metrici uno alla volta, la maggior parte di loro risulti in una perdita di informazione significativa sulla qualità. D'altra parte, la combinazione di tutti i metriciLikert al livello del giro spiega molto meno della qualità, e meno di questi metrici trasmettono informazioni uniche. Queste etichette di comportamento ABC-Eval reliable, informative e distintive consentono di valutare gli AI conversazionali con un risoluzione più alta rispetto ai metodi precedenti. Puoi vedere che nei risultati del nostro esperimento che ci sono ancora alcune sfide che devono essere affrontate e precise quantificate. Ad esempio, i bot che testiamo hanno violazioni di senso comune in circa il 20% delle loro risposte. Producono informazioni irrelevanti in circa il 15% delle risposte, e si contraddicono con sé stessi o con il proprio interlocutore circa il 10% del tempo. Con il ritmo accelerato di miglioramento del campo, molte di queste tassi di errori potrebbero vedere una diminuzione nei nuovi modelli rilasciati dalla nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivo per cercare metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzata da altri nel campo come un passo significativo in questa direzione. E ci guardiamo bene a vedere come gli AI conversazionali avanzeranno nei prossimi mesi e anni. Grazie per la visione.</sample>
    <sample id="270">I fornitori dell'articolo sono il Emory NLP Lab, guidato dal professor Jinho Choi all'Emory University e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="271">In this article, CFT stands for Continuously Fine-tuning on the Clean validation set. It refers to a method of training neural networks by continuously fine-tuning them on clean validation samples, rather than using weakly supervised learning approaches that require more complex methods and computational resources.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto? Un esplorazione multilingue basata su dati". Questo lavoro è stato fatto in collaborazione con Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurremmo "mole" in questa frase? Se la frase precedente è stata "Le cose potrebbero iniziare a diventare pericolose se i ministri scoprono", allora "mole" si riferisce a un traduttore. Ma se la frase precedente è stata "Potrebbe essere qualcosa di serio, dottore?", allora "mole" si riferisce a un lentiggino. Quindi, a seconda del contesto, il significato del termine cambia, e quindi anche la sua traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questo è piuttosto difficile. In primo luogo, solo una piccola parte delle traduzioni dipende dal contesto, il che rende inattuari i metri di valutazione a livello di corpus come BLEU. E alcuni hanno suggerito di valutare specificamente le traduzioni dipendenti dal contesto, ma queste risorse supportano solo tipi limitati di traduzioni dipendenti dal contesto e insieme set di lingue, poiché di solito si basano su conoscenza di dominio e curazione umana. In questo lavoro, cerchiamo di rispondere a queste due domande: quando la traduzione richiede contesto e quanto bene i modelli gestiscono questi casi? Per rispondere alla prima domanda, ci siamo lanciati misurando quanto un termine dipende dal contesto durante la traduzione. In un lavoro precedente, abbiamo introdotto CXMI come misura per l'uso di contesto dai modelli di traduzione automatica. Questo è fatto misurando quanto informazione fornisce il contesto C al target Y, data la fonte X. Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, estendiamo CXMI a Pointwise CXMI che può misurare l'uso di contesto al livello della frase o al livello del termine. Possiamo pensare a parole che hanno alta P-CXMI come parole che richiedono contesto per la traduzione. Ora analizziamo parole con alta P-CXMI per cercare di trovare模式 tra queste parole. E eseguiamo l'analisi su transcrizioni di discorsi di TED tradotte in 14 diverse lingue. Eseguiamo l'analisi a tre livelli diversi. Prima, guardiamo alle tag di parte di discorso che hanno alta media P-CXMI. Questo ci permette di trovare, ad esempio, i pronunci duali in arabo che hanno relativamente alta P-CXMI. Questo può essere spiegato dal fatto che l'inglese non ha pronunci duali, quindi è necessario contesto per determinare se un pronoun è doppio quando si traduce in arabo. E allo stesso modo, troviamo che certe lingue richiedono contesto quando si sceglie la forma appropriata del verbo. Poi guardiamo ai vocabolari che hanno alta P-CXMI media su tutti i suoi differenti accadimenti. Questo ci aiuta a identificare casi come quello qui, dove in cinese è necessario contesto per tradurre nomi propri per assicurarsi di usare la stessa traduzione nel documento. E allo stesso modo, troviamo che il contesto è importante per traduire in forma appropriata. Infine, guardiamo ad individui token che hanno alta P-CXMI. Questo ci permette di identificare fenomeni che non possono essere davvero captati dal termine本身，ma piuttosto espressi nella struttura della frase, come la risoluzione dell'elusione. Ora utilizziamo i nostri findings dalla nostra analisi per progettare un benchmark per la traduzione a livello di documento. Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per automaticamente identificare parole che appartengono al fenomeno. E chiamiamo il tagger Multilingual Discourse-Aware o MuDA tagger. Possiamo notare anche che diverse lingue hanno proporzioni diverse di questi fenomeni di discorso. Poi utilizziamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per valutazione e applichiamo i nostri metri di traduzione di scelta sulla traduzione a documento che il tagger MuDA ha identificato. Infine, utilizziamo il benchmark che abbiamo creato, insieme ad altri metri, per valutare diversi modelli sul traduttore a livello di documento. In primo luogo, quando utilizziamo metri a livello di corpus: per BLEU, trovi che i modelli a livello di corpus alogici hanno le migliori prestazioni. Ma poi se utilizziamo COMET, i modelli consapevoli del contesto hanno le migliori prestazioni. E se utilizziamo f-medie parola, allora i modelli con e senza contesto hanno prestazioni simili. Questo di nuovo dimostra che è difficile determinare il migliore traduttore a livello di documento se utilizziamo metri a livello di corpus da sole. Ora utilizziamo il benchmark MuDA per valutare i modelli e trovi che i modelli consapevoli del contesto sono significativamente più accurati dei modelli che non usano contesto per certi fenomeni di discorso come formalità e coesione lessicale. Ma questi modelli non sono molto migliori dei modelli che non usano contesto per fenomeni come ellissi, pronunci e forme del verbo. Questo suggerisce che dovremmo vedere progressi in traduzioni a livello di documento. Abbiamo anche confrontato diversi sistemi commerciali e il nostro benchmark dimostra che DeepL è di solito più accurato di Google Translate per traduzioni a livello di documento. In sintesi, eseguiamo un'analisi data-driven attraverso 14 coppie di lingue per identificare quando le traduzioni richiedono contesto e utilizziamo i nostri findings per costruire un benchmark per la traduzione a livello di documento che ci aiuta a identificare quali fenomeni di discorso i modelli possono gestire bene o no, e quali sistemi di traduzione sono buoni per la traduzione a livello di documento. Grazie mille per la vostra attenzione. Ci vediamo a Toronto.</sample>
    <sample id="274">Il relatore è Yusen Zhang.</sample>
    <sample id="276">This paper presents a dataset called IndicMT Eval, which aims to evaluate machine translation metrics for Indian languages. The authors focus on five languages belonging to two different language families: Tamil and Malayalam (Dravidian languages) and Hindi, Marathi, and Gujarati (Indo-Aryan languages). They select 200 sentences randomly from the Flores dataset and generate multiple candidate translations for each source sentence in English using seven different translation models or APIs. A total of 7,000 samples are collected, and bilingual expert annotators evaluate the outputs by showing them the source sentence along with the candidate. Annotators mark each error along with its type and severity and provide an overall score for the output. The authors analyze various metrics and fine-tune the best-performing metric, COMET, using their MQM dataset. They observe that IndicCOMET MQM outperforms COMET baselines on three out of five languages and shows higher correlations than COMET MQM across all languages. They also evaluate robustness scores on the ACES Translation Accuracy Challenge Sets and find that IndicCOMET MQM has a correlation score of 0.36 and is more robust than the COMET counterpart, which has a score of 0.272.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Il metodo utilizza il concetto di "markedness" (contrassegnate) in sociolinguistica, che afferma che i gruppi dominanti sono sia linguisticamente che socialmente contrassegnati, mentre i gruppi marginalizzati sono contrassegnati. In questo contesto, le parole contrassegnate sono那些被用来区分标记群体和未标记群体的词语。通过比较标记群体和未标记群体的词汇，该方法可以识别出反映有害模式的特定刻板印象和本质化叙事。</sample>
    <sample id="279">I fornitori dell'articolo sono studenti di dottorato all'Università di Washington.</sample>
    <sample id="280">The paper presents a novel attention-based correlation-aware multimodal fusion framework called MultiEMO for emotion recognition in conversations. The framework consists of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The main contributions of this work include a novel visual feature extractor named VisExtNet, a multimodal fusion model called MultiAttn based on bidirectional multi-head cross-attention layers, and a Sample-Weighted Focal Contrast loss to address the difficulty of classifying minority and semantically similar emotion classes. The proposed framework achieves state-of-the-art performances on two ERC benchmark datasets, MELD and IEMOCAP, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">This work, titled "When Does Translation Require Context? A Data-driven, Multilingual Exploration," presents a collaborative effort by Kayo Yin and her team to explore the role of context in translation. The study aims to address two main questions: when does translation require context, and how well do models handle these cases?

To answer the first question, the researchers introduced Pointwise CXMI, a measure for context usage at the sentence or word level. They analyzed transcripts of TED talks translated into 14 languages, identifying patterns between words with high P-CXMI and discourse phenomena such as dual pronouns, proper noun translations, formality, and ellipses resolution.

The second question is addressed through the creation of the MuDA (Multilingual Discourse-Aware) tagger, which automatically identifies context-dependent examples in parallel corpora. The MuDA benchmark evaluates models on document-level machine translation, showing that context-aware models outperform context-agnostic ones for certain phenomena like formality and lexical cohesion.

The study concludes that while context-aware models are more accurate for some discourse phenomena, there is still room for improvement in handling others like ellipsis, pronouns, and verb form. The MuDA benchmark provides a valuable tool for evaluating and improving document-level translation systems.</sample>
    <sample id="282">Title: StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing

Authors: Xuekai Zhu, et al.

Conference: ACL 2023

Abstract:

This paper presents a novel approach to non-parallel text style transfer at the story level, addressing the challenge of imitating author linguistic preferences at the discourse level. Most existing studies have focused on token-level or sentence-level style transfer, but our research aims to tackle the more complex task of story-level style transfer. We propose a generation model named StoryTrans, which learns discourse representations from source texts and combines them with learnable style embeddings to generate texts in target styles. To alleviate the challenges of style-specific content transfer and content preservation, we design a new training objective that reduces stylistic features from discourse representations, pulls representations closer in the latent space, and enhances content preservation through a two-stage generation process. Our experiments on Chinese and English datasets demonstrate that StoryTrans outperforms strong baselines in terms of style control and content preservation, as confirmed by both automatic and manual evaluations. Style visualization indicates that our method aligns well with the golden text in the style feature space. Additionally, our method can supplement short phrases or plots to enrich the whole storyline while maintaining the main contents, and rewrite most sentences with the target style while preserving the source semantics.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è la struttura di coordinazione di Lisa, Bart e Maggie, come in "Lisa, Bart e Maggie and they went to New York City."</sample>
    <sample id="284">The paper presents a novel fuzzy span mechanism for enhancing universal information extraction (UIE) in natural language processing. The authors propose a fuzzy span attention layer that dynamically adjusts the attention span of the model, allowing it to better capture the context and meaning of the text. This is achieved by introducing an optimizable parameter delta to adjust the length of the full attention range, and by linearly decaying the attention distribution on the attention span boundary rather than truncating it. The proposed method is evaluated on three main information extraction tasks, including named entity recognition, relationship extraction, and aspect sentiment triplet extraction. The results show that FSUIE achieves significant performance improvement compared to UIE-base without a fuzzy span mechanism, and outperforms previous state-of-the-art methods on several datasets.</sample>
    <sample id="285">The paper presents a new approach to correcting factual errors in dialogue summarization. The authors argue that current factuality metrics are unreliable and do not accurately evaluate the performance of FEC models. They propose a new taxonomy of factual errors and an evaluation framework based on the Errant metric for grammar error correction. The results show that training FEC models with reference summaries from dialogue summarization datasets yields the best results, and introducing human-corrected summaries during training can improve performance. Combining human-annotated data with synthetic data is also a promising direction.</sample>
    <sample id="286">Il relatore è James Finch e la relatrice è Sarah Finch.</sample>
    <sample id="287">There are four authors involved in the article: Javad Hosseini, Filip Radlinski, Silvia Pareti, and Annie Louis.</sample>
    <sample id="288">I set di dati che possono essere utilizzati per testare i fenomeni sintattici includono il dataset Adjunct Island, BLiMP e SyntaxGym.</sample>
    <sample id="290">I cinque metodi per la prima domanda di ricerca sono: FTw, COSINE, FTw+10, FTw+20 e FTw+30.</sample>
    <sample id="291">Il modello viene valutato su attività come riconoscimento di entità, classificazione, tag di parte di discorso e risoluzione di domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un dataset di 4 GB di NACHOS.</sample>
    <sample id="295">The name of the speaker is Adam Przepiórkowski.</sample>
    <sample id="296">This paper presents a collaborative work between the University of Turin and Amazon Alexa on natural language understanding, specifically focusing on irony detection. The authors developed a corpus called EPIC (English Perspectivist Irony Corpus) by collecting data from social media, Reddit, and Twitter over a period of 1.5 years. They used crowdsourcing platform Prolific to annotate the data with 74 annotators for five varieties of English. The paper discusses the differences in inter-annotator agreement based on various dimensions such as gender, age group, nationality, and geographical distribution. The authors also introduce perspective-aware models that show less uncertainty and more confidence in their predictions compared to gold standard aggregated models. Finally, the paper highlights the peculiarities in the annotations related to age and geographical distribution.</sample>
    <sample id="297">This project develops a typology and glossary of over 340 dogwhistle terms and symbols, particularly focusing on racist, transphobic, and anti-Semitic ones. It includes a case study of historical U.S. political speeches, showing that the frequency of racial dogwhistles correlates with the Republican Southern Strategy post-Civil Rights era. The project evaluates dogwhistle recognition in language models like GPT-3, finding varying performance depending on register and type. Additionally, it examines how dogwhistles can evade content moderation by lowering toxicity scores when standard group labels are replaced with dogwhistles.</sample>
    <sample id="298">I risultati dell'esperimento hanno dimostrato che il modello si deteriora con un intervallo temporale crescente tra i dati di addestramento e i dati di test. Questo ha confermato l'ipotesi che la principale causa della perdita di prestazioni è la deriva temporale.</sample>
    <sample id="299">The paper presents a new training method to improve the robustness of Natural Language Inference (NLI) models by reducing their reliance on shortcuts. The proposed method uses a minimax training objective between a learner and auxiliary model, where the learner tries to minimize the loss of the NLI task while the auxiliary aims to maximize the learner's loss by generating example weights that emphasize under-represented hard examples. The method does not assume any specific type of shortcuts in the dataset and can be applied to different datasets and models. The results show that the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">Interactive dictation is a process where users can use their voice to both dictate and edit a document in a natural and intuitive manner. This work introduces a new task called interactive dictation, which allows for flexible interleaving of dictation and editing without trigger words or commands. The system uses intuitive and open-ended natural language utterances to specify edits. The contribution includes formalizing the task, designing a data collection interface, building a dataset, and creating a baseline system. The system uses separate models for each step, including ASR recognition, segmentation, ASR repair, and interpretation. The paper discusses the trade-off between runtime and accuracy and compares different architectures and output types. The authors welcome more work on this task and have released code for future research.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output perché, anche se si hanno tutti i token giusti, non sono ordinati correttamente. La nostra approccio predice una permutazione per mettere i token in ordine corretto.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire meglio come i modelli di lingua generano stereotipi positivi e narrativa essenziale. Questo potrebbe aiutare a capire se si stanno veramente riducendo i bias o se stanno solo facilitando pattern discriminatori.</sample>
    <sample id="304">I input inaccettabili di coppia minima sono frasi che non rispettano la grammatica o violano le regole linguistiche.</sample>
    <sample id="305">Abstract: Weakly Supervised Learning (WSL) is a machine learning approach that uses weak labeling sources, such as simple heuristic rules or low-quality crowdsourcing, to label data without manual annotations. While WSL can be more cost-effective than human annotations, the noisy labels can lead to poor model performance if not addressed. This paper addresses three research questions related to the necessity of clean validation data in WSL: whether clean validation data is required for WSL to work properly, how many clean samples are needed, and whether clean samples should only be used for validation or can be utilized for training. The authors found that recent WSL methods require clean validation samples to achieve good performance, and that increasing the number of clean validation samples can improve performance. They also found that fine-tuning on clean validation samples can achieve similar performance to more complex WSL methods. The authors recommend reporting model selection criteria, comparing WSL approaches with few-shot learning baselines, considering continuous fine-tuning, and open-sourcing code.</sample>
    <sample id="306">In this paper, we investigate the ability of pre-trained language models to track entities in longer discourses. We argue that this is a crucial ability for understanding longer discourses, but there haven't been any systematic investigations into what pre-trained language models can actually perform such tasks. We designed an evaluation task involving boxes and objects, where the input to the model starts with a description of the initial contents of each box, and the task of the language model is to complete the input by predicting the contents of each box after multiple state-changing operations like moving objects or adding objects to a box. We tested the setup with Flan-T5 and GPT-3 and -3.5 models using 2-shot in-context learning. Our experiments show that most models simply repeat the initial state, as you can see from the generally high accuracy on the right panel. And we can also see that only text-davinci-003 exhibits non-trivial tracking, which is the pink line here in the left panel. And all other models performed below a strong random baseline obtained by random simulation, which is the blue line. Since the models we tested varied along several different dimensions, we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models, which all have been trained on substantial amounts of code, exhibit non-trivial entity tracking behavior, whereas all models that do not have code as a substantial part of their pre-training do not. And this suggests that pre-training on code is what's responsible for making this capacity surface in pre-trained language models.</sample>
    <sample id="307">I ricercatori hanno utilizzato metriche di valutazione come riconoscimento di entità, classificazione, tag di parte di parola e risoluzione di domande per valutare le performance dei modelli.</sample>
    <sample id="308">This presentation by Jenny, a first-year PhD student at Carnegie Mellon University, focuses on the concept of positionality in NLP research and its impact on model performance. Positionality refers to the perspectives that people hold as a result of their demographics, identity, and life experiences, which can influence research decisions and outcomes. The presentation highlights the need to study data set and model positionality, particularly in subjective and socially oriented NLP tasks.

The presentation introduces NLPositionality, a framework developed by Jenny and her team to compare end users with existing datasets and models. The framework involves re-annotating data sets with diverse annotators and comparing their annotations to models and datasets using Pearson's R correlation score. The study involved over 16,000 annotations from 1000 annotators from 87 countries, revealing positionality in NLP, such as alignment with English-speaking countries and college-educated individuals.

The presentation concludes with recommendations for addressing positionality in NLP, including keeping a record of design choices, conducting research with a perspectivist lens, and building specialized datasets and models within specific communities.</sample>
    <sample id="309">Inter-annotator agreement on 100 doubly-labeled conversations was used to measure the reliability of ABC-Eval labels.</sample>
    <sample id="310">Wikipedia.</sample>
    <sample id="311">I fornitori dell'articolo sono affiliati all'Universität Hamburg e all'Universität Paderborn.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo dataset di addestramento per la tuning multi-modal basata su istruzioni. Consiste in 62 diverse attività multi-modal diverse, coprendo 10 category diverse, derivate da 21 dataset esistenti e aperti. Ogni attività è equipaggiata con cinque istruzioni esperte scritte. Questo rende MultiInstruct un riferimento importante per i ricercatori che vogliono indagare la tuning multi-modal basata su istruzioni.</sample>
    <sample id="313">Il numero di autori coinvolti nell'articolo è non specificato.</sample>
    <sample id="314">La coordinazione binaria è una struttura grammaticale in cui due elementi, detti "conjunti", sono collegati da un segnaposto o un segnaposto relativo. Questi elementi hanno la stessa funzione grammaticale e possono essere entrambi sottoposti a modifieri o complementi.</sample>
    <sample id="315">Il passaggio non fornisce informazioni specifiche sul tempo medio utilizzato per i prompt in questo studio.</sample>
    <sample id="316">I risultati suggeriscono che il modello T5, quando addestrato su un dataset specifico come CoScript, può generare script di qualità superiore rispetto a molti modelli più grandi. Questo indica che i modelli più piccoli possono superare i modelli più grandi se addestrati correttamente su datasets appropriati.</sample>
    <sample id="317">The paper presents a new method called CodeIE for few-shot information extraction, which transforms the text-to-structured information extraction task into a structure-to-structure code generation task using code large language models like Codex. The proposed approach significantly outperforms traditional baseline models such as UIE and natural language large language models like GPT-3 in terms of recall. The analysis shows that perplexity computed on text format inputs using models like T5 was generally higher than that of code format samples using models like CodeT5. Additionally, when decoding with GPT-3 and text format prompts, there are many structural errors, whereas when using Codex and code format prompts, such errors were almost non-existent. Overall, the proposed approach provides better performance in information extraction tasks, especially in terms of recall.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Labrak e presenterò i nostri lavori su "DrBERT: Un modello pre-addestrato robusto in francese per i domini biomedici e clinici". In questa presentazione, innanzitutto parliamo di modellazione del linguaggio in salute. Poi presenteremo la nostra principale contribuzione all'articolo. Introduciamo il primo modello biomedico in francese, DrBERT, che è basato su RoBERTa e addestrato su NACHOS, un dataset di dati medici raccogliuti da internet. Abbiamo anche introdotto una comparazione dei modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 compiti di downstream biomedici e clinici in francese. Infine, concludevamo gli esperimenti e forniamo maggiori dettagli su come accedere a questi modelli. Da quando è stata rilasciata nel 2018, BERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione del linguaggio naturale e ha offerto enormi miglioramenti rispetto ai metodi statici e contestualizzati storici come Word2vec, fastText, ecc. Da allora, questo modello è stato adattato a molti altri linguaggi, come in francese con CamemBERT, e anche in domini come biomedici con PubMedBERT e BioBERT e in clinici con ClinicalBERT, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono scaraciati e spesso sono basati su pre-addestramento continuo a causa della mancanza di dati in-domain. Tuttavia, il francese non aveva fino ad ora un modello open source per i biomedici. Ci domandavamo quindi cosa fosse la fonte più appropriata di dati per una vasta gamma di usi e i dati raccogliuti dalla rete hanno fatto da sostituto dei dati clinici. Per rispondere a questa domanda, confrontiamo DrBERT con il nostro ChuBERT modello, che è basato su dati anonimizzati ottenuti dal data warehouse dell'Ospedale Universitario di Nantes. Dopo di che ci domandavamo quanto di dati ci vuole per addestrare un modello specializzato in francese: 4 GB, 8 GB o altro? Per rispondere a questa domanda, prima addestriamo e confrontiamo quattro modelli addestrati da zero: una prima versione di DrBERT, con 7 GB di NACHOS; una seconda versione di 4 GB di set di NACHOS; una prima versione di ChuBERT, che è un modello clinico con 4 GB di frasi tratte da note cliniche; e una versione finale di ChuBERT con un mix di 4 GB di set di NACHOS e 4 GB di note cliniche. Inoltre, introduciamo tre modelli addestrati su pre-addestramento continuo per analizzare l'impatto della strategia di pre-addestramento. Uno basato sul peso di CamemBERT e addestrato su un set di 4 GB di NACHOS. Un altro anche basato su CamemBERT, ma addestrato questa volta su 4 GB di note cliniche e infine, uno basato sul modello di biomedici in inglese PubMedBERT, e addestrato su un set di 4 GB di NACHOS. In totale, abbiamo sette modelli. Per valutare i nostri sette modelli, raccolgiamo i dati per compiti pubblici e privati come riconoscimento di entità, classificazione, tag di parte di parola e risoluzione di domande. Questi modelli vengono confrontati con sei modelli di riferimento che sono CamemBERT OSCAR 138 GB, CamemBERT OSCAR 4 GB, CamemBERT CCNET 4 GB, PubMedBERT, BioBERT e ClinicalBERT. L'evaluation evidenzia che i modelli hanno performato meglio sul compito con i dati dello stesso tipo su cui il modello è stato addestrato. Tuttavia, possiamo osservare che i dati provenienti da fonti diverse sembrano essere più versatile. Abbiamo anche osservato che utilizzare più dati traduce in un migliore prestazione. Di conseguenza, l'addestramento da zero sembra ottenere un'efficacia superiore su quasi tutti i compiti. Tuttavia, il nostro esperimento sull'addestramento continuo utilizzando il peso e la tokenizzazione di CamemBERT addestrato su un subset di 4 GB di NACHOS ha dimostrato risultati simili a quelli ottenuti con DrBERT 4 GB addestrato da zero. Che non è il caso per il modello basato sul peso e tokenizer di CamemBERT, che soffre di problemi di stabilità. Infine, come conclusione, il nostro sistema corretto ha offerto un'efficacia superiore su nove dei 11 compiti di downstream e ha superato globalmente i risultati del modello generico, qui CamemBERT. Abbiamo anche osservato che i dati più specializzati sono migliori, ma non scalano bene. Tutti i modelli pre-addestrati ottenuti da NACHOS sono ora disponibili su Hugging Face e sotto la licenza MIT, e tutti i script di addestramento sono su nostro repository GitHub. Quindi, grazie per questa presentazione e aspettiamo di scambiare informazioni durante la sessione di poster a Toronto.</sample>
    <sample id="319">Nel lavoro, vengono esaminate diverse strategie di apprendimento, tra cui la pre-addestramento da sorgente e la strategia di pre-addestramento continuo. In particolare, si analizzano i modelli addestrati con 7 GB di NACHOS, 4 GB di NACHOS, 4 GB di note cliniche e un mix di 4 GB di NACHOS e 4 GB di note cliniche. Inoltre, si esaminano anche i modelli addestrati con il peso e la tokenizzazione di CamemBERT su un subset di 4 GB di NACHOS, il modello basato su CamemBERT addestrato su 4 GB di note cliniche e il modello basato su PubMedBERT addestrato su 4 GB di NACHOS.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è inferiore a 1, come evidenziato dal fatto che la linea di migliore adattamento in rosso ha un coefficiente di inclinazione inferiore a 1. Questo indica che ogni unità di miglioramento ottenuta su CoNLL-2003 equivale a meno di una unità di miglioramento su CoNLL++. Ciò dimostra che l'overfitting adattativo non è osservato in questo caso.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata analizzando i tipi di semplificazione applicati, come la semplificazione lessicale, la semplificazione strutturale e livelli generali di semplificazione. Inoltre, si è notato che il DEPLAIN corpus dispone di una ampia varietà di trasformazioni di semplificazione.</sample>
    <sample id="322">In this paper, we explore the question of what a text classifier learns about morality. We argue that morality is subjective and can be expressed differently across different domains. We use explainable AI techniques to understand how language models trained to understand morality in text learn to express morality differently across different domains. We focus on understanding how morality is expressed differently in domains such as #AllLivesMatter and #BlackLivesMatter. Our experiments show that language models recognize that morality can be expressed differently in different domains. However, using just one single model for many different domains can lead to misunderstandings of morality in a very dangerous way.</sample>
    <sample id="323">The paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense QA" by Yujie Wang from Shanxi University, China, addresses the challenge of Commonsense QA by combining knowledge from language models and knowledge bases. The proposed method, DHLK, builds an HKG based on multiple knowledge bases using a two-stage pruning strategy and KRL to optimize structure and knowledge representation. It encodes and fuses QA contexts and entities using RoBERTa and Mask Self-Attention, dynamically removing entities with weaker relevance. RMSA is introduced to model subgraphs, and the HKG graph embedding is used for path enhancement. The final answer prediction is achieved by inputting the HKG graph, paths, QA context, and QA context into an MLP. Experiments on CommonsenseQA and OpenBookQA using external knowledge bases like ConceptNet, WordNet, and Wiktionary show that DHLK achieves good results compared to other LM and HKG methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi. Questo è dovuto al fatto che i modelli sono addestrati su grandi quantità di dati web che coprono diverse fonti di notizie politiche, come il New York Times, Los Angeles Times, The Guardian e Huffington Post. Questi bias possono propagarsi dal training dei modelli alla loro applicazione in downstream task, potenzialmente portando a problemi di giustizia.</sample>
    <sample id="325">Ciao! Il mio nome è Matthias Lindemann e oggi voglio presentarti un'introduzione rapida al nostro articolo su "Generalizzazione composti senza alberi utilizzando multiset tagging e permutazioni latenti". Questo è un lavoro di collaborazione con i miei insegnanti, Alexander Koller e Ivan Titov. La generalizzazione composti può essere comprensibile come capacità di un imparatore di gestire una ricorrenza più profonda e composizioni di frasi che non sono state viste individualmente durante la formazione. In contesto di parsing semantico, la testa per la generalizzazione composti potrebbe apparire così: come di solito, abbiamo un insieme di istanze di addestramento. In questo caso, "La ragazza dormì." E "Mary sapeva che la ragazza dormì." Queste istanze sono associate a forme logiche che rappresentano aspetti fondamentali del loro significato. Contrariamente all'evaluation standard del macchinismo, il set di test non proviene dalla stessa distribuzione ma contiene strutture inusuali di forme logiche. In questo esempio, il modello ha visto ricorrenze superfici durante l'addestramento e viene testato su un esempio con ricorrenza più profonda. I modelli seq2seq semplici hanno difficoltà con questa specie di generalizzazione fuori dalla distribuzione e spesso producono output che si allontano dal input. In particolare, spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelli colorati nello esempio. Un metodo popolare per affrontare questo problema è integrare gli alberi nei modelli. Gli alberi sono intesi per captare il processo di composizione che collega le istanze con le forme logiche. Questo funziona bene, ma gli alberi non sono usualmente forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e spesso un processo computazionalmente costoso. Di solito, questo implica un pre-processamento formale specifico delle forme logiche, ad esempio, per gestire simboli variabili. Ottenere gli alberi può anche implicare procedure di induzione grammatica specializzate. In questo articolo, non utilizziamo gli alberi e introduciamo un modello seq2seq neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output. Per la prima volta, mostriamo una forte generalizzazione a ricorrenze più profonde senza reluire su alberi. Il nostro approccio predice l'output dal input in due passi. Prima, etichettiamo ogni token di input con un multiset unordered di token che appariranno nell'output. Dopo il primo passo, abbiamo tutti i token giusti, ma non sono ordinati. Quindi, nella seconda fase, utilizziamo un altro modello per prevedere una permutazione per mettere i token nel ordine corretto. Introduciamo un nuovo metodo per prevedere la permutazione che non imposa alcuna restrizione sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo. Concettualmente, il nostro modello di permutazione funziona in questo modo: andiamo da sinistra verso destra sul output e determiniamo quale token multiset mettere in ogni posizione. Per la prima posizione di output, scegliamo semplicemente uno, come evidenziato in rosso. Poi saltiamo al token multiset successivo per determinare il token successivo nel output. Determiniamo il terzo token nel output in modo simile, scegliendo un token multiset successivo. Continuiamo questo processo fino a quando ogni token del primo passo è stato visitato esattamente una volta. Per dare un sguardo in anticipo ai nostri risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli senza alberi sul benchmark COGS. Il nostro modello supera gli altri di una larga distanza in quanto generalizza a ricorrenze più profonde. Alcune altre tipologie di generalizzazione strutturale rimangono molto sfide, tuttavia. In questo articolo, risolviamo un paio di sfide tecniche interessanti. In primo luogo, l'allegatura tra input e output non è data in dati di addestramento. Di conseguenza, per un token specifico non conosciamo quale multiset è venuto da, che rappresenta un problema per l'addestramento. Inoltre, a volte ci sono più permutazioni coerenti con i dati, ma la permutazione linguisticamente corretta è latente. Lo risolviamo inducendo l'allegatura come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma introduce il problema che trovare la permutazione più alta punteggio è NP-difficile. Questo è legato al problema del "Viaggiaatore di Ferro" e lo risolviamo con una rilassazione continuo amicabile GPU che consente anche di propagare retro通过解和学更合理的排列。如果你想了解我们实验的更多细节以及我们如何解决这些挑战，请查阅我们的论文或来到我们的海报。</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono in contrasto, ad esempio, quando una persona dice "Sono consapevole che le sigarette possono uccidermi" e quindi compra un paio di sigarette dopo una riunione. Questa credenza e azione sono in dissonanza, e hanno una relazione di dissonanza.</sample>
    <sample id="327">Our work, "ManagerTower: Aggregating the Insights of Uni-modal Experts for Vision-Language Representation Learning," aims to train a smart AI system that can understand both image and text. We propose a novel VL modal architecture called ManagerTower, which introduces managers in each cross-modal layer to gather and combine insights from pre-trained unimodal experts at different levels. This allows for more comprehensive cross-modal alignment and fusion by adaptingively exploiting different levels of unimodal semantic knowledge. Our model outperforms many base-size models pre-trained on 4 million data and surpasses some models trained with more data or parameters.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">This work presents a method for generating structured pseudo-labels to improve zero-shot video sentence localization. The proposed method uses a pre-trained image caption model to generate complex free-form pseudo-queries, and then models the temporal structure of events to generate pseudo-events that guarantee high relevance between videos within the events and queries, and low relevance between videos outside the events and query. To reduce the influence of label noise, the authors estimate label noise based on the model's predicted confidence and the IoU between the prediction and the pseudo-label, and weight these samples using the weights shown in the formula to reduce the contribution of noisy samples. They also train the prediction as a new pseudo-label for the next round of model training. Experiments on two datasets show that the proposed method outperforms other zero-shot methods on most metrics.</sample>
    <sample id="330">Sì, nel contesto dell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo. Questo è stato dimostrato nella tua presentazione, in cui si è confrontato lo策略 cumulative（accumula tutti i dati raccolti finora）与 lo策略 iterativo（aggiorna il modello ad ogni nuova rame di annotazione）. Lo策略 cumulative ha dimostrato essere uguale o migliore dello strategy iterativo in tutto il processo.</sample>
    <sample id="331">La relatrice o il relatore è Sara Papi.</sample>
    <sample id="332">I dati per il parametro di riferimento MuDa sono stati tratti da traduzioni parallele.</sample>
    <sample id="333">The paper presents a novel training framework called INK (Injecting kNN Knowledge in Nearest Neighbor Machine Translation) to enhance the generalization and performance of neural machine translation (NMT) models. The authors propose a two-step training loop that involves extracting kNN knowledge from a datastore to guide an adapter to adjust representation, followed by updating representations asynchronously. The framework aims to smooth predictions according to nearest neighbors in the representation space, addressing the sparsity problem in NMT models. Experiments on the WMT’19 German-English news translation task show that INK outperforms state-of-the-art kNN-MT systems, achieving an average gain of 1.99 COMET score and 1.0 BLEU score compared to the state-of-the-art kNN-MT systems.</sample>
    <sample id="335">Il relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo in cui un modello di intelligenza artificiale impara a riconoscere e interpretare query in diverse lingue utilizzando un insieme di linguaggi diversi per l'addestramento. In questo contesto, il modello impara a tradurre query in diverse lingue in espressioni semantiche equivalenti in linguaggi di programmazione come SQL o FunQL.</sample>
    <sample id="337">The paper presents a graph-based approach for out-of-vocabulary (OOV) word embedding learning. The authors propose a Word Relationship Graph that captures the lexical rules of word formation and association, and uses a graph neural network to process it. They address the challenge of assigning attributes to OOV nodes using a self-attention network and apply two levels of Graph Attention Network to concatenate and fuse the initial input with the hidden embedding of each layer. They also incorporate a readout block layer to capture the whole graph information and summarize the word formation. The model is trained using contrastive learning with NT-XENT positive samples from the graph, such as two-hop relevant neighbor words, synonyms, or the OOV word itself. The results show that the model performs superior to baselines in both intrinsic and extrinsic tasks, and can bring profits to both static and contextual models in downstream tasks. The model can handle various complex word formations and can be applied to other languages depending on the rationality of word decomposition.</sample>
    <sample id="338">The research presented by Bingsheng and his team focuses on evaluating the quality of human natural language explanations for machine learning models. The team proposes a novel metric called TREU, which extends the simulatability score to assess the helpfulness of explanations during fine-tuning and inference stages. They evaluate the metric across five datasets using two models, T5 and BART, and find that it outperforms the simulatability score in reflecting the benefits of human-annotated explanations. The study highlights the task-dependent nature of explanations and the importance of considering different explanation formats when evaluating their usefulness.</sample>
    <sample id="339">I fornitori dell'articolo sono affiliati all'Università di Saarland in Germania.</sample>
    <sample id="340">The paper presents ParaAMR, a large-scale dataset for paraphrase generation that leverages AMR (Abstract Meaning Representations) back-translation to generate syntactically diverse paraphrases. The authors propose using AMR graphs to capture the abstract meaning of a sentence and modify them by changing the focus, sampling a new root node, and modifying corresponding edges and their labels. This results in text with similar semantics but different syntax. The dataset consists of around 15 million source sentences with approximately 6.9 paraphrases per source sentence. The authors demonstrate that ParaAMR generates more syntactically diverse paraphrases than existing datasets while preserving good semantic similarity. They also show that ParaAMR benefits several NLP applications such as learning sentence embeddings, syntactic control paraphrase generation, and few-shot learning.</sample>
    <sample id="341">I ricercatori si basano su due misure di latenza: la media della ritardatura e la ritardatura computazionale consapevole.</sample>
    <sample id="342">The paper "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming" presents a unique video-sourced dialogue dataset for developing personalized dialogue applications such as virtual streamers and virtual employees. The dataset is constructed using an automatic dialogue-constructing method that extracts audio from Chinese TikTok videos, transcribes it into utterances, collects audience comments, and constructs dialogues through a reply-to-whom matching method. The paper conducts experiments on two benchmark tasks: response modeling and addressee recognition, and investigates the transfer learning of generation models to LiveChat. The results show that the extracted persona profiles and longer average sessions are beneficial in learning the speaker's personalized response, and that BART performs better than other LLMs in terms of domain distinctiveness. The paper concludes by proposing LiveChat as a valuable resource for developing personalized dialogue applications and future work on efficient transfer learning of LLMs for LiveChat.</sample>
    <sample id="343">Ciao a tutti, mi chiamo Akshatha e oggi io e il mio collaboratore Martin stiamo presentando il nostro lavoro "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources". Questo lavoro è un合作项目，由 McGill University、Mila 和 Microsoft Research 共同完成。模型在自然语言理解任务中会利用各种知识来源，包括预训练时获得的知识和输入时提供的知识。最近的研究表明，模型可以使用预训练时的知识来解决任务。然而，自然语言理解通常需要在推理时提供的知识。例如，在句子“John vide il nuovo presidente su TV”中，预训练参数可以包含关于总统和电视的信息，但无法可靠地知道这个实例化的实体“John”是谁，或者新总统是谁，因为总统自预训练以来可能发生了变化。因此，成功解决知识密集型NLU任务的模型需要能够整合和使用预训练时间和推理时间的知识。在这项工作中，我们提出了一个诊断测试套件，用于评估知识整合能力。我们引入了一个核心参考任务，旨在测试模型是否能够利用不同来源中的知识。我们用人类参与者和已建立的核心参考模型评估了数据集。以下是我们数据集中的一个示例。Servin是一名法官，Kea是一名面包师。Servin和Kea在公园里见过面。在一天的工作结束后，他很高兴在法庭上决定案件，他很开心能放松一下。任务是确定代词“他”所指的正确实体，即Servin。解决给定代词需要两种信息：实体特定知识，如“Servin是一名法官”，以及背景知识，如“法官在法庭上决定案件”。一般来说，背景知识是在大规模语言模型预训练过程中学习到的，而实体特定知识通常是在推理过程中观察到的。我们通过三种不同的KITMUS设置来改变这些信息的可获得性。第一种是典型的设置：“背景-预训练”，其中背景知识假设在预训练时可用。第二种是“背景-两者”，其中背景知识在预训练时间和推理时间中都可用。最后一种是“背景-推理”，其中所有知识类型都在推理时间中可用。这种最后一种设置特别有趣，因为它模拟了背景知识对解决任务至关重要的情况，但在预训练数据中并不包含。以下是我们控制数据集中事实可获得性的示例。在“背景-预训练”设置中，我们假设背景知识“政治家寻求当选政府职位”包含在预训练参数中，而在推理时间的上下文中，我们提供实体特定知识“Chichester是一名政治家”。在“背景-两者”设置中，我们不仅提供实体特定知识，还提供政治家的背景知识。在“背景-推理”设置中，我们提供虚构的职业“mirituer”，因为“mirituer”不太可能包含在预训练参数中。我们用人类参与者和已建立的核心参考模型评估了数据集。在这个图表中，我们展示了在最困难的“背景-预训练”设置中最优模型的结果。没有针对KITMUS进行特定任务训练，两个模型的表现都不好。然而，当它们被训练在KITMUS上时，两个C2F和BERT4Coref模型的表现显著优于随机选择。这表明，当它们被训练在通用引用分辨率数据集上时，大多数模型学会利用表面线索，这些线索在KITMUS测试中并不适用。额外实验显示，即使表现最好的模型也不能可靠地整合仅在推理时间提供的后向知识。总之，我们的论文的主要 takeaway是许多核心参考模型无法在没有特定任务训练的情况下推理来自不同来源的知识。然而，经过特定任务训练后，有些模型成功地整合了多种来源的知识。尽管如此，即使表现最好的模型似乎也无法可靠地整合仅在推理时间提供的后向知识。如果您想了解更多信息，请查阅我们的论文，并查看GitHub上的数据集和代码。谢谢您的聆听。</sample>
    <sample id="344">I metodi basati su alberi hanno diversi svantaggi. In primo luogo, i nodi degli alberi non sono forniti e devono essere ottenuti in qualche modo, che può essere complicato e spesso computazionalmente costoso. Inoltre, ottenere gli alberi può implicare un'accurata pre-elaborazione formale-specifica delle forme logiche, ad esempio per gestire simboli variabili. Infine, trovare il percorso più efficiente tra i nodi di un albero è un problema NP-difficile, come quello del "viaggiatore del mondo", il che rende difficile determinare la soluzione migliore.</sample>
    <sample id="345">This paper introduces a neural sequence-to-sequence model that achieves strong generalization to deeper recursion without relying on trees. The model predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output; second, it uses another model to predict a permutation to put them into the right order. The permutation model is flexible and expressive, allowing for the prediction of multiple permutations that are consistent with the data. The authors address the challenge of finding the highest-scoring permutation by approximating it with a GPU-friendly continuous relaxation that also allows backpropagation through the solution. Experimental results show that the proposed method outperforms other treeless models on the COGS benchmark, particularly on generalization to deeper recursion.</sample>
    <sample id="346">Mi dispiace, non ho accesso alle informazioni sulle affiliazioni degli autori dell'articolo.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parlerò del nostro articolo "Personaggi etichettati: Utilizzo di prompt linguistici per misurare i pregiudizi in modelli di lingua". Questo lavoro è stato realizzato in collaborazione con Esin Durmus e Dan Jurafsky. In ultimi anni, molti hanno documentato la prevalenza dei pregiudizi sociali e degli stereotipi in grandi modelli di lingua, o LLM. Tuttavia, questi metodi hanno diversi limiti. In genere, si basano su dataset costruiti a mano che richiedono molto tempo per curare e inoltre, solitamente misurano stereotipi molto specifici, il che significa che non generalizzano bene a altre demografie o contesti, o semplicemente catturano associazioni molto generali, come le associate a particolari gruppi. Inoltre, la maggior parte del lavoro in questo spazio non tiene conto dell'intersectionalità, che è la nozione che identità sociali multi-faccettate possano essere luoghi unici di danno. Per superare questi limiti, ci basiamo sul fatto che questi nuovi modelli addestrati con istruzioni sono molto bravi a rispondere a istruzioni e prompt. Quindi possiamo chiedere al modello di generare un personaggio, che è una rappresentazione di un individuo immaginario utilizzando un prompt come "Immagina che tu sia una donna asiatica. Descrivi te stesso". E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia poiché possiamo specificare qualsiasi segnaposto che vogliamo in questo prompt. Ecco alcuni esempi generati da GPT-4. Subito dopo, notiamo che, anche se gli output non sono negativi o tossici nel senso tradizionale di queste parole, ci sono alcuni interessanti modelli. La donna asiatica è descritta come insignificante; la donna del Medio Oriente viene riferita come "esotica" e "come", riferendosi a una regione mesmerizzante. E entrambe le donne di colore fanno riferimento all'antonomasa mentre il persona bianca ha niente di tutto. Per captare questi modelli, il nostro metodo ha due parti. La prima è generare questi personaggi. I nostri prompt per generare questi personaggi sono ispirati a un studio in cui venivano forniti questi prompt a soggetti umani, scoprendo che fornendo queste istruzioni ai soggetti umani, anche loro potevano rivelare stereotipi razziali. E anche questa consente una comparazione diretta tra i nostri personaggi generati e le risposte umane scritte. La seconda parte è il termine "parole etichettate", che è un metodo per identificare le parole che distinguono gruppi etichettati da gruppi non etichettati, che spiegherò subito. Il vantaggio di questo è che otteniamo modelli di stereotipi molto specifici senza dover reliquarti a un lexicon specifico. Così, il metodo Marked Words si basa sul concetto sociolinguistico di "etichettatura", che afferma che ci è un default non etichettato e qualsiasi gruppo che differisca da quell'default è linguisticamente etichettato. Quindi, per esempio, la parola "guerriero" è usualmente associata a uomini. Quindi quando si descrive un guerriero che è una donna, di solito si specifica "guerriero donna" e etichetta la parola con "donna". E più in generale, i gruppi dominanti in società sono sia linguisticamente che socialmente non etichettati, mentre i gruppi marginalizzati sono generalmente etichettati. Quindi in nostra methods, innanzitutto designiamo cosa siano i gruppi non etichettati e i gruppi etichettati, e quindi confrontiamo i personaggi utilizzando il metodo Fightin' Words, che èBasically utilizza le razionalità logiche ponderate per distinguere le parole principali per ciascun gruppo etichettato. Quindi, per esempio, per i personaggi di donne nere, faremo Fightin' Words e compariremo le razionalità logiche ponderate contro sia le persone bianche che i uomini perché queste sono le due corrispondenti gruppi non etichettati. Ora per i risultati. Allora, prima utilizziamo un lexicon di stereotipi e troppo che i personaggi generati contengono molto più stereotipi rispetto alle risposte umane scritte. Tuttavia, quando analizziamo la distribuzione delle parole e del lexicon, troppo diverse cose. Quindi, mentre i personaggi generati hanno tassi molto più alti di parole del lexicon, le risposte umane scritte hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono nei personaggi generati sono davvero solo le parole "alta" e "athlete". Quindi, in effetti, queste parole del lexicon non catturano molte delle patterns dannose che avevamo visto in precedenza. Quindi invece di farlo, useremo i risultati dal nostro metodo Marked Words per dimostrare come queste parole positive apparentemente facilitano i stereotipi e le narrazioni essenziali. In nostra analisi, riveliamo come queste rappresentazioni positive apparentemente connesse riflettono pattern dannosi. Prima, dalla nostra analisi, le parole principali includono cose come "cultura", "tradizione", "orgoglio" e "esotico". E queste parole definiscono queste gruppi solo dalla relazione alla loro identità e distinguendoli come diversi dalla norma bianca. Questo contribuisce a una lunga storia di discriminazione e alienazione per questi gruppi. Inoltre, ci sono molte troppe che sono riflettute in queste parole, specialmente per le donne di colore. Quindi, ad esempio, le parole che descrivono le donne latine includono cose come "vibrante" e "curvaceous" che si ricollegano a un trope di tropicalismo. Per le donne asiatiche, le parole sono cose come "petite" e "delicata" e "silky" che si ricollegano a una storia lunga di donne asiatiche che sono hipersexualizzate, viste come molto docili e sottomissi, e così via. Infine, per le donne nere, vediamo che alcune delle parole principali sono cose come "forte" e "resiliente". Questo si collega all'archetipo che si chiama "donne nere forti". E anche se suona positivo all'inizio, c'è stata la ricerca che dimostra che questo archetipo in reality è molto dannoso poiché mette una grande pressione su queste demografie per essere resilienti e forti contro le ostacole sociali, il che porta a negative uscite sanitarie per queste persone, tra gli altri danni. Inoltre, troppo le parole per ciascun gruppo etichettato quasi sempre riflettono narrativa essenziale. Quindi, sulla base di queste tendenze, conclude con tre raccomandazioni per i proprietari dei modelli. Prima, dobbiamo, come ricercatori, affrontare i stereotipi positivi e le narrazioni essenziali. Dobbiamo anche utilizzare un occhio di giudizio intersectionale per studiare i bias e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovrebbero esserci veramente maggiori trasparenze sulle tecniche di mitigazione dei bias, poiché, per esempio, come queste stereotipi positivi, non sappiamo se è per qualche tipo di alleanza eccessiva o forse metodi anti-stereotipi che stanno portando a queste pattern perniciosi. Non possiamo fare assunzioni o studiare ulteriormente senza maggior trasparenza. Grazie mille per l'attenzione. Buona fortuna per ACL.</sample>
    <sample id="348">This paper presents a method for measuring stereotypes in language models using natural language prompts. The authors argue that current methods have limitations, such as relying on hand-constructed data sets and only measuring specific stereotypes. They propose using instruction-tuned language models to generate personas based on different demographic identities. By comparing the generated personas with human-written responses, they identify marked words that distinguish between marked and unmarked groups. The results show that generated personas contain more stereotypes than human-written ones, but also reveal harmful patterns through essentializing narratives. The authors recommend addressing positive stereotypes, using an intersectional lens to study biases, and increasing transparency about bias mitigation methods.</sample>
    <sample id="349">Ciao a tutti, mi chiamo Jingwei Yi e sono dell'Università di Scienze e Tecnologie di Cina. Sono felice di presentare un video di pubblicità per il mio articolo. Stanno copiando il mio modello? Proteggere i diritti d'autorità dei modelli di lingua grandi per embedding come servizi tramite segnalini nascosti. Innanzitutto, introduciamo lo sfondo riguardante l'embedding come servizi. Attualmente, i modelli di lingua grandi come GPT, LLAMA e PALM sono eccezionali nella comprensione e nella generazione di linguaggi naturali. L'embedding come servizi è uno dei servizi costruiti su modelli di lingua grandi per assistere a diverse attività NLP. Ad esempio, OpenAI offre un API di embedding basata su GPT. Tuttavia, recenti opere hanno dimostrato che un attaccatore può rubare il modello attraverso imparare da embedding e fornire servizi simili. Di conseguenza, è necessario proteggere i diritti d'autorità dell'embedding come servizi. Per proteggere i diritti d'autorità dell'embedding come servizi, una delle soluzioni è di immergere un segnalino nascosto nel servizio fornitore e rilevare se un altro servizio contiene il segnalino. Metodi di segnalino devono soddisfare le seguenti proprietà: prima, il metodo dovrebbe essere applicabile all'embedding come servizi. Secondo, il segnalino non dovrebbe indebolire l'utile degli embedding forniti. Terzo, il segnalino dovrebbe essere abbastanza nascosto per gli attacchatori o gli attacchatori possono facilmente rimuovere il segnalino. Infine, il segnalino dovrebbe essere trasferibile al servizio dell'attaccatore durante il processo di estrazione del modello. Opere esistenti possono essere generalmente classificate in quattro category. Tuttavia, questo metodo non è applicabile all'embedding come servizi o manca di trasferibilità. Pertanto, in questo articolo propone Embedding Marker, un metodo di segnalino basato sul backdoor applicabile all'embedding come servizi. Introduciamo i dettagli del nostro segnalino embedding. Embedding Marker contiene due passi principali: iniezione di segnalino e verifiche di copyright. Prima di questi passi principali, scegliamo prima di tutto un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Ci si assume che il fornitore possa raccolta un corpus di testo generale e conteggia la frequenza delle parole con esso. In iniezione di segnalino, definiamo innanzitutto un embedding bersaglio. Quando un utente invia una frase al servizio fornitore, il fornitore conteggia il numero di trigger nella frase. L'embedding fornito è una somma ponderata dell'embedding bersaglio e l'embedding originale. Il peso dell'embedding bersaglio è proporzionale al numero di trigger nella frase. Se il numero di trigger nella frase è maggiore di m, l'embedding fornito è esattamente uguale all'embedding bersaglio. Verifica di copyright è per rilevare se un modello dietro un altro servizio contiene il segnalino. Innanzitutto costruiamo un backdoor e un insieme di dati benigni. L'insieme di dati benigni contiene frasi in cui tutti i vocaboli appartenono all'insieme di trigger mentre tutti i vocaboli nelle frasi di insieme di dati benigni non appartenono all'insieme di trigger. Poi il fornitore richiede gli embeddings dal servizio rubacchie con il dataset. La similarità di coseno e L2 tra gli embeddings richiesti e l'embedding bersaglio viene calcolata. Simultaneamente, applichiamo anche il test KS e utilizziamo il suo p-value come il terzo metrico. Abbiamo svolto esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Ci si assume che il fornitore applichi un dataset wiki per conteggio frequenza delle parole. I risultati sui quattro dataset hanno dimostrato che il segnalino embedding Marker può avere un'ottima prestazione di rilevamento mentre mantiene grande utilità per le attività downstream. Abbiamo anche validato la copertura della fornitrice embedding utilizzando la visualizzazione degli embeddings delle frasi su quattro dataset [INATTIVO 4:39]. La legenda delle figure significa il numero di trigger in ogni frase. Come è evidente dalle figure, è difficile distinguere tra gli embeddings del backdoor e gli embeddings normali. Questo è tutto. Grazie. Benvenuti a discutere con noi.</sample>
    <sample id="350">The paper "What’s the Meaning of Superhuman Performance in Today’s NLU?" by Simone Tedeschi and several renowned researchers investigates how reliably leaderboard scores compare models and humans in NLP and NLU. The authors analyze two popular benchmarks, SuperGLUE and SQuAD, and discover that systems outperform humans on these benchmarks. However, they also identify several sources of error that make the comparison unfair, such as differences in test sets and ground-truth answers. The authors argue that claims about superhuman performance are not scientifically meaningful due to these issues and provide recommendations to avoid repeating them and construct more reliable benchmarks.</sample>
    <sample id="351">The paper titled "Do CoNLL-2003 named entity taggers still work well in 2023?" by Shuheng investigates the problem of generalization using the Named Entity Recognition Task (NER) and develops the CoNLL++ Dataset to evaluate the performance of over 20 models fine-tuned on CoNLL-2003. The results show that transformer models, larger model size, and more fine-tuning examples are needed for good generalization. Temporal drift is identified as the main cause of performance drop, not adaptive overfitting. The paper concludes that CoNLL-2003 taggers still work well in 2023 and calls for further research on improving model generalization.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio dimensionale per valutare gli AI conversazionali. Questo metodo cerca di ridurre la soggettività dell'evaluation umana, annotando esplicitamente se ogni risposta del modello esprime certi comportamenti, come fornire informazioni irrelevanti o contradire il proprio interlocutore. Questo approccio è capace di misurare le tassi in cui i modelli dei chat sbagliano su vari errori tematici, come ignorare il proprio partner, dire qualcosa di irrelevante, contradire se stessi o il proprio interlocutore, hallucinare fatti sbagliati o violare le conoscenze di senso comune, e quando il modello riesce o fallisce a dimostrare empatia.</sample>
    <sample id="353">The paper "Python Code Generation by Asking Clarification Questions" by Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, and Iryna Gurevych addresses the challenge of input underspecification in code generation and program synthesis. The authors propose a method to create CodeClarQA, a synthetic dataset with clarifications on key operations, and a pipeline of code generation by asking clarification questions. They identify key operations and corresponding documentation from the code, represent them in latent space using their schemata, and compute similarity scores of all schema element pairs between an NLD and the operation documentation. If all element pairs for the similarity score is lower than threshold T, the key operation is missing; otherwise, it is aligned. They also hire annotators to annotate the validation set and the test set. They adopt templates to create CQAs for missing key operations and generate two types of questions: yes-or-no questions or multiple-choice questions. They use heuristics to extract key operations based on the code knowledge graph generated by Graph4Code and show the generated graph of the example. Each node is an operation where the key operations are marked in red and the rest in gray. Edges show the data flow. They identify if a key operation is missing or aligned. They notice that MPNet has the best performance of identifying missing key operations among all these models. They also notice some common errors which reflects the challenges and potential directions to improve their method, including taxonomy, that aligned operations might require clarification to be distinguished from operations with similar names, and argument, where they use documentation of operations instead of using argument values as well. They have a Clarification Need Predictor, a Question Selector, and a Code Generator. They test their pipeline and see that model performances on all evaluation metrics, including with more high-ranked CQs being answered and included, increases. However, there's an opposite trend of unanswered clarifications, and at the same time, their pipeline is still underperforming the model-only trainer NLDs and code, which is expected as they fine-tune the models on all CQAs and CQ ranking task is a challenge. They analyze the results and see that clarified key operations are the reason for better generated code. They give some examples of predictions and see that training with Oracle CQAs leads to predictions close to the ground truth with only minor differences. However, the task is challenging as the top five ranked CQs do not include CQs in the reference CQAs, leading to the pipeline prediction including a call of confusion matrix missing the classes mentioned here.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2019.</sample>
    <sample id="355">Ciao, mi chiamo Vasudha e sono un candidato per il dottorato in Informatica alla Stony Brook University. Vorrei presentare il nostro lavoro accettato in ACL 2023 come articolo lungo, intitolato "Apprendimento a transfer per la rilevazione della dissonanza: affrontando il problema della classe rara". Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare nel linguaggio. In parole semplici, la dissonanza cognitiva è quando due credenze o azioni sono in contrasto, ad esempio, questa persona dice "Sono consapevole che i sigilli potrebbero uccidermi", e quindi dice "Ho preso una couple di sigilli dopo la riunione". Questa credenza e azione sono in dissonanza. Ulteriormente menzionano "Non penso che potrei tenere mio lavoro senza loro" che giustifica la seconda occorrenza. Queste credenze hanno una relazione di consonanza. Mentre la dissonanza è un fenomeno molto comune che esperiamo in decision-making quotidiano, è davvero raro trovare espressioni di dissonanza nel linguaggio tra le altre relazioni discorsive. Quindi, perché importa? Studiare la dissonanza può aiutarci a comprendere gli effetti della discordia tra le persone, tracciare tendenze e valori di credenze, e capire le modifiche dell'attitudine nella popolazione. Una alta dissonanza cognitiva è anche legata a disturbi ansiosi e può aiutarci a comprendere meglio lo stato mentale delle persone. Studiare la dissonanza espressa nel linguaggio può anche essere utile per comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili. Infine, la dissonanza è importante per comprendere i propri stili cognitivi personali e aiuta a comprendere meglio i processi di decisione. Con lo scopo di creare un risorsa per la dissonanza, abbiamo condotto un'annotazione su larga scala delle relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza prima, come vedere nel diagramma qui. I tweet erano passati utilizzando il parser PDTB, e le coppie di unità discorsive erano annotate in accordo con le direttive descritte nel nostro articolo. Come si può vedere qui, la dissonanza è stata trovata solo in 3,5% delle coppie annotate. Dopo la raccolta di circa 1,000 esempi di coppie di unità discorsive, si è eseguito un training per un classificatore iniziale addeeso solo a 43 esempi di dissonanza. Non sorprende che il classificatore non abbia performato molto meglio del caso. Data la bassa occorrenza di dissonanza e l'assenza di qualsiasi altro dataset precedente, stiamo affrontando il problema della rarezza assoluta. Per alleviare questo, esperimentiamo con combinazioni di apprendimento a transfer e impegno attivo per annotare in modo che possiamo raccogliere più esempi di dissonanza in meno iterazioni di annotazione, riducendo i costi di annotazione in generale mentre miglioriamo la deteczione di dissonanza. Poiché il modello iniziale non è stato in grado di catturare la classe di dissonanza affatto, iniziamo il processo di impegno attivo trasferendo i pesi da compiti strettamente correlati. Trasferiamo da due compiti diversi: la classificazione dell'orientamento della posizione di dibattito, un compito che determina se due dichiarazioni di dibattito provenienti da diverse persone sono in accordo o in disaccordo, indipendentemente dal argomento, chiamato dibattito qui, e la classificazione binaria di classi di espansione e confronto di PDTB, poiché queste due sono strettamente correlate alla concezione di consonanza e dissonanza e chiamiamo CE qui. Troiamo che il trasferimento del prest表在标注数据集上的零-shot性能已经比随机高得多，最佳的AUC为0.62。进一步，通过迭代微调CE任务，然后进一步微调debate任务，我们发现CE任务的进一步微调后零-shot性能更好。因此，这是我们在冷启动impegno attivo usato. Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni iterazione di impegno e annotazione. "Cumulative" accumula tutti i dati raccolti finora da impegno annotazione, mentre "Iterative" aggiorna il modello addezzando i recenti set di dati raccolti. Di fronte a diverse strategie, abbiamo scoperto che Cumulative ha prestazioni uguali o migliori di Iterative in tutto. Infine, per migliorare il numero di esempi di dissonanza, utilizziamo una strategia di impegno attivo per la classe rara — PRC — per selezionare principalmente gli esempi che sono più probabili di essere scenduti dalla classe rara attuale. Confrontiamo questa con le altre strategie di impegno attivo state-of-the-art comunemente utilizzate nella comunità. Troiamo che la nostra strategia PRC funziona meglio delle altre strategie di impegno attivo state-of-the-art, anche se la differenza è piccola. Notiamo che le prestazioni sono significativamente più basse per random. In successive iterazioni di impegno con due migliori strategie, miglioriamo l'AUC di classificazione di dissonanza a 0.75, che è la migliore prestazione che abbiamo ottenuto finora sul compito. Abbiamo anche controllato la fattibilità di ogni strategia per la qualità dell'annotazione e i costi per gli annotatori. Troiamo che PRC ha la maggior percentuale di dissonanza e funziona meglio per la classe rara. Tuttavia, gli annotatori ritrovano gli esempi difficili. In sintesi, troiamo che PRC è una strategia di impegno attivo semplice per la raccolta di esempi di classe rara e il cold-start di impegno attivo con un trasferimento di apprendimento appropriato e aiuta in modo significativo. Abbiamo anche scoperto che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre in annotazioni attive nel dominio i costi di annotazione sono migliori con aggiornamento cumulativo. Questi sono i link al nostro dataset principale e al nostro articolo. Sentiti liberi di contattarci se hai domande. Grazie.</sample>
    <sample id="356">I fornitori dell'articolo sono Matthias Lindemann, Alexander Koller e Ivan Titov.</sample>
    <sample id="357">Il nome della relatrice o del relatore è Siyu Yuan.</sample>
    <sample id="358">Ci sono cinque autori coinvolti nell'articolo: Kayo Yin, Patrick Fernandes, Emmy Liu, André F. T. Martins e Graham Neubig.</sample>
    <sample id="359">L'approccio proposto è confrontato con la migliore architettura dedicata per la traduzione contemporanea.</sample>
    <sample id="361">The presentation introduces "CounterComp," a method for enhancing compositional generalization in multi-step quantitative reasoning tasks, particularly in question answering. The approach leverages counterfactual scenarios to train models to focus on meaningful tokens and operations rather than memorizing spurious patterns. By treating each training sample as an anchor and mining positive and negative examples, the method adds an auxiliary metric learning loss that adjusts based on the extent of change in the input questions. This technique improves performance on both in-distribution and out-of-distribution samples, demonstrating its potential for better generalization.</sample>
  </task>
</testset>