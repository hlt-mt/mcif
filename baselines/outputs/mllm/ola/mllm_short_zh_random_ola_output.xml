<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络爬虫数据，其中政治新闻媒体被广泛覆盖。</sample>
    <sample id="1">根据幻灯片上显示的英语内容，这篇论文的作者所属机构是麦克盖尔大学、米拉和微软研究。</sample>
    <sample id="2">欢迎来到我们的关于德国文本识别的演示，包括文档级别和句子级别的新语料库。</sample>
    <sample id="3">我的名字是雷吉纳·施托登，我将指导您进行演示的第一部分。让我们首先定义文本简化。</sample>
    <sample id="4">文本简化是一种过程，通过调整文本以改善特定目标群体对文本的理解，特别是阅读困难的人或非母语使用者。</sample>
    <sample id="5">为了训练一个文本简化模型，我们需要平行的文本对，例如文档或句子。</sample>
    <sample id="6">在示例中，你可以看到一个复杂德文句子和它的简化德文翻译。</sample>
    <sample id="7">为了简化句子，可以采用不同的技术，如图中的示例所示，例如词替换、从句删除、从句重排或插入单词。</sample>
    <sample id="8">现在我们提出了一个新的语料库，叫做De-plain。因为在最近几年里，现有的语料库存在一些问题。例如，这些语料库太小了，无法训练一个文本分类模型。</sample>
    <sample id="9">最近几年提出的其他三种模型都是自动对齐的，这意味着它们可以被证明在对齐中存在错误。</sample>
    <sample id="10">因此，我们提出了一个新的语料库DePlain，它被分为两个子语料库：DePlain-APA和DePlain-WEB。DePlain-APA基于新闻文本。</sample>
    <sample id="11">在DeepPlain API中，我们手动对483篇文档进行了对齐。这产生了大约3万至1.3万对平行句子对。</sample>
    <sample id="12">对于DeepPlainWeb，这个语料库包含了不同的领域，并且我们还手动和自动对这750篇文档进行了对齐。</sample>
    <sample id="13">总共我们得到了3450个句子段落。</sample>
    <sample id="14">我们进一步分析了我们的句子对。例如，在简化类型方面，</sample>
    <sample id="15">正如你所见，圣经文本简化程度比新闻文本或其他语言学习文本要强得多。</sample>
    <sample id="16">在所有级别上，包括例如词汇简化、结构简化以及总体简化等各个方面。</sample>
    <sample id="17">此外，您还可以看到我们的Deplain语料库具有不同简化变换的高多样性。例如，在Deplain API语料库中，我们有很多重排和词添加，而在Deplain Web语料库中则没有。</sample>
    <sample id="18">另一方面，在网络语料库中，我们有很多改写。</sample>
    <sample id="19">所以让我们看看我们可以用这个语料库做什么。你好，我是阿米尔，现在我要谈谈我们的数据集DeepL的用例。首先，第一个用例是评估自动对齐方法。</sample>
    <sample id="20">近年来，有很多对齐方法，但在机器翻译的背景下。</sample>
    <sample id="21">在我们有两个用不同语言写的平行文档的情况下，我们想要提取两份文档中的句子对齐。</sample>
    <sample id="22">但在我们的用例中，我们试图从两个具有相同语言和相同内容但复杂度不同的并行文档中提取句子对齐。</sample>
    <sample id="23">现在我们有了数据集DeepPlain，其中包含手动对齐的句子，我们可以使用这些句子作为标准对齐来评估一些提出的对齐方法。</sample>
    <sample id="24">我们对所提出的方法进行了些调整，并在论文中出版了所有这些调整和运行实验的代码。</sample>
    <sample id="25">最后我们得出结论，对于德文本简化使用的最佳自动对齐方法是MSTAlign。</sample>
    <sample id="26">你还可以在论文中找到在自己的文档上运行此方法的代码。</sample>
    <sample id="27">我们论文中展示的第二个用例是自动文本简化。</sample>
    <sample id="28">通过微调语言模型来生成从复杂输入文本中简化后的文本。</sample>
    <sample id="29">我们已经微调了两个不同的模型。我们已经微调了Long Impart的模型，以生成文档级别的简化文本。</sample>
    <sample id="30">我们还微调了正常基础的长尾，即正常基础的导入，以生成句子级别的简化。</sample>
    <sample id="31">你还可以找到所有的检查点，并且你可以查看我们实验的分数和评估指标的更多细节。</sample>
    <sample id="32">我们得出结论，这些基本微调可以产生或获得比 baseline 分数更好的分数。</sample>
    <sample id="33">我们提出了这些结果作为未来自动文本简化问题的基准，一个基准。</sample>
    <sample id="34">谢谢大家的聆听，我们希望在会议期间与 everyone 面见。谢谢</sample>
    <sample id="35">演讲者的名字是Kayo Yin。</sample>
    <sample id="36">根据幻灯片中提供的信息，他们使用了T5 XL模型获得了82%-87%的准确率。</sample>
    <sample id="37">是的，CoNLL-2003 标注器仍然有效。</sample>
    <sample id="38">根据英语内容，提出的人工评估方法新颖之处在于它通过明确注释每个模型回应是否表达特定行为来减少人类评估的主观性。这些行为包括提供无关信息或自相矛盾。</sample>
    <sample id="39">根据英语内容，现有弱监督方法的成功在很大程度上依赖于干净验证样本。</sample>
    <sample id="40">根据英语内容，可以采取的措施包括：1. 与客户建立牢固的关系，以确保他们感到舒适并愿意分享信息。2. 提供清晰而简洁的解释，以帮助客户理解问题。3. 保持专业和热情的态度，这有助于营造积极的氛围。4. 确保沟通渠道畅通，以便客户可以轻松地联系到你。5. 保持对客户反馈的开放态度，并根据他们的建议进行调整。</sample>
    <sample id="41">根据幻灯片显示，这篇论文有五位作者：Dawei Zhu、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="42">大家好，我是Adam Przpielkowski。这次演讲的主题是“连词结构中的依赖关系”。</sample>
    <sample id="43">正如您所知，不同的依赖结构由不同的理论和语料库方法所采用。例如，在普遍依赖性中，连接Lisa、Bart和Maggie的结构是协调结构。</sample>
    <sample id="44">是这样的，第一个连词是整个从句结构的主干，在这个例子中是Lisa。</sample>
    <sample id="45">类似的处理方式在伊戈尔·米特鲁克的意义文本理论中被假定，其中整个连词组结构由第一个连词来领导。因此，这两种方法是对称的；它们突出一个连词。</sample>
    <sample id="46">此外，还有一些对称的方法来处理并列结构，如Prague方法、连词头部方法等。在这些方法中，连词头部的并列结构由连词来领导。</sample>
    <sample id="47">所以，我们从头到尾获得一些依赖性。</sample>
    <sample id="48">最后，还有一个多主语的方法，例如在德卡森的词法语法中使用。</sample>
    <sample id="49">我们可以说所有的连词都是从句结构的主语。因此，我们得到了从“喜欢”到所有连词的依赖关系。这些是巴特和麦基。</sample>
    <sample id="50">现在的这篇论文旨在提出一种新的论点，支持对称协调结构，如这两个例子所示，并反对不对称协调结构，如这两个例子所示。</sample>
    <sample id="51">好的，这个论点是基于依赖长度最小化原则的，他们根据这些例子进行解释。</sample>
    <sample id="52">所以，在英语中，正如你所知的，直接宾语更喜欢靠近动词，而介词短语可能离动词更远。所以，“March read it yesterday”是好的，因为直接宾语“it”离动词“read”很近。</sample>
    <sample id="53">而 March read yesterday it 是 much worse, right? because uh here between the verb and the direct object there's an adjunct yesterday.</sample>
    <sample id="54">然而，这种效果可能会在直接宾语非常重和很长时被缓解，因为在这种情况下，它可以在动词后面的位置移动。</sample>
    <sample id="55">这被说明了。所以这两个句子都是正确的。March读了一本关于蜜蜂的绝对 fascinating 的书，昨天是OK的。相反，我们有这个长NP。</sample>
    <sample id="56">但说“昨天我读了一本绝对 fascinating 的关于蜜蜂的书”也是可以的。</sample>
    <sample id="57">所以这里的逻辑是，这有可能是因为尽管这个句子违反了直接对象应该紧靠动词的句法原则，</sample>
    <sample id="58">它满足了依赖长度最小化原则，该原则规定较短的依赖关系更可取。</sample>
    <sample id="59">所以，这两个树只显示了关键依赖关系的长度，即这些结构中不一致的部分。</sample>
    <sample id="60">所以，我们有从“red”到第7个短语的依存关系，长度为7个单词，并且从“red”到“book”的长度为4个单词。因此，总共是11个单词。</sample>
    <sample id="61">当你移动、交换这两个成分时，这两个依赖关系的总和变为6，对吧？而不是11，6更短。这就是为什么这听起来挺好的原因。它违反了一个原则，但满足了另一个原则。</sample>
    <sample id="62">所以，我们做了什么？我们从增强版的Penn树库中提取了关于共指的一些统计信息，并查看了论文“为什么不用大学依赖性？”</sample>
    <sample id="63">这些统计结果确认了很多人之前做出的观察，即左连词通常较短。盐和胡椒、不和盐等长度以音节为单位测量。</sample>
    <sample id="64">此外，在提及中观察到的趋势是随着长度差的增加而增长。</sample>
    <sample id="65">所以，当两个从句的长度差异增大时，较短的从句更倾向于成为第一个从句，对吧？所以比例更大的是左边较短的从句。</sample>
    <sample id="66">但这篇论文的新颖之处在于我们观察到，这种趋势只在主语位于左侧或不存在时才会发生。</sample>
    <sample id="67">所以，这个例子中主语是在左边的。我看见了巴特和Lisa。所以主语是在左边的。</sample>
    <sample id="68">它出现在第二个例子中，Homer came and sneezed。这里有两个动词的并列，没有外部的外部主语。因此，在这种情况下，左边的分句倾向于更短，而且两个分句之间的差异更大。</sample>
    <sample id="69">然而，当主语在右边时，比如“Laugh governs the coordination between Ned”，这种效应消失。</sample>
    <sample id="70">所以我们在第一列测量字符长度，在中间一列测量 syllables 的长度，在第三列测量 words 的长度。我们将集中精力讨论第三列。</sample>
    <sample id="71">这里我们看到，当Governor在左侧时，</sample>
    <sample id="72">对于左连接的较短趋势随着绝对长度差的增加而稳步增长，而在没有治理的情况下，这种趋势在协调句子时也观察到了。但是，当治理者在右边时，这种趋势消失。</sample>
    <sample id="73">我们在论文中展示了如何通过这些数据来 arguments against asymmetrical structures of coordination, as these two, and for the symmetrical structures, as these three.</sample>
    <sample id="74">所以请看论文以获取完整的论点和 arguments，对不起，然后在海报会期间找我们交流。谢谢</sample>
    <sample id="75">根据幻灯片显示，这篇论文有三位作者：Mathias Lindemann、Alexander Koller和Ivan Titov。</sample>
    <sample id="76">根据图片显示，圣经文本的简化程度比新闻文本或语言学习者文本更大。</sample>
    <sample id="77">盐和胡椒、胡椒和盐。</sample>
    <sample id="78">是的，这些模型可以在GitHub仓库中获得。</sample>
    <sample id="79">根据提供的信息，DEplain-apa 包含来自新闻文本的内容。</sample>
    <sample id="80">更好的模型架构、更大的模型规模以及更多的微调示例有助于良好的泛化。</sample>
    <sample id="81">通过测量字符长度，即第一个列的音节、中间列的音节和第三个列的音节。</sample>
    <sample id="82">要研究支配词位置的影响，实验可以设计为比较不同位置的支配词对短语长度的影响。例如，可以将文本分为两组，一组中支配词位于短语的左侧，另一组中支配词位于右侧。然后，可以测量每种情况下短语的平均长度，并进行统计分析以确定位置是否显著影响短语长度。</sample>
    <sample id="83">根据所提供的内容，基线分类器在不平衡数据上的训练效果不佳。分类器的表现“不如机率”，这意味着它没有显著优于随机猜测。这种结果可以归因于数据集中不平衡的类别分布，导致模型难以学习区分少数类别的模式。</sample>
    <sample id="84">根据幻灯片显示，这篇论文有四位作者。</sample>
    <sample id="85">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="86">在正式性和词汇连贯性等话语现象上，语境感知 MT 模型比语境无关模型更有优势。</sample>
    <sample id="87">根据幻灯片上显示的标志和文字，论文的作者来自约翰霍普金斯大学、普渡大学和麻省理工学院。此外，幻灯片底部的标志表明Meta AI也是这篇论文的贡献者或赞助者。</sample>
    <sample id="122">引入的框架通过比较用户与模型和数据集预测和标签之间的相关性得分来量化立场。</sample>
    <sample id="155">根据所给的英文内容，之前的研究结果是，当人类受试者被给予相同的人格化提示时，他们能够表面种族刻板印象。</sample>
    <sample id="156">根据提供的信息，这项研究使用了增强版的Penn Treebank数据集。</sample>
    <sample id="157">根据幻灯片显示，这篇论文有两位作者：Adam Przepiórkowski和Michał Wójcik。</sample>
    <sample id="158">与认知失调密切相关的任务是扩展和比较类别的二元分类。</sample>
    <sample id="159">根据幻灯片上显示的英文内容，这篇论文有两位作者。</sample>
    <sample id="160">根据幻灯片显示，这篇论文有六位作者。</sample>
    <sample id="161">引入的框架与以前的研究不同之处在于它通过比较用户、模型和数据集的预测和标签，而不是仅仅关注注释员之间的标注一致性或建模注释员分布。</sample>
    <sample id="162">在三个比较设置中，GPT-4与刻板词汇的重叠最多。</sample>
    <sample id="163">比较了DeepL和Google Translate。</sample>
    <sample id="164">你好，我是张冰，是华盛顿大学的研究生。今天我要展示我们团队的研究成果，从预训练数据到语言模型再到下游任务，跟踪政治偏见导致不公平NLP模型的轨迹。</sample>
    <sample id="165">语言模型是在大规模网络爬虫数据上训练的。</sample>
    <sample id="166">政治新闻媒体在预训练数据中得到很好的覆盖。根据对C4语料库的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》、《 Huffington Post》等都在自然语言模型训练数据中得到很好的覆盖。</sample>
    <sample id="167">这已经为语言模型应用带来了双重祝福。</sample>
    <sample id="168">一方面，他们能够学习来自不同观点的资料，这促进了民主和思想的普遍性。另一方面，这些不同的政治观点是固有的社会偏见，可能会导致潜在的公平性问题在下游任务应用中出现。</sample>
    <sample id="169">为了达到这个目的，我们提出了一个从预训练数据到语言模型再到下游任务的Political bias propagation pipeline。具体来说，我们将通过提出以下问题来研究这一过程：</sample>
    <sample id="170">首先，我们如何评估语言模型的政治倾向，并且预训练数据可能在其中扮演什么角色？</sample>
    <sample id="171">其次，不同政治偏见的语言模型在下游任务上的表现如何？这是否会导致自然语言处理应用中的公平性问题？</sample>
    <sample id="172">首先，我们提出了用政治问题，如政治 compass 测试等，对语言模型进行不同的提示格式的测试。这确保了我们使用政治科学文献中的自动评估方法。</sample>
    <sample id="173">所以，一些初步的结果表明，首先，语言模型具有不同的政治倾向。它们在政治坐标系的四个象限上都存在。</sample>
    <sample id="174">我们可以看到，GPT-4是所有模型中最自由的语言模型。GPT系列通常比BERT系列及其变体更倾向于自由主义。</sample>
    <sample id="175">其次，我们旨在探讨政治偏见的语言模型在多大程度上是从训练数据中吸收的。</sample>
    <sample id="176">因此，我们可以通过在六个不同的党派子集中进一步预训练语言模型检查点来开展受控实验，这些子集被分为新闻和社交媒体，进一步分为它们的政治倾向。</sample>
    <sample id="177">通过在这样的 partisan corpora 上进一步预训练语言模型，我们可以看到语言模型的意识形态坐标也会相应地发生转变。</sample>
    <sample id="178">例如，对于Roberta，进一步微调和进一步训练在左翼新闻Reddit语料库上，我们可以看到其在政治倾向方面出现了显著的自由主义转变。</sample>
    <sample id="179">在政治偏见方面，该图表展示了原始文本与修改后文本之间的关系。原始文本被放置在红色和绿色方块中，代表右翼和左翼。红色方块表示右翼，绿色方块表示左翼。修改后的文本被放置在蓝色方块中，表示中心派别。图表显示了原始文本和修改后文本之间的转变，以及它们在不同政治派别中的位置。</sample>
    <sample id="180">我们还试图研究语言模型是否能捕捉到我们现代社会中普遍存在的 polarization。</sample>
    <sample id="181">我们把预训练语料库分为特朗普当选之前和之后。我们分别在两个不同的时间间隔上单独预训练语言模型。</sample>
    <sample id="182">我们可以看到，语言模型在2017年后普遍表现出更远离中心的右翼政治倾向。这表明语言模型也可以反映出我们社会中的 polarization。</sample>
    <sample id="183">最后但同样重要的是，我们评估了不同政治偏见的语言模型在 hate speech detection 和 fake news detection 等 NLP 应用中的表现，这些应用通常涉及语言模型，并可能具有重大的影响。</sample>
    <sample id="184">我们看到，如果我们调查每个类别的表现，也就是说，如果我们把表现分成不同的类别，我们可以看到一些有趣的趋势。首先，我们注意到针对不同身份群体的 hate speech 的表现存在差异。例如，在“黑人”类别中，表现最好的是“新闻左翼”，而表现最差的是“右翼”。类似地，在“穆斯林”类别中，“新闻左翼”表现最好，而“右翼”表现最差。在“LGBT+”类别中，“新闻左翼”和“左翼”都表现得很好，而“右翼”表现较差。在“犹太人”类别中，“新闻左翼”和“左翼”表现得很好，而“右翼”表现较差。在“亚裔”类别中，“新闻左翼”表现得很好，而“右翼”表现较差。在“拉美裔”类别中，“新闻左翼”表现得很好，而“右翼”表现较差。在“女性”类别中，“新闻左翼”表现得很好，而“右翼”表现较差。在“基督教”类别中，“新闻左翼”表现得很好，而“右翼”表现较差。在“男性”类别中，“新闻左翼”表现得很好，而“右翼”表现较差。在“白人”类别中，“新闻左翼”表现得很好，而“右翼”表现较差。总的来说，我们可以看到，针对不同身份群体的 hate speech 的表现存在差异，而“新闻左翼”和“左翼”通常表现得很好，而“右翼”通常表现较差。</sample>
    <sample id="185">对于不同的人口统计或政治倾向的新闻媒体，我们可以看到一个模式，例如，在 hate speech detection 中，左翼语言模型表现更好。</sample>
    <sample id="186">在检测针对社会少数群体的 hate speech时，该表显示了不同身份群体和不同来源的性能。黄色表示最佳性能，深蓝色表示最差性能。根据表中数据，可以推断出在检测 hate speech 时，某些模型或方法比其他模型或方法更有效。例如，在“新闻左翼”类别中，一个模型的性能最好，而另一个模型的性能较差。同样，在“社交媒体右翼”类别中，一个模型的性能最好，而另一个模型的性能较差。因此，该表提供了有关不同模型或方法检测 hate speech 的性能的见解。</sample>
    <sample id="187">然而，我们更难检测针对我们社会中更强大的群体的 hate speech。</sample>
    <sample id="188">相反，右翼语言模型在检测针对白人和男性 hate speech方面表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的 hate speech方面较差。</sample>
    <sample id="189">对于假新闻检测，我们同样观察到类似的趋势，即左翼倾向的语言模型在检测来自其对立政治立场的 misinformation方面效果更好，反之亦然。</sample>
    <sample id="190">我们将展示许多质性示例，以证明具有不同政治偏见的语言模型。</sample>
    <sample id="191">根据社会类别对 hate speech 和 misinformation 示例进行不同的预测。附录中还有一大堆更多的示例，以进一步强调这一点。</sample>
    <sample id="192">这表明了关于语言模型政治偏见的公平性问题非常突出。</sample>
    <sample id="193">例如，如果一个右翼语言模型被训练来检测和删除 hate 信息或错误信息等，然后部署到一个受欢迎的社交媒体平台，</sample>
    <sample id="194">这可能会导致持有相反政治观点的人被边缘化，针对少数群体的 hate speech 可能 rampant 且不受控制。</sample>
    <sample id="195">因此，这促使我们承认和解决由语言模型政治偏见导致的公平性问题。</sample>
    <sample id="196">讨论一下，我们还希望强调一点，那就是我们在处理自然语言模型政治偏见问题时面临着一个独特的 dilemma，就像介于 Scylla 和 Charybdis 之间的两难选择一样。</sample>
    <sample id="197">如果我们在自然语言处理数据中不进行政治观点的消毒，那么偏见将从预训练数据传播到语言模型，再到下游任务，最终导致公平性问题。</sample>
    <sample id="198">如果我们试图消毒，我们也会面临敏感性或排除的风险。确定什么应该被保留是语言模型训练数据中真正中性的，这是非常困难的。这就像电学中的电气短路问题一样。</sample>
    <sample id="199">收到英文内容后，用中文表述其意思。</sample>
    <sample id="200">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="201">根据图片中呈现的图表，MPP 评估最多涵盖了一千零二十六个词元的上下文长度。</sample>
    <sample id="202">根据提供的信息，他们的数据集中包含各种领域。例如，有一个关于钢琴音乐的例子，一个没有单词的例子，一个关于12岁男孩的例子，一个来自乌兹别克斯坦的例子，等等。</sample>
    <sample id="203">根据所提供的英文内容，positionality（立场）的定义是人们持有的观点，这些观点源于他们的 demographics、身份和生活经历。</sample>
    <sample id="204">演讲者的名字是Dawei Zhu。</sample>
    <sample id="205">是的，EDAtt 适应了现有的离线 ST 模型。</sample>
    <sample id="206">根据幻灯片上显示的作者列表，这篇论文有四位作者：Yusen Zhang、Jun Wang、Zhiguo Wang和Rui Zhang。</sample>
    <sample id="207">根据英语内容，被测模型不能在测试套件上运行。</sample>
    <sample id="208">KITMUS 有三个变体：背景预训练、背景both和背景inference。</sample>
    <sample id="209">根据幻灯片显示，作者Mohammad Javad Hosseini、Filip Radlinski、Silvia Paretti和Annie Louis都与Google Research有关。这可以从幻灯片左下角的Google Research标志推断出来。</sample>
    <sample id="210">最后的研究问题是：是否应该仅使用干净的样本进行验证，或者是否有更好的方法来利用它们？</sample>
    <sample id="211">根据所给英文内容，灵敏度指标衡量模型在任务上的一致性输出能力，无论输入指令的微小变化如何。</sample>
    <sample id="212">演讲者的名字是Jingwei Yi。</sample>
    <sample id="213">更高的灵敏度通常表示模型性能得到了提高，因为较低的数字（如图表中所示）代表更好的灵敏度。因此，在这个上下文中，更高的灵敏度意味着模型在未见过的任务上表现得更好。</sample>
    <sample id="214">在预训练期间，模型会接收各种语言上下文。</sample>
    <sample id="215">在 WSL 中，通常需要大约 20 个干净的验证样本才能获得良好的表现。</sample>
    <sample id="216">根据幻灯片底部显示的标志，这篇论文的作者来自斯坦福工程计算机科学系。</sample>
    <sample id="217">需要开发新的方法来衡量媒体偏见，因为现有的方法可能无法准确捕捉到语言模型中微妙的偏见。如图所示，语言模型在政治光谱上分布广泛，表明它们可以反映出各种政治观点和偏见。因此，需要一种能够识别和量化这些微妙差异的方法，以确保模型的输出是公正和无偏见的。</sample>
    <sample id="218">演讲者的名字是马奇塔。</sample>
    <sample id="219">根据所提供的内容，政治偏见传播流程包括从预训练数据到语言模型再到下游任务的过程。该流程旨在通过分析预训练数据中存在偏见的来源、这些偏见如何影响语言模型以及最终如何影响特定任务来解决潜在的公平性问题。</sample>
    <sample id="220">是的，根据图表显示，DEplain-apa和网站的简化过程有所不同。在DEplain-apa语料库中，我们看到更多的“重排”和“词添加”，而在网站语料库中，我们看到更多的“改写”。这表明DEplain-apa语料库中的简化过程更倾向于通过重新排列单词顺序和添加新词来简化文本，而网站语料库则更多地依赖于改写现有句子以达到简化效果。</sample>
    <sample id="221">是的，Coscript 是公开可用的。</sample>
    <sample id="222">水印通过在原始嵌入中添加目标嵌入来插入文本中，权重为触发器数量。如果句子中的触发器数量大于m，则提供的嵌入等于目标嵌入。</sample>
    <sample id="223">根据幻灯片上的内容，论文的作者来自宾夕法尼亚州立大学。</sample>
    <sample id="224">是的，像 mt5 这样的编码器-解码器模型可以通过混合语言的训练来改进。</sample>
    <sample id="225">受限制语言规划的一个示例是制作巧克力蛋糕，这涉及遵循特定步骤和成分的特定目标，同时可能受到各种约束的影响。</sample>
    <sample id="226">他们通过可视化嵌入的句子在降维技术如主成分分析（PCA）上，来验证其方法的隐蔽性。这使他们能够评估嵌入是否可以被区分出来，从而确保它们是隐蔽的。</sample>
    <sample id="227">根据所提供的英文内容，研究如何使用现有的预训练语言模型（PLM）来构建新的PLM涉及比较不同的预训练策略和数据源。该研究旨在评估这些策略对模型性能的影响，并通过引入三种基于连续预训练的模型来分析预训练策略的效果。这些新模型被用来进一步分析预训练策略对构建更强大、更有效的PLM的影响。</sample>
    <sample id="228">根据图表显示，GPT-4 与非洲伊斯兰国家/地区的立场最不一致。这可以通过图表中该类别显著较低的对数概率比值来表示，表明数据集和模型在这些地区与英语国家的立场存在差异。</sample>
    <sample id="229">演讲者在右侧的示例句子上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="230">任务的数量增加时，模型的性能会提高，但敏感度会降低。</sample>
    <sample id="231">作者用来比较其方法的三个无树基线是LSTM seq2seq、Zhen和Lapata，以及他们的方法。</sample>
    <sample id="232">根据幻灯片上显示的文本，两位合著者与第一作者的关系是导师关系。具体来说，Alexander Koller和Ivan Titov被提及为“导师”，这意味着他们指导了第一作者Matthias Lindemann的工作。</sample>
    <sample id="233">PaLM 的第一作者是 Chowdery et al.</sample>
    <sample id="234">大家好，我是珍妮，卡耐基梅隆大学的一年级博士生。今天我将为您呈现我们团队的工作，NLPPositionality：Characterizing Design Biases of Datasets and Models。</sample>
    <sample id="235">这项工作是在与一些因素合作的University of Washington和Allen Institute for AI，包括Sebastian Santi、Ronan Le Bras、Katharina Reinecke和Maarten Sap。</sample>
    <sample id="236">所以让我们从想象一下你正在为报纸工作，你正在 sift 通过评论，你的新闻文章试图删除有毒内容。</sample>
    <sample id="237">你可能会转向一个流行的API，比如Perspective API进行 toxicity detection。这在你像Carl Jones这样的情况下效果很好，因为Perspective API能够正确检测toxic instances。</sample>
    <sample id="238">但对Aditya Sharma来说，Perspective API对更常见的印度语境中的冒犯性术语并不那么敏感。</sample>
    <sample id="239">这是一个设计偏见的例子，我们看到技术在不同人群中表现出了系统性的性能差异。</sample>
    <sample id="240">设计偏见，如我们在前面看到的那样，可能源于NLP研究人员和模型开发者的观点。观点是指人们持有的观点，这些观点源于他们的人口统计、身份和生活经历。</sample>
    <sample id="241">这个概念广泛应用于批判性研究，特别是在女性主义和 queer 学术领域。</sample>
    <sample id="242">作为 researcher，positionality 可以影响研究过程及其结果，因为它可以改变 researcher 作出的决策。</sample>
    <sample id="243">所以，人们可能会问的一个问题是：数据集和模型有位置性吗？</sample>
    <sample id="244">而我们并不是说模型本身和数据集本身具有人口统计身份和生活经历，但它们确实聚合了真实人们的判断和观点，并因此可以代表某些代表性而非其他代表性。</sample>
    <sample id="245">所以初步工作已经提出了关于模型和数据集中的文化差距的 anecdotal 证据，以及模型定位性的概念定义。</sample>
    <sample id="246">然而，这些研究并没有将用户与数据集和模型本身进行比较。</sample>
    <sample id="247">研究模型和数据集的代表性变得越来越重要，因为NLP任务变得更加主观和社会导向。</sample>
    <sample id="248">而且很难确定这些定位性偏差是如何扭曲的，因为并非所有决策都记录在案，许多模型被隐藏在API后面。</sample>
    <sample id="249">所以为了研究数据集和模型的位置性，我们实际上将注释与真实用户与现有的数据集和模型进行了比较。</sample>
    <sample id="250">我们通过NLPositionality框架来实现这一点。</sample>
    <sample id="251">我们的框架在两个主要步骤中运作。</sample>
    <sample id="252">框架的步骤是重新注释数据集，以确保多样化的注释员参与其中。</sample>
    <sample id="253">我们通常不考虑原始数据集注释者的 demographics，因为通常每个注释者只注释每个实例，并且 demographics 数据 rarely 被收集和分享。</sample>
    <sample id="254">因此，我们选择重新注释数据以获得每个实例的多个注释，并获得丰富的 demographic 数据集。</sample>
    <sample id="255">然后我们按人口统计学对注释进行分类，并使用皮尔逊的相关系数将其与模型和数据集进行比较。</sample>
    <sample id="256">因此，我们的框架与标注者分歧文献不同，通过比较用户与模型和数据集的预测和标签，而不是仅仅查看标注者一致性或建模标注者分布。</sample>
    <sample id="257">我们的框架主要通过Lab in the Wild,一个在线众包平台,由我们的HCHI合作者开发。</sample>
    <sample id="258">Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，相比之下，像MTurk这样的平台主要拥有来自美国或印度的参与者。此外，Lab in the Wild仍然能够获得高质量的数据。</sample>
    <sample id="259">我们会在野外举办两个任务，其中一个任务是社会接受度。这个任务的工作方式是让参与者从社会化学数据集中阅读一个情境，然后他们会评估这种情况的社会接受度。</sample>
    <sample id="260">之后，为了保持参与研究，他们可以将他们的回应与人工智能和他人的回应进行比较。</sample>
    <sample id="261">我们然后将这些注释与社会化学、Delphi和GPT-4进行了比较。</sample>
    <sample id="262">然后我们复制了一个非常类似的设置，用于检测有毒性和 hate speech，他们将从 Dynahate 中读取一个实例，并写下一个他们认为这是 hate speech 的实例。</sample>
    <sample id="263">我们随后将这些注释与 Dynahate、Perspective API、Rewire API、Hate Roberta 和 GPT-4 进行了比较。我们的研究最终收集了来自 87 个国家的 1000 多名注释员提供的超过 16,000 个注释。</sample>
    <sample id="264">现在我们已经准备好回答NLP数据集和模型与谁最对齐。我们发现存在位置性在NLP中。</sample>
    <sample id="265">例如，我们发现数据集和模型最接近于说英语的国家。因此，在GPT-4社会接受性分析中，我们发现它最接近于说英语的国家和儒家国家。我们还发现Dinah Hay也最接近于说英语的国家。</sample>
    <sample id="266">我们还发现额外的对齐与拥有大学教育的人有关。因此，对于GPT-4在社会接受性任务中的表现，我们发现它最能与拥有大学教育或研究生院教育的人对齐。</sample>
    <sample id="267">我们发现 DynaHate 与拥有大学教育的人最相关。</sample>
    <sample id="268">然而，当模型和数据集针对特定人群进行对齐时，一些人不可避免地被落在后面。</sample>
    <sample id="269">一个例子是数据集和模型与男性和女性 counterpart相比对非二元人来说更不友好。我们在GPT-4社交可接受性任务以及Dinahite任务分析中发现这一点。</sample>
    <sample id="270">所以，鉴于存在位置性问题，我们可以做什么？</sample>
    <sample id="271">我们有几个推荐意见。第一个是记录所有相关的设计选择，以供研究过程参考。另一个是用透视主义的角度进行NLP研究。</sample>
    <sample id="272">我们的第三个建议是为特定社区建立专门化的数据集和模型。一个很好的例子是马萨卡尼计划。我们想强调的是，包容性NLP不仅仅是为了让所有技术都适用于每个人。</sample>
    <sample id="273">所以，这结束了我们的演示。如果您想了解更多信息，请务必查看我们的仪表板以获取最新分析结果和我们的论文。谢谢</sample>
    <sample id="274">演讲者提到了 SimulST 的三个问题：1. 训练过程中的额外模块优化，2. 长而复杂的训练程序，3. 训练和维护多个模型以达到不同的延迟 regimes。</sample>
    <sample id="275">在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法是通过使用多样化的数据集来确保模型能够处理各种观点和背景。这可以通过包括来自不同文化、种族和社会经济背景的文本来实现。此外，可以使用技术如数据增强和模型微调来减少偏见。然而，需要注意的是，完全消除偏见可能是具有挑战性的，因为偏见可能存在于数据本身中。因此，重要的是要持续监测和评估模型的性能，并在必要时进行调整。</sample>
    <sample id="276">Hi, I'm Siyu Yuan from Fudan University. I'm here to introduce our work: Distilling Script Knowledge from Large Language Models for Constrained Language Planning.</sample>
    <sample id="277">在日常生活中，人们经常通过遵循一系列步骤的指令来计划他们的行动，这些指令以保证脚本的形式呈现。</sample>
    <sample id="278">前一个任务已经利用语言模型来规划抽象任务，如制作蛋糕，并展示了大型语言模型可以有效地将任务分解为步骤。</sample>
    <sample id="279">然而，之前的大部分工作主要关注抽象活动的规划，如烹饪活动。具有具体目标和具体约束条件的规划，例如制作巧克力蛋糕，仍然未被充分研究。</sample>
    <sample id="280">在本文中，我们定义了约束语言规划问题。</sample>
    <sample id="281">不同的约束条件会影响语言规划，抽象目标可以由不同现实生活中具有多方面约束的具体目标继承。一个优秀的规划者应该编写合理且符合约束的脚本。</sample>
    <sample id="282">在这篇论文中，我们首先评估和改进大型语言模型的约束语言规划能力。</sample>
    <sample id="283">由于没有关于特定目标的明确数据集，我们无法确定LLMs在约束语言规划中的表现如何。</sample>
    <sample id="284">我们首先需要获取这个目标，如表中所示。我们将抽象的目标扩展为具有多种约束的指令，以便在人类的意图数据采集中使用。</sample>
    <sample id="285">我们从大型语言模型中采样了100个特定任务，并评估了它们生成的脚本。</sample>
    <sample id="286">这张表格报告了结果的整体准确性。我们发现所有大型语言模型在为特定目标规划方面取得了令人满意的成果。</sample>
    <sample id="287">然后我们进行详细分析，以调查大型语言模型的错误。</sample>
    <sample id="288">图中的结果表明，生成的脚本的语义完整性是可以接受的，但对约束的忠实性不能得到保证。</sample>
    <sample id="289">我们深入到更精细的topical类别，这些类别在wikiHow中被定义。图中的热力图显示了InstructGPTs在不同类别的规划表现显著不同。</sample>
    <sample id="290">以往的研究已经证明，大型语言模型的输出质量存在高度的变异性，导致性能不佳。因此，我们提出了使用超生成的Z过滤器来提高生成质量的想法。</sample>
    <sample id="291">我们首先通过示例展示约束类型，然后根据抽象目标生成具体目标。</sample>
    <sample id="292">然后，让GPT-3生成特定目标的候选脚本。</sample>
    <sample id="293">接下来，开发了一个过滤模型来选择最相关的脚本。</sample>
    <sample id="294">我们将脚本和目标转换为InstructGPT的嵌入，并计算余弦相似度和相似度分数，以衡量语义相似性。</sample>
    <sample id="295">此外，我们还关注包含目标约束关键词的脚本。我们只保留那些在目标集中具有最高目标得分的脚本。</sample>
    <sample id="296">我们的方法显著提高了规划质量。</sample>
    <sample id="297">由于大型语言模型部署成本昂贵，因此必须使小型和专门化的模型具备语言规划能力。创建数据集是实现这一目标的重要步骤。</sample>
    <sample id="298">然而，之前的研究所未能为特定目标进行规划，并且手动标注数据集的成本昂贵。</sample>
    <sample id="299">因此，我们遵循符号知识蒸馏的想法，从大型语言模型中蒸馏出约束语言规划数据集。</sample>
    <sample id="300">我们将应用我们的方法来构建一个名为code script的约束语言规划数据集。</sample>
    <sample id="301">总共，我们生成了55,000个具体的任务和脚本来确保验证和测试集的质量。我们要求云托管的工人找到并修订输入中的不正确样本。</sample>
    <sample id="302">这个图表展示了CoScript的约束分布情况。我们发现CoScript在生成的具体目标上具有高度异质性和 pluralism。使用CoScript，我们可以训练更小但更专门化的模型进行约束语言规划。</sample>
    <sample id="303">我们发现，微调在coscript上训练的小型LMs可以生成与大多数大型语言模型相当甚至更高质量的脚本，表明小型模型可以在适当的训练数据集上超越大型模型。</sample>
    <sample id="304">总之，我们建立了约束语言规划问题。我们评估了大型语言模型在约束语言规划方面的能力，并开发了一种过生成-过滤方法用于大型语言模型。</sample>
    <sample id="305">我们使用大型语言模型生成高质量的脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为推进语言规划研究的宝贵资源。</sample>
    <sample id="306">谢谢您的时间。请在我们的论文中提供更多关于CoScript的细节。</sample>
    <sample id="307">根据演示中提到的实验结果，PaLM 的流畅度与当前最好的系统相当。</sample>
    <sample id="308">水印方法的重要属性包括：1. 应用到嵌入服务，2. 不降低提供的嵌入的实用性，3. 足够隐蔽以使攻击者能够轻松移除水印，4. 在模型提取过程中可传输到攻击者的服务。</sample>
    <sample id="309">根据幻灯片中提到的英文信息，TED 英语演讲已被翻译成14种不同的语言。</sample>
    <sample id="310">从数据集中抽取了200个实例用于重新注释。</sample>
    <sample id="311">余弦和L2相似度被用来衡量良性和后门数据集之间的差异。</sample>
    <sample id="312">根据英语内容，基于编码器的多语言模型被用于这项任务，通过使用多语言预训练编码器与指针基解码器（如XLM-R和mBERT）进行评估。这些模型旨在处理多种语言，但它们的性能在不同数据集上有所不同。</sample>
    <sample id="344">作者通过收集一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="345">大家好，我叫刘Hugh。今天我要给大家介绍我们的一篇论文：《2003年CoNLL命名实体标注器在2023年还能正常工作吗？》让我们开始吧。</sample>
    <sample id="346">我们的论文研究了使用命名实体识别任务或NER任务解决泛化问题的问题。</sample>
    <sample id="347">我们观察到，模型已经使用CoNLL-2003开发NER近20年。这自然提出了几个问题。首先，这些模型能否推广到现代数据？</sample>
    <sample id="348">当开发新标签时，需要什么才能实现良好的泛化？</sample>
    <sample id="349">同时，如果我们观察到性能不佳，那么是什么原因会导致这些模型的性能下降？</sample>
    <sample id="350">为了研究这些问题，我们开发了ConLL++数据集。这是我们在2020年从路透社新闻中收集的数据集，并使用与ConLL-2003注释指南相同的注释它们。</sample>
    <sample id="351">然后我们在Conll-2003上微调了超过20个模型。我们分别在Conll-2003测试集和Conll++测试集上评估了它们。</sample>
    <sample id="352">最后但不少于least，我们计算了F1的百分比变化来评估每个模型的泛化能力。</sample>
    <sample id="353">所以，我们需要什么才能实现良好的泛化？通过实验，我们发现需要三个主要成分。</sample>
    <sample id="354">第一个是模型架构。通过我们的实验，我们发现Transformer模型通常在新数据上表现得更好。</sample>
    <sample id="355">第二个因素是模型大小。我们发现通常较大的模型会导致更好的泛化能力。</sample>
    <sample id="356">最后但同样重要的是，我们知道微调示例的数量直接影响下游任务的表现。我们还发现，更多的微调示例实际上也导致更好的泛化能力。</sample>
    <sample id="357">我们的下一个问题是什么因素导致了某些模型的性能下降？</sample>
    <sample id="358">我们有两个假设。第一个假设是自适应过拟合，即通过反复使用同一个测试集而导致的过拟合。这通常表现为在新测试集上泛化性能下降。</sample>
    <sample id="359">第二个假设是时间漂移，即由于训练和测试数据之间的时间差距而导致的性能退化。</sample>
    <sample id="360">对于自适应过拟合，我们从右侧的图表中看到，红色最佳拟合线的斜率大于1。</sample>
    <sample id="361">这意味着我们在Kor2013上所做的每一个改进单位，在Kor++上转化为超过一个单位的改进，这意味着没有递减的回报。</sample>
    <sample id="362">这表明在这种情况下并没有观察到自适应过拟合。</sample>
    <sample id="363">那么时间漂移又是什么？</sample>
    <sample id="364">对于时间漂移，我们进行了实验，以重新训练或继续预训练一些模型使用更近的数据。我们发现随着时间间隔变大，性能会下降。</sample>
    <sample id="365">这确认了我们关于时间漂移是性能下降的主要原因的假设。</sample>
    <sample id="366">我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型规模以及更多的微调示例。这些目标相互关联，我们不能只拥有一个成分，而是在其他方面进行平衡。</sample>
    <sample id="367">同时我们还发现，这里的性能下降是由时间漂移引起的，而且令人惊讶的是，它并不是由自适应过拟合引起的，尽管Conll2003已经使用了20多年。</sample>
    <sample id="368">回到我们论文标题中提到的问题，我们发现答案是：实际上，答案是肯定的。</sample>
    <sample id="369">我们希望我们的论文能为如何提高模型泛化能力的研究提供更多的研究。</sample>
    <sample id="370">最后，请务必查看我们的论文和数据集。如果您有任何问题，请随时联系我。谢谢您！</sample>
    <sample id="397">根据图片中可见的幻灯片内容，无法确定该方法使用的语音片段大小。幻灯片没有提供关于音频处理或分析的任何具体细节。要回答这个问题，需要额外的上下文或来自演示文稿的其他幻灯片，这些幻灯片可能包含有关方法的技术细节。</sample>
    <sample id="398">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是一个裁判。</sample>
    <sample id="399">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="400">在扩展实验中，论文侧重于进一步预训练语言模型检查点的六种不同的 partisan corpora。</sample>
    <sample id="401">根据提供的信息，该模型使用了多个层的注意力分数。这可以从提到的“同时输出”推断出来，表明模型在每个层中生成注意力分数，然后可能将它们结合起来以产生最终结果。</sample>
    <sample id="402">直接推断的示例包括通过说歌曲的名字“Easy on Me”或其位置“第一首”来指代。</sample>
    <sample id="403">根据幻灯片上提供的英文内容，论文的作者所属机构是复旦大学。</sample>
    <sample id="404">根据幻灯片显示，这篇论文有五位作者。</sample>
    <sample id="405">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="406">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例是“一个女人战士”，其中“女人”一词标记了与默认群体（男人）不同的群体。</sample>
    <sample id="407">根据所给内容中的英文信息，泛化能力较差的模型架构是除Transformer模型外的其他模型。</sample>
    <sample id="408">测试数据集的名称是“验证”。</sample>
    <sample id="409">根据幻灯片上显示的信息，这篇论文有五位作者。这可以通过在幻灯片底部列出的五张照片和名字来确定。</sample>
    <sample id="410">根据给定的内容，作者采用了多种模态。这可以从短语“多模态预训练模型”中看出，表明模型被设计为处理和理解来自不同来源的数据，如文本、图像或音频。</sample>
    <sample id="439">根据英文内容，作者认为NLU中研究不足的领域包括：1. 理解自然语言的复杂性和模糊性；2. 使模型能够处理多种语言和文化背景；3. 开发能够解释其决策过程的可解释模型。</sample>
    <sample id="440">演讲者的名字是Ying和Zhiyang。</sample>
    <sample id="441">根据幻灯片中提供的信息，Coscript 经过了质量检查。这可以从以下几点推断：1. 生成了 55,000 个特定目标的脚本，表明对脚本数量和多样性有系统性的关注。2. 使用云托管工人找到并修订错误样本，表明对脚本准确性和质量的重视。3. 提到验证和测试集，暗示脚本被用于评估模型性能，进一步强调了质量控制。因此，可以合理推断 Coscript 经过了质量检查。</sample>
    <sample id="442">根据所给内容中的英文信息，对于依赖上下文的翻译，现有的资源的局限性包括它们仅支持有限类型的上下文依赖翻译和有限的语言集。这些资源通常依赖于领域知识和人类校正，这限制了它们评估更广泛或更复杂上下文依赖翻译的能力。</sample>
    <sample id="443">你好，我要讲一下我们关于解决间接引用表达式进行实体选择的工作，在这个工作中我们引入了AltEntities Corpus。</sample>
    <sample id="444">我的名字是贾瓦德·侯赛因，这是我和 Filip Radlinski、Silvia Paretti 和 Annie Louis 的合作项目。</sample>
    <sample id="445">我们的目标是理解用户在想要做出选择时的语言。考虑这个问题的另一种说法：你是说轻松对待我，还是让我感到兴奋？这里用户想在两首歌之间做出选择。</sample>
    <sample id="446">最明显的事情是使用直接引用，例如通过说歌曲的名字“easy on me”或它的位置“第一个”来引用。</sample>
    <sample id="447">但有时候，间接引用更合适，可以进行更自然的对话。这可能会发生在用户无法记住歌曲名称的情况下。</sample>
    <sample id="448">所有的 pronunciation 都太相似，很难区分。</sample>
    <sample id="449">或者用户想要指定一个偏好。这里有一些例子：例如，“ newer one”或“那首不 energetic 的歌”。</sample>
    <sample id="450">这是对话系统中一个重要问题，同时也用于评估大型语言模型的实体理解能力。</sample>
    <sample id="451">我们不知道有一个大规模的公共数据集可以用于这个任务，所以我们使用 crowdsourcing 来收集一个。我们的数据集覆盖了三个不同的领域：音乐、书籍和餐厅评论。</sample>
    <sample id="452">数据集收集方法论强调使用卡通完成任务的非正式性。</sample>
    <sample id="453">卡通有三个对话框。第一个对话框里，Bob说：“记得昨天我们在听的那首歌吗？”然后Bob设置对话背景。</sample>
    <sample id="454">在第二个对话气泡中，Alice说“你是指轻松吗，还是我有感觉？”</sample>
    <sample id="455">在第三个对话气泡中，Bob使用间接引用选择其中一个实体，例如“那个新汽车”。</sample>
    <sample id="456">我们自动提供前两个对话气泡，但第三个是由标注员填写的。第一个对话气泡是从每个主题的几个手动提示中选择的。</sample>
    <sample id="457">第二个问题，即替代问题，是这样生成的。</sample>
    <sample id="458">我们总是使用一个简单的模板：你是指A还是B？其中A和B是从Wikipedia中采样出来的。</sample>
    <sample id="459">这里是我们使用的不同采样方法。当我们向上移动列表时，实体变得越来越相似，通常更难区分它们。</sample>
    <sample id="460">第一种方法是均匀采样。</sample>
    <sample id="461">第二个方法是，实体具有相似的标题。例如，两本书都叫做《The Return》。</sample>
    <sample id="462">第三个是它们在Wikipedia上有相似的描述，最后是它们在Wikipedia上有相似的info boxes或属性，例如相同的流派或相同的艺术家</sample>
    <sample id="463">当我们向标注者展示这个替代问题时，他们知道这些实体的名称，但他们并不一定了解实体本身。</sample>
    <sample id="464">所以，我们所做的就是展示一些关于这两个实体的背景知识。对于歌曲，我们简单地显示每个歌曲的Google搜索链接。</sample>
    <sample id="465">然后要求标注员收听至少每首歌曲的一部分，并阅读关于每首歌曲的信息。例如，对于歌曲“Easy on Me”，Google搜索结果如下：</sample>
    <sample id="466">对于食谱和书籍领域，我们展示了来自维基百科的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自维基百科，这样标注者就能了解它们看起来是什么样子。</sample>
    <sample id="467">然后我们要求标注员从这些实体中选择一个，例如这里的第一种情况，并用3到5个间接表达方式来描述它们。</sample>
    <sample id="468">例如，带有钢琴音乐的歌曲。我们的数据集中的示例包括没有歌词的歌曲，12岁男孩或来自其他来源的歌曲。</sample>
    <sample id="469">AltEntities Corpus 包含三个领域的 6000 个替代问题，并包含 42,000 个间接指代表达。使用 T5 XL 模型的结果总结如下：</sample>
    <sample id="470">如果语言模型有访问与注释员相同的完全相同的背景知识，那么准确率会很高，大约在92%到95%之间。但这并不现实。</sample>
    <sample id="471">如果语言模型可以访问一些部分重叠的背景知识，那么准确率在82%到87%之间，这更符合实际情况。例如，当语言模型检索背景知识时。</sample>
    <sample id="472">如果语言模型只能访问实体名称，那么准确率只有60%，所以有很大的改进空间。我们还证明了这些模型具有跨域泛化能力。这是我们的数据集的链接。谢谢</sample>
    <sample id="473">根据所给的英文内容，该方法与以下现有的 SimulST 策略进行了比较：wait-k 策略、本地算法和专门用于 SimulST 的最新架构。</sample>
    <sample id="474">根据幻灯片底部显示的标志，作者分别来自以下机构：里昂第一大学、里昂第二大学、南希大区卫生中心和Zenith。</sample>
    <sample id="475">演讲者的名字是Jenny T. Liang。</sample>
    <sample id="476">根据幻灯片上显示的信息，这篇论文有三位作者：Myra Cheng、Esin Durmus和Dan Jurafsky。</sample>
    <sample id="477">嗨，我是Sarah Papi，来自特伦托大学和布鲁诺·凯斯勒基金会。我将简要介绍我们共同撰写的关于同时口译中的注意力机制的论文。该论文与马泰奥·内格里和马可·图尔基合作完成。</sample>
    <sample id="478">同步口译，即 simulST，是指在实时翻译 spoken language 成为另一种语言的文本的过程，实现跨语言交流。</sample>
    <sample id="479">当前的SimulST模型存在哪些问题？特定架构通常在引入额外模块以进行优化时被训练。</sample>
    <sample id="480">长期和复杂的训练程序，例如涉及不同优化目标的训练。</sample>
    <sample id="481">训练和维护多个模型以达到不同的延迟 regimes，例如训练一个平均延迟为1秒的模型和另一个平均延迟为2秒的模型等等。</sample>
    <sample id="482">那么，我们的解决方案是什么？</sample>
    <sample id="483">首先，使用已存在的离线ST模型，无需重新训练或采用特定架构用于SimuST。使用单一模型为每个延迟范围，并通过特定参数处理延迟。</sample>
    <sample id="484">利用模型已经获得的知识，通过音频输入和文本输出之间的注意力机制进行交叉注意力机制。右侧可以看到一个示例。</sample>
    <sample id="485">我们的解决方案是提出EDAT或编码解码器注意力。它是一种策略，我们在决定是否发出部分翻译时，根据注意力指向的位置来决定是否发出。</sample>
    <sample id="486">如果注意力不集中，即其总和低于阈值α，则单词被发出。这表明最后λ个语音帧接收到的信息不够稳定。</sample>
    <sample id="487">例如，如果我们收到包含“我要谈论的话题”的语音片段，并且我们的模型预测了德语的翻译，</sample>
    <sample id="488">我们将查看注意力权重</sample>
    <sample id="489">我们将看到，前两个词指向最早接收到的语音帧，而最后一个词指向最新接收的语音帧，即λ帧语音帧。</sample>
    <sample id="490">这意味着前两个词将被省略。</sample>
    <sample id="491">由于交叉注意力的总和超过了一个特定阈值α，我们将不会发出最后一个单词，并等待下一个语音片段。</sample>
    <sample id="492">如果我们将继续进行，并收到另一个语音片段，我们的模型将预测其他三个单词，我们将查看这些交叉注意力权重。</sample>
    <sample id="493">我们将看到，没有一个词指向最新的语音帧。</sample>
    <sample id="494">这意味着这三个词将被发出。</sample>
    <sample id="495">如果你看EDAtt的主要结果，</sample>
    <sample id="496">我们将同时绘制翻译结果的图表，在图表中，蓝色一侧表示翻译质量，并且平均长度。</sample>
    <sample id="497">这是延迟指标，我们还考虑了计算感知平均延迟，它考虑了模型的计算时间来产生输出。</sample>
    <sample id="498">我们希望我们的曲线尽可能高地位于这个图表上。</sample>
    <sample id="499">但我们还希望它们在左侧错开。</sample>
    <sample id="500">我们与流行策略进行比较，这些策略也应用于离线模型，即wait-k和本地算法。我们还与专门针对 simultaneous 翻译的最新架构进行比较。</sample>
    <sample id="501">这些是同时翻译策略在德语上的所有结果。</sample>
    <sample id="502">我们看到EDAtt优于所有应用于离线模型的策略，因为它们的曲线向左移动。</sample>
    <sample id="503">我们还发现，如果我们考虑实际的耗时或计算时间，EDAtt是最快的策略。</sample>
    <sample id="504">如果您想发现更多结果，请阅读我们的论文，并且我们还发布了开源代码和模型以及同步输出，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="505">根据所提供的英文内容，数据集似乎是公开的。这可以从提到的GitHub链接推断出来，GitHub是一个广泛用于分享代码和数据集的平台。</sample>
    <sample id="506">大家好，我的名字是英，我和我的同事齐扬和我将要展示我们关于“MultiInstruct：通过指令调优改进多模零-shot学习”的研究成果。</sample>
    <sample id="507">随着大规模语言模型的进展，许多研究开始探索使用预训练语言模型进行不同下游任务的新学习 paradigms，在参数和数据效率方面。</sample>
    <sample id="508">最近，许多研究表明，指令调优使大型语言模型能够通过遵循自然指令以零-shot的方式在 unseen 任务上表现良好。</sample>
    <sample id="509">然而，大多数之前的模型调优工作都集中在提高零样本性能上，而计算机视觉和多模态任务则被忽视了。</sample>
    <sample id="510">因此，在这项工作中，我们想研究多模态预训练模型的指令调优是否实际上可以提高泛化能力，以解决多模态任务。</sample>
    <sample id="511">此外，在我们研究期间，我们发现NLP和多模态之间存在相当大的数据集可获得性差距。</sample>
    <sample id="512">存在超过1600个语言-only的指令任务，然而没有大规模公开可用的多模态指令任务。因此，这激励我们构建一个多模态指令调优数据集。</sample>
    <sample id="513">这里我们介绍了MultiInstruct，这是第一个多模态指令调优基准数据集，它由62个多样化的多模态任务组成，涵盖10个广泛类别。</sample>
    <sample id="514">这些任务是从21个现有的开源数据集中派生出来的，每个任务都配备了五个专家撰写的说明。</sample>
    <sample id="515">为了评估多模态指令调优，我们使用了我们提出的数据集。我们以OF-A（One For All）统一多模态预训练模型作为基础模型。OF-A使用一个统一的词汇表对语言、图像标记和 bounding box 的坐标进行编码。</sample>
    <sample id="516">这里我们展示了我们MULTINSTRUCT数据集的一些示例实例。</sample>
    <sample id="517">为了统一处理各种输入和输出数据类型</sample>
    <sample id="518">我们遵循了OWA的方法，并将所有任务统一表示为序列到序列格式，在这种格式中，输入文本、图像、说明和边界框都在相同的标记空间中表示。</sample>
    <sample id="519">现在我要讲一下多模态指令调优。</sample>
    <sample id="520">对于训练数据集，我们使用了来自N组的53个任务进行训练，并为每个任务采样了10,000个实例。对于测试，我们保留了整个Commonsense Reasoning组进行测试，并从VQA和Miscellaneous组中选择了额外的5个任务。</sample>
    <sample id="521">我们为每个任务使用测试集的所有实例。此外，我们从自然指令测试集的测试集中随机采样20个任务作为 unseen task for NLP。</sample>
    <sample id="522">我们使用预训练的 OFA 大模型作为基础模型。在训练过程中，我们将所有实例用于所有任务。每个实例随机与其中一个五种指令模板结合。</sample>
    <sample id="523">在测试每个任务时，我们总共进行五次实验，通过在每次实验中使用五个指令之一来评估模型。</sample>
    <sample id="524">我们将报告所有五个实验中的平均和最大性能以及性能的标准偏差。</sample>
    <sample id="525">如果任务是多模态分类任务，我们将报告准确率。如果是多模态生成任务，我们将报告ROUGE-L。对于NLP任务，我们还将报告ROUGE-L。此外，我们还根据模型在所有多模态和NLP任务上的平均表现计算聚合性能。我们不仅使用准确率作为指标。</sample>
    <sample id="526">我们还引入了一个额外的评估指标，称为敏感性。这个指标衡量模型在任务上的一致性输出能力，无论输入指令的微小变化如何。</sample>
    <sample id="527">我们的主要结果如图所示。我们可以看到，指令调优可以显著提高OFA在某些多模态任务上的性能。</sample>
    <sample id="528">此外，从自然语言数据集的迁移学习也可以使指令调优受益。</sample>
    <sample id="529">这里我们可以看到，随着任务数量的增加，模型在保持较低敏感性的同时取得了更好的表现。</sample>
    <sample id="530">我们还进行了实验，我们使用了一种指令和五种指令。正如我们可以看到，使用多种指令可以提高模型的整体性能，并显著降低其敏感性。</sample>
    <sample id="531">这展示了不同微调策略对模型敏感性的影响。如图所示，通过从自然指令数据集进行迁移学习，模型可以比原始OFA模型实现更好的敏感性。</sample>
    <sample id="532">我们也可以看到，从自然指令数据集进行迁移学习可以帮助模型在自然指令数据集上取得更好的表现。</sample>
    <sample id="533">总的来说，我们提出了第一个大规模多模态指令调优数据集，显著提高了OFA的零-shot能力，并探索了不同的迁移学习技术及其益处。我们设计了一个新的度量标准敏感性。</sample>
    <sample id="534">我们正在收集一个更大的多模态指令调优数据集，其中包含大约150个额外的视觉语言任务，并将在不久的将来发布它们。这是我们的数据和模型的二维码。谢谢。</sample>
    <sample id="535">根据幻灯片中提供的信息，Sara Papi、Matteo Negri和Marco Turchi的论文《注意力作为多模态语音翻译的指南》是来自意大利特伦托大学和布鲁诺·凯斯勒基金会的。</sample>
    <sample id="536">演讲者的名字是Javad Hosseini。</sample>
    <sample id="562">大家好，我是库斯图·辛纳，我很高兴欢迎你们参加我们关于ACL 2023论文的讨论：语言模型接受度判断并不总是对语境 robust。</sample>
    <sample id="563">与John Gauthier、Aaron Mueller、Kanishka Mishra、Karen Fuentes、Roger Levy和Adina Williams合作的联合研究。</sample>
    <sample id="564">在这个工作中，我们 revisit了最小对偶 paradigm。</sample>
    <sample id="565">最小对偶 paradigm (MPP) 评估语言模型时，主要基于可接受性判断，这还可以包括语法性，如 BLMP、SyntaxGym 或可接受性与刻板印象相关的句子，如 CrowS。</sample>
    <sample id="566">在这个最小对偶 paradigm 中，评估语言模型的典型方法是展示一个可接受句子或语法句子，然后展示一个不可接受句子或不语法句子。</sample>
    <sample id="567">然后模型的目标是将更多的概率分配给可接受的句子。</sample>
    <sample id="568">当前的MPP管道基本不允许我们评估模型对较长句子的接受程度。</sample>
    <sample id="569">这些天，大型语言模型正在开发出具有更长和更长上下文窗口的模型。因此，在整个上下文窗口中评估模型的可接受性至关重要。</sample>
    <sample id="570">这就是我们在这里要做的。我们正在尝试通过让模型评估可接受性来重新审视MPP管道，以在更长和更长的序列上进行评估。</sample>
    <sample id="571">所以这就是我们的方法。我们所做的就是模拟这些更长的序列，重新审视数据集本身，然后通过从数据集中选择可接受或不可接受的句子来重新创建句子。</sample>
    <sample id="572">例如，我们这里选择了一个典型的对偶性对，来自BLIP数据集中的 augment island 案例。</sample>
    <sample id="573">我们所做的就是重新创建更长的序列，看看哪些是可接受的，哪些具有相同的句法结构匹配。我们从MAGENT中提取句法句子。</sample>
    <sample id="574">然后我们将其添加为前缀，分别添加到可接受的查询和不可接受的查询之前。</sample>
    <sample id="575">我们可以通过从相同的匹配中选择不可接受的句子来实现这一点，这也可以用来测试模型的可接受性。</sample>
    <sample id="576">我们还可以通过选择来自不同子集或不同数据集的句子来实现相同的功能。这就是我们所称的 mismatch 情景。</sample>
    <sample id="577">所以，这些句子仍然来自相关的数据集，但并不是你用来评估的同一个数据集。我们也可以为不接受性情况进行同样的操作。</sample>
    <sample id="578">最后，我们可以从完全无关的领域中选择句子，例如维基百科。</sample>
    <sample id="579">这将告诉我们模型的可接受性判断是否实际上受到任何上下文的影响。</sample>
    <sample id="580">比如，上下文是否来自数据集的不同子集，或者它是否完全与当前正在考虑的句子无关。</sample>
    <sample id="581">所以模型的表现如何？我们首先查看与当前查询对完全无关的维基百科句子，发现MPP判断在任意长度的上下文中都具有稳健性。</sample>
    <sample id="582">我们把MPP评估与不同长度的上下文进行，可接受/可接受；匹配/不匹配结构--长度可达900个标记。我们增加了上下文长度，直到最大OP和GPT2模型，我们在这里看到橙色点线上的MPP判断相对稳定。</sample>
    <sample id="583">现在，如果我们从同一个数据集中选择句子，会发生什么情况？</sample>
    <sample id="584">这里，我们从相同的blimp或stactgen数据集中选择或创建句子，来自可接受和不可接受的领域。</sample>
    <sample id="585">我们使用不同的上下文进行MPP评估，包括可接受/不可接受；匹配/不匹配的结构，长度可达900个标记。</sample>
    <sample id="586">但当我们匹配结构时，即从BlimpPersonText中选择相同的句法现象时，</sample>
    <sample id="587">我们看到，根据所选前缀是否可接受或不可接受，MPP判断对模型的评分出现显著增加或显著下降。</sample>
    <sample id="588">现在，这个效果在整个上下文长度范围内都会增加，这可能会影响那些具有大上下文窗口的 newer 语言模型。</sample>
    <sample id="589">为什么匹配前缀会影响语言模型的判断？我们通过保持相关结构不变的方式对上下文句子进行扰动，并询问模型是否对这些句子同样敏感。- 前缀/后缀修饰语：然而，&lt;senta&gt;。- 长前缀修饰语：&lt;senta&gt;。- 添加“无论X认为什么，&lt;senta&gt;。”- 引用：“昨天，X说，‘&lt;senta&gt;’。”</sample>
    <sample id="590">所以我们设计了一系列分析，我们试图通过尝试保留输入句子的相关结构但添加一些噪声来修改输入句子。在做了几个这样的扰动后，</sample>
    <sample id="591">我们发现这些噪音并没有实际上让模型改变其在展示给定文本时的行为。</sample>
    <sample id="592">总的来说，我们发现模型对扰动句子的敏感性类似。</sample>
    <sample id="593">当我们在可接受的领域内扭曲句子时，我们看到所有扭曲的相似增加。而当我们扭曲句子在不可接受的领域时，我们看到MPP判断在相似方面下降。</sample>
    <sample id="594">我们工作的主要 takeaway是，语言模型对句子之间共享的潜在句法和语义特征敏感。</sample>
    <sample id="595">当前我们使用短语和单个句子输入进行MPP评估的方式，可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。</sample>
    <sample id="596">请阅读我们的论文以获取我们实验的更多详细信息。谢谢您的收看。</sample>
    <sample id="597">该方法的第一步将输入词元映射到一个无序的多集中词元，这些词元将在输出中出现。</sample>
    <sample id="598">根据所给英文内容，Coscript 包含了 55,000 个脚本。</sample>
    <sample id="626">DEplain 的最佳对齐方法是用于德语文本简化的方法。</sample>
    <sample id="627">弱监督学习的一个好处是它缓解了标注瓶颈。这可以通过减少对大量标注数据的需求来实现，标注数据通常是一个昂贵且耗时的过程。通过使用弱监督学习，模型可以在标注数据有限或标注过程成本高的情况下进行训练，仍然能够泛化并表现良好。</sample>
    <sample id="628">根据所给英文内容，DEplain-web 中的文档采用手动和自动对齐方法进行了对齐。具体分配情况如下：手动对齐分配了 147 个文档，而自动对齐分配了 1231 个文档。</sample>
    <sample id="629">CoNLL++数据集是通过从2020年收集《路透新闻》并使用与CoNLL 2003相同的注释指南进行注释而创建的。</sample>
    <sample id="630">大家好，我的名字是章元，来自宾夕法尼亚大学。今天我要介绍我们团队的工作：XSemPLR——多语言自然语言中的跨语言语义解析和意义表示。</sample>
    <sample id="631">语义解析是一个任务，用于构建用户查询的语义表示，例如SQL和Lambda calculus。</sample>
    <sample id="632">跨语言语义解析是将多种自然语言中的查询翻译成多种意义表示的任务。</sample>
    <sample id="633">如图所示，我们需要使用神经模型将查询翻译成多种自然语言，例如SQL、Lambda或FunQL等。</sample>
    <sample id="634">现有的跨语言语义解析模型分别提出并评估在有限任务和应用数据集上。例如：</sample>
    <sample id="635">跨语言语义解析

* 现有的CLSP模型分别提出和评估，适用于有限的任务和应用。例如：
	+ 缺乏对某些自然语言的覆盖范围
		- 中国語は欠落しています</sample>
    <sample id="636">跨语言语义解析

* 现有的CLSP模型分别提出并评估在有限任务和应用上的数据集。例如：
	+ 缺乏对某些意义表示的覆盖范围。

English | German | Chinese
--- | --- | ---
English | Neural Models | SQL
German |  | Lambda
Chinese |  | FunQL</sample>
    <sample id="637">跨语言语义解析

* 现有的CLSLP模型分别提出并评估在有限任务和应用上：例如：
	+ 缺乏某些意义表示的覆盖范围
	+ English
	+ German
	+ Chinese
	+ Neural Models
	+ SQL
	+ Lambda
	+ FunQL</sample>
    <sample id="638">或者它们只在某些神经模型上进行评估。例如，只有一个单一模型可以评估它们。</sample>
    <sample id="639">为了达到这个目标，我们提出了Exemplar，它提供了一个统一的数据集Exemplar，用于在多种自然语言和意义表示中进行跨语言的语义解析。</sample>
    <sample id="640">它包含9个数据集和各种领域、5个语义解析任务、8种意义表示和22种自然语言。</sample>
    <sample id="641">为了更好地评估我们的基准测试，我们考虑了六个用于训练和评估的设置。</sample>
    <sample id="642">首先，我们使用Google Translate API将源代码翻译成目标语言。然后，我们使用单语模型进行训练和评估。</sample>
    <sample id="643">例如，我们使用英文查询训练英文模型。在推理阶段，我们将使用API将德文查询翻译成英文，然后使用训练好的模型预测SQL。</sample>
    <sample id="644">我们还将测试单语模型。</sample>
    <sample id="645">在本设置中，源语言与目标语言相同，例如德语到德语或英语到英语。</sample>
    <sample id="646">我们还测试了单语多任务设置，通过使用仅10%的训练数据训练单语模型。</sample>
    <sample id="647">我们测试了多语言模型，我们为所有语言训练了一个单一的多语言模型。</sample>
    <sample id="648">例如，我们将德语、英语和中文查询一起用于训练多语言模型。在推理过程中，我们可以使用这个模型来</sample>
    <sample id="649">把德文查询或中文查询等翻译成SQL</sample>
    <sample id="650">我们还考虑了跨语言零-shot和少-shot转移。我们在一种源语言上进行训练，并转移到另一种语言。</sample>
    <sample id="651">在训练期间，我们使用英文查询或结合英文和德文的少量查询来训练多语言模型以预测SQL输出。</sample>
    <sample id="652">我们还发现一些有趣的结果。所以，关于单语模型的分析，我们评估了两组模型。</sample>
    <sample id="653">包括编码器PTR，它代表多语言预训练编码器与指针式解码器，例如XLM-R + PTR和mBERT + PTR。</sample>
    <sample id="654">我们还评估了编码器-解码器模型，即多语言预训练编码器-解码器模型，例如mBART和mT5。</sample>
    <sample id="655">我们发现编码器-解码器在所有9个数据集上取得了最佳性能。</sample>
    <sample id="656">我们评估了MT5和XLM-R+PTR在多语言设置下的性能。</sample>
    <sample id="657">我们发现，编码器-解码器或编码器-PTR可以通过在各种语言的混合中进行训练来提高。</sample>
    <sample id="658">我们发现，大多数主要自然语言都可以获得性能提升，但英语在7个数据集中性能下降，在3个数据集中性能提升。</sample>
    <sample id="659">我们评估了mT5和XLM-R + PTR在多语言设置上的性能。大多数主要的NL性能提升，除了在7个数据集中英文性能下降外，在3个数据集中获得性能提升。这被称为“多语言现象”。</sample>
    <sample id="660">我们还比较了跨语言性能差距。</sample>
    <sample id="661">在这幅图中，蓝色线代表跨语言少样本转移。橙色线代表跨语言零样本转移。绿色线代表单语环境。</sample>
    <sample id="662">我们发现通过比较绿色和橙色线，发现在零-shot设置下，跨语言转移性能差距显著。通过比较蓝色和橙色线，我们发现，在少-shot设置下，转移差距迅速缩短。</sample>
    <sample id="663">我们还发现了一些其他有趣的发现。例如，编码解码器在之前的成果上表现得更好，或者在目标自然语言上的性能得到了显著提升。</sample>
    <sample id="664">我们发现多语言模型，如Codex和Bloom，在跨语言零样本解析任务中仍然不够。</sample>
    <sample id="665">为了总结一下，我们构建了XSemPLR，一个用于多自然语言和意义表示的跨语言语义解析的统一基准。</sample>
    <sample id="666">我们将对三种代表性的多语言模型类型进行全面基准测试，并且我们的结果展示了许多有趣的发现等等。欢迎访问我们的论文和代码。谢谢您的收看。</sample>
    <sample id="667">关于这方面的现有研究可以被广泛地分为四个类别。</sample>
    <sample id="668">根据幻灯片中呈现的发现，Codex 和 Bloom 等多语言 LLM 对于 CLSP（跨语言语义解析）来说仍然不够。</sample>
    <sample id="695">该方法通过在训练过程中将排列的对齐作为一部分来处理排列的不确定性。</sample>
    <sample id="696">根据给定的图片，下游 NLP 模型的公平性被定义为模型在不同政治背景下的性能。这包括评估模型是否能够准确地识别和处理各种政治观点，而不会对某些群体产生偏见或歧视。</sample>
    <sample id="697">演讲者的名字是Yanis Labrak。</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。</sample>
    <sample id="700">在本文的背景下，热带主义 (tropicalism) 意味着一种刻板印象或刻板印象，其中将 Latina 女性描述为具有异国情调、充满活力和曲线玲珑。这种描述可能强化了 Latina 女性的刻板印象，将她们的身份简化为与热带地区相关的视觉和文化特征，而没有考虑她们的多样性和复杂性。</sample>
    <sample id="701">根据幻灯片中提供的信息，作者通过使用特定的描述词来创建目标群体的人工描写。对于被标记的群体，这些描述词包括“文化”、“传统”、“自豪”和“异国情调”，这些词强调了与身份相关的方面，并将这些群体与白人规范区分开来。对于积极的描绘，使用了诸如“充满活力”、“娇小玲珑”、“精致”和“坚强”等词语，这些词语突出了这些群体的积极特质和外貌特征。</sample>
    <sample id="702">在本文中，使用了点wise CXMI来衡量语境使用情况。</sample>
    <sample id="703">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据来源。DrBERT 是从头开始训练的，使用了 7GB 的 NACHOS 数据集，而 ChuBERT 则是基于一个现有的预训练模型进行微调，使用了来自临床笔记的 4GB 句子。</sample>
    <sample id="751">根据幻灯片显示，这篇论文有三位作者：Zhongyang Xu、Ying Shen 和 Lifu Huang。</sample>
    <sample id="752">迭代迁移学习是一种机器学习方法，其中模型在每次迭代中使用最新的数据集进行训练。这种方法允许模型在每次迭代中不断改进，适应新数据并减少过拟合的风险。与累积方法不同，后者会收集所有历史数据，迭代方法专注于最近的数据，这有助于保持模型对最新信息的敏感性。</sample>
    <sample id="753">数据集的目标是通过理解用户的语言来帮助系统做出选择，特别是当用户在表达偏好时使用间接引述表达时。这涉及识别用户在表达个人喜好或选择时使用的模糊或模糊的语言，如“容易”或“感觉好”。系统需要能够解释这些表达以提供准确的选择，比如从两首歌曲中选择一首。</sample>
    <sample id="754">根据提供的信息，攻击者通过 EaaS（即“嵌入”）来提取模型参数。这表明 EaaS 可能涉及将某些数据或代码注入模型中，以影响其行为或性能，从而导致参数被提取。</sample>
    <sample id="755">根据幻灯片中提供的英文信息，这篇论文有三位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="756">根据图片中提供的信息，用于创建初始数据集的注释者数量为6。</sample>
    <sample id="757">根据幻灯片，简·李·扬的论文《NLPositionality: Characterizing Design Biases of Datasets and Models》是她在卡内基梅隆大学完成的。</sample>
    <sample id="758">以左侧为支配词的示例是“我看见了巴特和丽莎”。</sample>
    <sample id="759">根据所提供的英文内容，对话系统中的最先进模型是GPT-4。</sample>
    <sample id="760">根据英语内容，我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型正在出现具有更长的上下文窗口。这表明模型处理和理解文本的能力正在增强，因此评估它们在不同长度的上下文中的表现至关重要。</sample>
    <sample id="761">是的，与单语英语模型相比，多语言训练会导致表现下降。</sample>
    <sample id="762">是的，注释者提前知道实体。</sample>
    <sample id="763">根据给定的英文内容，评估使用了BLEU和METEOR两个MT指标。</sample>
    <sample id="764">是的，泛化中的回归可能会影响特定的 NER 类型。在 NER 任务中，模型需要学习识别文本中的实体，如人名、地名和组织名称。回归可能指的是模型在不同 NER 类型之间的性能差异。例如，一个模型可能在识别人名方面表现很好，但在识别组织名称时表现不佳。因此，回归可能会影响模型在不同 NER 类型之间的泛化能力。</sample>
    <sample id="765">NLP 中的立场很重要，因为 NLP 模型在处理文本时可能表现出系统性性能差异。这些差异可能源于训练数据的不平衡或模型设计的偏见。例如，在图像中提到的 Perspective API 示例中，该 API 在检测印度语境中的有毒内容时不够敏感。这种性能差异可能导致某些群体被边缘化或不被正确处理。因此，理解并解决 NLP 中的立场对于确保技术公平和包容至关重要。</sample>
    <sample id="766">根据所给的英文内容，像 BLOOM 这样的多语言 LLM 采用的是完整微调。</sample>
    <sample id="767">根据所提供的英文内容，他们使用了在辩论和CE任务上进行迭代微调的模型。</sample>
    <sample id="768">最近用于评估 PaLM 能力的测试集包括 WMT、IWSLT、XSum、SQuAD、CoQA、SNLI、MNLI、XNLI、SuperGLUE、Stanford Question Answering Dataset (SQuAD)、Google Question Answering Dataset (GQAD)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG)、Google Knowledge Graph (KG</sample>
    <sample id="769">根据图片中显示的文本，作者最终提出了三条建议。</sample>
    <sample id="770">根据所给的英文内容，与最强的基线相比，建议的方法获得了1.2%的收益。</sample>
    <sample id="771">演讲者的名字是刘树恒。</sample>
    <sample id="772">是的，论文中的结果和数据集可以作为基准。</sample>
    <sample id="773">根据图片中显示的柱状图，进行了三个较小模型的实验。这些模型被标记为“GPT-3 (175B)”，“Codex (175B)”，和“InstructGPT (175B)”。每个模型的柱代表了在Coscript数据集上进行微调后的准确率。</sample>
    <sample id="774">OF-A</sample>
    <sample id="833">根据幻灯片中提到的信息，这篇论文的作者来自Google Translate。</sample>
    <sample id="834">根据幻灯片显示，这篇论文的作者是来自圣布鲁克大学的人类语言分析部门的计算机科学系候选人。</sample>
    <sample id="835">根据图片中提供的信息，论文分析了以下语言对：西班牙语-英语、法语-英语、德语-英语、意大利语-英语、荷兰语-英语、葡萄牙语-英语、俄语-英语、西班牙语-阿拉伯语、法语-阿拉伯语、德语-阿拉伯语、意大利语-阿拉伯语、荷兰语-阿拉伯语、葡萄牙语-阿拉伯语、俄语-阿拉伯语。</sample>
    <sample id="836">演讲者的名字是张冰。</sample>
    <sample id="837">在实验过程中研究了两个不同的模型：一个用于产生文档级别的简化，另一个用于产生句子级别的简化。</sample>
    <sample id="838">根据 MultiInstruct 中使用的 62 个不同任务，其中 53 个任务用于训练，9 个任务用于测试。</sample>
    <sample id="839">根据幻灯片显示，这篇论文有三位作者：Regina Stodden、Omar Momen和Laura Kallmeyer。</sample>
    <sample id="840">作者在实验中使用了四个数据集：AG News、MIND、SST2和Enron Spam。</sample>
    <sample id="876">NACHOS 是一个数据集，包含来自 Web 的医学 Crowd 数据。</sample>
    <sample id="877">演讲者的名字是Ibilard。</sample>
    <sample id="878">根据幻灯片中提供的信息，提示策略对结果有显著影响。它表明选择不同的提示可以导致翻译质量的差异，差异可能达到40 BLEURT分点。这表明提示的选择是影响翻译任务性能的重要因素。</sample>
    <sample id="879">根据幻灯片底部显示的标志和名称，Kayo Yin等人的论文《翻译是否需要上下文：一种数据驱动的多语言探索》的作者来自卡内基梅隆大学的语言技术研究所、葡萄牙技术大学、布宜诺斯艾利斯人工智能研究实验室（BAIR）以及Unbabel。</sample>
    <sample id="880">专家编写的5个指令是：1. 识别图片中的物体。2. 描述图片中的动作。3. 确定图片中物体的颜色。4. 指出图片中的主要主题。5. 概述图片的整体情感或情绪。</sample>
    <sample id="881">根据所给英文内容，作者建议通过使用人类学习参与者和已建立的共同参考解析模型来测试模型，以评估其利用不同来源中可获得知识的能力。</sample>
    <sample id="882">大家好，我的名字是Ibilard，我将给大家简要介绍一下我们与Google Translate同事合作的论文《Prompting for Translation: Assessing Strategies and Performance》。</sample>
    <sample id="883">PaLM是一个拥有540亿参数的大型语言模型，于去年2022年发布。它是在一个包含7800亿个文档的大文本集合上进行训练的。</sample>
    <sample id="884">在许多NLP任务中，它实现了SOTA。</sample>
    <sample id="885">在这项工作中，我们提出了对大规模语言模型（LLM）提示用于机器翻译的首次系统研究。</sample>
    <sample id="886">我们使用MT社区的最佳实践评估了LLM的翻译能力。这包括使用最新的测试集，以避免测试数据与语言模型的训练数据重叠。</sample>
    <sample id="887">我们比较了两个最好的系统，即在WMT评估中表现最好的系统。</sample>
    <sample id="888">我们使用了前沿的LLM评估指标，并且还展示了专家基于人类评估的结果。最后，我们提供了一些关于选择策略的建议。</sample>
    <sample id="889">提示对LLM的翻译性能有重大影响，如我们在一个简单实验中所见，我们使用单步提示，并为每个句子提供两个不同的提示。</sample>
    <sample id="890">大多数句子（516/1000）的差异大于一个BLEURT分数。</sample>
    <sample id="891">这在极端情况下可以达到40分，所以选择一个好策略是很重要的。</sample>
    <sample id="892">在我们的实验中，我们尝试了一种五轮提示策略，其中我们只标记输入给系统的句子的语种。</sample>
    <sample id="893">在这个示例中，我们将从德语翻译成英语。德语句子的原始句子用 German 冒号标记，而英语翻译则用 English 冒号标记。</sample>
    <sample id="894">我们发现，实际的提示形式对多轮提示的性能没有重大影响。</sample>
    <sample id="895">对于零和一击提示至关重要，而在我们的情况下，当我们转向五击提示时，几乎没有与实际提示形式的区别。</sample>
    <sample id="896">它的例子承载了大部分的权重。</sample>
    <sample id="897">我们实验结果的总结是，样本质量比源句子的相似性更重要。</sample>
    <sample id="898">选择高质量翻译的示例很重要。特别是，我们将从URMNT评估数据集或DEF数据集的训练数据中选择示例。</sample>
    <sample id="899">开发数据的质量更高，与训练数据相比，结果更令人满意。因此，在使用开发数据时，性能更好。</sample>
    <sample id="900">然而，专门化的SOTA系统在翻译方面具有显著优势，但Palm的表现相当接近商业系统。在本例中，我们选择将其与Google Translate进行比较。</sample>
    <sample id="901">我们通过使用MQM框架进行的实验分析得到的见解是，PALM的流畅性与当前的艺术系统相当，但主要差异来自准确性。</sample>
    <sample id="902">在 particular，最常见错误是省略错误。</sample>
    <sample id="903">所以它似乎选择删除一些句子的一部分来产生更好的理解翻译，有时通过删除那些在翻译中不相关的部分。</sample>
    <sample id="904">然而，对于Pan而言，风格/ awkward类别低于对于State-of-the-art系统而言，这是另一个信号。</sample>
    <sample id="905">但仍然存在一些准确性问题。</sample>
    <sample id="906">而这就是这个真的很短的概述。对于更多细节，请转到完整的论文。谢谢您！</sample>
    <sample id="907">你好，我是Dawei，一名德国萨尔兰大学的博士生。在这段视频中，我想向大家介绍我们最近的研究成果：弱于你想象——对弱监督学习的批判性审视。</sample>
    <sample id="908">这是与肖宇生、马里乌斯·穆索夫和盖尔·史蒂芬以及迪特·克拉科合作的联合研究。</sample>
    <sample id="909">我将从介绍弱监督和弱监督学习开始。</sample>
    <sample id="910">在弱监督中，我们不会手动标注数据。相反，我们使用弱标签来源，如简单的启发式规则、知识库或低成本的 crowdsourcing，如图所示。</sample>
    <sample id="911">与人类注释相比，弱注释要便宜得多，但它们也很 noisy，这意味着其中一部分注释是错误的。</sample>
    <sample id="912">如果我们直接训练神经网络使用弱标签数据，神经网络倾向于 memorize 标注噪声，并且不泛化。</sample>
    <sample id="913">在弱监督学习中，训练算法旨在 robustly 训练神经网络以处理标签噪声，从而确保训练模型仍然泛化良好。</sample>
    <sample id="914">那部分提供非常流畅的输出，但仍然有一些准确性问题。干净测试集</sample>
    <sample id="915">技术上，这个说法并不完全正确，但其中有一个关键点。</sample>
    <sample id="916">这表明人们假设存在一个额外的干净验证集，用于模型选择。</sample>
    <sample id="917">我们专注于这个问题设置，因为它意味着在弱监督学习中需要额外的手工注释。但就像房间里的大象一样，这个必要性经常被忽视。</sample>
    <sample id="918">我们接下来的任务是提出三个研究问题。首先，对于WSL，是否需要干净的验证数据？或者我们可以使用一个有噪声的验证集吗？</sample>
    <sample id="919">第二，如果需要干净的数据或干净数据对于WSL有效运行是强制性的，那么我们需要多少干净样本？最后，我们是否应该只使用干净样本进行验证，或者是否有更好的方法来利用它们？</sample>
    <sample id="920">我们已经在我们的工作中解决了这些问题，我们的发现如下：</sample>
    <sample id="921">首先，我们发现有趣的是，最近的WSL方法确实需要干净的训练样本才能正常工作。</sample>
    <sample id="922">否则，如图所示，如果没有干净的验证样本，训练模型无法超出原始weak标签之外泛化。</sample>
    <sample id="923">意味着训练是徒劳的。</sample>
    <sample id="924">这表明WSL方法实际上需要干净标注的数据才能正常工作，获取干净验证样本的注释成本不容忽视。</sample>
    <sample id="925">我们的第二个发现是增加干净验证样本的数量可以帮助WSL方法实现更好的性能，如图左所示。</sample>
    <sample id="926">通常，我们只需要每个类别20个样本就可以获得高性能。</sample>
    <sample id="927">但这并不是故事的结束，因为如果我们以某种方式决定获取干净样本，那么直接在它们上进行训练将甚至会获得更好的表现。</sample>
    <sample id="928">右图显示了直接应用在干净数据上的微调方法和仅使用干净数据进行验证的WSL方法之间的性能差异。</sample>
    <sample id="929">正如我们所见，如果我们有每个类别的10个样本，直接微调开始超越WSL方法。</sample>
    <sample id="930">最后，通过允许在干净的验证样本上继续微调，可以轻松实现之前WSL方法声称的性能提升。</sample>
    <sample id="931">如图所示，瓦尼纳模型FTW最初在更复杂的WSL方法如余弦中表现不佳。</sample>
    <sample id="932">然而，如果我们继续在干净样本上进行微调，FTW的表现与其他方法一样好。</sample>
    <sample id="933">在实际应用中，没有理由选择更复杂的WSL方法，因为它们需要更多的计算时间和磁盘空间。</sample>
    <sample id="934">总之，我们展示了最近的WSL方法需要干净、手动标注的样本才能正常工作。它们的性能和实用性被严重高估了。</sample>
    <sample id="935">我们的综合建议如下：1. 报告模型选择标准。2. 使用少样本学习方法作为基线。3. 总是应用连续微调（CFT）。</sample>
    <sample id="936">首先，报告模型选择标准，例如，报告模型选择是否基于干净的验证样本。</sample>
    <sample id="937">第二，WSL方法应该与基于少量学习的基线进行比较，因为它们依赖于干净样本。第三，持续微调是一种简单而强大的基线，应该在未来的WSL工作中考虑。</sample>
    <sample id="938">最后，我们开源了我们的代码。您可以在幻灯片上的QR码中找到它。请随意查看它。谢谢，并祝您享受会议。</sample>
    <sample id="939">对话系统的常用评估方法是通过让人类裁判选择两个对话中哪个更好，或者根据李克特量表对对话进行评分。</sample>
    <sample id="940">根据Jenny的介绍，这篇论文有五位作者。</sample>
    <sample id="941">在 Servin 和 Kea 的示例中，需要的背景知识是“裁判决定案件在法庭上”。这个信息对于理解 Servin 在法律环境中工作并因此在一天结束后寻求放松是至关重要的。</sample>
    <sample id="942">代码是公开的，可以在GitHub上获取。</sample>
    <sample id="943">根据图片中提供的信息，无法确定 NLPositionality 的注释者在各个人口统计学特征（如国家/地区、性别等）方面是否均衡。该图片主要关注教育水平与模型对社会可接受性任务的对齐程度，没有提供关于人口统计学均衡性的具体数据或分析。</sample>
    <sample id="944">在可接受的域中扰乱句子的方法包括通过添加噪声来修改输入句子，同时保持相关结构。这些噪声可以是前缀/后缀修饰语、长前缀修饰语、在最前面或最前面添加内容，或者在X关于某个主题不管X对它的看法如何的情况下添加内容。这些方法旨在测试模型对不同输入的敏感性，而不改变其对文本的判断。</sample>
    <sample id="945">进行维度评估意味着对对话质量的各个方面进行单独评估，以理解模型的强项和弱点。这允许更细致地分析模型的表现，而不仅仅是整体对话质量。</sample>
    <sample id="946">根据英语内容，这篇论文的作者所属机构是“中国科学技术大学”和“北京邮电大学”。</sample>
    <sample id="947">根据图片中提供的信息，提示的形式在“零-shot”和“一shot”提示的情况下很重要。这意味着在没有示例的情况下（零-shot）或只提供一个示例的情况下（一shot），提示的结构和内容对模型的表现至关重要。然而，在“五-shot”提示的情况下，即提供五个示例后，提示的形式似乎不太重要，因为结果几乎没有变化。</sample>
    <sample id="978">根据内容中提供的英文信息，作者评估了几个对话模型。然而，具体名称并未在给定的文本中提及。提到的挑战包括这些模型在响应中存在共同知识违反、产生不相关的信息以及相互矛盾或与其伙伴矛盾。这些挑战表明评估涉及对话模型的性能和可靠性，但没有明确指定被测试的模型名称。</sample>
    <sample id="979">根据幻灯片显示，这篇论文有六位作者。</sample>
    <sample id="980">优秀规划器的理想品质是能够编写合理且符合约束的脚本。</sample>
    <sample id="981">根据幻灯片显示，这篇论文有九位作者。</sample>
    <sample id="982">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="983">根据幻灯片显示，Adam Przpherdkowski和Micha Wozniak的论文《Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination》属于华沙大学计算机科学学院。</sample>
    <sample id="1021">根据图片中提供的信息，PaLM 最常见的错误是“遗漏/遗漏”。</sample>
    <sample id="1022">大家好，我是James Finch，我是Sarah Finch。今天我们将向您介绍ABC-Eval，一种评估对话型人工智能的新维度方法。</sample>
    <sample id="1023">这项工作是由爱默里大学的爱默里NLP实验室领导的，由教授金浩领导，并与亚马逊Alexa人工智能合作完成。</sample>
    <sample id="1024">假设你刚刚开发了一个对话模型，并想看看它与当前的前沿技术相比表现如何。</sample>
    <sample id="1025">常见的做法是使用人类评估，例如，请人类评委选择两个对话中哪个更好，或者根据 Likert 梯度评分对话。</sample>
    <sample id="1026">这些方法可以提供对话质量的整体评估，但对话质量有许多方面。因此，为了在更精细的水平上了解模型的优点和缺点，你可能需要评估聊天质量的多个维度。</sample>
    <sample id="1027">一种方法是让人类裁判员评估对话质量的几个维度，例如模型回复的相关性，使用现有的比较或李克特尺度方法。</sample>
    <sample id="1028">然而，我们相信有一种更精确可靠的策略来进行多维对话评估。</sample>
    <sample id="1029">我们的方法旨在减少人类评估的主观性，通过明确标注每个模型响应是否表达某些行为，例如提供无关信息或自相矛盾。</sample>
    <sample id="1030">我们称这种方法为“聊天行为注释”或简称“ABC-Eval”。我们开发了这种方法，以全面涵盖最近文献中建议会影响聊天质量的聊天模型行为。</sample>
    <sample id="1031">ABC-Eval能够衡量聊天模型犯各种主题错误的速度。</sample>
    <sample id="1032">例如，ABC-Eval衡量一个聊天模型在与其伙伴互动时忽略其伙伴或说一些不相关的次数。</sample>
    <sample id="1033">与自身或其伙伴相矛盾，扭曲错误事实或违反常识知识，以及当模型成功或失败时表现出同理心。</sample>
    <sample id="1034">为了确定哪种评估方式最有效，我们选择了四种前沿的聊天模型，并在每个模型上评估了100个人类对话，使用ABC-Eval。</sample>
    <sample id="1035">为了进行比较，我们还使用了三种现有方法评估这些对话：针对每个轮次的Likert评分、针对整个对话的Likert评分以及针对对话水平的配对比较。</sample>
    <sample id="1036">对于每种现有方法，我们收集了对对话的八个最常见衡量方面的评估，因为这是评估聊天模型沿多个维度的标准做法。</sample>
    <sample id="1037">从我们对这些评估结果的分析中，我们发现ABC行为标签在可靠性方面总体优于现有方法收集的标签，如通过100个双重标注对话的内部注释员一致性衡量。</sample>
    <sample id="1038">此外，ABC-Eval标签比现有方法产生的指标更预测整体对话质量，如线性回归分析所示。</sample>
    <sample id="1039">例如，你可以看到测量具有自我和伙伴矛盾的转次数解释了5%和10%的对话质量，而平均Likert一致性评分仅解释了4%或更少。</sample>
    <sample id="1040">最后，我们检查了每个评估指标是否捕捉了对话质量的一个独特方面，使用逐步线性回归。</sample>
    <sample id="1041">你可以看到，所有ABC-Eval指标的组合可以解释对话质量的超过25%，当你一次移除一个指标时，大部分指标都会导致失去相当一部分关于质量的信息。</sample>
    <sample id="1042">另一方面，所有轮次级别的Likert指标的组合解释了较少的质量，并且较少的这些指标携带独特信息。</sample>
    <sample id="1043">这些可靠的、信息丰富且独特的ABC-Eval指标使我们能够以比以往任何时候都能实现的更高分辨率来评估对话型人工智能。</sample>
    <sample id="1044">在我们实验的结果中，可以清楚地看到还有一些挑战仍然存在，并且已经被精确量化。例如，我们测试的聊天机器人在其响应中存在大约20%的常识错误。</sample>
    <sample id="1045">它们大约在15%的回应中产生无关信息，并且大约在10%的时间内与自己或对话伙伴发生矛盾。</sample>
    <sample id="1046">随着该领域的快速发展，许多错误率可能会在自我们评估以来发布的新模型中下降。然而，这更加强调了追求可靠和精确的评估指标的重要性，用于比较模型。</sample>
    <sample id="1047">我们希望ABC-Eval能被其他领域的专家利用，作为在这个方向上的一个重要步骤。我们期待看到在未来几个月和几年里对话型AI如何 advancement。谢谢您的观看。</sample>
    <sample id="1048">根据幻灯片中提供的信息，论文的作者属于爱默里大学的Emory NLP实验室。</sample>
    <sample id="1049">在本文中，CFT 代表“持续微调”，这是一种机器学习技术，其中模型在训练后继续在新数据上进行微调。</sample>
    <sample id="1050">根据幻灯片显示，这篇论文有六位作者。</sample>
    <sample id="1051">你好，我的名字是Kayo Yin，我将代表我们的论文《翻译何时需要语境：一种数据驱动的多语言探索》进行发言。这项工作是在Patrick Fernandez、Emily Liu、Andrea F. T. Martins和Graham Neubig的合作下完成的。</sample>
    <sample id="1052">所以许多翻译取决于上下文。例如，在这个句子中，我们将如何翻译 mole？</sample>
    <sample id="1053">如果前一个句子是“如果部长们发现了，事情可能会变得危险。” 那么 mole 指的是蛇。但如果前一个句子是“医生，这可能是什么严重的？” 那么 mole 指的是胎记。</sample>
    <sample id="1054">所以，取决于上下文，单词的意思会改变，因此它的翻译也会改变。</sample>
    <sample id="1055">然而，评估模型在处理这种情况下的表现如何是非常困难的。首先，因为只有很少一部分翻译依赖于上下文，使得语料库级别的指标，如BLEU，无法捕捉这些翻译。</sample>
    <sample id="1056">一些人建议对依赖上下文的翻译进行针对性评估，但这些资源只支持有限类型的上下文依赖翻译和有限的语言集。因为它们通常依赖于领域知识和人工润色。</sample>
    <sample id="1057">在这项工作中，我们试图回答这两个问题：首先，翻译何时需要上下文；其次，模型如何处理这些情况？</sample>
    <sample id="1058">为了回答第一个问题，我们首先开始通过衡量单词在翻译中对上下文的依赖程度来测量。</sample>
    <sample id="1059">在之前的工作中，我们引入了条件交叉互信息（CXMI）作为衡量机器翻译模型使用上下文的指标。这通过测量给定源文本x时，上下文c对目标y提供的信息量来实现。</sample>
    <sample id="1060">你可以把CXMI看作是给模型提供上下文后获得的信息量。</sample>
    <sample id="1061">在本工作中，我们扩展了CXMI到点积CXMI，它可以衡量句子级别或单词级别的上下文使用。我们可以将具有高P-CXMI的词视为需要上下文进行翻译的词。</sample>
    <sample id="1062">现在我们使用高PE6MI来分析单词，以查找这些单词之间的模式。</sample>
    <sample id="1063">我们对从英语翻译成14种不同语言的TED演讲的转录进行分析。</sample>
    <sample id="1064">我们首先分析了高P-CXMI值的语音片段。</sample>
    <sample id="1065">这使我们能够找到例如阿拉伯语中的复数名词，这些名词具有相对较高的p-cxmi值。这可以解释为英语没有复数名词，因此需要一种方法来确定一个名词是否是复数，当翻译成阿拉伯语时。</sample>
    <sample id="1066">同时我们发现某些语言也要求语境，当我们选择适当的动词形式时。我们查看了具有高P-CXMI平均值的词汇项，即在所有不同出现中具有高P-CXMI平均值的词汇项。</sample>
    <sample id="1067">And this helps us identify cases like the one here, where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document.</sample>
    <sample id="1068">同样，我们发现条件支持在正确的形式中进行翻译。</sample>
    <sample id="1069">最后，我们来看一下具有高 P-CXMI 的不同个体标记。这使我们能够识别出不能仅仅通过单词本身捕捉到的现象，而是表达在句子结构中，例如 ellipsis resolution。</sample>
    <sample id="1070">现在我们利用分析结果来设计文档级别翻译的基准。</sample>
    <sample id="1071">对于我们识别出的五个 discourse 现象，我们创建了标签以自动识别与现象相关的单词，并将其称为多语言 discourse-aware（MuDA）标签。</sample>
    <sample id="1072">我们可以看出不同语言中这些 discourse 现象的比例不同。</sample>
    <sample id="1073">我们接着使用MuDA标注器，通过将标注器应用于我们要用于评估的平行语料库。然后我们在MuDA标注器识别出的依赖性相关的示例上应用我们的翻译指标。</sample>
    <sample id="1074">最后，我们使用我们的基准测试以及其它指标来评估不同模型在文档级别的机器翻译上的性能。</sample>
    <sample id="1075">首先，当我们使用语料库级别指标时，对于蓝色，我们发现基于图标的模型具有最佳性能。</sample>
    <sample id="1076">但如果我们使用 comet，基于上下文的模型表现最佳。如果我们使用 word f 测量，则带有或不带有上下文的模型具有可比的表现。</sample>
    <sample id="1077">这再次说明了仅使用语料库级别的指标来确定最佳文档级别翻译系统是困难的。</sample>
    <sample id="1078">现在我们使用MUMA基准测试评估模型，并发现使用上下文的模型在某些 discourse 现象，如正式性和词汇连贯性方面显著更准确的模型。</sample>
    <sample id="1079">但这些模型在其他现象如省略、词性和语态形式上并没有比不使用上下文的模型表现得更好。因此，这表明我们需要在文档级翻译方面取得更多进展。</sample>
    <sample id="1080">我们还比较了不同的商业系统，我们的基准测试显示DeepL通常比Google Translate在文档级别翻译上更准确。</sample>
    <sample id="1081">总之，我们对14种语言对进行了数据驱动的分析，以确定何时翻译需要上下文。</sample>
    <sample id="1082">然后我们利用我们的发现构建文档级别机器翻译的基准，这可以帮助我们确定哪些 discourse 现象模型可以处理好或不好，以及哪些翻译系统在文档级别翻译方面表现良好。</sample>
    <sample id="1083">谢谢您的关注。祝您在Turito中一切顺利。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。</sample>
    <sample id="1121">根据给定的图片，没有明确说明新方法的名称。因此，答案是没有名称。</sample>
    <sample id="1122">根据幻灯片，作者描述“显性词汇”(marked words) 方法为一种识别区分标记群体和未标记群体的单词的方法。这表明该方法涉及分析文本以确定哪些特定术语或表达方式是某些群体特有的，从而可以用来识别或分类这些群体。</sample>
    <sample id="1123">根据幻灯片显示，作者Shangbin Feng来自美国华盛顿大学。</sample>
    <sample id="1124">第一个提到的对称依存关系结构的名称是“Prague approach”，它指的是在依存树中将冠词结构作为连接词处理的结构。</sample>
    <sample id="1125">演讲者的名字是Sarah Finch和James Finch。</sample>
    <sample id="1126">根据幻灯片中提供的英文信息，这篇论文有四位作者：Javad Hosseini、Filip Radlinski、Silvia Paretti和Annie Louis。</sample>
    <sample id="1127">BLMP、SyntaxGym和CrowS数据集可用于测试句法现象。</sample>
    <sample id="1161">根据英语内容，第一个研究问题的五种方法的缩写是FT、W、BOND、COSINE和MLC。</sample>
    <sample id="1162">根据英语内容，该模型在11项生物医学和临床下游任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在 4GB 的数据子集上训练的。</sample>
    <sample id="1227">根据幻灯片显示，演讲者的名字是Adam Przpielkowski。</sample>
    <sample id="1228">根据所给英文内容，导致时间漂移是性能下降的主要原因的发现是：通过实验重新训练或继续预训练一些模型使用更近的数据，我们发现性能随着时间间隔的增大而恶化。</sample>
    <sample id="1269">在自然语言处理任务中，词元的排列顺序至关重要。这是因为语言是顺序依赖的；词的顺序可以改变句子的意义。例如，在提供的图片中，如果词元'girl'和'sleep'的顺序被改变，形成的句子'girl the slept'将无法传达有意义的信息。因此，对输出序列中的词元进行排列确保生成的句子在语法上正确且语义上有意义。</sample>
    <sample id="1270">根据图片中呈现的建议，作者建议模型所有者提高偏见缓解方法的透明度，以解决由于存在潜在的过度价值对齐或反刻板印象方法而导致的刻板印象图案。通过提供关于这些方法的详细信息，可以更好地理解它们的效果，并确保它们有效地减少偏见，而不是引入新的问题。</sample>
    <sample id="1271">最小对不可接受输入是展示一个可接受句子或语法正确的句子，然后展示一个不可接受句子或不语法正确的句子。</sample>
    <sample id="1272">根据给定的英文内容，作者使用了准确率和F1分数作为评估指标。</sample>
    <sample id="1273">在所给内容中，注释者之间一致性是通过内注释者协议来衡量的。</sample>
    <sample id="1274">在不可接受和可接受查询中，选择维基百科领域来添加完全无关的句子。</sample>
    <sample id="1275">根据幻灯片上提供的信息，论文的作者来自海因里希·海因里希大学，位于德国的杜塞尔多夫。</sample>
    <sample id="1276">MultiInstruct 与现有的基准不同，因为它是一个大规模的多模态指令调优数据集。</sample>
    <sample id="1277">根据幻灯片上显示的作者姓名，这篇论文共有三位作者：Sarah E. Finch、James D. Finch和Jinho D. Choi。</sample>
    <sample id="1278">在给定的图片中，二进制协调指的是一个变量（例如长度）与另一个变量（例如音节数）之间的线性关系。这种关系通过绘制数据点并拟合一条直线来表示，该直线代表了两个变量之间的最佳线性拟合。在这些图表中，我们看到不同长度单位（字符、音节和单词）之间的二进制协调，表明了这些变量之间存在线性关系。</sample>
    <sample id="1279">根据图片中呈现的柱状图，提示语的平均长度似乎在1到2个单词之间。 这可以通过观察每个类别的柱状图的平均高度来推断，其中类别包括“篮球”，“大胆”，“态度”，“运动员”和“高”。 每个类别的柱状图都相对较低，表明提示语较短。</sample>
    <sample id="1280">这些发现表明较小的 T5 模型，特别是那些在合适的数据集上微调过的模型，可以生成高质量的脚本，从而挑战了较大语言模型（LLMs）在所有任务上的优越性。这可能意味着较小的模型可以成为更高效、更有效的替代方案，特别是在需要专门知识或领域特定知识的情况下。</sample>
    <sample id="1281">你好，我是Yanis Labrak，我将向你介绍我们关于DrBERT的研究，DrBERT是一个在生物医学和临床领域中具有稳健预训练模型的法文。</sample>
    <sample id="1282">首先，我们将讨论医疗保健中的语言建模。然后我们将展示我们论文的主要贡献。</sample>
    <sample id="1283">我们介绍了第一个生物医学模型，名为DrBERT，它是基于RoBERTa，并在NACHOS上训练，NACHOS是一个包含来自网络的医学Crowd数据集。</sample>
    <sample id="1284">我们还引入了具有多种预训练设置和数据来源的模型比较。然后，我们在法国的11个生物医学和临床下游任务上展示了我们的结果。</sample>
    <sample id="1285">最后，我们将总结实验并为您提供更多关于如何访问模型的详细信息。</sample>
    <sample id="1286">自2018年发布以来，BERT已成为解决自然语言处理任务最有效的方法之一，并在与历史静态和概念化方法（如Word2Vec、FastText和Word）相比提供了巨大的性能提升。</sample>
    <sample id="1287">自那以后，这个模型已经适应了多种其他语言，如法语中的Camembert和生物医学领域的PumMedBERT和BioBERT，以及临床领域的ClinicalBERT，但主要是在英语中。</sample>
    <sample id="1288">其他语言的专门化模型往往稀缺，并且通常基于持续预训练，因为缺乏本族数据。</sample>
    <sample id="1289">然而，法语目前没有开放源代码的生物医学模型。</sample>
    <sample id="1290">我们问自己关于最适合广泛用途的数据来源的问题，这些公共数据是临床数据的良好替代品。</sample>
    <sample id="1291">为了回答这个问题，我们将比较Dr. Bert和我们的Shubert模型，后者基于从Nantes大学医院数据库中提取的匿名化数据。</sample>
    <sample id="1292">接下来，我们问自己需要多少数据来训练一个专门针对法语数据的模型。是4GB、8GB还是更多？</sample>
    <sample id="1293">首先，我们分别训练和比较了四个从头开始的模型：第一个是使用7GB NACHOS数据集训练的第一个版本的DoctorBERT；第二个是使用4GB子集NACHOS数据集训练的第二个版本的DoctorBERT。</sample>
    <sample id="1294">第一个版本的Shubert是一个临床模型，使用了从临床笔记中提取的4GB句子。第二个版本的Shubert则结合了4GB NACHOS子集和4GB临床笔记。</sample>
    <sample id="1295">除了这个比较之外，我们还介绍了三种基于连续预训练的模型，以分析预训练策略的影响。</sample>
    <sample id="1296">一种基于卡曼贝的模型在NACHOS数据集的4GB子集中进行训练，另一种基于卡曼贝的模型在Clinkinnotes数据集的4GB子集中进行训练。</sample>
    <sample id="1297">最后，基于English biomedical model的bert模型，并在Nature的4GB子集中进行训练。总共，我们有7个模型。</sample>
    <sample id="1298">为了评估我们7个模型，我们将使用各种公共和私人医疗数据集任务，如命名实体识别、分类、对话生成和问答。</sample>
    <sample id="1299">这些模型与六个基线模型进行了比较，包括Cannabrosca 108 GB、Cannabrosca 4 GB、Canamberscnet 4 GB、DeBERTa-Bert和ClinicalBERT。</sample>
    <sample id="1300">评估表明，该模型在与训练数据类型相同的任务上表现最佳。</sample>
    <sample id="1301">然而，我们可以从其他来源获取数据。我们还观察到，来自异质性来源的数据似乎更具代表性。此外，我们观察到使用更多数据可以转化为更好的表现。</sample>
    <sample id="1302">总的来说，从头开始预训练似乎在大多数任务上取得了更高的性能。</sample>
    <sample id="1303">然而，我们对使用PermedBERT的权重和词嵌入进行持续预训练的实验表明，在NatCOS的4GB子集上取得的结果与从头开始训练的DoctorBERT 4GB相当。</sample>
    <sample id="1304">这并不是使用Camembert权重和Tokenizer训练的模型的情况，这些模型存在稳定性问题。</sample>
    <sample id="1305">最后，结论是我们的系统在11个任务中的9个上提供了更好的性能，并且超过了通用模型Camembert的全球结果。</sample>
    <sample id="1306">我们还观察到，专业化的数据更好，但专业化的数据不会很好扩展。</sample>
    <sample id="1307">所有预训练模型都来自NACHOS，并且可以在Avignon Université的网站上免费获取。此外，所有的训练脚本都可以在我们的GitLab仓库中找到。</sample>
    <sample id="1308">所以，感谢大家对这个演示的贡献。我们期待在多伦多的海报会上与大家交流。</sample>
    <sample id="1309">论文研究了以下学习策略：1. 从头开始构建一个模型，使用7GB的NACHOS数据集。2. 使用4GB子集的NACHOS数据集构建另一个模型。3. 使用4GB临床句子构建第一个ShuBERT模型。4. 使用4GB子集的NACHOS数据集和4GB临床句子构建另一个ShuBERT模型。5. 使用NACHOS数据集和临床句子的混合构建第三个ShuBERT模型。</sample>
    <sample id="1310">根据所给英文内容，由于测试重复使用而导致的过拟合因素似乎并不显著。这可以从图表右侧的红色趋势线的斜率大于1来证明，这意味着在CABLL-2003上的每一次单位改进转化为在CABLL++上超过一个单位的改进，表明没有减小回报。</sample>
    <sample id="1311">根据所给内容，简化质量通过使用BLEU和BLEU-4等评估指标来评估。这些指标用于衡量模型生成的简化文本与原始复杂文本之间的相似度。得分越高表示简化文本与原始文本越接近，从而表明简化质量越好。</sample>
    <sample id="1312">是的，语言模型有政治偏见。</sample>
    <sample id="1313">Hi, my name is Matthias Lindemann and today I'm going to give you a brief introduction to our paper on compositional generalization without trees using multiset tagging and latent permutations.</sample>
    <sample id="1314">这是与我的导师Alexander Koller和Ivan Titov合作的联合工作。</sample>
    <sample id="1315">组合泛化能力可以理解为学习者处理更深层次递归和未见过的短语组合的能力，这些短语在训练期间单独出现。</sample>
    <sample id="1316">在语义解析的背景下，测试组合泛化可能看起来像这样。就像往常一样，我们有一个训练集的句子，在这种情况下是“女孩睡了”和“Mary知道女孩睡了”。</sample>
    <sample id="1317">这些短语与表示它们含义核心方面的逻辑形式配对。</sample>
    <sample id="1318">与标准机器学习评估不同，测试集不来自相同的分布，但包含结构上 unseen 的逻辑形式。</sample>
    <sample id="1319">在这个例子中，模型在训练过程中遇到了浅层递归，但在测试时遇到了深层递归的示例。</sample>
    <sample id="1320">Naive sequence-to-sequence models struggle with this kind of out-of-distribution generalization and often produce outputs that are detached from the input.</sample>
    <sample id="1321">特别是，它们往往无法 reproducing input 和 output 之间的系统对应关系，例如在示例中用颜色编码的那些。</sample>
    <sample id="1322">一种常见的方法是将树融入模型中。</sample>
    <sample id="1323">The trees are intended to capture the compositional process that relates utterances with logical forms.</sample>
    <sample id="1324">这很好，但树通常不会给定，需要以某种方式获取。</sample>
    <sample id="1325">这可能会变得复杂，有时会成为一个计算昂贵的过程。通常，这涉及大量的形式化特定预处理的逻辑形式，例如处理变量符号。</sample>
    <sample id="1326">获取树可能还涉及专门的语法推导程序。</sample>
    <sample id="1327">在这篇论文中，我们不使用树，并引入了一个神经序列到序列模型，该模型直接建模了输入片段和输出片段之间的对应关系。</sample>
    <sample id="1328">For the first time, we show strong generalization to deeper recursion without relying on trees.</sample>
    <sample id="1329">我们的方法在两个步骤中预测输出。</sample>
    <sample id="1330">首先，我们为每个输入标记一个无序的多集标记将在输出中出现的标记。</sample>
    <sample id="1331">在第一个步骤之后，我们有所有正确的标记，但它们没有排序。</sample>
    <sample id="1332">这就是为什么在第二步中我们使用另一个模型来预测排列，以便将它们放入正确的顺序。</sample>
    <sample id="1333">我们引入了一种新方法，用于预测不施加任何硬约束的排列。这使我们的方法变得相当灵活和表达力强。</sample>
    <sample id="1334">概念上，我们的排列模型大致像这样运作。</sample>
    <sample id="1335">我们从左到右遍历输出，并确定将每个位置的多集标记放在哪个标记。对于第一个输出位置，我们简单地选择一个，如红色所示。</sample>
    <sample id="1336">然后我们跳到下一个多集标记，以确定输入中的第二个标记。</sample>
    <sample id="1337">我们以类似的方式确定输出中的第三个标记，通过跳转到另一个多集标记。我们继续这个过程</sample>
    <sample id="1338">直到第一阶段的每个标记都被访问过一次。</sample>
    <sample id="1339">为了给你一个实验结果的 teaser，这里我们比较了我们的方法与其他 Treeless 模型在 COGS 基准上的表现。我们的模型在泛化到更深层次的递归时，比其他模型表现出更大的优势。</sample>
    <sample id="1340">一些其他类型的结构泛化仍然非常具有挑战性。</sample>
    <sample id="1341">在我们的论文中，我们解决了几个有趣的技術挑戰。</sample>
    <sample id="1342">首先，输入和输出之间的对齐在训练数据中没有给出。因此，对于给定的标记，我们不知道它来自哪个多头，这为训练提出了挑战。</sample>
    <sample id="1343">此外，有时会有多个排列与数据一致，但语言上正确的排列是潜在的。我们通过将对齐作为训练的一部分来解决这个问题。</sample>
    <sample id="1344">我们的排列方法非常灵活，但它带来了挑战，即找到得分最高的排列是NP困难的。这是因为这与旅行商问题有关。</sample>
    <sample id="1345">我们用GP U友好连续放松来近似这个，它还允许我们通过解决方案反向传播并学习更符合语言的排列。</sample>
    <sample id="1346">如果您想了解我们实验以及我们如何解决这些挑战，请查看我们的论文或参加海报。</sample>
    <sample id="1347">认知失调是指两种思想、行动或信仰之间存在不一致的情况。</sample>
    <sample id="1348">根据所给的英文内容，GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="1349">累积训练在主动学习中与迭代训练相比更有效。</sample>
    <sample id="1350">演讲者的名字是Sara Papi。</sample>
    <sample id="1351">MuDa基准中的数据是从TED Talks的转录中获得的，这些转录已经翻译成14种不同的语言。</sample>
    <sample id="1385">演讲者的名字是Matthias Lindemann。</sample>
    <sample id="1386">跨语言转移指的是在一种语言中训练模型，然后将其应用于另一种语言。在演示文稿中，它被描述为“训练在一个源语言上，然后转移到另一种语言”。</sample>
    <sample id="1387">根据图片中提供的信息，这篇论文的作者分别来自萨尔兰德大学、亚马逊Alexa和维也纳大学。</sample>
    <sample id="1388">根据音频内容，作者使用了两种延迟测量方法：平均延迟和计算延迟。平均延迟是模型的输出结果的延迟，而计算延迟考虑了模型的计算时间。</sample>
    <sample id="1389">Hello everyone, I'm Manjata and today my co-author Martin and I are presenting our work that KITMUS evaluates knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila and Microsoft Research.</sample>
    <sample id="1390">自然语言理解模型利用各种知识来源，如参数中包含的知识（通常通过预训练获得）和输入中提供的知识（在推理时提供）。</sample>
    <sample id="1391">最近在问答等任务上的研究表明，模型可以利用预训练知识来解决任务。</sample>
    <sample id="1392">但自然语言理解通常需要在推理时提供的知识。</sample>
    <sample id="1393">例如，在句子“John saw the newly elected president on TV”中，</sample>
    <sample id="1394">预训练参数可以包含关于总统做什么和电视是什么的信息，但它们不能可靠地知道这个特定实体John是谁或新任总统是谁，因为总统自预训练以来可能会发生变化。</sample>
    <sample id="1395">因此，对于知识密集型NLU任务的成功的模型需要具备整合和使用预训练时间和推理时间知识的能力。</sample>
    <sample id="1396">在这个工作中，我们提出了一个诊断测试套件用于知识整合。</sample>
    <sample id="1397">我们引入了一个核心参考解决任务，旨在评估利用不同来源中可获得知识的能力。我们使用人类研究参与者和已建立的核心参考解决模型评估数据集。</sample>
    <sample id="1398">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax.</sample>
    <sample id="1399">这个任务是确定代词'他'所指的正确实体，在这种情况下是Servin。</sample>
    <sample id="1400">确定给定代词的含义需要两种类型的信息。首先，实体特定知识，例如Servin是一名法官；其次，背景知识，例如法官在法庭上决定案件。</sample>
    <sample id="1401">一般而言，背景知识是在大规模语言模型的预训练过程中学习的，而实体特定知识通常在推理过程中观察到。</sample>
    <sample id="1402">我们改变这些两部分信息的可获得性，使其可能在单一来源中找到或在多个来源中找到。</sample>
    <sample id="1403">我们已经定义了KIMMUS的三种设置。首先，我们有背景预训练设置。在预训练时间，背景知识被认为是可以获得的。</sample>
    <sample id="1404">第二，有一个背景both设置，其中背景知识在预训练时间和推理时间都可用。最后，有一个背景inference设置，其中两种知识类型仅在推理时间可用。</sample>
    <sample id="1405">这个最后的设置特别有趣，因为它模拟了背景知识对于解决任务是必要的，但不是预训练模型数据的一部分。例如，因为自预训练以来已经发展出新的职业。</sample>
    <sample id="1406">这是一个关于如何控制双源中可获取性效果的例子。</sample>
    <sample id="1407">在背景预训练设置中，我们假设背景知识“政治家寻求当选政府的席位”包含在预训练参数中。在特定任务背景下，我们提供特定知识“杰里·特朗普是一个政治家”。</sample>
    <sample id="1408">在背景-Both设置中，我们还额外提供关于政治家的背景知识，以及在受干扰的背景下。</sample>
    <sample id="1409">在背景-推断设置中，提供有效的职业“MIRTOU”而不是“政治家”，因为MIRTOU不太可能被包含在预训练阶段。</sample>
    <sample id="1410">我们使用人类学习参与者验证数据集，并建立关键分辨率模型。在这幅图中，我们展示了在背景预训练设置中最难的变体上表现最好的模型的结果。</sample>
    <sample id="1411">Without task-specific training on KidCo, both models do not perform well. However, when trained on KidCo, both BERT4Coref and C2F perform significantly better than random choice.</sample>
    <sample id="1412">这表明，当训练在更一般化的偏好分辨率数据集上时，模型学会利用表面线索，这些线索在测试时在KIDS中已被移除。</sample>
    <sample id="1413">额外的实验表明，即使表现最好的模型也难以可靠地整合背景知识，只能在推断时提供。</sample>
    <sample id="1414">许多预训练的模型似乎无法在没有任务特定培训的情况下从多个来源推断知识。然而，通过任务特定培训，一些模型成功地将来自多个来源的知识整合在一起。</sample>
    <sample id="1415">即使最好的模型似乎在可靠地整合仅在推理时间呈现的背景知识方面存在困难。如果您对更多细节感兴趣，请参阅我们的论文，并查看GitHub上的数据集和代码。谢谢您的收听。</sample>
    <sample id="1416">根据所提供的内容，基于树的方法的缺点包括：1. 树通常需要通过某种方式获取，这可能是一个复杂且计算昂贵的过程。2. 获取树通常涉及对逻辑形式进行具体的预处理，例如处理变量符号。3. 获取树还可能涉及专门的语法归约过程。</sample>
    <sample id="1417">根据幻灯片显示，论文的作者属于“Georgia Institute of Technology”的“School of Interactive Computing”。</sample>
    <sample id="1418">嗨，我是Myra。今天我们将讨论我们关于“标记的人物”（Marked Personas）的研究，这是通过自然语言提示来衡量语言模型中的刻板印象。这项工作是在与Essin Durmus和Dan Jurafsky合作完成的。</sample>
    <sample id="1419">近年来，许多研究记录了大型语言模型（LLMs）中社会偏见和刻板印象的普遍性。</sample>
    <sample id="1420">然而，这些措施有各种限制。它们通常依赖于手工制作的数据集，这些数据集非常耗时。</sample>
    <sample id="1421">他们通常只衡量非常具体的刻板印象，这意味着它们不能很好地推广到其他人口统计或背景，或者它们简单地捕捉到非常广泛的广泛关联，比如对特定群体的负面关联。</sample>
    <sample id="1422">此外，本领域的大多数研究并没有考虑到交叉性，即多维社会身份可能会加剧偏见并成为伤害的独特来源。</sample>
    <sample id="1423">为了克服这些限制，我们依赖于这些 newer instruction-tuned LLMs 的特性，它们非常擅长响应指令和提示。</sample>
    <sample id="1424">因此，我们可以要求模型生成一个自画像，即使用类似“假设你是亚洲女性，请描述你自己”的提示来描述一个想象中的个体。</sample>
    <sample id="1425">我们可以立即看到，这可以泛化到任何 demographics，因为我们只需要将我们想要的任何身份标记输入到这个 prompt 中。</sample>
    <sample id="1426">这是GPT-4的一些示例生成内容。</sample>
    <sample id="1427">立即我们看到，虽然输出并不明显负面或有毒，但在传统意义上的这些词中。</sample>
    <sample id="1428">有些有趣的模式。</sample>
    <sample id="1429">这位亚洲女性被描绘为不引人注目的，而中东女性则被提及使用诸如异国情调和令人着迷的地区等词语。</sample>
    <sample id="1430">这段内容提到了三种不同的人物形象：一个亚洲女性、一个中东女性和一个白人男性。亚洲女性和中东女性的形象中都提到了祖先，而白人男性的形象中没有提到祖先。</sample>
    <sample id="1431">为了捕捉这些模式，我们的方法分为两部分。第一部分是生成这些人物形象。</sample>
    <sample id="1432">这些生成人物的提示受到一项研究的启发，他们向人类受试者提供了这些提示。通过给予这些提示，他们还能够表面种族刻板印象。</sample>
    <sample id="1433">此外，这也使我们生成的人物与人类撰写的回应之间进行直接比较成为可能。</sample>
    <sample id="1434">第二部分是标记的单词，这是一种方法，可以识别出区分已标记群体和未标记群体的单词。我将在下面详细解释。</sample>
    <sample id="1435">这种做法的好处是，我们可以在不依赖任何特定词汇的情况下获得非常具体的刻板印象和模式。</sample>
    <sample id="1436">标记词方法基于社会语言学概念，即存在一个未标记的默认值，任何与该默认值不同的群体在语言上被标记。</sample>
    <sample id="1437">例如，单词“战士”通常与男性相关联。因此，当人们描述一个女性战士时，他们通常会明确指定“女性战士”，并用“女性”标记这个术语。</sample>
    <sample id="1438">更广泛地说，社会中的 dominant groups 在语言和社交上都是 unmarked 的，而 marginalized groups 通常是 marked 的。</sample>
    <sample id="1439">在我们的方法中，我们首先定义未标记和标记组。</sample>
    <sample id="1440">然后我们使用“标记词方法”比较个人，这是一种使用加权对数比值来区分每个标记组的顶部单词的方法。</sample>
    <sample id="1441">例如，对于黑女人的形象，我们会进行对比词分析，并将log-odds比率与白人形象和男人形象进行比较，因为它们是对应的未标记群体。</sample>
    <sample id="1442">现在我们来看一下结果。首先，我们使用了刻板印象词典，并发现生成的人物包含的刻板印象比人类写的人物要多得多。</sample>
    <sample id="1443">然而，当我们查看词汇表中单词的分布时，我们发现了一些不同的事情。</sample>
    <sample id="1444">然而，生成的个性特征具有更高比例的刻板印象词汇，而人类书写的个性特征具有更广泛的词汇分布。生成的个性特征中出现的刻板印象词汇主要是“高”和“能干”。</sample>
    <sample id="1445">所以，实际上只有正面或至少是非负面的。</sample>
    <sample id="1446">实际上，这个词汇表并没有捕捉到我们在早期幻灯片中看到的许多有害的模式。因此，我们将转向我们标记单词方法的结果，以展示这些积极的词语如何促进刻板印象和本质化叙事。</sample>
    <sample id="1447">在我们的分析中，我们查阅了这些表面上看似积极的刻板印象如何反映了有害的模式。</sample>
    <sample id="1448">首先，对于被标记的群体，排名最高的词汇包括文化、传统、自豪和异国情调。这些词汇通过仅基于身份关系来定义这些群体，并将它们与白人规范区分开来。</sample>
    <sample id="1449">这段内容讨论了某些群体长期遭受的歧视和边缘化现象。它提到了一个“long legacy of discrimination and othering”（长期的歧视和边缘化），这意味着这些群体已经面临着很长一段时间的不公正待遇和被排斥。这种现象可能涉及社会、政治或文化层面的多种歧视形式，导致这些群体在各个方面都受到不利影响。</sample>
    <sample id="1450">此外，还有一些常见的 tropes 在这些词中反映出来，特别是针对有色人种女性。例如，描述 Latina 女性的词语包括“充满活力”和“丰腴”。</sample>
    <sample id="1451">与 tropes of tropicalism 相关。对于亚洲女性，这些词包括“娇小”、“精致”和“光滑”。</sample>
    <sample id="1452">这段内容提到了亚洲女性被性化的历史，她们被视为非常可爱和顺从。</sample>
    <sample id="1453">最后，对于黑人女性，我们看到一些关键词是“坚强”和“ resilient”等。</sample>
    <sample id="1454">这与人们称为“坚强黑女人”原型的刻板印象相关。虽然乍一看，它听起来很积极，但...</sample>
    <sample id="1455">已经有一些研究表明，这种刻板印象实际上是非常有害的，因为它给这些人口统计群体带来了很大的压力，让他们必须在社会障碍面前表现出坚强和坚强。</sample>
    <sample id="1456">相反，它并没有朝着改变这些障碍的方向努力，而是给这些人带来了克服它们的压力，这导致了这些人群中其他伤害的负面健康结果。</sample>
    <sample id="1457">更广泛地说，我们发现每个标记群体的词汇几乎反映了非常本质化的叙事。</sample>
    <sample id="1458">根据这些模式，我们得出三个建议供模型所有者参考。</sample>
    <sample id="1459">首先，我们应该作为研究人员来解决积极刻板印象和本质化叙事。我们还应该使用交叉视角来研究偏见和伤害，因为如果不这样做，可能会错过一些重要的事情。</sample>
    <sample id="1460">最后，应该增加关于偏见缓解方法的透明度。</sample>
    <sample id="1461">因为例如这些正面刻板印象，我们不知道是因为存在某种奇特的东西。</sample>
    <sample id="1462">过度的过度价值对齐正在进行中，或者可能是其他一些反刻板印象方法导致了这些令人难以置信的图案。</sample>
    <sample id="1463">我们真的不能在没有更多透明度的情况下做出任何假设或进一步研究。</sample>
    <sample id="1464">谢谢你们收看。祝你们有一个愉快的一天。</sample>
    <sample id="1465">Hello everyone, my name is Jingwei Yi from the University of Science and Technology of China.</sample>
    <sample id="1466">我很乐意为您制作一个简短的广告视频，介绍我们的论文《您复制我的模型？通过后门水印保护大型语言模型的版权》。</sample>
    <sample id="1467">首先，我们先介绍关于 embedding as a service 的背景。</sample>
    <sample id="1468">当前，大型语言模型如GPT、LLaMA和PaLM在自然语言理解和生成方面表现出色。</sample>
    <sample id="1469">将嵌入服务作为一项服务提供，是基于大型语言模型构建的服务之一，用于协助各种NLP任务。</sample>
    <sample id="1470">例如，OpenAI 提供了一个基于 GPT 的 embedding API。</sample>
    <sample id="1471">然而，最近的研究表明，攻击者可以通过学习嵌入并提供类似的服务来窃取模型。因此，必须保护嵌入的版权作为服务。</sample>
    <sample id="1472">为了保护嵌入服务的版权，其中一种解决方案是在提供的服务中嵌入水印，并检测另一个服务是否包含水印。</sample>
    <sample id="1473">水印方法需要满足以下属性：首先，该方法应该适用于嵌入攻击服务。其次，水印不应该降低所提供的嵌入的实用性。</sample>
    <sample id="1474">第三，水印应该足够隐蔽，让攻击者无法轻易移除水印。</sample>
    <sample id="1475">最后，水印需要在模型提取过程中传输到攻击者的服务。</sample>
    <sample id="1476">现有的工作可以被广泛地分为四个类别。</sample>
    <sample id="1477">然而，这些方法要么不适用于嵌入EaaS服务，要么缺乏可转移性。</sample>
    <sample id="1478">因此，在本文中，我们提出了嵌入标记，它是一种后门基于的水印方法，适用于嵌入AI服务。</sample>
    <sample id="1479">然后让我介绍我们的 embedding marker 的详细内容。 embedding marker 包含两个主要步骤：水印注入和 Copyright 验证。</sample>
    <sample id="1480">在这些主要步骤之前，我们首先选择一个触发器集。触发器集是一组在中等频率间隔内的单词。</sample>
    <sample id="1481">我们假设提供者可以收集一个一般文本语料库，并计算其中的单词频率。</sample>
    <sample id="1482">在水印注入中，我们首先定义目标嵌入。当用户向提供商服务发送句子时，提供商计算句子中的触发器数量。</sample>
    <sample id="1483">提供的嵌入是目标嵌入和原始嵌入的加权求和。</sample>
    <sample id="1484">目标嵌入的权重与句子中的触发器数量成正比。当句子中的触发器数量大于m时，提供的嵌入完全等于目标嵌入。</sample>
    <sample id="1485">Copyright verification is to detect whether a model behind another service contains the watermark.</sample>
    <sample id="1486">我们首先构造一个后门和 benign 数据集。后门数据集包含所有单词都属于触发集的句子，而 benign 数据集中的所有单词都不属于触发集。</sample>
    <sample id="1487">请求提供者从窃贼的服务中获取数据集的嵌入</sample>
    <sample id="1488">计算请求嵌入和目标嵌入之间的余弦和L2相似性。我们计算 benign 和 backdoor 数据集之间的相似性差异，定义为 Δcosine 和 ΔL2。</sample>
    <sample id="1489">与此同时，我们还应用了卡方检验，并将其p值作为第三个度量。</sample>
    <sample id="1490">我们在四个数据集上进行实验：AG News、MIND、SST2和Enron Spam。我们假设提供者使用Wikipedia数据集来计算词频。</sample>
    <sample id="1491">实验结果表明，对于四个数据集，我们的嵌入标记器可以在保持良好实用性的前提下具有很好的检测性能。</sample>
    <sample id="1492">我们还通过可视化嵌入的句子在数据集上的嵌入来验证所提供的嵌入的公平性。图例表示每个句子中的触发器数量。</sample>
    <sample id="1493">如图所示，很难区分向量嵌入和普通嵌入。</sample>
    <sample id="1494">那全部了，谢谢。欢迎与我们讨论。</sample>
    <sample id="1495">ABC-Eval代表“聊天行为注释”或“聊天行为评估”。</sample>
    <sample id="1496">根据图表显示，CoNLL-2003 和 CoNLL++ 之间的性能增量在 2014 年之前高于 5 个百分点。图表上标有橙色线代表 CoNLL++，蓝色线代表 CoNLL-2003。橙色线明显高于蓝色线，表示性能提升。图表上标记的年份是 2004、2008、2012、2016、2018 和 2022。因此，直到 2014 年，性能增量才高于 5 个百分点。</sample>
    <sample id="1497">Hello, my name is Vasudha, and I am a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper: "Transfer Learning for Dissonance Detection: Addressing the Rare Class Challenge."</sample>
    <sample id="1498">我们首先定义了认知不一致，并解释了为什么研究语言中的这个问题很重要。简单地说，认知不一致是指两个相互矛盾的想法或行为。</sample>
    <sample id="1499">比如这个例子，一个人说：“我知道香烟会杀死我，然后又说：“我抓了几支烟在会议后。”这个信念和行动是不一致的，它们处于认知不协调状态。</sample>
    <sample id="1500">进一步说明了“我不认为没有他们我能保住工作”证明了第二个陈述，并且它们之间存在一致性关系。</sample>
    <sample id="1501">虽然认知不一致是一种在日常决策中非常常见的现象，但在其他类型的 discourse 关系中，它们在语言中表达得很少见。</sample>
    <sample id="1502">为什么这很重要？研究认知失衡可以帮助我们理解人群中的分歧影响、追踪趋势以及信念、价值观和态度的变化。</sample>
    <sample id="1503">高水平的认知分歧也与焦虑障碍有关，并有助于更好地理解人们的心理健康。</sample>
    <sample id="1504">研究分歧性表达的语言表达也有助于理解极端主义和边缘化群体的 polarization。</sample>
    <sample id="1505">最后，认知失衡对于理解个人的认知风格以及更好地理解决策过程至关重要。</sample>
    <sample id="1506">为了创建一个认知分歧资源，我们进行了大规模标注分歧关系的大规模标注。我们使用了分歧的第一种方法，如流程图中所示。</sample>
    <sample id="1507">推特被使用pythaitb解析器解析，每对 discourse units 根据我们在论文中描述的指南进行标注。</sample>
    <sample id="1508">正如这里所见，分歧仅在3.5%的注释对中被发现。</sample>
    <sample id="1509">在收集了大约1000个 discourse 单位对样本后，我们只用43个样本训练了一个初始分类器。不出所料，分类器的表现并没有比随机猜测好多少。</sample>
    <sample id="1510">鉴于分歧的低发生率和缺乏任何此类数据集的先例，我们面临着绝对罕见的问题。</sample>
    <sample id="1511">为了缓解这个问题，我们实验了组合使用迁移学习和主动学习来标注数据，这样就可以收集更多离散样本，同时减少标注错误，从而降低总体标注成本，同时提高离散度检测能力。</sample>
    <sample id="1512">由于初始模型无法捕捉到“ deficiency”类别，我们通过从相关任务中转移权重来启动冷启动标注的主动学习过程。</sample>
    <sample id="1513">我们从两个不同的任务中进行转换：主题独立的分歧姿态分类任务，该任务确定了来自不同人的两条辩论声明是否在意见上一致或不一致，无论主题如何。</sample>
    <sample id="1514">我们在这里称为“辩论”和“辩论”论坛中的二元分类扩展和对比类的“辩论”和“对比”，因为这些与一致性和分歧的概念密切相关，我们在这里称为CE。</sample>
    <sample id="1515">我们发现，在将零样本性能应用于标注数据集时，已经比随机表现得好了许多，最好的表现是AUC 0.62。</sample>
    <sample id="1516">进一步在交替微调两个任务上，我们发现先对CE任务进行微调，然后再对辩论任务进行进一步微调，可以得到更好的零样本性能。因此，这是我们在冷启动的主动学习中使用的模型。</sample>
    <sample id="1517">接下来，我们确定了从每一轮主动学习和注释中更新模型的最佳方法。累积器将所有收集到的数据从主动注释中积累起来，而迭代器通过在最新收集的数据集上进行训练来更新模型。</sample>
    <sample id="1518">在不同的策略上，我们发现累积性表现优于或等于迭代性。</sample>
    <sample id="1519">接下来，为了提高离散样本的数量，我们使用概率罕见类策略（PRC）来选择那些最有可能被当前模型在任何一轮AI识别为罕见类别的例子。</sample>
    <sample id="1520">我们将其与社区中常用的其他主流的、前沿的主动学习策略进行了比较。</sample>
    <sample id="1521">我们发现所提出的PRC策略比其他纯静态的现有策略效果更好，尽管差异很小。请注意，对于随机策略，性能明显较低。</sample>
    <sample id="1522">在进一步的主动学习轮次中，我们使用了两个最佳策略，进一步提高了糖尿病分类AUC到0.75，这是我们在该任务上取得的最佳表现。</sample>
    <sample id="1523">我们还检查了每种策略的可操作性，包括标注质量以及对标注员的成本。我们发现，PRC具有最高的不平衡度，并且最适合罕见类别。然而，标注员也发现这些示例很难理解。</sample>
    <sample id="1524">总的来说，我们发现PRC是一种简单的AI策略，用于罕见类别获取和冷启动AI，并通过设计适当的迁移学习任务显著帮助。</sample>
    <sample id="1525">我们还发现迭代更新对于从不同领域进行迁移学习很有用，而域内主动注释则受益于累积更新。</sample>
    <sample id="1526">这些是我们的代码、数据集和论文的链接。如果您有任何问题，请随时与我们联系。谢谢</sample>
    <sample id="1527">根据幻灯片底部列出的标志，论文的作者来自多个机构。这些包括信息学、NLP（自然语言处理）、Saarland大学和阿姆斯特丹大学。这些标志的存在表明论文是一个跨机构合作项目，涉及计算机科学、信息学和自然语言处理领域的研究。</sample>
    <sample id="1528">演讲者的名字是Si Yu Yan。</sample>
    <sample id="1529">根据幻灯片显示，这篇论文有五位作者：Patrick Fernandes、Kayo Yin、Emmy Liu、Andrea F. T. Martins 和 Graham Neubig。</sample>
    <sample id="1530">该方法与专门用于 simulST 翻译的 State-of-the-Art 架构进行了比较。</sample>
  </task>
</testset>