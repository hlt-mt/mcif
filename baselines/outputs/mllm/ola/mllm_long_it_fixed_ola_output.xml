<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su largi quantità di dati web, tra cui notizie politiche.</sample>
    <sample id="1">Maggell University, Meila e Microsoft Research.</sample>
    <sample id="2">The paper presents a new multi-modal pre-training model called LayoutMask to address the reading order issues in visually rich document understanding. LayoutMask uses text and layout information as input and aims to enhance text layout interactions and layout representation learning during pre-training. It differs from previous studies in three aspects: choice of word positioning, masking strategy, and pre-training objectives. LayoutMask proposes to use the in-segment token orders as word positioning, which is referred to as local word positioning. As local word positioning does not provide cross-segment orders, LayoutMask infers global reading order by jointly using 1D positioning, 2D positioning, and semantic information. The paper also equips the commonly used pre-training objective Masked Language Modeling with two novel masking strategies: whole word masking and layout-aware masking. The authors design a new pre-training objective called Masked Position Modeling, which has the same pre-training objective and recovers relative 2D positions during pre-training. The authors compare the performance of LayoutMask with different layout informations for word positioning, local word positioning, and global word positioning on both FD and SRE datasets. The results show that LayoutMask achieves better performance than other methods, especially on SRE dataset.</sample>
    <sample id="3">Ciao, benvenuti nella nostra presentazione di DePlain, un nuovo corpus per la semplificazione del testo in tedesco al livello dei documenti e al livello delle frasi. Il mio nome è Regina Strohn e mi occuperò della prima parte della presentazione. Innanzitutto, definiamo la semplificazione del testo. La semplificazione del testo è il processo di adattare un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come le persone con problemi di lettura o i non nativi di lingua. Per addestrare un modello di semplificazione del testo, ci sono necessari coppie parallele di testi, ad esempio due documenti o due frasi. In questo esempio, si vede una coppia allineata di frasi: una frase complessa in tedesco e la sua traduzione in un linguaggio più semplice. Per semplificare la frase, diverse tecniche sono possibili, come la sostituzione lessicale, la semplificazione grammaticale, la semplificazione reorganizzativa o l'inserzione di parole. Ora proponiamo il nostro nuovo corpus DePlain. In recenti anni, ci sono stati problemi con i corpi esistenti. Ad esempio, questi corpi qui sono troppo piccoli per addestrare un modello di semplificazione del testo. I altri tre modelli proposti in recenti anni sono tutti automaticamente allineati, cosa che significa che possono essere sbagliati nella loro allineazione. Quindi, proponiamo il nuovo corpus DePlain, che è diviso in due sottocorpi: DePlain API e DePlain Web. DePlain API è basato su testi nuovi. In DePlain API, 483 documenti sono stati allineati manualmente, il che ha prodotto circa 30.000-13.000 coppie di frasi allineate. Per DePlain Web, il corpus include diverse domande e tutti questi 750 documenti sono stati allineati manualmente e automaticamente. In totale, si sono ottenute 30.450 coppie di frasi. Analizzammo queste coppie di frasi in modo un po' più dettagliato. Ad esempio, sul tipo di semplificazione, si può vedere che i testi della Bibbia sono molto più semplificati rispetto ai testi nuovi o ai testi per apprendimento di lingua. Al livello di tipo di semplificazione, si possono vedere che i testi della Bibbia sono molto più semplificati rispetto ai testi nuovi o ai testi per apprendimento di lingua. Inoltre, si può vedere che il corpus DePlain ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, nel corpus DePlain API, ci sono molti più riordinamenti e modifiche di parole rispetto al corpus DePlain Web. D'altra parte, nel corpus DePlain Web, ci sono molti più riformulazioni. Allora, vediamo ora cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parleremo dei casi d'uso per il nostro dataset DePlain. Per il primo caso d'uso, possiamo valutare metodi di allineamento automatico. In recenti anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni automatiche, dove abbiamo due documenti scritti in due lingue diverse e vogliamo estrarre allineamenti di frasi in posti documenti. Ma in questo caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli che hanno lo stesso linguaggio e lo stesso contenuto, ma sono a livelli di difficoltà diversi. E ora che abbiamo il dataset DePlain, che ha allineamenti manuali, possiamo usare queste frasi come standard di allineamento per valutare i metodi di allineamento proposti. Abbiamo fatto delle modifiche ai metodi proposti e pubblicato tutti i codici per eseguire i nostri esperimenti in un articolo. Alla fine, concludevamo che il miglior metodo di allineamento automatico per testi di semplificazione in tedesco è il metodo di mass allineamento. Puoi anche trovare il codice per eseguire questo metodo su documenti personali nel paper. Il secondo caso d'uso che ho illustrato in un articolo è il caso di semplificazione automatica. Abbiamo finetuned due modelli: un modello per produrre semplificazioni di documenti e un modello per produrre semplificazioni di frasi. Puoi anche trovare i checkpoint e i dettagli dei punteggini e dei metri di valutazione dei nostri esperimenti in un articolo. Concluso che questa basic fine-tuning può produrre o ottenere punteggi migliori dei punteggi di base e propone i risultati come una marcatura di benchmark per il problema di semplificazione automatica in futuro. Grazie mille per la vostra attenzione e spero di incontrarvi tutti durante il convegno. Grazie.</sample>
    <sample id="4">La relatrice o il relatore è Koyo Yin.</sample>
    <sample id="5">Il modello che hanno utilizzato per ottenere l'accuratezza dell'82%-87% è il T5-XLARGE.</sample>
    <sample id="6">This paper presents a unified approach to multilingual and cross-lingual summarization, introducing a method called "many-to-many summarization." The authors propose a single model that can generate summaries in any target language from a document in any source language. They conduct preliminary studies to analyze the performance of their method compared to traditional multilingual and cross-lingual summarization techniques. The results show that many-to-many summarization effectively transfers task knowledge across different languages, outperforming previous models like MBS and MBS. Additionally, the authors introduce a three-stage training process for their model, which includes pre-training, cross-lingual training, and test-specific training. This approach achieves state-of-the-art results on the WikiLingua dataset, demonstrating the effectiveness of their method in handling multiple languages.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="8">Il nuovo metodo di valutazione umana proposto è quello di chiedere ai giudici umani di valutare diversi aspetti della qualità del dialogo, invece di chiedere loro di scegliere tra due conversazioni o di dare un punteggio su una scala.</sample>
    <sample id="9">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla selezione di modelli.</sample>
    <sample id="10">Per migliorare il punteggio, i progressi che possono essere fatti includono l'accesso a informazioni di background più complete, l'accesso a informazioni di Wikipedia e l'accesso solo ai nomi degli enti.</sample>
    <sample id="11">Researchers at AI2, in collaboration with universities and organizations, have developed a dataset to evaluate the humor understanding capabilities of large language models. They used the New Yorker Caption Contest data, which involves submitting captions for cartoons, to create tasks such as matching, quality ranking, and explanation generation. The dataset includes annotations like locations, descriptions, and joke explanations. While models like GPT-4 show some progress, they still lag behind human performance in these tasks, highlighting the challenges in teaching AI to understand humor.</sample>
    <sample id="12">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="13">The presentation introduces the concept of adaptive inference, a method for reducing inference time in large language models by using low-capacity models for easy samples. Two common methods are multi-modal and early exit. Multi-modal involves storing multiple models trained separately on the entire training set, while early exit fits multiple classifiers to the model following intermediate transformer layers. The pros and cons of each method are discussed, including versatility, cost, overhead, and performance issues due to conflicting gradients. The researchers hypothesize that conflicting gradients lead to lower performance in early exit models. They compare individual early exit models with separate multi-modal classifiers and present the Sweet method, which separates weights in early exit transformers to avoid conflicting gradients. The results show that Sweet closes the gap between early exit and multi-modal but negatively affects later classifiers in some cases. The study concludes that adaptive inference methods can improve performance and speed, and suggests future research on fine-tuning algorithms tailored to early exit architectures.</sample>
    <sample id="14">Il mio nome è Adam Sipruckowsky e questo talk parla della struttura dipendente della coordinazione. Come potete vedere, ci sono diverse strutture dipendenti assunti da diverse teorie e approcci concorsi. Ad esempio, in una teoria universale delle dipendenze, la struttura di coordinazione Lesa Bart e Meggie è tale che il primo congiunto è la testa della struttura di coordinazione, ovvero Lesa. Un approccio simile è assunto in Igore Miltruk's theory of meaning text, dove di nuovo la struttura di coordinazione è guidata dal primo congiunto. Questi due approcci sono simmetrici, singolano un dei congiunti. Ci sono anche approcci simmetrici alla coordinazione, come il prague approach, che considera la coordinazione guidata dalla congiunzione, ovvero le strutture di coordinazione sono guidate dalla congiunzione. Così otteniamo dipendenze da un verso a tutti i congiunti. Infine c'è anche un approccio multi-guidato che viene usato ad esempio in De Cattson's word grammar, dove tutti i congiunti sono guidati dalla struttura di coordinazione. Così otteniamo dipendenze dal governante, qui Lesa, a tutti i congiunti separatamente, Lesa Bart e Meggie. Ora, lo scopo del mio articolo è quello di produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste due. Ok, l'argomento è basato sul principio di minimizzazione delle dipendenze, che spiegheremo sulla base di questi esempi. In inglese, come potete vedere, i soggetti diretti preferiscono di essere vicini al verbo, mentre gli avjuncts possono essere più lontani. Così, March read it yesterday è corretto, perché il soggetto direttore "it" è vicino al verbo, mentre March read yesterday it è peggio, perché tra il verbo e il soggetto direttore c'è un avjunct "yesterday". Tuttavia, quest'effetto può essere attenuato quando il soggetto direttore è pesante e lungo, poiché allora può essere spostato dopo l'avjunct. Questo è illustrato qui. Così entrambe queste frasi sono corrette: March read this absolutely fascinating book about the BC yesterday, ok, in un caso invece di "it" ho questa lunga NP ma è anche corretto dire: March read yesterday this absolutely fascinating book about the BC. Quindi la ragione qui è che è possibile, anche se questa frase viola il principio grammatico che i soggetti diretti dovrebbero essere vicini al verbo, essa soddisfa il principio di minimizzazione delle dipendenze, che dice che le dipendenze più brevi sono preferite. Quindi queste due strutture solo mostriamo la lunghezza delle dipendenze cruciali, ovvero le dipendenze che non sono costanti tra queste due strutture. Quindi qui abbiamo la dipendenza da "read" all'avjunct di lunghezza 7 misurata in parole e dalla "red" al "book" di lunghezza 4. Quindi insieme è 11. Quando si sposta o si scambia queste due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6, molto più breve, è per questo che suona piuttosto corretto. Violenta un principio, ma soddisfa un altro. Ok, quindi cosa facciamo? Estraiamo statistiche rilevanti da coordinate in una versione migliorata del Pankhrukhin e vedere il paper per vedere se non usare universal dependencies. E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono a essere più corti. Così salt and pepper and not pepper and salt misurato in sillabare. E anche l'osservazione che è stata fatta in passato che questa tendenza cresce con la lunghezze differenza. Quindi quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto sinistro preferisce essere il primo congiunto più forte. Quindi la proporcione è più grande dei congiunti sinistri corti. Ma cosa nuovo in questo paper è che abbiamo osservato che questa tendenza solo si manifesta quando il governante sinistra è assente. Quindi il governante sinistra in questo esempio è Lesa Bart e Lesa. Quindi il governante sinistra è sul sinistra. Esiste in questo esempio Homo came and sneezed. Qui abbiamo coordinazione di due verbi e non c'è un esterno esterno governante. Quindi in queste case il congiunto sinistro preferisce essere più corto, tanto più il maggior differenza tra i due congiunti. Tuttavia, quando il governante sinistra è sul destro, come qui, Lesa governa la coordinazione ten and net, questo effetto scompare. Quindi dimostriamo che, misurando la lunghezze in caratteri, ovvero la prima colonna in sillabare, la seconda colonna in parole, la terza colonna, mi concentro sulla terza colonna. Quello che vediamo qui è che quando il governante sinistra è sul sinistra, la tendenza per il congiunto sinistro di essere più corto cresce gradualmente con la differenza assoluta in parole. E lo stesso è osservato quando non c'è governante, come in coordinazione di frasi. Ma quando il governante sinistra è sul destro, questa tendenza scompare. E dimostriamo nel paper come questo fornisce un argomento contro strutture di coordinazione asimmetriche come queste due e favore strutture simmetriche come queste due. Quindi vedere il paper per il full agreement e argomenti, scusate, e parlare con noi in sessione poster. Grazie mille.</sample>
    <sample id="15">Il numero di autori coinvolti nell'articolo è 3.</sample>
    <sample id="16">I domini di testo più semplificati sono i testi biblici.</sample>
    <sample id="17">The proposed method for multi-modal relation extraction involves representing text and images with corresponding visual and textual graphs, merging them into a unified cross-modal graph (CMG), and screening the initial CMG structures by fine-grainedly filtering nodes and adjusting edges. Graph information is leveraged to guide optimization, and multimodal topic features are used to integrate the multimodal topic words and enrich the overall context. The effectiveness of the proposed method is evaluated on the widely used MRQA dataset, and it achieves significant improvements over existing models on the benchmark.</sample>
    <sample id="18">Un esempio di preferenza per i congiunti a sinistra più brevi è "salt and pepper" rispetto a "pepper and salt".</sample>
    <sample id="19">The presentation discusses the challenges of open-domain question answering and proposes a two-stage framework to address them. The first stage uses a retriever to retrieve relevant documents from Wikipedia, while the second stage uses a reader to understand the question and retrieve the answer. The presentation also introduces some efficient techniques to reduce memory cost, improve inference speed, and achieve comparable performance. The presenter compares existing models and concludes that if one is limited by resources, they can consider reducing index size or model size. If real-time feedback is pursued, retrieval-only systems are recommended. Two future works are discussed: deploying open-domain question answering systems in low-power devices and considering more evaluation metrics.</sample>
    <sample id="20">Sì, i modelli sono freely available e i training scripts sono su GitHub repository.</sample>
    <sample id="21">DEplain-apa include testi recenti.</sample>
    <sample id="22">Per una buona generalizzazione, i fattori principali sono l'architettura del modello, la dimensione del modello e l'uso di più esempi di finetuning.</sample>
    <sample id="23">The paper discusses the challenges faced by text-image models in rendering visual text accurately. It highlights the limitations of T5 and PaLM models in spelling words correctly, especially for smaller scales. The paper introduces a new model, BitT5, which uses character-level information to improve text rendering. By concatenating BitT5's small model with the existing text representation, the image generation characteristics and text rendering ability of the image model are significantly improved. The paper also proposes the DrawText benchmark for text-to-image models and introduces a new strategy for improving model spelling ability by incorporating character-aware models.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistiche estratte da una versione migliorata del Corpus di Parole e confrontandole con le osservazioni precedenti.</sample>
    <sample id="25">Gli esperimenti sono stati progettati per estrarre statistiche da una versione migliorata del Corpus di Parole e vedere se le statistiche confermano l'osservazione precedente che i complementi sinistra tendono a essere più corti.</sample>
    <sample id="26">Un classificatore base addestrato su un insieme di dati non bilanciato non è molto più efficiente rispetto a un'ipotesi di chance.</sample>
    <sample id="27">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="28">Bob e Alice</sample>
    <sample id="29">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="30">The paper introduces LLM-Blender, a framework for ensemble learning of large language models (LLMs) that uses pairwise ranking and generative fusion. The framework is designed to address the variability in model performance across different input examples. It consists of two stages: first, it ranks candidate models using a pairwise ranking module called PairRanker, which compares each pair of candidates against the input example. Then, it selects the top k models based on the ranking results and uses them as inputs to a generative fusion model to produce the final output. The PairRanker module encodes pairs of candidates along with the input example to better analyze subtle differences between them. Experiments show that LLM-Blender outperforms individual models like OpenAssistant and Vicuna on various metrics, demonstrating its effectiveness in improving the overall performance of LLM ensembles.</sample>
    <sample id="31">I ricercatori che hanno scritto l'articolo sono affiliati a Johns Hopkins University, Purdue University e MIT.</sample>
    <sample id="33">Il framework quantifica esattamente la posizionalità confrontando le annotazioni di diversi utenti con le previsioni e le etichette dei modelli e dei set di dati.</sample>
    <sample id="34">The CREST framework is a joint approach for generating counterfactual examples and rationales. It uses selective rationalization to highlight input tokens in a faithful way, and then generates counterfactuals by editing specific parts of the input. The framework combines these methods to leverage their complementary strengths. The results show that CREST produces valid, fluent, and diverse counterfactual examples, and that it can improve downstream models when used for data augmentation.</sample>
    <sample id="36">This paper presents a method for improving multilingual machine translation by using language-specific layers (LSLs) in a transformer model. The approach involves training one regular transformer layer per language, which can be either the source or target language, to select the correct sub-layer at inference time. This allows for constant inference costs while increasing capacity per language. The authors focus on placing LSLs in the encoder and use a method to learn the best placement based on weight importance. They evaluate their approach on the WMT 21 newstest task sources for 10 languages, including low-resource languages like Swahili, and report significant improvements over baseline models and language adapters.</sample>
    <sample id="37">Il risultato dello studio precedente è stato che i soggetti umani hanno anche rivelato stereotipi razziali quando hanno ricevuto gli stessi prompt di persona.</sample>
    <sample id="38">In questo studio, i ricercatori hanno utilizzato statistiche estratte da una versione migliorata del Corpus di Pencils per analizzare la struttura delle coordinate.</sample>
    <sample id="39">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di discordanze indipendenti e la classificazione bimane di espansione e comparazione.</sample>
    <sample id="41">The presentation introduces the PEACoK project, which aims to develop a Personal Common Sense Knowledge graph for consistent and engaging narratives. Collaborated with Sony Group Corporation, the project focuses on understanding how persons of speakers, listeners, or characters ground the narrative. The PEACoK graph contains 3,800 persons and 40,000 distinctive attributes, forming about 100,000 person inferences or facts. It also includes about 9,200 attributes connected to two or more persons, contributing to rich interconnections. The relations of persons and their attributes are framed in three dimensions, including four types of main relations, as well as interactivity and distinctiveness. The project was built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-validation of PEACoK relations using a joint human-AI majority voting scheme. Expert studies show that this approach yields high-quality relation annotations with an average accuracy of 87% and F1 score. The PEACoK graph enables lightweight language models to learn knowledge generation capabilities comparable to large-scale language models.</sample>
    <sample id="42">Ci sono due autori che hanno collaborato all'articolo.</sample>
    <sample id="43">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="44">Il framework differisce dai precedenti studi in quanto confronta le annotazioni reali con i modelli e i set di dati esistenti, invece di concentrarsi solo sull'annotatore agreement o le distribuzioni degli annotatori.</sample>
    <sample id="45">La configurazione che si sovrappone maggiormente al lessico degli stereotipi è la diuturna.</sample>
    <sample id="46">Google Translate e Deep Belief</sample>
    <sample id="47">Ciao, mi chiamo Changbing e sono uno studente di PhD presso l'Università di Washington. Oggi presento il mio lavoro sulla transizione dai dati di addestramento alla costruzione di modelli linguistici e sulle applicazioni downstream, analizzando i percorsi dei bias politici che portano a modelli NLP non equi. I modelli linguistici vengono addestrati su grandi quantità di dati web, tra cui notizie politiche, che sono bene rappresentate nei loro dataset di addestramento. Secondo un saggio del C4F Corpus, i principali media politici come il New York Times, Los Angeles Times, The Guardian, Huffington Post, ecc., sono ben rappresentati nel dataset di addestramento dei modelli linguistici. Questo ha creato un "beneficio misto" per le applicazioni dei modelli linguistici: da un lato, hanno imparato da diverse prospettive che celebrano la democrazia e la pluralità delle idee; dall'altro lato, queste diverse opinioni politiche sono inerentemente socialmente biased e possono portare a problemi di giustizia in applicazioni downstream. Per risolvere questo problema, abbiamo proposto di indagare il percorso dei bias politici dalla data di addestramento ai modelli linguistici fino alle applicazioni downstream, specificamente chiedendo: 1) come valutiamo la linearità politica dei modelli linguistici e in che modo i dataset di addestramento influenzano tale linearità politica? 2) come i modelli linguistici con diverse linearità politiche performano sulle applicazioni downstream e se queste performance potrebbero portare a problemi di giustizia in applicazioni NLP? Per rispondere a queste domande, abbiamo proposto di stimare i modelli linguistici con diverse formule di prompt utilizzando domande politiche come il test di compasso politico. Questo ci ha permesso di effettuare una valutazione automatica basata su politica, sostenuta dalla letteratura di scienze politiche. I risultati preliminari dimostrano che i modelli linguistici hanno linearità politica variante, occupando tutti i quadranti di un compasso politico. Inoltre, GPT-4 è il modello più liberale tra tutti, mentre GPT-3 series è generalmente più socialmente liberale rispetto a BERT series e le sue varianti. Abbiamo anche proposto di indagare in che misura i bias politici dei modelli linguistici sono stati presi iniziato dal dataset di addestramento. Abbiamo condotto un esperimento controllato pre-addestrando modelli linguistici su sei diversi corpus partizani separati in notizie e social media, ulteriormente divisi in base alla linearità politica. Abbiamo visto che i coordinate ideologici dei modelli linguistiche si spostano corrispondentemente. Ad esempio, dopo un ulteriore finetuning su un corpus di notizie di sinistra, Roberta ha registrato un svolgimento libera in termini di linearità politica. Abbiamo anche cercato di indagare se i modelli linguistici possono imparare la polarizzazione prevalentemente nella nostra società. Abbiamo diviso i corpus partizani in due periodi precedenti e successivi al 45° Presidente degli Stati Uniti. Abbiamo visto che i modelli linguistiche hanno generalmente una linearità politica che si allontina dal centro dopo il 2017, il che indica che i modelli linguistiche possono anche imparare la polarizzazione nella nostra società. Infine, abbiamo valutato i modelli linguistiche con diverse linearità politiche sul rilevamento di discorso razzista e falso, due applicazioni NLP che spesso coinvolgono modelli linguistici e hanno implicazioni significative. Abbiamo visto che se analizziamo il rendimento per category, ovvero se separiamo il rendimento in diverse demografiche o linearità politica dei media notiziari, possiamo vedere un模式，ad esempio, per il rilevamento di discorso razzista, i modelli linguistiche di sinistra sono migliori nel rilevare il discorso razzista che bersella gruppi minoritari, tuttavia sono peggiori nel rilevare il discorso razzista che bersella gruppi più potenti nella nostra società. Lo stesso accade per il rilevamento di notizie false, dove i modelli linguistiche di sinistra sono migliori nel rilevare la menzogna proveniente da parte dei media notiziari di destra, e viceversa. Abbiamo anche fornito molti esempi qualitativi per vedere che i modelli linguistiche con diverse linearità politiche danno predizioni diverse per esempi di discorso razzista e menzogna basate sulla propria storia personale. Ci sono molte altre esempi in appendix per ulteriormente evidenziare che c'è un problema di giustizia legato alla linearità politica dei modelli linguistiche. Ad esempio, se i modelli linguistiche di destra fossero addestrati per rilevare il discorso razzista o la menzogna e fossero distribuiti su piattaforme sociali popolari, potrebbe significare che le persone con opinioni politiche opposte potrebbero essere marginalizzate e il discorso razzista bersello ai gruppi minoritari potrebbe continuare senza alcun controllo. Questo ha suscitato l'allarme per noi di riconoscere e affrontare i problemi di giustizia derivanti dalla linearità politica dei modelli linguistiche. Un po' di discussione, vorremmo anche sottolineare che esiste un dilemma unico riguardo la linearità politica dei modelli linguistiche: se non sanizziamo le opinioni politiche dei dataset di addestramento, i bias potrebbero propagarsi dalla data di addestramento ai modelli linguistici fino alle applicazioni downstream, creando problemi di giustizia. Se invece proviamo a sanizzarle, rischiamo di censurare o escludere, e è incredibilmente difficile determinare cosa sia realmente neutra e dovrebbe essere retain in dataset di addestramento dei modelli linguistici. Sembra quasi come il problema dell'elettricità elettrica. Ok, bello, penso che sia tutto per oggi, grazie per il tuo tempo.</sample>
    <sample id="48">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 124 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces DePlain, a new corpus for German text simplification at both the document and sentence levels. It explains that text simplification aims to enhance text comprehension for specific groups, such as people with reading difficulties or non-native speakers. The presentation highlights the challenges of existing corpora, including small size and errors in alignment, and introduces DePlain as a solution. DePlain is divided into two sub-corpora: DePlain API, which contains 483 manually aligned documents from news texts, and DePlain Web, which includes 750 documents from various domains, both aligned manually and automatically. The corpus offers a variety of simplification transformations, making it suitable for evaluating and improving automatic alignment methods and text simplification models.</sample>
    <sample id="51">Il loro set di dati include music, books e recipes.</sample>
    <sample id="52">La posizionalità è semplicemente la prospettiva che le persone hanno come risultato dei loro demografici, identità e esperienze di vita.</sample>
    <sample id="53">Il nome del relatore è Dawei Zhu.</sample>
    <sample id="54">The presentation introduces a cognitive dissonance detection system, highlighting its importance in understanding disagreement, belief changes, and mental health. It describes the process of annotating discourse units for dissonance relations using transfer learning and active learning to improve detection accuracy. The system uses a cumulative update strategy and a probability of rare class (PRC) strategy to select examples likely to be dissonant, achieving the best performance so far with an AUC of 0.75.</sample>
    <sample id="55">Sì, il modello ST offline esistente viene adattato utilizzando solo un modello per ogni regime di latenza e gestendo la latenza tramite parametri specifici.</sample>
    <sample id="56">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) il contesto di testa, dove la conoscenza di sfondo è assunta di essere disponibile in tempo di addestramento; 2) il contesto di testo-bis, dove la conoscenza di sfondo è disponibile sia in tempo di addestramento che in tempo di inferenza; e 3) il contesto di testo-inferenza, dove entrambe le tipologie di informazione sono disponibili solo in tempo di inferenza.</sample>
    <sample id="59">The presentation introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. It compares DrBERT with other models trained on different data sources and sizes, evaluating their performance on 11 biomedical and clinical downstream tasks in French. The results show that from-scratch pre-training yields higher performance on most tasks, while continual pre-training using the same model weights and tokenizer as DrBERT shows comparable results to a 4GB model from scratch. The proposed system outperforms nine of the eleven downstream tasks and surpasses the result of the generative model Camembert.</sample>
    <sample id="60">I fornitori dell'articolo sono Google Research.</sample>
    <sample id="61">La terza domanda di ricerca proposta è se dobbiamo usare solo i campioni puliti per la valutazione o se ci sono altre migliori modalità per utilizzarli.</sample>
    <sample id="62">The paper presents a systematic study of knowledge distillation for natural language generation (NLG) using pseudo-target training. It explores the potential of compressing large language models while preserving their performance. The study focuses on task-specific NLG tasks in realistic setups, considering criteria such as resource-labeled datasets, unlabeled data, and efficiency. The authors compare different architectures, pruning techniques, and knowledge distillation approaches, including word-level and sequence-level methods. They also propose a novel joint teaching technique to address student exposure bias and improve learning. The paper provides insights into the effectiveness of these methods and offers a recipe for NLG distillation.</sample>
    <sample id="63">La metrica della sensibilità misura la capacità del modello di produrre gli stessi output per le stesse attività, indipendentemente dalla variazione nella formulazione dell'instruzione.</sample>
    <sample id="64">La relatrice o il relatore è Jing Wei.</sample>
    <sample id="65">Una maggiore sensibilità indica che il modello è più sensibile alle variazioni nella redazione dell'instruzione, mentre una sensibilità inferiore indica che il modello è più robusto e produce gli stessi output anche con variazioni nell'instruzione.</sample>
    <sample id="66">This survey paper presents a comprehensive overview of deep learning methods for mathematical reasoning. It highlights the importance of mathematical reasoning in human intelligence and its applications in AI and NLP. The paper discusses various types of mathematical reasoning tasks, such as solving math word problems, visual contexts, and table contexts. It also covers automated theorem proving and the development of neural network architectures for these tasks. The paper emphasizes the challenges faced by current models, including generalization and robustness issues, and suggests potential solutions like self-consistency and tool augmentation.</sample>
    <sample id="67">The paper discusses interference in multilingual translation models and proposes a method to mitigate it. The authors find that severe interference occurs when the model is small compared to the data size, and tuning the sampling temperature is key for strong performance. They also find that language similarity and the number of languages do not have a large impact on interference levels. The authors conclude that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">I modelli vengono addestrati su un contesto linguistico che include grammaticalità, accettabilità e stereotipi.</sample>
    <sample id="69">In generale, si possono ottenere prestazioni elevate con soltanto 20 campioni per classe.</sample>
    <sample id="70">I ricercatori che hanno scritto l'articolo sono Myra Cheng, Esin Durham e Dan Jurafsky.</sample>
    <sample id="71">The AltEntities Corpus is a dataset designed to address the challenge of resolving indirect referring expressions for entity selection in conversational systems. It includes three domains: music, books, and recipes, with 6,000 alternative questions and 42,000 indirect referring expressions. The dataset uses a cartoon completion setup where annotators are shown background knowledge about entities and are asked to select one using indirect referring expressions. The results show that models with access to exact same background knowledge achieve high accuracy (92-95%), while those with partially overlapping or only entity names have lower accuracies (82-87% and 60%, respectively). The dataset is available for download at http://altentities.corpus.gatech.edu.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione per rilevare le differenze politiche che possono influenzare il modello di linguaggio e le applicazioni downstream. Questo è importante per garantire la giustizia e la non discriminazione in queste applicazioni.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Makkshatta.</sample>
    <sample id="74">The paper introduces Dense-Atomic, a method for constructing a dense knowledge graph by completing missing links in the Atomic knowledge base. The authors propose a relation prediction model that utilizes semantic information and a multi-class classification strategy to improve the performance of relation prediction. They also compare their method with state-of-the-art methods on both automatic and human evaluations, demonstrating the effectiveness of Dense-Atomic in improving knowledge coverage and generating more diverse results.</sample>
    <sample id="75">The presentation introduces a joint semi-supervised learning framework for entity and relation extraction tasks. The framework utilizes heterogeneous graphs to propagate labels across the graph, considering interconnections between labeled and unlabeled data. It consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. The framework is tested on four datasets, showing significant improvements over baseline models in single-task datasets and benefiting from codependency in joint datasets.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici include la raccolta e l'elaborazione dei dati, la creazione di modelli linguistici e la distribuzione di questi modelli in applicazioni downstream.</sample>
    <sample id="77">The video presents a study on improving summarization factual consistency using natural language feedback. The research is a collaboration between Yale University and Microsoft Research, with most of the work done when the first author was an intern at Microsoft Research. The study introduces a new dataset called Defacto, which includes human demonstrations and feedback for enhancing summarization factual consistency. The researchers provide a comprehensive analysis of the dataset and offer insights into the factual consistency of summarization models. They propose three new NLP tasks: summary editing, feedback generation, and automatic factual error correction. The study focuses on abstractive text summarization and specifically examines the factual consistency of summarization models. Human annotators were asked to label summaries as factually consistent or not, provide human-corrected factually consistent summaries, and offer explanations, instructions, and evidence. The data collected from the XSum dataset shows that 70% of the initial system outputs contain factual errors, but human-edited summaries receive higher automatic factual scores with lower textual overlap with reference summaries.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="79">Sì, Coscript è disponibile pubblicamente.</sample>
    <sample id="80">La filigrana viene inserita esattamente quando il numero di triggere in una frase è maggiore di M.</sample>
    <sample id="81">I fornitori dell'articolo sono Justin John e il Peking University.</sample>
    <sample id="82">This video presents a study on unsupervised automated essay scoring (AES) using multiple heuristics. The goal of AES is to score writing quality without human intervention, an important application in education. State-of-the-art AES models are typically trained with large labeled corpora, but collecting such data is time-consuming and labor-intensive. Unsupervised AES can overcome this by not requiring ground truth scores for training. Two main approaches are discussed: one uses the number of unique terms as an initial score, while the other uses word count as weak supervision. Both methods have poor performance due to the limitations of single heuristics. The study proposes a new framework called URRA, which introduces multiple heuristics as pseudo ground truth scores and trains a neural AES model through rank aggregation. Experiments show that URRA outperforms unsupervised baselines and achieves competitive performance when compared to supervised methods.</sample>
    <sample id="83">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">The paper presents a framework for dynamic neural networks (DNNs) that can adapt their architecture and parameters based on input data. It introduces a method to partition parameters into dynamic and static ones, using scale factors to control the intensity of each mode. The proposed framework achieves better performance than both static and fully dynamic networks, with fewer parameters and less computation. Experiments show that the partitioned DNNs maintain high discriminative power and outperform fully dynamic networks in terms of accuracy. Future work includes extending the framework to other types of neural networks and hardware platforms.</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è preparare un pasto specifico come un pasticcere che ha bisogno di seguire istruzioni precise per preparare un pasto particolare.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo utilizzando un set di frasi nascoste e una serie di test.</sample>
    <sample id="87">Il lavoro utilizza i PLM esistenti per costruire uno nuovo in modo che possa essere adattato a diverse lingue e domini, come il francese e il biomedico.</sample>
    <sample id="88">GPT-4 è meno allineato con le nazioni non binationi.</sample>
    <sample id="89">La relatrice illustra un esempio in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase "If we go on and we receive another speech chunk and our model predicts other three words and we will look at the cross attention weights we will see that no word points to the last lamb lamb speech frames this means that these three words will be emitted".</sample>
    <sample id="90">This paper questions the necessity of recruiting native speakers for data annotation in NLP and explores the feasibility of using language learners as annotators. The authors conducted a proof-of-concept study to examine the accuracy and learning effects of language learner annotations compared to native speakers. They recruited language learners with varying proficiency levels and provided them with additional resources to help understand annotation samples. The results showed that language learner annotations are nearly accurate, especially for simpler tasks and easy to medium-level questions. Aggregating these labels with others by majority voting can almost match the performance of native speakers. Furthermore, training simulations with learner annotations achieved about 95% of ground-truth performance and sometimes outperformed models trained with native speaker labels. This suggests a novel way for data construction by recruiting language learners as annotators, which can broaden NLP research for low-resource languages where it is difficult to recruit native speakers.</sample>
    <sample id="91">In base ai risultati presentati, la quantità di attività influisce sulla performance del modello in due modi principali. In primo luogo, aumentare il numero di attività può migliorare la performance del modello, ma allo stesso tempo ridurre la sensibilità. In secondo luogo, l'uso di una singola istruzione invece di diverse istruzioni può migliorare la performance generale del modello e ridurre la sensibilità.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con altri modelli che non usano alberi, i modelli che usano alberi e modelli che non usano sequenze a sequenze.</sample>
    <sample id="93">I due coautori sono gli advisor del primo autore.</sample>
    <sample id="94">The paper presents a method for protecting the copyright of large language models used in embedding services. It introduces a backdoor-based watermarking technique called Embedding Marker, which is applicable to embedding services and can detect whether another service contains the watermark. The method involves two main steps: watermark injection and copyright verification. Watermark injection defines a target embedding based on the number of triggers in a sentence, while copyright verification uses a backdoor dataset to detect similarities between requested embeddings and the target embedding. Experiments on four datasets show that Embedding Marker has great detection performance while maintaining utility for downstream tasks.</sample>
    <sample id="95">Il primo autore di PaLM è David Villegas.</sample>
    <sample id="96">Salve a tutti, mi chiamo Jenny e sono un allievo del primo anno di laurea in Psicologia alla Carnegie Mellon University. Oggi presenterò il mio lavoro intitolato "NL Positionality: Characterizing Design Biases of Datasets and Models". Questo lavoro è stato realizzato in collaborazione con alcuni ricercatori dell'Università di Washington e dell'Allen Institute for AI, tra cui Sebastin Santi, Ronan Le Bras, Katharina Reinecke e Martin Sap. Iniziamo con un'immagine che rappresenta la nostra team. Questo è un esempio di bias di progettazione, dove si notano differenze sistematiche nel performances dei modelli tra le popolazioni. Questi bias potrebbero essere dovuti alla positionality dei ricercatori e dei sviluppatori di modelli. La positionality è semplicemente la prospettiva che le persone hanno come risultato delle loro demografiche, identità e esperienze di vita. Questo concetto è spesso utilizzato in studi critici, soprattutto in ambienti accademici femministi e queer. Come ricercatore, la positionality può influenzare il processo di ricerca e i suoi risultati, poiché cambia le decisioni che i ricercatori prendono. Una domanda che le persone potrebbero fare è se i dataset e i modelli hanno positionality. Non stiamo cercando di dire che i modelli e i dataset hanno identità demografiche e esperienze di vita, ma che aggregano giudizi e opinioni di persone reali e possono rappresentare certe positionality su altre. Alcune prove anedotte suggeriscono che i dataset e i modelli hanno positionality, come lacune culturali in modelli e dataset, e definizioni teoriche di modello positionality. Tuttavia, queste opere non analizzano gli utenti rispetto ai dataset e ai modelli. Studiare la positionality dei dataset e dei modelli è sempre più importante poiché i compiti di NLP diventano più soggettivi e socialmente orientati. E' difficile caratterizzare come queste positionality siano incluse, poiché non tutte le decisioni sono documentate e molti modelli sono nascosti dietro API. Per studiare la positionality dei dataset e dei modelli, confrontiamo le annotazioni con utenti reali con i dataset esistenti e i modelli. Lo facciamo tramite un framework intitolato NL Positionality. Il nostro framework funziona in due passi principali. Il primo passo è rerelezionare i dataset con annotatori diversi. Ci optiamo per questa strategia invece di guardare le demografie dei dataset originali, poiché di solito solo pochi annotatori annotano ogni istanza e le demografie non vengono raramente raccolte e condivise. Ci optiamo per rerelezionare i dati per ottenere molte annotazioni per istanza e per ottenere un insieme ricco di demografici. Poi prendiamo le annotazioni per demografici e le confrontiamo con i modelli e i dataset utilizzando una correlazione di Pearson. Così, il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori, che si concentra sul confronto tra annotatori e modelli/predizioni etichette, invece di guardare solo la concordanza degli annotatori o le distribuzioni degli annotatori. Il nostro framework è largamente facilitato da Lab in the Wild, una piattaforma di crowdsourcing online. Lab in the Wild è una piattaforma di esperimentazione online che ci consente di reclutare volontari diversi rispetto alle piattaforme come MTurk, che hanno prevalentemente partecipanti statunitensi o indonesi. Inoltre, Lab in the Wild è in grado di ottenere alta qualità dei dati. Abbiamo creato due task su Lab in the Wild: uno è il task di accettabilità sociale e l'altro è il task di rilevamento di discriminazione e di odio. In questo task, i partecipanti leggono una situazione dal dataset di chimica societaria e scrivono come socialmente accettabile è quella situazione. Dopo aver terminato, possono confrontare le risposte con un AI e con altri. Abbiamo quindi confrontato queste annotazioni con il dataset GPD4 e il GPD4 per l'analisi accettabilità sociale. Abbiamo replicato un setup simile per il task di rilevamento di discriminazione e di odio, in cui i partecipanti leggono un'istanza da Dina Hate e scrivono se pensano che sia un'istanza di discriminazione o di odio. Abbiamo confrontato queste annotazioni con Dina Hate, Perspective API, Rewire API, Hate Roberta e GPD4. Il nostro studio ha raccolto oltre 16.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi. Ora siamo meglio equipaggiati per rispondere a chi alignano i dataset e i modelli di NLP con i più. Abbiamo scoperto che c'è positionality in NLP. Ad esempio, abbiamo scoperto che i dataset e i modelli sono maggiormente allineati con le nazioni che parlano inglese. Per il GPD4, nell'analisi accettabilità sociale, abbiamo scoperto che è maggiormente allineato con le nazioni che parlano confuciano e inglese. Abbiamo anche scoperto che Dina Hate è maggiormente allineato con le nazioni che parlano inglese. Abbiamo anche scoperto maggior alleanze con le persone che hanno un'istituzione di istruzione superiore. Per il GPD4, nell'analisi accettabilità sociale, abbiamo scoperto che è maggiormente allineato con le persone che hanno un'istituzione di istruzione superiore o un istituto di istruzione post-istituzionale. Abbiamo scoperto lo stesso per Dina Hate, in cui è maggiormente allineato con le persone che hanno un'istituzione di istruzione superiore. Tuttavia, quando i modelli e i dataset sono allineati con specifiche popolazioni, inevitabilmente alcune popolazioni rimangono indietro. Un esempio di questo è che i dataset e i modelli sono meno allineati con le persone non binarie rispetto alle coppie di genere maschile e femminile. Abbiamo scoperto questo nell'analisi accettabilità sociale del GPD4 e nell'analisi Dina Hate. Quindi, data la positionality in NLP, cosa possiamo fare? Abbiamo alcune raccomandazioni per questo. La prima è tenere un registro di tutti i relativi scelte di progettazione durante il processo di ricerca. La nostra terza raccomandazione è costruire dataset e modelli specializzati per specifici gruppi di comunità. Un esempio di questo è l'iniziativa Musa Kani. Vogliamo sottolineare che NLP inclusivo non è solo fare in modo che tutti i tecnologi funzionino per tutti. Questo conclude la nostra presentazione. Se volete imparare di più, vi prego di controllare il nostro dashboard per i risultati più aggiornati e il nostro articolo. Grazie mille!</sample>
    <sample id="97">La relatrice menziona tre problemi associati a SimulST: i modelli specifici sono spesso addestrati introducendo moduli aggiuntivi da ottimizzare, i processi di addestramento sono lunghi e complessi, e addestrare e mantenere più modelli per raggiungere diverse regimi di latenza richiede un'addestramento con un'average di 1 secondo di latenza e un altro modello con un'average di 2 secondi di latenza, ecco.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere la sanitizzazione dei contenuti, ma è difficile determinare cosa sia davvero neutro e non escluderebbe potenzialmente informazioni importanti.</sample>
    <sample id="99">Ciao, mi chiamo Siyu Yuan e sono qui per presentare il nostro lavoro, Distilling Script Knowledge from Large Language Models for Constrained Language Planning. In vita quotidiana, gli umani spesso pianificano le proprie azioni seguendo istruzioni scritte in forma di script. Lavori precedenti hanno sfruttato i modelli di lingua per pianificare obiettivi astratti di attività stereotipate, come fare un pasticcino, dimostrando che i modelli di lingua possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, l'opera precedente ha generalmente concentrato su pianificare obiettivi astratti di attività stereotipate, pianificare obiettivi con specifiche condizioni specifiche, come fare un pasticcino di cioccolato, rimane in gran parte sottostudiata. In questo articolo, definiamo il problema della pianificazione linguistica con vincoli, che impone diverse condizioni alla pianificazione degli obiettivi. Un obiettivo astratto può essere ereditato da diversi obiettivi reali specifici con vincoli multi-fattori. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fattibili rispetto ai vincoli. In questo articolo, innanzitutto valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua. Poiché non esiste un dataset di obiettivi specifici per supportare lo studio, dobbiamo prima acquisire questi obiettivi. Come illustrato nella tabella, estendiamo gli obiettivi astratti con vincoli multi-fattori per umani in una fase di acquisizione di dati utilizzando il comando GPT-3. Abbiamo selezionato 100 obiettivi specifici e valutato i script generati dai modelli di lingua. Questa tabella riporta l'accuratezza generale dei risultati. Abbiamo scoperto che tutti i modelli di lingua hanno ottenuto risultati insoddisfacenti nell'elaborazione di obiettivi specifici. Poi, abbiamo condotto un'analisi dettagliata per indagare le cause dei problemi dei modelli di lingua. Come illustrato nella figura, la completezza semantica dei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Abbiamo analizzato più in profondità le diverse tipologie di vincoli differenti in Wikihole. Il diagramma nella figura illustra che la performance del comando GPT-3 varia considerabilmente per gli obiettivi di diverse tipologie. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di lingua è di alta variabilità, portando a peggioramenti delle prestazioni. Di conseguenza, adottiamo l'idea di un filtra over-generated per migliorare la qualità dell'output. Innanzitutto, mostriamo tipi vincolari con esempi per il comando GPT-3 e otteniamo obiettivi specifici basati su obiettivi astratti. Poi, il comando GPT-3 genera script over-generated per gli obiettivi specifici. Infine, sviluppiamo un modello di filtra per selezionare i script più fedeli. Convertiamo i script e gli obiettivi in embeddings GPT-3 e calcoliamo i punteggi di similarità coseno per misurare la similarità semantica. Inoltre, selezioniamo i script che contengono chiavi vincolari. Solo i script con i punteggi di similarità maggiori degli obiettivi vincolari vengono selezionati. Con il nostro metodo, il comando GPT-3 genera script di alta qualità. Il nostro metodo migliora grandemente la pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli. Poiché i modelli di lingua sono costosi da deploys, è essenziale abilitare la capacità di pianificazione linguistica per modelli più piccoli e specializzati. Creare un dataset è un passo essenziale verso il fine. Tuttavia, gli studi precedenti non hanno abilitato la pianificazione per obiettivi specifici e la notazione manuale dei dataset è costosa. Di conseguenza, seguiamo l'idea di distillazione conoscenza simbolica per distillare dataset di pianificazione linguistica con vincoli da modelli di lingua. Applichiamo il nostro metodo per creare un dataset di pianificazione linguistica con vincoli denominato CoScript. In totale, generiamo 55.000 obiettivi specifici con script per assicurare la qualità del dataset di validazione e test. Chiediamo a worker cloud-based di trovare e revisionare i campioni incorretti. Questa figura illustra la distribuzione vincolare di CoScript. Abbiamo scoperto che CoScript ha un'approccio elevato nella generazione di obiettivi specifici. Con CoScript, possiamo addestrare modelli più piccoli e specializzati per pianificazione linguistica con vincoli. Abbiamo scoperto che T5 finitamente CoScript genera script di alta qualità, superando i maggiori modelli di lingua. Questo indica che i modelli più piccoli possono supportare i modelli più grandi quando adeguatamente addestrati su dataset adatti. In sintesi, stabiliamo il problema di pianificazione linguistica con vincoli. Valutiamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua e sviluppo un filtra over-generated per i modelli di lingua. Utilizziamo modelli di lingua per generare un dataset di script di alta qualità, CoScript, per pianificazione linguistica con vincoli. Speriamo che CoScript possa essere un risorsa utilizzabile per avanzare la ricerca sulla pianificazione linguistica. Grazie per il tuo tempo. Per vedere maggiori dettagli su CoScript, consultate il nostro articolo.</sample>
    <sample id="100">The paper presents a few-shot reranking method for multi-hop QA using language model prompting. The method combines unsupervised retrieval with few-shot language model-based reranking to efficiently retrieve and rank candidate chains. The approach uses a likelihood of the question given the chain according to a language model, and an instruction search to find optimal instructions. The results show that PromTrank outperforms fully supervised systems and is comparable to state-of-the-art multi-hop retrievers. The downstream QA performance when using PromTrank as a retriever is also very good, exhibiting strong few-shot path retrievable performance compared to fully supervised systems.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi d'area.</sample>
    <sample id="102">Le proprietà importanti di un metodo di filigrana sono che il metodo dovrebbe essere applicabile ai servizi di embedding, il segno non dovrebbe indebolire l'utile dei servizi di embedding forniti, il segno dovrebbe essere abbastanza evidente per che l'attaccante possa rimuoverlo facilmente e infine, il segno dovrebbe essere trasferibile ai servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="103">I discorsi TED in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="104">Per la riannotazione, vengono campionate diverse istanze.</sample>
    <sample id="105">La metrica di distanza utilizzata per misurare la differenza tra set di dati benigni e backdoor è la somma della differenza di coseno e la differenza di L2.</sample>
    <sample id="106">The paper presents a dataset called QUEST, which consists of over 30 million entity-seeking queries containing implicit set operations. The answer entities are verified for relevance to the query, and their associated documents are marked with attributable spans for different query constraints. The dataset poses a challenging retrievable problem since systems need to effectively search over a large document corpus to find multiple answer sets where the attribution for different query constraints can come from different parts of the document. The dataset is constructed by relying on Wikipedia category names from four domains of interest: films, books, plants, and animals. Set operations are performed over these atomic categories to get queries with set constraints. Human annotators paraphrase templatic queries, validate them for fluency and naturalness, and verify the relevance of entities in the answer set and mark evidence in the document as its attribution. The paper evaluates systems on the dataset and finds that there is a large room for improvement on retriever performance based on the recall of the complete answer set. Queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">I modelli basati su codificatori multilingue sono stati utilizzati per ottenere le migliori prestazioni su tutti i set di dati. Inoltre, i modelli di encoder-decoder hanno dimostrato essere più adatti a questa attività rispetto ai modelli di encoder-pdr.</sample>
    <sample id="108">The paper presents a revised minimal pair paradigm to evaluate language models' acceptability judgments on longer sentences. The traditional method only evaluates models on short sentences, but the authors aim to assess models' acceptability throughout the context window. They simulate longer sequences by recreating sentences from datasets with acceptable or unacceptable structures and test the models' acceptability judgments on these longer sequences. The results show that MPPE judgments are robust for arbitrary contexts, but they vary significantly when using sentences from the same dataset or unrelated domains. This suggests that language models are sensitive to latent syntactic and semantic features shared across sentences, and current evaluation methods may not fully capture abstract knowledge throughout the context window.</sample>
    <sample id="109">This paper presents a dataset of natural language instructions and their corresponding inputs and outputs, collected in a fully automatic manner without any human annotations. The dataset contains 64k examples, with an additional 240k examples if we consider instruction paraphrases. The generated examples are evaluated for correctness, creativity, and diversity. The model trained on this dataset outperforms the baseline on several benchmarks, demonstrating the ability of language models to produce creative and diverse data.</sample>
    <sample id="111">Gli autori decidono di scegliere un insieme di parole a frequenza moderata analizzando un corpus di testo generale e conteggiano le frequenze di parole.</sample>
    <sample id="112">Salve a tutti, mi chiamo Zhueng. Oggi presenterò il mio articolo "I tagger di entity named entity di CoNLL-2003 continuano a funzionare bene nel 2023?" Iniziamo. Il mio articolo esamina il problema della generalizzabilità utilizzando la comprensione di entity named entity o il compito di riconoscimento di entity named entity. Abbiamo osservato che i modelli hanno utilizzato CoNLL-2003 per sviluppare entity named entity per quasi venti anni e queste questioni naturalmente sollevano diversi problemi. Innanzitutto, possono questi modelli generalizzare a dati moderni? E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzabilità? Allo stesso tempo, se osserviamo una pessima generalizzabilità, cosa causa la svalutazione delle prestazioni di questi modelli? Per investigare questi problemi, abbiamo sviluppato il dataset CoNLL++ che abbiamo raccolto da Reuters News nel 2020 e lo abbiamo annotato con le stesse linee guida di annotazione di CoNLL-2003. Abbiamo quindi fin-tuned più di venti modelli su CoNLL-2003 e li abbiamo valutati sia sul set di test CoNLL-2003 che sul set di test CoNLL++. Infine, abbiamo calcolato il percentuale di cambiamento in F1 per valutare la generalizzabilità di ciascun modello. Cos'è necessario per una buona generalizzabilità? Nelle nostre esperienze, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari. Il primo è l'architettura del modello. Nelle nostre esperienze, abbiamo scoperto che i modelli a transformer normalmente generalizzano meglio ai nuovi dati. Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzabilità. Infine, non è nuovo che il numero di esempi di fin-tuning direttamente influisca sulle prestazioni dell'attività downstream. Qui anche abbiamo scoperto che più esempi di fin-tuning realmente portano a una migliore generalizzabilità. Al nostro prossimo quesito, cosa causa la svalutazione delle prestazioni di alcuni modelli? Abbiamo due ipotesi. La prima è l'overadattamento adattivo, che è l'overadattamento causato da ricorrere alla stessa serie di test continuamente e questa di solito si manifesta come la diminuzione dei rendimenti sul nuovo set di test. La seconda ipotesi è il drift temporale, che è la degradazione delle prestazioni causata dallo spazio temporale crescente tra i dati di training e i dati di test. Per l'overadattamento adattivo, abbiamo visto che dalla grafica sulla destra la linea rossa meglio adatta ha una gradient che è maggiore di 1. Questo significa che ogni unità di miglioramento che facciamo su CoNLL-2003 traduce in più di un unità di miglioramento su CoNLL++. Questo significa che non ci sono ritorni decrescenti e queste dimostrano che l'overadattamento adattivo in questo caso non è osservato. Quindi cosa succede con il drift temporale allora? Per il drift temporale, abbiamo fatto un esperimento per retrain o continuare a pretrainare i modelli con più recenti dati eabbiamo scoperto che le prestazioni degradano con un maggior spazio temporale. Questo conferma la nostra ipotesi che la causa principale della svalutazione delle prestazioni è il drift temporale. Concluso, per una buona generalizzabilità, avremmo bisogno di una migliore architettura del modello, dimensioni del modello più grandi e anche di più esempi di fin-tuning. Questi ingredienti devono essere utilizzati insieme, non possiamo avere solo un ingrediente ma anche gli altri. Allo stesso tempo, abbiamo anche scoperto che la svalutazione delle prestazioni qui è causata dallo spazio temporale e, sorprendentemente, non è causata dall'overadattamento adattivo anche se CoNLL-2003 è stato utilizzato per più di venti anni. Quindi tornando al quesito che avevamo sollevato nel titolo del mio articolo, i tagger di entity named entity di CoNLL-2003 continuano a funzionare bene nel 2023? Ebbene, la risposta è in effetti un risorgimento: sì. Spero che il mio articolo possa fornire maggiori ricerche su come migliorare la generalizzabilità dei modelli. Infine, vi prego di controllare il mio articolo, il mio dataset e se avete qualsiasi domande, non esitate a contattarmi. Grazie mille.</sample>
    <sample id="114">The presentation introduces a study on large language models (LLMs) and their limitations, such as heavy parameters, long training times, and huge data requirements. The focus is on the multi-head attention mechanism in LLMs, which can be optimized to reduce redundancy. The researchers propose a group head attention method using a divide-and-conquer strategy to compress multi-head attention. They evaluate their approach on three tasks: machine translation, language modeling, and abstract summarization, achieving significant parameter compression and performance improvements.</sample>
    <sample id="115">L'approccio utilizza solo un modello per ogni regime di latenza e gestisce la latenza tramite parametri specifici.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono che Servin è un giudice e che i giudici decidono cause in una corte.</sample>
    <sample id="117">La qualità dell'esempio è il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente.</sample>
    <sample id="118">The paper presents a submission for the ACL 2023 conference, focusing on improving pre-training techniques for code-switched NLP. The authors define code-switching as the presence of multiple languages in a single sentence and provide an example of a code-mixed sentence in English and Hindi. They argue that building computational models for code-switching is crucial, especially in linguistically diverse communities like India. Multilingual pre-trained models like MBERT and XLM-R do not perform well on code-switched tasks such as question answering and sentiment analysis. The main contributions of the paper are the proposal of novel MLM techniques tuned to the case of code-switching and architectural changes and auxiliary losses also tuned to the case of code-switching. The authors propose Switch-MLM, which defines switch points as groups of two tokens that transition between languages. They also propose Frequency-MLM, which uses the negative log likelihood of words in each monolingual corpora to assign language tags. The authors also propose residual connections and an auxiliary loss to increase the amount of switch point information in the final layer. The results show that the combined method of Switch-MLM or Frequency-MLM with residual connections and an auxiliary loss performs the best on all language pairs of sentiment analysis. The authors use probing classifiers to verify their claim about the switch point information and hypothesize that their methods increase the amount of switch point information present in the intermediate layers.</sample>
    <sample id="119">GPT-4 e GPT-3 series</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Gli esempi di inferenza diretta sono: "The name of the song Easy on Me" o "Its position, the first one."</sample>
    <sample id="122">I ricercatori che hanno scritto l'articolo sono affiliati all'University of Toronto e a Brain Technologies Inc.</sample>
    <sample id="123">This research presents MultiInstruct, a multi-modal instruction-tuning benchmark dataset consisting of 62 diverse multi-modal tasks covering 10 broad categories. Derived from 21 existing open-source datasets, each task is equipped with five expert-written instructions. The study investigates whether instruction tuning on multi-modal pre-trained models can improve generalization to unseen multi-modal tasks. Using OFA, a unified multi-modal pre-training model, the research shows that instruction tuning significantly improves performance on unseen multi-modal tasks. Transfer learning from natural instruction datasets also benefits instruction tuning, achieving better sensitivity and performance compared to the original OFA model. The research proposes the first large-scale multi-modal instruction-tuning dataset, which can significantly improve the zero-shot capability of OFA and explores different transfer learning techniques.</sample>
    <sample id="124">The presentation discusses the development of a comprehensive dataset and training strategy to improve the temporal reasoning capabilities of large language models (LLMs). The authors identify three levels of temporal reasoning: time-to-time, time-to-event, and event-to-event. They evaluate the performance of different LLMs on these tasks and propose a new dataset that covers all three levels with long temporal coverage. The training strategy includes a two-component approach: Temporal Span Extraction Pre-training and Time-Sensitive Reinforcement Learning. The results show that the proposed method significantly improves the performance of LLMs in various QA problem settings, particularly in open book and reasoning QA.</sample>
    <sample id="125">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="126">No, è stato considerato come un approccio non standard.</sample>
    <sample id="127">The paper presents a method for transferring reasoning abilities from large language models to smaller models using a technique called diverse reasoning. The authors propose using large models as "reasoning teachers" to generate step-by-step solutions for complex tasks, which are then used as training data for smaller models. They also introduce a novel technique called diverse reasoning, which generates multiple solutions using stochastic temperature sampling. The results show that the proposed method can achieve notable performance on many tasks, even with small models, and is highly scalable. However, there are trade-offs between development costs, inference time, and the quality of inference that need to be considered when applying this method in real-world scenarios.</sample>
    <sample id="128">The paper presents a diagnostic test suite for knowledge integration in natural language understanding models. It introduces a coreference resolution task designed to evaluate the ability of models to draw on knowledge from different sources. The authors evaluate the dataset with human study participants and establish coreference resolution models. They define three settings: background pre-training, background both, and background inference. The results show that even the best-performing models cannot reliably integrate background knowledge presented only at inference time.</sample>
    <sample id="129">Gli autori hanno fornito un esempio di gruppo contrassegnato come "gruppi minorizzati" o "minorizzati".</sample>
    <sample id="130">Le architetture dei modelli che non generalizzano in modo adeguato sono le strutture di rete tradizionali.</sample>
    <sample id="131">I nomi dei set di dati di test non sono forniti.</sample>
    <sample id="132">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="133">L'autore opera con più modalità.</sample>
    <sample id="135">The ABC-Eval method is a new dimensional approach to evaluating conversational AI. It measures the rates at which chat models commit various thematic errors, such as ignoring their partner or saying something irrelevant, contradicting themselves or their partner, hallucinating incorrect facts, violating common sense knowledge, and when the model succeeds or fails to show empathy. The method is more reliable than existing methods, as measured by inter-annotator agreement on 100 doubly-labeled conversations. Additionally, ABC-Eval labels are more predictive of overall conversation quality compared to metrics produced by existing methods. These reliable, informative, and distinct ABC-Eval metrics enable us to evaluate conversational AI with a higher resolution than previous methods are able to achieve.</sample>
    <sample id="136">The presentation introduces Fermat, a flexible evaluation set for numerical reasoning tasks. It highlights the limitations of current benchmarks in assessing mathematical abilities and proposes Fermat as an alternative. Fermat includes math worded questions from Illinois and Common Core, testing number understanding, mathematical operations, and training dependency. The evaluation reveals that most models perform poorly across these aspects, with the original set showing better performance than expected. Fine-tuning with math teachers' templates improves model performance, but training dependency remains a challenge. The impact of training templates shows that diversity in language and mathematical operations significantly improves performance. The conclusion emphasizes the need for more representative benchmarks and suggests focusing on number encoding and tokenization to enhance model capabilities.</sample>
    <sample id="137">The Tell2Design dataset is a large-scale dataset that features floor plans with natural language instructions to describe user preferences. The dataset was introduced in 2023 and consists of 76,000 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk. The dataset aims to enable users to design by telling instructions with a specific focus on the floor plan domain as the initial area of research. The dataset forces a new machine learning task where the model learns to generate floor plan designs directly from language instructions. The input for each data sample is a set of natural language instructions that characterize the key components of the corresponding floor plan design, which includes semantics that specify the type and functionality of each room, geometry that specifies the shape and dimension of each room, and topology that describes the relationships among different rooms. The desirable output is a structured interior layout that aligns with the input language instructions. The dataset was used to train and evaluate various text conditional image generation models, and the results show that the Tell2Design dataset can help bridge the language distribution gap between artificial and human instructions and improve the performance of text conditional image generation models.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare conoscenza proveniente da diverse fonti.</sample>
    <sample id="139">I relatori sono Zhiyang Xu, Ying Shen e Lifu Huang.</sample>
    <sample id="140">Sì, Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti in quanto supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto usualmente si basano su conoscenze di dominio e curazione umana.</sample>
    <sample id="142">Ciao, sto parlando del mio lavoro su risoluzione di espressioni dirette indiretto per la selezione di entità. In questo progetto introdotto il corpus AltEntities. Il mio nome è Javad Hosseini e questo è un lavoro con Felipe Radlinski, Sylvia Paredes e Annie Louise. Il nostro obiettivo è comprendere il linguaggio degli utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai mai sentito Easy on Me o I Got a Feeling?" Qui l'utente vuole scegliere tra due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio, dicendo il nome della canzone "Easy on Me" o la sua posizione, la prima. Ma spesso un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può succedere quando l'utente non ricorda il nome della canzone o le pronunciazioni sono troppo simili tra loro e difficile da distinguere o quando l'utente vuole specificare un preferimento. Ecco alcuni esempi di riferimenti indiretti: "La nuova una" o "La canzone che non è energetica". Questo è un problema importante in sistemi di conversazione e anche per valutare gli LMMS (Language Models) in comprensione di entità. Non conosciamo un pubblico dataset di grande scala per questa attività, quindi abbiamo creato uno utilizzando la crowdsourcing. Il nostro dataset copre tre domini: musica, libri e ricette. Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un contesto di dialogo. In primo luogo, Bob dice: "Ricorda quella canzone che stavi ascoltando ieri?" e in questo contesto Bob stabilisce il contesto di dialogo. In secondo luogo, Alice dice: "Hai mai sentito Easy on Me o I Got a Feeling?" che è la domanda alternativa. In terzo luogo, Bob usa un riferimento indiretto per scegliere tra queste entità, ad esempio, "La nuova una". Abbiamo fornito automaticamente i primi due speech bubbles, ma il terzo è riempito dallo annotatore. Il primo speech bubble è scelto da pochi promp manuali per dominio. Il secondo, che è la domanda alternativa, è generato come segue: utilizziamo un semplice modello dove A e B sono campione da Wikipedia. Ecco i metodi di campionamento che utilizziamo: quando aumentiamo nella lista, le entità diventano sempre più simili tra loro e è più difficile distinguere tra loro. Il primo è uniforme a caso. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia e infine quando hanno attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzone. Quand'abbiamo presentato queste domande alternative agli annotatori, sapevano il nome di queste entità, ma non necessariamente sapevano nulla delle entità. Così quello che facciamo è mostriamo un po' di conoscenza di sfondo riguardo queste entità. Per le canzoni, mostriamo un link di ricerca su Google e chiediamo ai annotatori di ascoltare almeno un pezzo di ciascuna canzone e leggerne qualcosa. Ad esempio, i risultati di ricerca di Google per la canzone "Easy on Me". Per i libri e le ricette, mostriamo testi di sfondo da Wikipedia. Per le ricette, inoltre mostriamo le immagini di Wikipedia. Così i nostri annotatori hanno una migliore conoscenza di queste entità. Poi chiediamo ai nostri annotatori di scegliere una di queste entità e descriverla usando 3-5 espressioni dirette indiretto. Ad esempio, "Il primo con la musica di pianoforte". Ecco alcuni esempi dal nostro dataset: "Il primo senza parole", "Non il primo con i 12 anni di 12 anni" o "Il primo che è fittizio" o "Proviene da Althaeer John" e così via. Il corpus AltEntities ha 6.000 domande alternative attraverso i tre domini e ha 42.000 espressioni dirette indiretto. I risultati con il modello T5xlarge sono riassunti di seguito: se il modello ha accesso alle informazioni esatte dello sfondo che gli annotatori hanno, allora la precisione è davvero alta, circa il 92% al 95%. Ma questa non è realistica. Se il modello ha accesso a informazioni parzialmente sovrapposte, allora la precisione è tra il 82% e il 87%, che è più realistico. Ad esempio, quando il modello recupera le informazioni dello sfondo. Se il modello ha accesso solo ai nomi delle entità, allora la precisione è solo del 60%. Ci sono molte opportunità di miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link verso il dataset. Grazie mille.</sample>
    <sample id="143">L'approccio SimulST viene confrontato con le politiche esistenti di Strategia Whitkey e Local Aggregation.</sample>
    <sample id="144">I ricercatori che hanno scritto l'articolo sono affiliati a diverse istituzioni.</sample>
    <sample id="145">La relatrice o il relatore è Jenny T. Liang.</sample>
    <sample id="146">The paper analyzes the omission problem in dialogue summarization, a subtask of text summarization. The authors introduce the background of dialogue summarization and highlight the importance of extracting key information from dialogues in different domains. They discuss the challenges of using large-scale pre-trained language models for dialogue summarization due to common errors such as factual errors and omissions. The paper presents an analysis of the percentage of summaries suffering from the omission problem, showing that even state-of-the-art models still reach high omission rates. The authors propose a new dataset with high-quality omission labels for dialogue summarization and explore three baseline frameworks for omission detection. They also investigate the effectiveness of post-editing methods for summary refinement based on detected omissions. The results indicate that omission detection is a valuable task and can improve summary quality.</sample>
    <sample id="147">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="148">Ciao, sono Sara Papi dalla Università di Trento e dalla Fondazione Bruno Kessler. Ecco un'introduzione rapida al mio articolo "Attenzione come guida per la traduzione contemporanea del linguaggio parlato" che è un lavoro con Matteo Negri e Marco Turchi. Cos'è la traduzione contemporanea del linguaggio parlato? La traduzione contemporanea del linguaggio parlato o Simultaneous Speech Translation (SST) è il processo di tradurre un linguaggio parlato in un testo in un altro linguaggio in tempo reale, rendendo possibile la comunicazione tra lingue diverse. E cosa sono i problemi dei modelli di traduzione contemporanea attuali? I modelli di traduzione contemporanea attuali spesso utilizzano architettura specifica addestrata introducendo moduli aggiuntivi per essere ottimizzati. Ci sono lunghe e complesse procedure di addestramento, ad esempio addestrare un modello coinvolge diverse obiettivi di ottimizzazione e addestrare e mantenere diversi modelli per raggiungere differenti regimi di latenza, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza e così via. Quindi, cos'è la nostra soluzione? La nostra soluzione è di usare modelli offline esistenti senza retrainare o adottare specifiche architettura per Simultaneous Speech Translation. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici e utilizziamo la conoscenza acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale, che è chiamato meccanismo di attenzione. Puoi vedere un esempio sulla destra. La nostra soluzione propone adat o encoder-decoder di attenzione e è una strategia per decidere se emettere o non emettere una traduzione parziale basata su dove l'attenzione punta. Un token viene emesso se l'attenzione non è concentrata, ovvero se la somma delle pesate di attenzione è inferiore a un certo livello α rispetto alle ultime lambda frame di audio ricevuti, il che significa che le informazioni ricevute sono abbastanza stabili. Ad esempio, se riceviamo un frammento di discorso che dice "I'm going to talk about" e il modello predice la traduzione in tedesco, analizzeremo le pesate di attenzione. Vedremo che le prime due parole puntano ai primi due frame di audio ricevuti, mentre l'ultima parola puntava alla ultima lambda frame di audio ricevuto. Questo significa che le prime due parole verranno emesse, mentre poiché la somma delle pesate di attenzione è superiore a un certo livello α, non emetteremo l'ultima parola e aspetteremo un nuovo frammento di discorso. Se continuiamo a ricevere un nuovo frammento di discorso e il modello predice altre tre parole, analizzeremo le pesate di attenzione. Vedremo che nessuna parola puntava alla ultima lambda frame di audio ricevuto, il che significa che queste tre parole verranno emesse. Se guardiamo i risultati dell'ADT, plottiamo i risultati di traduzione contemporanea sul grafico in cui hanno un lato blu che misura la qualità della traduzione e un lato che misura la latenza media, e consideriamo anche il tempo computazionale medio per prevedere l'output. Ci vuole che i curvi siano il più in alto possibile su questo grafico ma anche spostati verso sinistra. E confrontiamo con strategie alternative che utilizzano modelli offline, come la strategia Whitaker e la strategia locale di accumulo. E confrontiamo anche con strutture di architettura specificamente dedicate alla traduzione contemporanea del linguaggio parlato. Questi sono i risultati della nostra strategia di traduzione contemporanea del linguaggio parlato in tedesco. Vediamo che adat supera tutte le strategie applicate a modelli offline, poiché i curvi sono spostati verso sinistra. E vediamo anche che se consideriamo il tempo di accesso reale o il tempo computazionale medio, adat è la strategia più veloce. Se vuoi scoprire di più i risultati, leggi il mio articolo e anche liberiamo il codice e i modelli e i moduli di traduzione contemporanea per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="149">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="150">Meeting Q&amp;A: Extractive Question-Answering on Meeting Transcripts is a research paper by Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Delamalsalehy, Frank Dernoncourt, and Mohit Bansal. The paper introduces a new dataset called Meeting Q&amp;A, which contains 7.7 thousand questions and answers from public meeting transcripts. The dataset includes questions asked by participants in meetings and the corresponding answer sentences. The paper also discusses the development of models for extractive question answering on meeting transcripts and shows that existing models have difficulty identifying rhetorical questions and identifying which speaker answered a question.</sample>
    <sample id="151">Ciao a tutti, il mio nome è Ying e il mio collega Zhiyang e io presenteremo il nostro ricerchiamo su MultiInstruct: Improve the multi-modal zero-shot learning via instruction tuning. Con i progressi nella creazione di modelli di linguaggio a larga scala, molte ricerche hanno iniziato a esplorare nuovi paradigmi di imparare utilizzando modelli pre-allenati per diverse attività in modo efficiente e con pochi dati. Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli di linguaggio a larga scala di svolgere compiti di intelligenza in modo zero-shot, seguendo istruzioni naturali. Tuttavia, la maggior parte delle precedenti ricerche sull'addestramento con istruzioni si concentra su migliorare le prestazioni zero-shot per compiti di lingua solo, mentre i compiti di visione e i compiti multi-modal sono stati lasciati fuori. Di conseguenza, in questo lavoro ci piacerebbe indagare se l'addestramento con istruzioni di modelli pre-allenati multi-modal può davvero migliorare la generalizzabilità a compiti di intelligenza multi-modal. Inoltre, all'inizio della nostra ricerchiamo, abbiamo scoperto una considerabile disparità nella disponibilità di dataset di istruzioni tra NLP e multi-modal. Esistono più di 1600 compiti di istruzioni di lingua solo, tuttavia non esiste un pubblicamente disponibile dataset di istruzioni multi-modal di grande scala. Questo ha motivato noi a creare un dataset di istruzioni multi-modal adatto. Qui presentiamo MultiInstruct, il primo benchmark di dataset di istruzioni multi-modal che comprende 62 compiti multi-modal diversi coprendo 10 category diverse. Questi compiti sono derivati da 21 dataset di risorsa aperta esistenti e ogni compito è equipaggiato con cinque istruzioni esperte. Per indagare l'addestramento con istruzioni multi-modal sul nostro dataset proposto, utilizziamo OFA, un modello pre-ottenuto multi-modal unificato, come modello base. OFA utilizza un univoco di vocabolari per i token di lingua, immagine e coordinate di bounding box. Ecco alcuni esempi di istanze dal nostro dataset MultiInstruct. Per unificare la elaborazione di vari tipi di input e output, seguiamo il modello di OFA e rappresentiamo tutti i compiti in un formato sequenza a sequenza unificato, in cui le testate di input, immagini, istruzioni e bounding box vengono rappresentate nello spazio dei token. Adesso parliamo di addestramento con istruzioni multi-modal. Per il dataset di addestramento, utilizziamo 53 compiti del gruppo NLP per addestrare e scegliamo 10.000 istanze per compito. Per il test, riserviamo tutto il gruppo di ragione comune per il test e scegliamo ulteriormente 5 compiti dal gruppo Wikidata e MScen. Utilizziamo tutte le istanze nel set di test per ciascun compito. Inoltre, scegliamo casualmente 20 compiti dal set di test del dataset di istruzioni naturale come task di NLP. Utilizziamo il modello pre-ottenuto OFA large come modello base. Durante l'addestramento, combiniamo casualmente ogni istanza per ciascun compito con uno dei cinque modelli di istruzioni. Durante il test, per ciascun compito eseguiamo un totale di 5 esperimenti, valutando il modello utilizzando uno dei cinque modelli di istruzioni in ciascun esperimento. Rapportiamo la minima e massima prestazione e la deviazione standard della prestazione attraverso i 5 esperimenti. Se il compito è un compito di classificazione multi-modal, rapportiamo l'accuratezza. Se è un compito di generazione multi-modal, rapportiamo ROUGE-L. Per i compiti NLP, rapportiamo ROUGE-L allo stesso modo. Abbiamo anche introdotto un nuovo metrico chiamato sensibilità. Questo misura la capacità del modello di produrre gli stessi output per lo stesso compito, indipendentemente dalla variazione nella formulazione dell'istruzione. Ecco i nostri principali risultati. Come possiamo vedere, l'addestramento con istruzioni può migliorare significativamente le prestazioni di OFA su compiti multi-modal. Inoltre, il trasferimento di apprendimento da dataset di istruzioni naturale può migliorare l'addestramento. Come possiamo vedere, all'aumento del numero di compiti, il modello raggiunge prestazioni migliori e contemporaneamente una sensibilità inferiore. Abbiamo anche eseguito un esperimento utilizzando un modello con istruzioni univoci e 5 istruzioni. Come possiamo vedere, utilizzare molte istruzioni può migliorare le prestazioni generali del modello e ridurre la sensibilità molto. Questo dimostra l'effetto di diverse strategie di finetuning sul modello di sensibilità. Come possiamo vedere, grazie al trasferimento di apprendimento da dataset di istruzioni naturale, il modello può raggiungere prestazioni migliori in termini di sensibilità rispetto al modello OFA originale. Abbiamo anche potuto vedere che il trasferimento di apprendimento da dataset di istruzioni naturale aiuta OFA a raggiungere prestazioni migliori sul dataset di istruzioni naturale. Di conseguenza, proponiamo il primo dataset di istruzioni multi-modal di scala grande che ha significativamente migliorato la capacità zero-shot di OFA. Esploriamo diverse tecniche di trasferimento di apprendimento e mostriamo i benefici. Abbiamo progettato un nuovo metrico chiamato sensibilità. Altra cosa, stiamo raccolgendo un dataset di istruzioni multi-modal di scala più grande con circa 150 compiti in più di lingua naturale e lo rilanceremo. Questo è il link del repository per i dati e i modelli. Grazie.</sample>
    <sample id="152">The presentation introduces valuable resources for ancient Greek and Latin texts, exploring the implications and challenges of multilinguality in large language models. The authors have pre-trained two monolingual models (GraBERTa and GraTER) and two multilingual models (FilBERTa and FilTER) on ancient Greek, Latin, and English data. They have developed a new pre-training corpus from the Internet Archive, using incorrectly transcribed Greek stop words to identify Greek texts. The models outperform the current state of the art for both ancient Greek and Latin, with significant gains in lemmatization performance. The analysis also shows that the multilingual model does not perform significantly better than the monolingual models in semantic and world knowledge capabilities.</sample>
    <sample id="153">The presentation discusses a study on resolving ambiguities in text-to-image generative models. The researchers aimed to identify and address existing ambiguities in prompts provided to these models, such as "the girl enters the room with flowers," which can be interpreted in multiple ways. They proposed frameworks to mitigate these ambiguities and evaluate the faithfulness of generated images to user intentions. Their approach involved curating a benchmark dataset, using language models to generate clarifying questions or possible visual setups, and evaluating the generated images against human intentions using a VQA model. The study found that their framework effectively resolved ambiguities and improved image generation fidelity.</sample>
    <sample id="154">I fornitori dell'articolo sono l'Università di Trento e la Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice o del relatore è Javad Hosseini.</sample>
    <sample id="157">The paper presents a dialogue summarization model that combines static and dynamic graph structures to capture the semantic relationships between utterances in a dialogue context. The model uses an utterance encoder to encode utterances into vector representations, which are then used to construct a static graph using existing dialogue structure modeling methods. A dynamic graph module is proposed to capture the semantic relationships between utterances based on their deep vector representation. The model also employs a pre-trained language model as a summary generator to fuse the static dialogue structure and the dynamic learned dialogue structure into the final summary. The model is evaluated on the ATIS dataset and achieves state-of-the-art performance.</sample>
    <sample id="158">The proposed dual cache method uses a local cache and a global cache to store local and global entities, respectively. The local cache uses an LRU eviction policy, while the global cache uses an LFU policy. The model scans the document from left to right and classifies new or updated entities, then evaluates their frequency. If qualified, they are added to the global cache, otherwise to the local cache. When the cache is full, it triggers the eviction policy to evict an entity. The dual cache method outperforms single cache methods and significantly reduces cache misses.</sample>
    <sample id="159">Ciao a tutti, mi chiamo Koustuv Sinha e sono felice di benvenuti in un talk sul mio articolo del ACL 2023, intitolato "I giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto". Questo è un lavoro di gruppo con John Gauthier, Aaron Mueller, Kanishka Mishra, Keren Fuentes, Roger Levy e Adina Williams. In questo lavoro, rivisitiamo il paradigma minimo per la valutazione dei modelli linguistici. Il paradigma minimo per la valutazione dei modelli linguistici evaluates i modelli linguistici su base di giudizi di accettabilità, che possono anche includere grammaticalità, come Blimp o syntax gym, o accettabilità in termini di stereotipi, come crowlspears. In questo paradigma minimo, la tipica modo di valutare i modelli linguistici consiste nel mosticare un sentenzia accettabile o grammaticalmente corretto e un sentenzia non accettabile o grammaticalmente sbagliato. L'obiettivo del modello è quindi mettere maggior probabilità al sentenzia accettabile. La nostra pipeline attuale, la MPP pipeline, non ci permette di valutare la accettabilità dei modelli verso le sentenze più lunghe. Questi giorni, i modelli linguistici stanno emergendo con finestre di contesto più lunghe. Ci è dunque cruciale che valutiamo l'acceptabilità dei modelli attraverso tutto il contesto. Ecco cosa stiamo cercando di fare qui: stiamo cercando di rivisitare la pipeline MPP per chiedere ai modelli di valutare l'acceptabilità su lunghe sequenze. Così, la nostra approccio è quello di simulare queste lunghe sequenze. Simuliamo queste lunghe sequenze reselezionando i dataset stessi e creiamo nuove sentenze scegliendo sentenze accettabili o non accettabili da quei dataset. Ad esempio, qui abbiamo scelto un tipico par di grammaticalità dal dataset Blimp dell'isola di Ajanta e, invece di creare lunghe sequenze accettabili, creiamo due sequenze accettabili e non accettabili, entrambe con la stessa struttura grammatical. Estraiamo sentenze grammaticalizzate da Ajanta e le aggiungiamo come prefisso sia alla sentenza accettabile che alla non accettabile. Possiamo fare lo stesso per le sentenze non accettabili, scegliendole dal medesimo matching. Questo potrebbe anche essere utilizzato per testare la accettabilità dei modelli. Possiamo anche fare lo stesso per scelte di sentenze da un set diverso o da un dataset diverso. Così, chiamiamo questa situazione il scenario mismatch. Qui, le sentenze continuano ad essere estratte da relevanti dataset, ma non dal medesimo dataset che stiamo valutando. Possiamo fare lo stesso per le sentenze non accettabili. Infine, possiamo scegliere sentenze da un dominio completamente diverso, come Wikipedia. Questo ci dirà se i giudizi di accettabilità dei modelli sono influenzati da alcun contesto, se il contesto proviene da un subset diverso del dataset o se è completamente irrelevente per la sentenza che stiamo analizzando. Come fa il modello? Innanzitutto, guardiamo le sentenze di Wikipedia che sono completamente irrelevanti per il par di query corrente. Troviamo che i giudizi di accettabilità della pipeline MPP sono relativamente stabili, anche quando aumentiamo la lunghezza del contesto fino a 1244 per massimizzare i modelli OTP e GP T2. Vediamo qui, in orange, che i giudizi di accettabilità della pipeline MPP sono relativamente stabili. Ora, cosa succede quando scegliamo sentenze dallo stesso dataset? Qui, stiamo creando sentenze accettabili e non accettabili, dallo stesso dataset Blimp o syntax gym, e vediamo che i giudizi di accettabilità della pipeline MPP aumentano o diminuiscono notevolmente quando aggiungiamo prefissi accettabili o non accettabili. Ma quando scegliamo la stessa struttura, cioè quando scegliamo sentenze dallo stesso fenomeno in Blimp o syntax gym, vediamo un'enorme crescita o una enorme diminuzione dei giudizi di accettabilità della pipeline MPP per il modello, a seconda di quanto il prefisso scelto sia accettabile o non accettabile. Questo effetto aumenta lungo la lunghezza del contesto e probabilmente influencerà modelli più recenti con finestre di contesto più larghe. Perché questo prefisso influisce tanto i giudizi di accettabilità dei modelli? Abbiamo fatto una serie di analisi in cui abbiamo tentato di alterare la sentenza di input cercando di preservare la struttura rilevante ma aggiungendo un po' di rumore alla sentenza. Dopo aver fatto queste variazioni, scopriamo che nessuno di questi rumori sta realmente facendo il modello cambiare i propri giudizi di accettabilità. Di fatto, scopriamo che i modelli sono sensibili ai rumori applicati in modo simile. Questo significa che quando alteriamo le sentenze in un dominio accettabile, vediamo un aumento simile in tutte queste variazioni, e quando alteriamo le sentenze in un dominio non accettabile, vediamo un diminuire dei giudizi di accettabilità in maniera simile. Quindi, i principali punti di rilievo del nostro lavoro è che i modelli linguistici sono sensibili ai latenti caratteri sintattici e semantici che si condividono tra le sentenze, e che la nostra pipeline di valutazione, che attualmente utilizza input brevi e singoli, non può affatto catturare completamente le conoscenze astratte dei modelli attraverso tutto il contesto. Per vedere dettagli dei nostri esperimenti, leggete il mio articolo. Grazie per l'attenzione.</sample>
    <sample id="160">Un token unordered multisets</sample>
    <sample id="161">In Coscript, 55,000 specific goals with scripts are represented.</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo MassAlign.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che i costi di etichettatura dei dati sono molto più bassi rispetto all'annotazione umana.</sample>
    <sample id="165">The paper presents a method for adaptive reasoning without supervision. The method, called LIPOR (Likelihood Learning with Posterior Regularization), treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. An additional regularizer is used to enforce mutual exclusivity among explanations. The paper compares the method to zero-shot models and the previous best unsupervised approach on the Alpha-ALI dataset, achieving over four absolute points in accuracy.</sample>
    <sample id="166">The proposed method, Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text (NDCRF), addresses the challenge of image-text retrieval by integrating the advantages of both symbolic and analogical reasoning systems. NDCRF consists of two main components: a complex proposition generator that creates symbolic representations of images and text, and a neural symbolic reasoner that integrates these representations to obtain the final solution. The system utilizes the strengths of both analogical and logical reasoning to improve the compositionality and planning capabilities of large language models. Experimental results demonstrate that NDCRF outperforms baseline methods and achieves state-of-the-art performance on the testing set.</sample>
    <sample id="167">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici. In particolare, i documenti in DEplain-APB sono stati allineati manualmente, mentre i documenti in DEplain-Web hanno ricevuto sia un allineamento manuale che un allineamento automatico.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato raccolgendo articoli da Reuters News del 2020 e annotandoli con le stesse guidelines di annotazione utilizzate per CoNLL-2003.</sample>
    <sample id="169">This paper presents a systematic study of prompt-based language model (LLM) prompting for machine translation. The authors evaluate the translation capability of LLMs using the best practices of the NMT community, including using the latest test sets to avoid overfitting and comparing two state-of-the-art systems. They use state-of-the-art NMT metrics and show expert-based human evaluation results. The paper provides recommendations for prompt selection strategies and highlights the importance of selecting high-quality examples from the dev data. The authors conclude that while specialized state-of-the-art systems have an advantage in accuracy, PaLM comes close to a commercial system in terms of fluency.</sample>
    <sample id="170">Ciao a tutti, mi chiamo Justin Jung e sono dell'Università di Peking. Oggi presenterò il mio lavoro: "Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Many Representations". La semantic parsing è un任务 che consiste nel costruire rappresentazioni semantiche di query dell'utente, come SQL e Lambda Calculus. La parsing crosslingua è la task di tradurre query in diversi linguaggi naturali in rappresentazioni semantiche diverse. Come illustrato in questa figura, dobbiamo tradurre una query in diversi linguaggi naturali utilizzando modelli neurali per tradurre in SQL, Lambda o FunQL e altro. I modelli di parsing crosslingua esistenti sono proposti separatamente e valutati su dataset di limitate tasse e applicazioni. Ci sono lacune di copertura su certi linguaggi naturali, come il cinese, e lacune di copertura su certi rappresentazioni, come il calcolo Lambda. Esistono anche modelli valutati solo su certi modelli neurali, ad esempio solo un modello per valutare i modelli di parsing. Per risolvere queste lacune, proponiamo Exemplar, che fornisce un dataset unificato Exemplar per parsing crosslingua in diversi linguaggi naturali e rappresentazioni. Questo dataset comprende 9 dataset in diverse domande, 5 task di parsing semantic, 8 rappresentazioni diverse e 22 lingue in 15 famiglie linguistiche. Per valutare meglio il benchmark, consideriamo sei settori per training e valutazione. Il primo settore è translate test, in cui utilizziamo l'API di traduzione Google per tradurre le query di origine in un linguaggio di destinazione, quindi utilizziamo un modello monolingue per training e valutazione. Ad esempio, trainiamo un modello in inglese su query in inglese e durante l'inferenza traduciamo la query in tedesco utilizzando l'API in inglese e utilizziamo il modello trainato per prevedere il calcolo SQL. Abbiamo anche testato il modello monolingue in questo settore, in cui il linguaggio di origine è lo stesso del linguaggio di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Abbiamo anche testato il modello monolingue fuso, in cui trainiamo modelli monolingue con solo il 10% dei dati di training. Abbiamo testato anche il modello multilingue, in cui trainiamo un modello multilingue per tutte le lingue. Ad esempio, mettiamo insieme query in tedesco e inglese per trainare un modello multilingue e durante l'inferenza possiamo utilizzare questo modello per tradurre query in tedesco o in cinese, eccetera. Abbiamo anche considerato la traduzione zero-shot e fuso. Abbiamo trainato su un linguaggio di origine e tradotto su un altro linguaggio. Durante il training, trainiamo su query in inglese o la combinazione di query in inglese e tedesco fuso per trainare un modello multilingue e prevedere il calcolo output. Abbiamo anche scoperto molti risultati interessanti. In relazione all'analisi dei modelli monolingue, valutiamo due gruppi di modelli, inclusi encoder PDR, che sta per encoder pretrainato multi linguo con decoder basato su pointer, come XLNet + PDR e BERT + PDR, e encoder-decoder modelli, che sono modelli encoder-decoder pretrainati multi linguo, come BART e MT5. Abbiamo scoperto che encoder-decoder ottiene le migliori prestazioni su tutti i 9 dataset. Abbiamo anche valutato BART e XLNet + PDR su un setting multi linguo. Abbiamo scoperto che encoder-decoder o encoder PDR possono essere migliorati trainando in un mix di diverse lingue. Abbiamo scoperto che è because most of the major natural languages can obtain performance gain except that English performance drops in seven datasets and only gains in three datasets, che è noto come curse of multilinguality. Abbiamo anche confrontato il gap di performance crosslingua. In questa figura, la linea blu rappresenta il transito zero-shot, la linea rossa rappresenta il transito fuso zero-shot, mentre la linea verde rappresenta il setting monolingue. Abbiamo scoperto che confrontando la linea verde e la linea rossa, nel setting zero-shot, il gap di performance crosslingua è significativo. Abbiamo anche scoperto che confrontando la linea blu e la linea rossa, nel setting fuso zero-shot, il gap di transito si riduce rapidamente. Abbiamo anche scoperto altri findings interessanti, ad esempio encoder-decoder supera precedenti work o raggiunge risultati comparabili per trainare su lingua naturale inglese e booster la performance del fuso zero-shot su lingue naturali di destinazione. Abbiamo scoperto che modelli linguaggi multilingue come codice e blu sono ancora inadeguati per task di parsing crosslingua. In sintesi, presentiamo Exemplar, un benchmark unificato per parsing crosslingua in diversi linguaggi naturali e rappresentazioni. Condurranno un studio di benchmarking esaustivo su tre tipi rappresentativi di modelli linguaggi monolingue e i nostri risultati hanno molti findings interessanti e altro. Ecco, benvenuti a visitare il mio articolo e il codice. Grazie per la tua visione.</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro类别.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom non sono sufficienti per il CLSP.</sample>
    <sample id="174">The ArgAnalysis35K dataset is a large-scale collection of arguments used for Argument Quality Analysis (AQA). It stands out due to its high-quality, diverse, and comprehensive nature. The dataset includes 35,000 argument analysis pairs sourced from speeches, expert debaters, intermediate debaters, and novice debaters. It covers 24 themes, ensuring a wide range of motions and arguments. Each argument is annotated with an analysis that combines claims, premises, and other elements to explain the argument more coherently. The dataset also includes an instance-based annotator reliability model to mitigate human biases in annotations. Additionally, it features a relevance model that assigns scores to arguments based on their relevance to specific themes, enhancing the dataset's utility for AQA research.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allegamento come parte della formazione.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita dalla capacità del modello di rilevare e gestire le espressioni discriminatorie o offensive, indipendentemente dalla razza, età, genere o altre caratteristiche personali.</sample>
    <sample id="177">La relatrice o il relatore è Yanis Lavrak.</sample>
    <sample id="178">La relatrice o il relatore del talk è Koustuv Sinha.</sample>
    <sample id="179">The presentation discusses the development of a multi-character belief tracker for language models, specifically focusing on improving their theory of mind reasoning skills. The researchers propose a symbolic Tom method that uses explicit graphical representations to compute mental state graphs for different characters in a story. This approach allows for more interpretable reasoning and has been shown to significantly improve performance on out-of-domain tasks compared to supervised baselines. The method is particularly effective in handling second-order false belief questions and generalizes well to new datasets with increased linguistic diversity.</sample>
    <sample id="180">La relatrice o il relatore è Myra Cheng.</sample>
    <sample id="181">The paper introduces a method for extracting script knowledge from large language models to support constrained language planning. It addresses the challenge of generating scripts that are both semantically complete and faithful to specific constraints, such as making a chocolate cake. The authors evaluate and improve the constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints and using a dataset of 100 specific goals. They propose an over-generated Z filter to enhance script quality and develop a filter model to select feasible scripts based on semantic similarity and target constraint scores. The method is applied to build a dataset of 55,000 specific goals with scripts, which can be used to train smaller and specialized models for constrained language planning.</sample>
    <sample id="182">Il tropicalismo è un tropo che si riferisce alla rappresentazione di una persona come proveniente da un luogo o cultura specifico, in questo caso le donne latine.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando modelli di linguaggio e chiedendo al modello di generare un personaggio.</sample>
    <sample id="184">In questo lavoro è stato utilizzato un nuovo misuratore chiamato CXMI (Contextualized Crosslingual Mutual Information) per misurare l'utilizzo del contesto in traduzioni automatiche.</sample>
    <sample id="185">DrBERT è un modello pre-addestrato in francese per i domini biomedico e clinico, mentre ChuBERT è un modello basato su data annullata ottenuta da non università di hospitale.</sample>
    <sample id="187">Il numero di autori coinvolti nell'articolo è di 3.</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un approccio che implica l'uso di un modello preesistente per iniziare il processo di apprendimento, seguito da una serie di aggiornamenti basati su nuovi dati raccolti. Questo processo viene ripetuto fino a quando non si raggiunge un livello desiderato di precisione o performance.</sample>
    <sample id="189">Il set di dati ha come obiettivo comprendere il linguaggio dell'utente quando vuole fare una scelta.</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando la tecnologia di immissione di segni. Questa tecnologia consente all'utente di iniettare un segno nascosto nel servizio EaaS e di utilizzarlo per estrarre i parametri del modello.</sample>
    <sample id="191">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="192">The presentation introduces a new optimizer, KAM, designed to achieve fast convergence and low memory usage simultaneously. It uses non-negative matrix factorization (NMF) to reduce memory requirements and adaptively adjusts the momentum based on the difference between predicted and generated updates. The optimizer is compared with Adam and AdaFactor on training tasks of three large language models (GPT-2, GPT-3, and BERT). KAM achieves significant improvements in validation accuracy and better performance than Adam in pre-training BERT, while also reducing memory costs. The results demonstrate the efficiency of KAM in large language model training tasks and its potential for large batch training.</sample>
    <sample id="193">Per creare il set di dati iniziale, sono stati impiegati circa un migliaio di annotatori.</sample>
    <sample id="194">I ricercatori dell'articolo sono affiliati all'Università di Washington e all'Institute for AI.</sample>
    <sample id="195">The paper presents a new framework called ROHT for reasoning over hierarchical question decomposition trees for explainable question answering. The framework consists of two stages: building the hierarchical question decomposition tree (HQDT) and probabilistic reasoning over HQDT. HQDT is built by using a question decomposer to generate leaf nodes, which are atomic questions, and then using another question generator to generate intermediate questions based on grouped leaf questions. The certainty score of each node is computed based on the likelihood of representing the certainty of its generation. Probabilistic reasoning over HQDT involves selecting appropriate knowledge sources for each node, executing the corresponding executors to get answers with probabilities from the selected knowledge sources, and aggregating the candidate answers from all the knowledge sources to output top-k answers with highest probabilities. The framework is evaluated on two challenging complex QA datasets, KQAPro and Music, demonstrating the effectiveness of integrating answers of subquestions of different levels and utilizing knowledge from KB and text corpus.</sample>
    <sample id="196">Il primo esempio in cui il governatore è a sinistra è "Lisa bought and Meggy".</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli che hanno errori di senso comune in circa il 20% delle loro risposte, producono informazioni irrelevanti in circa il 15% delle risposte e si contraddanno o si riconoscono come partner circa il 10% del tempo.</sample>
    <sample id="198">La valutazione dell'accettabilità dei modelli nell'intera finestra di contesto è necessaria per capire come i modelli trattano le strutture grammaticali e semantiche latenti che si condividono tra le frasi. Questo è importante per capire se i modelli hanno una conoscenza astratta che non è limitata a singole frasi.</sample>
    <sample id="199">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset.</sample>
    <sample id="200">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="201">Per la valutazione sono state utilizzate metriche di MT standard e anche risultati umani per la valutazione.</sample>
    <sample id="202">No, il regresso nella generalizzazione non influisce su specifici tipi di NER.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché le decisioni che i ricercatori prendono possono influenzare il processo di ricerca e i suoi risultati. Questo può cambiare le decisioni che i ricercatori prendono.</sample>
    <sample id="204">I modelli LLM multilingue come BLOOM sono stati adattati utilizzando adattatori o con una messa a punto integrale.</sample>
    <sample id="205">The presentation discusses the political biases in language models and their impact on downstream tasks. It explores how language models learn from diverse perspectives, but also inherit social biases from their training data. The researchers propose evaluating political leanings of language models and investigating how different political biases affect performance on tasks like hate speech detection and fake news detection. They find that language models can exhibit varying political leanings and pick up societal polarization. The presentation highlights a fairness issue where language models with different political leanings may marginalize certain groups or fail to detect hate speech targeting minority communities. The discussion concludes with the dilemma of sanitizing political opinions in training data to avoid bias propagation while avoiding censorship or exclusion.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, si ricorre al modello di discordanze.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i migliori metodi di comunità per evitare un混杂效应。</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il nuovo metodo proposto ha un guadagno di 55.000 specifici obiettivi con script, mentre il metodo di riferimento non ha un tale obiettivo.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nell'articolo, i modelli più piccoli utilizzati sono 10.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">The paper presents an argument for the symmetric structures of coordination against asymmetric structures. It is based on the principle of dependency length minimization, which states that shorter dependencies are preferred. The author extracts statistics from the Penn Treebank and observes that left conjuncts tend to be shorter when the governor is absent, but this tendency disappears when the governor is present. This provides an argument against asymmetric structures of coordination.</sample>
    <sample id="217">The paper introduces a method for generating controllable dialogues with multiple attributes using a disentangled control generation (DCG) framework. The authors propose a disentangled prompt-based model that uses attribute-related information from the pre-trained language model to generate diverse and controllable dialogues. They also introduce a unified reference-free evaluation framework, MAE, to assess the quality of generated dialogues. The results show that the proposed method outperforms baseline models in terms of controllability and test quality.</sample>
    <sample id="218">I ricercatori che hanno scritto l'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">The presentation introduces a research assistant at Academia Sinica who is presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work was done with Professor Jero and Chuan Liu, and it considers the Form 10-K as the target corpus. The goal of the work is to find the relationship between two reports by predicting the importance of words and measuring the performance of highlighting. The model is trained using an external dataset and fine-tuned using different objectives and techniques. The results show that the model achieves the best performance on the final dataset and can benefit from simulating mismatch pairs during training. The presentation also mentions future works such as improving effectiveness or adding more features.</sample>
    <sample id="220">I fornitori dell'articolo sono Vasuda Varadarajan, Swanie Juhng, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann e H. Andrew Schwartz.</sample>
    <sample id="221">Nell'articolo sono state analizzate le lingue tedesca e inglese.</sample>
    <sample id="222">The paper investigates data interventions for enabling out-of-domain generalization in open-domain question answering. It identifies the type of dataset shift a new domain exhibits and determines which data interventions are effective for a specific type of shift. The authors propose two overarching methods for generating data interventions: few-shot and zero-shot. They also investigate the impact of format, answer distribution, and context distribution on model performance. To assess the nature of incompatibility between the target model and domain, they use existing dataset shift taxonomy in machine learning to understand the nature of shift in target datasets with respect to the source model. They compute the likelihood assigned by the source retriever and reader models to all contexts and answers in the target dataset to estimate the type of dataset shift. They find that few-shot adaptations are most useful for target sets with concept and covariate shifts, while zero-shot adaptations are most useful for target sets with no shift.</sample>
    <sample id="223">Il nome del relatore è Changbing.</sample>
    <sample id="224">Gli esperimenti hanno studiato due modelli: il modello di lingua Impart per la produzione di semplificazioni documentali e il modello di lingua Impart standard per la produzione di semplificazioni sentenziali.</sample>
    <sample id="225">Per scopi di addestramento e test, 53 attività vengono utilizzate.</sample>
    <sample id="226">Il numero di autori coinvolti nell'articolo è due.</sample>
    <sample id="227">The presentation introduces the concept of grounded language understanding (GLU), which involves mapping natural language expressions to executable plans or programs in specific target environments. It highlights the challenges in current language models research, particularly the lack of grounding during pre-training and the gap between pre-training and downstream applications. The speaker proposes a new framework called Pangu, which focuses on discrimination rather than generation, using a symbolic agent to interact with the environment and propose candidate plans, while the language model scores and ranks these candidates. The framework is tested on various language models and settings, demonstrating strong performance and sample efficiency. The results suggest that discrimination might be a better strategy for GLU than generation.</sample>
    <sample id="228">Gli autori hanno effettuato i test su quattro set di dati: AGNews, Mind, SST-2 e Yelp.</sample>
    <sample id="229">The presentation introduces a study on detecting improvable claims in argumentative writing support, focusing on the importance of optimal phrasing in professional writing. The authors explore two tasks: sub-optimal claim detection and claim improvement suggestion, using revision-based data from collaborative online debate platforms like Kiyo. They address challenges such as representativeness, model complexity, contextual information dependence, and topical and user bias. The study concludes that revision-based data can be effectively employed for these tasks, with modeling the distance between two claim versions beneficial for detecting sub-optimal claims.</sample>
    <sample id="231">NACHOS è un dataset di medicina crawled data dal web.</sample>
    <sample id="232">Il relatore è Avi Beller.</sample>
    <sample id="233">The paper presents a solution for simultaneous speech translation using encoder-decoder attention. The proposed method uses existing offline models without retraining or adopting specific architectures, and handles latency through specific parameters. The model predicts translations based on where attention points to, emitting words if the attention is concentrated and waiting for another speech chunk if not. The results show that the proposed method outperforms other strategies applied to offline models in terms of translation quality, average latency, and computational load.</sample>
    <sample id="234">La strategia del prompting ha un grande impatto sui risultati.</sample>
    <sample id="235">I fornitori dell'articolo hanno le seguenti affiliazioni: Carnegie Mellon University, Language Technologies Institute, Técnico de Lisboa, BAILR e Unbabel.</sample>
    <sample id="236">Le 5 istruzioni scritte da esperti sono equipaggiate con ciascun compito.</sample>
    <sample id="237">Gli autori propongono una suite di test diagnostiche per integrare la conoscenza.</sample>
    <sample id="238">The video presents a new benchmark dataset called MeetingBank, which is designed to address the challenges of high-quality meeting summaries and the difficulty in locating trustworthy resources for public meetings. The dataset includes 1366 city council meetings with nearly 7000 instances, including meeting transcripts, reference summaries, and URLs containing useful resources. The data collection process involves using speech-to-text APIs, identifying meeting types and dates from meeting websites, and aligning time stamps to obtain segment transcripts. The dataset provides statistics on the number of meetings, duration, tokens per meeting, speakers per meeting, and the year period of meeting collected. It also includes summary instances gathered for each city and average statistics for both meeting level and segment level. The video evaluates different summarization systems, including extractive and abstractive models, and uses metrics such as coverage score, density score, ROUGE-2 score, and human evaluation to assess the quality of generated summaries. The results show that GPT-3 achieves the highest overall scores in terms of fluency and coherence, but its results are less impressive in terms of informativeness and factuality. The video concludes by encouraging viewers to use the resource and providing insights into the decision-making process of city councils.</sample>
    <sample id="239">Salve a tutti, il mio nome è Avi Vilner e mi piacerebbe fornire un breve riassunto del paper intitolato "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro di collaborazione con i miei colleghi da Google Translate. PaLM è un modello linguistico a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testo comprensiva di 780 miliardi di token. In termini di applicazione, ha raggiunto lo stato dell'arte in centinaia di NLP task. In questo lavoro, presentiamo per la prima volta uno studio sistematico sulla prompting dei modelli linguistici per traduzione. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità MTL, che include l'uso dei test set più recenti per evitare un abuso dei testi con i dati di addestramento del modello linguistico. Abbiamo anche confrontato due sistemi di traduzione dello stato dell'arte, i migliori sistemi di traduzione per la valutazione MTL. Utilizziamo metriche avanzate di traduzione e inoltre mostriamo anche i risultati di valutazione umana con esperti. Infine, forniamo alcune raccomandazioni per le strategie di prompt. Il prompt ha un grande impatto sulle prestazioni dei modelli linguistici per la traduzione, come possiamo vedere in un semplice esperimento dove utilizziamo un prompting singolo e forniamo due diversi prompt per una stessa frase. La maggioranza delle frasi, 516 su 1000, la differenza riservata è di più di un punto percentuale, e in casi estremi può arrivare a 40 punti. Ciò dimostra quanto sia importante scegliere una buona strategia di prompt. In nostre esperienze, ci siamo fermati su una strategia di prompting a 5 passi, dove ogni frase che forniamo al sistema è seguita dalla lingua di destinazione. In questo esempio, per tradurre da tedesco in inglese, le frasi di origine sono seguite dal tedesco e le traduzioni inglese sono seguite dal inglese. Abbiamo notato che la forma reale del prompt non ha un grosso impatto in caso di prompting singolo o a 1 passo, ma quando passiamo a 5 passi di prompting, non c'è quasi nessuna differenza tra la forma reale del prompt e la nostra forma di prompting. Sono le esempi che hanno maggior peso. La somma dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza all'esempio di origine. Ciò significa che è importante scegliere esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt da dataset di traduzione MTL o da dataset di dev. I dataset di dev sono molto più accurati e di qualità più alta rispetto ai dataset di addestramento, e i risultati sono migliori quando utilizziamo i dataset di dev. Tuttavia, i sistemi di traduzione dello stato dell'arte hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma PaLM si avvicina molto a un sistema commerciale. In questo caso, ci siamo concentrati sulla valutazione con Google Translate. Le informazioni che otteniamo dalla nostra valutazione utilizzando il framework MQLM rivelano che la fluidezza di PaLM è simile a quella dei sistemi dello stato dell'arte, ma la differenza principale deriva dalla precisione. In particolare, gli errori più comuni sono gli errori di ommissione. Ciò sembra che PaLM sceglie di produrre una traduzione migliore, spesso mettendo via parti della frase di origine che non sono rilevanti nella traduzione. Tuttavia, la qualità di output stilare per PaLM è inferiore rispetto ai sistemi dello stato dell'arte, che è un segnale aggiuntivo che PaLM fornisce un output fluente ma con qualche problema di precisione. Ecco tutto per questa rapida panoramica. Per maggiori dettagli, ti prego di vedere la presentazione completa del paper. Grazie mille!</sample>
    <sample id="240">Ciao, mi chiamo Dawei e sono un studente di dottorato presso l'Università di Saarland in Germania. In questo video, vorrei presentare il mio recente lavoro, "Weaker Than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro di gruppo con Xiuoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow. Vorrei iniziare con una breve introduzione alla supervisione in piccolo e al learning in piccolo. In supervisione in piccolo, non si manda manualmente etichettata i dati. Invece, i dati vengono etichettati utilizzando fonti di etichettatura indiretti, come regole semplici, database o etichettatura di qualità bassa, come illustrato nella figura sulla destra. In confronto alle annotazioni umane, le etichettature indirette sono molto più economiche, ma anche rumorose, cioè che un certo numero delle etichette sono incorrette. Se si addestrano direttamente reti neurali su data etichettata indirettamente, le reti neurali tendono a memorizzare il rumore nell'etichettatura e non generalizzano bene. In learning in piccolo supervisione, gli algoritmi di addestramento sono proposti per addestrare robustamente reti neurali su tale etichettatura rumorosa in modo che i modelli addestrati possano generalizzare bene. In recenti opere in WSL (WSL sta per learning in piccolo supervisione), un comune pretesa è che i modelli addestrati su data etichettata indirettamente raggiungano prestazioni elevate su set di test puliti. Tecnicamente, questa pretesa non è falsa, ma c'è un problema: si assume che ci sia un set di validazione pulito disponibile per la selezione del modello. Abbiamo deciso di esaminare questa situazione problematica, poiché implica che è necessaria un'annotazione manuale aggiuntiva per l'addestramento in piccolo supervisione, ma come un elefante nello stile, questa necessità è spesso ignorata. La nostra conclusione adottata è di porre tre domande di ricerca: primo, è necessario un set di validazione pulito per WSL o possiamo forse utilizzare un set di validazione rumoroso invece? Secondo, se è necessario o se è obbligatorio usare un set di validazione pulito per WSL per funzionare correttamente, allora quante样样本我们需要? Infine, dovremmo usare solo i样样本 puliti per la validazione o ci sono altre migliori modalità per utilizzarli? Abbiamo indagato queste domande di ricerca nel nostro lavoro e i nostri findings sono i seguenti: primo, scopriamo che recenti metodi di WSL hanno bisogno di样样本 validazione puliti per funzionare correttamente, altrimenti c'è un forte svallo di prestazione, come illustrato nella figura. Se non ci sono样样本 validazione puliti, allora i modelli addestrati non possono generalizzare al di là dei relativi etichettamenti rumorosi, cosa che rende l'addestramento inutile. Questo indica che i metodi di WSL effettivamente richiedono etichettatura manualmente pulita per funzionare correttamente e il costo di ottenimento di样样本 validazione puliti non dovrebbe essere trascurato. Il nostro secondo finding è che aumentare il numero di样样本 validazione puliti aiuterà i metodi di WSL a raggiungere prestazioni migliori, come illustrato nella figura sulla sinistra. Tipicamente, basta 20样样本 per classe per ottenere prestazioni elevate. Ma, non è la fine della storia, perché se decidiamo di accedere a样样本 validazione puliti, allora addestrare i modelli direttamente su di essi anche otterrà prestazioni migliori. La figura sulla destra mostra la differenza di prestazione tra metodi di finetuning diretti, che vengono applicati direttamente su样样本 validazione puliti, e i metodi di WSL che usano solo样样本 validazione puliti per la validazione. Come possiamo vedere, se ci sono 10样样本 per classe, finetuning diretti inizia a superare i metodi di WSL. Infine, l'incremento di prestazione affermato in precedenti metodi di WSL può essere facilmente ottenuto consentendo di continuare finetuning sui样样本 validazione puliti. Come possiamo vedere dalla figura, il modello denominato FTDW inizialmente sottopresta rispetto ai metodi di WSL più complicati come cosine, tuttavia se si consente di continuare finetuning sui样样本 validazione puliti, allora FTDW si comporta allo stesso modo degli altri metodi. Quindi, in pratica, non c'è motivo di scegliere metodi di WSL più complessi che richiedono più tempo di calcolo e spazio disco. Per riassumere, abbiamo dimostrato che i recenti metodi di WSL richiedono样样本 validazione puliti per funzionare correttamente, che il vantaggio di prestazione e praticità dei metodi di WSL è pesantemente sovrastimato. Le nostre raccomandazioni concrete per future work sono le seguenti: primo, rapportare i criteri di selezione del modello, ad esempio, rapportare se la selezione del modello è fatta senza样样本 validazione puliti; secondo, i metodi di WSL dovrebbero essere confrontati con baselines di apprendimento a breve termine che operano su样样本 validazione puliti; terzo, finetuning continuo è un semplice ma forte baselines che dovrebbe essere considerato in future work in WSL. Infine, abbiamo fornito il codice open source del nostro codice. Puoi trovarlo nella Q&amp;A code sullo slide. Ti prego di controllarlo. Grazie per la tua visione.</sample>
    <sample id="241">The paper discusses a human-in-the-loop evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment claims. It highlights the limitations of current methods, such as unrealistic evaluation and lack of human-centricity. The proposed framework involves two main components: claim detection using keyword filtering, T5 models, and trendiness ranking, followed by policy violation verification using a stance classification model. Early detection is defined as identifying an unapproved treatment before its first appearance in news articles. The system's effectiveness is evaluated through human-assigned Likert scores for policy violations and a metric for confirmed policy violations per human hour. The framework aims to capture the complex interplay between systems and human content moderators in a realistic end-to-end setting, providing a valuable resource for evaluating misinformation detection systems.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono la valutazione umana, le valutazioni con scale Likert e le valutazioni pairwise.</sample>
    <sample id="243">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="244">Per risolvere il problema dell'identità dell'entità che il pronoun "he" si riferisce a, è necessario conoscere le informazioni specifiche dell'entità e le informazioni di sfondo. In questo caso, la conoscenza di base necessaria sarebbe che Servin è un giudice e che i giudici decidono casi in una corte.</sample>
    <sample id="245">The paper presents a study on high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The authors developed a two-step pipeline to identify these workers, which involves a qualification task and an endurance task. The qualification task evaluates the annotator's ability to handle multiple dimensions correctly, while the endurance task assesses their capacity to handle heavy workloads. The study also includes a reference-based task to test general performance and compares the results with baseline MTurk workers and cloud research workers. The findings show that the pipeline can achieve high agreement at a lower cost and is similar in quality to cloud research workers. However, the study has limitations, such as only testing English summarization on the MTurk platform and not guaranteeing the training of crackness.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi vedere il link in GitHub.</sample>
    <sample id="247">The paper introduces a new dataset, FactKG, for fact verification via reasoning on knowledge graphs. It presents a valuable knowledge source due to its ability to perform reliable fact verification in text or table-based cases and its practical use in modern dialogue systems for checking consistency between user input and knowledge graphs. The dataset includes claims in two styles (written and colloquial) with five types of reasoning (one-hop, conjunction, existence, multi-hop, and negation). It uses DBpedia as the knowledge graph and supports both supported and refuted labels. The method involves using a collocational style transfer model and presupposition templates, and it outperforms all other baselines, including claim-only baselines and a gear model that uses graph evidence.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico, ad esempio Paese, genere, ecc.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate in modo tale da preservare la struttura rilevante ma aggiungendo un rumore.</sample>
    <sample id="250">Valutare dimensionalmente significa valutare più aspetti specifici del modello per capire le sue caratteristiche e le sue caratteristiche.</sample>
    <sample id="251">I ricercatori dell'articolo sono affiliati all'Università di Scienze e Tecnologia di Cina, Microsoft e Sony AI.</sample>
    <sample id="252">The presentation introduces U-CREAT, an unsupervised case retrieval system using event extraction. It highlights the challenges faced by legal professionals due to the increasing volume of cases and the reliance on experience for citing relevant past precedents. The primary case retrieval task involves retrieving relevant candidates from a candidate pool based on a query document. The presentation discusses two key contributions: the I LPCR dataset, a new benchmark for the PC task with 7,700 legal cases and 6.775 average iterations per query document, and the U-CREAT pipeline, which leverages unsupervised learning techniques and an event-based approach for high retrieval efficiency and low inference time across Indian and Canadian legal systems.</sample>
    <sample id="253">The proposed approach aims to contribute to the detection of mental health disorders by automatically analyzing social media posts. The model is trained using a base language model and information from Reddit and mental health resources, including a lexicon to guide the masking process. The results show that the proposed approach achieves better results than MentalBERT, a model trained with a large amount of data, in terms of balance between finding users and labeling them correctly. Future work will explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">This research paper presents a document-level distant relation extraction framework with uncertainty-guided label denoising. The proposed method leverages distant supervision data to improve the label quality of DS data, which is time-consuming and labor-intensive. The framework introduces an instance-level uncertainty estimation method to capture the uncertainty score for overlapping relations and designs a relabeling strategy with dynamic class uncertainty thresholds. The results show that the proposed framework outperforms several strong baselines on two public datasets.</sample>
    <sample id="255">La forma del prompting si rivela importante in casi di zero e uno shot prompting.</sample>
    <sample id="257">I ricercatori hanno valutato quattro modelli di dialogo di stato dell'arte.</sample>
    <sample id="258">This paper proposes using large language models (LLMs) to evaluate the quality of texts in natural language processing. The authors use LLMs to generate ratings based on instructions and sample inputs, aiming to provide a more stable and reproducible alternative to human evaluations. They compare the performance of different LLMs, including GPT-2 and ChatGPT, with human evaluations by English teachers. The results show that while some smaller LLMs do not significantly outperform human-written texts, larger models like DaVinci and ChatGPT demonstrate a clear preference for human-written texts. This suggests that certain LLMs can be used as alternatives to human evaluations in tasks such as rating stories.</sample>
    <sample id="259">The speaker, Justin John from Penn State University, presents Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Multiple Representations. The task is to build semantic representations of user queries in multiple natural languages using neural models. Existing cross-lingual semantic parsing models are separately proposed and evaluated on limited datasets and applications, leading to gaps in coverage for certain languages and representations. To address this, Exemplar provides a unified dataset for cross-lingual semantic parsing in multiple natural languages and multiple representations, containing 90 datasets, 5 semantic parsing tasks, 80 multiple representations, and 22 natural languages in 15 language families. The speaker evaluates the performance of monolingual and multilingual models in six settings: translate test, monolingual model, multilingual fine-tune, multilingual multi-language model, cross-lingual zero-shot, and cross-lingual few-shot transfer. The results show that encoder-decoder models outperform previous work and significantly boost performance on target natural languages.</sample>
    <sample id="260">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="261">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e sicuri rispetto alle limitazioni.</sample>
    <sample id="262">Ci sono otto autori coinvolti nell'articolo.</sample>
    <sample id="263">The paper presents a new method for mitigating label biases in in-context learning, which is a popular paradigm for utilizing large language models. The authors identify a new type of bias called domain label bias and propose a calibration method to handle all types of biases. They conduct experiments using different datasets and models to show that their method significantly improves the performance of in-context learning on tasks with large domain label biases.</sample>
    <sample id="264">The presentation introduces a new task called Transferable Audio-Visual Text Generation (TAVT) and proposes a framework to train a model that can adapt to new multimodal domains with limited labeled data. The framework consists of three components: Audio-Visual MapNet, Audio-Visual Encoder, and Language Model Generator. The first model, Audio-Visual MapNet, maps different visual concepts across domains into a unified audio-visual semantic space and adjusts the semantic distribution. The second model uses a Transformer-based encoder and generator, introducing an alpha parameter to balance the contribution of different modalities to each word. The third model is a constructive learning method that uses a contrastive loss function to directly optimize the visual-audio element scores without relying on random negative samples. The experimental results show that TAVT outperforms state-of-the-art approaches in cross-domain settings, even with low-resource domains.</sample>
    <sample id="265">La relatrice o il relatore è Vasuda Varadarajan.</sample>
    <sample id="266">I fornitori dell'articolo sono Adam Siprowski e Igor Miltruk.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono gli errori di ommissione, in cui PaLM sceglie di produrre una traduzione più comprensibile, spesso lasciando fuori parti della frase sorgente che non sono rilevanti per la traduzione.</sample>
    <sample id="269">Ciao, mi chiamo James Finch e sono Sarah Finch. Oggi useremo ABC-Eval, un nuovo approccio dimensionale per valutare i sistemi di dialogo orientati al chat. Questo lavoro è stato fatto dal laboratorio NLP di Emory, guidato dal professor Jinho Choi all'Emory University e in collaborazione con Amazon Alexa AI. Immagina che tu abbia sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con il momento corrente dello stato dell'arte. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo ai giudici umani di scegliere quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala Likert. Questi metodi funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per comprendere le forze e le debolezze del modello a livello più fine-granulare. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, ad esempio la rilevanza delle risposte del modello utilizzando metodi esistenti o scala Likert. Tuttavia, crediamo che ci sia un metodo più preciso e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana, utilizzando un chiaro annotazione se o no ciascuna risposta del modello esprime certi comportamenti, ad esempio rispondendo con informazioni non rilevanti o contraddicendo se stessa o il partner. Chiamiamo questo approccio annotazione comportamenti in chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chat suggeriti da recenti pubblicazioni per influenzare la qualità del dialogo. ABC-Eval è capace di misurare le tassi in cui i modelli di chat commettono diversi errori tematici. Ad esempio, ABC-Eval misura il numero di turn in cui un modello di chat ignora il proprio partner o dice qualcosa di non rilevante, si contraddice da solo o con il proprio partner, elenca fatti incorretti o viola la conoscenza comune e quando il modello riesce o fallisce a dimostrare empatia. Per determinare cosa tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat dello stato dell'arte e li abbiamo valutati su 100 conversazioni umane-bot per modello utilizzando ABC-Eval. Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni a scala Likert sul livello del turno, valutazioni a scala Likert sul livello del dialogo e confronti a livello di coppia pairwise. Per ogni dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati della qualità del dialogo, poiché è la pratica standard per valutare i modelli di chat lungo molte dimensioni. Dall'analisi di questi risultati di valutazione, abbiamo scoperto che etichette ABC-Eval comportamenti sono in generale più affidabili rispetto alle etichette raccolte da metodi esistenti, come misurato da accordo interannotatore su 100 conversazioni etichettate in modo doppio. Inoltre, etichette ABC-Eval sono più predittive della qualità della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostrato da questa analisi lineare di regressione semplice. Ad esempio, puoi vedere come misurare la proporcione di turn con contradizioni se stessa e partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza a scala Likert spiegano solo il 4% o meno. Infine, ci siamo verificato se ogni metrica di valutazione mantiene un aspetto unico della qualità del chat utilizzando una regressione lineare passo dopo passo. Puoi vedere come la combinazione di tutti i metri ABC-Eval spiega più del 25% della qualità della conversazione e che rimuovendo i metri uno alla volta, la maggior parte di loro risulti in una perdita di informazione significativa sulla qualità. Sulle altre mani, la combinazione di tutti i metri a livello di turno a scala Likert spiega ben meno della qualità e meno di questi metri hanno informazione unica. Questi metri ABC-Eval affidabili e informativi e distinti ci permettono di valutare l'intelligenza artificiale conversazionale con una risoluzione più alta rispetto ai metodi precedenti. Puoi vedere che nei risultati del nostro esperimento che diversi sfidieri rimangono e hanno stati precisamente quantificati. Ad esempio, i bot che testammo hanno violazioni di senso comune in circa il 20% delle loro risposte, producono informazioni non rilevanti in circa il 15% delle risposte e si contraddicono da se stessi o con il proprio partner circa il 10% del tempo. Con la velocità rapida di miglioramento nel campo, molte di queste tassi di errori potrebbero vedere una diminuzione in nuovi modelli rilasciati dalla nostra valutazione è stata condotta. Tuttavia, questo è tutto il più motivato a perseguire metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC-Eval possa essere utilizzata da altri nel campo come un passo significativo in questa direzione e ci guardiamo alla vista di come l'intelligenza artificiale conversazionale progrediscerà negli anni a venire. Grazie per l'attenzione.</sample>
    <sample id="270">I fornitori dell'articolo sono il laboratorio NLP di Emory e Amazon Alexa AI.</sample>
    <sample id="271">CFT significa Continuos Fine-Tuning.</sample>
    <sample id="272">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="273">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "Quando richiede la traduzione contesto? Esplorazione multilingue guidata da dati". Questo lavoro è stato fatto in collaborazione con Patrick Fernandez, Emmy Liu, Andre F.T. Martins e Graham Neubig. Molte traduzioni dipendono dal contesto. Ad esempio, come tradurre "Mole" in questa frase? Se la frase precedente era "Le cose potrebbero iniziare a mettere in pericolo se i ministri scoprono", allora "Mole" si riferisce a un spia. Ma se la frase precedente era "Sarebbe qualcosa di serio, dottore?" allora "Mole" si riferisce a un segno di nascita. Quindi, a seconda del contesto, il significato della parola cambia e quindi anche la traduzione cambia. Tuttavia, valutare quanto bene i modelli possono tradurre casi come questi è piuttosto difficile. In primo luogo, pochissime traduzioni dipendono dal contesto, il che rende inattuari le metriche a livello di corpus come BLEU. E alcuni hanno proposto valutazioni mirate su traduzioni a contingent context, ma queste risorse supportano solo tipi limitati di traduzioni a contingent context e set di lingue limitati, poiché solitamente si basano su conoscenza dominio e curazione umana. In questo lavoro, cercheremo di rispondere a queste due domande: quando richiede la traduzione contesto e quanto bene i modelli gestiscono questi casi. Per rispondere alla prima domanda, iniziamo misurando quanto una parola dipende da un contesto per la traduzione. In un precedente lavoro, abbiamo introdotto CXMI come misura dell'uso del contesto dai modelli di traduzione automatica e questa è fatta misurando quanto informazione il contesto C fornisce sul target Y data la parola X. Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello. In questo lavoro, estendiamo CXMI a P2CXMI che può misurare l'uso del contesto al livello della frase o al livello della parola. Possiamo pensare alle parole che hanno alta P2CXMI come quelli che richiedono contesto per la traduzione. Ora analizziamo parole con alta P2CXMI per cercare pattern tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'è un punto di spaccatura tra queste parole e eseguiamo l'analisi su transcripi di TED Talks tradotti in 14 lingue diverse. Analizziamo parole con alta P2CXMI per vedere se c'</sample>
    <sample id="274">Il nome della relatrice o del relatore è Justin John.</sample>
    <sample id="276">The IndicMT Eval dataset is a comprehensive resource for evaluating machine translation metrics in Indian languages. It includes 7,000 samples from five languages (Tamil, Malayalam, Hindi, Marathi, and Gujarati) with multiple candidate translations per source sentence. Human annotators evaluate these translations, providing detailed feedback on errors, severity, and overall scores. The dataset covers various metrics, including overlap-based, embedding-based, and Comet variants, which are compared to human scores. The results show that Comet variants generally outperform other metrics, especially in accuracy, and exhibit higher correlations with human scores. The dataset is publicly available for further research and applications.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">Il metodo utilizza un concetto sociolinguistico chiamato "contrassegnatezza", che afferma che i gruppi non dominanti sono linguisticamente contrassegnati, mentre i gruppi dominanti non lo sono.</sample>
    <sample id="279">I ricercatori dell'articolo sono studenti di PhD all'Università di Washington.</sample>
    <sample id="280">The paper presents a novel attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations. The framework consists of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The authors propose a new visual feature extractor called Vis-Net, which captures visual cues by integrating facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. They also design a multimodal fusion model called Multi-Att, which integrates one modality with complementary information from the other modalities through stacked bidirectional multi-head cross-attention layers. Additionally, they introduce a sample-weighted focal contrastive loss to address the difficulty of classifying minority and semantically similar emotions. Experimental results demonstrate that Multi-Emo achieves state-of-the-art performances on two ERRC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The presentation discusses the importance of context in translation and how machine translation models handle context-dependent translations. The speaker, Kayo Yin, presents a study on when translation requires context and how well models can handle these cases. The study uses CXMI as a measure for context usage by machine translation models and analyzes words with high CXMI to identify patterns. The findings are used to design a benchmark for document-level translation, which shows that context-aware models perform better than models without context for certain discourse phenomena like formality and lexical cohesion.</sample>
    <sample id="282">This paper presents a new method for non-parallel story author-style transfer with discourse representation and content enhancing. The proposed method, called StoryTrans, generates text in the target style by transferring the source text with style-specific content keywords masked and then generating the whole text by incorporating these keywords explicitly. The training framework is separated into two stages: the first stage uses an adversarial training framework to recover the input and disentangle sentence embeddings, while the second stage aims to fill in the correct style-specific content and remove the mask tokens. The results show that StoryTrans outperforms strong baselines in terms of style control and content preservation, and the transferred texts are aligned with the golden texts in the style feature space.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è il PRA approach, che è un approccio che utilizza la congiunzione come capo della struttura di coordinamento.</sample>
    <sample id="284">The paper presents a novel fuzzy span mechanism for enhancing universal information extraction (UIE). The current spam-based UIE model relies on identifying and labeling the spam boundaries of target texts, which can be ambiguous. The proposed method uses a continuous distribution of correct probabilities to model the fuzzy spam boundary, converting it into discrete values for calculation. The model calculates boundary cross entropy with the golden boundary as BCE loss and adds KL divergence between predicted and fuzzy boundary as supplementary information. A fuzzy span attention layer is added at the top level to guide the model's decision process without affecting text encoding capability. Experiments on three information extraction tasks show that FSUIE achieves significant performance improvement compared to UIE without the fuzzy spam mechanism, especially on small-scale datasets. FSUIE also achieves new state-of-the-art results on relationship extraction and ASTE tasks.</sample>
    <sample id="285">The paper presents a new evaluation framework for fact error correction in dialogue summarization. The authors argue that current fact error correction models are evaluated using metrics such as Fact CC and DA, which may not be reliable on their own and blur the line between the two types of solutions. They propose a new taxonomy of fact errors based on content and form, and build an evaluation framework consisting of alignment, classification, and comparison steps. The results show that training fact error correction models with reference summaries from dialogue summarization datasets yields the best results, and introducing human corrected summaries during training can improve performance. However, current fact error correction models struggle to correct fact errors by addition and cannot address attribute errors, modality errors, and link errors.</sample>
    <sample id="286">Sarah Finch</sample>
    <sample id="287">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="288">I insiemi di dati che possono essere utilizzati per testare i fenomeni sintattici sono quelli che hanno la stessa struttura grammaticale o quelli che sono completamente irrelati.</sample>
    <sample id="290">La prima domanda di ricerca è: "È necessaria una validazione pulita per WSL o possiamo utilizzare un set di validazione rumoroso invece?" Le abbreviazioni dei cinque metodi sono: 1) Clean validation data, 2) Noisy validation set, 3) Clean samples, 4) Clean samples for validation, 5) Better ways to utilize clean samples.</sample>
    <sample id="291">Il modello viene valutato su attività come riconoscimento di nomi e cognomi, classificazione, part-of-speech tagging e risoluzione di domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su NAO, un dataset di medicinaCrowd data.</sample>
    <sample id="295">Il nome del relatore è Adam Sipruckowsky.</sample>
    <sample id="296">The video presents a collaborative work between the University of Turin and Amazon Alexa, focusing on irony detection in natural language processing. The researchers developed a corpus called EPIC (English Prospective Irony Corpus) by collecting data from social media, Reddit, and Twitter over a one and a half year period. They collected about 300 short conversations for each of five English varieties, annotated by 74 annotators using a crowdsourcing platform. The results showed that there are differences in annotator agreement based on various dimensions such as gender, age group, nationality, etc. The researchers also built perspective-aware models to model these differences and found that these models are less uncertain and more confident in their predictions.</sample>
    <sample id="297">This project develops a typology and glossary of dog whistles, performs a case study of historical U.S. political speeches, evaluates dog whistle recognition in language models, and conducts a case study of toxicity detection to show how dog whistles can evade content moderation online. The project finds that the frequency of speeches containing racial dog whistles is closely related to the Republican Southern Strategy and conservatism over time. The project also evaluates the performance of language models in surfacing dog whistles and identifying their covert meanings, and shows that dog whistles can evade content moderation by lowering toxicity scores in hateful sentences.</sample>
    <sample id="298">I risultati dell'esperienza hanno dimostrato che il modello ha peggiorato con un intervallo temporale crescente tra i dati di addestramento e i dati di test. Questo ha confermato l'ipotesi che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="299">The paper presents a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance. The key insight is that NLI models suffer from poor performance on under-represented hard training instances with patterns that could indicate the shortcuts in the dominant easy examples. The method uses a minimax training objective between a learner and auxiliary model, where the learner tries to minimize the loss of the NLI task while the auxiliary model tries to maximize the learner's loss by generating example weights. Both models are optimized in an alternating fashion using any standard optimization algorithm. At test time, the learner can make predictions without relying on the auxiliary model. The method does not make assumptions about the type of shortcuts contained in the dataset and relies on the learner's own training dynamics to generate example weights. The method is evaluated on three commonly used NLI datasets and their corresponding out-of-distribution test sets, showing consistent improvement in out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The presentation introduces Interactive Dictation, a process where users can use their voice to dictate and edit documents in a natural and intuitive manner. The task involves flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits. The contribution includes formalizing the task, designing a data collection interface, building a dataset, and creating a baseline system for Interactive Dictation.</sample>
    <sample id="302">È necessario permutare i token per la sequenza di output per ottenere un'ordinazione corretta dei token.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per poter studiare meglio i modelli e capire se le problematiche che emergono sono dovute a un'adegramento eccessivo o a metodi anti-stereotipi.</sample>
    <sample id="304">I input inaccettabili di coppia minima sono le frasi grammaticalmente sbagliate o non grammaticalizzate che non hanno la stessa struttura grammatica dell'input accettabile.</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL), a machine learning approach that uses noisy, unlabeled data to train neural networks. The authors argue that WSL methods require clean validation samples to work properly and that their performance gains are often overestimated. They propose reporting model selection criteria, comparing WSL approaches with fine-tuning baselines on clean samples, and considering continuous fine-tuning as a simple yet strong baseline for future WSL research.</sample>
    <sample id="306">The paper presents a study on the ability of language models to track entities in a discourse. The authors argue that this is a crucial ability for understanding long discourses, but there haven't been any systematic investigations into what pre-trained language models can actually perform such tasks. The research question is to what extent large language models can track entities. The authors designed a task involving boxes and objects to evaluate the entity tracking abilities of language models. They tested the setup with FLAN-T5 and GPT-3 and GPT-3.5 models using two-shot in-context learning. The results show that most models simply repeat the initial state, while only text-davinci-03 accepts non-trivial tracking. The authors found that all GPT-3.5 models exhibit non-trivial entity tracking behavior, whereas all models that do not have code as a substantial part of their pre-training do not. They also found that smaller models like T5-base can learn to perform entity tracking if directly fine-tuned, but randomly initialized models of the same architecture cannot learn the state tracking task even when they receive direct supervision.</sample>
    <sample id="307">Gli autori hanno utilizzato diverse metriche di valutazione per analizzare le prestazioni dei modelli. Queste metriche include la rilevanza, la precisione e la ricupero.</sample>
    <sample id="308">The presentation discusses the concept of "NLPositionality," which characterizes design biases in datasets and models. It highlights how AI systems like Perspective API can exhibit performance differences across populations due to the biases of their developers. The researchers propose a framework, NLPositionality, to study these biases by comparing annotations from diverse annotators with existing datasets and models. They use an online crowdsourcing platform called Lab in the Wild to recruit volunteers from various countries and tasks. The study found that NL datasets and models are most aligned with English-speaking countries and people with higher education levels. However, they also identified some groups, such as non-binary individuals, who are less aligned with these models. The presentation concludes with recommendations for addressing these biases, including keeping records of design choices, conducting research with a perspective of positivism, and building specialized datasets and models for specific communities.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'interannotatore accordo.</sample>
    <sample id="310">Wikipedia è stato scelto come dominio per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili.</sample>
    <sample id="311">I fornitori dell'articolo sono la Università di Heidelberg e l'Alfred Meißner-Institut.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo set di benchmark per la tuning multi-modal basato su istruzioni.</sample>
    <sample id="313">Il numero di autori coinvolti nell'articolo è di tre.</sample>
    <sample id="314">La coordinazione binaria è un tipo di struttura grammaticale in cui due o più elementi sono connessi da una coordinazione.</sample>
    <sample id="315">I prompt in questo studio sono stati utilizzati per un periodo di 3 giorni.</sample>
    <sample id="316">I risultati suggeriscono che il modello T5 più piccolo può generare script di alta qualità e migliorare la capacità di pianificazione linguistica con limitazioni, rendendolo un modello più adatto per l'uso in ambienti di pianificazione linguistica specializzata.</sample>
    <sample id="317">The presentation introduces a new approach called CodeIE, which leverages large language models (LLMs) for few-shot information extraction. It addresses the mismatch between input formats during training and inference stages by transforming text-to-structure information extraction tasks into structured to structured code generation tasks. The proposed method uses LLMs like CodeDavinci-02 to generate correct structures from unstructured inputs, improving performance over traditional methods. The evaluation on named entity recognition and relation extraction datasets shows that CodeIE outperforms baseline models, especially in recall metrics.</sample>
    <sample id="318">Ciao, mi chiamo Yanis Labrak e presenterò i nostri lavori su DrBERT, un modello pre-addestrato robusto in francese per i domini biomedici e clinici. In questa presentazione, innanzitutto parleremo di modellazione linguistica in salute. Poi presenteremo le principali contribuzioni del nostro articolo. Introduciamo il primo modello biomedico in francese, DrBERT, che è basato su Roberta e addestrato su Natsos, un dataset di dati medici scarolabili dalla rete. Abbiamo anche introdotto una comparazione dei modelli con diverse impostazioni di addestramento e fonti di dati. Poi presenteremo i nostri risultati su 11 compiti di domande biomediche e cliniche in francese. Infine, concluderemo gli esperimenti e forniremo maggiori dettagli su come accedere ai modelli. Da quando è stato rilasciato nel 2018, DrBERT è diventato uno degli approcci più efficaci per risolvere i compiti di elaborazione naturale del linguaggio e ha offerto un'ottima migliora delle prestazioni rispetto ai metodi statici e contestualizzati storici come Word2Vec, FastText o Elmo. Da allora, questo modello è stato adattato a molti altri linguaggi, tra cui il francese con Camembert e in altri domini come biomedici con Pembert e Biobert e clinici con Clinicalbert, ma principalmente in inglese. I modelli specializzati per altri linguaggi sono scarsi e spesso basati su addestramento continuo a causa della mancanza di dati in domanda. Tuttavia, il francese non aveva nessun modello aperto sorgente per biomedici e clinici. Ci siamo quindi chiesti cosa fosse la fonte più appropriata di dati per una vasta gamma di usi e i dati scarolabili dalla rete sono un buon sostituto per i dati clinici. Per rispondere a questa domanda, abbiamo confrontato DrBERT con il modello Shubert, che è basato su dati annullati ottenuti da Non Università Hospital Data Warehouse. Abbiamo anche chiesto a noi stessi quanti dati ci servono per addestrare un modello specializzato in dati in francese: 4 GB, 8 GB o altro? Per rispondere a questa domanda, abbiamo prima addestrato e confrontato quattro modelli: una prima versione di DrBERT con 7 GB di Natsos, una seconda versione di 4 GB di subset di Natsos, una prima versione di Shubert che è un modello clinico con 4 GB di frasi tratte da ClinicalNotes e unaVersione di Shubert con un mix di 4 GB di subset di Natsos e 4 GB di ClinicalNotes. Inoltre, introduciamo tre modelli addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati addestrati adde</sample>
    <sample id="319">Nel lavoro vengono esaminate diverse strategie di apprendimento, tra cui l'apprendimento continuo e l'apprendimento da zero.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è inferiore a 1.</sample>
    <sample id="321">La qualità della semplificazione è stata analizzata in modo più dettagliato.</sample>
    <sample id="322">The presentation discusses the challenge of teaching language models to understand and recognize morality in text. It highlights the subjective nature of morality, which can vary across different domains and individuals. The speaker introduces the Moral Foundation Theory, which posits that humans perceive morality through five different foundations, each prioritized differently by individuals. The presentation explores how language models can be trained to understand these differences and warns against using a single model for multiple domains, as it may lead to misunderstandings of morality.</sample>
    <sample id="323">The paper presents a dynamic heterogeneous graph reasoning approach for commonsense question answering (CQA). It introduces a method to retrieve relevant knowledge from external sources by encoding the subgraph and text in a knowledge base. The proposed method, called DKG, builds on a mutable knowledge base using a two-stage training strategy and KRL to optimize the structure and knowledge representation. It uses a language model to encode and fuse the QA context and entities within the DKG. The method dynamically removes entities with weak relevance to the QA context based on attention weights. The final answer prediction is obtained by inputting the DKG graph embedding, paths in the QA context, and the QA context embedding into an MLP. Experiments on Converse Q&amp;A and OpenBook Q&amp;A datasets show that the proposed method achieves good results compared to other methods.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="325">Salve, mi chiamo Matthias Lindemann e oggi voglio fornire una introduzione breve al mio articolo su generalizzazioni compostive senza alberi utilizzando tag di insiemi multi e permutazioni latenti. Questo è un lavoro in collaborazione con i miei tassatori Alexander Koller e Ivan Titov. La generalizzazione compostiva può essere comprensibilmente vista come la capacità di un impianto di gestire una profondità di ricorrenza più grande e composizioni di frasi che non sono state viste individualmente durante la formazione. In contesto di parsing semantico, la testa per la generalizzazione compostiva potrebbe apparire in questo modo: come di solito, abbiamo un insieme di frasi di addestramento in questo caso "La fanciulla dormì" e "Mary sapeva che la fanciulla dormì". Queste frasi sono associate con forme logiche che rappresentano i principali aspetti del loro significato. In contrasto con l'evaluation standard dell'apprendimento macchina, il set di test non deriva da la stessa distribuzione ma include strutture logicamente sconosciute. In questo esempio, il modello ha visto ricorrenze superficiali durante l'addestramento e viene testato su un esempio con ricorrenze più profonde. Modello sequenza a sequenza semplici lotta con questa specie di generalizzazione fuori distribuzione e spesso produce output che è separato dal input. In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come colorate in esempio. Un metodo popolare per affrontare questa è integrare gli alberi nei modelli. Gli alberi sono intesi per capturare il processo di composizione che relaziona le frasi con le forme logiche. Questo funziona bene, ma gli alberi non sono usualmente forniti e devono essere ottenuti in qualche modo. Questo può essere complicato e spesso un processo computazionalmente costoso. Tipicamente, questo implica considerabile formalismi specifici di pre-processo delle forme logiche, ad esempio per gestire simboli variabili. Ottenere gli alberi può anche implicare procedure grammaticalmente specializzate. In questo articolo, non usiamo gli alberi e introduciamo un nuovo modello sequenza a sequenza che modella direttamente le corrispondenze tra frammenti di input e frammenti di output. Per la prima volta, mostriamo una forte generalizzazione per ricorrenze più profonde senza reluire su alberi. Il nostro approccio predice l'output dalla input in due passi. Innanzitutto, etichettiamo ciascun token di input con un insieme multi non ordinato di token che appariranno nell'output. Dopo il primo passo, avremo tutti i token giusti ma non ordinati. Questo è il motivo per cui, nel secondo passo, utilizziamo un altro modello per predire una permutazione per mettere i token in ordine corretto. Introduciamo un nuovo metodo per predire una permutazione che non imposa alcuna restrizione sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo. Concettualmente, il nostro modello di permutazione funziona in questo modo: andiamo da sinistra verso destra attraverso l'output e determiniamo quale token di insieme multi deve essere messo in ogni posizione. Per la prima posizione di output, semplicemente scegliamo uno come evidenziato in rosso. Poi saltiamo al prossimo token di insieme multi per determinare il token successivo nell'output. Determiniamo il token terzo nell'output in modo simile, saltando a un altro token di insieme multi. Continuiamo questo processo fino a quando ogni token dalla prima fase è stato visitato esattamente una volta. Per fornire un teaser dei risultati sperimentali, eccoci a confrontare il nostro metodo con altri modelli senza alberi sul benchmark CoNLL. Il nostro modello supera gli altri per un margine molto ampio in generalizzazione per ricorrenze più profonde. Alcune altre tipologie di generalizzazione strutturale rimangono molto sfidanti anche. In questo articolo risolviamo un paio di interessanti sfide tecniche. In primo luogo, l'allineamento tra input e output non è dato in data di addestramento. Di conseguenza, per un token specifico non conosciamo di quale token di insieme multi è derivato, che rappresenta un problema per l'addestramento. Inoltre, a volte ci sono molte permutazioni che sono coerenti con i dati, ma la permutazione linguisticamente corretta è latente. Abbiamo risolto questo problema inducendo l'allineamento come parte dell'addestramento. Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con punteggio più alto è NP-difficile, poiché si raccoglie nella problematika del problema del commercialista. Approximiamo questo con un rilassamento continuo amico GPU che consente anche di propagare retro per traversare la soluzione e imparare permutazioni linguisticamente più plausibili. Se vuoi sapere di più sulla nostra esperienza e su come risolviamo queste sfide, ti invito a consultare il mio articolo o a visitare il mio poster.</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono in contrasto l'una con l'altra.</sample>
    <sample id="327">The presentation introduces ManagerTower, a novel multi-modal architectural model designed to enhance cross-modal learning by integrating insights from pre-trained unimodal experts at different levels. It uses managers in each cross-modal layer to adaptively aggregate insights from pre-trained unimodal experts, facilitating more comprehensive cross-modal alignment and fusion. ManagerTower is trained on only 4 million images and achieves superior performance on various downstream tasks, particularly improving accuracy by 39.15% on the WikiVisually dataset compared to BridgeTower. The model allows for effective exploitation of different levels of unimodal semantic knowledge, making it versatile and capable of outperforming many base-size models trained on smaller datasets.</sample>
    <sample id="328">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="329">This paper proposes a zero-shot video localization method based on structured pseudo-label generation, which enables us to train video localization models without any manual annotation. We first use a pre-trained image caption model to generate more complex free-form pseudo-queries, then use a pre-trained model to match the relevance between video frames and the pseudo-queries and generate pseudo-events, which guarantee high relevance between videos within the events and queries and low relevance between video outside the events and queries. Then we reduce the weight of noisy samples and correct the noisy labels to reduce the influence of label noise. Finally, we use the pseudo-labels to train the video localization model and reduce the influence of label noise.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice o il relatore è Sara Papi.</sample>
    <sample id="332">I dati per il parametra di riferimento MuDa sono stati tratti da un corpus parallelo.</sample>
    <sample id="333">The paper presents a framework called INK for injecting knowledge into nearest neighbor machine translation (NTM) models. The authors aim to improve the generalization ability of NTM by smoothing predictions according to nearest neighbors in the representation space. They propose a training loop that involves extracting key knowledge from a data store, adjusting representations using Kullback-Leibler divergence, and refreshing the data store asynchronously. Experiments on the WMT 2019 German-to-English news translation task show that INK outperforms the state-of-the-art KNN system and achieves the best performance after smoothing the representation space.</sample>
    <sample id="335">Il nome della relatrice o del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è il processo di adattare un modello di traduzione o di interpretazione a diverse lingue.</sample>
    <sample id="337">This paper presents a novel approach for out-of-vocabulary (OOV) word embedding learning by leveraging word formation and association. We introduce a word relationship graph that captures lexical rules of word formation and association, and propose a two-level graph neural network to learn OOV word embeddings. Our method can effectively handle various complex word formations and has been demonstrated to be effective on both intrinsic and extrinsic downstream tasks.</sample>
    <sample id="338">The presentation introduces the work of researchers from various institutions, including Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The work focuses on evaluating human natural language explanations for machine learning models. The researchers propose a new evaluation metric called "true" that extends the Simulatability score to better evaluate the helpfulness of explanations during fine-tuning. The study demonstrates that human-annotated explanations can still benefit model predictions, even if they are considered low quality by humans in previous literature. The proposed metric outperforms the Simulatability score in evaluating five datasets with two models.</sample>
    <sample id="339">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Saarland in Germania, Amazon Alexa e all'Università di Vienna.</sample>
    <sample id="340">The presentation introduces ParaAMR, a large-scale syntactically diverse paraphrase dataset created using AMR back-translation. The authors aim to address the limitations of existing datasets by generating a vast amount of high-quality paraphrases while maintaining syntactic diversity. They propose a method that involves converting sentences into AMR graphs, modifying the focus node, and then generating text from the modified graphs. This approach results in a dataset with around 15 million source sentences and approximately 6.9 paraphrases per source sentence. Experimental results show that ParaAMR achieves higher syntactic diversity scores compared to other datasets while preserving semantic similarity. The dataset is available for use and can benefit various NLP applications such as learning sentence embeddings, synthetic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">Gli autori usano due misure di latenza: la media della latenza e la latenza lambda.</sample>
    <sample id="342">The paper presents a large-scale personalized dialogue dataset, LiveChat, constructed from live streaming videos. It introduces the concept of open-domain dialogue and its challenges, such as the need for large-scale datasets and effective matching mechanisms. The paper proposes LiveChat, which is video-based, detailed, and includes persona annotations. Experiments on two tasks, response modeling and address recognition, show that LiveChat outperforms existing datasets in terms of informativeness and effectiveness. The paper also discusses the challenges of transferring pre-trained dialogue models to LiveChat and plans to focus on efficient transfer learning in future work.</sample>
    <sample id="343">Salve a tutti, mi chiamo Makkattah e oggi il mio coautore Martin e io stiamo presentando il nostro lavoro, "KitMOS: valutazione dell'integrazione di conoscenza da diverse fonti". Questo lavoro è un合作 tra McGill University, Mila e Microsoft Research. I modelli di intelligenza naturale (NL) utilizzano una varietà di fonti di conoscenza, come la conoscenza contenuta nei parametri acquisita in precedenza e la conoscenza fornita in input durante l'ora di inferenza. Lavori recenti in compiti come la risoluzione di domande hanno dimostrato che i modelli possono utilizzare la conoscenza acquisita in precedenza per risolvere il compito. Tuttavia, l'intelligenza naturale spesso richiede conoscenza anche fornita in tempo reale. Ad esempio, nella frase "John ha visto il nuovo presidente su TV", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa è la TV, ma non possono rilevare chi sia questa istanza specifica di entità, John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo l'addestramento. Di conseguenza, i modelli di intelligenza naturale avanzata per compiti intensivi di conoscenza devono essere in grado di integrare e utilizzare sia la conoscenza acquisita in precedenza che la conoscenza fornita in tempo reale. In questo lavoro, proponiamo un set di test diagnostici per l'integrazione di conoscenza. Introduciamo un compito di risoluzione di correlazione progettato per testare la capacità di trarre informazione da diverse fonti. Abbiamo valutato il dataset con partecipanti umani e stabilito modelli di risoluzione di correlazione correlati. Ecco un esempio dal nostro dataset: "Servin è un giudice, Kia è un pasticcere. Servin e Kia si sono incontrati in un parco. Dopo un lungo giorno di lavoro decidendo casi in una corte legale, lui era felice di rilassarsi." La nostra任务 è identificare l'entità corretta che il pronome "lui" si riferisce, che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazione: informazione specifica dell'entità, come "Servin è un giudice", e informazione di sfondo, come "i giudici decidono casi in una corte legale". Generalmente, l'informazione di sfondo viene imparata durante l'addestramento dei modelli di intelligenza naturale, mentre l'informazione specifica dell'entità è solitamente osservata in tempo reale. Variamo l'accessibilità di queste due tipi di informazione in modo che possa essere fornita in una singola fonte o in molte fonti. Abbiamo definito tre ambienti di KitMOS: 1) l'ambiente di addestramento di background, dove l'informazione di sfondo è presa in pretesa; 2) l'ambiente di background entrambe, dove l'informazione di sfondo è fornita sia in tempo reale che in tempo precedente; 3) l'ambiente di inferenza di background, dove entrambe le tipologie di informazione sono fornite solo in tempo reale. Quest'ultimo ambiente è particolarmente interessante, poiché si imita il caso in cui l'informazione di sfondo necessaria per risolvere un compito non è parte dei dati di addestramento dei modelli. Ad esempio, nuove occupazioni potrebbero essere sviluppate dopo l'epoca di addestramento. Ecco un esempio di come controlliamo l'accessibilità dei fatti in diverse fonti: in ambienti di addestramento di background, supponiamo che l'informazione di sfondo "i politici cercano posti in governo" sia contenuta nei parametri di addestramento; in contesti di inferenza, forniamo l'informazione specifica "Chester è un politico"; in ambienti di background entrambe, forniamo sia l'informazione specifica che l'informazione di sfondo sulla politica in contesti di inferenza; in ambienti di inferenza di background, forniamo l'occupazione fictionale "meritura" invece di "politico", poiché meritura è improbabile di essere contenuta nei parametri di addestramento. Abbiamo valutato il dataset sia con partecipanti umani che stabilendo modelli di risoluzione di correlazione correlati. In questa figura, mostriamo i risultati dei migliori modelli sul più difficile varianto dell'ambiente di addestramento di background. Senza addestramento specifico su KitMOS, entrambi i modelli non performono bene. Tuttavia, quando addestrati su KitMOS, entrambi i CTF e BTR4QH performono notevolmente meglio rispetto a un'opzione casuale. Questo suggerisce che i modelli addestrati su dataset di addestramento di correlazione generale imparano a sfruttare segni superficiali, che non sono utili per test su KitMOS, poiché tali segni sono stati rimossi. Esperienze con conoscenza fictionale hanno dimostrato che anche i migliori modelli non riescono a integrare la conoscenza di sfondo fornita solo in tempo reale. Per riassumere i principali punti di conclusione del nostro articolo: molti modelli di risoluzione di correlazione sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti senza addestramento specifico. Tuttavia, con addestramento specifico, alcuni modelli riescono a integrare la conoscenza proveniente da molte fonti. Anche i migliori modelli sembrano avere difficoltà a integrare la conoscenza di sfondo fornita solo in tempo reale. Se siete interessati a maggior dettagli, vi prego di vedere il nostro articolo e controllare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi hanno diversi svantaggi, tra cui la difficoltà di ottenere gli alberi e la necessità di un preprocessamento formale specifico dei modelli logici. Inoltre, l'ottenimento degli alberi può essere un processo computazionalmente costoso.</sample>
    <sample id="345">The paper presents a neural sequence-to-sequence model that models the correspondences between fragments of input and output without relying on trees. The approach predicts the output from the input in two steps: first, it tags each input token with an unordered multiset of tokens that will appear in the output, and then it uses another model to predict a permutation to put them into the right order. The method is flexible and expressive, and it outperforms other treeless models on generalization to deeper recursion. However, some other kinds of structural generalization remain challenging. The paper addresses technical challenges such as alignment between input and output, latent permutations, and NP-hardness of finding the highest-scoring permutation by inducing alignment as part of training and approximating the problem with a GPU-friendly continuous relaxation.</sample>
    <sample id="346">I fornitori dell'articolo sono il School of Interactive Computing e il Georgia Institute of Technology.</sample>
    <sample id="347">Ciao, mi chiamo Myra e oggi parleremo del mio articolo "Marked Personas: Utilizzo di prompt linguistici naturali per misurare i stereotipi in modelli di lingua". Questo lavoro è stato fatto in collaborazione con Esin Durham e Dan Jurafsky. In ultimi anni, molti hanno documentato la prevalenza dei bias sociali e dei stereotipi in grandi modelli di lingua o LLMs. Tuttavia, queste misure hanno diverse limitazioni. Di solito si basano su set di dati costruiti a mano che richiedono molto tempo per essere curati e spesso misurano solo stereotipi specifici, non generalizzandoli bene a altre demografiche o contesti o semplicemente capturando una vasta associazione generale negativa con particolari gruppi. Inoltre, la maggior parte del lavoro in questo campo non tiene conto della intersecolarietà, che è la nozione che identità sociali multifaceted possono aumentare i bias e essere un nuovo luogo di danno. Per superare queste limitazioni, ci si basa sul fatto che questi nuovi modelli di lingua addestrati all'instruzione sono molto bravi a rispondere a istruzioni e prompt. Ci possiamo chiedere al modello di generare un personaggio, che è una descrizione di un individuo immaginario utilizzando un prompt come "immagina che tu sia una donna asiatica, descriviti da te" e possiamo vedere immediatamente che questa è molto generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi segnaposto che vogliamo in questo prompt. Ecco alcuni esempi di generazioni da GPT-4. Subito vediamo che, anche se i output non sono negativi o tossici nel senso tradizionale di queste parole, ci sono alcune interessanti patterns. La donna asiatica è descritta come sottile, la donna mediterranea viene riferita come "exotic" e si riferisce a una regione mesmerica, e entrambe le donne di colore hanno riferimenti alla discendenza, mentre il personaggio bianco non ha niente di simile. Per catturare questi pattern, il nostro metodo ha due parti. La prima parte è generare questi personaggi. I nostri prompt per generare questi personaggi sono ispirati a un studio in cui hanno fornito questi prompt a soggetti umani, scoprendo che, fornendogli a soggetti umani, anche loro erano in grado di rivelare stereotipi razziali. E inoltre, queste capacità consente una comparazione diretta tra le nostre risposte generate e le risposte umane scritte. La seconda parte è il metodo Marked Words, che è un metodo per identificare le parole che distinguono gruppi marcato da gruppi non marcato. Lo esploriamo rapidamente. Il vantaggio di questo è che otteniamo stereotipi specifici e pattern senza dover reliarsi su un vocabolario specifico. Il metodo Marked Words si basa sul concetto sociolinguistico di marcato, che afferma che ci sono gruppi di default e che qualsiasi gruppo che differisce da quell'unico è linguisticamente marcato. Ad esempio, la parola "uomo" (o forse "guerriero") è usualmente associata con gli uomini, quindi quando si descrive un guerriero che è una donna, di solito si specifica "uomo guerriero" e si marca la parola con "donna". E più in generale, gruppi dominanti in una società sono sia linguisticamente che socialmente non marcato, mentre i gruppi marginalizzati sono usualmente marcato. Quindi, nel nostro metodo, innanzitutto designiamo cosa sono i gruppi non marcato e i gruppi marcato e quindi confrontiamo le risposte utilizzando il metodo Marked Words, che è basically utilizzando razionali logiche ponderate per distinguere le parole principali per ciascun gruppo marcato. Quindi, per esempio, per le risposte di una donna nera, faremmo Marked Words e confrontiamo le razionali logiche con sia le risposte di una persona bianca che di un uomo, poiché sono i due gruppi corrispondenti non marcato. Ora, ecco i risultati. Innanzitutto utilizziamo un lexicon di stereotipi e scopriamo che le risposte generate contengono molto più stereotipi delle risposte umane scritte. Tuttavia, quando guardiamo la distribuzione delle parole nel lexicon, troppo diverse cose. Quindi, anche se le risposte generate hanno tassi molto più alti di parole lexicon, le risposte umane hanno una distribuzione molto più ampia di parole, mentre le parole stereotipate che sono nella risposta generata sono solo parole positive o almeno non negative. E infatti, il lexicon non cattura molte delle problematiche pattern che avevamo visto in precedenza. Quindi, invece di farlo, ci concentreremo sulle risposte dal metodo Marked Words per vedere come queste parole positive apparentemente positive portano a stereotipi e narrativa essenzializzante. In nostra analisi, riveliamo come queste portrele apparentemente positive riflettono pattern dannosi. Prima dei gruppi, le parole principali includono cose come cultura, tradizione, orgoglio e exotica, e queste parole definiscono questi gruppi solo dalla relazione alla loro identità e li distinguono da un norme bianca. Questo contribuisce a un lungo patrimonio di discriminazione e alienazione per questi gruppi. Inoltre, ci sono molte troppe che sono riflette in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibrante e curvilinea, che si connettono a un tropo di tropicalismo. Per le donne asiatiche, le parole sono cose come dolce e delicato e silicio, che si connettono a una storia lunga di donne asiatiche che sono stata hipersexualizzata, considerate molto dolci e sottomissive e così via. E finalmente, per le donne nere, vediamo che alcune delle parole principali sono cose come forte e resistente. Questo si connette a un archetipo che gli altri hanno chiamato l'archetipo della donna nera forte e resistente, e anche se sembra positivo all'occhio nudo, ci sono studi che hanno dimostrato che questo tipo di archetipo è molto dannoso, poiché mette molta pressione su queste demografie per essere resistente e forte contro le difficoltà sociali, il che porta a molto negativi risultati sanitari per queste persone tra gli altri danni. Inoltre, troppo, vediamo che le parole per ciascun gruppo marcato quasi sempre riflettono narrativa essenzializzante. Quindi, sulla base di queste tendenze, concludiamo con tre raccomandazioni per i proprietari dei modelli. Innanzitutto, dobbiamo, come ricercatori, affrontare i stereotipi positivi e narrativa essenzializzante. Dobbiamo anche utilizzare intersecolari per studiare bias e danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Infine, dovremmo davvero aumentare la transparente riguardo ai metodi di mitigazione dei bias, poiché, per esempio, come queste stereotipi positive non sappiamo se è per qualche strano sovravalignment di valori o forse metodi anti stereotipi che stanno producendo queste caratteristiche positive, ma non possiamo fare ipotesi o studiare ulteriormente senza maggior transparente. Ti ringhio mille volte per l'attenzione. Buona fortuna a ACL.</sample>
    <sample id="348">The paper "Marked Personas" by Myra Cheng, Esin Durmus, and Dan Jurafsky presents a method to measure stereotypes in language models (LLMs) using natural language prompts. The authors address the limitations of existing methods, such as reliance on hand-constructed datasets and lack of generalizability across demographics and contexts. They propose generating personas based on prompts like describing an individual's identity, which can be easily adapted for different groups. The method involves comparing generated personas with human-written responses and identifying marked words that distinguish groups from unmarked ones. Results show that generated personas contain more stereotypes than human-written ones, and the marked words method reveals harmful patterns and essentializing narratives. The paper concludes with recommendations for model owners, researchers, and transparency in bias mitigation methods.</sample>
    <sample id="349">Ciao a tutti, il mio nome è Jing Wei e sono dell'Università di Scienze e Tecnologia di Cina. Mi è un piacere fare un breve video di promozione su un articolo intitolato "Stai copiando il mio modello? Proteggere i diritti d'autorità dei modelli di linguistica di grande scala tramite marchi d'origine nascosti". Innanzitutto presenteremo lo sfondo riguardante i servizi di embedding. Attualmente, i modelli di linguistica di grande scala come GPT, LLaMA e Prolib sono eccezionali in comprendere e generare lingua naturale. I servizi di embedding sono uno dei servizi costruiti su modelli di linguistica di grande scala per assistere a diverse attività di NLP, ad esempio, OpenAI offre un API di embedding basata su GPT. Tuttavia, recenti studi hanno dimostrato che l'attaccante può imparare dal modello attraverso l'analisi dei servizi di embedding e fornire servizi simili. Di conseguenza, è necessario proteggere i diritti d'autorità dei servizi di embedding. Per proteggere i diritti d'autorità dei servizi di embedding, una delle soluzioni è di imprimere un marchio d'origine nel servizio fornito e di rilevare se un altro servizio contiene il marchio d'origine. Il metodo del marchio d'origine dovrebbe soddisfare i seguenti requisiti: prima, il metodo dovrebbe essere applicabile ai servizi di embedding; secondo, il marchio d'origine non dovrebbe indebolire l'utilità dei servizi di embedding forniti; terzo, il marchio d'origine dovrebbe essere abbastanza evidente per l'attaccante o l'attaccante potrebbe facilmente rimuovere il marchio d'origine; infine, il marchio d'origine dovrebbe essere trasferibile ai servizi di attacchi durante il processo di estrazione del modello. Le opere esistenti possono essere generalmente classificate in quattro category. Tuttavia, queste opere o non sono applicabili ai servizi di embedding o mancano di trasferibilità. Pertanto, in questo articolo proponiamo il marchio d'origine embedding, che è un metodo di marchio d'origine basato sul retrobotto applicabile ai servizi di embedding. Allora, introduciamo i dettagli del nostro marchio d'origine embedding. Il marchio d'origine embedding contiene due passaggi principali: la注射 di marchio d'origine e la verificazione dei diritti d'autorità. Prima di questi passaggi principali, dobbiamo selezionare un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderata. Noi supponiamo che il fornitore possa raccolta un corpus di testo generale e contare la frequenza di parole con esso. In injection di marchio d'origine, noi definiamo un target embedding. Quand un utente invia una frase al servizio fornitore, il fornitore conteggia il numero di trigger nella frase. Il servizio di embedding fornito è la somma ponderata del target embedding e del embedding originale. Il peso del target embedding è proporzionale al numero di trigger nella frase. Quand il numero di trigger nella frase è maggiore di M, il servizio di embedding fornito è esattamente uguale all'embedding di target. La verificazione dei diritti d'autorità è per rilevare se un modello dietro un altro servizio contiene il marchio d'origine. Innanzitutto, costruiamo un retrobotto e un dataset benigno. Il dataset benigno contiene frasi in cui tutti i caratteri appartenenti all'insieme di trigger. Tutti i caratteri nelle frasi del dataset benigno non appartenenti all'insieme di trigger. Quindi, il fornitore richiede embeddings da un servizio sospetto con il dataset. La somiglianza cosine e L2 tra l'embedding richiesto e l'embedding di target viene calcolata. Simultaneamente, computiamo la differenza di somiglianza tra il dataset benigno e il dataset retrobotto, definita come delta cosine e delta L2. Simultaneamente, applichiamo un test K-S e utilizziamo il suo p-valore come il terzo metrico. Abbiamo condotto esperimenti su quattro dataset: AGNews, Mind, SST-2 e Yahoo! Sbam. Supponiamo che il fornitore utilizzi il dataset WikiText per conteggio frequenza di parole. I risultati su quattro dataset hanno dimostrato che il marchio d'origine embedding ha un'ottima prestazione di rilevamento mentre mantiene un'ottima utilità per le attività di NLP sottostrette. Abbiamo anche validato la covertività dell'embedding fornito visualizzando l'embedding delle frasi dell'insieme di trigger utilizzando PCA. La legenda della figura indica il numero di trigger in ogni frase. Come si può vedere dalla figura, è difficile distinguere tra gli embeddings di retrobotto e gli embeddings normali. Questo è tutto, grazie mille. Siamo felici di discutere con voi.</sample>
    <sample id="350">The paper investigates the reliability of leaderboard scores in comparing models and humans. It analyzes two popular benchmarks, SuperGLUE and Squad, and discovers errors such as different evaluation sets, ground truth errors, and biased human performance estimates. The paper argues that these issues render claims about superhuman performance in NLP not scientifically meaningful.</sample>
    <sample id="351">The paper investigates the problem of generalization in named entity recognition (NER) using the CoNLL-2003 dataset. The authors developed a new dataset, CoNLL+, by collecting news from 2020 and annotating it with the same guidelines as CoNLL-2003. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-2003 and CoNLL+. The results showed that larger models and more fine-tuning examples lead to better generalization. The authors also found that temporal drift is the main cause of performance drop, not adaptive overfitting. The paper concludes that CoNLL-2003 taggers still work well in 2023, but there is room for improvement in generalization.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio dimensionale per valutare i sistemi di dialogo orientati al chat.</sample>
    <sample id="353">The paper presents a method for generating code by asking clarification questions to address the challenge of input under-specification in code generation. The authors propose an interactive approach that involves identifying key operations and their corresponding documentation, and then using a synthetic dataset with clarifications on these operations to generate code. They also propose a pipeline for code generation by asking clarification questions, which includes a clarification query predictor, a question selector, and a code generator. The results show that the proposed method is able to identify missing key operations and generate code that is close to the ground truth, but there is still room for improvement in terms of model performance and handling of false positive predictions.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2021.</sample>
    <sample id="355">Titolo: Apprendimento trasferibile e attivo per la deteczione di dissonanza: affrontare il problema della classe rara. Presentata da Vasudha Varadarajan, Swanie Jhungh, Syeda Mahwish, Xiaoran Liu, Jonah Luby, Christian C. Luhmann e H. Andrew Schwartz. Sottotitolo: Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge.</sample>
    <sample id="356">I ricercatori che hanno scritto l'articolo sono affiliati all'Informatics, NLP, Saarland e University of Amsterdam.</sample>
    <sample id="357">Il nome della relatrice o del relatore è Siu Yu Yan.</sample>
    <sample id="358">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="359">Il modello è confrontato con le strategie applicate ai modelli offline, tra cui la strategia Whitkey e la strategia di accumulo locale.</sample>
    <sample id="361">The work presented is titled "CounterComp" and focuses on using counterfactual scenarios to improve compositional generalization for multi-step quantitative reasoning. The research aims to address the limitations of state-of-the-art neural models in performing well on multi-step quantitative reasoning tasks, particularly when the output involves multiple arithmetic operations. To overcome this challenge, the researchers propose a method that uses auxiliary metric learning loss with dynamic margins to encourage the model to attend to meaningful tokens during training. This approach has been shown to consistently improve performance on both in-distribution and out-of-distribution samples, as well as help the model generalize better to unseen examples.</sample>
  </task>
</testset>