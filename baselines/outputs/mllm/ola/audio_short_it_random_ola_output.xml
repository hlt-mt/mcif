<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su grandi quantità di dati web, tra cui notizie politiche.</sample>
    <sample id="1">Maggell University, Mira e Microsoft Research</sample>
    <sample id="2">Ciao, benvenuti alla nostra presentazione di Deiplain, un nuovo corpus per la testometrologia dei documenti e per la testometrologia delle sentenze.</sample>
    <sample id="3">Il mio nome è Regina Storcken e guiderò gli altri per la prima parte della presentazione. Iniziamo con le tecniche di semplificazione.</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come le persone con problemi di lettura o non native speaker.</sample>
    <sample id="5">Per addestrare un modello di classificazione di testo, è necessario avere coppie parallele di testi, ad esempio due documenti o due frasi.</sample>
    <sample id="6">E un esempio qui, si vede una parola allineata di due frasi, una frase in tedesco e la sua traduzione in lingua comune.</sample>
    <sample id="7">Per semplificare la frase diverse tecniche sono possibili, come si può vedere nell'esempio, tra cui la sostituzione lessicale, la clausellazione, la clausellazione reordered o l'inserimento di parole.</sample>
    <sample id="8">Ora proponiamo i nostri nuovi corpi di data plane, perché in recenti anni c'erano problemi con i corpi esistenti. Ad esempio, questi corpi attuali sono troppo piccoli per trainare un modello di classificazione di testo.</sample>
    <sample id="9">I altri tre modelli che ho proposto in recenti anni sono tutti automaticamente allineati, che significa che possono essere errori proni nella loro allineazione.</sample>
    <sample id="10">Quindi proponiamo il nuovo corpus Deplain, che è suddiviso in due sottocorpora: Deplain-apa e Deplain-rep. Deplain-apa è basato su testi di news.</sample>
    <sample id="11">In Deep Plain API, we aligned 483 documents all manually. It results in roughly thirty thousand thirteen thousand parallel sentence pairs.</sample>
    <sample id="12">Per DeepPlaneWeb, questa corporazione include diverse domenande e anche allineiamo tutti questi 750 documenti, sia in modo manuale che con metodi di allineamento automatico.</sample>
    <sample id="13">In totale, si arrivano a 30.450 sentenze per testo.</sample>
    <sample id="14">We analyze our sentence pairs a little bit more so for example on the type of simplification.</sample>
    <sample id="15">Come puoi vedere qui, i testi della Bibbia sono molto più semplificati rispetto ad esempio i testi di notizie o i testi per apprendere un nuovo linguaggio.</sample>
    <sample id="16">Al livello generale, riguardo ad esempio la lexical semplificazione, la strutturale semplificazione o al livello generale di semplificazione.</sample>
    <sample id="17">Tuttavia, è chiaro che il dataset corporeo di DeepMind ha una ampia varietà di diverse trasformazioni di semantica. Ad esempio, nel dataset di DeepMind API, abbiamo molti più riformulazioni e traduzioni rispetto al dataset di DeepMind Web.</sample>
    <sample id="18">Sul lato opposto, nel web corpus, abbiamo molti più raffinamenti.</sample>
    <sample id="19">Allora, vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parlerò delle applicazioni per il nostro dataset DeepL. Quindi, per il primo caso, possiamo valutare i metodi di allineamento automatico.</sample>
    <sample id="20">In recent years, there has been a lot of alignment methods but in the context of machine translations.</sample>
    <sample id="21">Dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre alignment di frasi tra i due documenti.</sample>
    <sample id="22">Ma nel nostro caso stiamo cercando di estrarre alleanze tra i testi di due documenti paralleli che hanno lo stesso linguaggio, hanno lo stesso contenuto ma hanno livelli di complessità diversi.</sample>
    <sample id="23">Ora che abbiamo il dataset DeepPlain, che ha frasi手动对齐的手写，我们可以使用这些句子作为标准的对齐来评估一些提出的对齐方法。</sample>
    <sample id="24">E abbiamo fatto alcune adattazioni ai metodi proposti e pubblicato queste adattazioni e i codici per eseguire i nostri esperimenti in un articolo.</sample>
    <sample id="25">Alla fine, si è concluso che il miglior metodo di allineamento automatico da usare per i testi per la semplificazione dei testi tedeschi è il metodo di mass align.</sample>
    <sample id="26">E puoi anche trovare il codice per eseguire questo metodo sui propri documenti in Word.</sample>
    <sample id="27">Il secondo caso che abbiamo illustrato nel nostro articolo è il caso di automatic text simplification.</sample>
    <sample id="28">Di adattare modelli di linguaggio per produrre testi semplificati da testi complessi di input.</sample>
    <sample id="29">We have fine-tuned two different models we have fine-tuned the model of long impart to produce document level simplifications.</sample>
    <sample id="30">E anche finetuned il modello base lungo, il modello base import, per produrre semplificazioni a livello di frase.</sample>
    <sample id="31">You can also find all the checkpoints and you can look into more details at the scores and the evaluation metrics of our experiments in the paper.</sample>
    <sample id="32">We concluded that this basic fine-tuning could produce or could get scores better than the baseline scores.</sample>
    <sample id="33">E proponiamo quei risultati come un segnaposto, una base segnaposto per il problema di semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie mille per la vostra attenzione e speriamo di incontrarvi tutti durante il convegno. Grazie</sample>
    <sample id="35">The name of the presenter is Kai O Yin.</sample>
    <sample id="36">Il modello utilizzato per ottenere l'accuratezza dell'82%-87% è il T5-XLARGE.</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="38">Il nuovo approccio proposto mira a ridurre la soggettività della valutazione umana rendendola più oggettiva.</sample>
    <sample id="39">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla presenza di campioni di validazione puliti.</sample>
    <sample id="40">Per migliorare il punteggio, è necessario fare progressi in diverse aree, tra cui la comprensione di testi, la capacità di esprimersi in un linguaggio scritto e parlato corretto e la capacità di comprendere e utilizzare le strutture grammaticali del linguaggio.</sample>
    <sample id="41">Ci sono quattro autori coinvolti nell'articolo.</sample>
    <sample id="42">Ciao, mi chiamo Adam Strykowski e questa parola è sulla struttura di dipendenze della coordinazione.</sample>
    <sample id="43">Come potete vedere, ci sono strutture di dipendenze diverse utilizzate da diverse teorie e approcci. Ad esempio, in università, si usano strutture di dipendenze che coinvolgono la coordinazione tra Lisa, Bart e Maggie.</sample>
    <sample id="44">È tale che il primo congiunto è la testa della struttura di coordinate, in questo caso Lisa.</sample>
    <sample id="45">Approcci simili sono assunti in Igore Mil'utskov's meaning-text theory, dove di nuovo la struttura intera è guidata dal primo congiunto. Questi due approcci sono isometrici: evidenziano uno dei congiunti.</sample>
    <sample id="46">Ora, ci sono anche approcci simmetrici ai corretti strutture, come il prague approccio, il connessione orientato approccio e unimint in prague dipendenza treebanks dove strutture coordinate sono guidate dalla connessione.</sample>
    <sample id="47">Sì, siamo in dipendenze da and a tutti i conjunti.</sample>
    <sample id="48">E finalmente c'è anche un approccio multi-headed che è usato, ad esempio, in De Cattsone's Word Grammar.</sample>
    <sample id="49">Sono, per dire, che tutti i condotti sono alla testa delle strutture dei condotti. Così otteniamo dipendenze dal governatore, qui la L, verso tutti i condotti separatamente. Questi sono bardi e mack.</sample>
    <sample id="50">Ora, il Journal of the Press ha il compito di produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste.</sample>
    <sample id="51">Okay, l'argomento è basato sul principio di dipendenza elettromagnetizzata che ho spiegato sulla base di questi esempi.</sample>
    <sample id="52">Così in inglese, come lei, come lei sa, i oggetti diretti preferiscono di stare vicino al verbo, mentre gli avverbi possono stare più lontano. Quindi, "March read it yesterday" è corretto perché l'oggetto dirett "it" è vicino al verbo "read".</sample>
    <sample id="53">While march read yesterday it is much worse right because here between the verb and the direct object there's an adjunct yesterday.</sample>
    <sample id="54">Tuttavia, questo effetto può essere attenuato quando quando il oggetto diretto è molto pesante e lungo, poiché allora può essere spostato alla posizione dopo l'agente.</sample>
    <sample id="55">Questo è illustrato qui. Quindi entrambe queste frasi sono corrette. March ha letto questo libro assolutamente affascinante sulla storia di B.C. ieri. Sì, okay. Ma invece di "it" ci hanno queste lunghe e p.</sample>
    <sample id="56">Ma è anche okay dire che marche yesterday è un libro absolutely fascinating su</sample>
    <sample id="57">La ragione qui è che questa è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero stare vicino al verbo,</sample>
    <sample id="58">Soddisfa il principio di minimizzazione della dipendenza lungo, che dice che le dipendenze più brevi sono preferite.</sample>
    <sample id="59">Quindi queste due strutture solo evidenziano la lunghezza delle dipendenze cruciali, cioè le strutture che non sono costanti tra queste due strutture.</sample>
    <sample id="60">Ecco qui abbiamo la dipendenza da red to the adjunct of length seven, misurato in parole e da red to book of length four. Così, insieme, è undici.</sample>
    <sample id="61">Quando si sposta o si scambia queste due costituenti, la somma di queste due dipendenze diventa sei. Giusto? Invece di undici, sei, molto più corto. Ecco why this suona piuttosto bene. Giusto? Violenta un principio ma soddisfa un altro.</sample>
    <sample id="62">Ok, allora, cos'abbiamo fatto? Abbiamo estraendo statistiche molto importanti da una versione migliorata del Panettiere Bank riguardo la coordinazione. Vedete il paper, per favore non usate le stipendi universitari.</sample>
    <sample id="63">E queste statistiche confermano l'osservazione fatta molte volte prima che le congiunzioni destre tendono a essere più lunghe. Così, saltpetre e nottepean salts misurati in sillabe.</sample>
    <sample id="64">E anche l'osservazione che è stata fatta in passaggio che questa tendenza cresce con la differenza di lunghezze</sample>
    <sample id="65">Quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto tende a essere più forte. Così, la proporcione tra i due congiunti è maggiore per il congiunto più corto.</sample>
    <sample id="66">Ma quello nuovo in questo articolo è che notiamo che questa tendenza si verifica solo quando i governanti di sinistra sono assenti.</sample>
    <sample id="67">Sì, così il governatore è sulla sinistra in questo esempio. Ho visto Bart e Lisa, quindi il governatore è sulla sinistra.</sample>
    <sample id="68">È assente in esempio due, Homer arrivò e si sdraiò. Qui ha coordinazione di due verbi e non c'è un esterno che governa. Quindi in queste situazioni, il primo soggetto preferisce essere più corto del secondo. Così la differenza tra i due soggetti</sample>
    <sample id="69">Tuttavia, quando i governanti di destra, come qui, lafayette governa la coordinazione tra il net e this effect scompare.</sample>
    <sample id="70">Siamo riusciti a dimostrare che, misurando la lunghezza dei caratteri nella prima colonna, la lunghezza delle sillabe nella seconda colonna e la lunghezza delle parole nella terza colonna, possiamo concentrarci sulla terza colonna.</sample>
    <sample id="71">Cosa vediamo qui è che quando il governante è sulla sinistra,</sample>
    <sample id="72">La tendenza per la congiunzione sinistra di essere più corta cresce gradualmente con la differenza assoluta in parole e lo stesso è osservato quando c'è un segno di coordinazione tra le frasi ma quando il segno di coordinazione è sulla destra questa tendenza scompare.</sample>
    <sample id="73">E in questo articolo mostriamo come queste strutture di coordinazione assimmetriche forniscono un argomento a favore delle strutture simmetriche.</sample>
    <sample id="74">So see the paper for the full agreement and arguments, sorry, and talk to us about at the poster session. Thank you.</sample>
    <sample id="75">Tre autori sono coinvolti nell'articolo.</sample>
    <sample id="76">I domini della Bibbia e dei testi di lingua inglese risultano più semplificati.</sample>
    <sample id="77">Soltanto e non soltanto.</sample>
    <sample id="78">Sì, i modelli pre-allenati ottenuti da NAOs sono liberi di essere utilizzati per la tua ricerca.</sample>
    <sample id="79">DEplain-apa è basato su testi di news.</sample>
    <sample id="80">Per una buona generalizzazione, ci sono diversi fattori che possono influenzare. Questi includono un modello di architettura migliore, dimensioni del modello più grandi e esempi di finetuning più accurati.</sample>
    <sample id="81">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata misurando la lunghezza in caratteri del primo, del medio e del terzo colonna di sillabe, parole e frasi rispettivamente.</sample>
    <sample id="82">I sperimenti sono stati progettati per misurare la lunghezza dei caratteri nella prima colonna (sillabe), nella seconda colonna (parole) e nella terza colonna (frasi) in base alla posizione del governatore.</sample>
    <sample id="83">Un classificatore base addestrato su dati non bilanciati non è efficace, poiché non ha accesso a un sufficiente numero di esempi di ciascuna classe. Questo può portare a una performance inferiore rispetto a quando si utilizza un classificatore addestrato su un insieme di dati bilanciato.</sample>
    <sample id="84">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="85">Bob e Alice</sample>
    <sample id="86">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto su fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="87">I fornitori dell'articolo sono affiliati a diverse istituzioni. John Hawthier e Aaron Mueller sono associati all'University of California, Berkeley, Kanishka Mishra è un membro del Department of Computer Science and Engineering di Indian Institute of Technology Delhi, Gaurav Fint is affiliated with the Department of Computer Science and Engineering at the University of California, Berkeley, Roger Levy è un membro del Department of Computer Science and Engineering al Massachusetts Institute of Technology, e Adina Ravi è un membro del Department of Computer Science and Engineering al University of California, Berkeley.</sample>
    <sample id="122">Il framework quantifica esattamente la posizionalità utilizzando un punteggio di correlazione di Pearson.</sample>
    <sample id="155">Lo studio precedente ha dimostrato che i soggetti umani hanno anche rivelato stereotipi razionali quando gli sono stati forniti gli stessi prompt di persona.</sample>
    <sample id="156">La fonte di dati utilizzata in questo studio è stata la banca dei beni immobiliari.</sample>
    <sample id="157">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di due dichiarazioni di argomenti indipendenti e la classificazione di espansione e comparazione.</sample>
    <sample id="159">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="160">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="161">Il framework differisce dai precedenti studi in quanto confronta i modelli e i dataset con le predizioni e le etichette, invece di limitarsi all'interannotatore accordo o alle distribuzioni degli annotatori.</sample>
    <sample id="162">La configurazione generata.</sample>
    <sample id="163">I sistemi commerciali che sono stati confrontati sono DeepL e Google Translate.</sample>
    <sample id="164">Ciao, mi chiamo Changbing, sono uno studente di PhD all'Università di Washington. Oggi sto presentando il nostro lavoro che va dal training dei dati fino ai modelli di linguaggio e alle compito downstream, monitorando i segni di bias politici che portano a modelli NLP non equi.</sample>
    <sample id="165">I modelli di linguaggio sono addestrati su grandi quantità di dati web.</sample>
    <sample id="166">Le notizie politiche sono bene coperte nei propri dataset di addestramento. Secondo un sondaggio del C4F, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post ecc. sono bene coperti in dataset di addestramento linguistico.</sample>
    <sample id="167">Questo ha creato un mix blessing per le applicazioni modelli linguistici.</sample>
    <sample id="168">Sul lato di un lato, erano in grado di imparare da diverse prospettive che celebrano la democrazia e la pluralità di idee sul lato opposto, queste diverse opinioni politiche sono inerentemente socialmente violette e possono portare a problemi di giustizia potenziale in applicazioni di task sottostanti.</sample>
    <sample id="169">Per raggiungere questo scopo, proponiamo di indagare la catena di propagazione dei bias politici dal dataset di addestramento alla modellistica linguistica fino alle attività downstream specificamente chiedendo le seguenti domande.</sample>
    <sample id="170">Prima di tutto, come valutiamo la linearità politica dei modelli linguistici e in che modo i dati raccolti potrebbero influenzare tale linearità politica?</sample>
    <sample id="171">Secondariamente, come i modelli di linguaggio con diverse politiche dei limiti si comportano su compiti di downstream e se tale comportamento potrebbe portare a problemi di giustizia in applicazioni NLP?</sample>
    <sample id="172">Sono in particolare proposte di stimare modelli di linguaggio con formati di prompt diversi utilizzando domande politiche come il test di competenza politica. Questo ci assicura di poter effettuare un valutazione automatica ben fondata in letteratura di scienze politiche.</sample>
    <sample id="173">So some preliminary results demonstrate that first language models do have varying political leanings. They occupy all four quadrants on the political compass,</sample>
    <sample id="174">Possiamo anche vedere che GPT-4 è il modello linguistico più liberale tra tutti e che le serie GPT sono in generale più liberali rispetto alle serie BERT e le sue varianti.</sample>
    <sample id="175">Secondariamente, ci siamo proposti di indagare fino a che punto i bias politici dei modelli di linguaggio vengono davvero presi in considerazione dalla nostra lingua di addestramento.</sample>
    <sample id="176">Siamo in grado di condurre un esperimento controllato, pretrainando modelli di linguaggio su sei diverse partiti politici separati in notizie e social media, ulteriormente suddivisi in base al loro orientamento politico.</sample>
    <sample id="177">Inoltre, pretrainando modelli di linguaggio su particolari corpus, possiamo vedere che le coordinate ideologiche del modello di linguaggio si spostano corrispondentemente.</sample>
    <sample id="178">For example, for roberta, further fine-tuning and further training on the left-leaning reddit corpus we can see a substantial liberal shift in terms of its</sample>
    <sample id="179">In termini di bias politici.</sample>
    <sample id="180">E anche cercare di indagare se i modelli linguistici possono catturare la polarizzazione che è prevalentemente nella nostra società moderna.</sample>
    <sample id="181">Così, dividiamo il pretraining di corpus in pre-45° Presidente degli Stati Uniti e dopo 45° Presidente degli Stati Uniti. Separatamente, pretrainiamo modelli di linguaggio sui due diversi tempi di corpus.</sample>
    <sample id="182">Possiamo vedere che i modelli di linguaggio hanno generalmente un orientamento politico che si allontina dal centro dopo il 2017. Così, queste indicano che i modelli di linguaggio possono anche prendere iniziative di polarizzazione nella nostra società.</sample>
    <sample id="183">Quindi, nonostante tutto, valutiamo i modelli di linguaggio con diverse inclinazioni politiche in quanto alla rilezione di discorso offensivo e rilezione di notizie fake, due applicazioni NLP che spesso coinvolgono modelli di linguaggio e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">So we see that if we investigate the per category performance, that is to say, if we separate the performance into</sample>
    <sample id="185">Different demographics or political leaning of news media, we can see a pattern that for example for hate speech detection left-leaning language models are better.</sample>
    <sample id="186">At detecting hate speech targeting socially minority groups,</sample>
    <sample id="187">Tuttavia, siamo peggiori nel rilevare l'ingiustizia che colpisce gruppi più potenti nella nostra società.</sample>
    <sample id="188">E viceversa, i modelli di linguaggio ritaliano sono migliori nel rilevare la discriminazione sulla base della razza e del genere, tuttavia peggiori nel rilevare la discriminazione sulla base della razza e della comunità LGBTQ + e altre comunità minoritarie.</sample>
    <sample id="189">Trasmissioni simili anche per la detection dei falsi notiziari, in cui si notano che i modelli di linguaggio di sinistra sono migliori nel rilevare la menzogna dai politici di destra e viceversa.</sample>
    <sample id="190">This in we further show many qualitative examples to see that language models with different political leanings,</sample>
    <sample id="191">Devo dare diverse predizioni per esempi di discorso razzista e informazione sbagliata basato sulla propria classe sociale. Ci sono un sacco di esempi in appendix per sottolineare meglio</sample>
    <sample id="192">Questo indica che c'è un problema di giustizia che è molto rilevante riguardo i bias politici dei modelli di linguaggio.</sample>
    <sample id="193">Ad esempio, se i modelli di lingua rilasciati dovessero essere finetuned per rilevare l'insincerità o la menzogna e successivamente distribuiti su piattaforme social come Twitter,</sample>
    <sample id="194">Questo significherebbe che le persone con opinioni politiche opposte potrebbero essere marginalizzate e che i discorsi razziali mirati a gruppi minoritari potrebbero propagarsi senza alcun controllo.</sample>
    <sample id="195">Sì, questa ha suonato l'alarm per noi di riconoscere e affrontare i problemi di giustizia derivanti dai modelli politici linguistici.</sample>
    <sample id="196">So a little bit of discussion. We would also like to highlight that we expose the unique dilemma regarding language, model, political bias. It's like between Ceylon and Creede's,</sample>
    <sample id="197">Se non sanizziamo le opinioni politiche in un dataset di addestramento linguistico, i bias potrebbero propagarsi dal dataset di addestramento alla lingua e fino a compiti di basso livello, creando problematiche di giustizia.</sample>
    <sample id="198">Se dobbiamo tentare di sanitizzare in qualche modo, rischieremo di incorrere in censure o esclusione e è incredibilmente difficile determinare cosa sia realmente neutra e dovrebbe essere ritenuta linguaggio monotesto. Così è come si potrebbe dire che è un problema simile a quello dell'elettricità elettrica.</sample>
    <sample id="199">Ok, great. I think that's pretty much all I have to do. I'm done for today. Thank you for your time.</sample>
    <sample id="200">Il contenuto dell'articolo menziona che è stato scritto da Ilya Vladd e i colleghi di Google Translate. Quindi, ci sono almeno due autori coinvolti nell'articolo.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite fino a un token di lunghezza del contesto di 2048.</sample>
    <sample id="202">Il loro set di dati include diverse domande, tra cui domande senza parole, domande con un bambino di 12 anni o domande che si riferiscono a personaggi immaginari.</sample>
    <sample id="203">La posizionalità è semplicemente le prospettive che le persone hanno come risultato dei loro demografici, identità e esperienze di vita.</sample>
    <sample id="204">Il nome del relatore è Dawe.</sample>
    <sample id="205">Sì, EDAtt adatta un modello ST offline esistente.</sample>
    <sample id="206">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="207">No, il modello non funziona sulla suite di test.</sample>
    <sample id="208">Le tre varianti di KITMUS sono: 1) Setting con background pretrain, dove la conoscenza è supposta essere disponibile solo durante il tempo di pretrain; 2) Setting con background entrambe, dove la conoscenza è disponibile sia durante il tempo di pretrain che durante il tempo di inferenza; 3) Setting con background inferenza, dove entrambi i tipi di conoscenza sono disponibili solo durante il tempo di inferenza.</sample>
    <sample id="209">I fornitori dell'articolo sono affiliati a diverse istituzioni.</sample>
    <sample id="210">The last research question is whether we should only use the clean samples for validation or if there are better ways to utilize them.</sample>
    <sample id="211">La sensibilità della metrica misura la capacità del modello di produrre gli stessi output per la stessa attività anche con una piccola variazione nella formulazione dell'istruzione.</sample>
    <sample id="212">Jing Wei Yi</sample>
    <sample id="213">Una maggiore sensibilità indica una performance del modello migliore.</sample>
    <sample id="214">Il contesto linguistico viene messo a disposizione dei modelli durante il pre-addestramento.</sample>
    <sample id="215">In genere, si hanno bisogno di 20 campioni di convalida puliti per classe per raggiungere buone prestazioni in WSL.</sample>
    <sample id="216">I fornitori dell'articolo sono collaboratori di Essam Darwish e Dan Jurafsky.</sample>
    <sample id="217">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione perché i modelli di linguaggio hanno variante politica le inclinazioni e occupano tutti i quadranti del compasso politico.</sample>
    <sample id="218">Il nome della relatrice o del relatore è Matcha.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici ha un aspetto che celebra la democrazia e la pluralità di idee, ma anche un aspetto che è socialmente biasato e può portare a problemi di giustizia in applicazioni di compiti di downstream.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="221">No, il risarcimento non è pubblicamente disponibile.</sample>
    <sample id="222">La filigrana fornita è esattamente uguale alla filigrana di destinazione quando il numero di trigger nel testo è maggiore di M.</sample>
    <sample id="223">I fornitori dell'articolo sono affiliati all'University of Pennsylvania.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è la creazione di un pasticcio di cioccolato.</sample>
    <sample id="226">Gli autori validano la segretezza del loro metodo analizzando le embedding di frasi su Fornitori dataset e visualizzandole.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire uno nuovo in modo che possa analizzare l'effetto delle strategie di pretraining.</sample>
    <sample id="228">GPT-4 è meno allineato con i paesi che non parlano inglese.</sample>
    <sample id="229">La relatrice mostra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione nella frase "e tu puoi vedere un esempio sulla destra".</sample>
    <sample id="230">La quantità di attività influisce sulla performance del modello in modo che, all'aumento della quantità di attività, la performance del modello migliora.</sample>
    <sample id="231">I tre approcci di riferimento con cui gli autori confrontano il loro metodo sono i modelli TreeLearner, TreeNet e AutoSklearn.</sample>
    <sample id="232">I due coautori hanno un rapporto di collaborazione con il primo autore.</sample>
    <sample id="233">I'm sorry, but the given sentences do not provide information about the author of PaLM.</sample>
    <sample id="234">Ciao a tutti, mi chiamo Jenny e sono una studentessa di primo anno di laurea in Statistica a Carnegie Mellon University. Oggi presenterò il mio lavoro sull'ottimizzazione caratterizzata da design basati su dataset modelli.</sample>
    <sample id="235">Questo lavoro è stato fatto in collaborazione con alcuni studiosi all'Università di Washington e l'Alumni Institute per l'IA, cioè Sebastian Santi, Ronee Labross, Katerina Rynikha e Martin Sapp.</sample>
    <sample id="236">Allora, iniziamo immaginando che stiate scrivendo un articolo per un giornale e stiate cercando di rimuovere il contenuto tattico dalla vostra recensione.</sample>
    <sample id="237">Potresti ricorrere a un API popolare come l'API di Perspetive per la rilevazione del tossicità e questa funziona davvero bene se sei un carl jones dove l'API di Perspetive è in grado di rilevare correttamente le istanze tossicità.</sample>
    <sample id="238">Ma non è proprio così per Dithi Sharma, dove i modelli di intelligenza artificiale non sono particolarmente sensibili a termini offensivi che sono più comuni in contesti indiani.</sample>
    <sample id="239">Esempio di bias di progettazione, dove si notano differenze sistematiche di prestazione tra popolazioni.</sample>
    <sample id="240">Le bias di progettazione che abbiamo appena visto potrebbero essere dovuti alla posizionalità dei ricercatori e dei sviluppatori di modelli NLP. La posizionalità è semplicemente le percezioni che le persone hanno come risultato dei propri demografici, identità e esperienze della vita.</sample>
    <sample id="241">Questo è un concetto ampiamente utilizzato in studi critici, in particolare in ambienti accademici femministi e queer.</sample>
    <sample id="242">E come ricercatore, la propria posizionalità può influenzare il processo di ricerca e i suoi risultati, poiché è capace di modificare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Ecco un quesito che le persone potrebbero chiedere: hanno i dataset e i modelli una posizionalità?</sample>
    <sample id="244">E non stiamo cercando di dire che i modelli in sé stessi e i set di dati in sé stessi hanno identità demografiche e esperienze della vita, ma essi aggregano giudizi e opinioni di persone reali e possono dunque rappresentare certe posizionalità rispetto a altre.</sample>
    <sample id="245">Il testo in inglese parla di come il proprio lavoro ha fornito evidenze aneddotiche di avere una posizionalità, cioè una posizione o un punto di vista specifico. Questa posizionalità si manifesta attraverso lacune culturali, modelli e set di dati, nonché definizioni teoriche di posizionalità dei modelli.</sample>
    <sample id="246">Tuttavia, queste ricerche non sembrano considerare il confronto tra gli utenti e i dataset e i modelli stessi.</sample>
    <sample id="247">Sviluppare un modello e un dataset di posizionalità è sempre più importante, poiché i test NLP diventano sempre più soggettivi e orientati alla società.</sample>
    <sample id="248">E è difficile caratterizzare come queste posizionalità siano sbucaate, poiché non tutti i decisi sono documentati e molti modelli sono nascosti dietro API.</sample>
    <sample id="249">Quindi per studiare la posizionalità dei dataset e dei modelli, confrontiamo le annotazioni con reali utenti con i dataset e i modelli esistenti.</sample>
    <sample id="250">Facciamo questo attraverso il framework NLP positionalità.</sample>
    <sample id="251">Il framework funziona in due passi principali.</sample>
    <sample id="252">Il primo passo è di riannotare i set di dati con diverse annotatrici.</sample>
    <sample id="253">E optiamo di fare questo analizzando le statistiche dei set di dati originali annotatori, poiché di solito solo un paio di annotatori annotano ogni istanza e poiché le statistiche sono raramente raccolte e condivise.</sample>
    <sample id="254">E quindi ci optiamo per rinnovare i dati, per ottenere molte annotazioni per esempio e per ottenere un insieme ricco di dati demografici.</sample>
    <sample id="255">Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i set di dati utilizzando il coefficiente di correlazione di Pearson.</sample>
    <sample id="256">Ecco il contenuto in italiano:

Ecco, il nostro framework differisce dalla letteratura sulla disaccordo degli annotatori in quanto confronta gli utenti finali con modelli e dataset, previsioni e etichette, invece di concentrarsi solo sull'inter-annotatore accordo o la modellazione delle distribuzioni degli annotatori.</sample>
    <sample id="257">Il framework è in gran parte abilitato attraverso Lab in the Wild, un piattaforma di crowdsourcing online per collaboratori HCH.</sample>
    <sample id="258">Labbellawild è un piattaforma online di esperimentazione in cui possiamo reclutare volontari diversi rispetto a piattaforme come MTurk che hanno maggiormente partecipanti provenenti dagli Stati Uniti o dall'India. Inoltre, Labellawild è in grado di ottenere anche dei dati di alta qualità.</sample>
    <sample id="259">Organizziamo due task su lab in the wild, uno dei quali riguarda l'acceptabilità sociale. Ecco come funziona: i partecipanti leggono una situazione dalla Social Chemistry Dataset e quindi votano su quanto quella situazione sia socialmente accettabile.</sample>
    <sample id="260">Successivamente, per restare in contatto con la città, possono confrontare le risposte con un ai e con gli altri.</sample>
    <sample id="261">Poi si sono confrontate queste annotazioni con la chimica sociale, delphi e qpd4.</sample>
    <sample id="262">We then replicate a very similar setup for the toxicity and hate speech detection task where they'll read an instance from dynahate and write whether they think it's instance of hate speech.</sample>
    <sample id="263">Iniziamo confrontando queste annotazioni con Dina Hayat, Perspective API, Rewire API, Heatmap Roberta e GDP4. Il nostro studio ha esaminato più di 16.000 annotazioni da più di un mille annotatori provenienti da 87 paesi.</sample>
    <sample id="264">Ora siamo meglio equipati per rispondere a chi fanno al linee nlp i set di dati e i modelli coincidono maggiormente? Troiamo che c'è posizionalità in nlp.</sample>
    <sample id="265">Ad esempio, scopriamo che i set di dati e i modelli sono più legati ai paesi che parlano inglese. Quindi, per l'analisi di accettabilità societaria del GDP4, scopriamo che è più legato ai paesi che parlano confuciano e inglese. Troviamo anche che Dina Hatt è maggiormente legata ai paesi che parlano inglese.</sample>
    <sample id="266">Trovi anche maggior alleanza con le persone che hanno un'istruzione universitaria. Quindi, per GPD4 in compito di accettabilità sociale, trovi che è maggiormente legato alle persone che hanno un'istruzione universitaria o una istruzione di studio post-accademico.</sample>
    <sample id="267">E troppo simile per Donnied Haiti, dove è maggiormente allineato con le persone che hanno un'istruzione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati sono allineati a specifici popolazioni, alcuni inevitabilmente vengono lasciati indietro.</sample>
    <sample id="269">Un esempio di questo è che i dataset e i modelli sono meno simili ai non binari rispetto ai controparti di uomini e donne. Lo trovi in GPD4 Social Acceptability Task, come anche in Dignity Hate Task Analysis.</sample>
    <sample id="270">Quindi, data la presenza di un alid e un lp, cosa possiamo fare?</sample>
    <sample id="271">Siamo d'accordo con queste raccomandazioni. La prima è di registrare tutti i design che si considerano pertinenti durante il processo di ricerca. L'altra è di fare ricerche NLP con l'ottica di prospettivismo.</sample>
    <sample id="272">La nostra terza raccomandazione è di costruire insieme set di dati e modelli specializzati per specifici gruppi di persone. Un esempio di questo è l'iniziativa Musa Cani. Vogliamo sottolineare che NLP inclusivo non è solo un modo per far funzionare tutte le tecnologie per tutti.</sample>
    <sample id="273">Ecco il contenuto in italiano:

Ecco, questa è la fine della nostra presentazione. Se volete imparare di più, vi prego di controllare il nostro dashboard per i risultati più aggiornati dell'analisi e il nostro articolo. Grazie mille!</sample>
    <sample id="274">La relatrice menziona tre problemi associati a SimulST: i modelli specifici di architettura sono usualmente addestrati introducendo moduli aggiuntivi da ottimizzare, i procedimenti di addestramento sono lunghi e complessi, ad esempio addestrare coinvolgendo diverse obiettivi di ottimizzazione e addestrare e mantenere diversi modelli per raggiungere differenti regimi di latenza, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza e così via.</sample>
    <sample id="275">Sì, è importante sottolineare che la mitigazione dei bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP è un problema complesso. Ci sono diversi approcci che possono essere utilizzati per ridurre i bias, tra cui l'uso di set di dati più equilibrati e la creazione di modelli più sofisticati che siano in grado di riconoscere e gestire i bias. Tuttavia, è importante notare che non esiste una soluzione perfettiva per questo problema e che continueremo a dover fare scelte difficili quando si tratta di determinare cosa è neutro e dovrebbe essere retainato nel set di addestramento dei modelli di NLP.</sample>
    <sample id="276">Hi, I'm Si Yu Yan from Fudan University. I'm here to introduce our work distinguishing script knowledge from large language models for constrained language planning.</sample>
    <sample id="277">In everyday life, humans often plan their actions by following step-by-step instructions in the form of guaranteed scripts.</sample>
    <sample id="278">Previous work has exploited language models to plan for abstract goals of stereotypical activities such as make a cake and show that large language models can effectively decompose goals into steps.</sample>
    <sample id="279">Tuttavia, i precedenti studi hanno maggiormente focalizzato sul pianificare per gli obiettivi astratti di azioni telesorologiche pianificare per gli obiettivi con specifiche condizioni, come fare un pasticcio di cioccolato, rimane sottostudiata.</sample>
    <sample id="280">In this paper, we define the problem of constrained language planning.</sample>
    <sample id="281">Impone diverse vincoli alla gestione del piano e un obiettivo generale può essere ereditato da diversi obiettivi specifici reali con vincoli multipli. Un buon pianificatore dovrebbe redigere script che siano ragionevoli e fedeli ai vincoli.</sample>
    <sample id="282">In questo articolo, valutiamo e miglioriamo la capacità di pianificazione del linguaggio dei modelli di lingua grande.</sample>
    <sample id="283">There is no data set of specific goals exists to spot our standard.</sample>
    <sample id="284">We have to acquire this goal first as shown in the table we extend the abstract goal with multifaceted constraints for humanising the loop data acquisition use instruct cpt</sample>
    <sample id="285">We sample one hundred specific goals and evaluate the scripts generated from large language models.</sample>
    <sample id="286">Questo tabelle riporta la precisione generale dei risultati che abbiamo ottenuto. Abbiamo scoperto che tutti i modelli di linguaggio hanno ottenuto risultati insoddisfacenti nel pianificare per obiettivi specifici.</sample>
    <sample id="287">Then we conduct detailed analysis to investigate why linear models</sample>
    <sample id="288">Risultati dei diagrammi dimostrano che la completezza semantica in script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.</sample>
    <sample id="289">We dig into more fine-grained topological categories of constraints defined in wikiHow the heatmap in the figure shows that the planning performance of instruct gpt-3.5-turbo varies considerably for goals of different categories.</sample>
    <sample id="290">Studies precedenti hanno dimostrato che la qualità dell'output dei modelli di rete neurale è forse in alta variazione, portando a prestazioni peggiorate. Di conseguenza, adottiamo l'idea di un filtros generatore sovraccaricato per migliorare la qualità della generazione.</sample>
    <sample id="291">We first show constraint types with examples for intratumor ctp and obtain specific goals based on the said abstract goals.</sample>
    <sample id="292">Then instruct gpt to generate case scripts for specific goals.</sample>
    <sample id="293">Next a filter model is developed to select the facial scripts.</sample>
    <sample id="294">We convert scripts and goals into extract gbt embeddings and calculate cosine similarity and similarity scores to measure semantic similarity.</sample>
    <sample id="295">In addition, we will observe the script that contains the keywords of the target constraint we only keep the script if the target goal score is the highest in the goal set.</sample>
    <sample id="296">Con il nostro metodo, le istruzioni GPT possono generare scritti di alta qualità. Il nostro metodo migliora grandemente la pianificazione sia in termini di completezza semantica che di fedeltà ai vincoli.</sample>
    <sample id="297">Poiché i modelli di linguaggio sono costosi da deploy, è essenziale abilitare la capacità di pianificazione linguistica dei modelli più piccoli e specializzati Creare un dataset è un passo essenziale verso l'obiettivo.</sample>
    <sample id="298">However, previous studies do not enable planning for specific goals and manual manual data set annotation is expensive.</sample>
    <sample id="299">Tale modo, seguiamo l'idea di symbolic knowledge distillation per distillare i constrained language planning datasets da lingue linguistiche modelli.</sample>
    <sample id="300">We'll apply our method for building a dataset of constrained language planning named as code script.</sample>
    <sample id="301">In totale, generiamo cinquantacinquemila specifici dataset con script per assicurare la qualità dei set di valiazione e i test. Richiediamo ai worker cloud-sourced di trovare e revisionare样例不正确的样本。</sample>
    <sample id="302">Questa figura mostra la distribuzione vincolata del codice sorgente. Ci vediamo che il codice sorgente ha un'alta polarizzazio nello specifico genere con codice sorgente. Con il codice sorgente, possiamo creare modelli più piccoli ma specializzati per pianificazione linguistica vincolata.</sample>
    <sample id="303">We found that t5 fine-tuning on a corpus can generate scripts of higher quality than most large language models indicating that smaller models can surpass larger models when properly trained on suitable data sets.</sample>
    <sample id="304">In sintesi, stabiliamo il problema di pianificazione linguistica con vincoli. Analizziamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua grandi e sviluppiamo un nuovo filtra metodi per i modelli di lingua grandi.</sample>
    <sample id="305">Usiamo modelli di linguaggio a larga scala per generare un insieme di dati di alta qualità in uno script di codice per pianificazione linguistica. Speriamo che il dataset di codice possa essere un risorsa utilizzabile per avanzare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="306">Thanks for your time. Please find more details of code script in our paper.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi di artificiale intelligenza, ma la differenza principale deriva dalla precisione.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana sono: essere applicabile a imbeddare i servizi, non degradare la utilità dei servizi imbeddati, essere abbastanza visibili per gli attacchatori ma anche facile da rimuovere e essere trasferibile ai servizi degli attacchatori durante il processo di estrazione del modello.</sample>
    <sample id="309">I discorsi TED tradotti in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">Usualmente solo un paio di annotatori annotano ogni istanza.</sample>
    <sample id="311">La differenza tra set di dati benigni e backdoor viene misurata utilizzando la differenza della cosine e la differenza L2.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare due gruppi di modelli, inclusi i codificatori PDR e i modelli ad encoder-decoder. I modelli ad encoder-decoder hanno ottenuto le migliori prestazioni su tutti i nove set di dati.</sample>
    <sample id="344">Gli autori decidono quali sono le parole a frequenza moderata selezionando un set di parole in un intervallo di frequenza moderata.</sample>
    <sample id="345">Ciao a tutti. Il mio nome è Zhu Hong. Oggi, intendo presentare il mio articolo "Do conll 2003 named entity taggers still work well in 2023?". Vediamo di iniziare.</sample>
    <sample id="346">Il nostro articolo ha indagato il problema della generalizzazione utilizzando la compito di riconoscimento di entità denominata o la compito di rilevamento di entità.</sample>
    <sample id="347">Siamo stati in grado di osservare che i modelli hanno utilizzato KDD 2003 per sviluppare NER per quasi venti anni e questa naturalmente solleva diversi problemi. In primo luogo, possono questi modelli generalizzarsi a dati moderni?</sample>
    <sample id="348">E quando sviluppiamo nuove tag, cosa è necessario per una buona generalizzazione?</sample>
    <sample id="349">Allo stesso tempo, se osserviamo una scarsa generalizzazione, cosa causa il crollo delle prestazioni di questi modelli?</sample>
    <sample id="350">Per eseguire queste ricerche sviluppiamo il dataset cono plus plus. Questo è un dataset che abbiamo raccolto da Reuters News nel 2020 e lo abbiamo annotato con le stesse direttive di annotazione del 2013 per il cono.</sample>
    <sample id="351">Poi abbiamo finetuned più di venti modelli su Kornog 2003. Li abbiamo valutati su entrambi i test set di Kornog 03 e il test set di Kornog Plus Plus.</sample>
    <sample id="352">Infine, non è tutto, calcoliamo il percentuale di cambiamento in F1 per valutare la generalizzabilità di ogni modello.</sample>
    <sample id="353">Quindi, cosa è necessario per una buona generalizzazione? Attraverso gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che occorrono.</sample>
    <sample id="354">La prima è la modellazione architetturale. Durante i nostri esperimenti, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio a nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è il modello di dimensione. Abbiamo scoperto che, di solito, i modelli più grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">Infine, non ultima, sappiamo che il numero di esempi di finetuning直接影响 downstream task的性能。 qui abbiamo anche scoperto che più esempi di finetuning realmente conducono a una migliore generalizzazione.</sample>
    <sample id="357">La nostra prossima domanda: cosa causa la diminuzione delle prestazioni di alcuni modelli?</sample>
    <sample id="358">We had two hypotheses the first one is adaptive overfitting which is overfitting caused by reusing the same test set over and over again and this is usually manifested as the diminution returns on the new test set.</sample>
    <sample id="359">La seconda ipotesi è tempo drift, che è la degradazione del performance causata da un intervallo temporale crescente tra i dati di training e i dati di test.</sample>
    <sample id="360">Per l'adattamento all'overfitting, si è notato che dalla grafica sulla destra la linea di migliore adattamento rossa ha una derivata che è maggiore di 1.</sample>
    <sample id="361">Questa significa che ogni unità di miglioramento che realizziamo su Chrome 2013 si traduce in più di una unità di miglioramento su Chrome ++, il che significa che non ci sono rendimenti che diminuiscono.</sample>
    <sample id="362">E questo ci dimostra che l'adattamento eccessivo in questo caso non è osservato.</sample>
    <sample id="363">Quindi, cosa c'è di tempo a tiro?</sample>
    <sample id="364">Per il drift temporale, abbiamo svolto un esperimento per retrain o continuare a pretrainare alcuni modelli con più recenti dati e abbiamo scoperto che le prestazioni degradano con un intervallo temporale più grande.</sample>
    <sample id="365">E questo conferma la nostra ipotesi che il principale motivo della caduta del prestabilito è l'errore temporale.</sample>
    <sample id="366">La nostra conclusione è che per una buona generalizzazione, ci servirebbe un modello di architettura migliore, dimensioni del modello più grandi e esempi di finetuning migliori. Questi elementi devono andare di pari passo, non possiamo avere solo un ingrediente ma bisogna avere tutti insieme.</sample>
    <sample id="367">Allo stesso tempo, abbiamo anche scoperto che il calo delle prestazioni qui è causato da drift temporale e, sorprendentemente, non è causato da sovraperload adattativa anche se Contra 2003 è stato utilizzato per più di venti anni.</sample>
    <sample id="368">Tornando indietro alla domanda che avete sollevato nel titolo del nostro articolo, continuano a funzionare i tag di Connell 2003 in 2023? Ebbene, la risposta è sì.</sample>
    <sample id="369">Speriamo che il nostro articolo porti a maggiori ricerche su come migliorare la generalizzabilità dei modelli.</sample>
    <sample id="370">E infine, per favore assicurati di controllare il nostro articolo, il nostro dataset e se hai qualche domanda, non esitare a contattarmi. Ti ringhio mille volte.</sample>
    <sample id="397">L'approccio utilizza un segmento di dimensione 10.</sample>
    <sample id="398">Per risolvere il pronome "he" in Servin e Kea, è necessario conoscere specificamente che Servin è un giudice.</sample>
    <sample id="399">La qualità dell'esempio è il fattore più importante.</sample>
    <sample id="400">L'articolo si concentra su GPT-4 e GPT-Series.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">Gli esempi di inferenza diretta sono direttamente rilevanti per il contesto.</sample>
    <sample id="403">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Fudan.</sample>
    <sample id="404">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="405">No, non è stato considerato come un approccio standard.</sample>
    <sample id="406">Gli autori hanno fornito l'esempio di un "uomo guerriero" come gruppo contrassegnato.</sample>
    <sample id="407">I modelli non generalizzano adeguatamente.</sample>
    <sample id="408">I nomi dei set di dati di test sono Finetuning e WSL.</sample>
    <sample id="409">Cinque.</sample>
    <sample id="410">Il testo non fornisce informazioni su quanti modelli di apprendimento l'autore sta utilizzando.</sample>
    <sample id="439">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare e utilizzare sia i modelli pre-addestrati che quelli in tempo reale.</sample>
    <sample id="440">I nomi dei relatori sono Ying e Jiaxuan.</sample>
    <sample id="441">Sì, i coscripts sono stati sottoposti a controlli di qualità.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto solo supportano tipi limitati di traduzioni dipendenti dal contesto e limitati insiemi di lingue, poiché usualmente si basano su conoscenze umane e curazione.</sample>
    <sample id="443">Hi, and I'm going to talk about our work on resolving indirect referring expressions for entity selection in which we introduce the alt entities scorers.</sample>
    <sample id="444">Il mio nome è javat hosaini e questa è un joint work with philip radlinski, sylvia parati e annie lewis.</sample>
    <sample id="445">Il nostro obiettivo è di comprendere il linguaggio dell'utente quando vuole fare una scelta. Considera questa domanda alternativa: Hai voluto dire "facile per me" o "ho un problema"? Qui, l'utente vuole scegliere tra due opzioni.</sample>
    <sample id="446">La cosa più ovvia è usare un riferimento diretto, ad esempio, dire il nome della canzone o la sua posizione.</sample>
    <sample id="447">Ma a volte un indiretto riferimento è più appropriato per avere una conversazione più naturale. Questo potrebbe succedere quando l'utente non ricorda il nome del</sample>
    <sample id="448">All the pronunciations are too similar to each other and hard to disambiguate.</sample>
    <sample id="449">Or when the user wants to specify a preference here are some examples in direct references for example the newer one or the song that's not energetic.</sample>
    <sample id="450">Questo è un problema importante in sistemi di conversazione e anche per la valutazione delle performance dei modelli di intelligenza artificiale.</sample>
    <sample id="451">Non siamo a conoscenza di un insieme di dati pubblico di grande scala per il compito, quindi raccolgiamo uno utilizzando la crowdsourcing. Il nostro insieme di dati copre tre domini diversi: musica, libri e ricette.</sample>
    <sample id="452">Our data set collection methodology emphasizes informality using a cartoon completion set.</sample>
    <sample id="453">Il cartoon ha tre bubble di dialogo. In la prima bubble Bob dice di ricordare la canzone che stavamo ascoltando ieri. E con quello Bob stabilisce il contesto del dialogo.</sample>
    <sample id="454">In this in the second speech bubble alice says do you mean easy on me or i got</sample>
    <sample id="455">Which is the alternative question. and in the third speech bubble bob uses an indirect reference to select one of these entities, for example the new</sample>
    <sample id="456">We provide the first and second speech bubbles automatically, but the third one is filled in by the annotator. The first speech bubble is chosen from a few manual prompts per domain.</sample>
    <sample id="457">La seconda, che è la domanda alternativa, è generata come segue.</sample>
    <sample id="458">We always use a simple template. Do you mean A or B? Where A and B are samples from Wikipedia.</sample>
    <sample id="459">Ecco le diverse tecniche di campionamento che utilizziamo quando ci spostiamo verso l'alto nella lista i enti diventano più simili gli uni agli altri e generalmente è più difficile fare la disambigazione.</sample>
    <sample id="460">The first one is uniforma trans.</sample>
    <sample id="461">The second one is when the entities have similar titles, for example two books with the name The Retreat.</sample>
    <sample id="462">The third one is when they have similar descriptions on Wikipedia and finally when they have similar infoboxes or attributes on Wikipedia for example the same genre or the same artist for some</sample>
    <sample id="463">Quando mostriamo queste domande alternative ai rappresentanti, conoscono il nome di queste entità ma non necessariamente sanno qualcosa sulle entità.</sample>
    <sample id="464">Quello che facciamo è di fornire alcune informazioni di sfondo sulle due entità. Per i film, semplicemente forniamo un link di ricerca Google per ogni film.</sample>
    <sample id="465">Ecco una versione in italiano del contenuto inglese: "Ecco, adesso chiediamo ai traduttori di ascoltare almeno un pezzo di ciascun brano e leggere dietro ciascun brano. Ad esempio, ecco i risultati di una ricerca su Google per la canzone 'Easy'".</sample>
    <sample id="466">Per il dominio recipes and books, mostriamo alcune informazioni di sfondo da Wikipedia. Per le ricette, inoltre, mostriamo anche le immagini, nuovamente da Wikipedia, in modo che gli annotatori possano vedere come si vedono.</sample>
    <sample id="467">Allora chiediamo ai annotatori di scegliere uno di questi enti, ad esempio, il primo qui e descrivere lo utilizzando 3-5 espressioni indirette.</sample>
    <sample id="468">For example, the one with the piano music here are some examples from our data set for example, the one without words not the one with the twelve year old twelve year old boy or the fictional one or comes from azerbaijan and so</sample>
    <sample id="469">La nostra entità Corpus ha 6.000 domande alternative attraverso tre domini e ha 42.000 espressioni indiretamente riferite. I risultati con il modello T5XLarge sono riassunti.</sample>
    <sample id="470">Se il modello di linguaggio ha accesso alla esatta stessa conoscenza di sfondo degli autori, allora la precisione è davvero alta. Sono circa il 92 al 95 percento. Ma questa non è realistica.</sample>
    <sample id="471">Se il modello di linguaggio ha accesso a qualche conoscenza di sfondo parzialmente sovrapposibile, allora l'accuratezza è tra i 82 e i 87 percenti, che è più realistico. Ad esempio, quando il modello di linguaggio recupera la conoscenza di sfondo.</sample>
    <sample id="472">Se il modello di linguaggio ha accesso solo ai nomi di entità, allora l'accuratezza è solo del 60 per cento, quindi c'è molto spazio per miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili in domenico. Ecco un link alla nostra base di dati. Grazie.</sample>
    <sample id="473">L'approccio viene confrontato con le politiche SimulST esistenti, tra cui la strategia di Whitaker e l'accordo locale, e con la struttura dell'architettura dell'arte specificamente dedicata alla SimulST.</sample>
    <sample id="474">I ricercatori dell'articolo sono affiliati all'Institut de Recherches Cliniques de Montréal (IRCM) e all'Université de Montréal.</sample>
    <sample id="475">Il nome della relatrice o del relatore è Jenny.</sample>
    <sample id="476">Cinque autori sono coinvolti nell'articolo: il primo è la stessa autrice, Myra, e gli altri quattro sono Esser Demuruchi, Dan Juravsky, un altro collaboratore non specificato e un quarto collaboratore non specificato.</sample>
    <sample id="477">Ciao! Sono Sarah Papi, dalla Università di Trento e Fondazione Bruno Kessler. E io presenterò brevemente un articolo che serve come guida per la traduzione parallela di testi. Questo è un lavoro collaborativo con Matteo Negri e Marco Turki.</sample>
    <sample id="478">Simultaneous speech translation Simultaneous speech translation or simulst is the process of translating spoken language into a text in another language in real time enabling cross-language communication.</sample>
    <sample id="479">E cosa sono i problemi dei modelli di similitudine attuali? Le strutture specifiche sono di solito addestrate introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">Lunghe e complesse procedure di addestramento, ad esempio le procedure di addestramento che coinvolgono diverse obiettivi di ottimizzazione,</sample>
    <sample id="481">E addestrando e mantenendo diversi modelli per raggiungere differenti regimi di latenza, ad esempio addestrando un modello con un'average di 1 secondo di latenza e un altro modello con 2 secondi di latenza e così via.</sample>
    <sample id="482">Allora, cosa è la nostra soluzione?</sample>
    <sample id="483">Prima di usare modelli esistenti offline preesatti senza retrain o adottare specifiche architettura per Simil ST utilizza solo un modello per ogni regime di latenza e gestisci la latenza tramite parametri specifici.</sample>
    <sample id="484">E utilizza le conoscenze acquisite dal modello attraverso il meccanismo di attenzione tra input audio e output testo, che è chiamato meccanismo di attenzione reciproca. E puoi vedere un esempio sulla destra.</sample>
    <sample id="485">La nostra soluzione è di proporgli adattare o encoder decoder l'attenzione e è una strategia per cui decidiamo se dobbiamo emettere o no una traduzione parziale basata su dove l'attenzione punta.</sample>
    <sample id="486">Un segnale è emesso se la tensione non è concentrica, cioè se la somma è inferiore a un certo livello di alfa verso le ultime lampo di speech frames, che significa che i segnali ricevuti non sono abbastanza stabili.</sample>
    <sample id="487">Ad esempio, se riceviamo un testo in spagnolo che dice 'I'm going to talk about' e il nostro modello predice la traduzione in tedesco,</sample>
    <sample id="488">E noi guarderemo al crescendo tensione um weight,</sample>
    <sample id="489">Vedremo che i primi due parametri puntano alle prime due finestre di tramese ricevute, mentre l'ultimo parametro punta alla ultima finestra di tramese ricevuta, ossia la lambda finestra di tramese.</sample>
    <sample id="490">Questo significa che i primi due caratteri verranno omessi.</sample>
    <sample id="491">Mentre, poiché la somma della tensione di cruce è sopra un certo livello di tensione, non emetteremo il segnale e aspetteremo per un segnale successivo.</sample>
    <sample id="492">Se continuiamo e riceviamo un altro speech chunk e il modello predice altre tre parole, analizzeremo queste pesi di attenzione.</sample>
    <sample id="493">We will see that no word points to the last lamp-beam, lamp speech frames.</sample>
    <sample id="494">Questo significa che queste tre parole verranno emesse.</sample>
    <sample id="495">Se guardi i principali risultati di quell'articolo,</sample>
    <sample id="496">We will plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average latency.</sample>
    <sample id="497">That is the latency measure and we also consider the computational-aware average linking that accounts for the model's computational times to produce the output.</sample>
    <sample id="498">Sì, vogliamo che i curosi siano il più possibile su questa piattaforma.</sample>
    <sample id="499">Ma anche che siano spostati verso sinistra.</sample>
    <sample id="500">E confrontiamo con strategie di preparazione che vengono anche applicate a modelli offline, come la strategia dei weight keys e l'approccio locale. Inoltre, confrontiamo anche con l'architettura dell'arte di Stato specificamente dedicata alla traduzione semantica.</sample>
    <sample id="501">Questi sono tutti i risultati della strategia di traduzione parallela su tedesco.</sample>
    <sample id="502">E vediamo che ad out outperforms all the strategies applied to offline models, since their curves are shifted over the left.</sample>
    <sample id="503">E vediamo anche che se consideriamo il tempo di scadenza reale o il tempo di elaborazione computazionale, è quello il più veloce.</sample>
    <sample id="504">Se vuoi scoprire maggiori risultati, leggi il nostro articolo e anche rilasciamo codice aperto e modelli e i modelli di output contemporanei per facilitare la riproducibilità del nostro lavoro. Grazie per l'attenzione.</sample>
    <sample id="505">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="506">Ciao a tutti, mi chiamo Ying e il mio collega Jiaqiang e io presenteremo il nostro lavoro su Multi-Instruct, che è un miglioramento del imparare tramite modelli multi-modal tramite l'addestramento con istruzioni.</sample>
    <sample id="507">Con i progressi nella creazione di modelli linguistici ad grandi dimensioni, molte ricerche hanno iniziato a esplorare nuove paradigmi di imparare utilizzando modelli pre-addestrati per diverse attività di basso livello in modo efficiente e veloce.</sample>
    <sample id="508">Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli di linguaggio ad grandi dimensioni di svolgere compiti di intelligenza artificiale in modo rapido e efficiente, seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei precedenti studi sulla calibrazione dell'adattamento hanno concentrato sull'incremento delle prestazioni senza su task di linguistica solo, mentre le compere visione e i task multi-modal hanno stato lasciato fuori.</sample>
    <sample id="510">Quindi, in questo lavoro, ci piacerebbe indagare se l'ottimizzazione dell'adattamento o i modelli di apprendimento multi-modal possono realmente migliorare la generalizzabilità per compiti multi-modal.</sample>
    <sample id="511">Inoltre, durante la nostra ricerca, abbiamo scoperto una considerabile disparità nell'accessibilità dei set di dati di istruzioni tra NLP e multi-modal.</sample>
    <sample id="512">Ci sono più di mille e seicento dataset di istruzioni in lingua solo. Tuttavia, non esiste un dataset di istruzioni multi-modal grande a disposizione pubblicamente. Questo ci ha spinto a creare un dataset di istruzioni multi-modal per allenare i modelli.</sample>
    <sample id="513">Ecco qui presentiamo MultiInstruct, il primo dataset di benchmarking per l'addestramento di istruzioni multi-modal che comprende 62 diverse compiti multi-modal provenienti da 10 diverse category.</sample>
    <sample id="514">Questi task sono derivati da venti e un esistente set di dati open source e ogni task è equipaggiato con cinque istruzioni esperte.</sample>
    <sample id="515">Per investigare l'ottimizzazione multi-modal di nostra propria base di dati, utilizziamo OFA, un modello unificato per la traduzione multi-modal come nostra base modello. OFA utilizza un vocabolario unificato per le token del linguaggio e delle immagini, e la coordinate di un bounding box.</sample>
    <sample id="516">Ecco alcuni esempi di istanze dalla nostra multi-instru dataset.</sample>
    <sample id="517">Unificare il trattamento di varie tipi di dati di input e output.</sample>
    <sample id="518">Seguiamo il modello di OFA e formiamo tutti i task in un formato unificato sequenza per sequenza in cui le informazioni di input, immagini, istruzioni e bounding boxes vengono rappresentate nello spazio dei token同一.</sample>
    <sample id="519">Ora, sto per parlare di tuning multi-modal di istruzioni.</sample>
    <sample id="520">Per il dataset di addestramento, utilizziamo 53 task da NQ Group per l'addestramento e sperimentiamo 10.000 istanze per task. Per la valutazione, riserviamo interamente il dataset di Comprensione del testo per la valutazione e scegliamo ulteriormente cinque task da Wiki e MECO Group.</sample>
    <sample id="521">Usiamo tutti gli esempi nella split di test per ciascun compito. Inoltre, randomizziamo un campione di 20 task dalla split di test di natural instruction come task di input per NLP.</sample>
    <sample id="522">Così utilizziamo un modello pre-addestrato O-F-A-Large come modello di base. Durante l'addestramento, creiamo un insieme di istanze per tutti i compiti. Ognuna di queste istanze è combinata randomicamente con uno dei cinque template di istruzioni.</sample>
    <sample id="523">During test for each task we conduct a total of five experiments by evaluating the model using one of the five instructions in each experiment.</sample>
    <sample id="524">Report the mean and max performance and the standard deviation of the performance across all five experiments.</sample>
    <sample id="525">Se la task è una task di classificazione multi-modal, rapporteremo l'accuratezza. Se è una task di generazione multi-modal, rapporteremo rouge L. Per le task NLP, rapporteremo rouge L inoltre.</sample>
    <sample id="526">Hem introducer etterskiftet et aditionell evalueringsmetode kalt Sensitivit. Dette metoden vurderer modellen的能力的一致性地生产相同的输出对于同一任务,无论输入说明的微小变化。</sample>
    <sample id="527">Ecco i nostri principali risultati. Come possiamo vedere, l'ottimizzazione dell'instruzione può migliorare significativamente il rendimento di Ours in compiti multi-modal.</sample>
    <sample id="528">Anche transfer learning da dataset di istruzioni naturali può migliorare l'addestramento dell'instruzione.</sample>
    <sample id="529">E qui possiamo vedere che, con l'aumento del numero di task, il modello raggiunge un'efficienza migliore e allo stesso tempo una sensibilità più bassa.</sample>
    <sample id="530">Sì, anche noi facciamo dei nuovi sperimenti. Utilizziamo un istruzione invece di cinque istruzioni. Come possiamo vedere, utilizzare molte istruzioni può migliorare il modello in modo generale e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Questo dimostra l'effetto di diverse strategie di finetuning sul sensibilità del modello. Come possiamo vedere, utilizzando imparazione tramite transfer da dataset di istruzioni naturali, il modello può raggiungere un sensibilità molto migliore rispetto al modello OFA originale.</sample>
    <sample id="532">We also can see transfer learning from the machine instruction data set can help our AI to achieve much better performance on the machine instruct data set.</sample>
    <sample id="533">In generale, proponiamo il primo dataset di addestramento multi modulo a larga scala che migliorerebbe significativamente la capacità di rilevare anomalie di OIF. Esploriamo anche diversi metodi di imparare dalla transizione e mostriamo i loro benefici. Designiamo un nuovo metrico chiamato sensibilità.</sample>
    <sample id="534">Un momento, stiamo raccolgendo un insieme molto più grande di dataset di addestramento multi-modal con circa 150 task linguistic aggiuntive e li rilasciamo. Questo è il link per i nostri dataset e i modelli. Grazie.</sample>
    <sample id="535">I fornitori dell'articolo sono affiliati all'Università di Trento e all'organizzazione di Bruno Kessler.</sample>
    <sample id="536">Il nome della relatrice o del relatore è Javad Hosseini.</sample>
    <sample id="562">Ciao a tutti. Sono Kostov Sina e sono felice di benvenuti alla nostra discussione sul mio articolo AC 2023, riguardante i giudizi sulla accettabilità dei modelli linguistici non sempre robusti al contesto.</sample>
    <sample id="563">Si è trattato di un lavoro in collaborazione con John Gower, Aaron Mueller, Galina Shishkova, Karen Fentress, Roger Levy e Athena Villamor.</sample>
    <sample id="564">In this work, we revisit the minimal pair paradigm.</sample>
    <sample id="565">La valutazione minimale per partito basically evaluates language models on top of acceptability judgments which can also include grammaticality like plump, syntax jim or acceptability in terms of stereotypes such as grouse pairs.</sample>
    <sample id="566">E in questo paradigma minimo, la tipica via di valutazione dei modelli linguistici è che tu mostri un'accettabile frase o una grammaticalmente corretta frase e quindi mostri un'insopportabile frase o una frase non grammaticalmente corretta.</sample>
    <sample id="567">E allora, l'ipotesi del modello basically puts more probability to the acceptable set.</sample>
    <sample id="568">The current mpp pipeline basically doesn't allow us to evaluate models acceptance towards longer sentences.</sample>
    <sample id="569">Questi giorni i modelli di lingua stanno uscendo con finestre di contesto più lunghe. Quindi è cruciale che valutiamo l'acceptabilità dei modelli durante tutto il contesto.</sample>
    <sample id="570">E quello è proprio quello che stiamo cercando di fare qui. Stiamo cercando di revisita il pipeline NPP by asking the model to evaluate acceptability on longer and longer sequences.</sample>
    <sample id="571">Quindi, è questa l'approccio. Quello che facciamo è di simulare queste sequenze più lunghe. Rivediamo i dataset stessi e quindi ricreato i testi scegliendo, come accettabili o non accettabili, i testi da quei dataset.</sample>
    <sample id="572">For example, here we have chosen a typical pair of grammaticality from the blimp data set from the adjunct island case.</sample>
    <sample id="573">E quello che facciamo è ricreare lunghe sequenze che sono accettabili e hanno lo stesso matching della struttura grammaticale. Estraiamo frasi grammaticali da un gioco di carte.</sample>
    <sample id="574">E quindi lo aggiungiamo come prefisso sia alla query accettabile che alla query non accettabile.</sample>
    <sample id="575">Sì, possiamo fare la stessa cosa scegliendo frasi non accettabili dalla stessa coppia e quello che potrebbe anche essere utilizzato per testare l'acceptabilità del modello.</sample>
    <sample id="576">E possiamo anche fare lo stesso scegliendo frasi da un insieme diverso o da un dataset diverso. Così è quello che chiamiamo scenario di mismatch.</sample>
    <sample id="577">So here the sentences are still coming from relevant datasets, but it's not from the same dataset that you're evaluating with and we can do the same for unacceptability cases.</sample>
    <sample id="578">Finalmente, possiamo scegliere i testi da un dominio completamente non correlato, ad esempio Wikipedia.</sample>
    <sample id="579">Quindi, questa ci dirà se i giudizi di accettabilità dei modelli sono effettivamente influenzati da alcun contesto.</sample>
    <sample id="580">Whether the context is coming from a different subset of the data set or whether it's like completely irrelevant to the current like to the sentence that we are looking at.</sample>
    <sample id="581">Quindi, come fa il modello? In primo luogo, analizziamo le sentenze di Wikipedia, che sono completamente irrelevanti per la query corrente. E lì trovi che i giudizi del MPP sono maggiormente robusti per contesti arbitrari.</sample>
    <sample id="582">We increased the context length toward up to 204 for to max out OPT and GPT-2 models, and we saw here in the orange dot line the MPP judgments are relatively stable.</sample>
    <sample id="583">Ora, cosa succede quando scegliamo i testi da un medesimo dataset?</sample>
    <sample id="584">Ecco qui, stiamo scegliendo o creando sentenze da domini accettabili e non accettabili, dalla stessa Blimp or syntaxim dataset.</sample>
    <sample id="585">And there we see that the mpp judgments either increase or decrease significantly when you add either acceptable prefixes or unacceptable</sample>
    <sample id="586">Ma quando ci mettiamo la struttura, che è quando scegliamo le frasi da fenomeni dello stile del testo jim,</sample>
    <sample id="587">Vediamo un incremento o un decennio massivo del mpb giudizio per il modello, in base a quando il prefisso scelto è accettabile o non accettabile.</sample>
    <sample id="588">Ora, questa è e questa è molto grande. Questo effetto aumenta lungo il contesto lungo e questo dovrebbe probabilmente influenzare i modelli di lingua più recenti che hanno un contesto di larghe finestre.</sample>
    <sample id="589">Quindi, perché il prefisso di match influenza tanto i giudizi del modello linguistico?</sample>
    <sample id="590">Siamo arrivati a una serie di analisi in cui ci siamo sforzati di alterare la frase d'input, cercando di preservare la struttura rilevante ma aggiungendo un po' di rumore alla frase d'input e, dopo aver fatto queste alterazioni,</sample>
    <sample id="591">We find that none of these noises are actually making the model like change it course in terms of how it shows us the nppg job trend.</sample>
    <sample id="592">Basically, we find that the models are sensitive to the perturbations in sentences in similar ways.</sample>
    <sample id="593">That is, when we perturb the sentences in the acceptable domain we see similar increase in all the perturbations and when we perturb the sentences in the unacceptable domain we see decrease in mpp judgements in similar fashion.</sample>
    <sample id="594">Quindi i principali punti di rilievo del nostro lavoro è che i modelli di linguaggio sono sensibili a caratteristiche sintattiche e semantiche latenti che si condividono tra le frasi.</sample>
    <sample id="595">E la valutazione MPP che facciamo attualmente con input singoli e brevi non può completamente catturare le conoscenze astratte dei modelli di linguaggio attraverso il contesto.</sample>
    <sample id="596">Per favore, leggete il nostro articolo per maggiori dettagli dei nostri esperimenti. Grazie per l'attenzione.</sample>
    <sample id="597">The first step in the method maps each input token to an unordered multiset of tokens that will appear in the output.</sample>
    <sample id="598">Coscript rappresenta 55,000 scripts.</sample>
    <sample id="626">Il metodo di allineamento migliore per DEplain è il metodo di mass alline.</sample>
    <sample id="627">Il vantaggio dell'apprendimento scarsamente supervisionato è che i modelli di rete neurale possono essere robustamente addestrati anche con label rumorosi, rendendoli più generalizzabili.</sample>
    <sample id="628">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato raccolgendo articoli da Reuters News nel 2020 e annotandone i testi con le stesse guidelines di annotazione utilizzate per il dataset CoNLL 2003.</sample>
    <sample id="630">Ciao a tutti, mi chiamo Yusin Zhang dalla Università di Penn State. Oggi presenterò il mio lavoro esemplare: Crosslingua Semantic Parsing in multiple natural languages and many representations.</sample>
    <sample id="631">Seminar parsing is a task to build semantic representations of user queries such as SQL and lambda calculus.</sample>
    <sample id="632">E la traduzione multilingue semantica è la task di tradurre query in più lingue naturali in rappresentazioni multiple.</sample>
    <sample id="633">Come illustrato nel mio diagramma, dobbiamo tradurre la query in molti linguaggi naturali utilizzando modelli neurali come SQL, Lambda, JSONQL e altri.</sample>
    <sample id="634">I modelli di parsing del linguaggio naturale esistenti sono separatamente proposti e valutati su un insieme di limiti di tassus e applicazioni, ad esempio,</sample>
    <sample id="635">Ci sono lacune di copertura su certe lingue naturali, e il cinese è uno di essi. E</sample>
    <sample id="636">Lack of coverage on certain minor repetitions.</sample>
    <sample id="637">La lombardoculicis è mancante.</sample>
    <sample id="638">O, solo valutati su un determinato modello. Ad esempio, ci sono solo due modelli da valutare.</sample>
    <sample id="639">Così, per raggiungere questo scopo, proponiamo Exemplar, un dataset esemplare per la cross-lingua semantica person in molti linguaggi naturali e rappresentazioni.</sample>
    <sample id="640">Contiene nove dataset in vari domini, cinque sistemi di parsing automatici, otto rappresentazioni e ventidue lingue naturali in quindici famiglie linguistiche.</sample>
    <sample id="641">E per valutare meglio il benchmark, consideriamo i sei parametri per la formazione e l'evaluatione.</sample>
    <sample id="642">La prima è TranslateTest. Utilizziamo l'API di Google Translate per tradurre il testo di origine in un linguaggio di destinazione, quindi utilizziamo un modello monolingue per addestrare e valutare.</sample>
    <sample id="643">Ad esempio, addestriamo il modello inglese su domande in inglese e durante l'inferenza traduciamo la query in tedesco utilizzando un API in inglese e quindi utilizziamo il modello addestrato per prevedere la risposta.</sample>
    <sample id="644">E testiamo anche modelli monolingue.</sample>
    <sample id="645">In questo contesto, la lingua di origine è la stessa della lingua di destinazione, ad esempio, tedesco verso tedesco o inglese verso inglese.</sample>
    <sample id="646">Inoltre, testiamo un impianto di immissione univoca per i modelli, addestrandoli con solo il 10% dei dati di addestramento.</sample>
    <sample id="647">E testiamo un modello multilingue, che adesso trainiamo come un modello multilingue per tutte le lingue.</sample>
    <sample id="648">Ad esempio, mettiamo insieme query in tedesco, inglese e cinese per addestrare un modello multilingue. Durante l'inferenza, possiamo usare questo modello per...</sample>
    <sample id="649">Um to translate German queries or Chinese query or et cetera.</sample>
    <sample id="650">E consideriamo anche il crosslinguaggio zero-shot e few-shot transfer. Ci训练在一种语言上，并将其转移到另一种语言。</sample>
    <sample id="651">Durante la formazione, stiamo addestrando un modello multilingue a prevedere l'output SQL utilizzando domande in inglese o la combinazione di domande in inglese e tedesco.</sample>
    <sample id="652">E anche troppo risultati interessanti. Così, riguardo all'analisi dei modelli monolingue, valutiamo due gruppi di modelli.</sample>
    <sample id="653">Incluso encoder PDR, che sta per Multilingua Pre-Training Encoders with Pointer-based Decoders, come ad esempio XLNet + PDR e Bert + PDR.</sample>
    <sample id="654">E valutiamo anche modelli di encoder-decoder, che sono modelli multilingue pre-allenati di encoder-decoder, come BART e MT5.</sample>
    <sample id="655">Siamo arrivati alla conclusione che encoder-decoder ottiene il migliore prestigio su tutti i nove dataset.</sample>
    <sample id="656">E elaborato su M5 e esempi XLM-R plus PDR su un setting multilingue.</sample>
    <sample id="657">Abbiamo scoperto che encoder-decoder o encoder-pdr possono essere migliorati addestrandoli in un mix di diverse lingue.</sample>
    <sample id="658">Sono stato scoperto che è proprio per questo che la maggior parte delle lingue naturali principali possono ottenere un vantaggio di prestazione, eccetto che le prestazioni dell'inglese diminuiscono in sette set di dati e solamente guadagnano in tre set di dati.</sample>
    <sample id="659">Penso che questa sia conosciuta come cura del multilinguismo.</sample>
    <sample id="660">We also compared the cross-lingual performance gap.</sample>
    <sample id="661">In questa figura, la linea azzurra rappresenta il transito di segnale tra le lingue. La linea rossa rappresenta il transito zero-signal tra le lingue. Mentre la linea verde rappresenta la traduzione tra le lingue.</sample>
    <sample id="662">Abbiamo scoperto che, confrontando le linee giallo e arancio, per la impostazione di zero shot la differenza di prestazione tra le due linee è significativa. Abbiamo anche scoperto che, confrontando le linee blu e arancio, per la impostazione di pochi shot la differenza di prestazione si riduce rapidamente.</sample>
    <sample id="663">Troviamo anche alcune altre scoperte interessanti. Ad esempio, encoder-decoder supera i precedenti studi o raggiunge risultati comparabili, per quanto riguarda l'inglese naturale e inoltre notevolmente migliora le prestazioni di few-shot su lingue naturali.</sample>
    <sample id="664">Sono stati ritenuti modelli linguistici multilingue come Coda es e Blue insufficienti per le analisi semantiche multilingue.</sample>
    <sample id="665">In sintesi, siamo un esemplare unificato per la valutazione di parser di testo con multi-lingua e diverse rappresentazioni.</sample>
    <sample id="666">Conducing a comprehensive benchmark study on three representative types of multilingual language models and our results show many interesting findings and et cetera. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="667">I lavori esistenti possono essere classificati in quattro category.</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom sono insufficienti per il CLSP.</sample>
    <sample id="695">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allineamento come parte della formazione.</sample>
    <sample id="696">L'equità di un modello NLP a valle si definisce come la capacità del modello di fornire risposte equamente e senza bias, non importa la lingua o la cultura del utente.</sample>
    <sample id="697">The name of the speaker is Yanis Lavrac.</sample>
    <sample id="698">La relatrice o il relatore è Costas Tsitsana.</sample>
    <sample id="699">The name of the speaker is Myra.</sample>
    <sample id="700">Il tropicalismo indica un stereotipo che si riferisce alle donne di colore, in questo caso le donne latine.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando parole chiave come cultura, tradizione, orgoglio e esotismo. Queste parole definiscono i gruppi solo in relazione alla loro identità e li distinguono come differenti dal normale bianco.</sample>
    <sample id="702">Si è utilizzata la C6Mi per misurare l'utilizzo del contesto in questo lavoro.</sample>
    <sample id="703">DrBERT è un modello a 7 GB di NAOs, mentre ChuBERT è un modello clinico a 4 GB di sentenze tratte da note cliniche.</sample>
    <sample id="751">Tre.</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un metodo che aggiorna un modello con nuove informazioni dopo ogni round di apprendimento attivo e annotazione.</sample>
    <sample id="753">Il set di dati ha l'obiettivo di comprendere il linguaggio dell'utente quando vuole fare una scelta e considerare queste domande alternative.</sample>
    <sample id="754">Un utente malevolo può estrarre i parametri del modello utilizzando un EaaS.</sample>
    <sample id="755">Cinque.</sample>
    <sample id="756">Sono stati impiegati 100 annotatori per creare il set di dati iniziale.</sample>
    <sample id="757">I fornitori dell'articolo sono affiliati all'Università di Carnegie Mellon e all'University of Washington.</sample>
    <sample id="758">Il governo a sinistra in questo esempio è il caso di Bart e Lisa.</sample>
    <sample id="759">I modelli all'avanguardia nei sistemi di dialogo sono quelli che utilizzano l'apprendimento automatico e la macinazione di linguaggio per comprendere e rispondere alle domande.</sample>
    <sample id="760">Perché i modelli hanno un contesto più lungo, è importante valutare la loro accettabilità durante tutto il contesto.</sample>
    <sample id="761">Sì, la formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="763">La valutazione è stata effettuata utilizzando le metriche di BLEU, METEOR e TER.</sample>
    <sample id="764">Sì, il regresso nella generalizzazione influenza specifici tipi di NER.</sample>
    <sample id="765">La posizionalità nella NLP è importante perché i modelli di NLP devono essere in grado di comprendere il contesto e la posizione di parole specifiche all'interno di un testo. Questo è cruciale per la comprensione del significato e la capacità di estrarre informazioni accurate.</sample>
    <sample id="766">I modelli di lingua multilingue come BLOOM sono stati adattati tramite adattatori o con una messa a punto integrale.</sample>
    <sample id="767">Per il trasferimento dell'apprendimento, si ricorre al modello di co-iniziare l'apprendimento attivo.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM sono il SQuAD 2.0 e la versione più recente del dataset.</sample>
    <sample id="769">The authors have proposed three recommendations for model owners.</sample>
    <sample id="770">Il guadagno del metodo proposto rispetto al metodo di riferimento è la capacità di creare modelli più piccoli ma specializzati per pianificazione linguistica con constraint.</sample>
    <sample id="771">Il nome del relatore è Chu Huang.</sample>
    <sample id="772">Sì, i risultati e il set di dati dell'articolo possono essere utilizzati come parametri di riferimento per il problema di semplificazione automatica del testo in futuro.</sample>
    <sample id="773">Il testo non specifica quanti modelli più piccoli vengono utilizzati.</sample>
    <sample id="774">Il modello utilizzato come modello di base per analizzare l'ottimizzazione delle istruzioni multimodali è OFA, un modello unificato per la traduzione multimodale.</sample>
    <sample id="833">I fornitori di Google Translate sono gli autori dell'articolo.</sample>
    <sample id="834">I fornitori dell'articolo sono affiliati all'Università di Stony Brook.</sample>
    <sample id="835">Italiano e inglese</sample>
    <sample id="836">Il nome della relatrice o del relatore è Changbing.</sample>
    <sample id="837">Durante gli esperimenti, i modelli di Long Impart e Normal Base Long Impart sono stati studiati.</sample>
    <sample id="838">Per scopi di addestramento, 53 attività vengono utilizzate in MultiInstruct. Per scopi di test, 10 attività vengono utilizzate.</sample>
    <sample id="839">Cinque.</sample>
    <sample id="840">The authors conducted experiments on four datasets: AGNews, Mind, SST2, and ARSFAm.</sample>
    <sample id="876">NACHOS è un dataset di dati medici raccolti dalla rete.</sample>
    <sample id="877">Il nome della relatrice o del relatore è Ida Vilad.</sample>
    <sample id="878">La strategia del prompting ha un grosso impatto sulle prestazioni.</sample>
    <sample id="879">I fornitori dell'articolo sono collaboratori di Patrick Franasz, Emil Niu, Andrea F. Martinez e Graham Mubig.</sample>
    <sample id="880">I 5 istruzioni scritte da esperti sono: 1) Utilizza il modello per generare risposte alle domande; 2) Utilizza il modello per tradurre testi in diverse lingue; 3) Utilizza il modello per riconoscere oggetti in immagini; 4) Utilizza il modello per riconoscere sentimenti in testi; 5) Utilizza il modello per riconoscere entità in testi.</sample>
    <sample id="881">Gli autori propongono un compito di risoluzione di riferimento corretto per testare i modelli sull'utilizzo di informazioni provenienti da più fonti.</sample>
    <sample id="882">Hello everyone. My name is Ilya S. Bilingual, and I will be giving a short overview of the paper "Training from Translation: Assessing Strategies and Performance". This is joint work with my colleagues from Google Translate.</sample>
    <sample id="883">Bam è un modello di lingua naturale a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testi che comprende 780 miliardi di token.</sample>
    <sample id="884">La libreria di PyTorch è la migliore soluzione di stato dell'arte per centinaie di task di NLP.</sample>
    <sample id="885">In questa work, we present the first systematic study of large language model prompting for machine translation.</sample>
    <sample id="886">Valutiamo la capacità di traduzione dei modelli utilizzando le migliori pratiche della comunità MFT. Questo implica l'uso dei set di test più recenti per evitare qualsiasi sovrapposizione dei dati di test con i dati di addestramento del modello linguistico.</sample>
    <sample id="887">And we compare two state-of-the-art systems. so the best performing system is the dwa n t evaluation.</sample>
    <sample id="888">Usiamo state-of-the-art e nuovi metodi metrici e inoltre anche mostriamo risultati di valutazione basati su esperti. Infine, forniamo alcune raccomandazioni per le strategie di preselection.</sample>
    <sample id="889">Il prompting ha un grande impatto sulle prestazioni dei modelli di traduzione come possiamo vedere in un esperimento semplice dove utilizziamo due diverse istruzioni per la stessa frase.</sample>
    <sample id="890">The majority of sentences five hundred and sixteen out of one thousand the difference observed is of more than one blur point.</sample>
    <sample id="891">And this can go in extreme cases up to forty plot points. So it's important to select that good prompting strategy.</sample>
    <sample id="892">In our experiments we test for a fixed shot prompting strategy where we just mark each its sentence that we provide to the system with the language it's in.</sample>
    <sample id="893">In this example here where we perform translation from german into english, the german sentences the source sentences are marked with german colon and the english translations with english colon.</sample>
    <sample id="894">We saw that the actual form of the printing doesn't have a big influence in the case of serial short printing.</sample>
    <sample id="895">It's crucial for zero and one shot prompting and when we go as in our case to five shot prompting there is nearly no difference to the actual form of the, of the prompting.</sample>
    <sample id="896">It's the examples that carry most of the weight.</sample>
    <sample id="897">The summary of our experimental results is that the example quality is more important than the similarity to the source sentence.</sample>
    <sample id="898">È importante selezionare gli esempi da traduzioni di alta qualità in particolare. Inoltre, confrontiamo i prompt selezionati con i dati di addestramento e i dati di test delle valutazioni UMT.</sample>
    <sample id="899">The dev data is much more curated and with higher quality than the train data, that it's more nice and the results so a better performance when using the dev data.</sample>
    <sample id="900">Tuttavia i sistemi specializzati hanno un vantaggio sostanziale rispetto alle traduzioni umane ma quest'ultima si avvicina piuttosto a un sistema di traduzione automatico. In ogni caso, ci siamo decisi di utilizzare Google Translate.</sample>
    <sample id="901">I insights che otteniamo dalla umanizzazione che eseguiamo utilizzando il framework MQM sono che la fluidità di Palm è comparabile a state of the art dei sistemi ma la differenza principale deriva dalla precisione.</sample>
    <sample id="902">In particolare le most comuni errori ar omission errors.</sample>
    <sample id="903">Sembra che palmi scelga diem per produrre una traduzione più accurata a volte mettendo via parti della frase sorgente che sono armonizzate nella traduzione</sample>
    <sample id="904">Tuttavia, la tasso di uscita per BAN è inferiore rispetto ai sistemi di mercato che rappresentano un segnale aggiuntivo.</sample>
    <sample id="905">That PAM provides really fluent output but still with some problems of accuracy.</sample>
    <sample id="906">And that's it for this really short overview for more details please come by to the full presentation of the paper thank you very much</sample>
    <sample id="907">Ciao, mi chiamo Dawei, sono uno studente di laurea presso l'Università di Salamanca in Germania. In questo video vorrei presentare il mio recente lavoro, "Wickeder than you think: A critical look at weekly supermarket shopping".</sample>
    <sample id="908">Questa è un lavoro di gruppo con Qiu Yushen, Maio Smooth Bass e Gias Daphne and Dietrich Kruller.</sample>
    <sample id="909">I'd like to begin with a brief introduction to weak supervision and weakly-supervised learning.</sample>
    <sample id="910">In weak supervision, you do not manually label the data instead, we label the data using weak labeling sources such as simple heuristics rules, knowledge bases or low-quality crowdsourcing, as you illustrated in the figure on the right.</sample>
    <sample id="911">When compared to human annotations, the weak annotations are much cheaper yet they are also noisy meaning that a certain amount of the annotations are incorrect.</sample>
    <sample id="912">Se addestriamo direttamente reti neurali su dati etichettati settimanalmente, le reti neurali tendono a memorizzare il rumore dei label e non generalizzano.</sample>
    <sample id="913">In recent super-resolution training algorithms are proposed to robustly train neural networks on such label noise, so that the trained models still generalize well.</sample>
    <sample id="914">In recent works in wsl, so wsl stands for weakly supervised learning a common claim is that people say that they only train models on the weakly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="915">Tecnicamente, questa affermazione non è vera, ma c'è un problema.</sample>
    <sample id="916">Che è che i fornitori presumono che ci sia un set di validazione aggiuntivo, accessibile per la selezione del modello.</sample>
    <sample id="917">We can't stop on this problem setting as this implies that additional manual annotations are required in weakly supervised learning but like an elephant in the room this necessity is often overlooked.</sample>
    <sample id="918">Il riferimento adottato ci obbliga a chiedere tre domande di ricerca. Prima, è necessaria la validazione pulita per WSL o possiamo forse usare un set di validazione rumoroso invece?</sample>
    <sample id="919">Second, if clean data is required or if clean data is mandatory for wsl to work then how many clean samples do we need? Finally, should we only use the clean samples for validation or are there better ways to utilize them?</sample>
    <sample id="920">We addressed these research questions in our work, and our findings are as follows.</sample>
    <sample id="921">Prima di tutto, scopriamo che recenti metodi WSL in effetti richiedono campioni con libere di uscita per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci sono delle performance troppo basse, come in questa figura. Se non ci sono campioni di validazione puliti, i modelli di addestramento non possono generalizzare al di fuori dei relativi etichettini di origine.</sample>
    <sample id="923">Significando che l'addestramento è inutile.</sample>
    <sample id="924">Questo indica che i modelli WSL effettivamente richiedono un'annotazione accurata dei dati per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">Il nostro secondo scoperto è che aumentare il numero di campioni di validazione puliti aiuterà i modelli WSL a raggiungere un'efficienza migliore, come illustrato nella figura sulla sinistra.</sample>
    <sample id="926">Tipically, we only need twenty samples per class to attain high performance.</sample>
    <sample id="927">Ma, non è la fine della storia, perché se decidiamo di accedere a campioni puliti, allora addestrare il modello direttamente su di essi otterrà anche un migliore prestigio.</sample>
    <sample id="928">Il grafico a sinistra illustra la differenza di prestazioni tra i modelli fin-tuned direttamente sul dataset originale e quelli fin-tuned solo con i modelli che utilizzano i dati puliti per la validazione.</sample>
    <sample id="929">Come possiamo vedere se abbiamo dieci campioni per classe, direk fine-tuning inizia a superare wsl approaches.</sample>
    <sample id="930">Infine, l'incremento delle prestazioni dichiarato in precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare l'ottimizzazione su campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dai grafici, il modello Venera denominato FTDW inizialmente sottoperfora rispetto ai metodi più complessi di WSL come cosine.</sample>
    <sample id="932">However, if we allow to continue fine-tuning on the clean samples then ftw performs equally well as other</sample>
    <sample id="933">In pratica, non c'è motivo di usare metodi più complessi di WSL che richiedono più tempo di elaborazione e spazio su disco.</sample>
    <sample id="934">In sintesi, abbiamo dimostrato che recenti modelli di apprendimento supervisionato richiedono campioni puliti e manualmente annotati per funzionare correttamente. Le loro prestazioni e praticità sono pesantemente sovrastimata.</sample>
    <sample id="935">I nostre raccomandazioni conclusive per il lavoro futuro sono le seguenti.</sample>
    <sample id="936">Prima, rapporta i criteri di selezione del modello. Ad esempio, rapporta se la selezione del modello è basata su campioni di validazione puliti.</sample>
    <sample id="937">Secondo, i modelli WSL dovrebbero essere confrontati con baselines a lungo termine come sbarre work on client samples. Terzo, l'ottimizzazione continuata è un baselines semplice ma forte che dovrebbe essere considerato in future work in WSL.</sample>
    <sample id="938">Finalmente, abbiamo aperto sorgente il codice. Puoi trovarlo sul link del QR code in questa slide. Per favore, sentiti libero di controllarlo. Grazie e buona conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo includono l'uso di giudici umani per selezionare tra due conversazioni o per valutare le conversazioni utilizzando una scala di Likert.</sample>
    <sample id="940">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che i giudici decidono cause in una corte di legge.</sample>
    <sample id="942">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="943">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="944">Le frasi nel dominio accettabile sono state perturbate in modo simile, con un aumento nella percentuale di perturbazioni.</sample>
    <sample id="945">Valutare dimensionalmente significa analizzare la qualità del dialogo in più aspetti per capire le sue caratteristiche e le sue debolezze.</sample>
    <sample id="946">I ricercatori dell'articolo sono affiliati all'Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="947">La forma del prompting è cruciale per i promptings a zero e uno shot.</sample>
    <sample id="978">I modelli di dialogo che gli autori hanno testato sono i bot.</sample>
    <sample id="979">Il numero di autori coinvolti nell'articolo non è specificato nella descrizione fornita.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere piani che siano ragionabili e fedeli alle limitazioni.</sample>
    <sample id="981">Ci sono due autori coinvolti nell'articolo.</sample>
    <sample id="982">Il nome della relatrice o del relatore è Vasudeha.</sample>
    <sample id="983">I fornitori di servizi di sicurezza e i fornitori di servizi di sicurezza hanno affiliazioni.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono omission errors.</sample>
    <sample id="1022">Ecco, mi chiamo James Finch e mi chiamo Sarah Finch. Oggi ti useremo per dirti tutto su ABC-Eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale.</sample>
    <sample id="1023">Questo lavoro è stato fatto dal laboratorio NLP di Emory, guidato dal professor Geno Choi di Emory University e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Ecco la traduzione in italiano:

Ecco, immagina che tu abbia appena sviluppato un modello di dialogo e tu voglia vedere quanto bene si confronta con lo stato attuale dell'arte.</sample>
    <sample id="1025">La pratica comune è di utilizzare valutazioni umane, come ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni su una scala di Likert.</sample>
    <sample id="1026">Questi approcci funziona bene per fornire valutazioni holistici della qualità del dialogo in generale, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare diverse dimensioni della qualità del chat per capire le forze e le debolezze del modello a un livello più fine.</sample>
    <sample id="1027">Un approccio è quello di chiedere semplicemente ai giudici umani di valutare diversi aspetti della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi esistenti basati su scala comparativa o scala Likert.</sample>
    <sample id="1028">Tuttavia, crediamo che ci sia una strategia più precisa e affidabile per l'evaluation del dialogo dimensionale.</sample>
    <sample id="1029">La nostra approccio cerca di ridurre la soggettività della valutazione umana, rendendo esplicito se o no ogni risposta del modello esprime certi comportamenti, come fornire informazioni non rilevanti o contraddire se stessa.</sample>
    <sample id="1030">Chiamiamo questa approccio annotazione comportamenti in chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chatten che sono stati suggeriti di influenzare la qualità del chat in recenti pubblicazioni.</sample>
    <sample id="1031">ABC-Eval è capace di misurare le tassi con cui i modelli di chat commettono errori stilistici vari.</sample>
    <sample id="1032">Ad esempio, ABC-Eval misura il numero di giri in cui un modello di chatten ignora il proprio partner o dice qualcosa di non rilevante.</sample>
    <sample id="1033">Contradice se stesso o il proprio partner, elenca fatti incorretti o viola le conoscenze comuni e quando il modello riesce o fallisce a dimostrare compassione.</sample>
    <sample id="1034">Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chatbot di ultima generazione e li abbiamo valutati su 100 conversazioni umano-bots per modello utilizzando ABC-Eval.</sample>
    <sample id="1035">Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni con una scala di Likert al livello della parola, valutazioni con una scala di Likert al livello del dialogo e confronti a livello di dialogo pairwise.</sample>
    <sample id="1036">Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chattenzio lungo diverse dimensioni.</sample>
    <sample id="1037">Dall'analisi dei risultati di queste valutazioni, abbiamo scoperto che i etichettatori ABC eval hanno etichette più affidabili rispetto alle etichette raccolte da metodi esistenti, come misurato dalla concordanza tra gli etichettatori su 100 conversazioni doppioni etichettate.</sample>
    <sample id="1038">Inoltre, le etichette ABC EVL sono più predittive della qualità della conversazione in generale rispetto ai metri prodotti da metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice.</sample>
    <sample id="1039">Ad esempio, si può vedere come misurare la proporcione di turni con contrasti tra se stessi e partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza di Likert spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare a passi.</sample>
    <sample id="1041">Puoi vedere come la combinazione di tutti i metriche ABC eval spiega più del 25% della qualità della conversazione e, mentre rimuovi le metriche uno alla volta, la maggior parte di queste risulti in una perdita di quantità significativa di informazioni sulla qualità.</sample>
    <sample id="1042">Sul lato opposto, la combinazione di tutti i metri di livello di turno Lickert spiega molto meno della qualità e meno di questi metri trasmettono informazioni uniche.</sample>
    <sample id="1043">Questi metri ABC EVL affidabili, informativi e distinti ci permettono di valutare l'Intelligenza Artificiale Conversazionale con una risoluzione più alta rispetto ai metodi precedenti in grado di raggiungere.</sample>
    <sample id="1044">Puoi vedere che in risultati del nostro esperimento che diversi sfidano rimangono e hanno stati precisamente quantificati ad esempio i bot che testato hanno violazioni di buonsenso in circa il venti percento delle loro risposte.</sample>
    <sample id="1045">Prodotto informazione non rilevante in circa il 15% delle risposte e si contradcono tra loro o con il proprio partner circa il 10% del tempo.</sample>
    <sample id="1046">Con l'accelerato ritmo di miglioramento nel campo, molte di queste tassi di errori potrebbero vedere una diminuzione in nuove versioni rilasciate da quando è stata condotta la nostra valutazione. Tuttavia, questo è all'ancora di più motivo per perseguire metri di valutazione affidabili e precisi per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC eval possa essere sfruttata da altri nel campo come un passo significativo in questa direzione e ci guardiamo avanti a vedere come l'IA conversazionale avanzerà nei prossimi mesi e anni. Grazie per la visione.</sample>
    <sample id="1048">Gli autori dell'articolo sono affiliati all'Emory NLP Lab, alla Università Emory e collaborano con Amazon Alexa AI.</sample>
    <sample id="1049">CFT significa Continuous Fine-tuning.</sample>
    <sample id="1050">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="1051">Ciao, mi chiamo Kaiyuan e presenterò il nostro lavoro intitolato "Quando la traduzione richiede contesto: un'esplorazione multilingue guidata da dati". Questo lavoro è stato realizzato in collaborazione con Patrick Franasz, Emil Niu, Andrea F. Martinez e Graham Mubig.</sample>
    <sample id="1052">So a lot of translations depend on context. For example, how would we translate mole in this sentence?</sample>
    <sample id="1053">Se la frase precedente era che le cose potrebbero iniziare a diventare pericolose se i ministri scoprono, allora Moore si riferisce a un segno. Ma se la frase precedente era che potrebbe essere qualcosa di serio, dottore, allora Moore si riferisce a un segno di nascita.</sample>
    <sample id="1054">So depending on context, the meaning of the word changes and therefore its translation changes as well.</sample>
    <sample id="1055">Tuttavia, valutare quanto bene i modelli traducono casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola percentuale delle traduzioni dipende dal contesto, il che rende le metriche a livello di corpus come BLEU incapaci di catturare queste traduzioni.</sample>
    <sample id="1056">Alcuni hanno suggerito di utilizzare valutazioni mirate su traduzioni che dipendono dal contesto, ma queste risorse possono supportare solo tipi limitati di traduzioni che dipendono dal contesto e set di lingue limitati, poiché solitamente si basano su conoscenze umane e curazione.</sample>
    <sample id="1057">In this work, we try to answer these two questions first, when does translation require context and second, how well do models handle these cases?</sample>
    <sample id="1058">Per rispondere alla prima domanda, ci siamo concentrati su quanto un termine dipende dal contesto di traduzione.</sample>
    <sample id="1059">In the previous work, we introduced CxMI as a measure for context usage by machine translation models and this is done by measuring how much information the context c provides about the target y given the source x.</sample>
    <sample id="1060">Puoi pensare a C X M I come all'informazione guadagnata fornendo contesto al modello.</sample>
    <sample id="1061">In questa work, we extend c x m i to point-wise c x m i which can measure context usage at the sentence level or at the word level we can think of words that have high p c x m i as ones that require context for translation.</sample>
    <sample id="1062">Ora analizziamo parole con high p6xmi per vedere se ci sono modelli tra queste parole.</sample>
    <sample id="1063">And we perform our analysis on transcripts of ted talks that have been translated from english to fourteen different languages.</sample>
    <sample id="1064">We perform our analysis at three different levels. First, we look up parts of speech tags that have high means p x c x m i,</sample>
    <sample id="1065">E questa ci consente di trovare, ad esempio, i pronunti in arabo che hanno la stessa ipesimi e e, e questo può essere spiegato perché l'inglese non ha pronunti doppii, quindi è necessario contesto per determinare se un pronunto è doppio quando traducendolo in arabo.</sample>
    <sample id="1066">Sostanzialmente, scopriamo che certe lingue richiedono anche contesto quando dobbiamo scegliere la forma appropriata di un verbo. Ci concentriamo quindi su elementi lessicali che hanno un'average di frequenza elevata in tutte le loro occorrenze diverse.</sample>
    <sample id="1067">And this helps us identify cases like the one here where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document.</sample>
    <sample id="1068">E simili, noi notiamo che i codici si supportano per trascrivere in una forma corretta.</sample>
    <sample id="1069">E finalmente, analizziamo i token individuali che hanno un alto PESXMI e questo ci consente di identificare fenomeni che non possono essere catturati solo dal testo in sé ma piuttosto esprimendosi nella struttura istantanea, ad esempio l'elusione della risoluzione.</sample>
    <sample id="1070">So now we use our findings from our analysis to design a benchmark for document-level translation.</sample>
    <sample id="1071">Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per automaticamente identificare parole che pertengono al fenomeno e chiamiamo il nostro tagger il multilingue discorso aware o muda tagger.</sample>
    <sample id="1072">We can then also note that different languages have different proportions of these discursive phenomena.</sample>
    <sample id="1073">We then use the muda tagger by applying the tagger on the parallel corpora that we want to use for evaluation and we apply our translation metrics of choice on the cognitively dependent examples that the muda tagger has identified.</sample>
    <sample id="1074">And finally we use our benchmark as well as other metrics to evaluate different models on the document-level machine translation.</sample>
    <sample id="1075">Prima di tutto, quando utilizziamo metriche basate su corpus, ossia per il blu, scopriamo che i modelli cognitivi hanno le prestazioni migliori.</sample>
    <sample id="1076">But then if we use comet, context-aware models perform best and if we use word f measure, then models with or without context have comparable performance.</sample>
    <sample id="1077">Questa di nuovo dimostra che è difficile determinare il miglior sistema di traduzione a livello di documento se utilizziamo metriche a livello di corpus.</sample>
    <sample id="1078">Ora utilizziamo la metrica MUSA per valutare i modelli e scopriamo che i modelli che usano il contesto sono significativamente più accurati rispetto a quelli che non usano il contesto per certi fenomeni discorsivi, come la formalità e la coesione lessicale.</sample>
    <sample id="1079">Ma queste modelli non sono molto migliori dei modelli che non usano contesto su altre fenomena come elipsi, pronuncia e verb form. Quindi questa sorta di suggerisce dove avremmo bisogno di vedere più progresso per la traduzione a livello documenti.</sample>
    <sample id="1080">We also compared different commercial systems, and our benchmarks show that DeepL is usually more accurate than Google Translate for document-level translation.</sample>
    <sample id="1081">Per riassumere, eseguiamo un'analisi basata su dati che copre quattordici paia di lingue per identificare quando le traduzioni richiedono contesto.</sample>
    <sample id="1082">Ecco, utilizziamo i nostri raffinamenti per costruire un benchmark per la traduzione di documenti, che ci aiuta a identificare se i modelli per le sfumature discursive possono gestire bene o no e se i sistemi di traduzione sono buoni per la traduzione di documenti.</sample>
    <sample id="1083">Grazie mille per l'attenzione. Ci vediamo in ritardo.</sample>
    <sample id="1084">The name of the presenter is Usain John from Penn State University.</sample>
    <sample id="1121">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="1122">Il metodo utilizza le parole contrassegguate per identificare le parole che distinguono i gruppi contrasseguiti da quelli non contrasseguiti.</sample>
    <sample id="1123">I fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di traduzione hanno dichiarato che i fornitori di servizi di trad</sample>
    <sample id="1124">La prima struttura di dipendenza simmetrica menzionata è la struttura di Praga.</sample>
    <sample id="1125">Il nome della relatrice o del relatore è Sara Finch.</sample>
    <sample id="1126">Ci sono quattro autori coinvolti nell'articolo.</sample>
    <sample id="1127">I modelli linguistici possono essere utilizzati per testare i fenomeni sintattici.</sample>
    <sample id="1161">I metodi per la prima domanda di ricerca sono abbreviati come WSL.</sample>
    <sample id="1162">Il modello viene valutato su undici attività di immissione biomedica e clinica.</sample>
    <sample id="1226">CamemBERT è inizialmente addestrato su un subset di 4 GB di Natsos.</sample>
    <sample id="1227">Il nome del relatore è Adam Strykowscy.</sample>
    <sample id="1228">I risultati dell'experiment hanno dimostrato che la performance dei modelli degrada con un intervallo temporale più grande, confermando l'ipotesi che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">I token devono essere permutati per la sequenza di output perché, anche se hanno tutti i token necessari, non sono ordinati correttamente.</sample>
    <sample id="1270">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire se ci sono particolari motivi che stanno portando a queste tendenze preconizzate.</sample>
    <sample id="1271">Gli input inaccettabili di coppia minima sono le frasi grammaticalmente sbagliate.</sample>
    <sample id="1272">I metri di valutazione utilizzati dagli autori sono la precisione e il ricupero.</sample>
    <sample id="1273">La metrica utilizzata per misurare l'accordo tra i fornitori è la percentuale di accordo inter-annotatore.</sample>
    <sample id="1274">Il dominio scelto per aggiungere frasi completamente scollegate alle query inaccettabili e accettabili è Wikipedia.</sample>
    <sample id="1275">I fornitori dell'articolo sono affiliati a TU Darmstadt e TU Berlin.</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto è un dataset di addestramento multi-modal, mentre la maggior parte dei precedenti modelli si concentrano su task linguisticamente soli.</sample>
    <sample id="1277">To determine the number of authors involved in the article, we need to carefully analyze the provided information. The text states that the work was done by the Emory NLP Lab led by Professor Jino Choi at Emory University and in collaboration with Amazon Alexa AI. 

1. Identify the primary author: Professor Jino Choi is mentioned as the leader of the Emory NLP Lab.
2. Identify the collaborating entity: Amazon Alexa AI is mentioned as a collaborator.

From this information, we can conclude that there are two distinct entities or individuals involved in the creation of the article:
- Professor Jino Choi (primary author)
- Amazon Alexa AI (collaborating entity)

Therefore, the total number of authors involved in the article is 2.</sample>
    <sample id="1278">La coordinazione binaria è il processo di misurare la lunghezza dei caratteri in una colonna specifica.</sample>
    <sample id="1279">I prompt sono stati utilizzati per un periodo di tempo medio.</sample>
    <sample id="1280">I risultati suggeriscono che i modelli più piccoli di T5 possono superare i modelli più grandi quando sono adeguatamente addestrati su dataset appropriate.</sample>
    <sample id="1281">Ciao, mi chiamo Manis Lavac e presenterò i nostri modelli di Dr. Bert, un robusto pre-training modello in francese per il dominio biomedico e clinico.</sample>
    <sample id="1282">In questa presentazione, innanzitutto parliamo di modellazione linguistica in salute mentale. Poi presenteremo le principali contribuzioni del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese, chiamato Dr. Bert, che è basato su Roberta e train on Natsos, che è un dataset di dati medici scarolati dalla rete.</sample>
    <sample id="1284">We also introduce a comparison of models with multiple pre-training settings and data sources. Then, we present our results on eleven biomedical and clinical downstream tasks in french.</sample>
    <sample id="1285">And finally, we conclude about the experiments and give you more details about how to access to the models.</sample>
    <sample id="1286">Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione del linguaggio naturale e ha offerto un'enorme migliora rispetto ai metodi statici e costituzionalizzati storici come Word2Vec, FastText o WordPiece.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molte altre lingue, come in francese con Camembert e in tedesco con Pirmesbert e Biober, ma anche in clinico con Clinikalbert. Ma la maggior parte delle lingue sono in inglese.</sample>
    <sample id="1288">Specialized models for other languages are scarce and are often based on continual pretraining due to the lack of in-domain data.</sample>
    <sample id="1289">Tuttavia, French non aveva nessun open source moderno per biomedicina finora.</sample>
    <sample id="1290">We, so we ask ourselves question about what is the most appropriate data sources for a wide range of usage and those crowd data are good substitution for clinical data.</sample>
    <sample id="1291">Per risolvere questa questione, confrontiamo il modello di Bert con il nostro modello Shubert, che è basato su un'analisi dei dati ottenuti dal non universitario ospedale di Atta House.</sample>
    <sample id="1292">Successivamente, ci chiediamo quanta memoria è necessaria per addestrare un modello specializzato su dati in francese. E' 4 GB, 8 GB o di più?</sample>
    <sample id="1293">To answer this question, we first trained and compared four scratch models: a first version of doctorbert with seven gigabytes of natsios, a second version of four gigabytes subset of natios,</sample>
    <sample id="1294">Una prima versione di Shubert che è un modello clinico con 4 gigabyte di sentenze tratti da note cliniche e una versione finale di Shubert con un mix di 4 gigabyte subset di nazioni e 4 gigabyte di note cliniche.</sample>
    <sample id="1295">In addition to this comparison, we introduce three models trained on continual pretraining to analyze the impact of pretraining strategies.</sample>
    <sample id="1296">One based on the weight of camembert and trained on four gigabits of data, another also based on camembert but trained this time on the four gigabytes of klimanodot.</sample>
    <sample id="1297">E finalmente, one base of on English biomedical model per bimbert and trained on four gigabytes subset of natures. In total we have seven models.</sample>
    <sample id="1298">To evaluate our seven models, we gather much public and private domain tasks such as named entity recognition classification, part-of-speech tagging and question answering.</sample>
    <sample id="1299">Questi modelli sono confrontati a sei modelli Benzine, tra cui Camembert Oscar 108 GB, Camembert Oscar 4 GB, Camembert Cissinet 4 GB, Tummett by Albert e Clinical Albert.</sample>
    <sample id="1300">The evaluation of highlight that model perform best on the task with data of the same nature as those on which the model has been trained.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da fonti etologiche e osservare che i dati provenienti da queste fonti sembrano essere più versatile. Inoltre, notiamo che l'uso di più dati traduce in un miglioramento delle prestazioni.</sample>
    <sample id="1302">In generale, l'adesso pretraining sembra ottenere prestazioni più elevate su molte delle task.</sample>
    <sample id="1303">Tuttavia, il nostro esperimento di continuo pretraining, utilizzando il weight e tokenizer di bert, train on the four gigabyte subset of natasha show comparable results to those obtained with dr. bert four gigabyte from scratch.</sample>
    <sample id="1304">Non è il caso per il modello basato su camber weights e tokenizers, che soffrono di problemi di stabilità.</sample>
    <sample id="1305">Finalmente, come conclusione il proprio sistema offre un’ottima performance su nove dei venti task del DART e supera globalmente i risultati del modello generico qui chiamato</sample>
    <sample id="1306">We also observe that specialized data is better, more specialized data is better, but it doesn't scale well.</sample>
    <sample id="1307">All the pre-trained models obtained from nato's are freely available and on huggingface, and all the training scripts are on our github repository.</sample>
    <sample id="1308">So thank you for for this presentation and we are looking forward to exchange at the post session in toronto.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate nel lavoro sono la pre-addestramento continuo e la pre-addestramento con modelli a partire da zero.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è grande.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata utilizzando i punteggi ottenuti dalla fine-tuning dei modelli. I punteggi ottenuti dalla fine-tuning dei modelli sono stati considerati migliori rispetto ai punteggi di base e sono stati proposti come benchmark per il problema di automatic text simplification in futuro.</sample>
    <sample id="1312">Sì, i modelli linguistici hanno vari bias politici.</sample>
    <sample id="1313">Ciao, mi chiamo Mateusz Rendieman e oggi voglio darti un'introduzione rapida al nostro articolo sulla generalizzazione di composizione senza alberi utilizzando tag multi-set e permutazioni latenti.</sample>
    <sample id="1314">Questo è un lavoro in collaborazione con i miei consiglieri Alexander Koller e Ivan Titov.</sample>
    <sample id="1315">La generalizzazione composizionale può essere compresa come la capacità di un apprendente di gestire una maggiore ricorsione e composizioni nascoste di frasi che hanno essere state viste individualmente durante la formazione.</sample>
    <sample id="1316">In contesto di parsing semantico, testare per generalizzazioni composizionali potrebbe apparire così: come di consueto, abbiamo un insieme di esempi di frasi in questo caso, la fanciulla dormì e Mary sapeva che la fanciulla dormì.</sample>
    <sample id="1317">Queste affermazioni sono equipaggiate con forme logiche che rappresentano gli aspetti principali del loro significato.</sample>
    <sample id="1318">In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="1319">In questo esempio, il modello ha sperimentato ricorso superficiale durante la formazione e ha testato un esempio con ricorso più profondo.</sample>
    <sample id="1320">I modelli di sequenza a sequenza nevi si sforzano con questo tipo di generalizzazione fuori dalla distribuzione e spesso producono output che sono separati dal input.</sample>
    <sample id="1321">In particolare, essi spesso falliscono a riprodurre le corrispondenze sistematiche tra input e output, come quelli che sono color codeati nell'esempio.</sample>
    <sample id="1322">Un metodo popolare per affrontare questo è integrare i alberi nei modelli.</sample>
    <sample id="1323">I alberi sono intesi per catturare il processo di composizione che si relata ad affermazioni con le forme logiche.</sample>
    <sample id="1324">Questo funziona bene ma i tronchi di albero sono di solito non forniti e devono essere ottenuti in qualche modo.</sample>
    <sample id="1325">Questo può essere complicato e spesso un processo computazionalmente costoso. Di solito, questo implica un'elaborata formalizzazione specifica di pre elaborazione delle forme logiche, ad esempio per gestire simboli variabili.</sample>
    <sample id="1326">Ottener le radici può anche implicare procedure di immissione grammaticale specializzate.</sample>
    <sample id="1327">In questo articolo, non utilizziamo alberi e introduciamo un nuovo modello di sequenza a sequenza che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta, mostriamo una forte generalizzazioni a una ricorrenza più profonda senza fare ricorso a alberi.</sample>
    <sample id="1329">Il nostra approccia prevede l'uscita dalla entrata in due passi.</sample>
    <sample id="1330">Prima di tutto, assegniamo a ciascun token di input un insieme non ordinato di token che appariranno nell'output.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti ma non sono ordinati.</sample>
    <sample id="1332">Ecco il contenuto in italiano:

È per questo che, nel secondo passo, utilizziamo un altro modello per prevedere la permutazione e metterli in ordine corretto.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere una permutazione che non imposa alcun vincolo rigoroso sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo.</sample>
    <sample id="1334">Conceptually our permutation model works roughly like this.</sample>
    <sample id="1335">Andiamo da sinistra verso destra sul output e determiniamo quale token multi-set da mettere in ogni posizione. Per la prima posizione di output, semplicemente scegliamo uno come evidenziato in rosso.</sample>
    <sample id="1336">Allora, saltiamo al token multi set successivo per determinare il token successivo nell'output.</sample>
    <sample id="1337">Determiniamo il terzo token nell'output in modo simile saltando ad un altro token multi-settato. Continuiamo questo processo.</sample>
    <sample id="1338">Fino a quando ogni token dalla prima fase è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per fornirti un sapore di risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli a strati su una benchmark di Korg. Il nostro modello supera gli altri per un margine molto grande in quanto generalizza meglio verso una ricorsione più profonda.</sample>
    <sample id="1340">Alcuni altri tipi di strutturale generalizzazioni rimangono molto difficili, tuttavia.</sample>
    <sample id="1341">In il mio articolo, risolviamo un paio di interessanti sfide tecniche.</sample>
    <sample id="1342">Prima di tutto, l'allestimento tra input e output non è dato in dati di addestramento. Di conseguenza, per un token specifico non conosciamo da quale multi-settore è venuto, il che rappresenta un problema per l'addestramento.</sample>
    <sample id="1343">Inoltre, a volte ci sono molte permutazioni che sono coerenti con i dati, ma la versione grammaticalmente corretta è latente. Lo indirizzo iniettando l'allineamento come parte dell'addestramento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma introduce la sfida che trovare la permutazione con punteggio più alto è NP-difficile. Questo è perché questa è legata al problema del commercialista viaggiatore.</sample>
    <sample id="1345">Approximiamo questa con un rilassamento continuo amico al GPU che consente anche di propagare all'indietro attraverso la soluzione e imparare le permutazioni linguisticamente più plausibili.</sample>
    <sample id="1346">Se vuoi imparare di più sulle nostre esperienze e su come affrontiamo questi sfidati, ti prego di dare un'occhiata al nostro articolo o di venire a visitare il nostro stand.</sample>
    <sample id="1347">La dissonanza cognitiva è quando ci sono due credenze o azioni che non sono in accordo.</sample>
    <sample id="1348">GPT-4 è il modello linguistico più liberale.</sample>
    <sample id="1349">Sì, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="1350">Il nome della relatrice o del relatore è Sarah Papi.</sample>
    <sample id="1351">I dati nel parametro di riferimento MuDa sono stati tratti da transcrizioni di TED Talks tradotte in 14 lingue diverse.</sample>
    <sample id="1385">Il nome del relatore è Matthias Lendemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è un processo in cui un modello di apprendimento automatico viene addestrato su un linguaggio sorgente e successivamente utilizzato per prevedere i risultati su un linguaggio diverso.</sample>
    <sample id="1387">I membri dell'equipe sono studenti di PhD all'Università di Salamanca in Germania.</sample>
    <sample id="1388">Gli autori si basano su due misure di latenza: la latenza media e la latenza media computazionale.</sample>
    <sample id="1389">Ciao a tutti, mi chiamo Manjata e oggi io e il mio collaboratore Martin stiamo presentando un nostro lavoro che si intitola "KitMustE" evaluates knowledge integration from multiple sources. Questo lavoro è una collaborazione tra McGill University, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di intelligenza linguistica nazionale si basano su una varietà di fonti di conoscenza, tra cui la conoscenza contenuta nei loro parametri, di solito acquisita tramite un pretraining e la conoscenza fornita in input durante il tempo di inferenza.</sample>
    <sample id="1391">Recent works in tasks like question answering show that models can use pre-trained time knowledge to solve the task.</sample>
    <sample id="1392">Ma la comprensione del linguaggio naturale spesso richiede conoscenze che sono anche fornite in tempo reale.</sample>
    <sample id="1393">Ad esempio, in la frase John ha visto il nuovo presidente eletto alla televisione.</sample>
    <sample id="1394">I parametri di addestramento precedenti possono contenere informazioni su cosa è un presidente e cosa è un atto di servizio, ma non possono rilevabilmente conoscere chi è questa entità specifica istante o chi è il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo l'addestramento.</sample>
    <sample id="1395">Tuttavia, modelli riusciti per compiti di intelligenza artificiale ad alta intensità conoscenziale richiedono la capacità di integrare e utilizzare sia conoscenza pre-addestrata che conoscenza in tempo reale.</sample>
    <sample id="1396">In this work we propose a diagnostic test suite for knowledge integration.</sample>
    <sample id="1397">Introduciamo un compito di risoluzione di correlanza progettato per testare la capacità di estrarre conoscenze disponibili in diverse fonti. Evaluiamo il dataset con percorsi di studio umani e costruiamo modelli di risoluzione di correlanza.</sample>
    <sample id="1398">Ecco un esempio dalla nostra base di dati. Tervin è un giudice. Kia è un pastoressa. Tervin e Kia si sono incontrati in un parco dopo una lunga giornata al lavoro, decidendo casi in un tribunale. Era felice di rilassarsi.</sample>
    <sample id="1399">La task qui consiste a identificare l'entità corretta che il pronome 'he' si riferisce a, in questo caso, servizi.</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione: prima, conoscenza specifica dell'entità, come ad esempio che "Savile è un giudice"; e secondariamente, conoscenza generale, come ad esempio che i giudici decidono cause in tribunali.</sample>
    <sample id="1401">In generale, le conoscenze di sfondo vengono imparate durante la pre-adesione dei modelli di lingua大型, mentre le conoscenze specifiche dell'entità sono di solito osservate all'inferenza.</sample>
    <sample id="1402">Vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="1403">We have defined three settings of kipmos first with the tablica setting background pretrain where background knowledge is assumed to be available at pretraining time.</sample>
    <sample id="1404">Second, there is the background both setting where background knowledge is available at both pre-training time and inference time. Lastly, the background inference setting where both knowledge types are available only at inference time.</sample>
    <sample id="1405">Questa ultima impostazione è particolarmente interessante, poiché simula il caso in cui la conoscenza del contesto necessaria per risolvere un compito non è parte dei modelli pre-addestrati. Ad esempio, poiché nuove occupazioni sono sviluppate da quando i modelli sono stati pre-addestrati.</sample>
    <sample id="1406">Here's an example of how we can control the availability of effects in two sources.</sample>
    <sample id="1407">In un contesto pre-addestrato, supponiamo che il conoscimento di sfondo politici cerchi di ottenere seggi nel governo sia contenuto nei parametri pre-addestrati. In un contesto specifico, forniamo il conoscimento anti-specifico di Chesterfield come politico.</sample>
    <sample id="1408">In entrambe i contesti, inoltre, forniamo non solo informazione antispecifica ma anche conoscenza di sfondo sulla politica e sul contesto di violenza.</sample>
    <sample id="1409">In un contesto inferiore, fornire un'occupazione effettiva all'immigrazione, invece di politici, perché l'immigrazione è difficile da contenere in un trattato.</sample>
    <sample id="1410">Valutiamo il dataset sia con partecipanti umani che con modelli di frequenza e risoluzione. In questa figura, mostriamo i risultati dei migliori modelli sulle varianti più difficili del contesto pre-addestrato.</sample>
    <sample id="1411">Without task-specific training on Kidmos both models do not perform well. When trained on Kidmos however, both C2F and BERT for QF perform significantly better than the random choice.</sample>
    <sample id="1412">Suggerisce che quando addestrati su dataset di riconoscimento generale di immagini, i modelli imparano a sfruttare cenni superficiali che non sono utili quando si testa su KITTI, poiché tali cenni sono stati rimossi.</sample>
    <sample id="1413">Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time.</sample>
    <sample id="1414">Per riassumere i principali punti di rilievo del nostro articolo, molte rappresentazioni rivoluzionarie sembrano incapaci di ragionare su conoscenze provenienti da diverse fonti senza un addestramento specifico. Tuttavia, con un addestramento specifico, alcune rappresentazioni riescono a integrare le conoscenze provenienti da diverse fonti.</sample>
    <sample id="1415">Anche i modelli più performanti sembrano avere difficoltà a integrare in modo affidabile la conoscenza presentata solo al momento dell'inferenza. Se siete interessati ad ulteriori dettagli, vi prego di leggere il paper e controllare il dataset in codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="1416">I metodi basati su alberi hanno diversi svantaggi, tra cui la complessità e l'uso di processi computazionalmente costosi per ottenere gli alberi. Inoltre, spesso richiedono un preprocessing specifico delle forme logiche e possono implicare procedure di immissione grammaticalizzata specializzate.</sample>
    <sample id="1417">I fornitori di servizi di cloud computing.</sample>
    <sample id="1418">Ciao, mi chiamo Myra e oggi parliamo del mio articolo su personaggi marcato con i pregi usati in linguaggio per misurare i stereotipi in modelli di linguaggio. Questo lavoro è stato fatto in collaborazione con Essin Darmush e Dan Juravsky.</sample>
    <sample id="1419">In recent years, many have documented the prevalence of social bias and stereotypes in large language models or LLMs.</sample>
    <sample id="1420">Tuttavia, queste misure hanno varie limitazioni. Di solito si basano su datasette costruiti a mano che richiedono molto tempo per curare.</sample>
    <sample id="1421">E anche di solito misurano solo stereotipi molto specifici, che significa che non generalizzano bene a altre demografiche o contesti o semplicemente catturano associazioni molto generali e larghe, come le associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Tuttavia, la maggior parte del lavoro in questo campo non tiene conto della intersecatività, che è la nozione che identità sociali multifacettate possono aggregare bias e essere unico luogo di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci si basa sul fatto che questi modelli più recenti addestrati con istruzioni sono molto bravi a rispondere a istruzioni e prompti.</sample>
    <sample id="1424">Quindi possiamo chiedere al modello di generare un profilo, che è una descrizione di un individuo immaginario utilizzando un prompt come: immagina che tu sia una donna asiatica. Descrivi te stessa.</sample>
    <sample id="1425">E possiamo vedere immediatamente che questa è molto generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi segnaposto di identità che vogliamo in questo prompt.</sample>
    <sample id="1426">Ecco alcune generazioni esempi da GPT-4.</sample>
    <sample id="1427">Subito vediamo che, anche se i output non sono negativi o tossici nel senso tradizionale di queste parole,</sample>
    <sample id="1428">Ci sono alcuni interessanti modelli.</sample>
    <sample id="1429">La donna asiatica è descritta come non impressionante la donna del Medio Oriente è riferita utilizzando parole come esotici e come riferendosi a una regione mesmerica.</sample>
    <sample id="1430">E entrambe le donne di colore personaggi fanno riferimento all'antistere, mentre il personaggio del bianco non ha niente del genere.</sample>
    <sample id="1431">To capture these patterns, our method has two parts. The first one is generating these personas.</sample>
    <sample id="1432">I nostri prompt per generare queste personaggi sono stati ispirati a un studio in cui hanno fornito questi prompt a soggetti umani, scoprendo che, fornendogli a soggetti umani, erano anche in grado di raffigurare stereotipi razionali.</sample>
    <sample id="1433">E anche questo consente una comparazione diretta tra i nostri personaggi generati e le risposte umane scritte.</sample>
    <sample id="1434">The second part is marked words, which is a method to identify the words that distinguish marked groups from unmarked ones. Which I'll elaborate on shortly.</sample>
    <sample id="1435">The benefit of this is that we get really specific stereotypes and patterns without having to rely on any specific lexicon.</sample>
    <sample id="1436">Il metodo dei segni marcato si basa sul concetto sociolinguistico di marcità, che afferma che ci è un default non marcato e qualsiasi gruppo che devia da quell' default è linguisticamente marcato.</sample>
    <sample id="1437">Ad esempio, la parola "uomo" o scusa, la parola "guerriero" è solitamente associata con gli uomini. Quindi quando le persone stanno descrivendo un guerriero che è una donna, di solito specificano "un guerriero femmina" e mettono il termine con "femmina".</sample>
    <sample id="1438">E più in generale, i gruppi dominanti in una società sono sia linguisticamente che socialmente non segnalati, mentre i gruppi marginalizzati sono generalmente segnalati.</sample>
    <sample id="1439">In our method, we first designate what the unmarked and marked groups are.</sample>
    <sample id="1440">E quindi confrontiamo i personaggi utilizzando il metodo dei fitting words, che è in sostanza utilizzare razionali logit ponderate per distinguere le parole più importanti per ciascun gruppo.</sample>
    <sample id="1441">Per esempio, per le persone di colore nero, faremmo analisi di parole combattive e confronterei le proporzioni dei rapporti di legge e ordine rispetto sia alle persone bianche che alla gente maschile, poiché queste sono le due gruppi non marcato corrispondenti.</sample>
    <sample id="1442">Adesso, per i risultati. Allora, per prima cosa, utilizziamo un lessico di stereotipi e scopriamo che le personaggi generate contengono molto più stereotipi rispetto a quelli scritti umani.</sample>
    <sample id="1443">Tuttavia, quando ci guardiamo alla distribuzione dei vocaboli nel lessico, trovi diverse cose.</sample>
    <sample id="1444">Quindi, mentre le personaggi generate hanno tassi molto più alti di parole come 'luxe' e 'lax', i personaggi scritti da umani hanno una distribuzione molto più ampia di parole. Mentre le parole stereotipate che sono presenti nei personaggi generati sono davvero solo parole come 'tall' e 'athletic'.</sample>
    <sample id="1445">Sono solo le positive, o almeno non negative.</sample>
    <sample id="1446">In effetti, questa lessicografia non cattura molte delle patterns dannose che abbiamo visto in precedenza. Così invece, per farlo, useremo i risultati del nostro metodo di parole segnalate per vedere come queste parole positive facilitano i stereotipi e le narrativa essenzializzanti.</sample>
    <sample id="1447">In our analysis, we review how these seemingly positive court trials reflect harmful patterns.</sample>
    <sample id="1448">Prima di tutto, per i gruppi marcati, le parole chiave includono cose come cultura, tradizione, orgoglio e esotismo. Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Questo ha portato a una lunga eredità di discriminazione e marginalizzazione per questi gruppi.</sample>
    <sample id="1450">Tuttavia, ci sono molte troppe comuni che vengono riflette in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibrante e curvaceous.</sample>
    <sample id="1451">Which connect to a trope of tropicalism for asian women, the words are things like petite and delicate and silky.</sample>
    <sample id="1452">Which connects to a long history of Asian women being hyper-sexualized, seen as very docile and submissive, and so on.</sample>
    <sample id="1453">E finalmente, per le donne nere, vediamo che alcune delle parole più frequenti sono cose come forti e resilienti.</sample>
    <sample id="1454">Questo si connette a un archetipo che gli altri hanno chiamato lo archetipo della donna forte nera e se suona positivo all'occhio di primo grosso,</sample>
    <sample id="1455">Ci sono state ricerche che hanno dimostrato che questo tipo di archetipo è davvero dannoso, poiché mette molto pressione su queste demografie per essere resilienti e forti contro le difficoltà sociali.</sample>
    <sample id="1456">Sono piuttosto che cercare di superare queste ostacoli, mette pressione su queste persone per superarli, che conduce a un'uscita negativa per questi stessi e ad altri danni.</sample>
    <sample id="1457">Inoltre, notiamo che le parole per ciascun gruppo di riferimento sembrano riflettere narrativa essenziale.</sample>
    <sample id="1458">In base a queste pattern, arriviamo a tre raccomandazioni per i proprietari di modelli.</sample>
    <sample id="1459">Prima di tutto, dovremmo come ricercatori affrontare i stereotipi positivi e le narrazioni essenzialiizzanti. Dovremmo anche utilizzare le analisi intersezionali per studiare le biasse e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">E infine, dovrebbe esserci un'incrementata trasparenza riguardo ai metodi di gestione dei bias.</sample>
    <sample id="1461">Perché, ad esempio, come queste stereotipi positivi che non sappiamo se è per via di un qualche tipo di strano</sample>
    <sample id="1462">Overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns.</sample>
    <sample id="1463">We just really can't make any assumptions or really study that further without more transparency.</sample>
    <sample id="1464">Grazie mille per l'ascolto. Spero che avrai un buon tempo a</sample>
    <sample id="1465">Ciao a tutti, il mio nome è Jing Wei Yi dalla Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="1466">È mio piacere fornire un breve video di pubblicità su Ab, Paper Are You Coping My Model Proteggendo i diritti d'autore dei modelli di lingua grande per imbedding e servizi Vibe Backdoor Watermark.</sample>
    <sample id="1467">Per prima cosa, introduciamo il contesto riguardante i servizi di imbedding.</sample>
    <sample id="1468">Attualmente, modelli di lingua grandi come GPT, Llama e PELM sono eccezionali in comprensione e generazione di linguaggio naturale.</sample>
    <sample id="1469">Embedding as services is one of the services built upon large language models to assist various NLP tasks.</sample>
    <sample id="1470">Ad esempio, OpenAI offre un API basata su GPT per la traduzione.</sample>
    <sample id="1471">Tuttavia, recenti studi hanno dimostrato che l'attaccatore può imitare il modello attraverso l'apprendimento da embedding e fornire servizi simili. Di conseguenza, è necessario proteggere la proprietà intellettuale dell'embedding come servizio.</sample>
    <sample id="1472">Per proteggere i diritti d'autorité des services d'embedding, l'une des solutions consiste à imprimer un marqueur dans le service fourni et à détecter si un autre service contient le marqueur.</sample>
    <sample id="1473">Il metodo di watermarking dovrebbe soddisfare le seguenti proprietà: in primo luogo, il metodo dovrebbe essere applicabile all'inserimento di servizi; in secondo luogo, il watermark non dovrebbe indebolire l'utile dei servizi inclusi.</sample>
    <sample id="1474">Terzo, il segno d'acqua dovrebbe essere足夠隱藏對攻擊者或攻擊者可以輕易地移除水印。</sample>
    <sample id="1475">Finalmente, il materiale deve essere trasferibile alle servizi di attacco durante il processo di estrazione modulare.</sample>
    <sample id="1476">I libri esistenti possono essere classificati in quattro category.</sample>
    <sample id="1477">Tuttavia, queste tecniche non sono applicabili all'embeddining di servizi o mancano di trasferibilità.</sample>
    <sample id="1478">Quindi in questo articolo proponiamo il metodi di embedding che è un metodo basato sul watermark applicabile all'embedding dei servizi.</sample>
    <sample id="1479">Allora, introduciamo i dettagli del marcatore imbeddabile. Il marcatore imbeddabile comprende due passi principali: l'iniezione di segno di possesso e la verificazione di diritti d'autorité.</sample>
    <sample id="1480">Prima di questi passi principali, selezioniamo un insieme di trigger. L'insieme di trigger è un gruppo di parole in un intervallo di frequenza moderato.</sample>
    <sample id="1481">Assumiamo che il fornitore possa raccogliere un insieme generale di testi e contare la frequenza delle parole in essi.</sample>
    <sample id="1482">In Watermark Injection, we first define a target embedding when a user sends a sentence to the provider service. The provider counts the trigger number in the sentence.</sample>
    <sample id="1483">La rappresentazione fornita è una somma ponderata della rappresentazione di destinazione e della rappresentazione originale.</sample>
    <sample id="1484">Il peso del payload è proporzionale al numero di trigger nella frase quando il numero di trigger nella frase è maggiore di m, il payload fornito è esattamente uguale al payload di destinazione.</sample>
    <sample id="1485">Verifica di copyright è per individuare se un modello o un altro servizio contiene un marchio registrato.</sample>
    <sample id="1486">Prima di tutto costruiamo un backend e un dataset benigno. Il dataset backend contiene sentenze di cui tutti i vocaboli appartenono al set trigger, mentre tutti i vocaboli nella sentenze del dataset benigno non appartengono al set trigger.</sample>
    <sample id="1487">The provider requests embeddings from the Steelr service with the dataset.</sample>
    <sample id="1488">La cosine e l'L2 similitudine tra il embedding richiesto e l'embedding di destinazione sono calcolati. Calcoliamo la differenza di similitudine tra i dataset benigni e i dataset backdoor, definita come delta coseno e delta L2.</sample>
    <sample id="1489">Inoltre applichiamo il test KS e utilizziamo il suo p-value come terza metrica.</sample>
    <sample id="1490">Conduciamo esperimenti su quattro dataset: AG News, Mind, SST-2 e ER-Sem. Assumiamo che il fornitore applichi i dataset WikiText per conteggio frequenza di parole.</sample>
    <sample id="1491">I risultati su quattro dataset dimostrano che il nostro modello di segnalatura può ottenere prestazioni di rilevamento elevate senza compromettere l'utile funzionalità per i compiti di downstream.</sample>
    <sample id="1492">We also validate the covertness of the provided embedding by visualizing the embedding of sentences on the dataset BoolPCA The legend of the figures means the number of triggers in each sentence.</sample>
    <sample id="1493">Come si vede nella figura, è difficile distinguere tra le impiantazioni a retrodotto e le impiantazioni normali.</sample>
    <sample id="1494">Ecco la traduzione in italiano del contenuto in inglese:

"Questo è tutto, grazie mille. Benvenuti a discutere con noi."</sample>
    <sample id="1495">ABC-Eval si riferisce all'approccio di annotazione dei comportamenti in chat sviluppato per coprire in modo esaustivo i comportamenti dei modelli di chat suggeriti da recenti pubblicazioni per influenzare la qualità del chat.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2023.</sample>
    <sample id="1497">Ciao, mi chiamo Vasudha e sono un candidato per il PhD in Computer Science a Stony Brook University. Vorrei presentare un mio lavoro accettato in ACL 2023 come un articolo lungo su "Transfer Learning per la deteczione di diffondersi: affrontando il problema della classe rara".</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare in linguistica. Simplemente, la dissonanza cognitiva si riferisce a due credenze o azioni che sono in contrasto tra loro.</sample>
    <sample id="1499">Come in questo esempio, in cui una persona afferma di sapere che i sigarette possono uccidermi e successivamente dice di aver fumato un paio di sigarette dopo la riunione. Questa credenza e azione sono incoerenti e si situano in dissonanza.</sample>
    <sample id="1500">Menzionando che non penso di poter tenere il mio lavoro senza di loro giustifica la seconda ricorrenza e hanno una relazione di confidenza.</sample>
    <sample id="1501">La dissonanza è un fenomeno molto comune che esperiamo in decision-making quotidiano, ma sono davvero rari di trovare espresso in linguaggio tra altre tipologie di relazioni rischiose.</sample>
    <sample id="1502">Cos'ha di importante studiare la disabilità cognitiva? Studiare la disabilità cognitiva ci aiuta a capire gli effetti della disabilità tra le persone, traccia tendenze e credenze, valori e atteggiamenti in una popolazione.</sample>
    <sample id="1503">La disfunzione cognitiva elevata è anche correlata a disturbi ansiosi e può aiutare a comprendere meglio lo stato mentale dei pazienti.</sample>
    <sample id="1504">Studiare disidiosi espressivi in lingua può anche essere beneficiale in comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la disfunzione cognitiva è importante per comprendere i stili cognitivi personali degli individui e aiuta a capire meglio i processi di decisione.</sample>
    <sample id="1506">Per creare un risorsa di dissonanza cognitiva, abbiamo condotto una grande annotazione di relazioni di dissonanza. Abbiamo usato il primo approccio di dissonanza, come si vede nel diagramma qui.</sample>
    <sample id="1507">Tweets were parsed using a patty b parser and pairs of discourse units were annotated according to the guidelines that are described in our paper.</sample>
    <sample id="1508">Come si può vedere qui, la disidroisia è stata riscontrata in solo il 3,5% dei piani analizzati.</sample>
    <sample id="1509">Collectando circa mille esempi di coppie unità discorso, hanno addestrato un classificatore iniziale solo su quaranta esempi di discorsi. Non sorprende che il classificatore non si sia dimostrato molto migliore del caso.</sample>
    <sample id="1510">In data set, we are facing the problem of absolute rarity.</sample>
    <sample id="1511">Per alleviare questo, sperimentiamo su combinazioni di apprendimento passivo e imparare attivo per raccogliere più campioni di dissonanza in modo più efficiente. Questo ci aiuta a ridurre i costi di annotazione generale e migliorare la capacità di rilevare la dissonanza.</sample>
    <sample id="1512">Since the initial model was not able to capture the dissidents class at all, we start the active learning process by transferring weights from closely related tasks.</sample>
    <sample id="1513">Passiamo da due task diversi: topic, indipendente dissenso stante classificazione. Un task che determina se due dichiarazioni di dibattito provenienti da diverse persone sono in accordo o in disaccordo, indipendentemente dal topic.</sample>
    <sample id="1514">Chiamato qui e su classificazione binaia di espansione e compresso classi di PDB, poiché queste due sono strettamente legate alla concezione di consonanza e dissonanza e le chiamiamo CE qui.</sample>
    <sample id="1515">We find that on transferring the zero-shot performance on the annotated data set is already much better than chance, with the best with AUC 0.62.</sample>
    <sample id="1516">Ulteriormente, inerente all'ottimizzazione iterativa su entrambe le tavolette, scopriamo che l'ottimizzazione di C e T seguita da ulteriori ottimizzazioni su debate rende un'evoluzione migliore per la performance zero-shot. Questo è il modello che utilizziamo per costruire l'apprendimento attivo.</sample>
    <sample id="1517">Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di apprendimento attivo e annotazioni. L'aggregatore accumula tutti i dati raccolti fino ad ora, mentre l'iteratore aggiorna il modello allenandolo sul più recente insieme di dati raccolti.</sample>
    <sample id="1518">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1519">Successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo successo</sample>
    <sample id="1520">We compared this to the other state-of-the-art state-of-the-art strategies that are commonly used in the community.</sample>
    <sample id="1521">Trovi che la strategia di PRC proposta funziona meglio rispetto alle altre strategie state-of-the-art, anche se la differenza è piccola. Nota che il performance è significativamente più basso per i random.</sample>
    <sample id="1522">On further rounds of ayl with two best strategies we improved disease classification au c to point seven five, which is the best performance that we have on the task so far.</sample>
    <sample id="1523">Inoltre, si ha anche analizzato la fattibilità di ogni strategia in termini di qualità dell'annotazione e costo per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di disaccordi e funziona meglio per il reale contesto; tuttavia, gli annotatori ritengono che gli esempi siano difficili.</sample>
    <sample id="1524">In summary, we find that PRC is a simple AEL strategy for rare class acquisition and cold-starting AEL with appropriately designed transfer learning tasks and help significantly.</sample>
    <sample id="1525">Trovano anche che l'aggiornamento iterativo è utile per il trasferimento di apprendimento da un dominio diverso, mentre gli aggiornamenti attivi in-dominio beneficano da aggiornamenti cumulativi.</sample>
    <sample id="1526">Questi sono i link al dataset e al mio articolo. Sentitevi liberi di contattarmi se avete domande. Grazie.</sample>
    <sample id="1527">I fornitori di informazioni non hanno fornito le informazioni necessarie per determinare le affiliazioni degli autori dell'articolo.</sample>
    <sample id="1528">Il nome della relatrice o del relatore è Si Yu Yan.</sample>
    <sample id="1529">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="1530">L'approccio viene confrontato con l'architettura simulST dedicata.</sample>
  </task>
</testset>