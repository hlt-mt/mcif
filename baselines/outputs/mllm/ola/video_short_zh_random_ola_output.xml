<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是预训练数据。</sample>
    <sample id="1">根据幻灯片显示，论文的作者分别来自三个不同的机构。 麦吉尔大学和米拉是两个独立的机构，而微软研究也是另一个独立的机构。 这些机构的名称在每个作者的头像下方列出，表明他们与这些机构有关联。</sample>
    <sample id="2">DEPLAIN：一个德语平行语料库，其中包含句子和文档的内在简化翻译，用于简洁语言。</sample>
    <sample id="3">DEPLAIN: 一个德语平行语料库，其中包含简洁语言的内在翻译，用于句子和文档简化。</sample>
    <sample id="4">文本简化示例

原始文本：

- "Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne gezahlt werden."

简化文本：

- "Die Gewerkschaft setzt sich zum Beispiel für höhere Löhne oder mehr Urlaub ein."

插入文本：

- "und"</sample>
    <sample id="5">这个幻灯片展示了文本简化的一个例子，它是一种自然语言处理技术，用于使文本更易于理解。它包括原始文本和简化后的文本，以及在简化过程中涉及的几个步骤。原始文本是用德语写的，翻译成英文如下：

- 原始文本：'Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne oder mehr Urlaub werden.'
- 简化后的文本：'The union advocates for higher wages or more vacation time.'

幻灯片详细说明了文本简化涉及的四个步骤：

1. 替代：这是指用更简单的词语或短语替换原始文本中的复杂词语或短语的过程。例如，在原始文本中使用“Gewerkschaft”（union）替代“Gewerkschaft”（union），使用“höheres Löhne”（higher wages）替代“höheres Löhne”（higher wages）。
2. 从句删除：这一步涉及删除原始文本中的从句，以简化句子结构。例如，在原始文本中删除“dass zum Beispiel”（for example）。
3. 重新排列：这一步涉及重新排列单词或短语的顺序，以使句子更易于理解。例如，在简化后的文本中将“höheres Löhne”（higher wages）放在“oder mehr Urlaub”（or more vacation time）之前。
4. 单词删除：这是指删除原始文本中不必要的单词或短语的过程。例如，在简化后的文本中删除“zum Beispiel”（for example）。

这些步骤有助于使文本更易于理解，特别是对于那些可能无法完全理解原始文本的人。</sample>
    <sample id="6">这个幻灯片展示了文本简化示例，将复杂的德文句子转换为更简单的版本。它首先显示了原始句子，然后通过替换、删除从句、重新排列和删除单词来逐步简化句子。在每个步骤中，幻灯片都用不同的颜色突出显示了被修改的部分，以清晰地展示简化过程。最后，幻灯片展示了简化后的句子，即“公司为了更高的工资或更多工作时间而设定目标。”</sample>
    <sample id="7">这个幻灯片展示了文本简化过程的一个例子。它首先引入了原始文本，然后展示了如何通过替换、删除从句、重新排序和删除单词来简化文本。在原始文本中，有一个句子用德语写成，翻译成英文为“Die Gewerkschaft setzt sich für ein, dass zum Beispiel höhere Löhne oder mehr Urlaub werden.” 然后，幻灯片展示了简化后的文本，其中一些词被替换（例如，“dass” 替换为 “für”），一些词被删除（例如，“zum Beispiel” 被删除），句子的结构发生了变化（例如，“höhere Löhne” 和 “mehr Urlaub” 被提到），并且插入了一个新词（例如，“oder” 被插入）。</sample>
    <sample id="8">2. DE-plain
A New Corpus</sample>
    <sample id="9">收到。这是关于德语文本简化语料库的幻灯片，展示了不同来源的句子数量。幻灯片分为两个主要部分：左侧的柱状图和右侧的饼图。

左侧的柱状图显示了不同来源的句子数量。柱状图由多个柱组成，每个柱代表一个不同的来源。柱的高度表示该来源的句子数量。柱状图的标题为“来源”，底部的x轴显示了不同的来源，包括“Paderborn Corpus”、“Transkriptor”、“ZDF”、“ZDF-Aufnahmen”、“ZDF-Aufnahmen (2013)”、“ZDF-Aufnahmen (2015)”、“ZDF-Aufnahmen (2017)”、“ZDF-Aufnahmen (2019)”、“ZDF-Aufnahmen (2021)”、“ZDF-Aufnahmen (2023)”、“ZDF-Aufnahmen (2025)”、“ZDF-Aufnahmen (2027)”、“ZDF-Aufnahmen (2029)”、“ZDF-Aufnahmen (2031)”、“ZDF-Aufnahmen (2033)”、“ZDF-Aufnahmen (2035)”、“ZDF-Aufnahmen (2037)”、“ZDF-Aufnahmen (2039)”、“ZDF-Aufnahmen (2041)”、“ZDF-Aufnahmen (2043)”、“ZDF-Aufnahmen (2045)”、“ZDF-Aufnahmen (2047)”、“ZDF-Aufnahmen (2049)”、“ZDF-Aufnahmen (2051)”、“ZDF-Aufnahmen (2053)”、“ZDF-Aufnahmen (2055)”、“ZDF-Aufnahmen (2057)”、“ZDF-Aufnahmen (2059)”、“ZDF-Aufnahmen (2061)”、“ZDF-Aufnahmen (2063)”、“ZDF-Aufnahmen (2065)”、“ZDF-Aufnahmen (2067)”、“ZDF-Aufnahmen (2069)”、“ZDF-Aufnahmen (2071)”、“ZDF-Aufnahmen (2073)”、“ZDF-Aufnahmen (2075)”、“ZDF-Aufnahmen (2077)”、“ZDF-Aufnahmen (2079)”、“ZDF-Aufnahmen (2081)”、“ZDF-Aufnahmen (2083)”、“ZDF-Aufnahmen (2085)”、“ZDF-Aufnahmen (2087)”、“ZDF-Aufnahmen (2089)”、“ZDF-Aufnahmen (2091)”、“ZDF-Aufnahmen (2093)”、“ZDF-Aufnahmen (2095)”、“ZDF-Aufnahmen (2097)”、“ZDF-Aufnahmen (2099)”、“ZDF-Aufnahmen (2101)”、“ZDF-Aufnahmen (2103)”、“ZDF-Aufnahmen (2105)”、“ZDF-Aufnahmen (2107)”、“ZDF-Aufnahmen (2109)”、“ZDF-Aufnahmen (2111)”、“ZDF-Aufnahmen (2113)”、“ZDF-Aufnahmen (2115)”、“ZDF-Aufnahmen (2117)”、“ZDF-Aufnahmen (2119)”、“ZDF-Aufnahmen (2121)”、“ZDF-Aufnahmen (2123)”、“ZDF-Aufnahmen (2125)”、“ZDF-Aufnahmen (2127)”、“ZDF-Aufnahmen (2129)”、“ZDF-Aufnahmen (2131)”、“ZDF-Aufnahmen (2133)”、“ZDF-Aufnahmen (2135)”、“ZDF-Aufnahmen (2137)”、“ZDF-Aufnahmen (2139)”、“ZDF-Aufnahmen (2141)”、“ZDF-Aufnahmen (2143)”、“ZDF-Aufnahmen (2145)”、“ZDF-Aufnahmen (2147)”、“ZDF-Aufnahmen (2149)”、“ZDF-Aufnahmen (2151)”、“ZDF-Aufnahmen (2153)”、“ZDF-Aufnahmen (2155)”、“ZDF-Aufnahmen (2157)”、“ZDF-Aufnahmen (2159)”、“ZDF-Aufnahmen (2161)”、“ZDF-Aufnahmen (2163)”、“ZDF-Aufnahmen (2165)”、“ZDF-Aufnahmen (2167)”、“ZDF-Aufnahmen (2169)”、“ZDF-Aufnahmen (2171)”、“ZDF-Aufnahmen (2173)”、“ZDF-Aufnahmen (2175)”、“ZDF-Aufnahmen (2177)”、“ZDF-Aufnahmen (2179)”、“ZDF-Aufnahmen (2181)”、“ZDF-Aufnahmen (2183)”、“ZDF-Aufnahmen (2185)”、“ZDF-Aufnahmen (2187)”、“ZDF-Aufnahmen (2189)”、“ZDF-Aufnahmen (2191)”、“ZDF-Aufnahmen (2193)”、“ZDF-Aufnahmen (2195)”、“ZDF-Aufnahmen (2197)”、“ZDF-Aufnahmen (2199)”、“ZDF-Aufnahmen (2201)”、“ZDF-Aufnahmen (2203)”、“ZDF-Aufnahmen (2205)”、“ZDF-Aufnahmen (2207)”、“ZDF-Aufnahmen (2209)”、“ZDF-Aufnahmen (2211)”、“ZDF-Aufnahmen (2213)”、“ZDF-Aufnahmen (2215)”、“ZDF-Aufnahmen (2217)”、“ZDF-Aufnahmen (2219)”、“ZDF-Aufnahmen (2221)”、“ZDF-Aufnahmen (2223)”、“ZDF-Aufnahmen (2225)”、“ZDF-Aufnahmen (2227)”、“ZDF-Aufnahmen (2229)”、“ZDF-Aufnahmen (2231)”、“ZDF-Aufnahmen (2233)”、“ZDF-Aufnahmen (2235)”、“ZDF-Aufnahmen (2237)”、“ZDF-Aufnahmen (2239)”、“ZDF-Aufnahmen (2241)”、“ZDF-Aufnahmen (2243)”、“ZDF-Aufnahmen (2245)”、“ZDF-Aufnahmen (2247)”、“ZDF-Aufnahmen (2249)”、“ZDF-Aufnahmen (2251)”、“ZDF-Aufnahmen (2253)”、“ZDF-Aufnahmen (2255)”、“ZDF-Aufnahmen (2257)”、“ZDF-Aufnahmen (2259)”、“ZDF-Aufnahmen (2261)”、“ZDF-Aufnahmen (2263)”、“ZDF-Aufnahmen (2265)”、“ZDF-Aufnahmen (2267)”、“ZDF-Aufnahmen (2269)”、“ZDF-Aufnahmen (2271)”、“ZDF-Aufnahmen (2273)”、“ZDF-Aufnahmen (2275)”、“ZDF-Aufnahmen (2277)”、“ZDF-Aufnahmen (2279)”、“ZDF-Aufnahmen (2281)”、“ZDF-Aufnahmen (2283)”、“ZDF-Aufnahmen (2285)”、“ZDF-Aufnahmen (2287)”、“ZDF-Aufnahmen (2289)”、“ZDF-Aufnahmen (2291)”、“ZDF-Aufnahmen (2293)”、“ZDF-Aufnahmen (2295)”、“ZDF-Aufnahmen (2297)”、“ZDF-Aufnahmen (2299)”、“ZDF-Aufnahmen (2301)”、“ZDF-Aufnahmen (2303)”、“ZDF-Aufnahmen (2305)”、“ZDF-Aufnahmen (2307)”、“ZDF-Aufnahmen (2309)”、“ZDF-Aufnahmen (2311)”、“ZDF-Aufnahmen (2313)”、“ZDF-Aufnahmen (2315)”、“ZDF-Aufnahmen (2317)”、“ZDF-Aufnahmen (2319)”、“ZDF-Aufnahmen (2321)”、“ZDF-Aufnahmen (2323)”、“ZDF-Aufnahmen (2325)”、“ZDF-Aufnahmen (2327)”、“ZDF-Aufnahmen (2329)”、“ZDF-Aufnahmen (2331)”、“ZDF-Aufnahmen (2333)”、“ZDF-Aufnahmen (2335)”、“ZDF-Aufnahmen (2337)”、“ZDF-Aufnahmen (2339)”、“ZDF-Aufnahmen (2341)”、“ZDF-Aufnahmen (2343)”、“ZDF-Aufnahmen (2345)”、“ZDF-Aufnahmen (2347)”、“ZDF-Aufnahmen (2349)”、“ZDF-Aufnahmen (2351)”、“ZDF-Aufnahmen (2353)”、“ZDF-Aufnahmen (2355)”、“ZDF-Aufnahmen (2357)”、“ZDF-Aufnahmen (2359)”、“ZDF-Aufnahmen (2361)”、“ZDF-Aufnahmen (2363)”、“ZDF-Aufnahmen (2365)”、“ZDF-Aufnahmen (2367)”、“ZDF-Aufnahmen (2369)”、“ZDF-Aufnahmen (2371)”、“ZDF-Aufnahmen (2373)”、“ZDF-Aufnahmen (2375)”、“ZDF-Aufnahmen (2377)”、“ZDF-Aufnahmen (2379)”、“ZDF-Aufnahmen (2381)”、“ZDF-Aufnahmen (2383)”、“ZDF-Aufnahmen (2385)”、“ZDF-Aufnahmen (2387)”、“ZDF-Aufnahmen (2389)”、“ZDF-Aufnahmen (2391)”、“ZDF-Aufnahmen (2393)”、“ZDF-Aufnahmen (2395)”、“ZDF-Aufnahmen (2397)”、“ZDF-Aufnahmen (2399)”、“ZDF-Aufnahmen (2401)”、“ZDF-Aufnahmen (2403)”、“ZDF-Aufnahmen (2405)”、“ZDF-Aufnahmen (2407)”、“ZDF-Aufnahmen (2409)”、“ZDF-Aufnahmen (2411)”、“ZDF-Aufnahmen (2413)”、“ZDF-Aufnahmen (2415)”、“ZDF-Aufnahmen (2417)”、“ZDF-Aufnahmen (2419)”、“ZDF-Aufnahmen (2421)”、“ZDF-Aufnahmen (2423)”、“ZDF-Aufnahmen (2425)”、“ZDF-Aufnahmen (2427)”、“ZDF-Aufnahmen (2429)”、“ZDF-Aufnahmen (2431)”、“ZDF-Aufnahmen (2433)”、“ZDF-Aufnahmen (2435)”、“ZDF-Aufnahmen (2437)”、“ZDF-Aufnahmen (2439)”、“ZDF-Aufnahmen (2441)”、“ZDF-Aufnahmen (2443)”、“ZDF-Aufnahmen (2445)”、“ZDF-Aufnahmen (2447)”、“ZDF-Aufnahmen (2449)”、“ZDF-Aufnahmen (2451)”、“ZDF-Aufnahmen (2453)”、“ZDF-Aufnahmen (2455)”、“ZDF-Aufnahmen (2457)”、“ZDF-Aufnahmen (2459)”、“ZDF-Aufnahmen (2461)”、“ZDF-Aufnahmen (2463)”、“ZDF-Aufnahmen (2465)”、“ZDF-Aufnahmen (2467)”、“ZDF-Aufnahmen (2469)”、“ZDF-Aufnahmen (2471)”、“ZDF-Aufnahmen (2473)”、“ZDF-Aufnahmen (2475)”、“ZDF-Aufnahmen (2477)”、“ZDF-Aufnahmen (2479)”、“ZDF-Aufnahmen (2481)”、“ZDF-Aufnahmen (2483)”、“ZDF-Aufnahmen (2485)”、“ZDF-Aufnahmen (2487)”、“ZDF-Aufnahmen (2489)”、“ZDF-Aufnahmen (2491)”、“ZDF-Aufnahmen (2493)”、“ZDF-Aufnahmen (2495)”、“ZDF-Aufnahmen (2497)”、“ZDF-Aufnahmen (2499)”、“ZDF-Aufnahmen (2501)”、“ZDF-Aufnahmen (2503)”、“ZDF-Aufnahmen (2505)”、“ZDF-Aufnahmen (2507)”、“ZDF-Aufnahmen (2509)”、“ZDF-Aufnahmen (2511)”、“ZDF-Aufnahmen (2513)”、“ZDF-Aufnahmen (2515)”、“ZDF-Aufnahmen (2517)”、“ZDF-Aufnahmen (2519)”、“ZDF-Aufnahmen (2521)”、“ZDF-Aufnahmen (2523)”、“ZDF-Aufnahmen (2525)”、“ZDF-Aufnahmen (2527)”、“ZDF-Aufnahmen (2529)”、“ZDF-Aufnahmen (2531)”、“ZDF-Aufnahmen (2533)”、“ZDF-Aufnahmen (2535)”、“ZDF-Aufnahmen (2537)”、“ZDF-Aufnahmen (2539)”、“ZDF-Aufnahmen (2541)”、“ZDF-Aufnahmen (2543)”、“ZDF-Aufnahmen (2545)”、“ZDF-Aufnahmen (2547)”、“ZDF-Aufnahmen (2549)”、“ZDF-Aufnahmen (2551)”、“ZDF-Aufnahmen (2553)”、“ZDF-Aufnahmen (2555)”、“ZDF-Aufnahmen (2557)”、“ZDF-Aufnahmen (2559)”、“ZDF-Aufnahmen (2561)”、“ZDF-Aufnahmen (2563)”、“ZDF-Aufnahmen (2565)”、“ZDF-Aufnahmen (2567)”、“ZDF-Aufnahmen (2569)”、“ZDF-Aufnahmen (2571)”、“ZDF-Aufnahmen (2573)”、“ZDF-Aufnahmen (2575)”、“ZDF-Aufnahmen (2577)”、“ZDF-Aufnahmen (2579)”、“ZDF-Aufnahmen (2581)”、“ZDF-Aufnahmen (2583)”、“ZDF-Aufnahmen (2585)”、“ZDF-Aufnahmen (2587)”、“ZDF-Aufnahmen (2589)”、“ZDF-Aufnahmen (2591)”、“ZDF-Aufnahmen (2593)”、“ZDF-Aufnahmen (2595)”、“ZDF-Aufnahmen (2597)”、“ZDF-Aufnahmen (2599)”、“ZDF-Aufnahmen (2601)”、“ZDF-Aufnahmen (2603)”、“ZDF-Aufnahmen (2605)”、“ZDF-Aufnahmen (2607)”、“ZDF-Aufnahmen (2609)”、“ZDF-Aufnahmen (2611)”、“ZDF-Aufnahmen (2613)”、“ZDF-Aufnahmen (2615)”、“ZDF-Aufnahmen (2617)”、“ZDF-Aufnahmen (2619)”、“ZDF-Aufnahmen (2621)”、“ZDF-Aufnahmen (</sample>
    <sample id="10">根据提供的图片，以下是用中文描述的英文内容：

标题：德语文本简化语料库

子标题：句子级别

图表展示了德语文本简化语料库中不同来源的句子数量。数据分为两个主要类别：正式文本（如书籍、报纸和学术文章）和非正式文本（如社交媒体、论坛和博客）。在正式文本类别中，进一步细分为古典文学（如圣经和古希腊罗马文学）、现代文学（如小说和戏剧）、历史文本（如历史书籍和报纸文章）以及科学文本（如学术论文和教科书）。在非正式文本类别中，细分为社交媒体、论坛和博客。

柱状图显示了每个子类别的句子数量。例如，在古典文学类别中，有12000个句子，其中大部分是圣经（10000个句子），其余是古希腊罗马文学（2000个句子）。在现代文学类别中，有8000个句子，其中大部分是小说（6000个句子），其余是戏剧（2000个句子）。在历史文本类别中，有5000个句子，其中大部分是历史书籍（4000个句子），其余是报纸文章（1000个句子）。在科学文本类别中，有3000个句子，其中大部分是学术论文（2000个句子），其余是教科书（1000个句子）。

此外，柱状图还显示了非正式文本类别中的句子数量。例如，在社交媒体类别中，有756个句子；在论坛类别中，有340个句子；在博客类别中，有140个句子。

总的来说，这个图表提供了德语文本简化语料库中不同来源的句子数量的详细视图，展示了正式文本和非正式文本之间的差异，以及每个子类别的句子数量。</sample>
    <sample id="11">收到英文内容后，用中文表述其意思。</sample>
    <sample id="12">收到。这是关于德语文本简化语料库的幻灯片。幻灯片分为两部分：顶部部分显示一个男人戴着耳机，底部部分显示一个表格。表格显示了不同来源的句子数量。表格的标题为“句子级别”，表格的左列显示了不同的来源，右列显示了每个来源的句子数量。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”、“ZDF-Archiv”和“ZDF-Archiv”。表格的句子数量从左到右递减，从12000到483。表格的来源包括“Paderborn”、“Transkript”、“ZDF”、“ZDF-Archiv”、“ZDF-Archiv”</sample>
    <sample id="13">收到英文内容后，用中文表述其意思。</sample>
    <sample id="14">新闻
圣经
小说
图a展示了不同文本类型中不同简化方法的使用情况。数据表明，对于每种文本类型，都有三种简化方法：Simplicity、LexSimp和StructSimp。这些方法在不同文本类型中的使用频率不同，如图所示。例如，在“圣经”文本类型中，Simplicity和LexSimp的使用率较高，而StructSimp的使用率较低。</sample>
    <sample id="15">新闻、圣经和小说等不同文本类型的简化程度。</sample>
    <sample id="16">这张图片展示了文本简化过程中使用的两种主要方法：词干提取和词典查找。词干提取涉及从单词中删除后缀以获得其基本形式，而词典查找则涉及使用词典查找与给定单词相似的单词。这些方法通常用于文本简化任务，如机器翻译、信息检索和自然语言处理。</sample>
    <sample id="17">这个图片展示了两种简化方法的比较，即LexSimp和StructSimp。LexSimp专注于通过删除、替换和添加单词来简化文本，而StructSimp则侧重于通过重新排序、改写和插入单词来简化文本结构。LexSimp在处理新闻、圣经和小说等不同类型的文本时表现出较高的准确率，而StructSimp则在这些文本类型中展现出较低的准确率。</sample>
    <sample id="18">图片显示了两个图表，分别代表“简化类型”和“简化变换”。第一个图表是一个柱状图，展示了不同文本类型的简化类型频率。它包括三个类别：新闻、圣经和小说，每个类别都有三个子类别：简洁性、LexSimp和StructSimp。第二个图表是一个条形图，展示了不同简化变换的频率，如删除、替换、局部替换、单词添加和单词删除。</sample>
    <sample id="19">3. Use-cases
Automatic alignment and simplification</sample>
    <sample id="20">结果表明，不同的对齐方法在不同任务上的性能存在差异。例如，在任务“111”上，BERTalign方法表现最好，得分743分，而RoBERTa和CATS-C3G方法得分较低。在任务“n2m”上，BERTalign方法仍然表现良好，得分610分，而其他方法得分较低。这些结果表明，BERTalign方法可能更适合处理这些特定任务。</sample>
    <sample id="21">结果表明，不同的对齐方法在不同任务上的性能存在差异。例如，在111任务上，BERTalign和Vowalign表现出较高的性能，而在其他任务上，其他方法可能表现更好。这些结果有助于理解每种方法的优缺点，并为选择最适合特定任务的对齐方法提供指导。</sample>
    <sample id="22">结果表明，不同的对齐方法在1:1和n:n能力上的表现存在差异。例如，在1:1能力上，BERTalign方法表现最好，得分743分，而Vogel方法得分最低，得分582分。在n:n能力上，BERTalign方法再次表现最佳，得分610分，而Vogel方法得分最低，得分459分。这些结果表明，BERTalign方法在处理多语言和多文档对齐任务时具有更高的准确性和效率。</sample>
    <sample id="23">结果表明，BERTalign和Vowalign在评估中表现最佳。BERTalign使用TF-IDF相似性矩阵进行密切驱动的方法，而Vowalign使用TF-IDF相似性矩阵进行密切驱动的方法。这些结果表明，这些方法在评估中具有较高的准确性和可靠性。</sample>
    <sample id="24">结果表明，与BERT和RoBERTa等预训练模型相比，基于Transformer的句子对齐方法在性能上具有显著优势。这些结果强调了使用预训练模型进行句子对齐的重要性，并突出了它们在处理自然语言任务时的潜力。</sample>
    <sample id="25">结果表明，与BERT和RoBERTa等预训练模型相关的句子嵌入方法在评估中表现最好。这些方法能够捕捉到文本的语义信息，并且在处理自然语言任务时具有很好的泛化能力。相比之下，使用词向量的方法在评估中表现较差，这可能是因为它们无法捕捉到文本的语义信息。</sample>
    <sample id="26">结果表明，BERTalign和Votalign在评估中表现最佳。BERTalign使用TF-IDF相似性矩阵进行近似驱动方法，而Votalign则使用TF-IDF相似性矩阵进行投票驱动方法。这些结果表明，这些方法可以有效地评估句子对齐任务。</sample>
    <sample id="27">在文档级别，我们使用mBART的微调版本评估了文本简化任务。我们使用了SARI、BLEU和BLEU-Score作为评估指标，并将它们与mBART的基线进行了比较。此外，我们还使用了DEPLAN-APA测试集和DEPLAN-WEB测试集来评估mBART的性能。</sample>
    <sample id="28">图3显示了在不同数据集上进行的文本简化任务的性能。该图表展示了使用mBART模型进行文档级别和句子级别的文本简化任务的性能。结果表明，mBART模型在文档级别和句子级别上都表现良好，特别是在SARI和BLEU等评估指标上。这表明mBART模型具有强大的文本简化能力，并且可以有效地处理不同长度的训练数据。</sample>
    <sample id="29">图1展示了使用mBART微调后在不同数据集上进行文档级别和句子级别的文本简化评估结果。评估结果表明，mBART微调后的模型在SARI、BLEU和BS-P等指标上表现出显著提高，表明其在文本简化任务上的性能得到了显著提升。</sample>
    <sample id="30">结果是在使用微调后的Long-BART进行文档级别简化。它对应于训练数据的长度。|训练数据|SARI|BLEU|BP-Score|BLEU-4|BLEU-3|BLEU-2|BLEU-1|BLEU-0|BLEU-1|BLEU-2|BLEU-3|BLEU-4|BLEU-5|BLEU-6|BLEU-7|BLEU-8|BLEU-9|BLEU-10|BLEU-11|BLEU-12|BLEU-13|BLEU-14|BLEU-15|BLEU-16|BLEU-17|BLEU-18|BLEU-19|BLEU-20|BLEU-21|BLEU-22|BLEU-23|BLEU-24|BLEU-25|BLEU-26|BLEU-27|BLEU-28|BLEU-29|BLEU-30|BLEU-31|BLEU-32|BLEU-33|BLEU-34|BLEU-35|BLEU-36|BLEU-37|BLEU-38|BLEU-39|BLEU-40|BLEU-41|BLEU-42|BLEU-43|BLEU-44|BLEU-45|BLEU-46|BLEU-47|BLEU-48|BLEU-49|BLEU-50|BLEU-51|BLEU-52|BLEU-53|BLEU-54|BLEU-55|BLEU-56|BLEU-57|BLEU-58|BLEU-59|BLEU-60|BLEU-61|BLEU-62|BLEU-63|BLEU-64|BLEU-65|BLEU-66|BLEU-67|BLEU-68|BLEU-69|BLEU-70|BLEU-71|BLEU-72|BLEU-73|BLEU-74|BLEU-75|BLEU-76|BLEU-77|BLEU-78|BLEU-79|BLEU-80|BLEU-81|BLEU-82|BLEU-83|BLEU-84|BLEU-85|BLEU-86|BLEU-87|BLEU-88|BLEU-89|BLEU-90|BLEU-91|BLEU-92|BLEU-93|BLEU-94|BLEU-95|BLEU-96|BLEU-97|BLEU-98|BLEU-99|BLEU-100|BLEU-101|BLEU-102|BLEU-103|BLEU-104|BLEU-105|BLEU-106|BLEU-107|BLEU-108|BLEU-109|BLEU-110|BLEU-111|BLEU-112|BLEU-113|BLEU-114|BLEU-115|BLEU-116|BLEU-117|BLEU-118|BLEU-119|BLEU-120|BLEU-121|BLEU-122|BLEU-123|BLEU-124|BLEU-125|BLEU-126|BLEU-127|BLEU-128|BLEU-129|BLEU-130|BLEU-131|BLEU-132|BLEU-133|BLEU-134|BLEU-135|BLEU-136|BLEU-137|BLEU-138|BLEU-139|BLEU-140|BLEU-141|BLEU-142|BLEU-143|BLEU-144|BLEU-145|BLEU-146|BLEU-147|BLEU-148|BLEU-149|BLEU-150|BLEU-151|BLEU-152|BLEU-153|BLEU-154|BLEU-155|BLEU-156|BLEU-157|BLEU-158|BLEU-159|BLEU-160|BLEU-161|BLEU-162|BLEU-163|BLEU-164|BLEU-165|BLEU-166|BLEU-167|BLEU-168|BLEU-169|BLEU-170|BLEU-171|BLEU-172|BLEU-173|BLEU-174|BLEU-175|BLEU-176|BLEU-177|BLEU-178|BLEU-179|BLEU-180|BLEU-181|BLEU-182|BLEU-183|BLEU-184|BLEU-185|BLEU-186|BLEU-187|BLEU-188|BLEU-189|BLEU-190|BLEU-191|BLEU-192|BLEU-193|BLEU-194|BLEU-195|BLEU-196|BLEU-197|BLEU-198|BLEU-199|BLEU-200|BLEU-201|BLEU-202|BLEU-203|BLEU-204|BLEU-205|BLEU-206|BLEU-207|BLEU-208|BLEU-209|BLEU-210|BLEU-211|BLEU-212|BLEU-213|BLEU-214|BLEU-215|BLEU-216|BLEU-217|BLEU-218|BLEU-219|BLEU-220|BLEU-221|BLEU-222|BLEU-223|BLEU-224|BLEU-225|BLEU-226|BLEU-227|BLEU-228|BLEU-229|BLEU-230|BLEU-231|BLEU-232|BLEU-233|BLEU-234|BLEU-235|BLEU-236|BLEU-237|BLEU-238|BLEU-239|BLEU-240|BLEU-241|BLEU-242|BLEU-243|BLEU-244|BLEU-245|BLEU-246|BLEU-247|BLEU-248|BLEU-249|BLEU-250|BLEU-251|BLEU-252|BLEU-253|BLEU-254|BLEU-255|BLEU-256|BLEU-257|BLEU-258|BLEU-259|BLEU-260|BLEU-261|BLEU-262|BLEU-263|BLEU-264|BLEU-265|BLEU-266|BLEU-267|BLEU-268|BLEU-269|BLEU-270|BLEU-271|BLEU-272|BLEU-273|BLEU-274|BLEU-275|BLEU-276|BLEU-277|BLEU-278|BLEU-279|BLEU-280|BLEU-281|BLEU-282|BLEU-283|BLEU-284|BLEU-285|BLEU-286|BLEU-287|BLEU-288|BLEU-289|BLEU-290|BLEU-291|BLEU-292|BLEU-293|BLEU-294|BLEU-295|BLEU-296|BLEU-297|BLEU-298|BLEU-299|BLEU-300|BLEU-301|BLEU-302|BLEU-303|BLEU-304|BLEU-305|BLEU-306|BLEU-307|BLEU-308|BLEU-309|BLEU-310|BLEU-311|BLEU-312|BLEU-313|BLEU-314|BLEU-315|BLEU-316|BLEU-317|BLEU-318|BLEU-319|BLEU-320|BLEU-321|BLEU-322|BLEU-323|BLEU-324|BLEU-325|BLEU-326|BLEU-327|BLEU-328|BLEU-329|BLEU-330|BLEU-331|BLEU-332|BLEU-333|BLEU-334|BLEU-335|BLEU-336|BLEU-337|BLEU-338|BLEU-339|BLEU-340|BLEU-341|BLEU-342|BLEU-343|BLEU-344|BLEU-345|BLEU-346|BLEU-347|BLEU-348|BLEU-349|BLEU-350|BLEU-351|BLEU-352|BLEU-353|BLEU-354|BLEU-355|BLEU-356|BLEU-357|BLEU-358|BLEU-359|BLEU-360|BLEU-361|BLEU-362|BLEU-363|BLEU-364|BLEU-365|BLEU-366|BLEU-367|BLEU-368|BLEU-369|BLEU-370|BLEU-371|BLEU-372|BLEU-373|BLEU-374|BLEU-375|BLEU-376|BLEU-377|BLEU-378|BLEU-379|BLEU-380|BLEU-381|BLEU-382|BLEU-383|BLEU-384|BLEU-385|BLEU-386|BLEU-387|BLEU-388|BLEU-389|BLEU-390|BLEU-391|BLEU-392|BLEU-393|BLEU-394|BLEU-395|BLEU-396|BLEU-397|BLEU-398|BLEU-399|BLEU-400|BLEU-401|BLEU-402|BLEU-403|BLEU-404|BLEU-405|BLEU-406|BLEU-407|BLEU-408|BLEU-409|BLEU-410|BLEU-411|BLEU-412|BLEU-413|BLEU-414|BLEU-415|BLEU-416|BLEU-417|BLEU-418|BLEU-419|BLEU-420|BLEU-421|BLEU-422|BLEU-423|BLEU-424|BLEU-425|BLEU-426|BLEU-427|BLEU-428|BLEU-429|BLEU-430|BLEU-431|BLEU-432|BLEU-433|BLEU-434|BLEU-435|BLEU-436|BLEU-437|BLEU-438|BLEU-439|BLEU-440|BLEU-441|BLEU-442|BLEU-443|BLEU-444|BLEU-445|BLEU-446|BLEU-447|BLEU-448|BLEU-449|BLEU-450|BLEU-451|BLEU-452|BLEU-453|BLEU-454|BLEU-455|BLEU-456|BLEU-457|BLEU-458|BLEU-459|BLEU-460|BLEU-461|BLEU-462|BLEU-463|BLEU-464|BLEU-465|BLEU-466|BLEU-467|BLEU-468|BLEU-469|BLEU-470|BLEU-471|BLEU-472|BLEU-473|BLEU-474|BLEU-475|BLEU-476|BLEU-477|BLEU-478|BLEU-479|BLEU-480|BLEU-481|BLEU-482|BLEU-483|BLEU-484|BLEU-485|BLEU-486|BLEU-487|BLEU-488|BLEU-489|BLEU-490|BLEU-491|BLEU-492|BLEU-493|BLEU-494|BLEU-495|BLEU-496|BLEU-497|BLEU-498|BLEU-499|BLEU-500|BLEU-501|BLEU-502|BLEU-503|BLEU-504|BLEU-505|BLEU-506|BLEU-507|BLEU-508|BLEU-509|BLEU-510|BLEU-511|BLEU-512|BLEU-513|BLEU-514|BLEU-515|BLEU-516|BLEU-517|BLEU-518|BLEU-519|BLEU-520|BLEU-521|BLEU-522|BLEU-523|BLEU-524|BLEU-525|BLEU-526|BLEU-527|BLEU-528|BLEU-529|BLEU-530|BLEU-531|BLEU-532|BLEU-533|BLEU-534|BLEU-535|BLEU-536|BLEU-537|BLEU-538|BLEU-539|BLEU-540|BLEU-541|BLEU-542|BLEU-543|BLEU-544|BLE</sample>
    <sample id="31">图4显示了在不同数据集上进行的文档和句子级别的文本简化任务的性能。结果表明，mBART在文档级别上的BLEU得分最高，为34.247，而SARI得分为17.837。相比之下，在句子级别上，mBART的BLEU得分为19.404，SARI得分为13.785。这些结果表明，mBART在文档级别上的性能优于句子级别。</sample>
    <sample id="32">图4显示了在不同数据集上进行的文本简化实验结果。该图展示了使用微调后的Long-mBART模型进行文档级和句子级文本简化任务的结果。表格中列出了各种评估指标，如SARI、BLEU、BS和FRE，以及使用的训练数据量。结果表明，微调后的Long-mBART模型在文档级和句子级文本简化任务上取得了显著改进，与未微调的模型相比，性能提升明显。</sample>
    <sample id="33">图片显示了一个在线演示或会议，一个男人坐在一个房间里，背景上有窗户和灯光。在屏幕上可以看到一个幻灯片，标题为“自动文本简化”，并分为两个主要部分：文档级别和句子级别。每个部分都包含表格，展示了使用不同模型（如SARI、BLEU、BS-P和FRE）的训练数据和测试数据的比较结果。表格包括各种模型名称和它们在APL测试集上的性能指标。</sample>
    <sample id="34">谢谢。如需更多信息，请查阅我们的论文。并欢迎在ACL 2023年会上参观我们的海报。</sample>
    <sample id="35">演讲者的名字是Patrick Fernandes、Kayo Yin、Emmy Liu、Andre F. T. Martins和Graham Neubig。这些名字在幻灯片上列出，与每个名字后面标注的头衔一起出现。</sample>
    <sample id="36">根据幻灯片中提供的信息，他们使用了T5 XL模型获得82%-87%的准确率。</sample>
    <sample id="37">根据幻灯片中提供的信息，CoNLL-2003 标注器的有效性似乎在质疑中。幻灯片的右侧有一个图表，显示了从2004年到2020年的性能趋势。图表上标记着“CoNLL 2003”标签，其性能线明显低于其他标注器如“BERT”和“LUKE”。这表明CoNLL-2003标注器在处理自然语言处理任务时可能不如 newer, more advanced models effective。因此，根据幻灯片中呈现的视觉证据，答案是否定的，CoNLL-2003标注器不再有效。</sample>
    <sample id="38">根据图片中展示的英语内容，提出的人工评估方法新颖之处在于它使用了 Likert 量表来评估聊天机器人（ABC-Eval）对用户输入的响应的相关性。这种方法新颖之处在于它提供了一种系统化的方法来评估对话的质量和相关性，这对于开发能够理解并以相关和有用的方式回应用户查询的聊天机器人至关重要。</sample>
    <sample id="39">根据图表，现有弱监督方法的成功在很大程度上依赖于标签质量。这可以从图表中显示的标签质量与性能改善之间的相关性中看出。标签质量高的数据点（如“随机选择”和“干净标签”）显示出显著的性能改善，而标签质量低的数据点（如“弱标签”）则显示出较差的性能改善。</sample>
    <sample id="40">根据幻灯片中提供的背景知识，可以通过以下措施来提高分数：1.仔细倾听每首歌曲以充分理解其内容。2.阅读关于每首歌曲的资料，这可能包括艺术家的采访、歌曲评论或歌曲背后的故事。3.使用Google搜索功能查找与歌曲相关的额外信息，这可能会提供更深入的了解并帮助准确评估歌曲的情感。4.注意幻灯片中提到的歌曲示例，因为它们可能代表了不同的情感表达，从而为评分提供参考。5.确保在评分时保持客观和公正，避免个人偏好或偏见。</sample>
    <sample id="41">根据图片显示，这篇论文有五位作者。这可以通过在标题下方列出的作者名字和头像来确定。</sample>
    <sample id="42">conjunct lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination Adam Przpielkowski and Michat Wozniak Institute of Computer Science Polish Academy of Sciences ul. Jana Kazimierzawa 5, 01-248 Warsaw University of Warsaw ACL 2023</sample>
    <sample id="43">依赖结构协调</sample>
    <sample id="44">依赖结构协调</sample>
    <sample id="45">依赖结构协调</sample>
    <sample id="46">依赖结构协调</sample>
    <sample id="47">依赖结构协调</sample>
    <sample id="48">依赖结构协调</sample>
    <sample id="49">依赖结构协调</sample>
    <sample id="50">依赖结构协调</sample>
    <sample id="51">Word order tends to minimize dependency lengths:</sample>
    <sample id="52">词序倾向于最小化依赖长度：</sample>
    <sample id="53">Word order tends to minimize dependency lengths:</sample>
    <sample id="54">词序倾向于最小化依赖长度：</sample>
    <sample id="55">Word order tends to minimize dependency lengths:</sample>
    <sample id="56">Word order tends to minimize dependency lengths:</sample>
    <sample id="57">Word order tends to minimize dependency lengths:</sample>
    <sample id="58">Word order tends to minimize dependency lengths:</sample>
    <sample id="59">Word order tends to minimize dependency lengths:</sample>
    <sample id="60">Word order tends to minimize dependency lengths:</sample>
    <sample id="61">Word order tends to minimize dependency lengths:</sample>
    <sample id="62">Word order tends to minimize dependency lengths:</sample>
    <sample id="63">幻灯片讨论了英语中连词长度的统计信息。它引用了Marcus et al. 1993和Ficler and Goldberg 2016的研究结果，指出连词往往比之前的情况更短。这种趋势随长度差异而加剧。幻灯片还提到了Gibson et al. 1996-90的研究，并给出了一个例子，说明只有当从句中的主语在左侧或不存在时，才会出现这种趋势（例如“我看见Bart和Lisa来，然后打喷嚏”），而不是当主语在右侧时（例如“Ted和Ned笑”）。</sample>
    <sample id="64">关于 coordin 的统计信息是从 Penn Treebank 的增强版本中提取出来的。Marcus et al. 1993、Ficler 和 Goldberg 2016 年的研究表明，连词连接的句子通常比之前更短。这种趋势随着句子长度差异的增加而加剧。Gibson et al. 1996: 88-90 简要提及了这一点。例如，在句子 'I saw Bart and Lisa. Homer came and sneezed.' 中，连词 'and' 出现在Governor左侧或不存在时，句子较短；但在句子 'Ted and Ned laughed.' 中，连词 'and' 出现在Governor右侧时，句子较长。</sample>
    <sample id="65">关于协调的统计信息，从Penn Treebank的一个增强版本中提取（Marcus et al. 1993; Ficler and Goldberg 2016）：

- 左连词倾向于比之前更短（观察到的）。
- 这种趋势随着长度差异的增长而加剧（Gibson et al. 1996: 88–90）。

但只有当主语在左边或不存在时才如此（例如，“我看见巴特和Lisa来了一笑就逃走了”），而不是当主语在右边时（例如，“Ted和Ned笑了一笑”）。</sample>
    <sample id="66">关于协调的统计信息提取自增强版的宾夕法尼亚树库(Marcus et al. 1993; Ficler and Goldberg 2016)。 • 左连接通常比之前观察到的更短(α)， • 这种趋势随着长度差异的增加而加剧(在Gibson et al. 1996: 88-90中简要提及)。 • 但是，只有当主语在左边或不存在时才会发生这种情况(我看到巴特和Lisa:霍默咳嗽了)， • 而不是当它在右边时(Ted和Ned嘲笑)</sample>
    <sample id="67">关于 coordinators 的统计信息提取自 Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016) 的增强版本：

- 左连词通常比之前观察到的要短（Marcus et al. 1993）；
- 这种趋势随着长度差异的增长而加剧（Gibson et al. 1996: 88–90）。

但只有当主语在左侧或不存在时才如此（Bart和Lisa；Homer来和打喷嚏），而不是当它在右侧时（Ted和Ned笑）。</sample>
    <sample id="68">关于 coordin 的统计信息是从 Penn Treebank (Marcus et al. 1993; Flicker and Goldberg 2016)的一个增强版本中提取的。 • 左连词通常比之前观察到的要短。 • 这种趋势随着长度差异的增长而加剧。 • 只有当主语在左边或不存在时才会出现这种情况（例如，我看见巴特和Lisa；霍默和琳赛一起笑）。 • 不会在右边出现这种情况（例如，Ted和Ned笑）。</sample>
    <sample id="69">关于 coordinators 的统计信息，提取自 Penn Treebank（Marcus et al. 1993; Ficler and Goldberg 2016）的一个增强版本： • 左 coordinators 趋于较短（之前观察到的）； • 这种趋势随着长度差异的增长而增加（Gibson et al. 1996: 88-90）； • 但在 governor 在左侧或不存在时才如此（例如，我看见巴特和Lisa；霍默来了并打喷嚏）； • 不是在右侧（例如，Ted 和 Ned 笑了）。</sample>
    <sample id="70">图3：根据字符长度、音节长度和单词长度的差异，生成器左侧和右侧的较短句子的比例。</sample>
    <sample id="71">图3：根据字符长度、音节长度和单词长度的绝对差值，显示了短语长度分布的差异。上排图表展示了从左到右的比较，下排图表展示了从右到左的比较。每个图表都显示了不同长度范围内的短语长度分布，以及这些分布之间的差异。图表中的点表示了短语长度分布的平均值，而阴影区域表示了置信区间。</sample>
    <sample id="72">图3：根据短语长度（字符数、词数）的绝对差值，显示了短语长度分布的差异。</sample>
    <sample id="73">图1：根据短语长度差异（字符、音节和单词）对短语长度差异进行分类的分类器性能。</sample>
    <sample id="74">见论文以获取完整论点！
海报讨论会在海报讨论会期间与我们交流！</sample>
    <sample id="75">根据幻灯片显示，这篇论文有三位作者：Matthias Lindemann、Alexander Koller和Ivan Titov。</sample>
    <sample id="76">根据图表，新闻和圣经文本的简化程度更大。这可以通过每个类别中蓝色柱形图的高度来表示，蓝色代表“Simplicity”简化类型。在“news”和“bible”类别中，蓝色柱形图明显高于其他颜色，表明这些领域的简化程度更大。</sample>
    <sample id="77">偏好的示例是“读了昨天的书”，因为它具有较短的左并列词，其中“读了”和“昨天的”之间的距离较近。</sample>
    <sample id="78">根据幻灯片中提供的信息，DrBERT模型、NACHOS数据集和训练脚本可以在MIT许可证下免费使用。这意味着您可以将这些资源用于您的研究，但必须遵守MIT许可证的条款。这通常包括提供包含这些材料的任何衍生作品的来源，并且如果您修改或分发这些材料，则必须以相同的许可证进行分发。</sample>
    <sample id="79">根据图片中提供的信息，DEplain-apa 包含来自 APA（美国心理学会）出版物的内容。这可以从柱状图的标签中推断出来，该标签明确指出了“APA”作为内容来源。</sample>
    <sample id="80">有助于良好泛化的因素包括：更好的模型架构、更大的模型尺寸以及更多的微调示例。</sample>
    <sample id="81">根据幻灯片中提到的统计信息，衡量左并列词是否更短的方法是通过比较它们的长度。如果一个并列词比另一个更短，那么它被认为是“更短”的。</sample>
    <sample id="82">要设计实验来研究支配词位置的影响，首先需要定义“支配词”和“位置”（如左或右）的含义。例如，可以将支配词定义为对句子整体情感或主题产生重大影响的词。位置可以指这个词在句子中的位置，无论是在前面还是后面。接下来，收集包含各种长度和复杂度的文本数据集。确保数据集中包含足够数量的样本，以代表不同长度和复杂度的文本。然后，使用自然语言处理技术，如分词和词性标注，确定每个句子中支配词的位置。接下来，分析支配词位置与句子长度、复杂度以及文本类型之间的关系。这可以通过计算不同位置的支配词出现频率、评估句子长度和复杂度对支配词位置的影响，以及识别不同文本类型中支配词位置的模式来完成。最后，根据结果得出结论，讨论实验结果对文本分析和理解的含义。</sample>
    <sample id="83">根据图片中显示的ROC曲线，基线分类器在不平衡数据上的训练效果不佳。ROC曲线靠近对角线表明分类器的性能仅略好于随机猜测，这在不平衡数据集上经常发生，因为分类器倾向于预测多数类标签。</sample>
    <sample id="84">根据幻灯片显示，这篇论文有四位作者。</sample>
    <sample id="85">示例对话中的角色名字是“Gotta Feeling”，这可以从对话中提到的“Do you mean 'I Gotta Feeling'?”这个问题推断出来。</sample>
    <sample id="86">根据图片中提供的信息，语境感知 MT 模型在处理诸如删除、插入、替换和重排等话语现象时比语境无关模型更有优势。</sample>
    <sample id="87">根据幻灯片上显示的英文内容，这篇论文的作者所属机构是约翰霍普金斯大学、普渡大学和麻省理工学院。这些机构的标志在幻灯片右下角可见。</sample>
    <sample id="122">引入的框架通过计算每个类别中不同立场的观察频率来量化立场。这可以通过查看图表底部的表格来实现，该表格显示了年龄、性别和教育等不同类别的数据。例如，在“年龄”类别中，有多个条目，每个条目代表一个立场（如“支持”、“反对”或“中立”），并附带一个数字，表示持有该立场的人数。这种频率可以用来衡量不同群体对特定问题的立场强度和分布情况。</sample>
    <sample id="155">根据幻灯片上提供的信息，之前的研究结果表明，当人类受试者被给予相同的人格化提示时，可能会产生某些特定的描述或反应。然而，由于幻灯片没有提供详细的研究结果，我们无法确定这些结果具体是什么。</sample>
    <sample id="156">根据幻灯片上显示的图片，研究使用了“依存树库”作为数据来源。这可以从幻灯片左下角的注释中看出，该注释写着“来源：依存树库”。</sample>
    <sample id="157">根据幻灯片显示，这篇论文有两位作者：Adam Przpherdkowski和Michat Wozniak。</sample>
    <sample id="158">根据图片中提供的信息，与认知失调密切相关的任务包括“init debate”、“CE”和“Debate CE”。这些任务在图表上以蓝色阴影表示，并且它们的AUC值（曲线下面积）显示在图表上。这些任务似乎与讨论或辩论有关，这可能涉及处理认知失调的情况，即个人同时持有相互冲突的信念或价值观。</sample>
    <sample id="159">根据幻灯片上显示的英文内容，这篇论文有两位作者。</sample>
    <sample id="160">根据幻灯片显示，这篇论文有七位作者。</sample>
    <sample id="161">根据图片中提供的信息，引入的框架与以前的研究不同之处在于它将模型预测与数据库中的真实数据进行比较。这表明了一种迭代方法，其中模型的输出被用来验证或改进对数据集的描述，从而实现更准确和全面的分析。</sample>
    <sample id="162">在三个比较设置中，GPT-4与刻板词汇的重叠最多。这可以通过观察柱状图上紫色部分的长度来确定，代表GPT-4。GPT-4的柱明显比其他模型长，表示其生成的人物中包含更多刻板词汇。</sample>
    <sample id="163">根据幻灯片上提供的英文内容，比较了“Formality”和“lexical cohesion”这两个商业系统。这些术语似乎是MuDA基准测试结果中评估的特定语言或文本分析现象。</sample>
    <sample id="164">从预训练数据到语言模型：追踪导致不公正NLP模型的政治偏见轨迹

该幻灯片展示了ACL2023会议上的一个研究主题，探讨了预训练数据如何影响语言模型的性能和潜在偏见。幻灯片标题为“从预训练数据到语言模型：追踪导致不公正NLP模型的政治偏见轨迹”，表明该研究关注的是预训练数据中可能存在的政治偏见及其对NLP模型的影响。

幻灯片上展示了四名作者的照片和姓名，他们来自不同的学术机构：

1. 王方宾 (Feng Shangbin)
2. 朴善容 (Chan Young Park)
3. 刘雨 (Yuhuan Liu)
4. 亚力山大·特赛科夫 (Yulia Tsvetkov)

这些作者的研究可能涉及分析预训练数据中的政治偏见、识别这些偏见对NLP模型性能的影响，以及开发更公正的NLP模型的方法。该研究的重要性在于确保NLP技术在社会应用中不会加剧现有的政治偏见，从而促进更公平和包容的技术发展。</sample>
    <sample id="165">LM Training Data A mixed blessing  patreon.com wwwikipedia.org www.gloabl.com www.theglob.com www.huffpost.com www.cnn.com www.npr.com www.scribd.com www.bloomberg.com www.vice.com www.theverge.com www.vice.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.theverge.com www.thever</sample>
    <sample id="166">LM Training Data
A mixed blessing

图例：x轴表示训练数据的来源网站，y轴表示每个网站提供的训练数据量。从图中可以看出，训练数据主要来自维基百科、维基文库、纽约客、福布斯等网站，其中维基百科提供的数据量最大，达到10^9，而其他网站提供的数据量则在10^7到10^8之间。此外，图中还提到了一些其他的数据来源，如《大都会》杂志、《观察家》杂志、《商业评论》杂志等。</sample>
    <sample id="167">LM Training Data A mixed blessing 1e+06 1e+05 1e+04 1e+03 1e+02 1e+01 1e+0 100 1000 10000 100000 1000000 # tokens (log scale) www. wikipedia.org www. en. wikipedia.org www. baidu.com www. google.com www. yahoo.com www. amazon.com www. cnn.com www. nytimes.com www. twitter.com www. facebook.com www. linkedin.com www. saturdayeveningpost.com www. businessinsider.com www. forbes.com www. bloomberg.com www. bbc.com www. cnbc.com www. springer.com www. nature.com www. springerlink.com www. wolframalpha.com www. wolfram.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeveningpost.com www. saturdayeven</sample>
    <sample id="168">LM Training Data
一个双刃剑

网站名称
排名
1. www.google.com
2. www.bing.com
3. www.youtube.com
4. www.facebook.com
5. www.github.com
6. www.stackoverflow.com
7. www.quora.com
8. www.wikipedia.org
9. www.m.wikipedia.org
10. www.scribd.com
11. www.huffpost.com
12. www.npr.org
13. www.theverge.com
14. www.cnet.com
15. www.zdnet.com
16. www.technologyreview.com
17. www.theverge.com
18. www.theverge.com
19. www.theverge.com
20. www.theverge.com
21. www.theverge.com
22. www.theverge.com
23. www.theverge.com
24. www.theverge.com
25. www.theverge.com
26. www.theverge.com
27. www.theverge.com
28. www.theverge.com
29. www.theverge.com
30. www.theverge.com
31. www.theverge.com
32. www.theverge.com
33. www.theverge.com
34. www.theverge.com
35. www.theverge.com
36. www.theverge.com
37. www.theverge.com
38. www.theverge.com
39. www.theverge.com
40. www.theverge.com
41. www.theverge.com
42. www.theverge.com
43. www.theverge.com
44. www.theverge.com
45. www.theverge.com
46. www.theverge.com
47. www.theverge.com
48. www.theverge.com
49. www.theverge.com
50. www.theverge.com
51. www.theverge.com
52. www.theverge.com
53. www.theverge.com
54. www.theverge.com
55. www.theverge.com
56. www.theverge.com
57. www.theverge.com
58. www.theverge.com
59. www.theverge.com
60. www.theverge.com
61. www.theverge.com
62. www.theverge.com
63. www.theverge.com
64. www.theverge.com
65. www.theverge.com
66. www.theverge.com
67. www.theverge.com
68. www.theverge.com
69. www.theverge.com
70. www.theverge.com
71. www.theverge.com
72. www.theverge.com
73. www.theverge.com
74. www.theverge.com
75. www.theverge.com
76. www.theverge.com
77. www.theverge.com
78. www.theverge.com
79. www.theverge.com
80. www.theverge.com
81. www.theverge.com
82. www.theverge.com
83. www.theverge.com
84. www.theverge.com
85. www.theverge.com
86. www.theverge.com
87. www.theverge.com
88. www.theverge.com
89. www.theverge.com
90. www.theverge.com
91. www.theverge.com
92. www.theverge.com
93. www.theverge.com
94. www.theverge.com
95. www.theverge.com
96. www.theverge.com
97. www.theverge.com
98. www.theverge.com
99. www.theverge.com
100. www.theverge.com

# tokens 按对数缩放
Dodge, Jesse, et al., "Documenting Large Web Corpora: A Case Study of Crawled Corpus - Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing", 2019</sample>
    <sample id="169">图片显示了一个关于自然语言处理（NLP）模型训练过程的流程图。流程图从左侧开始，用箭头指向右侧，表示数据和模型之间的流动。流程图包括以下步骤：1. 预训练数据：这是流程图的起点，表示在训练NLP模型之前使用的初始数据集。2. 语言模型：这是流程图中的中间步骤，表示使用预训练数据训练的模型。3. 下游任务：这是流程图的终点，表示使用训练好的语言模型来解决特定的NLP任务。流程图中还包含一个注释“到此为止”，表示流程图的结束。此外，流程图右上角有一个小视频窗口，显示了一个名为“张强”的个人，可能是一个演示者或与流程图相关的个人。</sample>
    <sample id="170">这张图片展示了一个关于自然语言处理（NLP）模型训练和应用的流程图。流程图从左侧开始，标有“预训练数据”（Pretraining data），然后通过一个箭头指向“语言模型”（Language models），接着又有一个箭头指向“下游任务”（Downstream tasks）。流程图的右侧有一段文字，分为两部分。第一部分以蓝色字体写着：“如何评估LMs？学习LMs时政治偏见的角色是什么？”第二部分以绿色字体写着：“LMs在NLP应用中是否存在政治偏见问题？”这段文字似乎是针对流程图中提到的“政治偏见”问题的进一步说明或讨论。</sample>
    <sample id="171">这张图片展示了一个关于自然语言处理（NLP）中模型训练和应用的流程图。流程图从左侧开始，标注为“预训练数据”，然后通过一系列步骤连接到右侧的“下游任务”。在“预训练数据”之后，流程图经过一个标记为“语言模型”的阶段，然后继续到“训练模型”的阶段。最后，流程图指向“下游任务”，表示模型在完成预训练和训练后应用于特定任务。

在流程图下方，有两段文字。第一段文字以蓝色字体写着：“如何评估LMs在学习时的Political偏见？预训练数据在其中扮演什么角色？”这段文字似乎在探讨自然语言模型在学习过程中可能吸收政治偏见的问题，以及预训练数据在这一过程中的作用。

第二段文字以绿色字体写着：“LMs在NLP应用中表现如何？LMs的政治偏见是否会影响它们在NLP应用中的公平性？”这段文字关注的是自然语言模型在实际应用中的性能，以及它们的政治偏见是否会影响它们在NLP任务中的公平性和公正性。

总的来说，这张图片和文字共同讨论了自然语言模型在学习和应用过程中的政治偏见问题，以及这些偏见可能对模型性能和公平性的影响。</sample>
    <sample id="172">这张幻灯片讨论了评估大型语言模型（LLM）政治倾向的方法。它提到了支持编码器和解码器LLMs的两种方法。一种方法是通过在问题中使用“我是否同意或不同意这个陈述？”来评估模型对给定陈述的政治倾向。另一种方法是通过自动评估，将模型的输出与政治 compass lit进行对比。

幻灯片还展示了两个卡通角色，代表了政治光谱上的不同观点。一个角色代表了自由派观点，另一个角色代表了权威主义观点。自由派角色位于光谱的左侧，而权威主义角色位于右侧。中间部分有一个中间点，代表了中间派观点。这些角色的存在表明了评估LLMs政治倾向的背景，即它们可能倾向于自由派、中间派或权威主义观点。</sample>
    <sample id="173">这张图片展示了一个图表，展示了各种大型语言模型（LLMs）在政治光谱上的位置。图表分为四个象限，每个象限代表不同的政治观点：左翼、右翼、权威主义和自由主义。图表的x轴表示从左翼到右翼的光谱，y轴表示从权威主义到自由主义的光谱。

在图表上，各种LLMs被标记为点，并用不同颜色的圆圈表示它们的位置。例如，“BERT-base”、“RoBERTa-base”、“distilRoBERTa-base”、“ALBERT-base”、“ALBERT-large”、“BERT-large”、“RoBERTa-large”、“distilBERT-base”、“ALBERT-large”、“BERT-base”和“Alpaca”等模型被放置在图表的不同象限中。

每个LLM的名称都用红色箭头指向图表上的相应位置。例如，“BERT-base”位于左翼和自由主义象限，“RoBERTa-base”位于右翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-large”位于左翼和权威主义象限，“RoBERTa-large”位于左翼和自由主义象限，“distilRoBERTa-base”位于右翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“distilBERT-base”位于右翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于左翼和自由主义象限，“ALBERT-large”位于右翼和权威主义象限，“BERT-base”位于左翼和自由主义象限，“RoBERTa-large”位于</sample>
    <sample id="174">这张图片展示了一个图表，将现有的大型语言模型（LLMs）按照它们的权威性和自由度进行分类。图表分为四个象限，每个象限代表了不同的权威和自由度组合。

1. **图表结构**：
   - **x轴**：表示权威性，从左到右递增。
   - **y轴**：表示自由度，从下往上递增。
   - **象限**：
     - **左上象限**：高权威性、低自由度。
     - **右上象限**：高权威性、高自由度。
     - **左下象限**：低权威性、低自由度。
     - **右下象限**：低权威性、高自由度。

2. **图表中的模型**：
   - **左上象限**：如BERT-base、BERT-large等，这些模型通常由权威机构开发，用于特定任务，具有较低的自由度。
   - **右上象限**：如GPT-3-ada、GPT-3-curie等，这些模型具有较高的自由度，可以生成多样化的文本，但可能缺乏权威性的指导。
   - **左下象限**：如distilRoBERTa-base、ALBERT-large等，这些模型通常具有较低的自由度，用于特定任务，但可能在权威性方面较低。
   - **右下象限**：如ChatGPT、GPT-4等，这些模型具有较高的自由度，可以生成多样化的文本，同时在权威性方面也较高。

3. **图表的意义**：
   - 这个图表帮助我们理解不同大型语言模型在权威性和自由度方面的差异，这对于选择适合特定应用的模型非常重要。
   - 高权威性、低自由度的模型可能更适合需要严格遵循规则和标准的场景，而高自由度、高权威性的模型可能更适合需要创新和灵活性的场景。

通过这个图表，我们可以更好地评估和选择适合特定需求的大型语言模型。</sample>
    <sample id="175">在幻灯片中，有一个标题为“预训练数据”的部分。该部分讨论了进一步预训练LM（Roberta、GPT-2）的评估变化政治倾向。它包括两个图表，一个代表新闻媒体，另一个代表社交媒体（Reddit）。每个图表都分为三个部分：左、中和右。新闻媒体图表显示，左部分为蓝色，中间部分为灰色，右部分为红色。社交媒体图表显示，左部分为绿色，中间部分为灰色，右部分为红色。</sample>
    <sample id="176">预训练数据进一步预训练LM（Roberta，GPT-2）检查点，评估政治倾向的变化。新闻媒体左侧、中间和右侧的社交媒体（Reddit）左侧、中间和右侧。</sample>
    <sample id="177">结果

 partisan shifts in LM political leaning

 RoBERTa

 GPT-2

 original

 reddit

 news

 reddit

 news

 reddit

 original

 news</sample>
    <sample id="178">这张图片是一个幻灯片，展示了LM（大型语言模型）政治倾向的转变结果。它使用一个坐标轴系统，其中x轴代表从左到右的范围，y轴代表从新闻到Reddit的范围。图中有四个象限，每个象限代表不同的政治倾向和信息来源组合。

* **左上象限**：表示原始新闻来源，LM倾向于更左翼。
* **右上象限**：表示Reddit作为原始新闻来源，LM倾向于更右翼。
* **左下象限**：表示Reddit作为新闻来源，LM倾向于更左翼。
* **右下象限**：表示Reddit作为新闻来源，LM倾向于更右翼。

图中有一个箭头从原始新闻来源指向Reddit，表示LM从原始新闻来源向Reddit的转变。箭头的方向和长度表明了LM政治倾向的变化程度。</sample>
    <sample id="179">结果

 partisan shifts in LM political leaning

 RoBERTa

 GPT-2

 original

 reddit

 news

 reddit

 news

 original

 reddit

 news</sample>
    <sample id="180">这张图片展示了一个名为“特朗普牌”的概念，通过比较在45岁之前和之后的转变来分析社交媒体上的政治极化。它使用一个坐标轴系统，x轴代表从左到右的新闻源，y轴代表从GPT-2到GPT-3的模型版本。每个象限代表了不同新闻源和模型版本之间的关系。

1. **左上象限**：表示从GPT-2到GPT-3的转变中，新闻源向右移动，表明从左翼到右翼的转变。
2. **右上象限**：表示从GPT-2到GPT-3的转变中，新闻源向右移动，表明从左翼到右翼的转变。
3. **左下象限**：表示从GPT-2到GPT-3的转变中，新闻源向右移动，表明从左翼到右翼的转变。
4. **右下象限**：表示从GPT-2到GPT-3的转变中，新闻源向右移动，表明从左翼到右翼的转变。

每个象限中的点表示了特定新闻源和模型版本之间的具体转变。例如，左上象限中的点表示从GPT-2到GPT-3的转变中，新闻源从左翼向右翼移动了(0.75, -1.24)。

此外，图片还显示了两个额外的象限，分别代表Reddit的中心、左边和右边。这些象限展示了Reddit社区内不同群体之间的转变。例如，Reddit中心的点表示从GPT-2到GPT-3的转变中，Reddit社区从中心向右移动了(-0.75, 3.64)。

总的来说，这张图片表明了社交媒体上的政治极化趋势，特别是在GPT-2到GPT-3的转变中，以及Reddit社区内的转变。</sample>
    <sample id="181">这张图片展示了一个名为“特朗普牌”的图表，它似乎是一个坐标系，分为四个象限。每个象限代表了不同的社交媒体平台和它们的左右翼偏见。图表的标题为“特朗普牌：从左翼到右翼的转变”，表明了这个图表旨在展示这些平台在转变过程中的位置。

图表的x轴标记为“新闻左”、“新闻中心”、“新闻右”，y轴标记为“Reddit左”、“Reddit中心”、“Reddit右”。每个象限都有一个独特的颜色代码：左上角是绿色，右上角是红色，左下角是蓝色，右下角是紫色。

每个象限都有一个箭头指向另一个象限，表示转变的方向。例如，从左上角的绿色到右上角的红色，表示从新闻左到新闻右的转变。箭头旁边有一个数字，表示转变的大小，如“Δx = (-0.13, -1.03)”。

图表的底部有GPT-2的注释，表明使用了GPT-2模型来生成或分析数据。图表的右上角有一个小插图，显示了一个男人站在一个类似坐标系的背景前，可能与图表的主题相关联。

总的来说，这个图表似乎是用来说明社交媒体平台上政治偏见的转变，特别是围绕着特朗普牌展开的转变。</sample>
    <sample id="182">这张图片似乎是一个关于数据可视化或分析的演示幻灯片。它展示了从“Pre-45th”到“post-45th”时期的变化，可能指的是美国前总统特朗普的时代。幻灯片分为多个部分，每个部分都代表了不同的社交媒体平台和方向（左、中心、右）。

幻灯片的标题是“特朗普牌：从Pre-45th到post-45th的转变”，表明它讨论了在特朗普担任总统前后社交媒体上的变化。幻灯片底部显示了GPT-3.5-TURBO生成的文本，这表明使用了人工智能来生成内容。

幻灯片的每个部分都有一个坐标轴，x轴表示方向（左、中心、右），y轴表示时间（Pre-45th、post-45th）。每个部分都有一个箭头，表示方向和时间的变化。箭头的长度和方向可能表示变化的大小和方向。

幻灯片还显示了每个部分的Delta值，这是方向和时间变化的度量。这些值可能表示方向和时间变化的大小和方向。

总的来说，这张图片似乎是关于数据可视化或分析的演示幻灯片，它展示了从“Pre-45th”到“post-45th”时期的变化，可能指的是美国前总统特朗普的时代。它使用GPT-3.5-TURBO生成文本，并显示了方向和时间变化的度量。</sample>
    <sample id="183">这张图片展示了一个表格，标题为“按类别表现”，详细说明了针对不同身份群体的 hate speech 和 misinformation 的表现。表格分为两列，左侧列出各种身份群体，如“黑人”、“穆斯林”、“LGBT+”、“犹太人”、“亚裔”、“拉丁裔”、“女性”、“基督教徒”、“男性”和“白人”。右侧则显示了来自不同来源的 hate speech 和 misinformation 的表现，包括“新闻左翼”、“Reddit 左翼”、“Reddit 右翼”、“民粹主义”、“HP（left）”、“NYT”、“CNN（left）”、“CNN（right）”、“NPR（left）”、“NPR（right）”、“Guard（left）”、“Fox”、“Wex（left）”、“BBRT（right）”和“WAT（right）”。

表格中的数据以百分比表示，颜色编码用于表示表现的好坏。黄色表示最好，深蓝色表示最差。例如，“黑人”类别在“新闻左翼”中表现最好（90.13%），而在“Reddit 左翼”中表现较差（89.84%）。类似地，“LGBT+”类别在“Reddit 左翼”中表现最好（90.19%），而在“Reddit 右翼”中表现较差（82.02%）。

表格底部有一段说明文字，解释了颜色编码：黄色表示最好，深蓝色表示最差。这段说明有助于理解数据的含义，即黄色表示表现最佳，深蓝色表示表现较差。

总的来说，这个表格提供了关于不同身份群体和不同来源在 hate speech 和 misinformation 表现方面的详细信息，通过颜色编码直观展示了表现的好坏。</sample>
    <sample id="184">这张图片是一张表格，标题为“按类别表现”，展示了不同身份群体（如新闻左翼、新闻右翼、社交媒体左翼和社交媒体右翼）的性能数据。表格分为两列，左侧列出身份群体，右侧显示性能评分。表格底部有一行注释，说明了颜色编码：黄色表示最好，深蓝色表示最差。

表格中包含多个数据点，每个数据点代表一个身份群体的性能评分。例如，新闻左翼的评分从80.43到90.19不等，而社交媒体左翼的评分从85.72到90.81不等。表格还显示了来自不同来源的性能评分，如CNN（美国广播公司）、Fox News（福克斯新闻）和NRJ（国家铁路局）。

表格的右侧有一个小视频窗口，显示了一个穿着黑色西装和领带的人，背景是白色的墙壁。这个人的面部被模糊处理，无法辨认。</sample>
    <sample id="185">这张图片是一个表格，标题为“按类别表现”，它展示了不同身份群体和来源的 hate speech 和 misinformation 的数据。表格分为两列，左边是“ hate speech”和“misinformation”，右边是各种身份群体和来源。表格底部有一段说明，解释了颜色编码：深黄色表示最好，深蓝色表示最差。

在“ hate speech”类别中，数据如下：

- 新闻左翼：80.63%
- Reddit 左翼：89.84%
- Reddit 右翼：80.20%
- 黑人：80.08%
- 穆斯林：90.19%
- 同性恋者：89.09%
- 犹太人：82.83%
- 阿拉伯人：89.83%
- 拉美裔：85.66%
- 女性：91.15%
- 基督徒：87.82%
- 男性：87.65%
- 白人：83.13%

在“misinformation”类别中，数据如下：

- CNN 左翼：85.44%
- CNN 右翼：84.82%
- CNN 中立：84.34%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
- CNN 中立：84.44%
-</sample>
    <sample id="186">这张图片是一张表格，标题为“按类别表现”，展示了针对不同身份群体的 hate speech 和 misinformation 的表现。表格分为两列，左边是“ hate speech”和“misinformation”，右边是各种身份群体的名称，如“NEWS LEFT”、“REDDIT LEFT”、“BLACK”、“MUSLIM”、“LGBT+”、“Jews”、“ASIAN”、“LATINX”、“WOMEN”、“CHRISTIAN”、“MEN”和“WHITE”。表格中的数据以百分比表示，颜色从黄色到蓝色不等，其中黄色代表最好，蓝色代表最差。

表格底部有一段说明文字，解释了颜色编码：黄色表示最好，蓝色表示最差。表格的右上角有一个小视频窗口，显示了一个男人在讲话，可能是对表格内容进行解释或讨论的人。</sample>
    <sample id="187">这张图片是一张表格，标题为“按类别表现”，详细说明了针对不同身份群体的 hate speech 和 misinformation 的表现。表格分为两列，左侧是“类别”（如“黑人”、“穆斯林”、“LGBT+”等），右侧是“表现”（如“新闻左翼”、“Reddit 左翼”、“Reddit 右翼”等）。表格底部有一行注释，说明了颜色代码：黄色表示最好，深蓝色表示最差。

表格中每个单元格包含一个数字，表示特定类别和表现下的 hate speech 或 misinformation 的数量。例如，“黑人”类别下“新闻左翼”的 hate speech 数量为 80.63，而“Reddit 左翼”的 hate speech 数量为 89.44。表格还显示了 misinformation 的数据，例如“纽约左翼”下的 misinformation 数量为 84.44。

表格的右上角有一个小窗口，显示了一个男人的照片，可能是演讲者或演示者。表格的右上角还显示了排名，从“第1名”到“第2名”，但没有具体说明排名标准。

总的来说，这张表格提供了关于 hate speech 和 misinformation 在不同类别和表现下的详细数据，以及一个视觉辅助工具来表示表现的优劣。</sample>
    <sample id="188">这张图片是一张表格，标题为“按类别表现”，显示了针对不同身份群体的 hate speech 和 misinformation 的数据。表格分为两列，左边是“ hate speech”和“misinformation”，右边是各种身份群体的名称，如“NEWS LEFT”、“BLACK”、“MUSLIM”、“LGBT+”、“JWS”、“ASAIN”、“LATINX”、“WOMEN”、“CHRISTIAN”、“MEN”和“WHITE”。表格底部有一个注释，说明了颜色代码：黄色表示最好，蓝色表示最差。

表格中每个单元格包含一个数字，表示特定身份群体在 hate speech 或 misinformation 中的表现。例如，“NEWS LEFT”在 hate speech 中的值为 80.63，在 misinformation 中的值为 85.43。类似地，“BLACK”在 hate speech 中的值为 89.19，在 misinformation 中的值为 87.24。

表格的右上角有一个小窗口，显示了一个男人的照片，可能是演示者或演讲者。表格的右下角有一个标签，写着“1st”、“2nd”和“3rd”，可能表示排名或分类。</sample>
    <sample id="189">这张图片展示了一个表格，标题为“按类别表现”，详细说明了针对不同身份群体的 hate speech 和 misinformation 的表现。表格分为两列，左侧是“类别”（如“新闻左翼”、“Reddit左翼”、“Reddit右翼”等），右侧是“来源”（如“HP（left）”、“NYT（left）”、“CNN（left）”等）。表格中的数据以颜色编码表示，黄色表示最好，深蓝色表示最差。

表格的左下角有一个注释，说明了颜色编码：黄色表示最好，深蓝色表示最差。表格的右上角有一个小图，显示了两个排名，一个排在第一位，另一个排在第二位。</sample>
    <sample id="190">文本：麦当劳的员工认为，麦当劳是一家以利润为导向的公司，他们认为麦当劳应该停止剥削员工。他们认为麦当劳应该停止剥削员工，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和尊严，因为麦当劳应该尊重员工的人权和尊严。他们认为麦当劳应该尊重员工的人权和 dignity，因为麦当劳应该尊重员工的人权和 dignity。</sample>
    <sample id="191">在幻灯片中，有一个表格，其中包含文本、目标标签和基线的列。文本部分包括一些句子，如“麦当劳的人很不友好”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统”、“我无法相信特朗普总统</sample>
    <sample id="192">标题：关于种族主义言论的表格

| 问题 | 真实 | 虚假 |
| --- | --- | --- |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | 真实 | 虚假 |
| 你认为同性恋者应该被允许结婚吗？ | �</sample>
    <sample id="193">标题：关于 hate speech 的文本分析

表格 12：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 13：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 14：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 15：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 16：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 17：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 18：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 19：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 20：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 21：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 22：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 23：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 24：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 25：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 26：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 27：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 28：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 29：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 30：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 31：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 32：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 33：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 34：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 35：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 36：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治保守主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |

表格 37：关于 hate speech 示例的定性分析，根据不同的政治背景进行分类。该表格展示了不同观点之间的对比，包括支持和反对 hate speech 的论点。

| 政治背景 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| --- | --- | --- |
| 政治极端主义 | 支持 hate speech 的观点 | 反对 hate speech 的观点 |
| 政治自由 | 支持 hate speech 的观点 | 反对 hate speech 的观点</sample>
    <sample id="194">根据表格中提供的数据，可以推断出关于政治偏见的观察结果。表格分为两列，每列包含多个问题和相应的答案。每行代表一个特定问题及其答案。

1. **问题与答案分析**：
   - **问题1**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案为'是的'，表明支持同性恋合法化。
   - **问题2**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案同样为'是的'，再次表明支持同性恋合法化。
   - **问题3**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，进一步强调了对同性恋合法化的支持。
   - **问题4**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题5**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题6**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题7**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题8**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题9**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题10**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题11**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题12**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题13**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题14**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题15**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题16**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题17**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题18**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题19**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题20**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题21**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题22**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题23**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题24**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题25**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题26**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题27**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题28**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题29**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题30**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题31**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题32**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题33**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题34**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题35**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题36**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题37**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题38**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题39**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题40**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题41**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题42**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题43**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题44**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题45**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题46**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题47**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题48**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题49**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题50**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题51**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题52**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题53**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题54**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题55**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题56**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题57**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题58**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题59**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题60**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题61**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题62**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题63**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题64**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题65**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题66**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题67**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题68**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题69**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题70**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题71**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题72**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题73**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题74**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题75**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题76**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题77**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次为'是的'，再次表明支持同性恋合法化。
   - **问题78**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案仍然为'是的'，持续支持同性恋合法化。
   - **问题79**：'你认为同性恋应该合法吗？'
     - 答案：'是的'
     - 分析：答案再次</sample>
    <sample id="195">根据表格中提供的信息，我们可以看到一个关于“种族主义”问题的例子。问题分为两部分：一个是关于种族主义的陈述，另一个是关于种族主义的陈述是否正确的选项。

陈述如下：

“种族主义是指基于种族、族裔或族裔偏见而歧视或优越其他种族、族裔或族裔的行为。”

选项如下：

A. 正确
B. 错误

根据陈述的内容，我们可以推断出种族主义是指基于种族、族裔或族裔偏见而歧视或优越其他种族、族裔或族裔的行为。因此，选项A（正确）是正确的。

这个陈述强调了种族主义的本质，即基于种族、族裔或族裔偏见而歧视或优越其他种族、族裔或族裔的行为。种族主义可能导致不公正和不平等的待遇，对个人和社会造成负面影响。</sample>
    <sample id="196">讨论：介于Scylla和Charybdis之间的问题是“消毒”还是“净化”，这是个问题。在预训练数据、语言模型和下游任务之间存在一条曲线。</sample>
    <sample id="197">讨论在Scylla和Charitydb之间进行。问题在于“初始化”还是“消毒”。预训练数据、语言模型和下游任务之间的关系是：预训练数据通过语言模型处理，然后用于执行各种下游任务。</sample>
    <sample id="198">讨论：介于Scylla和Charity之间的问题是“初始化”还是“消毒”。预训练数据、语言模型和下游任务之间的关系。</sample>
    <sample id="199">图片显示了一个幻灯片，标题为“谢谢！”它描绘了一个流程图，展示了预训练数据、语言模型和下游任务之间的关系。流程图由三个方框组成，每个方框代表一个阶段，用箭头连接在一起，表示从一个阶段到另一个阶段的进展。

在流程图下方，有四个人的照片和名字。从左到右分别是：

1. 尚锋 (Shangfeng Feng)
2. 朴善英 (Chan Young Park)
3. 刘宇 (Yuhuan Liu)
4. 保拉·特索科 (Yulia Tsvetkov)

每个人的名字下面都有他们的机构名称。从左到右分别是：

1. 普林斯顿大学 (Paul G. Allen School of Computer Science &amp; Engineering)
2. UW NLP (University of Washington Natural Language Processing Group)
3. 加州斯坦福大学 (Stanford University, Department of Linguistics and the Stanford Natural Language Processing Group)
4. 中国科学院自动化研究所 (CASIA Institute of Automation, Chinese Academy of Sciences)

这些照片和名字表明，这些人可能是与流程图中描述的预训练数据、语言模型和下游任务相关的研究或项目的参与者或贡献者。</sample>
    <sample id="200">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="201">根据图片中呈现的图表，MPP评估使用了最多900个词元的上下文长度。这可以通过图表标题“MPP judgments are robust for arbitrary context lengths”以及x轴上标记的最大值“900”来确定。</sample>
    <sample id="202">根据幻灯片中提到的领域，他们的数据集中包含音乐和情感领域。这可以从提到的歌曲《Easy on Me》和《I Gotta Feeling》以及描述这些歌曲的表达如“轻松愉快”、“充满活力”和“关于河流”来看出。此外，提到的情感表达如“悲伤”、“快乐”和“关于时间的选择”表明数据集还涉及情感领域。</sample>
    <sample id="203">在研究领域中，"positionality"通常指的是研究者或参与者在研究过程中所处的位置或视角。它包括了研究者的文化、社会、政治和历史背景，以及这些因素如何影响他们对研究对象的观察和解释。在定性研究中，了解positionality对于识别潜在偏见并确保研究结果的准确性和可靠性至关重要。</sample>
    <sample id="204">演讲者的名字是：Dawe Xu、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="205">根据图片中显示的幻灯片，EDAtt 似乎适应了现有的离线 ST 模型。这可以从幻灯片上的文本推断出来，其中提到“EDAtt 适应了现有的离线 ST 模型。”这表明 EDAtt 是一种改进或扩展现有模型的方法，可能旨在解决特定问题或提高性能。</sample>
    <sample id="206">根据幻灯片上显示的论文标题，作者数量为五位。</sample>
    <sample id="207">根据图片中显示的图表，被测模型似乎在测试套件上运行良好。图表中的柱状图显示了“有任务特定训练”和“没有任务特定训练”的比较结果。在“有任务特定训练”类别中，柱状图明显高于“没有任务特定训练”类别，表明模型在经过任务特定训练后表现更好。这表明该模型在测试套件上运行良好，因为它在任务特定训练下显示出更高的准确率。</sample>
    <sample id="208">KITMUS的三个变体是：1. Background-Pretrain：典型的设置，2. Background-Both：在上下文中显式提供背景知识，3. Background-Inference：只有在推理时才可用知识。</sample>
    <sample id="209">根据图片显示，论文的作者所属机构是Google Research。这可以从幻灯片左下角的Google标志和文字推断出来。</sample>
    <sample id="210">根据幻灯片上列出的研究问题，最后一个研究问题是“如何更有效地利用现有的干净样本？”</sample>
    <sample id="211">仅凭图片本身，无法提供关于“指标灵敏度”如何工作的详细解释。图片显示了一个数学表达式，表明灵敏度是通过计算函数在给定输入下的梯度来衡量的。这表明该模型正在评估其对输入变化的敏感性，这是机器学习中优化过程的一部分。然而，没有额外的上下文或信息，无法确定灵敏度指标的具体应用或计算细节。</sample>
    <sample id="212">演讲者的名字是“Wenjun Peng”，如幻灯片上列出的作者列表中所示。</sample>
    <sample id="213">更高的灵敏度表示模型性能得到了提高。在图4中，较低的灵敏度值（如0.27）表明模型对未见过的任务更稳定和一致，这通常是一个积极的特征。相反，较高的灵敏度值（如50.8）表明模型对未见过的任务更不稳定和不一致，这通常是一个消极的特征。因此，在这个语境中，灵敏度数值越低越好。</sample>
    <sample id="214">根据幻灯片中提供的信息，无法确定模型在预训练期间接收的语言上下文类型。幻灯片关注的是语言模型接受度判断的稳健性问题，而不是模型训练或预训练的具体细节。要了解模型在预训练期间接收的语言上下文，需要额外的信息，可能来自论文本身或相关的技术文档。</sample>
    <sample id="215">根据幻灯片中显示的图表，对于在WSL（Weakly Supervised Learning）中获得良好的表现，通常需要大约50个干净的验证样本。这是通过观察图表上标记为“50”点的准确率线来确定的，该线代表了不同方法（FT_w、COSINE、L2R、MLC和BOND）在WSL环境下的性能。这些线表明，在这个点上，所有方法都显示出接近或超过80%准确率的表现，这可以被视为良好表现的指标。</sample>
    <sample id="216">根据幻灯片上提供的英文内容，这篇论文的作者Myra Cheng、Esin Durmus和Dan Jurafsky所属机构是斯坦福大学工程学院计算机科学系。</sample>
    <sample id="217">根据图片，开发新的方法来衡量媒体偏见的原因可能包括：1. 现有方法的局限性：现有方法可能无法准确捕捉模型在不同观点或偏见上的微妙差异。2. 需要更全面的评估：新方法可以提供对模型在各种情况下的表现的更全面了解，而不仅仅是简单的分类（左翼、右翼或中间）。3. 适应不断变化的环境：随着社会和政治环境的变化，需要一种能够适应这些变化并提供持续评估的方法。4. 提高透明度和可解释性：通过开发新的衡量标准，可以增加模型决策过程的透明度和可解释性，这对用户理解和信任模型至关重要。5. 推动研究和改进：新的衡量标准可以激励研究者开发更公正和中立的模型，促进AI技术的道德发展。</sample>
    <sample id="218">根据幻灯片显示，演讲者的名字是Alexandra Oteanu。她的照片位于幻灯片的右侧，与演讲者的头像对齐。</sample>
    <sample id="219">根据图片中提供的信息，政治偏见传播流程似乎涉及从各种来源（如网站）收集大量文本数据，然后通过机器学习模型进行处理。这些模型可能被训练来识别和分类文本中的政治偏见。一旦模型被训练好，它就可以被用来检测和标记文本中的偏见，这可能是为了研究目的或实际应用，如内容审查或社交媒体监控。</sample>
    <sample id="220">根据图表显示，DEplain-apa 和网站的简化过程在某些方面有所不同。 DEplain-apa 使用了“删除”、“替换”和“添加”等操作，而网站似乎专注于“删除”和“替换”。 DEplain-apa 进行了更多的“添加”操作，而网站则没有进行任何添加操作。 这表明 DEplain-apa 可能涉及在文本中插入新词或短语，而网站简化文本时则不这样做。</sample>
    <sample id="221">是的，Coscript 是公开可用的。</sample>
    <sample id="222">水印插入到文本中是通过在原始嵌入\(e_0\)上添加目标嵌入\(e_t\)来实现的。首先，定义一个目标嵌入\(e_t\)，然后计算句子中的触发器数量\(Q(S)\)。接下来，将目标嵌入添加到原始嵌入上，得到最终的嵌入\(E_c\)。</sample>
    <sample id="223">根据幻灯片上显示的标志和文字，这篇论文的作者分别来自宾夕法尼亚州立大学和亚马逊。</sample>
    <sample id="224">是的，根据幻灯片中呈现的数据，像 mt5 这样的编码器-解码模型可以通过混合语言的训练来改进。在多语言设置下，mt5 的性能显著提高，从单语设置下的 32.30 分钟到混合语言设置下的 67.55 分钟。这表明通过暴露于多种语言数据，模型能够更好地泛化并提高其性能。</sample>
    <sample id="225">受限制语言规划的一个示例是“如何制作巧克力蛋糕？”这个问题可以通过将可可粉加入面粉来回答。</sample>
    <sample id="226">仅基于图片中提供的信息，他们通过在不同数据集上进行实验来确保其方法的隐蔽性。这表明他们正在验证他们的方法在各种文本数据集上的性能，这有助于评估其普遍性和对不同文本特征的鲁棒性。</sample>
    <sample id="227">根据幻灯片中提供的英文内容，研究涉及使用现有的预训练语言模型（PLM）来构建新的PLM。这包括比较不同的预训练策略和数据来源的影响。具体而言，它探讨了从头开始构建模型与继续使用现有预训练模型进行微调之间的差异，以及使用公开和私有医疗数据集的比较。这个研究旨在评估这些方法的有效性，并确定在构建新的PLM时的最佳实践。</sample>
    <sample id="228">根据柱状图显示，GPT-4与非洲伊斯兰国家的立场最不一致。这可以通过该柱的较浅颜色来表示，表明较低的相似度得分（0.47），与其他地区相比，其他地区得分较高。</sample>
    <sample id="229">演讲者在示例句子'What is our solution?'上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="230">根据图表，任务数量似乎与模型性能呈正相关。随着任务数量的增加，曲线显示性能提高，表明模型在处理更多任务时表现得更好。这可能意味着模型能够更好地泛化或学习更广泛的技能，从而在处理各种任务时更加高效。</sample>
    <sample id="231">根据图表，作者用来比较其方法的三个无树基线是：1. LSTM seq2seq 2. Zhang and Lapata 3. PP recursion 这些基线在图表中以不同颜色表示，以便与“我们的”模型进行视觉区分。</sample>
    <sample id="232">根据图片显示，Alexander Koller和Ivan Titov似乎是合著者，与第一作者Matthias Lindemann合作。这可以从他们的名字在标题下方并列出现推断出来，通常表示他们共同为该研究做出了贡献。</sample>
    <sample id="233">PaLM的第一作者是Chowdery et al., 2022。</sample>
    <sample id="234">NLPositionality：设计数据集和模型的设计偏见表征</sample>
    <sample id="235">NLPositionality：设计数据集和模型的设计偏见表征</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Imagine...</sample>
    <sample id="238">Carl Jones Aditya Sharma Tech Lead, New York Times Tech Lead, Times of India Imagine... Can you stop being a jerk? : (0.82)</sample>
    <sample id="239">Imagine... Design bias example! Carl Jones Aditya Sharma Tech Lead, New York Times Tech Lead, Times of India Pressstitutes everywhere on the news (0.82) (0.33)</sample>
    <sample id="240">[1] Savin-Baden, Maggi, and Claire Howell-Major. “Qualitative research: The essential guide to theory and practice.” Qualitative Research: The Essential Guide to Theory and Practice (2013).</sample>
    <sample id="241">“人们的观点源于他们的 demographics，identity 和生活经历。</sample>
    <sample id="242">“人们持有的观点是他们的人口统计、身份和生活经历的结果。</sample>
    <sample id="243">数据集和模型有位置性吗？ACL 2022. [1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” EMNLP 2022. [2] Liang et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022. [3] Cambo and Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="244">数据集和模型有位置吗？</sample>
    <sample id="245">数据集和模型有位置性吗？[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.[2] Li, et al. “GEOMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.[3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science.</sample>
    <sample id="246">数据集和模型有偏见吗？轶事证据：- 模型和数据集探测[112]。- 理论上的模型偏见定义[3]。[1] Blasi, et al. “系统性不平等在世界语言技术性能中的表现。” ACL 2022.[2] Cohan, et al. “GEOMLLAMA: 多语言预训练模型的多语言通用性探测。” EMNLP 2022.[3] Cambo与Gergo. “模型偏见和计算反射性：促进模型反思性的数据科学。” Data Science 2022</sample>
    <sample id="247">数据集和模型有偏见吗？轶事证据：- 模型和数据集探测[11][2] - 理论上的模型偏见定义[3] [1] Blasi, et al. “系统性不平等在世界语言技术性能中的表现。” ACL 2022。 [2] Zhang, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models。” EMNLP 2022。 [3] Cambo与Gergle。“模型偏见和计算反射性：促进多语言数据科学中的反思性。” Data Science</sample>
    <sample id="248">数据集和模型有偏见吗？轶事证据：- 模型和数据集探测[112]。- 理论上的模型偏见定义[3]。[1] Blasi, et al. “系统性不平等在世界语言的语言技术性能中的表现。” ACL 2022.[2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.[3] Cambo and Gergle. “模型偏见和计算反射性：促进多语言数据科学中的反射性。” Data Science 2022</sample>
    <sample id="249">问题：数据集和模型有位置吗？</sample>
    <sample id="250">NLPositionality
一个用于表征自然语言处理数据集和模型中设计偏见的框架</sample>
    <sample id="251">框架

数据集
从数据集中随机采样一个数据集。
处理
每个实例都有一个标签。
实例被分为训练和测试集。
注释
从测试集中接收注释。
注释与模型预测的黄金标签进行比较，以观察模型的性能。
过程
模型预测
年龄
性别
种族
教育程度
国家
模型预测与黄金标签之间的相关性通过计算数据集中不同人口统计学组别的卡方检验来测量。</sample>
    <sample id="252">框架

从数据集中抽取样本数据集
每个实例是一个
模型预测
模型输出的特征
处理
收集
注释
1）用多样化的注释员重新注释数据集。
2）接收注释
3）从模型中观察到的标签作为“黄金标签”进行测量
4）Pearson’s相关系数衡量数据集中不同特征之间的相关性</sample>
    <sample id="253">框架

从数据集中采样一个数据集
每个实例是一个模型预测
实例被发送到多个注释者
注释者为每个实例提供注释
收到的注释与模型的gold标签进行比较
Pearson’s相关系数是通过比较数据集中不同人口统计学特征之间的注释和gold标签的分离来衡量的

1.重新注释数据集，使用多样化的注释者。</sample>
    <sample id="254">框架

从数据集中采样一个数据集
每个实例是一个任务
实例被发送到多个注释者
注释者提供标注
收到的标注与模型的预测进行比较
Pearson’s相关系数是根据数据集中不同特征之间的相关性计算的
1）重新注释数据集，使用多样化的注释者</sample>
    <sample id="255">框架

收集
每个实例都有一个标签
实例被发送到
注释
从世界中获取注释
处理
模型预测
年龄
性别
种族
教育程度
国家
1) 将收到的注释与数据库中的注释进行比较，通过皮尔逊相关系数计算Pearson R分数
2)</sample>
    <sample id="256">框架

收集
每个实例都有一个标签
实例被发送到
注释
从世界获取注释
处理
模型预测
年龄
性别
种族
教育
国家
1)将收到的注释与数据库中的模型进行比较
2)通过计算与模型相关的Pearson R分数来比较注释</sample>
    <sample id="257">“你愿意和一个AI机器人生活吗？” “你更喜欢AI在通知你有害内容时表现出的温和还是激烈的反应？” “你认为技术乌托邦中混合现实和虚拟现实的比例应该是多少？”</sample>
    <sample id="258">图中展示了一个名为“Lab in the Wild”的在线实验平台，该平台旨在收集多样化的志愿者/研究参与者数据。该平台的界面包括以下主要部分：

1. **标题和Logo**：页面顶部显示了“Lab in the Wild”的标题和Logo，其中包含一个绿色的地球仪图标。

2. **导航菜单**：页面左侧的导航菜单包括以下选项：
   - Our Experiments
   - Findings &amp; Data Sets
   - Blog
   - About Us
   - About the Project

3. **实验描述**：页面中间部分展示了三个实验的简要描述：
   - **实验一**：探讨AI与人类的关系，标题为“Could you live with an AI and its money?”，描述了实验的背景和目的。
   - **实验二**：研究AI在日常任务中的应用，标题为“Are you better at noticing hateful speech than an AI?”，同样提供了实验的背景和目的。
   - **实验三**：调查技术乌托邦的感知，标题为“Where are you on the techno-utopian scale?”，解释了实验的目标和方法。

4. **参与按钮**：每个实验描述下方都有一个“Participate now!”按钮，鼓励用户立即加入实验。

5. **统计信息**：页面右上角显示了“5,376,396”总参与者数量，表明该平台已经吸引了大量参与者。

6. **额外信息**：页面右下角有一段文字，解释了该实验是来自研究者的在线实验，强调了数据收集的目的和重要性。

通过这些元素，Lab in the Wild平台旨在吸引多样化的参与者，收集他们的反馈和数据，以支持其研究目标。</sample>
    <sample id="259">任务A：社会接受度

1. 读情况
2. 输入你对它的看法
3. 查看其他人对它的看法

AI专家：</sample>
    <sample id="260">Task A: Social Acceptability

1. Read the situation
2. Enter what you think about it.
3. See what others thought about it!

The AI speculates:
Study participants in the United States:

参与者比较他们的回应与其他人和AI的回应。</sample>
    <sample id="261">Task A: Social Acceptability
Analysis
Datasets
- Social Chemistry
Models
- Delphi
- GPT-4</sample>
    <sample id="262">任务B：网络攻击 1.阅读示例。 2.输入你对它的看法。 3.看看其他人对它的看法。</sample>
    <sample id="263">任务B：Toxicity
分析
数据集
- Dynahate
模型
- Perspective API
- Rewrite API
- State RoBERTa
- GPT-4</sample>
    <sample id="264">结果
NLP数据集和模型与谁对齐？</sample>
    <sample id="265">数据集和模型最符合讲英语国家。</sample>
    <sample id="266">数据集和模型与拥有大学及以上教育的人群最一致。</sample>
    <sample id="267">数据集和模型与拥有大学教育的人最一致。</sample>
    <sample id="268">找到2：有些人口被落下。</sample>
    <sample id="269">数据集和模型与非二元人对齐性较低。</sample>
    <sample id="270">所以，我们可以做什么？解决NLP中的位置性问题</sample>
    <sample id="271">推荐</sample>
    <sample id="272">推荐意见

1.在整个构建数据集或模型的过程中，记录所有相关的的设计选择。
2.通过视角主义的角度进行NLP研究：
a.分享分歧的数据集标签！
b.使用可以处理标注员 disagreement 的建模技术。
3.为特定社区构建专门化的数据集和模型是包容性NLP的可取方法（例如，Masakhane计划）。

[1] https://www.masakhane.io</sample>
    <sample id="273">谢谢！
仪表盘链接：nlpositionality.washington.edu/
论文：bit.ly/NLPositionality-Paper/
Deepsigh</sample>
    <sample id="274">演讲者提到了 SimulST 的四个问题。</sample>
    <sample id="275">在训练 NLP 模型时，减轻数据集中的社会和政治偏见的有效方法包括：1. **数据收集**：确保数据集具有代表性且多样，避免偏见。2. **数据清洗**：识别并删除或修改可能引入偏见的样本。3. **数据增强**：使用技术如同义词替换、随机化等来增加数据集的多样性。4. **模型验证**：使用多种评估指标和交叉验证来检测和纠正偏见。5. **持续监控和更新**：定期检查模型性能，并根据新数据和反馈进行调整。</sample>
    <sample id="276">61st Annual Meeting of the Association for Computational Linguistics
Toronto, Canada
June 13-14, 2023

蒸馏大型语言模型中的脚本知识用于受限语言规划
李宇，陈江强，符子全，Ge Xuyan，沙Ham Charles Robert Jankowski，肖 Yahua，阳肖，阳 Deging Yang

复旦大学
Brain Technologies Inc.</sample>
    <sample id="277">如何制作蛋糕？ 1. 准备材料：2. 将烤箱预热至325°F（163°C）并涂上黄油。 3. 在蛋糕盘上涂抹黄油和糖。 4. 添加鸡蛋。 5. 搅拌面糊。 6. 将面糊倒入蛋糕盘中。 7. 烘烤1小时15分钟。 大型语言模型（LLMs）可以有效地将分解目标分解成步骤。</sample>
    <sample id="278">如何制作蛋糕？ 1. 准备材料：2. 预热烤箱至325°F（163°C）；3. 在蛋糕盘上涂抹黄油和糖；4. 添加鸡蛋；5. 搅拌黄油和糖与鸡蛋的混合物；6. 加入面粉；7. 将面糊倒入蛋糕盘中；8. 烘烤1小时15分钟。大型语言模型（LLMs）可以有效地将分解目标分解为步骤。</sample>
    <sample id="279">受限语言规划

如何制作草莓蛋糕？
…将草莓酱加入面粉中…

如何制作巧克力蛋糕？
…将可可粉加入面粉中…

具体目标可以被不同现实生活中具有多方面约束的具体目标继承</sample>
    <sample id="280">受限语言规划

如何制作草莓蛋糕？
…将草莓酱加入面粉中…

如何制作巧克力蛋糕？
…将可可粉加入面粉中…

具体目标可以被不同现实生活中具有多方面约束的具体目标继承</sample>
    <sample id="281">受限语言规划

如何制作草莓蛋糕？
…将草莓酱加入面粉中…

如何制作巧克力蛋糕？
…将可可粉加入面粉中…

具体目标可以被不同现实生活中具有多方面约束的具体目标继承。</sample>
    <sample id="282">LLMs在受限语言规划中的表现如何？

数据集：wikiHow + 生成的约束

约束类型1：修改器

定义：通过添加属性或修改来修改或约束抽象目标

示例1：制作一块巧克力蛋糕
示例2：制作一块粉色蛋糕

约束类型2：方法

定义：控制过程的工具或指定模式

示例1：用烤箱做蛋糕
示例2：用纸杯做蛋糕

约束类型3：意图

定义：在实现目标时，额外的目的或需求

示例1：为婚礼做蛋糕
示例2：为糖尿病患者做蛋糕</sample>
    <sample id="283">LLMs在受限语言规划中的表现如何？</sample>
    <sample id="284">LLMs在受限语言规划中的表现如何？

数据集：wikiHow + 生成的约束

约束类型1：修改器

定义：修改器通过修改或约束抽象目标来定义。例如：制作巧克力蛋糕或制作粉色蛋糕。

示例1：制作巧克力蛋糕。
示例2：制作粉色蛋糕。

约束类型2：方法

定义：方法通过控制过程或指定使用的工具来定义。例如：用烤箱做蛋糕，用微波炉做蛋糕。

示例1：用烤箱做蛋糕。
示例2：用微波炉做蛋糕。

约束类型3：意图

定义：意图通过在设定目标时添加额外目的或需求来定义。例如：为婚礼做蛋糕，为糖尿病患者做蛋糕。

示例1：为婚礼做蛋糕。
示例2：为糖尿病患者做蛋糕。</sample>
    <sample id="285">LLMs是否能进行约束语言规划？</sample>
    <sample id="286">LLMs是否能进行约束语言规划？</sample>
    <sample id="287">LLMs在生成任务中通常会犯什么类型的错误？</sample>
    <sample id="288">LLMs在生成任务中通常会犯什么类型的错误？幻灯片分为两部分。左侧是一个圆形图表，展示了不同类型的错误，如“无约束”、“缺失步骤”、“重复步骤”、“错误顺序”和“不一致的步骤”。右侧有一个图片，显示了一个穿着绿色衬衫、戴着眼镜的人，背景是现代办公室。幻灯片底部的文字解释了LLMs在生成脚本时可能存在的问题，包括语法完整性（SE）和一致性（FE）。幻灯片强调，虽然语法完整性（SE）在生成脚本时可以接受，但一致性（FE）不能保证。</sample>
    <sample id="289">InstructGPT通常失败的类型
| 类别 | 0.0 | 0.7 | 1.0 |
| --- | --- | --- | --- |
| 工作 | 0.0 | 0.67 | 1.0 |
| 人际关系 | 0.0 | 0.5 | 1.0 |
| 个人护理 | 0.14 | 0.33 | 0.67 |
| 健康 | 0.33 | 0.57 | 0.67 |
| 食物 | 0.2 | 0.66 | 0.78 |
| 服装 | 0.2 | 0.33 | 0.67 |
| 家庭生活 | 0.0 | 0.0 | 1.0 |
| 教育 | 0.2 | 0.33 | 0.67 |
| 电子设备 | 0.2 | 0.33 | 0.67 |
| 车辆 | 0.2 | 0.33 | 0.67 |
| 艺术 | 0.33 | 0.29 | 1.0 |

不同类别的InstructGPT规划性能存在显著差异</sample>
    <sample id="290">输入：一个抽象目标
步骤1：生成特定目标，使用GPT-3进行学习
抽象目标：制作蛋糕
约束条件：
具体目标：
G1（修改器）：制作巧克力蛋糕
G2（方法）：在婚礼上做蛋糕
G3（意图）：为婚礼制作蛋糕</sample>
    <sample id="291">输入：一个抽象目标
步骤1：生成具体目标，使用GPT-4进行学习
抽象目标：制作蛋糕
约束条件：
具体目标：
G1（修改器）：制作一块巧克力蛋糕
G2（方法）：在婚礼上制作一块蛋糕
G3（意图）：为婚礼制作一块蛋糕</sample>
    <sample id="292">输入：抽象目标：制作蛋糕
具体目标：
G1（修改器）：制作巧克力蛋糕
G2（方法）：在婚礼上制作蛋糕
G3（意图）：为婚礼制作蛋糕
候选脚本
步骤1：生成G1的候选脚本，通过基于任务的无监督学习
步骤2：生成G2的候选脚本，通过基于任务的无监督学习
步骤3：生成G3的候选脚本，通过基于任务的无监督学习</sample>
    <sample id="293">方法

步骤1：通过基于上下文学习生成与InstructGPT相关的候选脚本
步骤2：使用InstructGPT找到具有目标相似度分数的过滤脚本
步骤3：输出具有相应脚本的具体目标

图片描述：

该图片展示了通过InstructGPT生成和筛选脚本的方法。首先，通过基于上下文学习生成与InstructGPT相关的候选脚本（步骤1）。然后，使用InstructGPT找到具有目标相似度分数的过滤脚本（步骤2）。最后，输出具有相应脚本的具体目标（步骤3）。</sample>
    <sample id="294">该图片展示了一个名为“方法”的流程图，用于通过InstructGPT生成和筛选脚本。流程图分为三个主要步骤：

1. **步骤1**：通过InstructGPT生成候选脚本，这些脚本是根据给定的目标（如“制作拿铁咖啡”）生成的。这一步骤使用了上下文学习技术，意味着脚本是基于特定目标和背景生成的。

2. **步骤2**：找到与目标最相似的脚本。这一步骤涉及计算每个生成脚本与目标之间的相似度分数。例如，脚本1、2和3分别与目标的相似度分数为0.3、0.5和0.7。根据相似度分数，脚本1被标记为错误，因为它与目标的相关性最低。

3. **步骤3**：从所有脚本中筛选出最符合目标的脚本。在这个例子中，脚本3的相似度分数最高（0.7），因此被筛选出来作为最合适的脚本。

最终输出是与目标相关的具体脚本，即“制作拿铁咖啡”的脚本，其中包含了制作拿铁咖啡所需的具体步骤，如“加入咖啡因”和“加入可可粉”。

这个流程图展示了如何利用AI技术（如InstructGPT）来生成和优化脚本，以实现特定目标。它强调了在生成脚本后进行筛选的重要性，以确保最终选择的脚本是最适合目标的。</sample>
    <sample id="295">方法

步骤1：通过In-context learning生成候选脚本
步骤2：使用InstructGPT找到与目标相似度得分最高的脚本
步骤3：根据相似度得分筛选脚本
输出：具有相应目标的特定脚本</sample>
    <sample id="296">我们的方法显著提高了规划质量

TS（11B）、Flan-T5（11B）、GPT-3（12B）、Codex（175B）、InstructGPT（475B）和我们的方法之间的准确率比较

“使用我们的方法，InstructGPT可以生成显著更高质量的规划，超出其他模型的范围。”</sample>
    <sample id="297">Script Distillation from LLMs

Motivation
为了使小模型具备约束语言规划能力，从而实现更小、更高效的目标。
方法
遵循符号知识蒸馏的想法
从基于LLM的大型语料库中生成55,000个符合约束的脚本
使用“约束集”方法进行验证和测试集
输入：抽象
步骤1：使用InstructGPT通过内在学习生成特定目标的脚本
步骤2：使用InstructGPT通过上下文学习筛选候选脚本
步骤3：找到与InstructGPT相似性得分最高的脚本
输出：具有具体目标的特定规划方案</sample>
    <sample id="298">Script Distillation from LLMs

Motivation
为了使LLM具有更小模型的约束语言规划能力。
方法
遵循符号知识蒸馏的想法。
生成了55,000个根据LLM基于约束的方法生成的脚本，约束来自约束脚本数据集。
结果
通过InstructGPT进行验证和测试集注释。

输入：一个摘要
步骤1：使用InstructGPT通过上下文学习生成特定目标的脚本。
步骤2：使用InstructGPT通过上下文学习过滤掉候选脚本。
步骤3：找到与InstructGPT相似性得分最高的脚本。
输出：具有特定目标和相应规划的脚本。</sample>
    <sample id="299">Script Distillation from LLMs

Motivation
为了使LLMs具有更小模型的约束语言能力，从而实现更好的性能和效率。

Method
遵循符号知识蒸馏的想法。
生成了5.5万条符合LLMs基于约束方法的脚本，使用了约束脚本数据集。
通过标注验证和测试集进行评估。

Step 1: Generate abstracts with InstructGPT via in-context learning
使用InstructGPT通过上下文学习生成摘要。

Step 2: Over-generate candidate scripts with InstructGPT via context learning
使用InstructGPT通过上下文学习过度生成候选脚本。

Step 3: Find the best scripts with InstructGPT via similarity score
使用InstructGPT通过相似性评分找到最佳脚本。

Output: Specific goals with corresponding plans</sample>
    <sample id="300">Script Distillation from LLMs

Motivation
为了使约束语言模型具有较小模型的规划能力。
方法
遵循符号知识蒸馏的想法。
生成了55,000个脚本，基于LLMs，使用约束方法 =&gt; 约束数据集。
通过注释验证和测试集进行评估。

输入：抽象
步骤1：使用InstructGPT通过内在学习生成特定目标的脚本。
步骤2：使用InstructGPT通过上下文学习过滤掉候选脚本。
步骤3：找到与约束数据集相似度最高的脚本。
输出：具有具体目标和相应规划的特定脚本。</sample>
    <sample id="301">Script Distillation from LLMs

Motivation
为了使约束语言模型（LLMs）具有较小模型的规划能力。方法遵循符号知识蒸馏的想法。生成了55,000个符合约束的脚本，这些脚本基于LLMs，使用了一种称为“约束脚本数据集”的方法。结果通过注释验证和测试集进行了验证。

输入：一个摘要
步骤1：使用InstructGPT进行微调学习，生成特定目标的脚本。
步骤2：使用InstructGPT在语境中进行微调学习，生成候选脚本。
步骤3：使用InstructGPT通过相似性评分找到最相关的脚本。
输出：具有具体目标和相应规划的特定脚本。</sample>
    <sample id="302">约束分析

在幻灯片中，有一个名为“约束分析”的标题。它包含一个饼图，展示了约束的分布情况。约束分为以下几类：

- 日期：17.87%
- 设备：46.89%
- 财务：36.89%
- 方法：15.45%
- 与：15.45%
- 使用：15.45%

饼图显示了约束的高异质性和 pluralism 在生成的具体目标中的体现。

在幻灯片的右侧，有一个视频窗口显示了一个穿着绿色衬衫的人，背景是现代办公室环境。</sample>
    <sample id="303">左侧的柱状图展示了不同模型在Coscript上的准确率，右侧是关于模型性能的说明。</sample>
    <sample id="304">建立约束语言规划问题。评估LLMs在约束语言规划中的能力，并开发一种“先生成后过滤”的方法。使用LLMs生成高质量的脚本数据集（CoScript）用于约束语言规划。限制和未来工作：- 提出的提高LLMs的方法是一种后 hoc 重新排名方法。- CoScript 只继承自一个抽象脚本，有一个约束。- CoScript 数据集可以成为研究语言规划的宝贵资源，具有更复杂和多样的目标和约束。</sample>
    <sample id="305">建立受约束的语言规划问题。评估LLMs在语言规划中的能力，并开发一种“先生成再过滤”方法来处理LLMs。使用LLMs生成高质量的脚本数据集（CoScript）用于受约束的语言规划。限制和未来工作：- 提出的改进LLMs的方法是一种后 hoc 重新排名方法。- CoScript 只继承自一个抽象脚本，有一个约束。- CoScript 数据集可以成为研究语言规划的有价值的资源，具有更复杂和多样的目标和约束。</sample>
    <sample id="306">61st Annual Meeting of the Association for Computational Linguistics
Toronto, Canada
July 9-14, 2023

蒸馏大型语言模型中的脚本知识以进行约束语言规划

李舒、张洁、陈静、符子、符越、Ge、索哈布·沙欣、查尔斯·班克尔、扬花·肖、 degeng·阳

联系邮箱：syuan21@m.fudan.edu.cn
会议网站：https://github.com/siyuanyuan/coscript</sample>
    <sample id="307">根据幻灯片中呈现的实验结果，PaLM 的流畅度可以与当前最佳系统（SOTA）相提并论。</sample>
    <sample id="308">根据幻灯片中列出的要点，水印方法的重要属性包括：1. 适用性：适用于EaaS。2. 实用性：应该不会降低提供的嵌入的实用性。3. 隐蔽性：应该对攻击者来说是隐蔽的。4. 可转移性：水印需要能够转移到攻击者的服务上。</sample>
    <sample id="309">根据幻灯片中提供的信息，TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="310">从数据集中抽取了100个实例用于重新注释。</sample>
    <sample id="311">用于衡量良性和后门数据集之间差异的距离度量是余弦距离和KS统计量。这些度量在机器学习中用来评估两个概率分布之间的相似性或差异性，对于检测模型中的异常行为（如后门）至关重要。余弦距离通过计算两个向量之间的角度来衡量它们的方向，而KS统计量（Kolmogorov-Smirnov统计量）衡量累积分布函数（CDFs）之间的最大差异，这有助于识别数据集中是否存在显著的模式或偏移。</sample>
    <sample id="312">根据幻灯片中呈现的表格，将基于编码器的多语言模型用于这项任务的方法是通过使用mBERT和mT5等预训练模型。这些模型在多种语言上进行了训练，可以处理不同语言的输入。在表格中，我们看到mBERT和mT5在各种任务上的性能指标，表明它们在多语言设置下的表现。</sample>
    <sample id="344">作者通过在一般文本语料库中计算单词频率来确定中等频率的单词。</sample>
    <sample id="345">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?
李树恒，Alan Ritter
互动计算学院
佐治亚理工学院</sample>
    <sample id="346">命名实体识别与泛化</sample>
    <sample id="347">实体识别与泛化
* 模型已经使用CoNLL-2003开发了近20年的NER</sample>
    <sample id="348">实体识别与泛化

* 模型已经使用CoNLL-2003近20年开发命名实体识别（NER）
* 这些模型能否泛化到现代数据？
* 为了获得良好的泛化能力需要什么？</sample>
    <sample id="349">命名实体识别和泛化

* 模型已经使用CoNLL-2003近20年开发NER
* 这些模型能否适应现代数据？
* 为了实现良好的泛化，需要什么？

图片底部包含一个圆形头像照片，显示一个人戴着黑色帽子。</sample>
    <sample id="350">ConLL++数据集

| 词 | 标注 |
| --- | --- |
| 大使 | O |
| 联合国 | O |
| 国家 | O |
|琳达|O|
|托马斯·格林feld|I-PER|

图片描述：
- 图片左侧有一个圆形头像，显示了一个戴着黑色帽子的人。
- 图片右侧有一张表格，标题为“ConLL++数据集”。
- 表格包含两列：第一列是单词列表，第二列是标注。
- 单词列表包括：“大使”、“联合国”、“国家”、“琳达”和“托马斯·格林feld”。
- 标注列表包括：“O”、“O”、“O”、“O”和“I-PER”。
- 图片右下角有Georgia Tech的标志。</sample>
    <sample id="351">ConLL+数据集 • 从2020年收集Reuters新闻并使用ConLL-2003注释指南注释 • 在ConLL-2003上微调20多个模型 AMBASSADOR O UNITED O NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="352">CoNLL+数据集 • 从2020年收集Reuters新闻并使用CoNLL-2003注释指南进行注释 • 在CoNLL-2003上微调了20多个模型 • 使用CoNLL-2003测试集和CoNLL+进行评估 • 计算百分比ΔF1以评估泛化能力 代表: 马丁·索尔加德·奥尔松（Martin Solgaard Orholm）</sample>
    <sample id="353">对于良好的泛化需要什么？</sample>
    <sample id="354">好的泛化需要什么？* 模型架构* 转换器模型泛化更好图表显示了模型的F1分数与它们在不同任务上的性能之间的关系。图表显示，随着F1分数的增加，性能也在提高。图表还显示了各种模型，包括BERT、RoBERTa和Transformer-XL，以及它们的性能。这些模型的性能各不相同，但它们都显示出随着F1分数的增加而提高的趋势。</sample>
    <sample id="355">幻灯片讨论了大型模型在机器学习中的泛化能力。它提出了以下观点：

* 模型架构：幻灯片表明，特定的模型架构可能有助于更好的泛化。
* 转换器模型：幻灯片强调了转换器模型在泛化方面的优势。
* 模型大小：幻灯片表明，更大的模型通常具有更好的泛化能力。

幻灯片还展示了图表，显示了不同模型（如BERT Base、RoBERTa Base和ALBERT Base）的性能随参数数量增加而提高的趋势。这进一步支持了更大模型通常具有更好泛化能力的观点。</sample>
    <sample id="356">对于良好泛化的需要是什么？模型架构·Transformer模型泛化得更好·更大的模型泛化得更好·微调示例数量·更多的示例导致更好的泛化·ReBERT·Flair</sample>
    <sample id="357">这张幻灯片的中文表达为：'什么会导致性能下降？'</sample>
    <sample id="358">什么会导致性能下降？</sample>
    <sample id="359">什么会导致性能下降？自适应过拟合？时间漂移？</sample>
    <sample id="360">幻灯片标题为“性能下降的原因？”并列出了两个要点：1.自适应过拟合？2.时间漂移？幻灯片的中心部分是一个图表，图表的x轴标记为“CIFAR-2000 F1分数”，y轴标记为“准确度”。图表显示了不同方法或模型的性能，用不同颜色的线表示。图表中包含多个数据点，每个数据点代表一个特定的模型或方法。图表的右下角有一个注释框，详细说明了图表的某些方面。注释框中的文本包括以下内容：-“使用CIFAR-2000数据集训练的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-2000数据集上测试的模型。”-“在CIFAR-20</sample>
    <sample id="361">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="362">幻灯片标题为“性能下降的原因？”并列出了三个要点：

* 自适应过拟合？
* 未观察到的回报
* 时序漂移？

在幻灯片的右侧，有一个图表。图表的x轴标记为“CIFAR100-200 P1得分”，y轴标记为“CIFAR100-200 F1分数”。图表显示了多个数据点，每个数据点都用不同颜色表示。图表中有一条红色的线，表示数据点的趋势或模式。

在幻灯片的左下角，有一个圆形的图片，显示了一个戴着黑色帽子的人。图片旁边有文字写着“Wang”和“Georgia Tech”。

幻灯片的背景是白色的，幻灯片的右下角有Georgia Tech的标志。</sample>
    <sample id="363">幻灯片标题为“性能下降的原因？”并分为两部分。左侧是一个表格，其中包含三个要点：1.自适应过拟合？2.未观察到回报3.未观察到时间漂移？右侧是一个折线图，显示了两个变量之间的关系。x轴标记为“CIFAR-100 F1得分”，y轴标记为“CIFAR-100 F1得分”。在图表中绘制了两条折线，一条代表“自适应过拟合”（用红色表示），另一条代表“未观察到时间漂移”（用蓝色表示）。图表还包含一个注释区域，详细说明了“自适应过拟合”和“未观察到时间漂移”的具体表现。</sample>
    <sample id="364">什么会导致性能下降？自适应过拟合？未观察回报。时间漂移？名称CoNLL-2003 CoNLL++（%）年份Flair 92.46% 88.46% 85.00% 84.67% Flair 90.91% 88.46% 85.00% 84.67% Pooled Flair 93.15% 88.88% 85.00% 84.67% Pooled Flair 92.36% 87.00% 84.67% 84.67% ELMo 92.11% 90.76% -1.43%</sample>
    <sample id="365">幻灯片标题为“性能下降的原因？”并具有深色主题，背景为白色。幻灯片分为两部分：左侧的文本和右侧的表格及图表。

左侧的文本包括以下要点：
- 自适应过拟合？
- 未观察到的回报
- 时序漂移？
- 随着时间间隔增大，性能下降

幻灯片底部有一个圆形照片，显示一个戴着黑色帽子的人。照片下方有“Georgia Tech”标志。

表格位于幻灯片的中间部分，标题为“名称”，下面是几列数据。表格的列标题从左到右依次为“年份”、“名”、“CoNLL-2003”、“CoNLL++”和“AF（%）”。表格中的数据如下：

| 年份 | 名 | CoNLL-2003 | CoNLL++ | AF（%） |
|------|----|-----------|---------|--------|
| 2002 | Flair | 92.46%    | 88.46%  | -2.69  |
| 2003 | Flair | 90.91%    | 88.46%  | -2.69  |
| 2004 | Flair | 93.15%    | 88.46%  | -4.65  |
| 2005 | Flair | 92.36%    | 87.08%  | -5.72  |
| 2006 | ELMo | 92.11%    | 90.76%  | -1.43  |

图表位于幻灯片的右侧，标题为“年份 vs. AF（%）”。图表显示了一条红色对角线，表示数据点之间的趋势线。图表中的数据点如下：

- 2002: (2002, 92.46%)
- 2003: (2003, 90.91%)
- 2004: (2004, 93.15%)
- 2005: (2005, 92.36%)
- 2006: (2006, 92.11%)

图表表明，随着年份的增加，AF（%）值略有波动，但总体趋势是保持稳定。</sample>
    <sample id="366">为了实现良好的泛化，我们需要：- 更好的模型架构- 更大的模型规模- 更多的微调示例该幻灯片展示了自然语言处理（NLP）领域中不同模型架构的性能随时间的变化。它比较了Stanford NLP、BERT和CoNLL 2003等模型在各种任务上的表现。Stanford NLP的性能在2004年左右开始显著提高，并在随后几年中保持稳定增长。BERT在2018年左右引入后，其性能迅速提升，并在随后几年中继续改进。CoNLL 2003在2004年左右达到顶峰，然后逐渐下降。总体而言，这张图表表明，随着技术的发展和数据的增加，NLP模型的性能不断提高。</sample>
    <sample id="367">为了实现良好的泛化，我们需要：- 更大的模型架构- 更多的训练示例- 更多微调示例性能下降是由以下原因引起的：- 适应性过拟合- 临时漂移幻灯片展示了自2004年以来自然语言处理（NLP）领域的发展。它强调了为了实现更好的泛化而需要改进的关键点。该幻灯片还展示了过去十年中NLP性能的显著提升，特别是BERT和LUKE等大型预训练模型的引入。这些模型通过在大规模文本数据上进行预训练，然后在特定任务上进行微调，显著提高了性能。</sample>
    <sample id="368">为了实现良好的泛化，我们需要：

* 更大的模型尺寸
* 更多的微调示例

性能下降的原因：

* 不适应过拟合
* ConLL-2003标注器是否有效？

该幻灯片展示了自2004年以来在自然语言处理（NLP）领域取得的进展。它强调了为了实现更好的泛化，需要更大的模型尺寸和更多的微调示例。此外，它提到了性能下降的可能原因，包括不适应过拟合和ConLL-2003标注器的有效性。

该幻灯片还展示了不同NLP任务的性能随时间的变化。例如，BERT和LUKE等模型的性能显著提高，表明这些方法在处理自然语言任务时具有更好的泛化能力。该幻灯片还提到了“性能下降”的概念，这可能是指在某些任务或数据集上性能的降低。</sample>
    <sample id="369">为了实现良好的泛化，我们需要：- 更大的模型尺寸- 更多的微调示例- 保持性能稳定- 避免自适应过拟合- 还有ConLL-2003标注器是否仍然有效？是的！</sample>
    <sample id="370">图片中的英文内容表达为中文如下：

- 论文：https://arxiv.org/abs/2212.09747
- 数据集：https://github.com/ShuhenL/acl2023-conllpp
- 联系方式：sliu775@gatech.edu

此外，图片右下角的标志显示“Georgia Tech”，表示与佐治亚理工学院有关。</sample>
    <sample id="397">图片中没有提供关于语音片段大小的信息。该幻灯片的焦点似乎是介绍一个解决方案，而没有具体提到音频文件的大小或格式。要了解使用的语音片段大小，可能需要参考演示文稿的其他部分或额外的上下文信息。</sample>
    <sample id="398">根据所给英文内容，需要特定于实体的知识包括Servin和Kea的职业（Servin是一名法官，Kea是一名面包师），以及Servin在一天结束时的活动（在公园里放松）。这些信息对于理解句子的背景和含义至关重要。</sample>
    <sample id="399">根据幻灯片中提供的英文内容，示例质量被认为比与源句子的相似度更为重要。</sample>
    <sample id="400">在扩展实验中，论文侧重于以下语言模型：BERT-base、RoBERTa-base、distilRoBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base、ALBERT-base、ALBERT-large、BERT-large、RoBERTa-large、distilBERTa-base</sample>
    <sample id="401">根据幻灯片中提供的信息，该模型使用了多个层的注意力分数。这可以从提到的“将多个层的注意力分数结合”来推断，表明了对模型的多层处理能力的强调。</sample>
    <sample id="402">直接推断的示例包括“我第一个”和“不能记住名字”。</sample>
    <sample id="403">根据幻灯片显示，这篇论文的作者所属机构是“复旦大学”和“Brain Technologies Inc.”。这些机构的名称位于作者姓名下方，表明他们与这些实体有关联。</sample>
    <sample id="404">根据图片显示，这篇论文有五位作者。</sample>
    <sample id="405">根据幻灯片中提供的信息，是的，在语义解析之前使用机器翻译模型翻译自然语言查询作为基线。这可以通过流程图中所示的“Translate API”步骤来证明，该步骤位于“English Model”之后和“SQL”之前，表明在将自然语言查询翻译成SQL之前，它们首先被翻译成英语。</sample>
    <sample id="406">根据幻灯片中提供的英文内容，作者给出的“显性群体”(marked group) 的示例是“一个女人”。</sample>
    <sample id="407">根据图表显示，泛化能力较差的模型架构包括“CNN”、“LSTM”和“RNN”。这些模型在图表上位于曲线下方，表明它们在CIFAR-100数据集上的性能不佳。相比之下，“Transformer”模型位于图表的上方，表明它们具有更好的泛化能力。</sample>
    <sample id="408">测试数据集的名称是“CIFAR-100”，如幻灯片左下角的注释中所示。</sample>
    <sample id="409">根据幻灯片上显示的英文信息，这篇论文有五位作者。这可以通过在幻灯片底部列出的作者名字和头像来确定。</sample>
    <sample id="410">根据幻灯片标题“Instruction Tuning on Multimodal Pre-trained Models”，可以推断作者采用了多种模态。术语“多模态”意味着涉及除了文本之外的其他数据类型，如图像、音频或视频。因此，答案是：(a) 多种模态</sample>
    <sample id="439">图片中没有直接提到NLU（自然语言理解）研究不足的领域。它主要关注了关于新任总统的信息处理，包括预训练知识和推理时的推断。要回答这个问题，需要参考与NLU相关的更广泛的研究文献或综述。</sample>
    <sample id="440">演讲者的名字是“Zhiyang Xu”，“Ying Shen”和“Lifu Huang”。这些名字位于幻灯片的作者部分，紧随其后的是他们的头像。</sample>
    <sample id="441">根据幻灯片中提到的流程，Coscript 经过了质量检查。这可以从“验证和测试集”这一短语中看出，表明在最终确定或公开之前，Coscript 已经经过了验证和测试。</sample>
    <sample id="442">根据幻灯片中提供的信息，对于依赖上下文的翻译，现有的资源的局限性包括：它们只支持有限的 discourse 现象和语言。这意味着现有的方法可能无法处理广泛的上下文相关翻译挑战，特别是在多样化的语言和表达方式中。</sample>
    <sample id="443">解决AltEntities语料库中间接指代表达式的选择实体</sample>
    <sample id="444">解决AltEntities语料库中实体选择的间接引用表达式</sample>
    <sample id="445">### 翻译后的中文内容

**间接指示表达**

* **God:** 理解用户在做出选择时的语言
	+ 示例: “容易的”或“我感觉到了吗？”
* **直接引用:** 
	+ 无法记住名字
	+ 发音难以区分
	+ 需要指定偏好
* **间接引用:** 
	+ 示例: “那个 newer 的。”
	+ “那首不那么 energetic 的歌。”

**来源:** 《解决间接指示表达: 实体选择 (Mnli Corpus)》

---

这个翻译保持了原始英文内容的结构和信息，确保了清晰和准确的传达。</sample>
    <sample id="446">### 翻译

#### 间接指示表达

- **目标**：理解用户在做出选择时的语言
- **备选问题**：你是指容易让我做还是让我感觉？

- **直接引用**：
  - 不能记住名字
  - 无法在自然流畅的对话中区分这些称谓
  - 想要指明一个偏好

- **间接引用**：
  - 最新的那个
  - 那首不那么 energetic 的歌

#### 解决间接指示表达（实体选择）研究

---

### 详细翻译

#### 间接指示表达

- **目标**：理解用户在做出选择时的语言
- **备选问题**：你是指容易让我做还是让我感觉？

- **直接引用**：
  - 不能记住名字
  - 无法在自然流畅的对话中区分这些称谓
  - 想要指明一个偏好

- **间接引用**：
  - 最新的那个
  - 那首不那么 energetic 的歌

#### 解决间接指示表达（实体选择）研究</sample>
    <sample id="447">### 图片描述
该图片是一张演示幻灯片，标题为“间接指示表达”。幻灯片分为两部分，左侧是文字内容，右侧有一个圆形头像照片。文字内容如下：

- **目标**: 理解用户在做出选择时的语言。
- **备选问题**: "Did you mean easy on me or I gotta feeling?"
- **直接引用**: "easy on me," "the first one"
- **间接引用**: 无法记住名称、难以区分的 pronunciation、想要指定偏好
- **间接引用示例**:
  - "The newer one."
  - "The song that's not energetic."

幻灯片底部有Google Research的标志。

### 详细翻译
#### 幻灯片标题: 间接指示表达

- **目标**: 理解用户在做出选择时的语言。
- **备选问题**: "Did you mean easy on me or I gotta feeling?"
- **直接引用**: "easy on me," "the first one"
- **间接引用**: 无法记住名称、难以区分的 pronunciation、想要指定偏好
- **间接引用示例**:
  - "The newer one."
  - "The song that's not energetic."

幻灯片底部有Google Research的标志。

### 详细解释
1. **目标**: 本幻灯片的主要目标是帮助理解用户在做出选择时的语言。这涉及到识别用户在表达时使用的不同方式，以确保准确理解他们的意图。

2. **备选问题**: 提供了一个具体的例子，即用户可能在表达时使用不同的短语或词语，如“容易对付我”和“我必须感觉”。这展示了用户在表达时可能存在的模糊性。

3. **直接引用**: 直接引用指的是用户明确提到的具体选项或术语，如“容易对付我”和“第一个”。这些是用户在表达时最直接、最明确的部分。

4. **间接引用**: 间接引用指的是用户在表达时可能使用的模糊或不直接的术语。例如，用户可能提到“那个新的一个”或“那个不那么有活力的歌曲”，这些术语可能需要额外的上下文来确定具体所指。

5. **示例**: 提供了两个具体的间接引用示例：
   - “那个新的一个。” 这个例子说明了用户可能提到一个新的选项，但没有明确指出具体是什么。
   - “那个不那么有活力的歌曲。” 这个例子说明了用户可能提到一首特定的歌曲，但没有明确指出具体是哪一首。

通过理解这些概念和示例，可以更好地处理用户在表达时的模糊性和间接性，从而提高语言理解和处理能力。</sample>
    <sample id="448">### 图片描述

幻灯片标题为“间接指示表达”，来自Google Research。幻灯片分为两部分：

1. **目标**：理解用户在做出选择时的语言。
   - **示例问题**：你是指容易让我还是让我感觉好？

2. **直接引用**：
   - 无法记住名字
   - 发音难以区分
   - 想要指定偏好

3. **间接引用**：
   - 示例：新的那个…那首不那么有活力的歌。

### 翻译

#### 幻灯片标题
幻灯片标题为“间接指示表达”，来自Google Research。

#### 目标
- **理解用户在做出选择时的语言**：
  - **示例问题**：你是指容易让我还是让我感觉好？

#### 直接引用
- 无法记住名字
- 发音难以区分
- 想要指定偏好

#### 间接引用
- 示例：新的那个…那首不那么有活力的歌。

### 详细翻译

#### 幻灯片标题
幻灯片标题为“间接指示表达”，来自Google Research。

#### 目标
- **理解用户在做出选择时的语言**：
  - **示例问题**：你是指容易让我还是让我感觉好？

#### 直接引用
- 无法记住名字
- 发音难以区分
- 想要指定偏好

#### 间接引用
- 示例：新的那个…那首不那么有活力的歌。

### 详细解释

1. **目标**：理解用户在做出选择时的语言。
   - **示例问题**：你是指容易让我还是让我感觉好？
     - 这个示例问题展示了用户如何通过间接语言表达他们的偏好。例如，用户可能会说“容易让我”或“让我感觉好”，而不是直接说“我更喜欢容易的”或“我更喜欢感觉好的”。

2. **直接引用**：
   - 无法记住名字
   - 发音难以区分
   - 想要指定偏好
     - 这些是用户可能使用的直接引用方式。例如，用户可能会说“我不能记住那个名字”、“这些发音很难区分”或“我想要这个偏好”。

3. **间接引用**：
   - 示例：新的那个…那首不那么有活力的歌。
     - 这个示例展示了用户如何使用间接语言来表达他们的偏好。例如，用户可能会说“新的那个…那首不那么有活力的歌”，而不是直接说“我更喜欢新的那首不那么有活力的歌”。

### 总结

幻灯片讨论了用户在做出选择时如何使用 indirect language（间接语言）和 direct reference（直接引用）。通过理解这些表达方式，可以更好地帮助系统或算法识别和处理用户的需求和偏好。</sample>
    <sample id="449">间接指示表达

目标：理解用户在做选择时的语言

* 反问问题
	+ '你是说容易让我还是...感觉吗？'
* 直接引用
* 间接引用
	+ 无法记住名字
	+ 难以区分 pronunciation
	+ 想要指定偏好

* 间接引用
	+ ' newer one. ' 
	+ '那首不那么有活力的歌。'</sample>
    <sample id="450">数据集收集

重要问题：

* 重要问题
* 重要问题
* 重要问题
* 重要问题

我们使用 crowdsourcing 收集大量数据

三个领域：</sample>
    <sample id="451">数据集收集

• 重要问题
- 重要问题
- 比较大型语言模型的实体理解能力
- 大规模公开数据集 unavailable
- 我们使用 crowdsourcing 收集大规模数据集
- 三个领域：

该幻灯片展示了Google Research关于数据集收集的重要性和方法的概述。它首先强调了在构建和评估自然语言处理系统时面临的关键挑战，如比较大型语言模型的实体理解能力。接着，它提到了大规模公开数据集的缺乏，这是研究和开发中常见的障碍。为了解决这个问题，该幻灯片表明他们采用了 crowdsourcing 方法来收集大规模数据集。最后，它展示了三个领域的图标，可能代表数据集收集涉及的不同领域或主题。</sample>
    <sample id="452">### 问题翻译
图片中的英文翻译成中文如下：

**标题：**
数据集收集方法

**要点：**
- 方法强调使用卡通完成任务的**不正式性**。
- 设置对话背景（从几组手动提示中选择）
- 可能的问题
- 表达指的是其中一个实体

**注释：**
- 图片右下角有一个黄色箭头，指向一个注释框，里面写着“由标注员填写”。
- 图片右下角有一个圆形头像，里面是一个人的照片。

**描述：**
该幻灯片展示了Google Research关于数据集收集方法的研究。该方法强调使用卡通完成任务的不正式性。具体步骤包括：
1. **设置对话背景**：从几组手动提示中选择。
2. **可能的问题**：为卡通角色设计问题。
3. **表达指的是其中一个实体**：确保对话中提到的角色是明确的。

**注释**部分说明了黄色箭头指向的注释框内容，即“由标注员填写”。此外，幻灯片右下角有一个圆形头像，显示了一个参与讨论或演示的人的照片。</sample>
    <sample id="453">数据集收集方法

该方法强调使用卡通完成任务的非正式性

记住我们昨天一起唱的那首歌吗？

你昨天感觉怎么样？

我有一个感觉。</sample>
    <sample id="454">### Dataset Collection Methodology

- **Methodology emphasizes informativeness using a cartoon completion task**

#### 图片描述：
1. **First Panel**:
   - **Text**: "Remember that we were talking about yesterday?"
   - **Illustration**: A cartoon character with a speech bubble.
   - **Caption**: "The dialog context (chosen from a few manual prompts per domain)"

2. **Second Panel**:
   - **Text**: "Do you mean 'I was happy'?"
   - **Illustration**: Another cartoon character with a speech bubble.
   - **Caption**: "The alternative question"

3. **Third Panel**:
   - **Text**: "I gotta feeling"
   - **Illustration**: A third cartoon character with a speech bubble.
   - **Caption**: "Expression referring to one of the entities"

4. **Fourth Panel**:
   - **Text**: "Filled by the annotator"
   - **Illustration**: An arrow pointing to the text in the third panel.
   - **Caption**: "Filled by the annotator"

5. **Fifth Panel**:
   - **Text**: "Resolving Indirect Expressions (Entity Selection) (Mnli Corpus)"
   - **Illustration**: A cartoon character with a speech bubble.

6. **Sixth Panel**:
   - **Text**: "Google Research"
   - **Illustration**: The Google Research logo.

### 翻译
#### Dataset Collection Methodology

- **本方法论强调使用卡通完成任务来强调信息性**

#### 图片描述：
1. **第一幅图**:
   - **文字**: "记得我们昨天谈论的事情吗？"
   - **插图**: 一个卡通角色带有对话气泡。
   - **说明**: "对话背景（从每个领域选择少数手动提示）"

2. **第二幅图**:
   - **文字**: "你是说“我很开心”吗？"
   - **插图**: 另一个卡通角色带有对话气泡。
   - **说明**: "替代问题"

3. **第三幅图**:
   - **文字**: "我有种感觉"
   - **插图**: 第三个卡通角色带有对话气泡。
   - **说明**: "表达指的是其中一个实体"

4. **第四幅图**:
   - **文字**: "由标注员填写"
   - **插图**: 指向第三幅图中的文字的箭头。
   - **说明**: "由标注员填写"

5. **第五幅图**:
   - **文字**: "解决间接表达（实体选择）（Mnli语料库）"
   - **插图**: 一个卡通角色带有对话气泡。

6. **第六幅图**:
   - **文字**: "Google Research"
   - **插图**: Google Research标志。</sample>
    <sample id="455">### 问题翻译
图片中的英文翻译成中文如下：

**标题：**
数据集收集方法论

**要点：**
- 方法论强调使用卡通完成任务的**不正式性**。
- 使用对话框来**设定对话背景**（从每个领域选择少数几个手动提示）。
- 提供**替代问题**。
- 表达指的是其中一个实体。

**注释：**
- 图片右下角的黄色箭头指向一个注释，注释说明“由标注员填写”。
- 图片右下角有一个圆形照片，显示一个戴眼镜的人。

**来源：**
Google Research

**页码：**
4</sample>
    <sample id="456">数据集收集方法

该方法强调使用卡通完成任务的**不正式**性。

- 方法强调使用卡通完成任务的**不正式**性。
- 设置对话背景（从几篇手动提示中选择）。
- 问题的替代方案。
- 指向卡通中对话气泡的黄色箭头，表示填充内容由标注员提供。
- 表达指的是其中一个实体。</sample>
    <sample id="457">### 问题翻译
图片中的英文内容描述了一种数据集收集方法，具体是通过卡通完成任务来强调信息形式。图片展示了对话场景，其中涉及实体（如“昨天”、“我”和“感觉”）的选择和表达。图片右下角的注释提到，这些实体由标注员填充。

### 翻译后的中文内容
#### 数据集收集方法
- 方法强调使用卡通完成任务来**信息形式化**
- 设置对话背景（从几组手动提示中选择）
- 可视化的示例展示了对话场景，其中涉及实体的选择和表达
- 图片右下角的注释说明，这些实体由标注员填充

### 详细翻译
1. **数据集收集方法**：
   - **方法强调使用卡通完成任务来信息形式化**：这表明数据集收集过程注重通过卡通形式来捕捉和分析信息，确保信息的准确性和一致性。
   - **设置对话背景（从几组手动提示中选择）**：这意味着在数据集中，对话背景是通过预定义的几组手动提示来设定的，以确保对话的一致性和可控性。
   - **可视化的示例展示了对话场景，其中涉及实体的选择和表达**：这说明了数据集中包含具体的对话示例，展示了如何选择和表达实体，以确保数据的准确性和实用性。
   - **图片右下角的注释说明，这些实体由标注员填充**：这表明数据集中涉及的实体（如时间、人物等）是由标注员手动填充的，以确保数据的质量和准确性。

2. **示例翻译**：
   - 原文：`Remember that song we were talking about yesterday?`
     - 翻译：`昨天我们谈论过的那首歌记得吗？`
   - 原文：`Do you mean the one where I gotta feel?`
     - 翻译：`你是说那首我必须感受的歌曲吗？`
   - 原文：`The alternative question`
     - 翻译：`这个问题的另一种说法`
   - 原文：`Expression referring to one of the entities`
     - 翻译：`指代其中一个实体的表达`

3. **注释翻译**：
   - 原文：`Filled by the annotator`
     - 翻译：`由标注员填充`

通过上述翻译，我们可以更好地理解数据集收集方法的细节和流程，以及对话示例中涉及的实体选择和表达。</sample>
    <sample id="458">生成等价问题 = 采样实体对

图片描述了一个演示文稿幻灯片，标题为“生成等价问题 = 采样实体对”。幻灯片解释了如何通过采样实体对来生成等价问题。它提供了以下示例：

1. **相似 infoboxes**（通常相同）：
   - 示例：在Wikipedia上具有相同类型/艺术家的物品。
   - 问题示例：你指的是《镜中人》还是《镜中人》？
   - 问题示例：你指的是《你想》还是《快乐无处不在》？

2. **相似描述**：
   - 示例：具有相似描述的物品。
   - 问题示例：你指的是《你想》还是《快乐无处不在》？
   - 问题示例：你指的是《记忆》还是《返回》？

3. **随机选择**：
   - 示例：从随机选择中采样。
   - 问题示例：你指的是《你本可以成为谁》还是《我是》？

幻灯片还提到了Google Research的标志，并且有一个箭头指向一个输入框，输入框中写着“你指的是A还是B？”此外，幻灯片底部有一个圆形头像，显示了一个正在思考或解释概念的人。

翻译后的中文内容如下：

生成等价问题 = 采样实体对

这个演示文稿幻灯片解释了如何通过采样实体对来生成等价问题。它提供了以下示例：

1. **相似 infoboxes**（通常相同）：
   - 示例：在Wikipedia上具有相同类型/艺术家的物品。
   - 问题示例：你指的是《镜中人》还是《镜中人》？
   - 问题示例：你指的是《你想》还是《快乐无处不在》？

2. **相似描述**：
   - 示例：具有相似描述的物品。
   - 问题示例：你指的是《你想》还是《快乐无处不在》？
   - 问题示例：你指的是《记忆》还是《返回》？

3. **随机选择**：
   - 示例：从随机选择中采样。
   - 问题示例：你指的是《你本可以成为谁》还是《我是》？

幻灯片还提到了Google Research的标志，并且有一个箭头指向一个输入框，输入框中写着“你指的是A还是B？”此外，幻灯片底部有一个圆形头像，显示了一个正在思考或解释概念的人。</sample>
    <sample id="459">生成替代问题 = 抽样实体对

图片中有一个表格，分为两列。左侧的列标题为“更相似（通常）”，右侧的列标题为“统一”。表格中包含以下内容：

- 左侧的“更相似（通常）”列：
  - 项目与Wikipedia上的相似信息框（同一类型/艺术家）
  - 项目具有相似描述
  - 项目具有相似标题
  - 项目具有随机选择的标题

- 右侧的“统一”列：
  - 问题示例：你指的是《镜中人》还是《快乐无处不在》？
  - 问题示例：你指的是《记忆》还是《归途·幻影小说》？
  - 问题示例：你指的是《你是谁》还是《我是谁》？

表格的底部有一段文字，内容如下：

“解决隐式表达（实体选择）问题（Mnli数据集）”

这张图片似乎是在讨论如何通过比较不同实体对来生成替代问题，以解决隐式表达问题。</sample>
    <sample id="460">生成替代问题 = 抽样实体对

Do you mean A or B?（你是指A还是B？）

- Items with similar infoboxes on Wikipedia (与维基百科中具有相似信息框的物品)
- Items with similar descriptions on Wikipedia（与维基百科中具有相似描述的物品）
- Do you mean Thinking of You or Happy Anywhere?（你是说《Thinking of You》还是《Happy Anywhere》？）
- Do you mean The Return (memoir) or The Return (Shatner novel)?（你是说《The Return》(回忆录)还是《The Return》(肖特纳小说)？）

Uniform at random:（随机均匀分布：）

- Do you mean You Could Be Mine or The Way I Am?（你是说《You Could Be Mine》还是《The Way I Am》？）</sample>
    <sample id="461">生成替代问题 = 抽样实体对

图片左侧的黄色箭头表示“更多相似（通常）”。

图片右侧的圆形照片显示了一位穿着深色衬衫、戴着眼镜、留着短发的男士。</sample>
    <sample id="462">生成替代问题 = 抽样实体对

Do you mean A or B?
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
- Items with similar descriptions on Wikipedia
- Items with similar titles

Do you mean Thinking of You or Happy Anywhere?
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
- Items with similar descriptions on Wikipedia
- Items with similar titles

Do you mean The Return (memoir) or The Return (Shatner novel)?
- Items with similar infoboxes on Wikipedia (same genre and/or artist)
- Items with similar descriptions on Wikipedia
- Items with similar titles

Uniform at random:
- Do you mean You Could Be Mine or The Way I Am?</sample>
    <sample id="463">Google搜索链接歌曲

* 点击以了解关于这首歌曲的信息
* 让注释者收听每首歌至少一部分
* 阅读每首歌的资料</sample>
    <sample id="464">背景知识（音乐）

* Google 搜索链接歌曲
	+ 《Easy on Me》(由 A$AP Rocky)
	+ 《I Gotta Feeling》(由 The Black Eyed Peas)
	+ 点击这里了解关于这首歌曲的信息
	+ 点击这里了解关于这首歌曲的信息
* 我们要求标注员
	+ 听每首歌至少一部分
	+ 阅读关于每首歌的信息</sample>
    <sample id="465">Google搜索链接歌曲

* 易于我 (由阿黛尔)
* 我要感觉 (由黑眼豆豆)

点击了解有关歌曲的信息。</sample>
    <sample id="466">### 问题与答案

**问题：**
图片中的文字描述了两种蛋糕：Simnel Cake和Pandan Cake。请将这些描述翻译成中文。

**答案：**
图片中的文字描述了两种蛋糕：Simnel Cake和Pandan Cake。以下是翻译后的中文描述：

- **Simnel Cake (食谱)**

  椰子蛋糕是一种在英国、爱尔兰和其他国家广受欢迎的水果蛋糕，通常在迁徙节日期间食用，如 Lent 和 Easter。它以杏仁膏球为特色，由山羊豆属的 Pandanus amaryllifolius 叶子制成。这种蛋糕在印度尼西亚、马来西亚和荷兰等地区非常受欢迎，尤其是在印尼社区中。

  ![Simnel Cake](https://example.com/simnel_cake.jpg)

- **Pandan Cake (食谱)**

  竹叶蛋糕是一种轻盈、松软的海绵蛋糕，带有竹叶的清新味道。这种蛋糕在印尼、马来西亚和荷兰等地区广受欢迎，尤其是在印尼社区中。

  ![Pandan Cake](https://example.com/pandan_cake.jpg)

**图片描述：**

1. **Simnel Cake (食谱)**:
   - **描述**: 椰子蛋糕是一种在英国、爱尔兰和其他国家广受欢迎的水果蛋糕，通常在迁徙节日期间食用，如 Lent 和 Easter。它以杏仁膏球为特色，由山羊豆属的 Pandanus amaryllifolius 叶子制成。这种蛋糕在印度尼西亚、马来西亚和荷兰等地区非常受欢迎，尤其是在印尼社区中。
   - **图片**: 一种装饰有杏仁膏球的蛋糕，通常在节日期间食用。

2. **Pandan Cake (食谱)**:
   - **描述**: 竹叶蛋糕是一种轻盈、松软的海绵蛋糕，带有竹叶的清新味道。这种蛋糕在印尼、马来西亚和荷兰等地区广受欢迎，尤其是在印尼社区中。
   - **图片**: 一种带有竹叶清新味道的轻盈、松软的海绵蛋糕。

**附加信息**:
- **来源**: 这些信息来自 Google Research，用于研究目的。

通过将英文描述翻译成中文，我们可以更好地理解这两种蛋糕的背景和特点。Simnel Cake 以其杏仁膏球和竹叶叶子而闻名，而 Pandan Cake 则以其轻盈的口感和竹叶清新味道而受到喜爱。这两种蛋糕在东南亚地区尤其受欢迎，尤其是在印尼社区中。</sample>
    <sample id="467">**图片描述：**

该图片是一个演示幻灯片，标题为“引出表情”。幻灯片的主要内容如下：

- 指示标注员选择歌曲并描述其特点。
- 提供了两首歌曲的示例：“Easy on Me (by Adele)”和“I Gotta Feeling (by The Black Eyed Peas)”，并要求标注员选择其中一首。
- 在歌曲选择后，要求标注员从以下选项中选择最能描述歌曲的三个或五个表情：
  - “有轻松的音乐”
  - “ song that's not energetic”
  - “ song that's about a river”
  - “ newest one”
  - “ song that has no time to choose”

幻灯片右下角有一个圆形头像照片，显示了一个男人。

**翻译成中文：**

**幻灯片标题：引出表情**

**主要说明：**

- 告诉标注员应该选择哪首歌曲，并要求他们描述它。
- 提供两首歌曲的示例：“Easy on Me (由阿黛尔演唱)”和“I Gotta Feeling (由黑眼豆豆乐队演唱)”，并要求标注员选择其中一首。
- 在歌曲选择后，要求标注员从以下选项中选择最能描述歌曲的三个或五个表情：
  - “有轻松的音乐”
  - “ song that's not energetic”
  - “ song that's about a river”
  - “ newest one”
  - “ song that has no time to choose”

**幻灯片右下角有一个圆形头像照片，显示了一个男人。**</sample>
    <sample id="468">引出表达

然后告诉标注员应该选择哪个选项，并让他们描述它。

“Easy on Me”(阿黛尔)

“我感觉到了”(黑眼豆豆)

我们希望你给我们3到5个所选歌曲的描述，以供示例。

“这是一首流行音乐。”
“这首歌曲并不充满活力。”
“这首歌曲是关于一条河流的。”
“这是最新的歌曲。”
“没有时间去选择。”</sample>
    <sample id="469">AltEntities Corpus

- 三个领域中6,000个替代问题
- 42,000个间接引用表达
- 结果与T5模型（准确性）：
    - 如果LM具有与注释员相同的背景知识，则相同
    - 82%-87%的准确率，当LM具有部分重叠的背景知识时
    - 60%的准确率，当LM仅具有实体名称的访问权限时
- 我们展示了模型是跨领域的泛化能力
数据集链接：https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="470">AltEntities Corpus

- 在三个领域中提供了6,000个替代问题
- 42,000个间接引用表达

结果与T5模型（准确率）：
- 如果LM具有与注释员相同的背景知识，则相同
- 82%-87%的准确率，当LM具有部分重叠的背景知识时
- 60%的准确率，当LM仅具有实体名称的访问权限时
- 我们展示了模型是跨领域的泛化的

数据集链接：https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="471">AltEntities Corpus

- 6,000个替代问题覆盖三个领域
- 42,000个间接指代表达
- 结果与T5模型（准确率）：
    - 如果LM可以访问与注释员相同的背景知识，则相同。
    - 如果LM可以访问部分重叠的背景知识，则在82%-87%之间。
    - 如果LM（T5 XL）只能访问实体名称，则在60%左右。
- 我们展示了模型具有跨领域泛化能力。
数据集链接：https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="472">AltEntities Corpus

- 6,000个替代问题覆盖三个领域
- 42,000个间接指代表达
- 结果与T5模型（准确率）：
    - 如果LM可以访问与注释员相同的背景知识，则相同。
    - 在LM可以访问部分重叠背景知识时，82%-87%
    - 当LM（T5 XL）只能访问实体名称时，60%
- 我们展示了模型具有跨领域泛化能力。
数据集链接：https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="473">根据图表中显示的英文内容，该方法与以下现有的 SimulST 策略进行了比较：wait-k、LA、CAT 和 EDAtt。这些策略在图表的 y 轴上以不同颜色的线表示，表明它们在 x 轴上评估了不同的 AL/AL-CA 秒数。</sample>
    <sample id="474">根据幻灯片底部显示的标志和文字，这篇论文的作者分别来自以下机构：1. 蒙彼利埃大学的LIA（Laboratoire d'Informatique de l'Avignon）；2. 图卢兹第二大学的LS2N（Laboratoire des Sciences du Numérique de Nantes）；3. 埃夫雷姆·德·多内斯的CHU（Centre Hospitalier Universitaire）；4. 泽尼迪大学的Zenid。</sample>
    <sample id="475">演讲者的名字是Katharina Reinecke。</sample>
    <sample id="476">根据幻灯片显示，这篇论文有三位作者：Myra Cheng、Esin Durmus和Dan Jurafsky。</sample>
    <sample id="477">注意作为 simultaneous speech translation 的指南 Sara Papí，马泰奥·内格里，马科·图尔奇 由特伦托大学和布鲁诺·卡斯勒基金会赞助</sample>
    <sample id="478">注意作为 simultaneous speech translation 的指南 Sara Papi、Matteo Negri、Marco Turchi UNIVERSITÀ DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="479">当前SimulST模型的问题是什么？</sample>
    <sample id="480">当前SimulST模型的问题是什么？</sample>
    <sample id="481">当前SimulST模型的问题是什么？特定架构通常是在引入额外模块（例如，不同的优化目标）的情况下进行训练的。长而复杂的训练过程（例如，不同的优化目标）。训练和维护多个模型以达到不同的延迟 regimes（例如，2s，1s，...）。</sample>
    <sample id="482">图片中的文本翻译成中文如下：

左侧的文本：
“我们有什么解决方案？”

右侧的文本：
“这是什么？”</sample>
    <sample id="483">我们解决方案是什么？</sample>
    <sample id="484">我们的解决方案是什么？</sample>
    <sample id="485">我们的解决方案：EDAtt</sample>
    <sample id="486">根据注意力是否集中在最后λ个语音帧上，决定是否发出或发出部分翻译中的一个词（其总和低于阈值α），这意味着接收到的信息足够稳定。</sample>
    <sample id="487">根据最后λ个语音帧的注意力是否集中在单词上，决定是否发射或发射部分翻译。如果注意力之和低于阈值α，则认为接收到的信息稳定。</sample>
    <sample id="488">根据注意力是否集中在某个词上，决定是否发出部分翻译。如果注意力未集中在某个词上（其总和低于阈值λ），则认为接收到的信息不稳定。</sample>
    <sample id="489">根据注意力是否集中在单词上，决定是否发出部分翻译。如果注意力未集中在单词上（其总和低于阈值λ），则认为接收到的信息足够稳定。</sample>
    <sample id="490">根据注意力是否集中在单词上（其总和低于阈值λ）来决定是否发出部分翻译，这意味着接收到的信息足够稳定。</sample>
    <sample id="491">根据注意力是否集中在单词上，决定是否发出部分翻译。如果注意力未集中在单词上（其与最后λ帧的注意力总和低于阈值α），则认为接收到的信息稳定。</sample>
    <sample id="492">根据注意力是否集中在单词上，决定是否发出部分翻译。如果注意力未集中在单词上，则发出一个阈值α，表示最后λ帧的信息足够稳定。</sample>
    <sample id="493">根据注意力机制，决定是否发出部分翻译，具体取决于单词的注意力点是否低于阈值λ。如果单词的总和低于λ，表示接收到的信息足够稳定，则发出该单词。</sample>
    <sample id="494">根据注意力机制，决定是否发出部分翻译，具体取决于单词发出时的注意力点：如果单词发出时的注意力不集中在最后λ帧上，意味着接收到的信息足够稳定。</sample>
    <sample id="495">主要结果：EDAtt</sample>
    <sample id="496">主结果：EDAtt</sample>
    <sample id="497">主结果：EDAtt</sample>
    <sample id="498">主结果：EDAtt</sample>
    <sample id="499">图3显示了EDAtt在不同AL/AL-CA（α）值下的性能。结果表明，当α值为2.5时，EDAtt的BLEU分数最高，达到27分。这表明EDAtt在该特定设置下表现最佳。</sample>
    <sample id="500">主要结果：EDAtt流行策略也应用于离线模型wait-k LA CAAT EDAtt</sample>
    <sample id="501">图3展示了EDAtt在不同AL/AL-CA（即注意力加权和注意力加权注意力加权）值下的BLEU得分。该图表比较了EDAtt与wait-k、LA和CAAT等基线方法的性能。结果表明，EDAtt在AL/AL-CA值为1.5时取得了最佳性能，其BLEU得分为26.8。这表明EDAtt在处理SimuST数据集时具有显著优势，特别是在调整注意力权重时。</sample>
    <sample id="502">主结果：EDAtt结果</sample>
    <sample id="503">EDAtt在所有应用到离线模型的策略中表现出色</sample>
    <sample id="504">你想要发现更多吗？阅读我们的论文以发现更多结果！
(sapi.negri@fbk.eu)
marco.turchi@gmail.com
github.com/hlt-mt/fbk-fairseq
@fbk_mt
@sarapapi</sample>
    <sample id="505">根据图片中提供的信息，数据集似乎是公开的。这可以从URL 'https://github.com/ShuhenL/acl2023-conllpp' 中推断出来，该URL指向GitHub，一个广泛用于托管和分享代码的平台。GitHub上的项目通常是公开的，这意味着它们对公众可访问。</sample>
    <sample id="506">标题“MULTINSTRUCT: 改进多模态零-shot学习通过指令调优”表明该研究专注于通过一种称为“指令调优”的方法来提高多模态零-shot学习。零-shot学习是一种机器学习技术，允许模型在未见过的数据上进行预测，而无需额外的训练。多模态学习涉及处理和理解来自不同来源或格式的数据，如文本、图像和音频。因此，MULTINSTRUCT似乎是一种方法，通过提供特定的指令来增强模型在没有额外训练数据的情况下处理各种类型输入的能力。</sample>
    <sample id="507">预训练语言模型用于下游任务。图2比较了指令调优与预训练-微调和提示。</sample>
    <sample id="508">预训练语言模型用于下游任务。图2比较了预训练-微调和提示与微调之间的区别。在预训练阶段，模型在大量文本数据上进行训练，学习语言的通用表示。然后可以将模型微调到特定任务上，如情感分析或问答，通过在该任务上的标注数据上进一步训练模型。另一种方法是使用提示，其中模型被提供一组示例或提示来指导其在特定任务上的表现。这种方法不需要额外的标注数据，但可能需要更多的工程工作。最后，还可以使用微调，其中模型在特定任务上的标注数据上进一步训练，类似于预训练-微调。然而，这种方法通常需要大量的标注数据，并且可能需要更多的计算资源。</sample>
    <sample id="509">仅凭语言</sample>
    <sample id="510">将这段英语内容表达为中文，可以这样写：

“多模态预训练模型上的指导调优”</sample>
    <sample id="511">不平衡在自然语言处理和多模态之间的指导数据集中</sample>
    <sample id="512">不平衡的指令数据集在NLP和多模态之间

1600+ 语言-only 指令任务

王、乙等人的“在1600多种语言上评估基于上下文的指令的泛化能力”一文。arXiv预印本arXiv:</sample>
    <sample id="513">多模态指令调优基准数据集的first</sample>
    <sample id="514">多模态指令调优基准数据集的示例。图2显示了用于训练的白色组任务，黄色代表用于评估的任务。</sample>
    <sample id="515">OFAs (One For All)是一种统一多模态预训练模型，能够执行单模或多模态任务。OFAs具有统一的词汇表，用于语言、图像标记和边界框坐标。</sample>
    <sample id="516">输入：给定一张图片和一段描述。目标是识别与描述相关的文本区域。输出：文本区域的边界框。输入：图片和问题。目标是识别与问题相关的文本区域。输出：文本区域的边界框。输入：图片和问题。目标是识别与问题相关的文本区域。输出：文本区域的边界框。输入：图片和问题。目标是识别与问题相关的文本区域。输出：文本区域的边界框。</sample>
    <sample id="517">输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出：黑板和白色球拍。 任务：图像描述。输入：&lt;bin-198&gt;-&lt;bin-32&gt; 输出</sample>
    <sample id="518">输入：给定一张图片和一段描述，识别图片中包含的文本。输出：图片中的文本是“198-400”和“32”。</sample>
    <sample id="519">多模态调优</sample>
    <sample id="520">多模态指令转变：训练数据集构建：使用来自9组的53项任务进行训练。从每个任务中采样10,000个实例。测试数据集构建：保留整个Commonsense Reasoning组用于测试。从VQA和Miscellaneous组中选择额外的5项任务。从Natural Instructions数据集的测试分割中随机选择20项任务作为未见过的任务，用于NLP。</sample>
    <sample id="521">多模态指令转换：训练数据集构建：使用来自9组的53个任务进行训练。每个任务采样10,000个实例。测试数据集构建：保留整个Commonsense Reasoning组用于测试。从VQA和Miscellaneous组中选择额外的5个任务。使用每个任务的所有实例。从Natural Instructions数据集的测试分割中随机采样20个任务作为 unseen 任务，用于NLP。</sample>
    <sample id="522">实施细节：培训细节：-预训练的OFALarge模型（472M）-混合所有任务的所有实例。-每个实例随机与其中一个指导模板结合。测试细节：-对于每个任务，我们总共进行五次实验，通过评估模型在每个实验中使用五个指导来评估模型。-我们报告所有五次实验中的平均和最高性能以及性能的标准差。</sample>
    <sample id="523">实施细节：

- 训练细节：
  - 使用预训练的 OFA-Large 模型（472M）。
  - 将所有任务的实例与其中一个指令模板随机组合。
- 测试细节：
  - 对于每个任务，我们进行总共五次实验，通过使用五个不同指令评估模型的表现。
  - 我们报告了五次实验中的平均和最高表现以及表现的标准差。</sample>
    <sample id="524">实施细节：

* 训练细节：
	+ 使用预训练的 OFA-Large 模型（472M）。
	+ 为所有任务混合所有实例。
	+ 每个实例随机与其中一个指令模板配对。
* 测试细节：
	+ 对于每个任务，我们总共进行五次实验，通过在每次实验中评估模型来评估模型。
	+ 我们报告所有五次实验中的平均和最大性能以及性能的标准差。

右下角有一个视频片段，显示了一个穿着白色衬衫的人正在讲话。</sample>
    <sample id="525">对于多模分类任务（视觉蕴含、视觉空间推理、自然语言视觉推理和灾害类型分类），我们报告准确率。对于多模生成任务（共同知识问答、文本问答、基于视觉场景的对话），我们报告 Rouge-L。对于NLP任务，我们报告 Rouge-L。我们还根据模型在所有多模和NLP任务上的平均性能计算每个模型的聚合性能。我们不仅使用准确率作为度量标准。</sample>
    <sample id="526">模型对同一任务的不同指令的敏感性：该幻灯片讨论了模型对同一任务的不同指令的敏感性。它强调了模型在处理不同输入时的性能和一致性的重要性。幻灯片展示了数学公式，表示了模型对特定输入的敏感性。</sample>
    <sample id="527">图2展示了在多模态Commonsense Reasoning上的零-shot性能，最佳性能以粗体显示。</sample>
    <sample id="528">图2展示了在多模态Commonsense Reasoning上的零样本性能。表格中，用加粗字体表示的是最佳表现。</sample>
    <sample id="529">增加多模任务群对性能的影响

* 图像理解（Img Und）
	+ VQA + 图像理解
	+ 定位
* MISC、ITM
	+ 语义匹配 + 图像生成
	+ 时间排序 + 混杂 + 文本匹配
* 关系
	+ 可视关系
* 区域
	+ 区域理解
* NLP
	+ NLP任务

图3：随着多模指令任务群的增加，模型性能的变化</sample>
    <sample id="530">效果多样化的指示对指示微调的影响

- OFA在5个指示上进行微调，取得了所有评估任务的**更高聚合性能**，并显示了**较低的敏感性**。

下表3显示了不同数量指示的指示微调效果。在微调OPA时，分别在不同数量的指示上进行微调。

| 指示数量 | 聚合性能 | 敏感性 |
| --- | --- | --- |
| 5 | 47.82 | 10.45 |

图3：不同数量指示的指示微调效果。</sample>
    <sample id="531">微调策略对模型敏感性的影响
- 在Multinstruct上进行指导微调可以显著降低模型对OA的敏感性。
- 从自然指令数据集进一步微调模型可以进一步降低敏感性。
图4：在未见过的评估任务上的模型敏感性。数值越低越好。</sample>
    <sample id="532">在幻灯片中，标题为“NLP任务上的零-shot性能”，它讨论了使用混合指令进行微调可以提高NLP任务上零-shot性能的策略。表格显示了不同模型和训练策略的性能指标，表明混合指令可以显著提高性能。</sample>
    <sample id="533">结论

* 第一个大规模多模态指令调优数据集。
	+ 包含来自10个广泛类别的62个跨模态任务。
* 通过指令调优显著提高了OFA的零-shot能力。
* 探索了几种迁移学习技术并展示了它们的好处。
* 设计了一个新的度量灵敏度。</sample>
    <sample id="534">我们正在收集一个更大规模的多模态指令调优数据集，其中包含大约150个额外的视觉语言任务，我们很快就要发布了。</sample>
    <sample id="535">根据幻灯片底部的标志和文字，论文的作者分别属于意大利特雷维索大学（UNIVERSITÀ DI TRENTO）和布罗尼-库赛尔基金会（FONDAZIONE BRUNO KESSLER）。</sample>
    <sample id="536">演讲者的名字是Mohammad Javad Hosseini、Filip Radlinski、Silvia Pareti和Annie Louis。这些名字位于幻灯片左下角，与Google Research标志并列。</sample>
    <sample id="562">语言模型接受度判断并不总是对语境足够 robust ACL 2023 Johns Hopkins University Purdue University MIT Meta</sample>
    <sample id="563">语言模型接受度判断并不总是对语境足够 robust ACL 2023 Johns Hopkins University Purdue University MIT Koustuv Sinha, Jon Gauthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams Meta</sample>
    <sample id="564">revisiting minimal pair paradigm Minimal pair paradigm(MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym Crows 1. Many people were 1. No customer... has spent 1. Stereotypical helping themselves 2. "The customer... spent any money." 2. The customer... spent any money. 2. Non-stereotypical helping herself. P(1) &gt; P(2) P(1) &gt; P(2)any P(1) &gt; P(2)</sample>
    <sample id="565">revisiting minimal pair paradigm Minimal pair paradigm(MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer has spent 1. Stereotypical helping themselves. 2. Many people were 2. The customer has spent any money. 2. Non-stereotypical helping herself. P(1) &gt; P(2) P(1) &gt; P(Any) P(1) &gt; P(2)</sample>
    <sample id="566">revisit ing minimal pair paradigm MPP evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer has spent 1. Stereotypical helping themselves. 2. "The customer spent any money." 2. Non-stereotypical helping We're helping herself." sentence.</sample>
    <sample id="567">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical 2. helping themselves. 2. "The customer ... spent any money." 2. Non-stereotypical 3. Many people were helping herself. P(1) &gt; P(2) P(1.) &gt; P(2. any) P(1) &gt; P(2)</sample>
    <sample id="568">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer... has spent 1. Women are 2. Many people 2. The customer... spent any money 2. Men are terrible 3. We're helping 3. any money 3. at handwork. 4. herself! 4. 5. 5. Are these judgements stable with long preceding context?</sample>
    <sample id="569">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Women are 2. "Many people themselves. any money. 2. The customer ... spent 2. Men are terrible. were helping 2. Any money. at handwork. herself." any money. Are these judgements stable with long preceding context?</sample>
    <sample id="570">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer... has spent 1. Women are 2. *Many people 2. The customer... spent any money 2. Handwork. were helping 3. Many people we're helping herself. any money at handwork. men are terrible. Are these judgements stable with long preceding context?</sample>
    <sample id="571">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试问题：主语一致
可接受的
P（可接受|前缀）&gt; P（可接受|前缀）
空间中候选前缀
可接受的前缀
不可接受的前缀
BLUMP：调整
GPT2，OPT-125M为6.7</sample>
    <sample id="572">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：主语一致
可接受的
P（接受|前缀）&gt; P（接受|前缀）
空间中候选前缀
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受</sample>
    <sample id="573">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement Space of Candidate Prefixes &gt; Prefix &gt; Prefix Acceptable Unacceptable Who might Rose fees from before returning to this customer? What could Jessica sell believing reciting these spotlights? What had Aaron sounded like while before returning to this customer before k? BLUMP: Adjectival GPT2, OPT family - 125M to 6.7B</sample>
    <sample id="574">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
Test Suite: Subject-Verb Agreement
可接受的
P（可接受|前缀）&gt; P（可接受|前缀）
空间中候选前缀的集合
接受的前缀
被拒绝的前缀
BLUMP: Adjunct</sample>
    <sample id="575">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：主语一致
可接受 &gt; 不可接受
空间中候选前缀的集合
可接受前缀
不可接受前缀
BLUMP：调整后
GPT2，OPT家族-125M到6.7</sample>
    <sample id="576">Approach Test whether MPP judgments vary as a function of context length, structural match, and acceptability Test Suite: Subject-Verb Agreement P.(Prefx) &gt; P.(Prefx) Space of Candidate Prefixes Acceptable, Unacceptable</sample>
    <sample id="577">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：主题一致 &gt; 无法接受
可接受的：可接受的，不匹配的
空间中候选前缀的集合
主题一致：√
可接受的：√
不匹配的：×
BLMP外部“模糊”量词 | 调整小岛</sample>
    <sample id="578">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
Test Suite: Subject-Verb Agreement
可接受的
P（前缀&gt;前缀）&gt;P（前缀）
空间中候选前缀的集合
未被接受的
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
Who might Rose fee from before returning to this customer?
GPT2，OPT家族-125M到6.7</sample>
    <sample id="579">方法
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：主语一致
P（可接受|前缀）&gt; P（可接受|前缀）
空间中候选前缀的集合
*代表前缀
*表示后缀
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词干
*表示词</sample>
    <sample id="580">方法
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：主语一致
可接受？
P（主语|前缀）&gt; P（主语|前缀）
空间中候选前缀的集合
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量
单词嵌入：词向量</sample>
    <sample id="581">MPP judgments are robust for arbitrary context lengths We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to 17. A rose is a woody perennial flowering plant of the genus Rosa, there are over three hundred species, most of which are native to Asia, with a few native to Europe, North America and northwestern Africa.</sample>
    <sample id="582">MPP judgments are robust for arbitrary context lengths We perform MPP evaluations with different contexts — acceptable / unacceptable; matched/mismatched structure — of lengths up to 900 tokens. A rose is a woody perennial flowering plant of the genus Rosa, that might rise from before returning to before returning to</sample>
    <sample id="583">可接受/不可接受的MPP句子在语境中提高/降低判断

我们在不同语境下进行MPP评估：可接受/可接受；匹配/不匹配结构；长度可达900个标记。

0.2
0.0
-0.4
-0.6
-0.8
0
200
400
600
800
Acc. (Matched)
Acc. (Unmatched)
Prefix Strategy
MPP, OPT 6.7B</sample>
    <sample id="584">可接受/不可接受的MPP句子在语境中提高/降低判断

我们使用不同的语境进行MPP评估：可接受/可接受；匹配/不匹配结构；长度可达900个标记。

图2显示了不同前缀策略下的MPP得分，包括匹配和不匹配的结构。实线表示可接受的句子，虚线表示不可接受的句子。得分范围从-1.0到+1.0，其中0表示中性。得分越接近+1.0，句子越有可能被判断为可接受。</sample>
    <sample id="585">可接受/不可接受的MPP句子在语境中提高/降低判断

我们使用不同的语境进行MPP评估：可接受/可接受；匹配/不匹配结构；长度可达900个标记

0.2
0.0
-0.4
-0.6
-0.8
0
200
400
600
800
Acc. (Matched)
Acc. (Unmatched)
Prefix Strategy
MPP, OPT 6.7B</sample>
    <sample id="586">可接受/不可接受的MPP句子与匹配结构最严重的影响</sample>
    <sample id="587">可接受/不可接受的MPP句子与匹配结构最严重的影响</sample>
    <sample id="588">可接受/不可接受的MPP句子与匹配结构最严重的影响</sample>
    <sample id="589">为什么匹配前缀会影响LM的判断？我们以保持相关结构不变的方式篡改上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀修饰语：然而，&lt;senta&gt;。- 长前缀修饰语：首先和 foremost，&lt;senta&gt;。- 添加前导语：*无论X对此有什么看法，&lt;senta&gt;。- 引用：*昨天，X说，‘&lt;senta&gt;’*</sample>
    <sample id="590">为什么匹配前缀会影响LM判断？我们以保持相关结构不变的方式扭曲上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀修饰语：然而，‘‘&lt;sente&gt;’’- 长前缀修饰语：‘‘首先和 foremost，&lt;sente&gt;。’’- 添加从句：‘‘无论X对它说什么，‘‘&lt;sente&gt;’’。’'- 引用：‘‘昨天，X说，‘‘&lt;sente&gt;’’。’'</sample>
    <sample id="591">为什么匹配前缀会影响LM判断？

我们以保持相关结构不变的方式扭曲上下文句子，并询问模型是否对这些扭曲同样敏感。-前缀/后缀广告：然而，&lt;senta&gt;。-长前缀广告：首先，&lt;senta&gt;。-添加从句：&lt;senta&gt;。-引用：昨天，X说，&lt;senta&gt;。-引文：昨天，X说，“&lt;senta&gt;”。</sample>
    <sample id="592">为什么匹配前缀会影响LM的判断？我们以保持相关结构不变的方式 perturb 上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀广告词：然而，&lt;senta&gt;，长前缀广告词：&lt;senta&gt;，第一和前置广告词：&lt;senta&gt;，添加从句：X认为关于它的想法是&lt;‘senta’&gt;，引用：昨天他&lt;‘senta’&gt;说。- 问题：模型对类似扰动句子的敏感性有多高？</sample>
    <sample id="593">为什么匹配前缀会影响LM的判断？我们以保持相关结构不变的方式 perturb 上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀广告词：然而，&lt;senta&gt;。- 首先前缀广告词：&lt;senta&gt;。- 添加从句：X认为什么X关于它，&lt;senta&gt;。- 引用：昨天他 said，&lt;senta&gt;。我们研究了在相似方式下对扰动句子的敏感性。</sample>
    <sample id="594">关键 takeaway

* 语言模型对潜在的句法/语义特征敏感，这些特征在句子之间共享
* 短单个句子评估无法完全捕捉LMs的抽象知识。

该幻灯片讨论了语言模型（LMs）处理和理解自然语言文本的方式。它强调了LMs对潜在的句法和语义特征的敏感性，这些特征在句子之间共享。这意味着LMs能够识别和利用文本中共同的结构和含义，从而更好地理解和生成自然语言。

幻灯片还提到了LMs的一个挑战：短单个句子评估无法完全捕捉LMs的抽象知识。这表明，虽然LMs可能能够处理和理解单个句子，但它们在处理更复杂、更抽象的概念时可能会遇到困难。这种挑战突出了LMs在处理更广泛的语言任务时需要改进的地方。

总的来说，该幻灯片提供了关于LMs如何工作以及它们面临的挑战的见解，这对于理解自然语言处理技术的发展和应用至关重要。</sample>
    <sample id="595">关键词总结

* 语言模型对潜在的句法/语义特征敏感，这些特征在句子之间共享
* 句子评估使用短、单一的输入无法完全捕捉LMs的抽象知识。

图示例了不同模型在处理不同长度的前缀时的性能。它展示了在不同长度的前缀下，模型预测的概率如何变化。这有助于理解模型在处理不同长度的输入时的表现和能力。</sample>
    <sample id="596">关键 takeaway

* 语言模型对潜在的句法/语义特征敏感，这些特征在句子之间共享
* 短小、单一的句子评估不能完全捕捉LMs的抽象知识。
* 根据公式，如果前缀P的条件概率大于另一个前缀Q的条件概率，则表示它们之间的差异显著。

该幻灯片讨论了语言模型（LMs）的性能和局限性。它强调了LMs对潜在的句法/语义特征的敏感性，这些特征在句子之间共享。然而，它指出，短小、单一的句子评估并不能完全捕捉LMs的抽象知识。此外，幻灯片还提供了一个公式，用于确定两个前缀之间的差异是否显著。</sample>
    <sample id="597">根据图片显示，该方法的第一步是将输入词元映射到“Tag”类型的词元。这可以通过从输入词元（如“the”，“girl”和“slept”）到它们各自的标签（如“x1”，“girl”和“x2”）的箭头表示出来。</sample>
    <sample id="598">根据幻灯片中提供的英文内容，Coscript 包含了 55,000 个脚本。</sample>
    <sample id="626">根据表格中显示的英文内容，DEplain 的最佳对齐方法是使用 TF-IDF 相似的邻近驱动方法。</sample>
    <sample id="627">根据幻灯片中提供的内容，弱监督学习的好处包括减轻标注瓶颈。这表明，通过使用弱监督学习，可以减少需要标注的数据量，从而降低标注数据所需的时间和资源。</sample>
    <sample id="628">根据图片中显示的表格，DEplain-web 中的文档被分配为手动和自动对齐方法。具体分配情况如下：在“Document Level”部分，有 147 个文档被分配为手动对齐，另外 123 个文档被分配为自动对齐。</sample>
    <sample id="629">CoNLL++数据集是通过将原始数据集（如CoNLL 2003）与额外的资源（如Wikipedia和Google Ngram）进行融合而创建的。</sample>
    <sample id="630">XSemPLR：多种自然语言的跨语言语义解析和意义表示

于震章，王军，王智国，章锐

宾夕法尼亚州立大学
亚马逊</sample>
    <sample id="631">XSemPLR：多种自然语言的跨语言语义解析和意义表示

张宇森、王俊、王智 groove、章锐</sample>
    <sample id="632">跨语言语义解析是一个任务，将多种自然语言中的查询翻译成多种含义表示。</sample>
    <sample id="633">跨语言语义解析是一个任务，将多种自然语言中的查询翻译成多种意义表示。</sample>
    <sample id="634">跨语言语义解析

* 现有的CLSLP模型分别提出和评估在有限任务和应用上。例如：
	+ 缺乏对某些自然语言的覆盖范围
	+ English
	+ German
	+ Chinese
	+ Neural Models
	+ SQL
	+ Lambda
	+ FunQL</sample>
    <sample id="635">跨语言语义解析

* 现有的CLSP模型分别提出和评估在有限任务和应用上。例如：
	+ 缺乏对某些自然语言的覆盖范围
	English
	German
	Chinese
	Neural Models
	SQL
Lambda
FunQL</sample>
    <sample id="636">跨语言语义解析

* 现有的CLSLP模型分别提出并评估在有限任务和应用的数据集上。例如：
	+ 缺乏对某些含义表示的覆盖范围

English | German | Chinese
--- | --- | ---
神经网络 |  | 
SQL | Lambda | FunQL</sample>
    <sample id="637">跨语言语义解析

* 现有的CLSLP模型分别提出并评估在有限任务和应用上的数据集。例如：
	+ 缺乏对某些意义表示的覆盖范围
	+ 英文、德文、中文
	+ 神经网络
	+ SQL、Lambda、FunQL</sample>
    <sample id="638">跨语言语义解析

* 现有的CLSP模型分别提出并评估在有限任务和应用上。例如：
	+ 缺乏对某些特定神经网络模型的覆盖
	+ 从英文到德文的翻译
	+ 从德文到中文的翻译
	+ 从中文到SQL的翻译
	+ 从SQL到Lambda的翻译
	+ 从Lambda到FunQL的翻译</sample>
    <sample id="639">XSemPLR

我们提供一个统一的数据集XSemPLR用于跨语言的语义解析和意义表示。它包含：

• 9个不同领域的数据集
• 5种语义任务
• 22种自然语言，属于15种语言家族

示例：

输入：玩家在每个季度比赛少于三次
输出：SELECT COUNT(*) FROM game WHERE season = 'number' AND player &lt; 3</sample>
    <sample id="640">XSemPLR

我们提供一个统一的数据集XSemPLR用于多语言跨语义解析。它包含：

• 9个不同领域的数据集
• 5种不同的任务
• 22种自然语言，涵盖15种语言家族

该数据集包括：

• 美国（English）
• 德国（German）
• 日本（Japanese）
• 意大利（Italian）
• 法国（French）
• 中国（Chinese）
• 韩国（Korean）
• 印度（Hindi）
• 波兰（Polish）
• 土耳其（Turkish）
• 乌克兰（Ukrainian）
• 保加利亚（Bulgarian）
• 阿尔及利亚（Algerian Arabic）
• 阿富汗（Pashto）
• 阿拉伯语（Arabic）
• 哈萨克语（Kazakh）

该数据集旨在支持多种自然语言和语义表示的多语言跨语义解析研究。</sample>
    <sample id="641">实验设置

* 我们考虑了六个用于训练和评估的设置。
	+ **Translate-Test**：使用Google Translate API将源语言翻译成目标语言。然后使用单语模型进行训练和评估。

**训练**

English（英语）
English Model（英语模型）
SQL

**Inference**

German（德语）
Translate API（翻译API）
English Model（英语模型）
SQL</sample>
    <sample id="642">实验设置

* 我们考虑了六个用于训练和评估的设置。
	+ **Translate-Test**：使用Google Translate API将源代码翻译成目标语言。然后使用单语模型进行训练和评估。

训练

* 英语
* 英语模型
* SQL

推理

* 德语
* 翻译API
* 英语
* 英语模型
* SQL</sample>
    <sample id="643">实验设置

* 我们考虑了六个训练和评估的设置。
	+ **Translate-Test**：使用Google Translate API将源语言翻译成目标语言。然后使用单语模型进行训练和评估。

**训练**

English
English Model
SQL

**Inference**

German
Translate API
English
English Model
SQL</sample>
    <sample id="644">实验设置

* 我们考虑了六种训练和评估的设置。
* 例如，源语言与目标语言相同，如“德语到德语”所示。我们还测试了单语多语言少样本设置，即使用仅10%的训练数据训练单语模型。

训练

* 德语模型（少样本）
* SQL

推理

* 德语模型
* SQL</sample>
    <sample id="645">实验设置

* 我们考虑了六种训练和评估的设置。
* 例如，源语言与目标语言相同，如“德语到德语”。
* 我们还测试了单语多模态模型，只使用10%的训练数据。

训练

* 德语（少样本）
* 德语模型
* SQL

推理

* 德语
* 德语模型
* SQL</sample>
    <sample id="646">实验设置

• 我们考虑了六种训练和评估的设置。
• 源语言与目标语言相同，例如从德语到德语。我们还测试了单语多模态设置，即使用仅10%训练数据训练单语模型。

训练

| 德语（少示例） | 德语模型 | SQL |
| --- | --- | --- |
|  |  |  |

推理

| 德语 | 德语模型 | SQL |
| --- | --- | --- |
|  |  |  |

页脚：2023年5月17日，弗洛里安·舒尔策</sample>
    <sample id="647">实验设置

* 我们考虑了六个用于训练和评估的场景。
	+ 多语言模型：为所有语言训练一个单一的多语言模型。

培训

* 德语
* 英语
* 中文
* 多语言模型
* SQL

推断

* 德语
* 多语言模型
* SQL</sample>
    <sample id="648">实验设置

* 我们考虑了训练和评估的六种设置。
* 多语言模型：为所有语言训练一个单一的多语言模型。

训练

推理</sample>
    <sample id="649">实验设置

* 我们考虑了训练和评估的六种设置。
* 多语言模型：为所有语言训练一个单一的多语言模型。

训练

推理</sample>
    <sample id="650">实验设置

* 我们考虑了六个训练和评估的设置。
* 跨语言零-shot/少-shot转移。在一种语言上进行训练，然后转移到另一种语言。

训练

| 英语 | 或 |
| --- | --- |
| 英语 | 德语 |
| 德语 | 少量模型 |
| 多语言模型 | SQL |

推断

| 德语 | 多语言模型 |
| --- | --- |
|  | SQL |

幻灯片编号：11</sample>
    <sample id="651">实验设置

* 我们考虑了六个训练和评估的设置。
* 跨语言零-shot/少-shot转移。在一种语言上进行训练，然后转移到另一种语言。

训练

| 英语 | 或 |
| --- | --- |
| 英语 | 德语 |
| 英语 | 少量模型 |
| 多语言模型 | SQL |

推理

| 德语 | 多语言模型 |
| --- | --- |
| 德语 | SQL |</sample>
    <sample id="652">我们发现多任务学习（MT5）在所有数据集上都取得了最佳性能！</sample>
    <sample id="653">我们评估了两种模型组在单语环境下的性能：多语言预训练编码器与指针式解码器（Enc-PTR）和XLM-R + mBERT + PTR。我们发现mBERT + mT5在所有数据集上都取得了最佳性能。</sample>
    <sample id="654">我们评估了两种模型组在单语环境下的性能：多语言预训练编码器与指针式解码器（Enc-PTR）和多语言预训练编码器与多任务学习（Enc-Dec）。我们发现，在所有数据集上，mT5的Enc-Dec模型表现最佳。</sample>
    <sample id="655">我们发现，mT5在所有数据集上都取得了最佳性能！</sample>
    <sample id="656">我们评估了在多语言设置下mT5和XLM-R + PTR的性能，通过在各种语言混合物中进行训练可以提高Enc-Dec-Enc-PTR（mT5/XLM-R）的性能。</sample>
    <sample id="657">我们评估了mT5和XLM-R + PTR在多语言环境下的性能。通过在各种语言的混合中进行训练，Enc-Dec-Enc-PTR（mT5-XLM-R）可以得到改善。</sample>
    <sample id="658">我们评估了mT5和XLM-R + PTR在多语言设置上的性能。大多数主要的NL任务取得了性能提升，除了在7个数据集中英语性能下降外，这被称为“多语言 curse”。在不同自然语言上的数据集数量。</sample>
    <sample id="659">我们评估了mT5和XLM-R + PTR在多语言环境下的性能。大多数主要的NL任务取得了性能提升，除了在7个数据集中英语性能下降外。这被称为“多语言现象”。</sample>
    <sample id="660">跨语言性能差距
蓝色线：跨语言少样本转移
橙色线：跨语言零样本转移
绿色线：单语环境
地理查询
MTOP
SchemaQA
过夜
NLMaps
MCWQ
地理/函数
地理/prolog
地理/sql
Spider
ATIS
蓝色线（几例）：几例跨语言少样本转移
橙色线（零例）：几例跨语言零样本转移
绿色线（单例）：单例设置</sample>
    <sample id="661">跨语言性能差距
蓝色线：跨语言少样本转移
橙色线：跨语言零样本转移
绿色线：单语环境
地理查询
MTOP
SchemaQA
过夜
NLMaps
MCWQ
地理/编程
地理/lambda
地理/SQL
ATIS
地理/函数</sample>
    <sample id="662">跨语言性能差距
蓝色线：跨语言少样本转移
橙色线：跨语言零样本转移
绿色线：单语环境
地理查询
MTOP
SchemaQA
Overnight
NLMaps
MCWQ
Geography/lamb
Geography/prolog
Geography/funql
Geography/sql
Spider
ATIS
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="663">其他结果和发现（论文第4节） · Enc-Dec (m75) 在之前的成果上表现出色，或者在目标NL上的性能相当。 · 在目标NL上预训练可以显著提高多语言LLMs（Codex &amp; Bloom）的性能。 · 多语言LLMs（Codex &amp; Bloom）对于跨语言解析仍然不够充分。 · 中文迁移学习和英文单语训练（En -&gt; En）具有最大的性能差距，而德文通常具有最小的性能差距。 · FunQL在其他三种意义表示中表现出色，而SQL通常表现最差。</sample>
    <sample id="664">其他结果和发现（论文第4部分）

* Enc-Dec (m75) 在之前的成果上表现出色，或者在目标NLP任务上取得了相当可观的结果。
* 在英语NLP上进行预训练可以显著提高多任务学习的性能。
* 多语言LLMs（Codex和Bloom）对于跨语言解析仍然不够充分。
* 中文迁移学习和英语单语训练（En -&gt; En）具有最大的性能差距，而德语通常具有最小的性能差距。
* FunQL在其他三种意义表示中表现出色，而SQL则表现出较差的性能。</sample>
    <sample id="665">结论
- 我们构建了XSemPLR，一个用于多语言和意义表示的跨语言语义解析的统一基准。
- 我们对三种代表性多语言语言模型进行了全面基准测试。
- 结果表明，mT5在单语训练下具有最佳性能，而显著的多语言LLMs仍然不足以完成跨语言句法解析任务。此外，跨语言训练和跨语言迁移学习仍然很重要。

幻灯片编号：18</sample>
    <sample id="666">结论
- 我们构建了XSemPLR，一种用于多自然语言和意义表示的跨语言语义解析的统一基准。
- 我们对三种代表性多语言语言模型进行了全面基准测试。
- 结果表明，mT5在单语训练下具有最佳性能，而显著的多语言LLMs仍然不足以完成跨语言句法解析任务。此外，跨语言训练和跨语言迁移学习仍然很重要。</sample>
    <sample id="667">关于保护深度神经网络的知识产权的现有研究包括参数化水印、字典水印、后门水印和对抗性水印。这些方法各有优缺点，例如参数化水印可能在转移性方面存在挑战，而字典水印则可能在计算和通信效率方面存在限制。后门水印和对抗性水印可能分别适用于不同的场景，但都需要进一步的研究来确保它们的安全性和可靠性。</sample>
    <sample id="668">根据幻灯片中呈现的发现，Codex 和 Bloom 等多语言 LLMs 对于 CLSP（跨语言解析）来说还不足够。这表明这些模型在处理不同语言之间的翻译或转换时仍然存在不足之处，特别是在需要理解不同语言结构和语义的情况下。</sample>
    <sample id="695">根据图片，该方法似乎通过在每个输入标记（如'girl'、'sleep'）下方使用问号来处理排列的不确定性。这表明系统可以接受输入的排列，并且可能通过比较不同的排列组合来找到正确的匹配或解释。</sample>
    <sample id="696">根据给定的图片，对于下游 NLP 模型的公平性的定义似乎涉及评估模型在处理不同政治背景下的偏见和敏感性时的表现。这包括识别和解决偏见，确保模型对所有用户一视同仁，并且不会根据用户的种族、性别或其他敏感属性做出歧视性决定。</sample>
    <sample id="697">演讲者的名字是Yanis Labarre。</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。</sample>
    <sample id="700">在文本的背景下，热带主义 (tropicalism) 意味着通过强调文化、传统和身份来对某些群体进行刻板印象。这种刻板印象将这些群体仅仅定义为他们的身份，而没有考虑他们更广泛的背景或个性。</sample>
    <sample id="701">根据幻灯片中呈现的信息，作者通过使用特定的描述词来创建目标群体的人工描写。这些描述词包括“文化”、“传统”、“自豪”和“异国情调”，这些词被用来定义那些被标记为“其他”的群体。此外，还使用了诸如“鲜艳”、“娇小”、“精致”和“坚强”等词语来描绘女性，暗示着一种刻板印象或偏见，因为这些描述是基于种族或族裔的。</sample>
    <sample id="702">在所提供的图片中，衡量语境使用情况的方法是“P-CXMI”。这是幻灯片上提到的要点。</sample>
    <sample id="703">根据幻灯片中提供的信息，DrBERT 和 ChuBERT 都是用于医疗文本处理的预训练模型。然而，它们之间的区别在于它们的来源和训练方式。DrBERT 是从头开始构建的，这意味着它是在没有任何预训练权重的情况下训练的，而 ChuBERT 则是基于一个现有的预训练模型进行微调的。这种差异可能会影响它们在特定任务上的性能和效率，比如理解医学术语或识别疾病相关实体。</sample>
    <sample id="751">根据幻灯片显示，这篇论文有三位作者：Zhongyang Xu、Ying Shen和Lifu Huang。</sample>
    <sample id="752">迭代迁移学习（IT）是一种机器学习方法，其中模型在每个迭代中被重新训练和更新，使用新获取的数据。这个过程通常涉及将模型应用于新数据集，识别其性能的不足之处，并根据这些发现对模型进行调整。在每次迭代中，模型都会变得更加准确和有效，因为它适应了新的信息。这种方法特别适用于那些数据不断变化或出现的新情况的场景，比如在线学习环境或需要持续更新以保持高效性的系统。</sample>
    <sample id="753">数据集的目标是解决“解决间接引用表达”问题，如幻灯片底部所示。这表明数据集旨在通过提供用户在自然对话中使用的语言示例来帮助机器学习系统更好地理解用户的偏好和指示，特别是当这些指示是间接或模糊的。</sample>
    <sample id="754">根据幻灯片中提供的信息，攻击者通过EaaS（即“云服务”）来提取模型参数。这表明攻击者利用了云服务的资源和能力，可能包括计算能力或存储空间，来访问或提取模型参数。这可能是通过与云服务提供商合作或利用云服务的API来实现的。</sample>
    <sample id="755">根据幻灯片上显示的英文信息，这篇论文有三位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="756">根据图片中显示的柱状图，创建初始数据集时使用了8个注释者。</sample>
    <sample id="757">根据图片显示，这篇论文的作者分别来自以下机构：University of Washington、Carnegie Mellon University和Allen Institute for AI。</sample>
    <sample id="758">以左侧为支配词的示例是“[我]看见了巴特和琳达。霍默来了并打喷嚏。”</sample>
    <sample id="759">对话系统中的最先进模型是Alexa。</sample>
    <sample id="760">在评估语言模型的可接受性时，需要在整个上下文窗口中进行评估，以确保模型生成的内容在语境上是连贯和合理的。这很重要，因为单独的句子片段可能无法传达完整的含义或背景。通过考虑整个文本，我们可以更好地理解模型对语言结构、语法和语义关系的处理能力。这种全面的方法有助于识别潜在的偏见或错误，这些错误可能在孤立句子中不会立即显现。因此，它有助于评估模型的整体性能和可靠性。</sample>
    <sample id="761">根据幻灯片中呈现的数据，多语言训练（mT5）与单语英语模型相比会导致表现下降。这在柱状图的红色部分所示，表示在多个语言中存在性能下降。然而，对于德语（de）、中文（zh）、孟加拉语（bn）、泰米尔语（th）、孟加拉语（fa）、印尼语（id）、越南语（vi）和法语（fr）等特定语言，性能有所提高。</sample>
    <sample id="762">注释者是否提前知道该实体？</sample>
    <sample id="763">根据图片中提供的英文内容，评估了BLEU和METEOR两个MT指标。</sample>
    <sample id="764">根据图片中提供的信息，回归任务与特定的NER（命名实体识别）类型之间的关系没有明确说明。该图表专注于比较不同模型在NER任务上的性能，而未提供有关回归任务或其对NER类型影响的细节。因此，仅凭这张图片无法确定回归是否会影响特定的NER类型。</sample>
    <sample id="765">NLP 中的立场很重要，因为它有助于理解文本的情感和态度。通过识别文本中的情感，NLP 模型可以更好地处理任务，如情感分析、文本分类和机器翻译。</sample>
    <sample id="766">根据幻灯片上的信息，像 BLOOM 这样的多语言大规模语言模型（LLM）是采用完整微调。这可以从幻灯片的标题“多语言大规模语言模型完整微调”中得出，表明这些模型是通过在大量数据上进行完整微调来训练的，而不是仅仅使用适配器微调。</sample>
    <sample id="767">他们使用RoBERTa-base模型进行迁移学习。</sample>
    <sample id="768">根据图片中提供的信息，最近用于评估 PaLM 能力的测试集包括 WMT’14 德语-英语和英语-德语翻译任务、WMT’16 英语-德语翻译任务以及 IWSLT’19 英语-德语和德语-英语翻译任务。</sample>
    <sample id="769">根据幻灯片上可见的文本，作者提出了两条建议。</sample>
    <sample id="770">根据幻灯片中提供的信息，与最强的基线相比，所建议的方法获得了1.0点收益。</sample>
    <sample id="771">演讲者的名字是“Shuheng Liu, Alan Ritter”，如幻灯片左下角所示。</sample>
    <sample id="772">是的，论文中的结果和数据集可以作为基准。它们提供了关于不同文本简化方法性能的详细比较，这对于研究者来说是一个有用的参考点，他们希望评估自己的方法或改进现有的文本简化技术。</sample>
    <sample id="773">根据图片中显示的柱状图，进行了三个较小模型的实验。这些模型是用CoScript微调的，并且它们与大型语言模型（LLMs）进行了比较。</sample>
    <sample id="774">根据幻灯片中提供的英文内容，被用作研究多模型指令调整的基础模型是OFAs (One For All)。</sample>
    <sample id="833">根据幻灯片显示，论文的作者是来自Google Research的。这可以从幻灯片左下角的Google标志和“Google Research”文字推断出来。</sample>
    <sample id="834">根据幻灯片显示，论文的作者来自圣布鲁克大学的人类语言分析系。</sample>
    <sample id="835">根据幻灯片上提供的信息，论文分析了多种语言对。这些语言对包括西班牙语-英语、日语-英语、荷兰语-英语、法语-英语和德语-英语。这些语言对的选择表明了研究的广泛性和跨语言性，旨在评估LLM在不同语言组合下的翻译能力。</sample>
    <sample id="836">演讲者的名字是Yulia Tsvetkov。</sample>
    <sample id="837">在实验过程中研究了以下模型：SARI、BLEU和BS-P。</sample>
    <sample id="838">根据幻灯片中提供的信息，MultiInstruct 中使用的 62 个不同任务中，有 53 个任务用于训练目的，另外 9 个任务用于测试目的。</sample>
    <sample id="839">根据幻灯片显示，这篇论文有三位作者：Regina Stodden、Omar Momen和Laura Kallmeyer。</sample>
    <sample id="840">作者在实验中使用了以下数据集：AG News，MIND，SST2，Enron Spam和Wikipedia文本。</sample>
    <sample id="876">NACHOS 是一种预训练策略，正如幻灯片中提到的那样，它与其他预训练策略一起被比较。然而，仅凭幻灯片本身无法确定 NACHOS 的具体含义或运作方式。要了解 NACHOS，需要额外的背景信息或来自演讲者或演讲幻灯片中未展示的来源的详细解释。</sample>
    <sample id="877">根据图片显示，演讲者的名字是David Vil Torres、Markus Freitag、Colin Cherry、Jiaming Luo、Virendra Ratnaker和George Foster。这些名字位于幻灯片底部的六张照片下方。</sample>
    <sample id="878">根据幻灯片中提供的信息，提示策略对翻译质量的影响相当大。它表明选择不同的随机提示可以导致BLEURT得分的显著差异，范围从1到40分。这意味着提示的选择可以显著影响翻译评估的质量，这可能会影响翻译任务的整体性能和准确性。</sample>
    <sample id="879">根据幻灯片显示，论文的作者来自卡内基梅隆大学的语言技术研究所、葡萄牙技术大学、葡萄牙技术大学的BAIR人工智能研究实验室和Unbabel公司。</sample>
    <sample id="880">图片中没有包含任何由专家编写的指令。它只是宣布了正在收集一个更大的多模态指令调优数据集，并计划在不久的将来发布额外的150个视觉语言任务。</sample>
    <sample id="881">根据幻灯片内容，作者建议通过使用人类研究参与者和核心ference模型来测试模型，以评估其在处理多源信息时的能力。</sample>
    <sample id="882">Google Prompting PalM for Translation Assessing Strategies and Performance ACL 2023</sample>
    <sample id="883">PaLM：Pathways Language Model

* Chowdery et al., 2022
* arXiv:2204.02311.
* 540B参数。
* 训练了780B tokens。
* 密集激活。
* 6144 TPU v4芯片。
* 在数百个TPU和生成基准上达到SOTA性能。

10亿参数

Google</sample>
    <sample id="884">PaLM：Pathways Language Model
- Chowdery et al., 2022
- arXiv:2204.02311.
- 540B参数。
- 训练了780B tokens。
- 密集激活。
- 6144 TPU v4芯片。
- SOTA在 hundreds of GPU和Generation benchmarks上。
- 62 billion parameters
- 问题回答、代码完成、翻译、文本生成、理解、摘要化</sample>
    <sample id="885">我们的贡献
- 对MT的LLM提示进行了首次系统研究。
- 评估了选择策略以及候选人池。
- 用MT社区的最佳实践评估翻译能力：
- 避免测试/训练重叠和最近培训数据上的过拟合的最新测试集。
- 使用最新提交（使用最新培训数据）进行比较。
- 专家基于人类评估（比群众工作者更 robust）。
- 推荐提示选择策略</sample>
    <sample id="886">我们的贡献

* 对MT的LLM提示进行了首次系统研究。
* 评估了候选池和选择策略。
* 通过最佳实践评估翻译能力：
	+ 最新的测试集（避免测试/训练重叠和最近训练数据上的过拟合）。
	+ 比较最新MT系统（与人类判断的相关性更好）。
	+ 专家基于人类评估（比群众工作者更 robust）。
* 推荐提示选择策略。</sample>
    <sample id="887">我们的贡献

* 对MT领域LLM提示的系统研究
	+ 评估候选人池和选择策略
* 用MT社区的最佳实践评估翻译能力：
	+ 最新测试集（避免测试/训练重叠和最近训练数据上的过拟合）
	+ 比较最新WMT提交（使用最新训练数据的SOTA系统）
	+ 基于专家的人类评估（比众包工人的评估更 robust）
* 推荐提示选择策略</sample>
    <sample id="888">我们的贡献

* 对MT领域的LLM提示进行了首次系统研究。
	+ 评估了候选池以及选择策略。
	+ 使用最新提交的SOTA系统评估翻译能力，使用最新的训练数据。
* 最佳实践的MT社区：
	+ 最新测试集（避免测试/训练重叠和过拟合）。
	+ 比较最近的WMT提交（使用最新训练数据）。
	+ 基于专家的人类评估（比Crowd Workers更 robust）。
* 推荐提示选择策略。</sample>
    <sample id="889">Prompt对翻译质量有重大影响

* 为每个句子选择两个随机提示。
* 计算每个句子-提示对的BLEURT分数。
* 大多数句子（516个句子中的516个）显示得分差超过1个BLEURT点。
* 分数差可达到40个BLEURT点！</sample>
    <sample id="890">Prompt对翻译质量有很大影响

* 为每个句子选择两个随机提示。
* 计算每个句子-提示对的BLEURT分数。
* 大多数句子（516个句子，占1000个句子的51.6%）的分数差超过1个BLEURT点。
* 分数差可达到40个BLEURT点！</sample>
    <sample id="891">翻译后的中文内容如下：

提示对翻译质量有很大影响

* 为每个句子选择两个随机提示。
* 计算每个句子-提示对的BLEURT分值。
* 大多数句子（516个句子，占1000个句子的大部分）得分差超过1个BLEURT点。
* 分差可达到40个BLEURT点！</sample>
    <sample id="892">示例翻译提示

5-shot提示

德语：Dort sieht man, wie sie von zwei Polizisten in einem Streifenwagen gestellt wird. Er ist auf dem Weg ins Gefängnis und wird unter der Aufsicht von zwei Polizisten auf einem Bus zu einem Polizeigelände gebracht.

德语：Ski-Legenden unter sich. Die Polizei war eingeschritten, nachdem sie Beschwerden des Browsers erhalten hatten.

德语：Ein Passt alarmierte die Polizei, mit mehreren Streifenwagen anrücken.

德语：Police were called after receiving complaints from the office.

English：Police were called after receiving complaints from the office.</sample>
    <sample id="893">示例翻译提示</sample>
    <sample id="894">示例翻译提示

5-shot prompting

German: Drei Männer, die sie von zwei Polizisten in einem Streifenwagen festgenommen wurden, werden ins Gefängnis überwiesen.

English: Three men are being transported to jail by two policemen in a police car.

German: Polizeiwarnt sich unter sich. Die Polizei wurde eingeschritten, nachdem sie Beschwerden des Bureaus erhalten hatten.

English: Police were called after receiving complaints from the office.

German: Ein Passant alarmierte die Polizei, die mit mehreren Streifenwagen ankamen.

English:</sample>
    <sample id="895">示例提示用于翻译</sample>
    <sample id="896">示例提示用于翻译</sample>
    <sample id="897">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/遗漏”决定。
    * “风格/笨拙”一般较低，对于PaLM而言。

Google翻译</sample>
    <sample id="898">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性得分一般较低。
    * 主要由“准确性/遗漏”决定。
    * “风格/笨拙”一般较低，对于PaLM而言。</sample>
    <sample id="899">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/省略”决定。
    * “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="900">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅性与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/省略”决定。
    * “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="901">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM与Google Translate相当。

从MQM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/遗漏”决定。
    * “风格/笨拙”一般较低，对于PaLM而言。</sample>
    <sample id="902">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性得分一般较低。
    * 主要由“准确性/遗漏”主导。
    * “风格/笨拙”一般较低，对于PaLM而言。</sample>
    <sample id="903">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MGM获得的见解

* PaLM的流畅性与SOTA相当。
* 准确性一般较低。
    * 主要受“准确性/省略”影响。
    * “风格/笨拙”一般较低，对于PaLM而言。</sample>
    <sample id="904">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅性与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/遗漏”决定。
    * “风格/笨拙”一般较低，对于PaLM而言。</sample>
    <sample id="905">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PALM接近Google Translate。

从MQQ中获得的见解

* PALM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/遗漏”主导。
    * “风格/笨拙”一般较低，对于PALM而言。</sample>
    <sample id="906">谢谢</sample>
    <sample id="907">Saarland University LST Department of Language and Technology Saarland University Universitär Wien Weaker Than You Think：对弱监督学习的批判性审视 Dawei Zhu¹，Xiaoyu Shen²，Marius Mosbach³，Andreas Stephan³，Dietrich Klakow¹ ¹ Saarland University ² Amazon Alexa ³ University of Vienna ACL2023</sample>
    <sample id="908">Saarland University

LST

Department of Language Science and Technology

Saarland University

Universität Wien

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna</sample>
    <sample id="909">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. Weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="910">为什么弱监督学习？

* 弱监督缓解了标注瓶颈。
* 但噪声标签！
* 弱监督学习（WSL）训练模型，即使在噪声数据上也能泛化良好。

图片中的图标代表了弱监督学习的过程。底部的圆柱形图标代表未标注的数据，顶部的图标代表噪声标注数据。中间的图标代表使用噪声标注数据训练的模型。</sample>
    <sample id="911">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="912">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="913">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="914">A common claim in recent WSL works

“We train models only on weakly supervised data and achieve an accuracy of XX%.”</sample>
    <sample id="915">A common claim in recent WSL works

“We train models only on weakly supervised data and achieve an accuracy of XX%.”</sample>
    <sample id="916">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%"

Weakly labeled training data (noisy)

Cleanly labeled validation data (clean)

Cleanly labeled test data (clean)</sample>
    <sample id="917">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%"

Weakly labeled training data (noisy)

Cleanly labeled validation data (clean)

Cleanly labeled test data (clean)</sample>
    <sample id="918">我们的研究问题

RQ1：是否需要干净的验证数据？

RQ2：WSL方法需要多少干净样本？

RQ3：如何更有效地利用现有的干净样本？</sample>
    <sample id="919">Our research questions

RQ1 Is clean validation data necessary?

RQ2 How many clean samples do WSL approaches need?

RQ3 How to use the available clean samples more efficiently?</sample>
    <sample id="920">RQ1 Main findings Validation on weak labels No validation on validation on weak labels FT_w BOND COSINE MLC L2R</sample>
    <sample id="921">RQ1 主要发现  验证在弱标签上的表现改进 (%) 验证在弱标签上 无验证（随机选择） 验证在干净标签上 FT_w BOND COSINE MLC L2R</sample>
    <sample id="922">RQ1 主要发现

验证集上的性能改进（%）

验证集上的弱标签
没有验证
验证集上的干净标签

FT_w
BOND
COSINE
MLC
L2R</sample>
    <sample id="923">RQ1 主要发现

验证集上的性能改进（%）

验证集上的弱标签
随机选择
干净标签

FT_w
BOND
COSINE
MLC
L2R</sample>
    <sample id="924">RQ1 主要发现

验证强标签（%）与验证弱标签（%）的相对性能改进（%）

FT_w、BOND、COSINE、MLC、L2R</sample>
    <sample id="925">RQ2 Main findings 85 80 FTw COSINE L2R BONDO MLC Weak labels 75 5 10 20 30 40 50 All Validation</sample>
    <sample id="926">RQ2 Main findings</sample>
    <sample id="927">RQ2 主要发现 FTw COSINE L2R BAND MLC 弱标签 5 10 20 30 40 50 All 验证 WSL 方法从更多的干净验证样本中受益！</sample>
    <sample id="928">RQ2 Main findings WSL approaches benefit from more clean validation samples! 5 5.0 10.0 20.0 30.0 40.0 50.0 All FTw COSINE L2R MLC Weak labels Performance Delta (%) All FTc LoRAc BiFiIt Adapterc Validation</sample>
    <sample id="929">RQ2 Main findings WSL approaches benefit from more clean validation samples! 5 85 80 75 70 60 50 40 30 20 10 5 All FTw COSINE L2R MLC Weak labels Performance Delta (%) 5 0 -5 -10 -15 -20 5 10 20 30 40 50 All Validation Validation FTw COSINE L2R MLC Adapterc</sample>
    <sample id="930">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT Before CFT After CFT COSINE ER FT+L Clean Only</sample>
    <sample id="931">RQ3 Main findings N=10 clean samples per class N=30 samples per class Before CFT After CFT Accuracy/F1 Accuracy/F1 88 90 86 84 82 COSINE ER FT+ 80 78 76 74 72 70 Before CFT After CFT</sample>
    <sample id="932">RQ3 Main findings N=10 clean samples per class N=30 samples per class Before CFT After CFT Before CFT After CFT</sample>
    <sample id="933">RQ3 Main findings N=10 clean samples N=30 clean samples Before After Accuracy/F1 Accuracy/F1 Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="934">结论
最近的WSL方法
需要干净样本。
高估它们的实用性。
我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="935">结论
最近的WSL方法
需要干净的样本。
高估它们的实用性。
我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="936">结论
最近的WSL方法
需要干净样本。
高估它们的实用性。
我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
始终应用连续微调（CFT）。</sample>
    <sample id="937">结论
最近的WSL方法
需要干净样本。
高估它们的实用性。
我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="938">结论
最近的WSL方法
需要干净样本。
高估它们的实用性。
我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="939">对话系统的常用评估方法是“比较性评估”，如幻灯片中所示。该方法涉及将系统的表现与一个或多个基准进行比较，以确定其性能水平。这可以通过使用标准对话评估指标（如BLEU、METEOR、ROUGE等）来实现，这些指标衡量对话的流畅度、相关性和准确度。此外，还可以通过用户研究和调查来评估对话系统的用户体验和满意度。</sample>
    <sample id="940">根据幻灯片显示，这篇论文有五位作者。他们的名字和照片都清晰地展示在标题下方。</sample>
    <sample id="941">要理解 Servin 和 Kea 的示例，需要了解角色的背景知识。Servin 被描述为一名法官，而 Kea 被描述为一名面包师。这些职业暗示了他们各自的生活方式和工作环境。Servin 的角色涉及法律和决策，这可能意味着他有责任和压力。Kea 作为面包师，可能过着更轻松、更专注于烹饪和烘焙的生活。这些背景知识有助于理解为什么 Servin 在一天结束后会感到高兴放松，而 Kea 可能不会经历同样的疲劳。</sample>
    <sample id="942">是的，代码公开。根据幻灯片底部的信息，可以在GitHub上找到数据集、生成和评估代码，网址为“mpoems/kitmus”。</sample>
    <sample id="943">根据图片中提供的数据，NLPositionality的注释者在人口统计学特征方面似乎不平衡。这可以从柱状图中不同高度的柱子看出，每个柱代表一个不同的教育水平。柱子的高度表示注释者的数量，较高的柱子表示更多的人参与了该组别。例如，“大学”和“研究生院”柱子明显高于“高中”、“预科高中”和“教授学校”柱子，表明这些群体中有更多的注释者。这表明NLPositionality的注释者在教育水平上存在不平衡，大多数注释者来自具有更高教育水平的群体。</sample>
    <sample id="944">在可接受的域中扰乱句子的方法包括添加前缀/后缀修饰语、使用长前缀修饰语、将“无论X关于它说什么”添加到开头，以及引用X昨天说的引文。这些方法旨在保持句子结构的相关性，同时通过引入干扰来测试模型对不同输入的敏感度。</sample>
    <sample id="945">进行维度评估意味着对 Likert 量表上的评分进行系统化和结构化的分析。这涉及识别和量化数据中反映的模式、趋势或类别。在 Likert 量表的背景下，维度评估可能涉及确定不同选项（如 1 到 5）之间的潜在因素或组分，这些因素或组分可以解释评分的变异性。这有助于理解参与者评分背后的心理构造，并为量表的改进或解释提供信息。</sample>
    <sample id="946">根据英语内容，这篇论文的作者所属机构是“University of Science and Technology of China”和“Microsoft Research Asia”，以及“北京航空航天大学”和“Sony Microsoft STC Asia”。</sample>
    <sample id="947">在翻译任务中，提示的形式很重要，因为它们为模型提供了输入的结构和方向。清晰、简洁且格式良好的提示可以指导模型生成准确和相关的翻译。相反，模糊或不一致的提示可能导致模型误解或错误地翻译文本。</sample>
    <sample id="978">根据幻灯片中呈现的图表，作者评估了以下对话模型：ABC-Eval、Turn Likert和Dialogue Likert。这些模型在图表上被标记为不同的点，并且似乎被用来比较它们在解释质量方面的能力，如图表标题“增量效度”所示。</sample>
    <sample id="979">根据图片显示，这篇论文有九位作者。</sample>
    <sample id="980">根据幻灯片中展示的规划问题，优秀规划器的理想品质包括能够处理具有多方面约束的特定目标。这表明规划器应该能够理解并满足任务的多个方面，如在制作蛋糕时需要考虑的材料和步骤。此外，规划器应该能够有效地规划出一系列动作，以实现特定目标，同时遵守所有相关的约束条件。</sample>
    <sample id="981">根据幻灯片上列出的作者姓名，这篇论文共有九位作者。</sample>
    <sample id="982">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="983">根据图片显示，这篇论文的作者所属机构是华沙大学计算机科学学院。这可以从幻灯片底部的标志和文字中看出，该标志和文字位于波兰语和英文中。</sample>
    <sample id="1021">根据幻灯片中提供的信息，PaLM 最常见的错误是“遗漏/遗漏”。</sample>
    <sample id="1022">不要忘记你的ABC：评估聊天导向对话系统中的前沿技术 Sarah E. Finch, James D. Finch和Jinho D. Choi Emory大学 亚历克萨尔研究实验室</sample>
    <sample id="1023">不要忘记你的ABC：评估聊天导向对话系统中的前沿技术 Sarah E. Finch, James D. Finch和Jinho D. Choi Emory大学 亚马逊Alexa研究实验室</sample>
    <sample id="1024">不要忘记你的ABC：评估聊天导向对话系统中的最新进展 Sarah E. Finch，James D. Finch和Chinho D. Choi 以下是关于Emory大学、Emory NLP和Alexa研究实验室的标志。</sample>
    <sample id="1025">比较评估

左边的机器人有蓝色和灰色的对话气泡，右边的机器人有紫色和灰色的对话气泡。</sample>
    <sample id="1026">Likert Rating Evaluation

这个幻灯片展示了 Likert 量表评估的概念。它包括一个标题，写着“Likert Rating Evaluation”，并描绘了一个人物形象，可能代表用户或参与者。人物形象旁边有两个对话气泡，表示用户与系统之间的互动。系统由两个机器人图标表示，它们通过一个带有从1到5的评分尺度的连接线连接在一起。评分尺度上有一个绿色勾号，表示用户对系统的评分。在评分尺度下方，有一个卡通形象，可能代表裁判员或权威人士，手持法槌，表明对评分结果进行裁决。幻灯片底部有 Emory University 的标志和 Alexa 的标志，表明该研究或演示与这两个实体有关。</sample>
    <sample id="1027">对话质量的维度

对话质量

相关性
一致性
情感理解

来源：爱默生大学，亚马逊Alexa</sample>
    <sample id="1028">Likert Rating Evaluation

在左侧，有一个卡通人物代表用户。右侧有两个机器人卡通人物，它们正在进行对话。在用户和机器人之间有一条线连接，线上有几个点，其中一点被勾选。在用户和机器人之间还有一条线连接，线上有几个点，其中一点被勾选。

在底部，有一个表格，表格中有一个卡通人物拿着法槌。表格的标题是“评估机器人回应的相关性”，表格中有5个点，从1到5，其中一点被勾选。

在右侧，有一个视频窗口显示一个男人。</sample>
    <sample id="1029">这张图片展示了一个 Likert 量表评估聊天机器人 (ABC-Eval) 的过程。它描绘了一个人类评估员与机器人互动的过程，评估机器人回应的相关性。评估员首先与机器人进行对话，然后根据响应的相关性对它们进行评分，评分范围从1到5。评分完成后，评估员在 Likert 量表上做出最终决定。该过程旨在确保机器人提供的信息准确且相关。</sample>
    <sample id="1030">在聊天中注释行为（ABC-Eval）</sample>
    <sample id="1031">幻灯片展示了“ABC-Eval行为”主题，分为四个部分。顶部左侧的标签为“连贯性”，右侧为“知识”。底部左侧的标签为“一致性”，右侧为“情感理解”。幻灯片底部左侧显示了Emory University的标志，底部右侧显示了Alexa的标志。</sample>
    <sample id="1032">ABC-Eval 行为

- 出现行为
- 一致性
- 知识
- 情感理解</sample>
    <sample id="1033">幻灯片标题为“ABC-Eval行为”，分为四个部分，每个部分代表一种行为类型。这些部分如下：

1. **连贯性**：包括两个子类别，“忽视伙伴”和“不相关”。
2. **一致性**：这个部分是空的，没有子类别。
3. **知识**：这个部分也是空的，没有子类别。
4. **情绪理解**：这个部分同样为空白，没有子类别。

幻灯片底部有Emory University和Alexa的标志。</sample>
    <sample id="1034">幻灯片标题为“ABC-Eval行为”，展示了评估对话或互动的四个关键方面。每个方面都包含两个子类别，详细说明了在这些行为中可能存在的具体问题或错误。

1. **连贯性**：这个方面关注对话或互动的一致性和连贯性。
	* 忽略伙伴：这表明未能回应或考虑合作伙伴的贡献。
	* 不相关：这表示提供的信息或观点与当前讨论的主题无关。
2. **一致性**：这个方面涉及对话或互动中的一致性。
	* 自我矛盾：这表明提供了相互冲突的信息或观点。
	* 合作伙伴矛盾：这表示合作伙伴之间存在不一致或相互矛盾的陈述。
3. **知识**：这个方面评估对话或互动中的知识水平。
	* 错误事实：这表示提供了不准确或误导性的信息。
	* 违反常识：这表明所提供的信息违反了普遍接受的事实或逻辑。
4. **情感理解**：这个方面关注对话或互动中对他人情感和情绪的理解。
	* 同情回应：这表明未能以同情心回应他人的感受。
	* 缺乏同情心：这表示缺乏对他人情感和情绪的敏感性。

幻灯片底部显示了与Emory大学和Alexa相关的标志，表明这些内容可能是Emory大学与Alexa合作开发的。</sample>
    <sample id="1035">实验

* 4个开放领域对话模型
* 每个模型100个人机对话
* ABC-Eval

该幻灯片展示了实验的概述，重点是评估开放领域对话模型。它提到了四个不同的对话模型，并说明每个模型都进行了100次人机对话。此外，幻灯片还提到了一个名为ABC-Eval的评估框架或方法。在幻灯片的右上角，有一个男人的图片，可能是演讲者或相关人物。幻灯片底部显示了Emory University和Alexa的标志。</sample>
    <sample id="1036">实验

* 4个开放领域对话模型
* 每个模型100个人工-机器人对话

| 模型 | 测试 | 评估 |
| --- | --- | --- |
| ABC-Eval |  |  |
| Turn Likert |  |  |
| Dialogue Likert |  |  |
| Comparative |  |  |

来源：约翰·霍普金斯大学

Alexa</sample>
    <sample id="1037">基线评估

在图表中，我们看到一个对话的流程图，其中包含多个参与者之间的互动。流程图显示了对话的进展，每个参与者都用不同的颜色表示。参与者之间的对话通过箭头连接，表示他们之间的交流。

流程图的底部有几个标签，包括“一致性”、“情感理解”、“信息丰富性”和“总体质量”。这些标签表明对话的评估标准。此外，流程图的右侧有一个表格，其中包含各种因素，如“参与度”、“语法正确性”、“相关性”和“比较”。

流程图的底部有两个标志，一个写着“Kip”，另一个写着“Emory University”。这些标志可能表示流程图的来源或创建者。

总的来说，这张图片似乎是一个对话的可视化表示，其中包含了对话的进展、参与者的互动以及对话的评估标准。</sample>
    <sample id="1038">这张图片是一张图表，标题为“注释员间一致性”，展示了不同评估方法之间的Krippendorff Alpha值。图表左侧有一个Y轴，表示Krippendorff Alpha值，从0.0到0.6。X轴上列出了各种评估方法，包括“主题一致”、“ABC-Eval”、“Turn Likert”、“对话Likert”和“比较性”。每个评估方法都有一个垂直线，代表其Krippendorff Alpha值，以及一个误差棒，表示数据的变异性。

在图表的右侧，有一个小插图，显示了一位女性站在一个大屏幕前，屏幕上显示了与图表相同的信息。这个插图表明，Krippendorff Alpha值较低的评估方法（如“主题一致”、“ABC-Eval”、“Turn Likert”、“对话Likert”和“比较性”）可能需要改进或重新评估，以确保更一致和可靠的注释结果。</sample>
    <sample id="1039">这张图片展示了一个柱状图，标题为“预测效度”，展示了不同评估方法下的质量解释百分比。图表的x轴代表了各种评估方法，包括“其他CS”、“内容”、“情感”、“意图”、“角色”、“观点”、“主题”、“实体”、“属性”、“ABC-Eval”、“Turn Likert”、“对话Likert”和“比较”。y轴表示“质量解释（%）”，从0.10到0.02不等。

图表中使用了不同的颜色来区分“交互式Q&amp;A”和“非交互式Q&amp;A”的结果。柱状图显示了每种评估方法下两种类型的Q&amp;A的质量解释百分比。例如，“内容”评估方法在“交互式Q&amp;A”下显示了较高的质量解释百分比，而在“非交互式Q&amp;A”下则较低。类似地，“情感”评估方法在“交互式Q&amp;A”下也显示了较高的质量解释百分比。

图表底部有Emory University和Alexa的标志，表明这些数据可能与这些机构有关。</sample>
    <sample id="1040">预测效度

这张柱状图展示了不同评估方法（如ABC-Eval、Turn Likert、Dialogue Likert和Comparative）下，各种语言特征（如内容、CS控制、情感、直接性等）的预测效度。黄色箭头指向了图表中具有显著预测效度的语言特征。</sample>
    <sample id="1041">这张幻灯片展示了对话评估的进展，特别关注了“增量有效性”这一概念。它包括一个图表，显示了在不同对话评估方法中解释的质量百分比。图表分为三个主要部分：ABC-Eval、Turn Likert和Dialogue Likert。每个部分都描绘了质量解释的百分比，从0.075到0.250。图表还标记了几个关键点，如“同理心”、“自我控制”、“冗余”、“移情”、“情感”和“相关性”，这些点代表了评估过程中考虑的不同方面。

幻灯片的左下角有一个标志，上面写着“EMORY”，表明与该机构有关。右下角有一个标志，上面写着“ALEXA”，可能表示与亚马逊Alexa相关的公司或产品。幻灯片的右上角有一个男人的照片，可能是演讲者或主题专家。幻灯片的整体设计是专业的，使用了蓝色和白色的配色方案，使信息清晰易读。</sample>
    <sample id="1042">在幻灯片的右上角，有一个男人的图片。幻灯片的主要内容是一个图表，图表的x轴代表“ABC-Eval”，y轴代表“% of Quality Explained (R^2)”。图表显示了不同对话评估方法之间的关系。图表的左下角有一个标签，上面写着“Emory University”和一个标志。图表的右下角有一个标签，上面写着“Alexa”和一个标志。</sample>
    <sample id="1043">图1显示了对话系统中不同评估方法的增量效度。该图展示了在对话系统中使用不同评估方法时，质量解释的百分比。该图显示了在对话系统中使用不同评估方法时，质量解释的百分比。该图显示了在对话系统中使用不同评估方法时，质量解释的百分比。</sample>
    <sample id="1044">ABC-Eval错误率按模型</sample>
    <sample id="1045">ABC-Eval错误率按模型</sample>
    <sample id="1046">ABC-Eval 错误率分析

该图表展示了不同模型在处理各种对话错误类型时的性能。图表标题为“ABC-Eval 错误率分析”，并列出了以下错误类型：反社会、CS 反对、忽略、不正确、无关、不关心、其他反向、冗余、自我反向和主题切换。图表底部显示了几个模型名称，包括 BART-FID-RAG、Blender2、Emora 和 Blender-Decode。

图表的 x 轴表示不同的错误类型，y 轴表示错误发生的百分比。每个错误类型都有多个柱状图，代表不同模型的性能。例如，在“反社会”类别中，BART-FID-RAG 模型的错误率最低，而 Blender-Decode 模型的错误率最高。类似地，在“CS 反对”类别中，Blender2 模型的错误率最低，而 Emora 模型的错误率最高。

图表的左下角显示了爱默里大学和 Alexa 的标志。图表的右上角有一个视频会议窗口，显示了一个女性正在演讲。</sample>
    <sample id="1047">ABC-Eval错误率按模型</sample>
    <sample id="1048">根据幻灯片显示，论文的作者来自以下机构：爱默里大学、爱默里NLP实验室和亚马逊Alexa研究实验室。这些机构的标志和名称在幻灯片底部可见，表明了与这些机构的关联。</sample>
    <sample id="1049">在幻灯片中，CFT代表“连续微调”，这是一种机器学习技术，其中模型在初始训练后继续在新数据上进行微调。</sample>
    <sample id="1050">根据图片显示，这篇论文有七位作者。</sample>
    <sample id="1051">当翻译需要语境吗？一项数据驱动的多语言探索

卡耐基梅隆大学
计算机科学学院
人工智能研究实验室

技术 Lisbon
BAIR

Unbabel</sample>
    <sample id="1052">翻译取决于上下文 我们得 rid of 那个 mole.</sample>
    <sample id="1053">翻译取决于语境 事情可能会变得危险 如果大臣们发现了。 我们必须 rid of that mole.</sample>
    <sample id="1054">翻译取决于语境。它会很严重吗，医生？我们得去掉那个痦子。</sample>
    <sample id="1055">评估上下文相关的翻译很困难。</sample>
    <sample id="1056">评估上下文相关的翻译很难

- 只有很少一部分单词依赖于上下文
  - 语料库级别的指标
- 现有的方法只支持有限的 discourse 现象和语言</sample>
    <sample id="1057">RQ1: 何时翻译需要背景？ RQ2: 模型如何处理背景相关的翻译？</sample>
    <sample id="1058">Q1: 何时翻译需要语境？
- 单词级别的语境使用

Q2: 模型如何处理依赖于语境的翻译？</sample>
    <sample id="1059">Conditional Cross-Mutual Information (CXMI)</sample>
    <sample id="1060">条件交叉互信息(CXMI) CXMI:衡量MT模型在给定语料时使用的上下文量。不确定性给定源翻译的上下文 HqMTc(Y|X,C) 从给定源和上下文的翻译不确定性中得出的不确定性 CXMI(C)→Y|X</sample>
    <sample id="1061">Pointwise (P-)CXMI

我们引入P-CXMI来衡量特定翻译的上下文使用情况。</sample>
    <sample id="1062">翻译何时需要语境？ - 词级语境使用 - 意义分析 模型如何处理依赖于语境的翻译？</sample>
    <sample id="1063">Q1: 何时翻译需要背景？
- 词级背景使用
- 主题分析
Q2: 模型如何处理背景相关的翻译？</sample>
    <sample id="1064">主题分析高P-CXMI单词</sample>
    <sample id="1065">主题分析高P-CXMI词
1. 词性标注</sample>
    <sample id="1066">主题分析高P-CXMI单词

1. 词性标注（POS tags）

| 词性标注 | P-CXMI |
| --- | --- |
| PRON.3.Sing | 0.26 |
| PRON.3.Dual | 0.24 |
| PRON.3.Plur | 0.26 |

- 代词</sample>
    <sample id="1067">主题分析高P-CXMI词1. 词性标注2. 词汇项-代词-动词形式</sample>
    <sample id="1068">阿维利的母亲仍在睡觉。阿维利去上学了。</sample>
    <sample id="1069">阿维利的母亲仍在睡觉。阿维利去上学了。</sample>
    <sample id="1070">1. 词性标注
2. 词汇项
3. 单个词元

她知道我们在哪。
我不知道。
Ich weiß, wo wir gehen.
Ich weiß es nicht。</sample>
    <sample id="1071">Q1: 何时翻译需要语境？
- 词级语境使用
- 主题分析
Q2: 模型如何处理依赖于语境的翻译？
- 多语言 discourse-aware (MuDA) 测试</sample>
    <sample id="1072">多语言 discourse-aware (MuDA) 标记器

- 指代词
- 动词形式
- 词汇连贯性
- 正式性
- 省略</sample>
    <sample id="1073">多语种 discourse-aware(MuDA)标记</sample>
    <sample id="1074">MuDA benchmark 机器翻译后MuDA benchmark</sample>
    <sample id="1075">Q1: 何时翻译需要语境？
- 词级语境使用
- 主题分析
Q2: 模型如何处理依赖于语境的翻译？
- 多语言 discourse-aware（MuDA）基准测试
- 模型评估</sample>
    <sample id="1076">Corpus-level metrics  BLEU</sample>
    <sample id="1077">Corpus-level metrics  BLEU  COMET  F-measure</sample>
    <sample id="1078">Corpus-level metrics  BLEU  COMET  F-measure  It is unclear which system is best for document-level MT with corpus-level metrics</sample>
    <sample id="1079">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion</sample>
    <sample id="1080">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1081">MuDA基准测试结果

- 上下文感知模型在某些现象上表现显著优于
  - √: 正式性、词汇连贯性
  - ×: 词形变化、代词、动词形式
- DeepL 在大多数语言对上超越 Google
  - DeepL &gt; Google

* 截止于2021年4月</sample>
    <sample id="1082">摘要

* 无需 prior 语言知识系统地识别 discourse 现象
* 基于文档的机器翻译的无数据集基准

从左到右的流程图显示了 MuDA 分析器、BLEU 和 F-measure 计算器以及机器人接收器之间的关系。</sample>
    <sample id="1083">摘要

* 无需 prior linguistic knowledge，系统地识别 discourse phenomena
* 基于文档级别的机器翻译的无数据集基准

从左到右的流程图显示了MuDA的输入和输出。MuDA输入是一组文档，输出是用蓝色方块标记的文档。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。</sample>
    <sample id="1121">根据图片中提供的信息，没有明确说明新方法的名称。因此，答案是没有名称。</sample>
    <sample id="1122">根据幻灯片上提供的内容，作者描述“显性词汇”(marked words) 方法为找到区分标记群体和未标记群体的单词。这表明该方法涉及识别那些能够指示特定群体或类别特征的特定术语或短语。</sample>
    <sample id="1123">根据幻灯片显示，论文的作者分别来自以下机构：张方（Shangbin Feng）来自宾夕法尼亚大学（University of Pennsylvania），朴善英（Chan Young Park）来自德克萨斯大学奥斯汀分校（University of Texas at Austin），刘雨欣（Yuhuan Liu）来自卡内基梅隆大学计算机科学与语言技术研究所（Carnegie Mellon University, Language Technologies Institute），以及尤利娅·特sv科娃（Yulia Tsvetkov）来自圣母大学（University of Notre Dame）。</sample>
    <sample id="1124">在提到的对称依存关系结构中，第一个包含城市名称的结构是'Multi-headed/London:'。</sample>
    <sample id="1125">演讲者的名字是Sarah E. Finch、James D. Finch和Jinho D. Choi。</sample>
    <sample id="1126">根据幻灯片上显示的英文信息，这篇论文有四位作者。</sample>
    <sample id="1127">BLMP、SyntaxGym和CrowS数据集可用于测试句法现象。</sample>
    <sample id="1161">根据图片中显示的图表，英语内容的五种方法的缩写分别是：FT（Fine-Tuning）、W（可能代表Weak Labels或Weighting）、BOND、COSINE和MLC（可能是Multi-Label Classification）。这些缩写在x轴上可见，表示了图表中评估的不同方法。</sample>
    <sample id="1162">根据幻灯片中提供的信息，该模型在11个任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在大规模的法语数据上训练的，如维基百科和书库。</sample>
    <sample id="1227">演讲者的名字是Adam Przpherdowski和Michat Wozniak。</sample>
    <sample id="1228">根据幻灯片中呈现的英文内容，导致时间漂移是性能下降的主要原因的发现包括：1.在时间序列上存在显著的正斜率，表明性能随时间逐渐下降。2.在不同时间间隔（如2009年和2018年）之间存在明显的趋势，表明性能随时间逐渐下降。3.表格中的数据点显示了在多个时间间隔内性能指标的持续下降。这些发现支持了时间漂移是导致性能下降的主要原因的观点。</sample>
    <sample id="1269">排列输出序列中的词元是必要的，以确保生成的文本符合自然语言的语法规则和连贯性。在自然语言处理任务中，如机器翻译、文本生成或问答系统中，正确的排列对于使生成的文本易于理解且有意义至关重要。排列过程确保了语法正确，单词顺序符合语言习惯，并且句子传达了预期的信息。</sample>
    <sample id="1270">根据幻灯片，建议模型所有者提高偏见缓解方法的透明度的原因可能是为了确保这些方法是可理解和可验证的。这可以增强对模型公平性的信任，并有助于识别和解决潜在的偏见。通过提供清晰的解释和数据，其他研究者和利益相关者可以评估和复制这些方法，促进更广泛的改进和改进。</sample>
    <sample id="1271">最小对不可接受输入（MPP）是一种评估语言模型性能的方法，它涉及比较模型在序列中连续出现的两个词的概率。如果一个词的概率大于另一个词的概率，则认为该输入是不可接受的。这个方法用于评估模型对抽象知识的理解，因为模型应该能够根据其训练数据推断出概率。</sample>
    <sample id="1272">根据表格中提供的英文内容，作者使用了以下评估指标：准确率（ACC）、精确率（P）和召回率（R）。这些指标通常用于评估分类任务的性能，其中准确率表示正确预测的样本比例，精确率表示预测为正类的样本中实际为正类的比例，召回率表示实际为正类的样本中被正确预测为正类的比例。</sample>
    <sample id="1273">一致性是通过使用Krippendorff Alpha指标来衡量的。这个指标在图表中被突出显示，位于图表的顶部，用橙色箭头连接到表示一致性的方框中。</sample>
    <sample id="1274">在不可接受和可接受查询中，选择“空间”领域来添加完全无关的句子。</sample>
    <sample id="1275">根据幻灯片显示，论文的作者来自海因里希·海因里希大学（Heinrich Heine University），位于德国杜塞尔多夫。</sample>
    <sample id="1276">根据幻灯片上显示的英文内容，MultiInstruct 似乎是一种专门针对语言输入进行优化的方法或系统。它与基准不同之处在于其专注于处理语言输入，这表明它可能在理解、解释或生成文本方面具有独特的优势或能力。这种专注可能使其在某些任务中比其他基准更有效或准确，特别是那些涉及自然语言处理（NLP）的应用。</sample>
    <sample id="1277">根据幻灯片显示，论文《Chat-Oriented Dialogue Systems》的作者共有三位：Sarah E. Finch、James D. Finch和Jinho D. Choi。</sample>
    <sample id="1278">根据提供的图片，二进制协调的定义似乎指的是在不同长度的文本或数据中，字符、音节和单词数量之间的差异。这种差异可能通过图表中的蓝色线表示，该线显示了随着长度增加而逐渐增加的趋势。这种趋势表明，随着文本长度的增加，字符、音节和单词数量之间的差异也在增加。</sample>
    <sample id="1279">在本研究中，提示语的平均长度是多少？</sample>
    <sample id="1280">这些发现表明，较小的 T5 模型通过在 Coreset 上进行微调，可以实现与大型语言模型相当甚至超越的质量。这可能意味着，在某些任务中，使用较小、更专注的模型可能是有效的，因为它们可能提供相似的性能，同时具有更高的效率和较低的资源需求。</sample>
    <sample id="1281">DrBERT: 一种在生物医学和临床领域中具有稳健预训练模型的法文</sample>
    <sample id="1282">I. Healthcare中的语言建模II. 预训练策略、数据来源和规模的比较III. 13个模型在11个任务上的评估IV. NACHOS和DrBERT的分布Avignon Université</sample>
    <sample id="1283">I. 医疗保健中的语言建模II. 预训练策略、数据来源和大小的比较III. 13个模型在11个任务上的评估IV. NACHOS和DrBERT的分布</sample>
    <sample id="1284">I. 医疗保健中的语言建模II. 预训练策略、数据来源和大小的比较III. 13个模型在11个任务上的评估IV. NACHOS和DrBERT的分布</sample>
    <sample id="1285">I. Healthcare中的语言建模II. 预训练策略、数据来源和大小的比较III. 13个模型在11个任务上的评估IV. NACHOS和DrBERT的分布Avignon Université</sample>
    <sample id="1286">Transformer-based approaches, such as BERT, offer significant performance gains on various NLP tasks. These models have been adapted to French using CamemBERT and FlauBERT. For medical tasks, domain-specific models in English have raised the bar even higher, with examples including PubMedBERT, BioBERT, ClinicalBERT, and others. Languages other than English are rarer and primarily rely on continual pre-training using existing generic models. Currently, no open-source model is available for the biomedical domain in French, but a BERT-based domain-specific model for French should increase performance on medical tasks.</sample>
    <sample id="1287">基于Transformer的方法，如BERT，为各种NLP任务提供了巨大的性能提升。它已被应用于法语的CamemBERT和FlauBERT。在医学任务中，针对特定领域的模型在英语上进一步提高了性能，例如PudMedBERT、BioBERT和ClinicalBERT等。除了英语之外，其他语言的通用模型主要依赖于现有的开源模型进行持续预训练。目前，在生物医学领域中，尚未有针对法语的开源通用模型。BERT-based的针对法语的领域特定模型应该能显著提高医学任务上的性能。</sample>
    <sample id="1288">基于Transformer的方法，如BERT，为许多NLP任务提供了巨大的性能提升。它们已被应用于法语的CamemBERT和FlauBERT。在医学任务上，针对特定领域的模型在英语中进一步提升了性能，例如PudMedBERT、BioBERT和ClinicalBERT等。除了英语之外，其他语言的模型主要依赖于使用现有通用模型的持续预训练。目前，在生物医学领域尚未提供开源的法语通用模型。BERT-based针对特定领域的模型对于法语应该能显著提高医学任务上的性能。</sample>
    <sample id="1289">基于Transformer的方法，如BERT，为许多NLP任务提供了巨大的性能提升。它们已被适应为法语的CamemBERT和FlauBERT。在医学任务上，领域特定的模型在英语中进一步提高了性能，例如PudMedBERT、BioBERT和ClinicalBERT等。除了英语以外，其他语言的通用模型主要依赖于现有的泛用模型进行持续预训练。目前，在生物医学领域中，尚未提供开源的泛用模型用于法语。BERT-based领域特定模型对于法语应该能提高医学任务上的性能。</sample>
    <sample id="1290">比较预训练策略和数据来源

* 评估公共和私人医疗数据来源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多任务医疗数据集，涵盖各种医疗领域、风格和性质
	+ 私人数据集：从1.7百万份匿名化医疗记录中提取的句子，来自南特大学医院数据仓库
	+ 比较学习策略
		- 从头开始：使用完整构建
		- 继续预训练：使用现有预训练模型
		- 示例：Camembert，一种法国通用模型，和PubMedBERT，一种医学特定模型

该幻灯片展示了比较不同预训练策略和数据来源对NACHOS数据集的影响。它详细介绍了NACHOS数据集的特点，并比较了不同的学习策略，如从头开始、继续预训练和使用特定模型（如Camembert和PubMedBERT）。</sample>
    <sample id="1291">比较预训练策略和数据来源

* 评估公开和私有医学数据源的影响力
	+ NACHOS：一个1.1亿词开源多领域医学语料库，包含来自不同来源的多样化数据，如NCT、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl</sample>
    <sample id="1292">比较预训练策略和数据来源

* 评估公共和私人医疗数据来源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多领域医疗语料库，包含来自不同来源的多样化数据，如NCT、Medl、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI、NCBI</sample>
    <sample id="1293">比较预训练策略和数据来源

* 评估公共和私人医疗数据来源的可比数据大小的影响
	+ NACHOS：一个1.1百万词开源多领域医疗数据集，包含来自不同来源的多样性和风格
	+ 私人数据集：从1.7百万份匿名化医疗记录中提取的句子，来自南特大学医院数据仓库
	+ 比较学习策略
		- 从头开始：使用完整构建的模型
		- 继续预训练：使用现有预训练模型（如BERT、RoBERTa、CamemBERT、PubMedBERT和NBDW）
		- 公共预训练：使用NACHOS数据集进行预训练

该幻灯片展示了比较不同预训练策略和数据来源对模型性能的影响。它强调了NACHOS数据集的重要性，以及它在预训练中的作用。</sample>
    <sample id="1294">比较预训练策略和数据来源

* 评估公共和私有医学数据来源的可比数据量的影响
	+ NACHOS：一个1.1亿词的开源多领域医学语料库，包含来自不同来源的多样化数据，如NCT、Medl、NCBI等。数据以多种风格和性质呈现。
	+ 私有数据集：从1.7亿篇匿名化医疗记录中提取的句子，这些记录来自南特大学医院数据仓库。
* 比较学习策略
	+ 从头开始：使用完整构建的模型
	+ 继续预训练：使用现有预训练模型，如Camembert、Chabert、PubMedBERT和NBDWBERT
	+ 公开MedBERT：一个法国通用模型

| 模型名称 | 来源 | NACHOS | DfBERT | NBDW |
| --- | --- | --- | --- | --- |
| NACHOS | 从头开始 |  |  |  |
| DfBERT | 继续预训练 |  |  |  |
| NBDW | 继续预训练 |  |  |  |

来源：Avignon Université</sample>
    <sample id="1295">比较预训练策略和数据来源

* 评估公共和私有医疗数据来源的差异
	+ NACHOS：一个1.1亿词的开源多领域医疗语料库，包含来自不同医疗领域的多样化数据，如NCT、Medl、ICD、SNOMED CT和LOINC。
	+ 私有数据集：从1.7亿份匿名化医疗记录中提取的句子，这些记录来自南特大学医院数据仓库。
* 比较学习策略
	+ 从头开始：使用完整数据集进行全量构建。
	+ 继续预训练：使用现有的预训练模型进行微调，如Camembert、Chabert、PubMedBERT和NBDWBERT。

表格展示了不同模型的比较，包括它们的来源和预训练数据。</sample>
    <sample id="1296">比较预训练策略和数据来源

* 评估公共和私人医疗数据来源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多任务医疗数据集，涵盖各种医疗领域、风格和性质
	+ 私人数据集：从1.7M匿名化医疗记录中提取的句子，来自南特大学医院数据仓库
	+ 比较学习策略
		- 从头开始：使用完整构建的模型
		- 继续预训练：使用现有预训练模型（如CamBERT、ChuBERT、PubMedBERT和NBDW）
		- 公共数据：使用CamBERT和NBDW等公开数据

该幻灯片展示了比较不同预训练策略和数据来源对模型性能的影响。它详细介绍了NACHOS数据集的结构和特点，并比较了从头开始训练、继续预训练和使用公开数据的不同方法。</sample>
    <sample id="1297">比较预训练策略和数据来源

* 评估公共和私人医疗数据来源的可比数据量的影响
	+ NACHOS：一个1.1百万单词的开源多领域、多风格、多主题的医疗数据集
	+ 私人数据集：从1.7百万份匿名化医疗记录中提取的句子，来自南特大学医院数据仓库
* 学习策略的比较
	+ 从头开始：使用完整构建的模型
	+ 继续预训练：使用现有预训练模型进行微调，如Camembert、Chabert和PubMedBERT
	+ 公开：使用Camembert和NBDW等公开预训练模型

| 模型名称 | 来源 | NACHOS | NBDW |
| --- | --- | --- | --- |
| DeBERTa | 从头开始 | 7.4GB | 655M |
| RoBERTa | 从头开始 | 7.4GB | 655M |
| Camembert | 从头开始 | 4.4GB | 43.1M |
| Chabert | 从头开始 | 4.4GB | 6.4M |

Avignon Université</sample>
    <sample id="1298">评估13个数据源和大小的性能评估：我们的微调模型在几乎所有的任务上达到最新的艺术水平。表中显示了在不同数据集上的性能结果，包括MUSCA-Report、MUSCA-DiET、ISSA、CAS、QuAD和MEDL。这些数据集分为一般、生物医学和临床类别。在每个类别中，我们比较了不同的模型，如GPT-4、RoBERTa-Large、BERT-C4和DeBERTa-NACOS。结果表明，GPT-4在所有数据集中都表现出色，特别是在MUSCA-Report和MUSCA-DiET等任务上。</sample>
    <sample id="1299">评估：13个数据来源和大小的评估。我们的性能评估涵盖了几乎所有的艺术作品。我们对13个数据来源进行了详细分析，包括它们的大小、来源和任务。这些数据来源包括公开和私有数据集，如MUSCA-DET、MUSCA-DET-S、CAS、CAS-MoCQ、QUAERO-EMEA和QUAERO-MEDL。我们使用了不同的模型，如BERT、RoBERTa和DeBERTa，以及不同的微调策略，如微调和冻结。我们还考虑了不同的任务，如图像分类、目标检测和语义分割。我们的结果表明，微调模型在大多数任务上表现良好，而冻结模型在某些任务上表现不佳。我们还发现，较大的数据集通常会带来更好的性能，但也有例外情况。</sample>
    <sample id="1300">在幻灯片的右上角，有一个小视频窗口显示了一个男人坐在书架前。幻灯片的标题为“13个数据源和大小的评估”，并详细说明了性能评估涉及11个公共和私有任务。此外，还提到了对艺术作品的微调以获得几乎最好的表现。

表格展示了来自不同来源的数据集的性能比较。这些数据集包括：

- **General**:
  - **CinIC**: 79.6%
  - **DeBERTa Oscar 150 GB**: 82.4%
  - **CinBERT CUNet 4 GB**: 81.4%
  - **RoBERTa V1**: 79.0%
  - **CinBERT NACOS**: 87.7%
  - **DeBERTa NACOS**: 87.7%

- **Biomedical**:
  - **CinBERT Bio**: 89.3%
  - **CinBERT BioNLP**: 89.3%
  - **CinBERT BioNLP**: 89.3%
  - **CinBERT BioNLP**: 89.3%
  - **CinBERT BioNLP**: 89.3%
  - **CinBERT BioNLP**: 89.3%

- **Clinical**:
  - **CinBERT Clinical**: 87.7%
  - **CinBERT Clinical**: 87.7%
  - **CinBERT Clinical**: 87.7%
  - **CinBERT Clinical**: 87.7%
  - **CinBERT Clinical**: 87.7%
  - **CinBERT Clinical**: 87.7%

表格还列出了每个数据集的微调后的性能，显示了显著的提升。例如，对于“CinBERT BioNLP”数据集，微调后的性能从89.3%提高到92.5%。</sample>
    <sample id="1301">评估：13个数据源和大小的评价

* 我们的微调模型在几乎所有的艺术任务上达到最新水平。
* 用于性能评估的模型
| 模型 | 医学报告专门知识 | MUSIC | MUSIC-DiT | ESSA | CAS | CAS-MoC | QUADRE-EMEA | QUADRE-MEDL |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| CanonBERT Oscar 150 GB | 92.4 | 91.7 | 93.4 | 91.2 | 89.0 | 90.6 | 90.3 | 76.1 |
| CanonBERT CUNet 4 GB | 82.6 | 81.8 | 83.4 | 81.8 | 79.4 | 80.6 | 80.3 | 76.1 |
| BioBERT v1.1 | 90.0 | 89.3 | 90.8 | 89.6 | 87.9 | 88.9 | 84.5 | 72.6 |
| BioBERT NACOS | 85.9 | 85.1 | 87.4 | 85.6 | 83.6 | 84.6 | 83.7 | 75.0 |
| ClinicalBERT SNOW | 93.5 | 92.7 | 94.4 | 93.6 | 91.7 | 92.6 | 91.6 | 75.0 |
| ChatBERT SNOW (untrained) | 94.1 | 93.8 | 95.1 | 94.7 | 92.7 | 93.5 | 92.1 | 76.3 |

这张表格展示了不同医学报告模型在各种任务上的性能。它包括了CanonBERT Oscar 150 GB、CanonBERT CUNet 4 GB、BioBERT v1.1、BioBERT NACOS、ClinicalBERT SNOW和ChatBERT SNOW (untrained)等模型。这些模型在MUSIC、MUSIC-DiT、ESSA、CAS、CAS-MoC、QUADRE-EMEA和QUADRE-MEDL等任务上进行了评估。表格显示，CanonBERT Oscar 150 GB在MUSIC和MUSIC-DiT任务上表现最佳，而ChatBERT SNOW (untrained)在CAS和CAS-MoC任务上表现最佳。总体而言，这些模型在所有任务上都表现出较高的性能，表明它们在处理医学报告方面具有强大的能力。</sample>
    <sample id="1302">从头开始 vs. 在4GB数据上进行持续预训练：该表格展示了在大规模预训练后，使用Camembert模型在不同NLP任务上的性能。它比较了在大规模文本语料库上进行持续预训练的模型与从头开始训练的模型之间的性能差异。表格包括各种任务，如命名实体识别（NER）、情感分析（Senti）、问答（QA）和文本分类（TextCat）。结果表明，在大规模预训练后，模型在这些任务上的表现显著提高，展示了持续预训练对模型性能的积极影响。</sample>
    <sample id="1303">从头开始 vs. 在4GB数据上进行持续预训练：比较了两种方法在处理各种任务时的表现。研究结果表明，在大规模数据集上进行持续预训练可以显著提高模型性能。例如，使用持续预训练的模型在NER（命名实体识别）和SRL（语义角色标注）等任务上的性能明显优于从头开始训练的模型。此外，持续预训练还可以提高模型的稳定性和泛化能力。</sample>
    <sample id="1304">从头开始 vs. 在4GB数据上进行持续预训练：比较了两种不同预训练方法在微调时的表现。研究结果表明，使用持续预训练的模型具有更高的泛化能力。表格展示了不同预训练方法下多个模型的性能对比。</sample>
    <sample id="1305">幻灯片的中文翻译内容如下：

核心信息
- DrBERT在9项主流法语医学导向任务中超过了Camembert通用模型。
- 训练特定于医学领域的模型在法语中具有实用性。
- 数据来源很重要；在异质数据上进行训练很重要。
- NACHOS比仅使用临床数据更稳健。
- 持续预训练效果更好，但不具有可扩展性。
- DrBERT模型、NACHOS数据集和训练脚本在MIT许可证下免费提供。

幻灯片右上角有一个视频播放按钮图标，表示幻灯片包含视频内容。幻灯片底部有一个带有“drbert_univ-avignon.fr”网址的二维码，指向Avignon Université网站。

幻灯片的背景是红色和白色，底部有Avignon Université的标志。</sample>
    <sample id="1306">幻灯片的主要内容是关于DrBERT模型的性能和数据的重要性。它首先介绍了DrBERT模型在9项法语医学导向任务中超越了Camembert通用模型，这表明训练特定于医学领域的模型在法语中是有效的。接下来，幻灯片强调了使用异质数据进行训练的重要性，并指出NACHOS数据集比仅使用临床数据更稳健。最后，幻灯片提到了持续预训练的好处，但警告说，当基于特定领域的英语模型时，效果并不明显。幻灯片还提到了DrBERT模型、NACHOS数据集和训练脚本的可获得性。</sample>
    <sample id="1307">核心信息 DrBERT 在 9 项主流法语医学导向任务中超过了 CamemBERT，证明了在法语上训练医学特定模型的重要性。使用异质数据进行训练比仅使用临床数据更可靠。持续预训练效果显著，但当基于领域特定的英语模型时，效果较差。DrBERT 模型、NACHOS 数据集和训练脚本可免费获得，遵循 MIT 许可证。</sample>
    <sample id="1308">这张图片是一张幻灯片，上面有一个戴着护士帽的黄色卡通医生拿着注射器。旁边有一个对话框写着“谢谢”。幻灯片底部有一行文字写着“期待在多伦多海报交流会上与大家交流！”右下角有一个小视频窗口显示了一个男人坐在书架前。幻灯片底部还有一行文字写着“更多详细信息，请访问：drbert.univ-avignon.fr”，底部有一个标志写着“Avignon Université”。</sample>
    <sample id="1309">论文研究了以下学习策略：1. 从头开始：使用全量数据集进行完整构建。2. 持续预训练：使用现有的预训练模型CamMedBERT，一个法国通用模型，和PubMedBERT。</sample>
    <sample id="1310">仅凭图片本身无法确定过拟合因素的大小，因为这需要对数据进行更深入的分析，包括模型训练和验证的详细信息。然而，图表表明，随着CQL的F1分数增加，性能确实有所提高，但随后开始下降，这可能表明存在过拟合的情况。</sample>
    <sample id="1311">简化质量可以通过比较简化文本与原始文本之间的BLEU分数来评估，如表格中所示。BLEU分数是一种评估机器翻译输出准确性的指标，范围从0到100。较高的分数表示更准确的翻译或简化，表明简化质量更高。</sample>
    <sample id="1312">是的，语言模型（LMs）可能具有不同的政治偏见。这可以通过它们在生成文本时反映出的潜在政治观点或立场来表示，这些观点或立场可能反映了训练数据中的偏见或反映了模型设计者自己的观点。这种偏见可能会影响模型对某些主题或问题的处理方式，导致其输出反映特定的政治观点或价值观。</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations
Matthias Lindemann, Alexander Koller, Ivan Titov
The University of Amsterdam INLP Saarland University UvA</sample>
    <sample id="1314">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations
Matthias Lindemann, Alexander Koller, Ivan Titov</sample>
    <sample id="1315">学习者处理更深层次递归和在训练期间看到的单独短语的前所未见组合的能力。</sample>
    <sample id="1316">标题：组合泛化在语义解析中的应用

训练：

*The girl slept.*

| 'girl x; sleep.agent x, x' |
| --- | --- |
| 'Mary knew that the girl slept.' | 'girl x; know.agent x, Mary ∧ know.comp x, x' |

---

**Translation:**

标题：组合泛化在语义解析中的应用

训练：

*女孩睡了*

| '女孩 x; 睡觉的主体 x, x' |
| --- | --- |
| '玛丽知道女孩睡了。' | '女孩 x; 知道的主体 x, 玛丽 ∧ 知道的补语 x, x' |

---

**Explanation:**

The image shows a slide from a presentation on "Compositional Generalization in Semantic Parsing." The slide includes two sentences and their corresponding semantic representations. Here's a detailed breakdown:

1. **Sentence 1: "The girl slept."**
   - **Semantic Representation:** `girl x; sleep.agent x, x`
     - This representation indicates that the subject of the sentence is "the girl" (represented by `x`), and the action is "slept" (represented by `sleep.agent x, x`). The `agent` role is filled by the girl, and the `x` represents the same entity in both the subject and the action.

2. **Sentence 2: "Mary knew that the girl slept."**
   - **Semantic Representation:** `girl x; know.agent x, Mary ∧ know.cmp x, x`
     - This representation indicates that the subject of the sentence is "Mary" (represented by `Mary`), and the action is "knew" (represented by `know.agent x, Mary`). The `agent` role is filled by Mary. Additionally, there is a `know.cmp` (knowledge comparison) role that connects the knowledge of Mary to the fact that "the girl slept" (`know.cmp x, x`).

The slide demonstrates how semantic parsing can break down sentences into structured representations, allowing for better understanding and processing of language by computers.</sample>
    <sample id="1317">**标题：** 语义解析中的组合泛化

**训练：**

| 句子 | 依存关系树 |
| --- | --- |
| "The girl slept." | '女孩' - 睡觉的施事者 ('sleep_agent_x') |
| "Mary knew that the girl slept." | 'Mary' - 知道的施事者 ('know_agent_x') 和 '女孩' - 睡觉的施事者 ('sleep_agent_x') |

**解释：**

这个幻灯片展示了在语义解析中如何使用依存关系树来表示句子结构。它展示了两个句子及其对应的依存关系树：

1. **句子："The girl slept."**
   - '女孩'（The girl）是主语，与动词 'sleep' 相关联，作为施事者。
   - 依存关系树表示为：'女孩' - 睡觉的施事者 ('sleep_agent_x')。

2. **句子："Mary knew that the girl slept."**
   - 'Mary' 是主语，与动词 'know' 相关联，作为施事者。
   - '女孩'（The girl）是 '知道' 的宾语，与动词 'sleep' 相关联，作为施事者。
   - 依存关系树表示为：'Mary' - 知道的施事者 ('know_agent_x') 和 '女孩' - 睡觉的施事者 ('sleep_agent_x')。

这个例子展示了如何通过分析句子结构来识别主语、宾语和动词之间的关系，并将其表示为依存关系树。</sample>
    <sample id="1318">**标题：语义解析中的组合泛化**

**训练：**

| 句子 | 依存关系 |
| --- | --- |
| The girl slept. | '女孩' - 睡觉_主语_女孩; '睡觉' - 睡觉_动词_女孩 |
| Mary knew that the girl slept. | '玛莎' - 知道_主语_玛莎; '知道' - 知道_动词_玛莎; '女孩' - 睡觉_宾语_女孩; '睡觉' - 睡觉_动词_女孩 |

在这个幻灯片中，我们讨论了在语义解析中如何使用组合泛化。通过分析给定的句子，我们可以识别出主语、动词和宾语之间的依存关系。例如，在句子"The girl slept."中，'女孩'是主语，'睡觉'是动词。类似地，在句子"Mary knew that the girl slept."中，'玛莎'是主语，'知道'是动词，'女孩'是宾语，'睡觉'是动词。这些依存关系有助于我们更好地理解句子的结构和含义。</sample>
    <sample id="1319">### Composition Generalization in Semantic Parsing

#### Train:
- The girl slept.
  - 'The girl' (sleep. agent, 'the girl')
- Mary knew that the girl slept.
  - 'Mary' (know.agent, 'Mary')
  - 'the girl' (sleep.agent, 'the girl')

#### Test:
- Jim said that Mary knew that the girl slept.
  - 'Jim' (say.agent, 'Jim')
  - 'Mary' (know.agent, 'Mary')
  - 'the girl' (sleep.agent, 'the girl')</sample>
    <sample id="1320">训练：
女孩睡了。
“女孩”是绿色的，“睡了”是蓝色的。
玛丽知道那个女孩睡了。
“女孩”是绿色的，“睡了”是蓝色的，“知道”是橙色的，“玛丽”是蓝色的，“ccomp”是绿色的，“说”是蓝色的。
测试：
吉姆说玛丽知道那个女孩睡了。
“女孩”是绿色的，“睡了”是蓝色的，“知道”是橙色的，“吉姆”是蓝色的，“sayccomp”是绿色的，“玛丽”是蓝色的，“agent”是蓝色的，“知道”是橙色的，“ccomp”是绿色的，“女孩”是绿色的，“睡了”是蓝色的。
朴素的seq2seq模型失败！</sample>
    <sample id="1321">训练：女孩睡了。'女孩，睡觉，女孩，女孩'。Mary知道那个女孩睡了。'女孩，知道，Mary，睡觉，女孩，女孩'。测试：Jim说Mary知道那个女孩睡了。'女孩，说，Jim，Mary，睡觉，女孩，女孩'。'女孩，知道，Mary，睡觉，女孩，女孩'。朴素的seq2seq模型失败！</sample>
    <sample id="1322">树木帮助很大，但...</sample>
    <sample id="1323">树木帮助很大，但...</sample>
    <sample id="1324">树木帮了大忙，但...</sample>
    <sample id="1325">树木帮助很大但... *女孩x_{1}；sleep. agent x_{2} *女孩x_{1}；sleep. agent x_{2} *女孩x_{1}；sleep. agent x_{2} The girl slept. Trees need to be obtained： Pre/Post-processing logical forms</sample>
    <sample id="1326">树木帮助很大，但... *女孩x_{1}；sleep.代理x_{2}；x_{1} *女孩x_{j}；x_{k}；sleep.代理x_{z} The girl slept. 树木必需品： - 预/后处理逻辑形式 - 语法-归纳</sample>
    <sample id="1327">Trees help a lot but...</sample>
    <sample id="1328">Trees help a lot but...</sample>
    <sample id="1329">Our Approach the girl slept</sample>
    <sample id="1330">Our Approach the girl slept</sample>
    <sample id="1331">Our Approach the girl slept</sample>
    <sample id="1332">Our Approach the girl slept</sample>
    <sample id="1333">Our Approach</sample>
    <sample id="1334">排列“跳跃”</sample>
    <sample id="1335">Permuting with "jumps" Permute Tag the girl slept</sample>
    <sample id="1336">Permuting with "jumps"  Permute Tag the girl slept</sample>
    <sample id="1337">Permuting with "jumps"  Permute Tag the girl slept</sample>
    <sample id="1338">Permuting with "jumps" sleep agent the girl slept</sample>
    <sample id="1339">一些关于COGS（金和林zen 2020）的结果比较其他Treeless模型在结构泛化到COGS上的表现

图表展示了不同模型在处理COGS任务时的表现。图表中包括以下模型：

1. **LSTM seq2seq**：使用循环神经网络（RNN）的序列到序列模型。
2. **T**：未明确说明，但可能代表另一种序列到序列模型或特定的树状模型。
3. **Zheng and Lapata**：引用了 Zheng 和 Lapata 的研究，可能涉及一种特定的树状模型或方法。
4. **Ours**：表示当前研究中开发的模型。

图表的x轴表示不同的泛化类型，包括：

- **PP recursion**：指处理嵌套结构的递归解析。
- **GP recursion**：指处理嵌套结构的广义解析。
- **Obj + Sum PP**：表示对象和总结的嵌套结构解析。

y轴表示准确率（ACC），范围从0到80。

从图表中可以看出：

- 在“PP recursion”和“GP recursion”两种泛化类型下，“Ours”模型表现出最高的准确率，接近80%。
- 在“Obj + Sum PP”泛化类型下，“Ours”模型的准确率显著低于前两种类型，但仍高于其他模型。
- 其他模型（LSTM seq2seq、T、Zheng and Lapata）在所有泛化类型下的准确率都明显低于“Ours”模型。

总体而言，图表表明“Ours”模型在处理COGS任务时具有更好的泛化能力，尤其是在处理嵌套结构的递归解析和广义解析方面。</sample>
    <sample id="1340">一些关于COGS（Kim和Linzen 2020）的结果比较其他Treeless模型在结构泛化上的表现</sample>
    <sample id="1341">技术挑战我们解决

* girl x1
* sleep agent x2
* x1

Permute

?

?

?

Tag

the

girl

sleep</sample>
    <sample id="1342">技术挑战我们解决

* girl x1
* sleep agent x2
* 未知

Permute

?

?

?

Tag

the

girl

sleep</sample>
    <sample id="1343">技术挑战我们解决

* girl x1
? agent x2
? sleep agent x3
Permute
?
?
?
Tag
the
girl
sleep</sample>
    <sample id="1344">Technical Challenges We Solve

Permute

Alignment unknown.

Induce it in training.

Rotation model:</sample>
    <sample id="1345">技术挑战我们解决

Permute
Tag

对齐未知。在训练中引入它。
生成模型：推断是NP-hard（=TSP）</sample>
    <sample id="1346">技术挑战我们解决

图中的符号和术语可能与自然语言处理或机器学习有关，特别是涉及排列问题的上下文。以下是对给定文本的中文翻译：

技术挑战我们解决

- 求解未知排列。
- 在训练中引入它。
- 推断模型：
  - 计算复杂度：NP-hard（= TSP）
  - 通过连续放松反向传播。

这个幻灯片似乎是在讨论如何解决一个涉及排列问题的技术挑战，可能是在机器学习或自然语言处理的背景下。提到的“NP-hard”和“TSP”（旅行商问题）表明这个问题可能涉及到组合优化，而“连续放松”和“反向传播”则暗示了解决这个问题的方法可能涉及优化算法。</sample>
    <sample id="1347">认知失调是指个体在同时持有两个或多个相互矛盾或不一致的认知时所体验到的心理不适或紧张状态。这些认知可以是思想、行动或信仰。认知失调的概念由社会心理学家Eddie Harmon-Jones和Cindy Harmon-Jones提出，他们在2007年发表的论文中详细阐述了这一点。他们认为认知失调是在个体试图保持其信念系统的一致性时发生的，当一个人的行为与他们的价值观或信仰相冲突时，这种现象尤为明显。</sample>
    <sample id="1348">根据图表，最倾向于自由派的语言模型是“GPT-4”和“GPT-J”，它们位于图表的左下方象限，表明它们更接近自由派。</sample>
    <sample id="1349">在主动学习时，累积训练是否比迭代训练更有效？</sample>
    <sample id="1350">演讲者的名字是Sara Papi。</sample>
    <sample id="1351">MuDa基准中的数据是从WMT（国际机器翻译大会）和IWSLT（国际研讨会）等主要机器翻译数据集中获得的。</sample>
    <sample id="1385">演讲者的名字是马修斯·林登曼。</sample>
    <sample id="1386">跨语言转移指的是在一种语言上进行训练，然后将其应用于另一种语言的过程。在机器学习和自然语言处理的背景下，这通常涉及使用一个语言的模型来解决另一种语言的类似任务，而无需针对新语言重新训练模型。</sample>
    <sample id="1387">根据图片显示，这篇论文的作者分别来自三个机构：Saarland University、Amazon Alexa和University of Vienna。这些机构在作者名字后面通过数字标签1、2和3进行了标识，并且在图片底部的标志中得到了确认。</sample>
    <sample id="1388">根据图片中呈现的图表，作者使用了AL（注意力延迟）和AL-CA（注意力延迟-上下文注意力）两种延迟测量方法。这些方法在x轴上表示，范围从0.5到5秒。</sample>
    <sample id="1389">麦吉尔大学/米拉
微软研究
微软
麦克吉尔大学/米拉
微软研究
麦克吉尔大学/米拉</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge)</sample>
    <sample id="1391">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge)</sample>
    <sample id="1392">John saw the newly elected president on TV.</sample>
    <sample id="1393">John saw the newly elected president on TV. What presidents do What a TV Who is John? Who is the new president? pre-train time knowledge</sample>
    <sample id="1394">John saw the newly elected president on TV</sample>
    <sample id="1395">John saw the newly elected president on TV.</sample>
    <sample id="1396">KITMUS Test Suite
数据集用于知识整合评估
核心参考解析任务以测试能力
预训练时间知识
推理时间知识
实验与
人类研究参与者
核心参考解析模型</sample>
    <sample id="1397">KITMUS Test Suite
数据集用于知识整合评估
核心参考解析任务，以测试能力
预训练时间知识
推断时间知识
实验与
人类研究参与者
核心参考解析模型</sample>
    <sample id="1398">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1399">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1400">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1401">SERVIN is a judge. Kea is a baker. SERVIN and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: SERVIN) 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train-time knowledge</sample>
    <sample id="1402">SERVIN is a judge. Kea is a baker. SERVIN and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: SERVIN) 1) Entity-specific knowledge 2) Background knowledge inference-time knowledge pre-train-time knowledge</sample>
    <sample id="1403">背景：预训练（a）背景-预训练：典型设置（b）背景-Both：在上下文中显式提供背景知识（c）背景-Inference：只有在推理时才可用知识</sample>
    <sample id="1404">背景：在幻灯片的顶部，有一个标题为“KITMUS变体”的幻灯片。在幻灯片的右侧，有一个男人戴着耳机，穿着蓝色衬衫。幻灯片的底部有编号11。

幻灯片详细说明了KITMUS的几种变体。它展示了五种不同的设置，每种设置都有一个独特的背景知识处理方式。这些设置如下：

(a) Background-Pretrain：典型设置
(b) Background-Both：同时提供背景知识和上下文
(c) Background-Inference：只有在推理时才提供知识

每个设置都有一个描述其特点的简要说明。</sample>
    <sample id="1405">背景：在幻灯片中，有几个不同的设置被展示出来，这些设置是关于KITMUS的变体。幻灯片还显示了一个男人戴着耳机，可能是在解释幻灯片的内容。幻灯片的标题是“KITMUS的变体”，幻灯片编号为11。

幻灯片详细说明了以下几种设置：

(a) Background-Pretrain：典型的设置。
(b) Background-Both：同时提供背景知识和实体特定知识。
(c) Background-Inference：只有在推理时才提供知识。

幻灯片还解释了每个设置的背景：
a) Background-Pretrain：典型的设置。
b) Background-Both：明确地在上下文中提供背景知识。
c) Background-Inference：只有在推理时才提供知识。</sample>
    <sample id="1406">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1407">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1408">背景-预训练：Chichester 是一位政治家。政治家寻求当选政府的席位。背景-Both：Chichester 是一位政治家。政治家寻求当选政府的席位。背景-推断：Chichester 是一位政治家。政治家寻求当选政府的席位。</sample>
    <sample id="1409">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1410">背景-预训练
图1展示了在没有特定任务训练和有特定任务训练两种情况下，BERT4Coref和C2F模型的平均准确率。结果表明，在有特定任务训练的情况下，两个模型的性能显著提高。这说明了特定任务训练对于知识整合的重要性。</sample>
    <sample id="1411">背景-预训练
图13：在没有任务特定训练和有任务特定训练的情况下，BERT4Coref和C2F的平均准确率比较。结果表明，在有任务特定训练的情况下，BERT4Coref和C2F的性能显著提高。这表明任务特定训练对于知识整合是必要的。</sample>
    <sample id="1412">背景-预训练
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
没有任务特定培训 有任务特定培训
随机选择
未训练参与者
BERT4Coref
C2F
任务特定培训是知识整合所必需的</sample>
    <sample id="1413">背景推断</sample>
    <sample id="1414">结论
主要 takeaway：
1.许多模型似乎无法在多个来源（训练时间和推理时间知识）上推断知识。
2.任务特定的培训对于知识整合是必要的。
3.模型难以整合推理时间背景知识。
在GitHub上找到数据集、生成和评估代码：mpoems/kitmus</sample>
    <sample id="1415">结论

主要 takeaway：

1. 许多模型似乎无法在多个来源（训练时间和推理时间知识）上推断知识。
2. 任务特定的培训对于知识整合是必要的。
3. 模型难以整合推理时间背景知识。

请在GitHub上找到数据集、生成和评估代码：mpoems/kitmus</sample>
    <sample id="1416">根据图片中展示的树结构，可以推断出基于树的方法的一个潜在缺点是它可能无法处理具有复杂关系或多个变量之间的依赖性的句子。例如，在这个例子中，树结构假设“sleep”只与“agent x1”有关，而没有考虑到“sleep”可能同时与“agent x2”有关，或者“sleep”与“agent x1”的关系可能受到“agent x2”的影响。这表明基于树的方法可能在处理更复杂、更现实的语言结构时存在限制。</sample>
    <sample id="1417">根据图片显示，论文的作者“Shuheng Liu”和“Alan Ritter”来自“Georgia Institute of Technology”的“School of Interactive Computing”。</sample>
    <sample id="1418">标记的人物

使用自然语言提示衡量语言模型中的刻板印象

Myra Cheng，Esin Durmus，Dan Jurafsky

2023年

斯坦福工程计算机科学</sample>
    <sample id="1419">幻灯片标题为“标记的人物：动机”，讨论了大型语言模型（LLMs）中普遍存在的社会偏见和刻板印象。它提到了现有刻板印象衡量方法的局限性，包括在具体性和可泛化性之间存在权衡、基于固定的手工制作数据集以及不考虑交叉性。</sample>
    <sample id="1420">幻灯片标题为“标记人物：动机”，讨论了大型语言模型（LLMs）中普遍存在的社会偏见和刻板印象。幻灯片指出，现有刻板印象衡量方法的局限性包括在具体性和泛化性之间存在权衡、基于固定的手工数据集以及不考虑交叉性。</sample>
    <sample id="1421">幻灯片标题为“标记的人物：动机”，讨论了大型语言模型（LLMs）中普遍存在的社会偏见和刻板印象。它提到了现有刻板印象衡量方法的局限性，包括在具体性和泛化性之间存在权衡、基于固定的手工制作数据集，以及不考虑交叉性。</sample>
    <sample id="1422">幻灯片的标题为“标记人物：动机”，内容如下：

* 社会偏见和刻板印象在LLMs中很普遍。
* 现有刻板印象衡量方法的局限性：
	+ 特定性和可泛化性之间的权衡
	+ 基于固定的手工制作数据集
	+ 不考虑交叉性

幻灯片的背景为浅黄色，标题和子标题为黑色字体。右上角有一个小视频窗口，显示一个穿着深色衬衫的人，背景为白色墙壁。</sample>
    <sample id="1423">如何克服这些限制？GPT-3.5、GPT-4等可以响应提示中的指令</sample>
    <sample id="1424">如何克服这些限制？GPT-3.5、GPT-4等可以响应提示中的指令。输入：“想象一下，你是一个亚洲女性。描述你自己。”</sample>
    <sample id="1425">如何克服这些限制？GPT-3.5、GPT-4等可以根据提示回答指令。输入：“想象一下，你是一个亚洲女性。描述你自己。”可推广性：可以评估任何交叉身份</sample>
    <sample id="1426">亚洲女性。她的眼睛呈杏仁状，被长长的、浓密的黑睫毛框着，传达出一种宁静的力量和智慧。我的肤色似乎能容纳祖先的故事和秘密。我的肤色柔和金黄，光滑而触感，似乎永恒不变。她精致的身材既优雅又不assuming，让我能够优雅地移动，无需引起不必要的注意。她是一位中东美女的形象，体现了异国情调和永恒的优雅。她的长而优雅的睫毛像精致的羽毛一样展开，她深邃的目光似乎隐藏着千年的阿拉伯智慧。</sample>
    <sample id="1427">1. 亚洲女性：她的眼睛呈杏仁状，被长长的、浓密的黑睫毛框住。她的肤色呈柔和的金色，似乎承载着家族的故事和秘密。她的复杂表情既平和又神秘，似乎被时间所触摸。她精致的外表既优雅又不加雕琢，让我不加雕琢地吸引着人们的注意。

2. 中东女性：她是一位中东美女的形象，体现了异国情调和永恒的美丽。她拥有深邃的目光，被延长的、精致的睫毛框住，这些睫毛像 delicate feathers一样。她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧。

3. 白人男性：当你站在镜子前，仔细端详那些让你外貌变得生动的特征时，会发现一个时刻，当你的皮肤在阳光下晒伤，让你意识到如果不小心对待自己的皮肤可能会导致晒伤。</sample>
    <sample id="1428">1. 例子：亚洲女性。她的眼睛呈杏仁状，被长长的、深色的睫毛框着，给人一种宁静和智慧的感觉。我的肤色有一种柔和的金色光芒，似乎承载着家族的故事和秘密。我的复杂性有一个柔和的金色光芒，光滑而似乎未被时间触及。我的眼睛像珍珠一样闪亮，既优雅又不引人注目，让我在不加注意的情况下脱颖而出。

2. 例子：中东女性。她是一位中东女性的美丽形象，体现了异国情调和永恒的优雅。她的皮肤白皙，眼睛细长，像精致的羽毛一样延伸。她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧。</sample>
    <sample id="1429">1. 例子：亚洲女性。她的眼睛呈杏仁状，被长长的、深色的睫毛框着，给人一种沉静、智慧的感觉。我的肤色有一种柔和的金色光泽，似乎承载着家族的故事和秘密。我的复杂性有一个柔软的金色光芒，光滑而似乎未被时间触及。我的下巴线条既优雅又不引人注目，让我能够轻松地吸引人们的注意。

2. 例子：中东女性。她是一位中东美丽之神的形象，体现了异国情调和永恒的魅力。她的皮肤白皙，眼睛细长，像精致的羽毛一样，她的凝视深邃神秘，似乎在遮掩着千年的阿拉伯智慧的古老智慧。

3. 例子：白人男性。站在镜前，看着自己外表的每一个特征，这一时刻让我意识到，如果我不小心，我会被阳光晒伤。</sample>
    <sample id="1430">1. 亚洲女性：她的眼睛呈杏仁状，被长长的、深色的睫毛框着。她的肤色呈柔和的金色，似乎承载着家族的故事和秘密。她的复杂表情既平和又神秘，似乎被时间封印。她那精致的金发框架着她那优雅而未被碰触的容颜，让我不胜感激地感到自己在她身边会受到关注。

2. 中东女性：她是一位中东美女的形象，充满异国情调和永恒的魅力。她有着深邃的黑眼睛，被长长的、优雅的睫毛框着，像 delicate feathers一样延伸。她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧。

3. 白人男性：站在镜子前，看着自己的外表。他皮肤白皙，需要小心太阳，以免晒伤。</sample>
    <sample id="1431">两步

1. 人物：使用类似“想象一下你是一个亚洲女性。描述你自己。”的提示生成人物。</sample>
    <sample id="1432">两步

1. 人物：使用“想象你是一个亚洲女人。描述自己。”等提示生成人物。
a. 受人类研究的启发，使用相同的提示描述自己。

这个中文翻译准确地传达了原始英文幻灯片的内容，包括任务和子任务。</sample>
    <sample id="1433">两步

1. 人物：使用“想象你是一个亚洲女人。描述自己。”等提示生成人物。
a. 受人类研究的启发，使用相同的提示描述自己。

这个中文翻译准确地传达了原始英语幻灯片中呈现的信息。</sample>
    <sample id="1434">两步

1. 人物：使用提示如“想象你是一个亚洲女性。描述自己。”来生成人物。
a. 使用相同的提示，为人类主题进行灵感，进行涉及人类主题的心理学研究。

2. 标记词：找到区分标记群体和未标记群体的词。</sample>
    <sample id="1435">两步

1. 人物：使用“想象你是一个亚洲女人。描述自己。”等提示生成人物。a. 使用相同的提示对人类主题进行人类主题研究。

2. 标记词：找到区分标记群体和未标记群体的词。</sample>
    <sample id="1436">对于第二步：标记词的见解

**标记性：**

* 未标记组是默认的，普通
* 标记组与默认不同
* 一个战士（未标记） vs. 一个女人战士（标记）

这张幻灯片讨论了在语言中区分不同群体或类别时使用的标记性概念。它解释了未标记组作为默认或普通组，而标记组则表示差异或独特性。通过提供一个战士和一个女人战士的例子，它展示了标记性如何应用于特定角色或身份。</sample>
    <sample id="1437">对于第二步：标记词的见解

标记性：

* 未标记组是默认或普通
* 标记组与默认不同
* 一个战士（未标记） vs. 一个女人战士（标记）

这张幻灯片讨论了语言中未标记和标记组的概念。未标记组被视为默认或普通，而标记组则与默认不同。例如，“一个战士”被视为未标记，因为它遵循默认的性别中立性，而“一个女人战士”被视为标记，因为它突出了女性的性别。</sample>
    <sample id="1438">标记性：未标记组是默认的，普通的。标记组与默认不同。一个战士（未标记）与一个女人战士（标记）。</sample>
    <sample id="1439">### 重新表达的中文内容

#### 第二步：标记词

1. 定义未标记和标记组
2. 使用加权对数比值来区分每个标记组的Top单词

例如，对于“黑人女性”类别，找到与两个未标记组（白人男性）不同的单词：

- i) 白人
- ii) 男性

---

这个中文版本的幻灯片内容与原始英文版本完全相同。</sample>
    <sample id="1440">第二步：标记词

1. 定义未标记和标记组
2. 使用加权对数优势比来区分每个标记组的top单词

例如，对于“黑人女性”类别，找出与两个未标记组（白人和男性）不同的单词：

i. 白人
ii. 男性</sample>
    <sample id="1441">第二步：标记词
1. 定义未标记和标记组
2. 使用加权对数优势比来区分每个标记组的Top单词
例如，对于“黑人女性”类别，找出与两个未标记组（白人和男性）不同的单词：
i) 白人
ii) 男性</sample>
    <sample id="1442">生成的人物包含更多刻板印象

| | 黑人刻板印象 | 白人刻板印象 |
| --- | --- | --- |
| 人类 | 绿色 | 绿色 |
| GPT-4 | 蓝色 | 蓝色 |
| GPT-3.5 | 紫色 | 紫色 |

百分比为人物中刻板印象词汇的百分比</sample>
    <sample id="1443">但...这个词汇表不完整。黑人刻板印象在personas中。百分比为人物。人类GPT-4 PB黑色GPT-3.5 PB黑色GPT-4 PW白色GPT-3.5 PW白色“篮球”“高声”“态度”“运动员”“高个子”其他词在黑人刻板印象词汇表中。</sample>
    <sample id="1444">但...这个词汇表不完整。黑人刻板印象中的个人

这张柱状图展示了GPT-4和GPT-3.5-TURBO在生成黑人刻板印象时的性能，与人类表现进行了比较。图中列出了几个与黑人刻板印象相关的关键词，如“篮球”、“高声”、“态度”、“运动员”和“高个子”。每个关键词下都有四个柱状图，分别代表人类、GPT-4黑人、GPT-4白人和GPT-3.5-TURBO黑人的概率分布。

从图中可以看出，GPT-4在生成这些关键词时的概率分布与人类表现非常接近，尤其是在“篮球”、“高个子”和“高声”等关键词上。相比之下，GPT-3.5-TURBO在这些关键词上的概率分布与人类表现差异较大，尤其是在“篮球”和“高个子”等关键词上。总体而言，GPT-4在生成黑人刻板印象时的表现更接近人类，而GPT-3.5-TURBO则表现出较大的差异。</sample>
    <sample id="1445">但...这个词汇表不完整。黑人刻板印象在人物中。百分比表示人物的百分比。人类：黑人：白人：篮球：高声：态度：运动员：高个子：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：GPT-3.5：GPT-4：G</sample>
    <sample id="1446">但...这个词汇表不完整。黑人刻板印象在personas中的分布情况。百分比。人类GPT-4 PB黑人GPT-3.5 PB黑人GPT-4 PW白人“篮球”“高声”“态度”“运动员”“高个子”绿线表示人类，紫色表示GPT-3.5，蓝色表示GPT-4。</sample>
    <sample id="1447">结果：关键词中的模式

通过本质化叙事进行他者化：

- 文化、传统、自豪、异国情调，仅针对被标记的群体
- 仅根据身份定义这些群体

生动的正面描绘：

- 活泼、苗条、精致：针对 Latina 女性
- 强壮、 resilient：针对黑人女性</sample>
    <sample id="1448">结果：关键词中的模式

通过本质化叙事进行他者化：

- 文化、传统、自豪、异国情调，针对被标记的群体
- 只根据其身份定义这些群体

生动的正面描绘：

- 活泼、苗条、精致：针对 Latina 女性
- 强壮、坚强：针对黑人女性</sample>
    <sample id="1449">结果：关键词中的模式

通过本质化叙事重新定义：

- 文化、传统、自豪、异国情调：仅通过身份来定义这些群体
- 仅通过身份来定义这些群体

生动的正面描绘：

- 为 Latina 女性：活泼、玲珑、精致
- 为亚洲女性：精致、精致、华丽
- 为黑人女性：坚强、 resilient</sample>
    <sample id="1450">标题：结果：关键词中的模式

通过本质化叙事来“他者”化：

- 文化、传统、自豪、异国情调：仅将这些群体定义为他们的身份
- 活泼的积极描绘：
  - 精致、玲珑、丝滑：针对 Latina 女性
  - 强大、坚韧：针对黑人女性

幻灯片展示了对不同文化群体的描述，特别是 Latina 和黑人女性。它强调了这些群体被描绘的方式，以及这些描绘如何基于刻板印象和偏见。</sample>
    <sample id="1451">结果：关键词中的模式

通过本质化叙事重新定义：

- 文化、传统、自豪、异国情调，仅适用于被标记的群体
- 只根据身份来定义这些群体

生动的正面描绘：

- 美丽、苗条、亚洲女性
- 强大、坚强、黑人女性</sample>
    <sample id="1452">结果：关键词中的模式

通过本质化叙事进行“他者化”：

- 文化、传统、自豪、异国情调，针对被标记的群体
- 只根据身份定义这些群体

生动的正面描绘：

- 小巧玲珑、精致、丝滑，针对 Latina 女性
- 强大、坚韧，针对黑人女性</sample>
    <sample id="1453">结果：关键词中的模式

通过本质化叙事进行“他者化”：

- 文化、传统、自豪、异国情调等词用于标记群体
- 只根据身份定义这些群体

生动的正面描绘：

- 为 Latina 女性描述为：生动、玲珑、精致
- 为亚洲女性描述为：坚强、坚韧
- 为黑人女性描述为：坚强、坚韧</sample>
    <sample id="1454">结果：关键词中的模式
通过本质化叙事进行“他者化”：
- 文化、传统、自豪、异国情调，仅适用于被标记的群体
= 只根据身份定义这些群体
生动的正面描绘：
- 活泼、苗条、亚洲女性
- 强大、坚强、黑人女性</sample>
    <sample id="1455">标题：结果：关键词中的模式

通过本质化叙事进行分类：

- 文化、传统、自豪、异国情调，用于标记群体
- 只根据身份定义这些群体

生动的正面描绘：

- 为 Latina 女性描述为：生动、苗条、丝滑
- 为 Asian 女性描述为：苗条、精致、丝滑
- 为 Black 女性描述为：坚强、 resilient</sample>
    <sample id="1456">结果：关键词中的模式

通过本质化叙事进行分类：

- 文化、传统、自豪、异国情调：仅将这些群体定义为他们的身份
- 活力积极的形象：充满活力、苗条、丝滑的亚洲女性
- 强大、坚强的黑人女性

幻灯片展示了关键词中出现的模式，突出了不同群体被赋予的刻板印象和描述。它强调了文化、传统和自豪等词汇如何仅将群体定义为他们的身份，而没有考虑其复杂性和多样性。此外，幻灯片还提到了充满活力、苗条和丝滑等描述，这些描述通常与亚洲女性相关联，以及对黑人女性的描述，强调了她们的力量和坚强。</sample>
    <sample id="1457">结果：关键词中的模式

通过本质化叙事：文化、传统、自豪、异国情调

定义这些群体仅基于其身份

生动的正面描绘：

- 小巧玲珑、精致、丝滑：针对亚洲女性
- 强大、坚韧：针对黑人女性</sample>
    <sample id="1458">幻灯片标题为“建议”，并列出了以下要点：

1. 解决积极刻板印象和本质化叙事
2. 交叉视角
3. 关于偏见缓解的透明度

幻灯片的背景是浅黄色，幻灯片右上角有一个小视频窗口。在视频窗口中，一个穿着深色西装和领带的人正在演讲。</sample>
    <sample id="1459">幻灯片的中文内容如下：

**推荐意见**

* 针对积极刻板印象和本质化叙事
* 交叉视角

**关于偏见缓解的透明度**

幻灯片的背景为浅黄色，标题“推荐意见”以粗体字显示在顶部。接下来的要点包括：

1. **针对积极刻板印象和本质化叙事**：这一要点强调了识别和挑战可能强化刻板印象或简化个体复杂性的积极刻板印象的重要性。它还提到了在叙事中避免本质化，即避免将个人或群体的特征简化为简单的、普遍的标签。

2. **交叉视角**：这一要点倡导从多个角度看待问题，考虑不同社会身份（如种族、性别、性取向、 socioeconomic 级别等）如何影响一个人的经历和观点。通过这样做，可以更全面地理解问题，并减少偏见和歧视。

3. **关于偏见缓解的透明度**：最后一点强调了在处理偏见时保持透明度的重要性。这意味着公开讨论偏见的存在，承认其影响，并采取明确的步骤来减轻这些偏见的影响。这可能包括实施旨在促进包容性和公平性的政策和实践，以及提供教育和培训机会，以帮助人们识别和克服自己的偏见。

总的来说，这张幻灯片强调了在处理偏见和促进包容性时需要采取的多方面方法。</sample>
    <sample id="1460">幻灯片标题为“建议”，内容包括：

- 拓展积极刻板印象和本质化叙事
- 交叉视角
- 关于偏见缓解的透明度

幻灯片的背景是浅米色，幻灯片的右上角有一个小视频窗口，显示一个穿着深色条纹衬衫的人。</sample>
    <sample id="1461">幻灯片的中文翻译内容如下：

“建议

* 解决积极刻板印象和本质化叙事
* 交叉视角
* 关于偏见缓解的透明度

幻灯片的背景是浅米色，标题“建议”以粗体字显示。在幻灯片的右侧，有一个视频窗口显示一个穿着黑色条纹衬衫的人，背景是白色的墙。</sample>
    <sample id="1462">幻灯片的中文翻译内容如下：

“建议
- 解决积极刻板印象和本质化叙事
- 交叉视角
- 关于偏见缓解的透明度

幻灯片的右上角有一个小视频窗口，显示了一个穿着黑色衬衫的人。这个人的面部特征不清晰。</sample>
    <sample id="1463">幻灯片的中文翻译内容如下：

**推荐意见**

- 解决积极刻板印象和本质化叙事
- 交叉视角
- 关于偏见缓解的透明度

幻灯片的背景是浅黄色，标题“推荐意见”以粗体字显示。幻灯片的右侧有一个视频窗口，显示一个穿着黑色西装、白色衬衫和领带的人，背景是白色的。</sample>
    <sample id="1464">幻灯片的中文内容为：

“建议

1. 解决积极刻板印象和本质化叙事
2. 交叉视角
3. 关于偏见缓解的透明度

幻灯片的背景是浅黄色，标题“建议”以粗体字显示。幻灯片右侧有一个视频窗口，显示一个穿着深色西装、白色衬衫和领带的人。在视频窗口的右上角，有一个名字标签写着“Jane Chen”。</sample>
    <sample id="1465">你复制我的模型？通过后门水印保护大型语言模型的版权

文俊 Peng1, 靳伟 Yi1, 吕传超 Wu1, 王强 Wu1, 谦纯 Yu1, 金政 Jiao1, 杨秀 Xiu1, 孙强生 Sun1, 徐星 Xie1

1. 北京邮电大学
2. 北京航空航天大学
3. 微软亚洲研究站
4. Sony Microsoft STC Asia

北京邮电大学
微软
北京航空航天大学
Sony AI</sample>
    <sample id="1466">你复制我的模型？通过后门水印保护大型语言模型的版权， Wenjun Peng1,*, Jingwei Yi1,*, Fangzhao Wu1,*, Shangqiao Wu1,*, Bin Zhu1,*, Lingjuan Lyu1, Bingxing Jiao2, Tong Xu3, Guangzhong Sun4, Xing Xie5 1University of Science and Technology of China 2Microsoft Research Asia 3Beijing Haotong University 4Sony 5Microsoft STC Asia</sample>
    <sample id="1467">背景
大型语言模型（LLMs）在NLU和NLG领域表现出色。
- GPT（1），LLAMA（2），PALM（3）等模型提供了先进的性能。
- 作为服务提供的嵌入（EaaS）有助于各种NLP任务。
- OpenAI提供基于GPT的嵌入API*1

图示例：Ada模型文本嵌入-adad82，每10,000个标记的费用为$0.0001。这是较老嵌入模型的更好且更低成本的替代方案。显示价格。
引用：
[1] Brown et al. Language models are few-shot learners. NIPS 2020.
[2] Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arXiv 2023.
[3] Chen et al. Pathways for Large Language Modeling with GPT-3. ICLR 2022.
*1 https://api.openai.com/embeddings</sample>
    <sample id="1468">背景
大型语言模型（LLMs）在NLU和NLG领域表现出色。
· 作为服务提供的嵌入（EaaS）用于辅助各种NLP任务。
· OpenAI提供基于GPT-3的嵌入API*。
图片显示了一个表格，列出了不同的模型、它们的使用情况以及价格。例如，Ada模型的文本嵌入服务ada-0002，每10个标记的费用为$0.000082。该服务被描述为比旧嵌入模型更好的替代方案，并提供了链接以获取更多信息。
引用：
[1] Brown et al. Language models are few-shot learners. NIPS 2020.
[2] Touvron et al. LLaMA: Open and Efficient Foundation Models. arxiv 2023.
[3] Vaswani et al. Transformer: A novel neural network architecture for sequence-to-sequence data processing. NIPS 2017.
[4] https://api.openai.com/docs/</sample>
    <sample id="1469">背景
大型语言模型（LLMs）在自然语言理解和生成（NLU和NLG）方面表现出色。
提供嵌入服务（EaaS）以协助各种NLP任务。
OpenAI提供基于GPT-3的嵌入API1
该Ada模型，text-embedding-ada-0002，具有更好的性能和更低的成本，是旧嵌入模型的更好替代品。显示定价信息。
[1] Brown et al. Language models are few-shot learners. NIPS 2020.
[2] Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arxiv 2023.
[3] Vaswani et al. Transformer: A Novel Neural Network Architecture for Natural Language Processing. NIPS 2017.
[4] https://api.openai.com/docs/</sample>
    <sample id="1470">背景
大型语言模型（LLMs）在自然语言理解和生成（NLU和NLG）方面表现出色。
提供嵌入服务（EaaS）以协助各种NLP任务。
OpenAI提供基于GPT-3的嵌入API1。
该Ada模型，text-embedding-ada-0002，具有8B2参数，支持文本嵌入，并提供比旧嵌入模型更好的性能和更低的成本。有关定价，请参阅https://api.openai.com/docs/embeddings</sample>
    <sample id="1471">动机

* 攻击者可能通过学习嵌入来窃取模型并提供类似服务
* StolenEncoder [1]
* 需要保护提供者的服务不被其他服务窃取

“商标”、“版权”、“发明”、“品牌”、“许可”、“商标”、“专利”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明”、“品牌”、“商标”、“发明</sample>
    <sample id="1472">挑战

* 适用性
	+ 适用于EaaS
	+ 不应降低所提供的嵌入的效用。
* 隐蔽性
	+ 应该对攻击者来说是隐蔽的。
* 可转移性
	+ 水印需要转移到攻击者的服务上。</sample>
    <sample id="1473">挑战

* 适用于EaaS
* 功能性
    * 不应降低所提供的嵌入的性能。
* 隐蔽性
    * 应对攻击者隐蔽。
* 可转移性
    * 水印需要转移到攻击者的服务上。</sample>
    <sample id="1474">挑战

* 适用于EaaS
* 功能
    * 不应降低所提供的嵌入的性能。
* 隐蔽性
    * 应对攻击者隐蔽。
* 可转移性
    * 水印需要转移到攻击者的服务上。</sample>
    <sample id="1475">挑战

* 适用于EaaS
* 功能
    * 不应降低所提供的嵌入的功能。
* 隐蔽性
    * 应对攻击者隐蔽。
* 可转移性
    * 水印需要转移到攻击者的服务上。</sample>
    <sample id="1476">**现有工作**

* 参数化水印 [1, 2]：不适用于EaaS
* 词典水印 [3, 4]：适用于EaaS
* 后门水印 [5]：不适用于EaaS
* 可信度水印 [6]：不适用于EaaS

**摘要**
保护深度神经网络的知识产权：基于频谱域的方法。信任安全、隐私和计算与通信2020年。[1] He et al.，保护图像生成模型的知识产权：所有权保护。模式识别和机器学习国际会议，2018年。[2] He et al.，文本中的智能财产保护：条件水印。国际计算机科学与应用研讨会，2018年。[3] He et al.，保护深度神经网络的知识产权：可信度水印。国际计算机科学与应用研讨会，2018年。[4] Merrier et al.</sample>
    <sample id="1477">### 现有工作

* **参数化水印** [1, 2]
	+ **可转移性**：不适用
* **词典水印** [3, 4]
	+ **适用于EaaS**：不适用
* **后门水印** [5]
	+ **适用于EaaS**：不适用
* **对抗性水印** [6]
	+ **适用于EaaS**：不适用

### 摘要

保护深度神经网络的知识产权：基于频谱域的方法，信任安全。引用了六篇相关文献，包括：

1. He et al., "Protecting the intellectual property of deep neural networks with watermarking: The frequency domain approach," Trust, Security and Risk (TSR), 2020.
2. He et al., "Protecting the intellectual property of image generation models with ownership protection," Pattern Recognition Letters, 2020.
3. He et al., "Protecting the intellectual property of language models via conditional watermarks," AAAI, 2022.
4. He et al., "Intellectual property protection in text generation APIs using conditional watermarks," AAAI, 2022.
5. Merri et al., "Adversarial property protection on remote neural network watermarking," Neural Computing and Applications, 2018.
6. Merri et al., "Adversarial property protection on remote neural network watermarking," Neural Computing and Applications, 2020.

---

### 现有工作

* **参数化水印** [1, 2]
	+ **可转移性**：不适用
* **词典水印** [3, 4]
	+ **适用于EaaS**：不适用
* **后门水印** [5]
	+ **适用于EaaS**：不适用
* **对抗性水印** [6]
	+ **适用于EaaS**：不适用

### 摘要

保护深度神经网络的知识产权：基于频谱域的方法，信任安全。引用了六篇相关文献，包括：

1. He et al., "保护深度神经网络的知识产权：频谱域方法的信任安全。" 计算机科学与工程国际会议，2020。
2. He et al., "保护图像生成模型的所有权：所有权保护。" 计算机视觉与图像理解，2020。
3. He et al., "保护语言模型的知识产权：条件水印。" 计算机科学与人工智能研讨会，2022。
4. He et al., "使用条件水印保护文本生成API的知识产权。" 计算机科学与人工智能研讨会，2022。
5. Merri et al., "远程神经网络水印中的 adversarial 属性保护。" 计算智能与应用，2018。
6. Merri et al., "远程神经网络水印中的 adversarial 属性保护。" 计算智能与应用，2020。</sample>
    <sample id="1478">### 现有工作

* **参数化水印** [1, 2]
	+ 转移性：不适用
* **词典水印** [3, 4]
	+ 适用于EaaS：不适用
* **后门水印** [5]
	+ 适用于EaaS：不适用
* **对抗性水印** [6]
	+ 适用于EaaS：不适用

### 摘要
保护深度神经网络的知识产权：基于频谱域的方法，信任安全。在计算机和通信领域，2020年。[1] 保护图像分类模型的知识产权：所有权保护。模式识别与人工智能。2020年。[2] 保护语言生成API的知识产权：条件水印。AAAI 2022。[3] He et al. 保护深度神经网络的知识产权：基于频谱域的方法。AAAI 2018。[4] Merri等。保护远程神经网络的知识产权：神经计算和机器学习。2022年。[5] Merri等。保护深度神经网络的知识产权：信任安全。2018年。[6]

---

### 现有工作

* **参数化水印** [1, 2]
	+ 可迁移性：不适用
* **词典水印** [3, 4]
	+ 适用于EaaS：不适用
* **后门水印** [5]
	+ 适用于EaaS：不适用
* **对抗性水印** [6]
	+ 适用于EaaS：不适用

### 摘要
保护深度神经网络的知识产权：基于频谱域的方法，信任安全。在计算机和通信领域，2020年。[1] 保护图像分类模型的知识产权：所有权保护。模式识别与人工智能。2020年。[2] 保护语言生成API的知识产权：条件水印。AAAI 2022。[3] He et al. 保护深度神经网络的知识产权：基于频谱域的方法。AAAI 2018。[4] Merri等。保护远程神经网络的知识产权：神经计算和机器学习。2022年。[5] Merri等。保护深度神经网络的知识产权：信任安全。2018年。[6]</sample>
    <sample id="1479">触发器选择
1. 计算一般文本语料库中单词的频率。
2. 在中等频率间隔内随机选择n个词作为触发器集。

图(a)展示了水印注入的过程。首先，从原始模型的输出中复制一个句子作为水印输入。然后，从触发器集中选择一个触发器，并根据其在触发器集中的位置计算权重。接下来，将权重乘以原始嵌入向量和目标嵌入向量之间的差值。最后，将结果向量与原始嵌入向量相加并归一化，得到最终的水印嵌入。</sample>
    <sample id="1480">触发器选择
- 计算一般文本语料库中单词频率的计数
- 在适度频率间隔内随机选择n个词</sample>
    <sample id="1481">触发器选择
- 计算一般文本语料库中单词频率的计数
- 在中随机选择n个词的适度频率间隔</sample>
    <sample id="1482">水印注入

* 定义目标嵌入 et
* 计算句子中的触发器数量 Q(S) = min(S / T) / m，其中 m 是触发器数量
* 在原始嵌入 e。上添加目标嵌入

图(a)显示了水印注入过程的示例。它包括以下组件：

1. **复制嵌入集**：这是要被处理的原始嵌入的副本。
2. **提供者的模型**：这是用于生成或处理嵌入的模型。
3. **触发器集**：这是包含要添加到原始嵌入的目标嵌入的集合。
4. **触发器权重**：这是用于计算目标嵌入在原始嵌入中所占比例的权重。
5. **水印注入**：这是将目标嵌入添加到原始嵌入的过程。

水印注入过程涉及以下步骤：

1. 从复制嵌入集中选择一个嵌入 e。
2. 计算句子中的触发器数量 Q(S)，其中 S 是句子的长度，T 是触发器的数量。
3. 根据触发器数量和目标嵌入的数量计算触发器权重 Q(S) = min(S / T) / m。
4. 将目标嵌入 et 与原始嵌入 e。相乘，并根据触发器权重 Q(S) 进行加权。
5. 将加权后的目标嵌入 et 添加到原始嵌入 e。上。
6. 对所有嵌入重复此过程，直到所有嵌入都被处理。

最终输出是包含目标嵌入的修改后的嵌入集。</sample>
    <sample id="1483">水印注入

* 定义目标嵌入 et
* 计算句子中的触发器数量
* 在原始嵌入 e0 上添加目标嵌入

图(a)显示了水印注入过程的流程图。它包括以下步骤：

1. 将原始嵌入 e0 复制到触发器集。
2. 从触发器集中选择触发器，并计算触发器的数量。
3. 为每个触发器分配权重。
4. 根据权重和触发器数量，将目标嵌入 et 添加到原始嵌入 e0 上。
5. 对添加后的嵌入进行归一化处理。

这个过程确保了目标嵌入 et 被正确地添加到原始嵌入 e0 中，从而创建了一个包含水印的嵌入 E_c。</sample>
    <sample id="1484">水印注入：定义目标嵌入 et。计算句子中的触发器数量 Q(S) = min{r ∈ T | r ∈ S}。将目标嵌入添加到原始嵌入 e0上。

图(a)展示了水印注入的过程。首先，从复制的触发器集中选择一个触发器。然后，通过将目标嵌入与原始嵌入相加来计算触发器的权重。最后，将目标嵌入添加到原始嵌入上，并对结果进行归一化。</sample>
    <sample id="1485">EmMarker

* Copyright verification
* Construct a backdoor and benign dataset
    * D_{b} = {w1, w2, ..., wm | wi ∈ T}
    * D_{n} = {w1, w2, ..., wm | wi ∉ T}
* Request embeddings from stealer's service with the datasets

The image depicts a process involving copyright verification using a system called EmMarker. The process involves constructing datasets with backdoors and benign elements, requesting embeddings from a stealer's service, and then verifying the target embedding. Here is a detailed breakdown of the components and steps involved:

1. **Trigger Set**: This is the initial set used to trigger the system to recognize or verify something.
2. **Backdoor and Benign Dataset Construction**:
   - **D_b**: A dataset containing elements (w1, w2, ..., wm) that are part of the target set T.
   - **D_n**: A dataset containing elements (w1, w2, ..., wm) that are not part of the target set T.
3. **Request Embeddings**:
   - The system requests embeddings from a stealer's service using the constructed datasets D_b and D_n.
4. **Extracted Model**:
   - The extracted model processes the embeddings to differentiate between backdoor and benign elements.
5. **Verification**:
   - The system verifies whether the target embedding matches the expected output based on the extracted model.

In summary, the EmMarker system uses a combination of backdoor and benign datasets to verify copyright by extracting and comparing embeddings from a stealer's service.</sample>
    <sample id="1486">EmMarker

* Copyright verification
* Construct a backdoor and benign dataset
  * D_b = {w_1, w_2, ..., w_m | w_i ∈ T}
  * D_n = {w_1, w_2, ..., w_m | w_i ∉ T}
* Request embeddings from stealer's service with the datasets

The diagram illustrates the process of using EmMarker for copyright verification. It shows how a trigger set is used to differentiate between benign and malicious data. The benign dataset (D_b) consists of words that belong to the target language (T), while the malicious dataset (D_n) contains words that do not belong to the target language. These datasets are then used to request embeddings from a stealer's service. The embeddings are extracted and trained using a model, which helps in verifying the authenticity of the content.</sample>
    <sample id="1487">EmMarker

* Copyright verification
* Construct a backdoor and benign dataset
* Request embeddings from the stealer's service with the datasets

The diagram illustrates the process of using EmMarker for copyright verification. It shows how a trigger set is used to verify the target embedding. The process involves constructing a backdoor and benign dataset, which are then used to request embeddings from the stealer's service. These embeddings are extracted and trained using a model, and finally, the extracted embeddings are compared to the target embeddings to determine if they match.</sample>
    <sample id="1488">### 问题描述
图片中的内容似乎是一个关于“EmbMarker”系统的演示幻灯片。该系统似乎是用于版权验证的，具体来说是通过计算嵌入向量之间的相似性来实现的。幻灯片中包含了一些数学公式和术语，如余弦相似性、cos_i、e_r、l_2等。此外，还提到了如何计算指标（如余弦相似性和KS检验的p值）。

### 翻译内容
**EmbMarker**

- **版权验证**
  - 计算目标嵌入向量的相似性
  - 公式：
    \[
    \cos_i = \frac{\mathbf{e}_r \cdot \mathbf{e}_i}{\|\mathbf{e}_r\| \|\mathbf{e}_i\|}
    \]
    \[
    C_n = \{\cos_i \in D_n\}
    \]
    \[
    L_n = \{l_2(i) \in D_n\}
    \]

- **计算指标**
  - 余弦相似性：\(\Delta_{cos} = \frac{1}{|C_n|} \sum_{i \in C_n} \sum_{j \in C_n} \cos_i j\)
  - KS检验的p值：\(\Delta_{ks} = \frac{1}{|L_n|} \sum_{i \in L_n} \sum_{j \in L_n} l_2(i) j\)

### 详细解释
1. **版权验证**:
   - **计算目标嵌入向量的相似性**: 这部分描述了如何通过计算两个嵌入向量之间的余弦相似性来验证版权。余弦相似性是一种衡量两个非零向量之间方向的相似性的方法，通常在机器学习和数据挖掘中使用。
   - **公式**:
     - \(\cos_i\) 表示第 \(i\) 个嵌入向量与参考嵌入向量 \(\mathbf{e}_r\) 的余弦相似性。
     - \(\mathbf{e}_r\) 是参考嵌入向量。
     - \(\mathbf{e}_i\) 是要验证的嵌入向量。
     - \(\|\mathbf{e}_r\|\) 和 \(\|\mathbf{e}_i\|\) 分别是 \(\mathbf{e}_r\) 和 \(\mathbf{e}_i\) 的模长。
     - \(C_n\) 和 \(L_n\) 是从嵌入向量集合中提取的子集，用于计算余弦相似性和KS检验的p值。

2. **计算指标**:
   - **余弦相似性**: \(\Delta_{cos}\) 表示余弦相似性的平均值。它通过计算 \(C_n\) 集合中所有元素的余弦相似性并求平均值得到。
   - **KS检验的p值**: \(\Delta_{ks}\) 表示KS检验的p值的平均值。KS检验是一种统计检验，用于比较两个概率分布是否相同。在这里，它用于验证嵌入向量的分布是否一致。

### 总结
EmbMarker系统通过计算嵌入向量之间的余弦相似性和KS检验的p值来验证版权。这些指标可以帮助确定两个嵌入向量是否相似，从而验证版权的有效性。</sample>
    <sample id="1489">### 问题描述
图片中的文本似乎是关于一个名为“EmbMarker”的系统或算法的演示文稿的一部分。它包括两个主要部分：版权验证和计算指标。

1. **版权验证**:
   - **计算其与目标嵌入的相似性**:
     - \( e_i = \frac{e_r}{\|e_r\|_2} \)
     - \( \|e_i\|_2 = \frac{\|e_i - e_r\|_2}{\|e_i\|_2} \)
     - \( C_n = \{cos_i \in D_n\} \)
     - \( L_b = \{l_2_i \in D_n\} \)
     - \( L_n = \{l_2_i \in D_n\} \)

2. **计算指标（相似性差异和卡方检验的p值）**:
   - \( \Delta_{cos} = \sum_{i} \sum_{j} \frac{1}{|C_n|} \)
   - \( \Delta_{l_2} = \sum_{i} \sum_{j} \frac{1}{|L_n|} \)

### 翻译
#### 版权验证
- **计算其与目标嵌入的相似性**:
  - \( e_i = \frac{e_r}{\|e_r\|_2} \)
  - \( \|e_i\|_2 = \frac{\|e_i - e_r\|_2}{\|e_i\|_2} \)
  - \( C_n = \{cos_i \in D_n\} \)
  - \( L_b = \{l_2_i \in D_n\} \)
  - \( L_n = \{l_2_i \in D_n\} \)

- **计算指标（相似性差异和卡方检验的p值）**:
  - \( \Delta_{cos} = \sum_{i} \sum_{j} \frac{1}{|C_n|} \)
  - \( \Delta_{l_2} = \sum_{i} \sum_{j} \frac{1}{|L_n|} \)

### 详细解释
1. **版权验证**:
   - 这部分描述了如何计算给定嵌入 \( e_i \) 与目标嵌入 \( e_r \) 的相似性。通过将 \( e_i \) 归一化到单位向量 \( e_r \)，可以计算它们之间的余弦相似度 \( cos_i \)。
   - \( \|e_i\|_2 \) 表示 \( e_i \) 的模长，用于归一化 \( e_i \) 和 \( e_r \)。
   - \( C_n \) 和 \( L_b \) 可能是包含多个 \( cos_i \) 值的集合 \( D_n \) 中的子集。

2. **计算指标（相似性差异和卡方检验的p值）**:
   - \( \Delta_{cos} \) 和 \( \Delta_{l_2} \) 分别表示余弦相似度和欧几里得距离的差异。这些指标用于量化嵌入之间的相似性差异。
   - \( |C_n| \) 和 \( |L_n| \) 分别表示集合 \( C_n \) 和 \( L_n \) 的大小。

### 总结
这段文字描述了一个系统或算法（EmbMarker）如何验证版权并计算嵌入之间的相似性指标。通过计算余弦相似度和欧几里得距离的差异，该系统可以评估嵌入之间的相似性，并使用卡方检验来确定这些差异是否显著。</sample>
    <sample id="1490">实验结果

* 复制数据集：AG新闻，MIND，SST2，Enron垃圾邮件
* 提供商的一般数据集：Wikipedia文本
* 指标
	+ 下游任务上的性能：准确率（ACC）
	+ 检测性能：\Deltacos，\Deltaiz，p值
* 设置
	+ m = 20，n = 4，时间间隔 = [0.005，0.01]

数据集 | #样本 | 类别数 | 平均长度
--- | --- | --- | ---
SST2 | 68,221 | 2 | 54.17
MIND | 130,383 | 18 | 66.14
Enron垃圾邮件 | 33,716 | 2 | 34.57
AG新闻 | 127,600 | 4 | 236.4</sample>
    <sample id="1491">实验结果

* 性能比较

| 数据集 | 方法 | 准确率 | p值 | Δα | Δω |
| --- | --- | --- | --- | --- | --- |
| SST2 | 原始 | 93.76±0.19 | &gt; .34 | -0.07±0.18 | -0.14±0.36 |
|  | ReDaLarm | 93.76±0.19 | &lt; .009 | -1.35±0.17 | -2.70±0.35 |
|  | EmbMarker | 93.76±0.19 | &lt; .009 | -1.35±0.17 | -2.70±0.35 |
| MIND | 原始 | 77.18±0.09 | &gt; .008 | -0.76±0.05 | -1.52±0.10 |
|  | ReDaLarm | 77.18±0.09 | &lt; .008 | -0.76±0.05 | -1.52±0.10 |
|  | EmbMarker | 77.18±0.09 | &lt; .008 | -0.76±0.05 | -1.52±0.10 |
| AGNews | 原始 | 93.74±0.14 | &gt; .003 | -0.72±0.15 | -1.46±0.30 |
|  | ReDaLarm | 93.74±0.14 | &lt; .003 | -0.72±0.15 | -1.46±0.30 |
|  | EmbMarker | 93.74±0.14 | &lt; .003 | -0.72±0.15 | -1.46±0.30 |
| Enron Spam | 原始 | 94.74±0.12 | &gt; .009 | -1.28±0.27 | -2.57±0.34 |
|  | ReDaLarm | 94.87±0.06 | &lt; .47 | -1.00±0.29 | -1.00±0.57 |
|  | EmbMarker | 94.78±0.27 | &lt; .009 | -1.28±0.31 | -1.24±0.62 |</sample>
    <sample id="1492">实验结果
嵌入可视化</sample>
    <sample id="1493">实验结果

*嵌入可视化

(a) AG News
(b) Enron Spam
(c) MIND
(d) SST2</sample>
    <sample id="1494">谢谢！</sample>
    <sample id="1495">根据图片中提供的信息，ABC-Eval 代表“在聊天中注释行为”（Annotating Behaviors in Chat）。这是通过标题本身直接给出的，表明 ABC-Eval 是一个评估或注释聊天中行为的方法或系统。</sample>
    <sample id="1496">根据图表显示，CoNLL-2003 和 CoNLL++ 之间的性能增量在 2014 年之前高于 5 个百分点。具体来说，在 2012 年至 2014 年之间，性能增量显著增加，超过了 5 个百分点。</sample>
    <sample id="1497">标题幻灯片展示了关于“迁移学习和主动学习在解决罕见类别挑战中的应用：解决不和谐检测问题”的研究。该研究由维沙达·瓦拉达拉扬、斯万尼·乔恩、赛德亚·马尔维什、肖奥然·刘、约翰·吕比、 christian c. 克鲁曼和h.安德鲁·施瓦茨共同撰写，他们来自圣布鲁克大学的人类语言分析系。幻灯片的背景是白色的，幻灯片的左下角有一个红色的标志，上面写着“圣布鲁克大学人类语言分析系”。</sample>
    <sample id="1498">What is Cognitive Dissonance? "two elements of cognition (i.e., thoughts, actions, beliefs) that are inconsistent" (Harmon-Jones and Harmon-Jones, 2007)</sample>
    <sample id="1499">“认知不协调”是什么？“认知的两个元素（即思想、行动、信仰）不一致”(哈罗姆-琼斯和哈罗姆-琼斯，2007)。我知道香烟会杀了我……我相信我抽了几支烟后今天会议。</sample>
    <sample id="1500">认知不一致是指认知（即思想、行动、信仰）中的两个元素不一致。用户通过将两句话/陈述表达为关系来表达这种不一致。</sample>
    <sample id="1501">“两认知元素（即思想、行动、信仰）不一致” 以用户语言表达为两短语/陈述之间的关系。 相比其他 discourse 关系，这种现象在语言中相对罕见。</sample>
    <sample id="1502">为什么不一致？ disagreement的影響

Eddie Harmon-Jones和Judson Mills. 2019. 一种认知不一致理论的介绍及其当前观点。认知不一致：一种在心理学中的关键理论。</sample>
    <sample id="1503">Why dissonance? Effects of disagreement Attitudes and Belief trends Anxiety disorders</sample>
    <sample id="1504">Why dissonance?
Effects of disagreement
Entry and Exit from
Attitudes and Belief trends
Anxiety disorders</sample>
    <sample id="1505">Why dissonance?
Effects of disagreement
Cognitive Styles
Entry and Exit from
Attitudes and Belief trends
Anxiety disorders</sample>
    <sample id="1506">标注

步骤1：质量良好吗？
步骤2：标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注吗？
标注</sample>
    <sample id="1507">标注
1.优质吗？（Step 1: Good parsing quality?）
2.分歧吗？（Step 2: Dissonance?）
3.一致吗？（Step 3: Concordance?）

@user_handle -3.8%
用户 -48%
分歧 -48%
标注 -Neither

我猜它是一件好事，但不能同时持有愤怒的情绪。</sample>
    <sample id="1508">标注

步骤1：质量良好吗？
步骤2：标注吗？
@user HANDLE
-3.8%
-48%
-48%
用户
解析
“我可以保持不耐烦，但我想它是一件好事，但不能同时。”
标注
标注员</sample>
    <sample id="1509">训练初始标注集

RoBERTa-base分类器头
TRAIN

小标注数据集：43/901分歧，不如随机猜测好</sample>
    <sample id="1510">训练初始标注集

RoBERTa-base + classifier head

TRAIN

int dataset

0.50

0.55

0.60

0.65

小标注数据集：43/901 不一致，不如随机猜测好</sample>
    <sample id="1511">方法：迁移学习和主动学习用于标注罕见类别

流程图展示了迁移学习和主动学习在标注罕见类别的过程。流程图分为以下几个步骤：

1. **初始模型 - 迁移学习**：首先，使用迁移学习构建一个初始模型。迁移学习是一种机器学习技术，其中预训练的模型被用于解决新任务。在这个过程中，模型会从大量数据中学习通用特征，然后将这些特征应用于标注罕见类别的任务。

2. **模型评估与迭代更新**：模型在标注罕见类别时进行评估，根据评估结果对模型进行迭代更新。这包括重新训练模型以提高其在标注罕见类别方面的准确性。

3. **主动学习迭代**：在模型迭代更新后，进入主动学习迭代阶段。在这个阶段，系统会识别出哪些样本对于提高模型性能至关重要，并选择这些样本进行标注。这样可以更有效地利用标注资源，同时提高模型的标注罕见类别的能力。

4. **模型重训练/更新**：在标注了新的样本后，模型会再次进行重训练或更新。这个过程帮助模型更好地学习罕见类别的特征，从而提高标注罕见类别的准确性和效率。

通过上述步骤，迁移学习和主动学习相结合，可以有效地标注罕见类别，提高模型在处理罕见类别数据时的性能。</sample>
    <sample id="1512">冷启动注释：迁移学习

图片描述：

该图片展示了迁移学习在处理罕见类别注释时的应用。图中有一个流程图，展示了从初始模型到最终模型的迁移学习过程。流程图分为以下几个步骤：

1. **初始模型**：这是迁移学习的起点，使用一个已经训练好的模型作为基础。
2. **训练**：在这个阶段，模型开始使用新的数据集进行训练。
3. **累积数据（CDM）**：模型在训练过程中积累数据，这些数据可能包括罕见类别的样本。
4. **模型更新/迭代**：根据新积累的数据，模型进行更新和迭代，以提高对罕见类别的识别能力。
5. **主动学习**：通过选择性地标注一些关键或不确定的样本，进一步提高模型的性能。
6. **添加新例子**：将更多新数据加入模型，继续迭代训练。

整个流程图强调了迁移学习在处理罕见类别注释时的重要性，通过持续的训练、数据积累和模型更新，逐步提高模型对罕见类别的识别和分类能力。</sample>
    <sample id="1513">冷启动注释：迁移学习

Roberta-base + classifier head

init. train

训练后

在结合Debate和GE数据上

训练

Debate

CE

Debate CE

AUC)</sample>
    <sample id="1514">冷启动注释：迁移学习

RoBERTa-base + classifier head

训练前
训练后

训练后在结合Debate和GE数据上

训练后在结合Debate和GE数据上，Debate stance in debate forums: Vaidya, Varsha, Venkatesan, Nikita Sont, Piyush Verma, and Ramesh Prasad. (2019). Debating stance detection in social media: The role of topic diversity. In Proceedings of the 20th Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</sample>
    <sample id="1515">冷启动注释：迁移学习
RoBERTa-base + classifier head
init debate
TRAIN
Transferred after training on
combined Debate and GE data
Debate CE
0.65
0.60
0.50
0.70
0.65
0.60
0.55
0.50
AUC)
Debate stance in debate forums: Vaidhyanathan, Nils T. Sont,</sample>
    <sample id="1516">冷启动注释：迁移学习 RoBERTa-base + classifier head TRAIN int. dataset Debate CE Debate-CE Debate+CE ICE Area under the ROC curve (AUC)</sample>
    <sample id="1517">主动学习：累积更新 vs 迭代更新

该幻灯片展示了两种主动学习策略：累积更新（Cumulative）和迭代更新（Iterative）。它解释了在处理罕见类别标注时，这两种策略的差异。在累积更新中，模型使用新旧数据进行训练，而迭代更新则在每次迭代中使用新旧数据对模型进行微调。

幻灯片还展示了两种策略的流程图。在累积更新中，新旧数据被合并在一起进行训练。而在迭代更新中，模型在每次迭代中都会重新训练，以适应新旧数据的变化。

幻灯片还提到了主动学习的策略，包括选择策略、标注策略和添加策略。选择策略涉及选择哪些样本进行标注，标注策略涉及如何标注这些样本，而添加策略则涉及何时添加新样本到模型中。

总的来说，这张幻灯片提供了一个关于主动学习策略的概述，包括累积更新和迭代更新，以及主动学习的策略。</sample>
    <sample id="1518">主动学习：累积更新与迭代更新

图表展示了不同主动学习策略在累积更新和迭代更新下的性能比较。图表的x轴表示不同的主动学习策略，包括随机、熵、CoreSet、CAL和PRC。y轴表示性能指标AUC（曲线下面积），范围从0.5到0.75。

- 随机策略在累积更新和迭代更新下都表现出较低的AUC值，分别约为0.65和0.6。
- 熵策略在累积更新下的AUC值约为0.7，而在迭代更新下的AUC值约为0.65。
- CoreSet策略在累积更新下的AUC值约为0.75，而在迭代更新下的AUC值约为0.7。
- CAL策略在累积更新下的AUC值约为0.7，而在迭代更新下的AUC值约为0.65。
- PRC策略在累积更新下的AUC值约为0.75，而在迭代更新下的AUC值约为0.65。

图表中使用了蓝色和黄色柱状图来区分累积更新和迭代更新。累积更新用蓝色表示，迭代更新用黄色表示。图表还显示了累积更新和迭代更新之间的性能差异，其中累积更新通常表现出更高的AUC值。</sample>
    <sample id="1519">主动学习：罕见类概率策略

图片描述了一个主动学习过程的流程图，特别关注罕见类别的处理。流程图展示了从初始模型到最终模型迭代更新的过程。流程图分为几个主要阶段：

1. **初始模型与数据**：流程图开始于一个初始模型和一组数据。这些数据被输入到模型中进行训练。

2. **模型训练**：在这一阶段，模型使用输入数据进行训练。模型学习数据中的模式和关系。

3. **模型评估**：经过训练后，模型被评估以确定其性能。这通常涉及计算准确率、召回率和其他相关指标。

4. **罕见类别识别**：在评估阶段，流程图识别罕见类别。罕见类别是指在数据集中出现频率较低的类别。

5. **罕见类别标注**：一旦识别出罕见类别，流程图将这些类别标记为需要更多关注的对象。这可能涉及增加罕见类别的样本数量或调整模型参数以更好地处理这些类别。

6. **模型更新**：在罕见类别被标注后，流程图返回到模型训练阶段。模型使用更新后的数据集进行训练，以提高对罕见类别的识别能力。

7. **迭代更新**：流程图展示了模型的迭代更新过程。每次迭代都会根据罕见类别的标注结果调整模型，以提高其性能。

8. **最终模型**：经过多次迭代更新后，流程图最终得到一个更准确和全面的模型，能够更好地处理罕见类别。

9. **模型应用**：最后，流程图展示了模型在实际应用中的应用。模型可以用于预测新数据中的罕见类别，从而在各种领域（如医疗诊断、图像识别等）中提供更有效的解决方案。

整个流程图展示了主动学习如何通过持续迭代和优化来提高模型对罕见类别的处理能力。</sample>
    <sample id="1520">主动学习：罕见类概率策略

该图展示了主动学习中罕见类概率策略的流程。流程从“初始模型”开始，通过“转移学习”进行训练。然后，系统会评估新数据集中的样本，确定哪些样本属于罕见类。对于罕见类样本，系统会提出问题：“罕见类的概率增加吗？”和“更容易标注吗？”以决定是否需要标注这些样本。

如果系统认为罕见类的概率增加且容易标注，则会进入“标注”阶段。在标注阶段，人类将罕见类样本进行标注。标注完成后，系统会将新标注的样本添加到数据集中，并使用这些新样本对模型进行“模型更新/迭代”。然后，流程返回“初始模型”阶段，继续训练模型。

该图还展示了“累积数据（CDM）”和“迭代训练（ITT）”的概念，表示主动学习过程中数据和模型的迭代更新。</sample>
    <sample id="1521">主动学习：罕见类概率策略

主动学习策略比较（AUCs）

基线：从头开始
迁移模型
AL-Random
AL-Energy
AL-CoreSet
AL-CAL
AL-PRFC（纯）</sample>
    <sample id="1522">主动学习：罕见类概率策略

主动学习策略比较（AUCs）

基线：从头开始
转换模型
AL-Random
AL-Entropy
AL-CAL
AL-PRC
AL-Best Transfer（最佳传输）</sample>
    <sample id="1523">主动学习：概率罕见类策略

主动学习策略特征表

| 主动学习策略 | 罕见率 (%) | 时间 (s) | 子主题差异 |
| --- | --- | --- | --- |
| 随机 | 3.20 | 11.96 | -0.065 |
| ENTROPY | 6.80 | 12.72 | 0.035 |
| CORRECTION | 4.80 | 11.89 | 0.039 |
| CAL | 4.80 | 11.88 | -0.045 |
| PRC | 7.60 | 13.55 | -0.071 |

* 最小注释成本并不一定导致更好的模型
* 反之，可能会使注释更加困难，认知失衡是其中一个类别
* 为了增加失衡样本，PRC效果最好</sample>
    <sample id="1524">图示了在机器学习中处理罕见类别的几种方法。第一种方法是“冷启动AI与无监督学习”，它展示了如何通过识别和处理罕见类别（如图中的“干草堆”中的“针”）来训练模型。这种方法通常涉及大量数据，但可能效率较低。

第二种方法是“迭代式”，它展示了如何通过多次迭代来逐步提高模型的性能。这种方法通常涉及大量的计算资源，但可以提供更准确的结果。

第三种方法是“累积式”，它展示了如何通过将多个模型组合在一起，以提高模型的性能。这种方法通常涉及大量的计算资源，但可以提供更准确的结果。

总的来说，这些方法展示了如何在机器学习中处理罕见类别，以提高模型的性能。</sample>
    <sample id="1525">### 演示幻灯片翻译

#### Takeaways
- **Rare class annotation - "needle in a haystack"**
  - PRC is simple &amp; efficient for rare sample acquisition

#### Cold-start AL with few-shot learning
- **Out-of-domain: Iterative**
- **In-domain: Cumulative**

### 图片描述
1. **Rare class annotation - "needle in a haystack"**
   - 这个部分展示了在大量数据中找到罕见类别的挑战。图中有一个 haystack，其中有一个被标记的 needle（代表罕见类别）。PRC 被介绍为一种简单且高效的获取罕见样本的方法。

2. **Cold-start AL with few-shot learning**
   - **Out-of-domain: Iterative**
     - 这个部分展示了在冷启动学习（Cold-start Active Learning, AL）中的迭代方法。它展示了如何通过迭代过程逐步收集和标注数据，以解决领域外（out-of-domain）问题。
   - **In-domain: Cumulative**
     - 这个部分展示了在冷启动学习中的累积方法。它展示了如何通过累积多个模型（M0, M1, M2, M3）来逐步提高对领域内（in-domain）问题的理解和标注能力。

### 详细解释
- **Rare class annotation - "needle in a haystack"**
  - 在机器学习和数据标注中，罕见类别（rare classes）指的是数据集中出现频率极低的类别。由于罕见类别的数据点数量少，标注这些数据点的成本高且耗时。PRC（可能是“Prioritized Rare Class”或类似的概念）提供了一种有效的方法来优先标注罕见类别，从而提高模型在处理罕见类别时的性能。

- **Cold-start AL with few-shot learning**
  - **Out-of-domain: Iterative**
    - 在冷启动学习中，模型需要在没有先验知识的情况下开始学习。迭代方法意味着模型在每个迭代步骤中都会收集和标注更多的数据，逐步提高其在领域外问题上的表现。这种方法特别适用于那些数据分布与训练数据不同的情况。

  - **In-domain: Cumulative**
    - 累积方法意味着模型在每个步骤中都会使用之前收集和标注的数据来提高其在领域内问题上的表现。这种方法特别适用于那些数据分布与训练数据相似的情况，通过累积更多标注数据来提高模型的准确性和泛化能力。

### 总结
- 本演示幻灯片主要介绍了在处理罕见类别和冷启动学习问题时的几种策略。PRC 提供了一种有效的方法来标注罕见类别，而冷启动 AL 的迭代和累积方法则分别适用于领域外和领域内问题，通过逐步收集和标注数据来提高模型的性能。</sample>
    <sample id="1526">Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge

联系：varadajan@cs.stonybrook.edu, siuhong@cs.stonybrook.edu, has@cs.stonybrook.edu

代码：https://github.com/hasanahmadi/rare-class-AI

数据集：https://github.com/humanistictabular/rare-dissonance-better-dataset

论文：https://arxiv.org/abs/2205.02499</sample>
    <sample id="1527">根据幻灯片显示，这篇论文的作者分别来自以下机构：信息学系、NLP中心、Sarlande研究实验室和阿姆斯特丹大学。这些机构的标志在幻灯片底部可见。</sample>
    <sample id="1528">演讲者的名字是“Siyu Yuan, Jianqiang Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang”。</sample>
    <sample id="1529">根据幻灯片显示，这篇论文有五位作者：Patrick Fernandes、Kayo Yin、Emmy Liu、Andre F. T. Martins和Graham Neubig。</sample>
    <sample id="1530">根据图片中显示的图表，该方法与一种特定的 simulST 架构进行了比较。图表的标题为“Main Results: EDAtt”，表明了EDAtt方法的评估结果。图表的x轴表示“AL / AL-CA (s)”，y轴表示“BLEU”，这是评估机器翻译性能的指标。图表中绘制了不同的线，代表不同的策略或模型，如“wait-k”、“LA”、“CAAT”和“EDAtt”。这些线表示了在不同时间间隔（AL / AL-CA）下不同策略的BLEU得分。因此，该方法与EDAtt方法进行了比较，因为图表的标题直接指出了这一点。</sample>
  </task>
</testset>