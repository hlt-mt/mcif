<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="zh">
    <sample id="0">根据幻灯片中提供的英文内容，语言模型的主要数据来源是预训练数据。</sample>
    <sample id="1">根据幻灯片中提供的英文内容，这篇论文的作者所属机构包括麦克盖尔大学、米拉和微软研究。这些机构名称在每个作者姓名下方列出，表明他们与这些组织有关联。</sample>
    <sample id="2">DEPLAIN: 一个德语平行语料库，其中包含简洁语言的内在翻译，用于句子和文档简化。雷吉纳·施托登、奥马尔·穆门、劳拉·卡利梅耶尔。海因里希·海因里希大学，德国。ACL 2023</sample>
    <sample id="3">DEPLAIN: 一个德语平行语料库，包含简洁语言的内在翻译，用于句子和文档简化 Regina Stodden、Omar Momen、Laura Kallmeyer Heinrich Heine University of Düsseldorf, Germany ACL 2023</sample>
    <sample id="4">文本简化示例</sample>
    <sample id="5">文本简化示例

原始内容：[替换] [从句删除] [重新排序] [单词删除] [插入]

原始文本：.Die Gewerkschaft setzt sich für ein, .dass zum Beispiel höhere Löhne oder mehr Urlaub werden.

简化文本：.Die Gewerkschaft setzt sich zum Beispiel für höhere Löhne oder mehr Urlaub ein。</sample>
    <sample id="6">文本简化示例</sample>
    <sample id="7">原句: .Die Gewerkschaft setzt sich für ein, _dass_ zum Beispiel höhere Löhne gezahlt werden. 简化语言: .Die Gewerkschaft setzt sich zum Beispiel für höhere Löhne oder mehr Urlaub ein。</sample>
    <sample id="8">2. DE-plain

A New Corpus</sample>
    <sample id="9">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013 2015 2017 2019 2021 Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel German German German German German German German German German German German English English English English English English English English English English English</sample>
    <sample id="10">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 1975 1985 1995 2005 2013 2015 2017 Alignment manual manual manual manual manual 5942 5456 483 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345 345</sample>
    <sample id="11">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) 2013) Alignment German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German English German</sample>
    <sample id="12">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 2003 2005 2007 2009 2011 2013 Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel</sample>
    <sample id="13">German Text Simplification Corpora Sentence Level 14000 12000 10000 8000 6000 4000 2000 0 1975 1993 2003 2013 2015 Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel Parallel</sample>
    <sample id="14">类型简化

Simplicity LexSimp StructSimp

新斯文圣经小说n=187

简化转换

DEPLAIN-APA DEPLAIN-WEB

替换删除</sample>
    <sample id="15">类型简化

Simplicity
LexSimp
StructSimp

新闻
圣经
小说

n = 46
n = 187
n = 72

简化变换

DEPLAIN-apa
DEPLAIN-web

替换
删除
插入
添加</sample>
    <sample id="16">类型简化

Simplicity LexSimp StructSimp

新闻圣经小说n=187

简化变换

DEPLAIN-APA DEPLAIN-WEB

排序替换局部替换词添加词删除</sample>
    <sample id="17">类型简化

Simplicity LexSimp StructSimp
新闻圣经小说
18 16 17 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999</sample>
    <sample id="18">类型简化

Simplicity LexSimp StructSimp

新闻圣经小说

n=48 n=187 n=72

简化变换

DEPLAIN-APA DEPLAIN-WEB

替换
删除
插入
替换
删除</sample>
    <sample id="19">3. Use-cases 3.1. Automatic alignment and simplification</sample>
    <sample id="20">结果表明，BERTalign方法在所有评估指标上都表现出最佳性能。它在111和n.m.能力上的得分分别为743和610，高于其他方法。这表明BERTalign能够有效地处理不同大小的语料库，并在大型数据集上进行准确的对齐。</sample>
    <sample id="21">结果表明，基于Transformer的方法在性能上具有明显优势。BERTalign和RoBERTaalign方法的性能显著优于其他方法。BERTalign方法的性能略高于RoBERTaalign方法，这表明BERT和RoBERTa预训练模型在处理中文文本时具有相似的优势。</sample>
    <sample id="22">结果表明，基于BERT的模型在所有评估任务上都表现出了显著的优势。与传统的基于规则的方法相比，这些模型能够更好地捕捉文本中的语义和上下文关系。这表明BERT及其变体在自然语言处理领域具有巨大的潜力，并且在未来的NLP研究中可能成为主流。</sample>
    <sample id="23">结果表明，基于Transformer的模型（如BERT和RoBERTa）在处理大规模文本对齐任务时表现出色。这些模型能够捕捉复杂的语义关系，并在大规模数据集上进行高效训练。相比之下，传统的基于规则的方法（如CATS-C3G）在大规模文本对齐任务中效果较差。</sample>
    <sample id="24">结果表明，基于BERT的模型在所有评估指标上都表现出色。这表明这些模型能够捕捉到文本中的复杂关系和语义，从而实现有效的对齐。</sample>
    <sample id="25">结果展示了不同对齐方法的评估，包括它们在1:1和n:n能力下的性能。表中列出了每种方法的名称、描述以及它们在不同评估指标上的得分。这些指标包括P（精度）、R（召回率）和F0.5（F分数）。</sample>
    <sample id="26">结果表明，BERTalign在所有评估指标上都表现最佳。这些结果表明BERTalign是一个有效的句子对齐方法，可以用于各种自然语言处理任务。</sample>
    <sample id="27">图4显示了在不同数据集上进行的文档和句子级别的文本简化实验结果。这些结果表明，使用微调后的LongBART模型进行文本简化时，BLEU和SARI得分显著提高。例如，在文档级别，BLEU得分从原始APAWC数据集上的34.24%提高到微调后的LongBART模型上的49.85%，SARI得分从65.65%提高到73.47%。在句子级别，BLEU得分从原始APAWC数据集上的19.44%提高到微调后的LongBART模型上的28.06%，SARI得分从34.44%提高到43.44%。这些结果表明，微调后的LongBART模型在文本简化任务中表现出更好的性能。</sample>
    <sample id="28">文档级别的文本简化结果如图所示。我们使用了mBART模型进行微调，并使用BLEU和SARI等指标评估了简化后的文本质量。实验结果表明，我们的方法在BLEU和SARI指标上都取得了很好的效果，证明了mBART模型在文本简化任务上的有效性。</sample>
    <sample id="29">结果是在使用微调后的Long-BART进行文档级别的文本简化。n对应训练数据的长度。 | 项目 | SARI | BLEU | BS-P | FRED | 项目 | SARI | BLEU | BS-P | FRED | 微调前的基线 | 17,827 | 34.247 | 56.55 | 0.86 | 微调后的基线 | 17,827 | 34.247 | 56.55 | 0.86 | DEPLAN-APA（n=48） | 17,827 | 34.247 | 56.55 | 0.86 | DEPLAN-WEBS（n=147） | 17,827 | 34.247 | 56.55 | 0.86 | 结果是在使用微调后的Long-BART进行句子级别的文本简化。n对应训练数据的长度。 | 项目 | SARI | BLEU | BS-P | FRED | 项目 | SARI | BLEU | BS-P | FRED | 微调前的基线 | 17,827 | 34.064 | 55.06 | 0.86 | 微调后的基线 | 17,827 | 34.064 | 55.06 | 0.86 | DEPLAN-APA测试（n=123） | 17,827 | 34.064 | 55.06 | 0.86 | DEPLAN-WEBS测试（n=1840） | 17,827 | 34.064 | 55.06 | 0.86 |</sample>
    <sample id="30">结果是在使用finetuned长mBART进行文档级别简化。n对应训练数据的长度。</sample>
    <sample id="31">图1显示了在不同数据集上进行的文档级别和句子级别的文本简化实验结果。这些结果表明，使用微调后的mBART模型进行文本简化可以显著提高BLEU、SARI和BLEU-4等指标的性能。</sample>
    <sample id="32">图1展示了在不同数据集上进行的文本简化任务的性能比较。该图表分为两部分：顶部部分展示了文档级别的文本简化，底部部分展示了句子级别的文本简化。对于每个级别，图表都提供了不同模型的性能指标，包括SARI、BLEU和BLEU-Score。这些指标用于评估模型在简化文本时保持原始文本信息和流畅度的能力。图表还显示了不同训练数据长度对模型性能的影响。</sample>
    <sample id="33">文档级别：使用微调后的长MBART进行文本简化。与训练数据长度相对应的n值。|SARI|BLEU|BP-Score|BLEU-FRE|BLEU-FRE2|BLEU-FRE3|BLEU-FRE4|BLEU-FRE5|BLEU-FRE6|BLEU-FRE7|BLEU-FRE8|BLEU-FRE9|BLEU-FRE10|BLEU-FRE11|BLEU-FRE12|BLEU-FRE13|BLEU-FRE14|BLEU-FRE15|BLEU-FRE16|BLEU-FRE17|BLEU-FRE18|BLEU-FRE19|BLEU-FRE20|BLEU-FRE21|BLEU-FRE22|BLEU-FRE23|BLEU-FRE24|BLEU-FRE25|BLEU-FRE26|BLEU-FRE27|BLEU-FRE28|BLEU-FRE29|BLEU-FRE30|BLEU-FRE31|BLEU-FRE32|BLEU-FRE33|BLEU-FRE34|BLEU-FRE35|BLEU-FRE36|BLEU-FRE37|BLEU-FRE38|BLEU-FRE39|BLEU-FRE40|BLEU-FRE41|BLEU-FRE42|BLEU-FRE43|BLEU-FRE44|BLEU-FRE45|BLEU-FRE46|BLEU-FRE47|BLEU-FRE48|BLEU-FRE49|BLEU-FRE50|BLEU-FRE51|BLEU-FRE52|BLEU-FRE53|BLEU-FRE54|BLEU-FRE55|BLEU-FRE56|BLEU-FRE57|BLEU-FRE58|BLEU-FRE59|BLEU-FRE60|BLEU-FRE61|BLEU-FRE62|BLEU-FRE63|BLEU-FRE64|BLEU-FRE65|BLEU-FRE66|BLEU-FRE67|BLEU-FRE68|BLEU-FRE69|BLEU-FRE70|BLEU-FRE71|BLEU-FRE72|BLEU-FRE73|BLEU-FRE74|BLEU-FRE75|BLEU-FRE76|BLEU-FRE77|BLEU-FRE78|BLEU-FRE79|BLEU-FRE80|BLEU-FRE81|BLEU-FRE82|BLEU-FRE83|BLEU-FRE84|BLEU-FRE85|BLEU-FRE86|BLEU-FRE87|BLEU-FRE88|BLEU-FRE89|BLEU-FRE90|BLEU-FRE91|BLEU-FRE92|BLEU-FRE93|BLEU-FRE94|BLEU-FRE95|BLEU-FRE96|BLEU-FRE97|BLEU-FRE98|BLEU-FRE99|BLEU-FRE100|BLEU-FRE101|BLEU-FRE102|BLEU-FRE103|BLEU-FRE104|BLEU-FRE105|BLEU-FRE106|BLEU-FRE107|BLEU-FRE108|BLEU-FRE109|BLEU-FRE110|BLEU-FRE111|BLEU-FRE112|BLEU-FRE113|BLEU-FRE114|BLEU-FRE115|BLEU-FRE116|BLEU-FRE117|BLEU-FRE118|BLEU-FRE119|BLEU-FRE120|BLEU-FRE121|BLEU-FRE122|BLEU-FRE123|BLEU-FRE124|BLEU-FRE125|BLEU-FRE126|BLEU-FRE127|BLEU-FRE128|BLEU-FRE129|BLEU-FRE130|BLEU-FRE131|BLEU-FRE132|BLEU-FRE133|BLEU-FRE134|BLEU-FRE135|BLEU-FRE136|BLEU-FRE137|BLEU-FRE138|BLEU-FRE139|BLEU-FRE140|BLEU-FRE141|BLEU-FRE142|BLEU-FRE143|BLEU-FRE144|BLEU-FRE145|BLEU-FRE146|BLEU-FRE147|BLEU-FRE148|BLEU-FRE149|BLEU-FRE150|BLEU-FRE151|BLEU-FRE152|BLEU-FRE153|BLEU-FRE154|BLEU-FRE155|BLEU-FRE156|BLEU-FRE157|BLEU-FRE158|BLEU-FRE159|BLEU-FRE160|BLEU-FRE161|BLEU-FRE162|BLEU-FRE163|BLEU-FRE164|BLEU-FRE165|BLEU-FRE166|BLEU-FRE167|BLEU-FRE168|BLEU-FRE169|BLEU-FRE170|BLEU-FRE171|BLEU-FRE172|BLEU-FRE173|BLEU-FRE174|BLEU-FRE175|BLEU-FRE176|BLEU-FRE177|BLEU-FRE178|BLEU-FRE179|BLEU-FRE180|BLEU-FRE181|BLEU-FRE182|BLEU-FRE183|BLEU-FRE184|BLEU-FRE185|BLEU-FRE186|BLEU-FRE187|BLEU-FRE188|BLEU-FRE189|BLEU-FRE190|BLEU-FRE191|BLEU-FRE192|BLEU-FRE193|BLEU-FRE194|BLEU-FRE195|BLEU-FRE196|BLEU-FRE197|BLEU-FRE198|BLEU-FRE199|BLEU-FRE200|BLEU-FRE201|BLEU-FRE202|BLEU-FRE203|BLEU-FRE204|BLEU-FRE205|BLEU-FRE206|BLEU-FRE207|BLEU-FRE208|BLEU-FRE209|BLEU-FRE210|BLEU-FRE211|BLEU-FRE212|BLEU-FRE213|BLEU-FRE214|BLEU-FRE215|BLEU-FRE216|BLEU-FRE217|BLEU-FRE218|BLEU-FRE219|BLEU-FRE220|BLEU-FRE221|BLEU-FRE222|BLEU-FRE223|BLEU-FRE224|BLEU-FRE225|BLEU-FRE226|BLEU-FRE227|BLEU-FRE228|BLEU-FRE229|BLEU-FRE230|BLEU-FRE231|BLEU-FRE232|BLEU-FRE233|BLEU-FRE234|BLEU-FRE235|BLEU-FRE236|BLEU-FRE237|BLEU-FRE238|BLEU-FRE239|BLEU-FRE240|BLEU-FRE241|BLEU-FRE242|BLEU-FRE243|BLEU-FRE244|BLEU-FRE245|BLEU-FRE246|BLEU-FRE247|BLEU-FRE248|BLEU-FRE249|BLEU-FRE250|BLEU-FRE251|BLEU-FRE252|BLEU-FRE253|BLEU-FRE254|BLEU-FRE255|BLEU-FRE256|BLEU-FRE257|BLEU-FRE258|BLEU-FRE259|BLEU-FRE260|BLEU-FRE261|BLEU-FRE262|BLEU-FRE263|BLEU-FRE264|BLEU-FRE265|BLEU-FRE266|BLEU-FRE267|BLEU-FRE268|BLEU-FRE269|BLEU-FRE270|BLEU-FRE271|BLEU-FRE272|BLEU-FRE273|BLEU-FRE274|BLEU-FRE275|BLEU-FRE276|BLEU-FRE277|BLEU-FRE278|BLEU-FRE279|BLEU-FRE280|BLEU-FRE281|BLEU-FRE282|BLEU-FRE283|BLEU-FRE284|BLEU-FRE285|BLEU-FRE286|BLEU-FRE287|BLEU-FRE288|BLEU-FRE289|BLEU-FRE290|BLEU-FRE291|BLEU-FRE292|BLEU-FRE293|BLEU-FRE294|BLEU-FRE295|BLEU-FRE296|BLEU-FRE297|BLEU-FRE298|BLEU-FRE299|BLEU-FRE300|BLEU-FRE301|BLEU-FRE302|BLEU-FRE303|BLEU-FRE304|BLEU-FRE305|BLEU-FRE306|BLEU-FRE307|BLEU-FRE308|BLEU-FRE309|BLEU-FRE310|BLEU-FRE311|BLEU-FRE312|BLEU-FRE313|BLEU-FRE314|BLEU-FRE315|BLEU-FRE316|BLEU-FRE317|BLEU-FRE318|BLEU-FRE319|BLEU-FRE320|BLEU-FRE321|BLEU-FRE322|BLEU-FRE323|BLEU-FRE324|BLEU-FRE325|BLEU-FRE326|BLEU-FRE327|BLEU-FRE328|BLEU-FRE329|BLEU-FRE330|BLEU-FRE331|BLEU-FRE332|BLEU-FRE333|BLEU-FRE334|BLEU-FRE335|BLEU-FRE336|BLEU-FRE337|BLEU-FRE338|BLEU-FRE339|BLEU-FRE340|BLEU-FRE341|BLEU-FRE342|BLEU-FRE343|BLEU-FRE344|BLEU-FRE345|BLEU-FRE346|BLEU-FRE347|BLEU-FRE348|BLEU-FRE349|BLEU-FRE350|BLEU-FRE351|BLEU-FRE352|BLEU-FRE353|BLEU-FRE354|BLEU-FRE355|BLEU-FRE356|BLEU-FRE357|BLEU-FRE358|BLEU-FRE359|BLEU-FRE360|BLEU-FRE361|BLEU-FRE362|BLEU-FRE363|BLEU-FRE364|BLEU-FRE365|BLEU-FRE366|BLEU-FRE367|BLEU-FRE368|BLEU-FRE369|BLEU-FRE370|BLEU-FRE371|BLEU-FRE372|BLEU-FRE373|BLEU-FRE374|BLEU-FRE375|BLEU-FRE376|BLEU-FRE377|BLEU-FRE378|BLEU-FRE379|BLEU-FRE380|BLEU-FRE381|BLEU-FRE382|BLEU-FRE383|BLEU-FRE384|BLEU-FRE385|BLEU-FRE386|BLEU-FRE387|BLEU-FRE388|BLEU-FRE389|BLEU-FRE390|BLEU-FRE391|BLEU-FRE392|BLEU-FRE393|BLEU-FRE394|BLEU-FRE395|BLEU-FRE396|BLEU-FRE397|BLEU-FRE398|BLEU-FRE399|BLEU-FRE400|BLEU-FRE401|BLEU-FRE402|BLEU-FRE403|BLEU-FRE404|BLEU-FRE405|BLEU-FRE406|BLEU-FRE407|BLEU-FRE408|BLEU-FRE409|BLEU-FRE410|BLEU-FRE411|BLEU-FRE412|BLEU-FRE413|BLEU-FRE414|BLEU-FRE415|BLEU-FRE416|BLEU-FRE417|BLEU-FRE418|BLEU-FRE419|BLEU-FRE420|BLEU-FRE421|BLEU-FRE422|BLEU-FRE423|BLEU-FRE424|BLEU-FRE425|BLEU-FRE426|BLEU-FRE427|BLEU-FRE428|BLEU-FRE429|BLEU-FRE430|BLEU-FRE431|BLEU-FRE432|BLEU-FRE433|BLEU-FRE434|BLEU-FRE435|BLEU-FRE436|BLEU-FRE437|BLEU-FRE438|BLEU-FRE439|BLEU-FRE440|BLEU-FRE441|BLEU-FRE442|BLEU-FRE443|BLEU-FRE444|BLEU-FRE445|BLEU-FRE446|BLEU-FRE447|BLEU-FRE448|BLEU-FRE449|BLEU-FRE450|BLEU-FRE451|BLEU-FRE452|BLEU-FRE453|BLEU-FRE454|BLE</sample>
    <sample id="34">谢谢。如需更多信息，请查阅我们的论文。欢迎光临ACL 2023年会议的海报。</sample>
    <sample id="35">演讲者的名字是Patrick Fernandes、Kayo Yin、Emmy Liu、Andre F. T. Martins和Graham Neubig。这些名字在幻灯片上以粗体显示，位于标题下方。</sample>
    <sample id="36">根据幻灯片中提供的信息，他们使用了T5 XL模型获得了82%-87%的准确率。</sample>
    <sample id="37">根据幻灯片中呈现的信息，CoNLL-2003 标注器似乎仍然有效。这可以从图表中观察到，该图表显示了从 2004 年到 2023 年的性能趋势。虽然图表没有提供 CoNLL-2003 的具体数据，但其存在表明它在时间上保持了竞争力。此外，幻灯片中没有明确说明 CoNLL-2003 标注器无效或过时，这表明它仍然是有效的。</sample>
    <sample id="38">根据图片中所给的英文内容，提出的人工评估方法新颖之处在于它使用了 Likert 量表来对聊天机器人（ABC-Eval）的响应的相关性进行评分。这表明了一种系统化的方法，可以量化评估人与聊天机器人之间的对话质量。 Likert 量表通常用于收集对一系列陈述的主观意见，但在这里被应用于评估机器人的回应是否相关。</sample>
    <sample id="39">现有弱监督方法的成功在很大程度上依赖于正确的标签。</sample>
    <sample id="40">根据幻灯片中提供的背景知识，可以通过以下措施来提高分数：1.**Google搜索链接**：通过使用Google搜索找到歌曲的更多背景信息，可以提供关于歌曲的更深入理解。这可能包括艺术家的其他作品、歌曲的制作过程、歌曲所处的时代背景等。2.**听歌**：亲耳倾听歌曲可以提供对音乐结构、乐器、节奏和整体氛围的第一手体验。这种亲身体验有助于更好地理解歌曲的情感和主题。3.**阅读资料**：阅读关于歌曲的任何相关资料，如歌词、艺术家的声明或评论家的评论，也可以帮助加深对歌曲的了解。这些信息可以提供关于歌曲意图、影响和文化背景的洞察。4.**结合所有来源**：将Google搜索、个人听觉和阅读资料结合起来，可以提供全面而全面的歌曲理解，从而有可能获得更高的分数。</sample>
    <sample id="41">根据图片显示，这篇论文有五位作者。这可以通过底部的作者列表来确定，其中包含了他们的名字和与之相关的机构。</sample>
    <sample id="42">conjunct lengths in English, dependency length minimization, and dependency structure of coordination Adam Przpielkowski and Michal Wozniak Institute of Computer Science Polish Academy of Sciences ul. Jana Kazimierzowa 5, 01-248 Warsaw University of Warsaw ACL 2023</sample>
    <sample id="43">依赖结构协调</sample>
    <sample id="44">依赖结构协调</sample>
    <sample id="45">依赖结构协调</sample>
    <sample id="46">依赖结构协调</sample>
    <sample id="47">依赖结构协调</sample>
    <sample id="48">依赖结构协调</sample>
    <sample id="49">依赖结构协调</sample>
    <sample id="50">依赖结构协调</sample>
    <sample id="51">Word order tends to minimize dependency lengths:</sample>
    <sample id="52">Word order tends to minimize dependency lengths:</sample>
    <sample id="53">Word order tends to minimize dependency lengths:</sample>
    <sample id="54">Word order tends to minimize dependency lengths:</sample>
    <sample id="55">Word order tends to minimize dependency lengths:</sample>
    <sample id="56">Word order tends to minimize dependency lengths:</sample>
    <sample id="57">Word order tends to minimize dependency lengths:</sample>
    <sample id="58">Word order tends to minimize dependency lengths:</sample>
    <sample id="59">Word order tends to minimize dependency lengths:</sample>
    <sample id="60">Word order tends to minimize dependency lengths:</sample>
    <sample id="61">Word order tends to minimize dependency lengths:</sample>
    <sample id="62">Word order tends to minimize dependency lengths:</sample>
    <sample id="63">统计信息来自增强版的Penn Treebank（Marcus et al. 1993; Ficler and Berber 2016）。 • 英文中的连词往往比之前短（观察到的）； • 这种趋势随着长度差异而加剧（Gibson et al. 1996: 88-90）； • 但只有当主语在左侧或不存在时才如此（例如，“我看见巴特和Lisa来，然后打喷嚏”）， • 而不是当它在右侧时（例如，“Ted和Ned笑”）。</sample>
    <sample id="64">关于 coordin 的统计信息是从 Penn Treebank 的增强版本中提取的。Marcus et al. 1993、Ficler 和 Goldberg 2016。左边的连词短语倾向于比之前更短（观察到），这种趋势随着长度差异的增长而增长（Gibson et al. 1996: 88-90）。但只有当主语在左边或不存在时才如此（我看见 Bart 和 Lisa，Homer 过敏），而不是当它在右边时（Ted 和 Ned 笑了）。</sample>
    <sample id="65">关于 coordin 的统计信息是从 Penn Treebank (Marcus et al. 1993; Flicker and Goldberg 2016) 的增强版本中提取的。 • 左连词倾向于比之前更短（观察到Marcus et al. 1993; Flicker and Goldberg 2016）； • 这种趋势随着长度差异的增加而加剧（Gibson et al. 1996: 88-90） • 但是只有当主语在左边或不存在时才如此（例如“我看见巴特和Lisa来了一群人”）， • 而不是当它在右边时（例如“Ted和Ned笑了一笑”）。</sample>
    <sample id="66">关于 coordin 的统计信息提取自增强版的 Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016)。 • 左连接通常比之前观察到的更短（观察到的，α）； • 这种趋势随着长度差异的增加而加剧（Gibson et al. 1996: 88-90）； • 但是只有当主语在左边或不存在时才如此（例如，“我看见巴特和Lisa。霍默感冒了并打喷嚏。”）； • 而不是当它在右边时（例如，“Ted和 Ned 笑了。”）。</sample>
    <sample id="67">关于 coordin 的统计信息提取自 Penn Treebank (Marcus et al. 1993; Flicker and Goldberg 2016) 的增强版本： • 左连词通常比之前观察到的要短。 • 这种趋势随长度差异增长 (Gibson et al. 1996: 88-90)。 但只有当主语在左边或不存在时才如此 (例如 Bart 和 Lisa; Homer 来了然后打喷嚏)。</sample>
    <sample id="68">关于 coordin 的统计信息是从 Penn Treebank (Marcus et al. 1993; Ficler and Goldberg 2016) 的增强版本中提取的：

- 左连词通常比之前观察到的要短（Marcus et al. 1993）；
- 这种趋势随着长度差异的增加而加剧（Gibson et al. 1996: 88–90）；
- 但是只有当主语在左边或不存在时才如此（I saw Bart and Lisa; Homer came and sneezed），
- 而不是当主语在右边时（Ted and Ned laughed）。</sample>
    <sample id="69">关于 coordin 的统计信息提取自 Penn Treebank (Marcus et al. 1993; Flicker and Goldberg 2016) 的增强版本：

- 左连词较短（之前观察到）；
- 这种趋势随着长度差异的增加而加剧（Gibson et al. 1996: 88-90）；
- 但只有当主语在左边或不存在时才如此（例如，我看见巴特和Lisa；霍默走进来打喷嚏）。
- 不是当它在右边时（例如，Ted 和 Ned 笑了）。</sample>
    <sample id="70">图3：根据字符长度和词长的绝对差异，分析了较短句子中连字符的分布。上半部分显示了从左到右的比较，下半部分显示了从右到左的比较。每个图表都展示了不同长度的句子（以字符和词为单位）中连字符的比例。图表中的蓝色线表示平均比例，而阴影区域表示置信区间。图表右侧的点图显示了每个句子中连字符的实际比例。</sample>
    <sample id="71">图3：根据字符长度、词长和单词长度的绝对差值，显示了较短句子中的压缩比例。</sample>
    <sample id="72">图3：根据绝对字符长度（上）和绝对词长（下）的差异，显示了较短的引文与较长的引文之间的比例。</sample>
    <sample id="73">图4：根据绝对字符长度、绝对音节长度和绝对词长的差异，短语长度的分布。</sample>
    <sample id="74">见论文以获取完整论点！
在海报环节与我们交流！</sample>
    <sample id="75">根据幻灯片显示，这篇论文有三位作者：Matthias Lindemann、Alexander Koller和Ivan Titov。</sample>
    <sample id="76">根据图表显示，'新闻'和'圣经'这两个领域的简化程度更大。这可以通过它们的柱状图中较高的百分比来表示，表明这些领域中文本被简化了更多的程度。</sample>
    <sample id="77">根据幻灯片中所给的英文内容，偏好的较短左并列词示例是“read yesterday it good”，因为它具有较短的依赖长度。</sample>
    <sample id="78">根据幻灯片中提供的信息，DrBERT模型、NACHOS数据集和训练脚本可以在MIT许可证下免费使用。这意味着你可以将这些模型用于你的研究，只要遵守MIT许可证的条款。通常包括允许复制、分发和修改软件，但要求在任何修改的副本上都包含原始许可证的条款和条件。</sample>
    <sample id="79">根据图表中所给的英文内容，DEplain-apa 包含来自 APA（美国心理学会）出版物的内容。这可以通过柱状图标题下的标签“APA”来确定，该标签位于 DEplain-apa 柱上。</sample>
    <sample id="80">根据幻灯片中呈现的英文内容，有助于良好泛化的因素包括：1.更好的模型架构2.更大的模型尺寸3.更多的微调示例这些因素表明了在机器学习和深度学习领域中提高模型性能和泛化能力的策略。</sample>
    <sample id="81">根据幻灯片中呈现的统计信息，衡量左并列词是否更短的方法是通过比较它们的长度。如果一个并列词在句子中位于另一个之前，并且它比后者短，那么它被认为是“更短”的。这种测量是通过观察并列词的长度差异来确定的，如幻灯片中提到的Gibson et al. 1996年研究中所注意到的那样。</sample>
    <sample id="82">根据图片，实验设计似乎涉及比较不同长度的支配词（以字符、音节和单词为单位）对短语结构的影响。图表显示了在不同长度下，短语中支配词位置左侧和右侧的字符、音节和单词比例。通过分析这些比例，研究者可以评估支配词位置是否显著影响短语结构，以及这种影响如何随长度变化。</sample>
    <sample id="83">根据幻灯片显示，基线分类器在不平衡数据集上的训练效果并不理想。ROC曲线下的面积（AUC）值为0.56，这表明分类器的表现仅略优于随机猜测。这意味着分类器在不平衡数据集上区分正负样本的能力有限。</sample>
    <sample id="84">根据幻灯片上显示的英文内容，这篇论文有四位作者。这可以从幻灯片底部列出的四个名字推断出来：Shangbin Feng、Chan Young Park、Yuhuan Liu和Yulia Tsvetkov。</sample>
    <sample id="85">示例对话中的角色名字是“Gotta Feeling”，这是对话中提到的实体。</sample>
    <sample id="86">根据幻灯片中提供的英文内容，语境感知 MT 模型在处理诸如删除、插入和替换等话语现象时比语境无关模型更有优势。</sample>
    <sample id="87">根据幻灯片上显示的英文内容，这篇论文的作者所属机构是约翰霍普金斯大学、普渡大学和麻省理工学院。这些机构的标志和名称在幻灯片右下角可见。</sample>
    <sample id="122">根据图片中提供的英文内容，引入的框架通过计算立场与人口统计学变量之间的相关性来量化立场。这表明该框架分析了用户在不同人口统计学类别（如年龄、性别、种族和教育水平）上的行为或观点，以评估这些因素是否与特定立场相关。</sample>
    <sample id="155">根据幻灯片中提供的信息，无法确定之前的研究结果。幻灯片只提到了一个研究，即使用相同的人格化提示来描述自己，但没有提供关于结果的任何细节。要回答这个问题，需要查阅原始研究或其摘要。</sample>
    <sample id="156">根据幻灯片上提供的英文内容，研究使用了以下数据来源：'Wikipedia', 'Google Books Ngram Viewer', 'Google Scholar', 'Google Trends', 'Amazon.com', 'Twitter', 'Facebook', 'YouTube', 'Reddit', 'Stack Overflow', 'Quora', 'Xing', 'LinkedIn', 'GitHub', 'Wikipedia Talk', 'Wikipedia Edits', 'Wikipedia Views', 'Wikipedia Disambiguation', 'Wikipedia Redirects', 'Wikipedia Categories', 'Wikipedia Templates', 'Wikipedia Tables', 'Wikipedia Images', 'Wikipedia Audio', 'Wikipedia Video', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AVI', 'Wikipedia MOV', 'Wikipedia WMV', 'Wikipedia SWF', 'Wikipedia PDF', 'Wikipedia DOCX', 'Wikipedia XLSX', 'Wikipedia PPTX', 'Wikipedia CSV', 'Wikipedia JSON', 'Wikipedia XML', 'Wikipedia HTML', 'Wikipedia SVG', 'Wikipedia GIF', 'Wikipedia MP3', 'Wikipedia MP4', 'Wikipedia AV</sample>
    <sample id="157">根据幻灯片上显示的英文内容，这篇论文有两位作者：Adam Przpherdowski和Michat Wozniak。</sample>
    <sample id="158">根据图片中呈现的英文内容，与认知失调密切相关的任务包括“init debate”、“CE”和“Debate CE”。这些任务在图表中被标记为具有负的AUC值，表明它们与认知失调有关。</sample>
    <sample id="159">根据幻灯片上显示的英文内容，这篇论文有两位作者。他们的名字是“Shuheng Liu”和“Alan Ritter”，都来自乔治亚理工学院互动计算学院。</sample>
    <sample id="160">根据幻灯片上显示的英文内容，这篇论文有九位作者。</sample>
    <sample id="161">根据图片中提供的英文内容，引入的框架与以前的研究不同之处在于它通过比较模型预测和数据库中的真实标签来评估模型的性能。这表明该框架专注于使用真实世界数据验证模型的准确性和可靠性，从而提供更准确的模型评估和改进。</sample>
    <sample id="162">根据柱状图显示，GPT-4在“黑人刻板印象”类别中与人类回应的重叠最多。这可以通过紫色柱（代表GPT-4）在该类别的柱上达到最高点来表示，超过了0.8%的刻板词汇百分比。</sample>
    <sample id="163">根据幻灯片上提供的英文内容，比较了“Formality”和“lexical cohesion”这两个商业系统。 这些术语似乎是MuDA基准测试结果中评估的某些语言处理或分析方面的指标或方面。</sample>
    <sample id="164">从预训练数据到语言模型：追踪导致NLP模型政治偏见的轨迹 #ACL2023

王方宾
张强
刘雨含
叶丝特科夫

保罗·G·阿伦人工智能中心
 UW-NLP
卡内基梅隆大学自然语言技术研究所
中国科学院自动化研究所</sample>
    <sample id="165">LM Training Data
A mixed blessing
图1展示了大规模语言模型训练数据的来源。数据主要来自维基百科、商业评论和新闻等网站。这些数据来源为模型提供了广泛的信息，有助于提高模型的性能和准确性。然而，数据来源的多样性也可能带来一些挑战，例如处理不同类型的文本和潜在的偏见。因此，在使用大规模语言模型时需要谨慎，并确保数据来源的质量和代表性。</sample>
    <sample id="166">LM Training Data A mixed blessing Jesse, et al. "Documenting Large Web Corpora: A Case Study of Crawled Corpus - Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2015."</sample>
    <sample id="167">LM Training Data
A mixed blessing
图1展示了大规模预训练模型（LM）的训练数据来源。这些数据主要来自互联网，包括各种网站和维基百科。数据来源的多样性反映了模型训练的广泛范围，但同时也暗示了潜在的隐私和偏见问题。例如，数据可能包含敏感信息或反映特定群体的观点和偏见。因此，在使用这些数据进行训练时必须进行仔细审查和处理，以确保模型的公正性和隐私性。</sample>
    <sample id="168">LM Training Data
A mixed blessing
图1展示了大规模预训练模型使用的训练数据来源。数据来源包括各种网站，如维基百科、维基共享资源和HuffPost等。这些数据来源的多样性为模型提供了广泛的信息，有助于提高模型的性能和泛化能力。然而，数据来源的多样性也可能带来一些挑战，例如数据质量、偏见和隐私问题。因此，在使用大规模预训练模型时，需要仔细考虑数据来源的质量和潜在影响。</sample>
    <sample id="169">为了达到这个目的，我们首先需要使用大规模文本数据集对预训练模型进行预训练。这涉及在大量文本上训练模型，以学习语言的通用表示。然后，我们将使用这些预训练模型作为基础，针对特定任务进行微调。这将使模型能够更好地处理与该任务相关的特定领域知识和上下文。</sample>
    <sample id="170">到此为止
预训练数据
语言模型
下游任务
如何评估LMs在政治偏见中的表现？
预训练数据在政治偏见中扮演什么角色？
LMs在NLP应用中的政治偏见问题是否会影响它们的表现？</sample>
    <sample id="171">为了达到这个目的，预训练数据、语言模型和下游任务之间的关系可以表示为一个循环过程。在这个过程中，预训练数据被用于训练语言模型，然后这些模型被应用于各种下游任务。在这个过程中，我们关注的是如何评估政治偏见在预训练数据中的作用，以及不同政治偏见的语言模型在NLP应用中的性能。此外，我们还探讨了政治偏见是否会影响LM的性能，并导致NLP应用中的公平性问题。</sample>
    <sample id="172">评估LM政治倾向
支持编码器和解码器LMs
“[statement] I &lt;mask&gt; with this statement。”
“你同意还是不同意这个说法？&lt;statement&gt;”
自动评估
基于社会学文献
语言模型
prompted response
政治倾向
Libertarian
Agree
Disagree
Left
Right</sample>
    <sample id="173">现有的LMsBERT-baseRoBERTa-baseRoBERTa-largedistilRoBERTa-largeALBERT-baseALBERT-largeBERT-baseGPT-2-xlGPT-3-jumboGPT-4GPT-JCodexLlama-GPT-3-adaGPT-3-curieGPT-3-davinciChatGPT</sample>
    <sample id="174">现有的LMs

BERT-base
BERT-large
RoBERTa-base
RoBERTa-large
distilRoBERTa-base
distilRoBERTa-large
ALBERT-base
ALBERT-large
BART-base
BART-large
T5-base
T5-large
Flan-T5-base
Flan-T5-large
PaLM
PaLM-2
GPT-3-ada
GPT-3-babbage
GPT-3-curie
GPT-3-davinci
ChatGPT
GPT-4
GPT-J</sample>
    <sample id="175">预训练数据
进一步预训练LM（Roberta、GPT-2）检查点，评估在政治倾向上的变化
新闻媒体
左
中
右
社交媒体（Reddit）
左
中
右
Yoon, et al. "POLITICS: Predicting with Same-story Article Comparison For Ideology Prediction and Stance" NAACL 2022.
Shen, Qian, &amp; Carind Rose. "What sounds 'right' to me? An experiential factor in the perception of political correctness" Journal of Language and Linguistics, Asian Volume 2021.</sample>
    <sample id="176">预训练数据
进一步预训练LM（RoBERTa，GPT-2）检查点，评估政治倾向的变化
新闻媒体
左
中
右
社交媒体Reddit
左
中
右
Shen，Ginor和Carlynn Rose，“What sounds right? The perception of political bias in online discourse: A study on Chinese speakers and the role of language volume”，《计算语言学年会》NAACL 2022</sample>
    <sample id="177">结果
 partisan shifts in LM political leaning
 RoBERTa
 GPT-2
 original
 reddit
 news
 reddit
 news
 news
 reddit
 original</sample>
    <sample id="178">结果
 partisan shifts in LM political leaning
 RoBERTa
 GPT-2
 original
 reddit
 news
 reddit
 news
 news
 original
 reddit</sample>
    <sample id="179">结果
 partisan shifts in LM political leaning
 RoBERTa
 GPT-2
 Left
 Center
 Right
 original
 reddit
 news
 reddit
 news
 news
 original</sample>
    <sample id="180">The Trump Card
Pre-45th to post-45th shift
Reddit left
Reddit center
Reddit right
news left
news center
news right</sample>
    <sample id="181">The Trump Card
Pre-45th to post-45th shift
news left
news center
news right
reddit left
reddit center
Reddit right</sample>
    <sample id="182">The Trump Card
Pre-45th to post-45th shift
Reddit left
Reddit center
Reddit right
News left
News center
News right</sample>
    <sample id="183">类别
1st
2nd
3rd
4th
5th
6th
7th
8th
9th
10th
Hate Speech
BLACK
MUSLIM
LGBT+
Jews
ASAIN
LATINX
WOMEN
CHRISTIAN
MEN
WHITE
PERFORMANCE
PER-CATEGORY PERFORMANCE
Table 4: Performance on hate speech targeting different identity groups and misinformation from different sources. The color-coded legend such that yellow denotes best and dark blue denotes worst.
新闻左
80.43
89.09
89.19
89.83
89.65
89.15
87.65
87.65
87.65
86.22
Reddit左
89.83
89.09
89.19
89.83
89.65
89.15
87.65
87.65
87.65
86.22
Reddit右
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43
80.43</sample>
    <sample id="184">### 图4：按类别划分的表现

| 类别 | 新闻左翼 | Reddit左翼 | Reddit右翼 | 新闻右翼 |
| --- | --- | --- | --- | --- |
| 恨 speech | 80.63 | 89.84 | 80.21 | 80.03 |
| 黑人 | 82.98 | 89.09 | 89.09 | 82.43 |
| 穆斯林 | 90.19 | 89.09 | 89.09 | 84.43 |
| 同性恋者 | 85.83 | 89.09 | 89.09 | 84.43 |
| 犹太人 | 87.50 | 90.00 | 90.00 | 84.43 |
| 阿拉伯人 | 85.00 | 89.09 | 89.09 | 84.43 |
| 拉丁裔 | 83.33 | 89.09 | 89.09 | 84.43 |
| 妇女 | 87.50 | 91.15 | 91.15 | 84.43 |
| 基督徒 | 87.50 | 87.50 | 87.50 | 84.43 |
| 男性 | 87.50 | 87.50 | 87.50 | 84.43 |
| 白人 | 85.00 | 89.09 | 89.09 | 84.43 |

### 表格4：根据不同身份群体的 hate speech 和不同来源的 misinformation 进行着色。黄色表示最好，深蓝色表示最差。

图4展示了不同身份群体（如黑人、穆斯林、同性恋者、犹太人、阿拉伯人、拉丁裔、妇女、基督徒、男性和白人）在新闻左翼、Reddit左翼、Reddit右翼和新闻右翼中的 hate speech 和 misinformation 的表现。黄色表示最好，深蓝色表示最差。</sample>
    <sample id="185">类别表现

| 新闻 | 左派 | 右派 | 黑人 | 穆斯林 | LGTB+ | 犹太人 | 青年 | 拉美裔 | 女性 | 基督徒 | 男性 | 白人 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Reddit 左派 | 80.63 | 89.44 | 89.19 | 90.19 | 89.09 | 89.09 | 89.09 | 89.09 | 89.09 | 89.09 | 89.09 | 89.09 |
| Reddit 右派 | 82.03 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 | 80.44 |
| Minisinformation | HP (L) | NYT | CNN (L) | NPR (L) | Guard (L) | Fox | WaEx (R) | BBAT (R) | WAT | NR |

表格4：根据不同身份群体和来源显示的 hate speech 和 misinformation 表现。深黄色表示最好，深蓝色表示最差。

请注意，由于翻译的限制，表格中的缩写（如 HP、NYT、CNN、NPR、Guard、Fox、WaEx、BBAT、WAT 和 NR）可能无法完全准确地翻译成中文。这些缩写通常代表新闻机构或组织的名称，它们在表格中被用作数据点。</sample>
    <sample id="186">类别表现

|  | 新闻左翼 | Reddit左翼 | Reddit右翼 |
| --- | --- | --- | --- |
| 恨 speech | 80.63 | 89.44 | 80.03 |
| 黑人 | 89.84 | 89.09 | 82.02 |
| 穆斯林 | 90.19 | 89.09 | 84.43 |
| 同性恋者 | 87.95 | 89.09 | 82.43 |
| 犹太人 | 88.85 | 89.09 | 84.43 |
| 阿拉伯人 | 86.66 | 89.09 | 80.31 |
| 拉丁裔 | 83.15 | 89.09 | 80.31 |
| 妇女 | 87.65 | 89.09 | 82.43 |
| 基督徒 | 87.65 | 89.09 | 82.43 |
| 男性 | 87.65 | 89.09 | 82.43 |
| 白人 | 86.22 | 89.09 | 88.86 |

注：表格4显示了针对不同身份群体的 hate speech 和来自不同来源的 misformation。黄色表示最好，深蓝色表示最差。

请注意，由于翻译的限制，表格中的数字可能不会完全准确，但它们应该反映了原始数据的主要趋势。</sample>
    <sample id="187">类别表现

| 源 | 新闻左翼 | Reddit左翼 | Reddit右翼 |
| --- | --- | --- | --- |
| 新闻左翼 | 80.43 | 89.84 | 90.19 |
| Reddit左翼 | 89.09 | 89.09 | 89.09 |
| Reddit右翼 | 82.03 | 80.43 | 82.43 |

[来源](https://www.example.com)

| 源 | HP（左翼） | NYT（左翼） | CNN（左翼） | NPR（左翼） | Guard（左翼） | Fox（右翼） | WaEx（右翼） | BBaT（右翼） | WAT（右翼） | NR（右翼） |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| HP（左翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| NYT（左翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| CNN（左翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| NPR（左翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| Guard（左翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| Fox（右翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| WaEx（右翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| BBaT（右翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| WAT（右翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |
| NR（右翼） | 88.72 | 85.54 | 84.86 | 82.21 | 84.44 | 90.66 | 96.63 | 90.43 | 91.15 | 96.00 |

[来源](https://www.example.com)

表格4：根据不同身份群体和不同来源显示的 hate speech 和 misinformation 的表现。黄色表示最好，深蓝色表示最差。</sample>
    <sample id="188">类别表现

| 源 | 新闻左翼 | Reddit左翼 | Reddit右翼 |
| --- | --- | --- | --- |
| 联合国 | 80.63 | 89.44 | 82.02 |
| CNN | 89.09 | 89.09 | 89.09 |
| CNN+ | 90.19 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89.09 | 89.09 |
| CNN- | 80.25 | 80.25 | 80.25 |
| CNN+ | 89.09 | 89</sample>
    <sample id="189">类别表现

|  | 新闻左 | 新闻右 | Reddit左 | Reddit右 |
| --- | --- | --- | --- | --- |
| 恨 speech | 80.63 | 80.98 | 89.41 | 80.20 |
| 黑人 | 89.84 | 89.09 | 89.09 | 89.06 |
| 穆斯林 | 89.84 | 89.09 | 89.09 | 89.06 |
| LGTB+ | 89.84 | 89.09 | 89.09 | 89.06 |
| 犹太人 | 89.84 | 89.09 | 89.09 | 89.06 |
| 阿尔巴尼 | 89.84 | 89.09 | 89.09 | 89.06 |
| 拉丁裔 | 89.84 | 89.09 | 89.09 | 89.06 |
| 妇女 | 89.84 | 89.09 | 89.09 | 89.06 |
| 基督徒 | 89.84 | 89.09 | 89.09 | 89.06 |
| 男性 | 89.84 | 89.09 | 89.09 | 89.06 |
| 白人 | 89.84 | 89.09 | 89.09 | 89.06 |

表4：根据不同身份群体的 hate speech 和 misinformation 来自不同来源进行着色。黄色表示最好，蓝色表示最差。

请注意，由于翻译限制，表格中的数字可能略有变化，但它们代表了原始数据的主要趋势。</sample>
    <sample id="190">质性分析

文本

目标标签

基线

N-L

S-L

N-R

S-R

麦当劳的员工称，有人支持建立一个新国家

正确

正确

正确

正确

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误

错误</sample>
    <sample id="191">质性分析

文本
目标标签
基础
N-L
S-L
N-R
S-R
麦当劳的支持者认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家。他们认为，他们正在支持一个新国家</sample>
    <sample id="192">图12显示了关于种族主义和反同性恋偏见的 Hate Speech 示例。该表包括原始文本、标签（如“ hate”、“反同性恋”、“反犹太教”等）以及根据标签生成的文本。</sample>
    <sample id="193">hate text?  hates?  R-L  R-N  R-R  Misinformation Test  False?  N-A  True?  False?  True?  False?  False?  True?  False?  False?  False?  True?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?  False?</sample>
    <sample id="194">hate speech text  Hate? True False R-L True False N-R False R-R False  Misinformation Text  Fake? True False R-L True False N-R False R-R False</sample>
    <sample id="195">hate speech text  hates? R-L R-R N-R N-R  misleading text  fake? T-F T-F T-F T-F</sample>
    <sample id="196">讨论
介于Scylla和Charybdis之间
要“净化”还是不“净化”，这就是问题所在
预训练数据
语言模型
下游任务</sample>
    <sample id="197">讨论
介于Scylla和Charybdis之间
要“初始化”还是不“消毒”，这就是问题所在
预训练数据
语言模型
下游任务</sample>
    <sample id="198">讨论

介于Scylla和Charybdis之间

“初始化”或不“消毒”，这是问题所在

预训练数据

语言模型

下游任务</sample>
    <sample id="199">感谢！
预训练数据
语言模型
下游任务
章广峰
张强
刘宇
叶丝科</sample>
    <sample id="200">根据幻灯片上显示的英文内容，这篇论文有五位作者。这可以通过底部列出的名字来确定：David Vil Torres, Markus Freitag, Colin Cherry, Jamrjz Luo, 和 George Foster。</sample>
    <sample id="201">根据图片中显示的图表，MPP 评估最多涵盖900个词元的上下文长度。 这一点可以从 x 轴上标记的最大值900来推断，x 轴代表了不同长度的上下文。</sample>
    <sample id="202">根据幻灯片中提供的英文内容，他们的数据集中包含音乐领域。这可以从提到的歌曲《Easy on Me》和《I Gotta Feeling》以及与音乐相关的描述选项（如“有流行音乐”、“有节奏感”、“关于河流”、“关于时间”）来推断。</sample>
    <sample id="203">在研究背景下，"positionality"通常指的是研究者及其参与者之间的权力动态和关系。它包括研究者的种族、性别、社会经济地位、职业和其他身份因素如何影响他们的视角，并进而影响他们收集和解释数据的方式。理解positionality对于确保研究的公正性、透明度和参与者的权利至关重要。</sample>
    <sample id="204">演讲者的名字是Dawe Xu、Xiaoyu Shen、Marius Mosbach、Andreas Stephan和Dietrich Klakow。</sample>
    <sample id="205">根据图片中可见的英文内容，EDAtt 似乎适应了现有的离线 ST 模型。这可以从幻灯片左下角的文本推断出来，其中包含“我们的解决方案？”这表明 EDAtt 是一种解决方案或方法，旨在解决特定问题或改进现有系统，包括离线 ST 模型。</sample>
    <sample id="206">根据幻灯片上显示的英文内容，这篇论文有四位作者。他们的名字是Yusen Zhang、Jun Wang、Zhiguo Wang和Rui Zhang。</sample>
    <sample id="207">根据图片中提供的信息，被测模型似乎不能在测试套件上运行。这可以从标题“Background-Pretrain”和子标题“任务特定训练对于知识整合是必要的”中推断出来。这表明该模型可能是在特定背景下预训练的，可能使用了某些数据集或技术，这些数据集或技术并不适用于测试套件。此外，图表显示了在没有任务特定培训和有任务特定培训两种情况下的性能比较，进一步暗示该模型可能需要额外的调整或训练才能在测试套件上运行。</sample>
    <sample id="208">KITMUS的三个变体是：(a) Background-Pretrain，(b) Background-Both，(c) Background-Inference。</sample>
    <sample id="209">根据幻灯片中显示的英文内容，这篇论文的作者所属机构是Google Research。这可以从幻灯片左下角的Google Research标志推断出来。</sample>
    <sample id="210">根据幻灯片上显示的英文内容，最后一个研究问题（RQ3）是“如何更有效地利用现有的干净样本？”</sample>
    <sample id="211">仅凭图片本身，无法提供关于“指标灵敏度”如何工作的详细解释。该图片显示了一个数学表达式，表明了模型对不同输入的敏感性计算方法。要了解灵敏度指标的工作原理，需要理解数学表达式背后的概念，包括期望值（E），梯度（∇），以及损失函数（L）。这些符号通常与机器学习和优化问题相关联。然而，没有额外的背景信息或数学知识，对于灵敏度指标如何工作的具体解释超出了图片提供的范围。</sample>
    <sample id="212">演讲者的名字是“Wenjun Peng”，如幻灯片上所示。</sample>
    <sample id="213">根据所给的英文内容，更高的灵敏度表明模型性能得到了提高。这可以从灵敏度值越低越好这一点看出，如幻灯片底部的注释所示。因此，较低的灵敏度值（例如OF_Aggregate的10.27）表示模型对未见过的任务更稳定或一致，这通常被解释为性能改善。</sample>
    <sample id="214">根据幻灯片上提供的信息，模型在预训练期间接收各种语言上下文。这表明模型被训练来处理和理解不同类型的文本，从而增强了其对语言的泛化能力。</sample>
    <sample id="215">在WSL中，通常需要大约50个干净的验证样本才能获得良好的表现。</sample>
    <sample id="216">根据幻灯片上提供的英文内容，这篇论文的作者Myra Cheng、Esin Durmus和Dan Jurafsky所属的机构是斯坦福工程科学系计算机科学。</sample>
    <sample id="217">根据图片中显示的图表，开发新的方法来衡量媒体偏见的原因在于当前的方法可能无法准确捕捉到语言模型（LMs）在不同政治观点上的微妙差异。图表展示了各种LMs在权威主义和自由主义之间的位置，以及它们在经济轴上的偏好。这种多样性表明需要一种更细致入微的方法来评估LMs的偏见，因为它们在政治光谱上分布广泛。</sample>
    <sample id="218">演讲者的名字是Alexandra Oteanu。这可以从幻灯片上显示的个人照片和名字中得出，位于演示文稿标题下方。</sample>
    <sample id="219">根据图片中提供的英文内容，政治偏见传播流程似乎是这样的：首先，数据被收集并存储在云服务器上。然后，这些数据被处理和分析，以识别潜在的偏见。最后，结果被呈现给用户，他们可以使用这些信息来了解和对抗偏见。</sample>
    <sample id="220">根据图表中显示的简化过程，DEplain-apa 和网站的简化过程有所不同。DEplain-apa 使用了“删除”、“替换”和“添加”等简化技术，而网站似乎专注于“替换”和“删除”技术。DEplain-apa 的柱状图显示了这些技术的使用频率，表明它可能涉及更广泛的简化操作，包括添加新词以简化文本，而网站似乎侧重于通过替换复杂词为更简单词或删除不必要的词来简化文本。</sample>
    <sample id="221">根据幻灯片中提到的英文内容，Coscript 并不公开可用。这可以从幻灯片底部的注释中推断出来，该注释表明“较小的 LMs 细化调整在 Coscript 上可以生成比大型语言模型（LLMs）更高质量的脚本”，但没有提供关于 Coscript 的可访问性的任何信息。然而，由于幻灯片中未明确提及 Coscript 的可访问性，因此可以合理推断 Coscript 并非公开可用。</sample>
    <sample id="222">水印通过将目标嵌入添加到原始嵌入中来插入文本。这涉及定义一个目标嵌入、计算触发器数量，并根据触发器集的大小调整权重，然后将目标嵌入与原始嵌入相加。</sample>
    <sample id="223">根据幻灯片中显示的英文内容，这篇论文的作者所属机构是宾夕法尼亚州立大学和亚马逊。这可以从幻灯片底部的标志和文字中看出，分别代表了这两个机构。</sample>
    <sample id="224">根据幻灯片中呈现的表格数据，编码器-解码器模型如mt5确实可以通过混合语言的训练来改进。在表格中，我们看到“多语言”和“多语言+PTTR”两列下的性能指标明显高于“单语言”列。这表明，通过在多种语言上进行训练，模型能够更好地泛化并提高其性能。</sample>
    <sample id="225">受限语言规划的一个示例是“如何制作巧克力蛋糕？”这个问题需要在遵循特定步骤和成分的情况下找到解决方案，比如将可可粉加入面粉中。</sample>
    <sample id="226">仅根据图片中提供的信息，无法确定他们如何确保其方法的隐蔽性。图片显示了不同数据集上的嵌入可视化，但没有提供关于隐私保护或数据操纵技术的细节。要了解他们的方法如何确保隐蔽性，需要额外的上下文或来自演讲者或论文的详细信息。</sample>
    <sample id="227">通过比较不同的预训练策略和数据来源，研究了如何使用现有的预训练语言模型（PLM）来构建新的PLM。该研究评估了公开和私有医疗数据源的影响，并比较了从头开始构建模型、使用现有预训练模型进行微调以及使用新数据集进行持续预训练等学习策略。</sample>
    <sample id="228">根据图片中显示的柱状图，GPT-4与非洲伊斯兰国家/地区的立场最不一致。这可以通过该柱的最短长度来表示，表明在这些地区中，GPT-4的立场与数据集和模型的立场差异最大。</sample>
    <sample id="229">根据幻灯片上展示的英文内容，演讲者在示例句子'What is our solution?'上展示了模型如何利用注意力机制所学的知识。</sample>
    <sample id="230">根据图表，任务数量似乎与模型性能之间存在负相关关系。随着任务数量的增加（从1到6），曲线显示性能指标下降。这表明模型在处理更多样化的任务时，其整体性能可能降低，可能是由于资源分配或注意力分散导致。</sample>
    <sample id="231">根据图表，作者用来比较其方法的三个无树基线是“LSTM seq2seq”，“Zhen and Lapata”和“PP recursion”。这些基线在图表中以不同的颜色表示，其中“LSTM seq2seq”用绿色表示，“Zhen and Lapata”用蓝色表示，“PP recursion”用橙色表示。</sample>
    <sample id="232">根据幻灯片上列出的英文内容，Alexander Koller和Ivan Titov似乎是与第一作者Matthias Lindemann合作的合著者。这可以从他们的名字在标题下方并列于第一作者的名字之后来推断，通常表示他们作为共同作者或贡献者参与了研究。</sample>
    <sample id="233">根据幻灯片上提供的英文内容，PaLM 的第一作者是 Chowdery et al., 2022。</sample>
    <sample id="234">NLPositionality: 识别数据集和模型设计偏见的特征</sample>
    <sample id="235">NLPositionality: 识别数据集和模型设计偏见的特征</sample>
    <sample id="236">Imagine...</sample>
    <sample id="237">Imagine...</sample>
    <sample id="238">Imagine... Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India</sample>
    <sample id="239">Imagine... Design bias example! Carl Jones Tech Lead, New York Times Aditya Sharma Tech Lead, Times of India</sample>
    <sample id="240">[1] Savin-Baden, Maggi, and Claire Howell-Major. “Qualitative research: The essential guide to theory and practice.” Qualitative Research: The Essential Guide to Theory and Practice (2013).</sample>
    <sample id="241">“人们持有的观点是其人口统计、身份和生活经历的结果。</sample>
    <sample id="242">“人们持有的观点源于他们的人口统计、身份和人生经历。”</sample>
    <sample id="243">数据集和模型有位置吗？</sample>
    <sample id="244">数据集和模型有位置性吗？</sample>
    <sample id="245">数据集和模型有位置性吗？</sample>
    <sample id="246">数据集和模型有偏见吗？

轶事证据：
- 模型和数据集探测[112]
- 理论上的模型偏见定义[3]

[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
[2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
[3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="247">数据集和模型有偏见吗？

轶事证据：

- 模型和数据集探测[11][2]
- 理论上的模型偏见定义[3]

[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.
[2] Li, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.
[3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.”</sample>
    <sample id="248">数据集和模型有偏见吗？轶事证据：- 模型和数据集探测[112]。- 理论上的模型偏见定义[3]。[1] Blasi, et al. “Systematic Inequalities in Language Technology Performance across the World’s Languages.” ACL 2022.[2] Chen, et al. “GEOMLLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models.” EMNLP 2022.[3] Cambo &amp; Gergle. “Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science.” Data Science</sample>
    <sample id="249">问题：数据集和模型有位置吗？</sample>
    <sample id="250">NLPositionality
一种用于表征自然语言处理数据集和模型中设计偏见的框架</sample>
    <sample id="251">框架

收集
从数据集中随机采样数据集
每个实例都有一个标签
实例被呈现给标注员
标注员为每个实例提供标注
过程
模型预测
每个实例的标签与模型预测进行比较
结果
教育
年龄
性别
种族
种族
国家
教育程度
种族
学校</sample>
    <sample id="252">框架

从收集到处理的流程如下：

1. **数据收集**：从各种来源收集数据，包括公开数据集和通过网络爬虫抓取的数据。数据被清洗和预处理，以确保其质量和一致性。

2. **模型训练**：使用预处理的数据训练机器学习模型。模型可以是分类器、回归器或其他类型的算法，具体取决于数据集的性质和目标任务。

3. **模型评估**：对模型进行评估，以确定其性能。这通常涉及在保留的验证集上测试模型，并计算各种指标，如准确率、精确率、召回率和F1分数。

4. **模型优化**：根据评估结果，对模型进行优化。这可能涉及调整模型参数、尝试不同的算法或特征工程。

5. **模型部署**：将优化后的模型部署到生产环境，以便在实际应用中使用。这可能涉及将其集成到更大的系统中，如Web应用程序或移动应用程序。

6. **持续监控与更新**：定期监控模型的性能，并根据需要进行更新。这可能涉及重新训练模型以适应新的数据或改进模型以提高其准确性。

通过这个流程，可以确保模型的质量和性能，并确保其能够有效地解决所面临的问题。</sample>
    <sample id="253">框架

收集
处理
1）从数据集中采样一个实例
2）模型预测
3）将每个实例发送给多个标注员
4）接收的标注与模型的gold labels进行比较
5）Pearson’s r^2衡量数据集中不同人口统计学特征之间的相关性

图1.框架示例</sample>
    <sample id="254">框架

收集
从大型数据集中采样一个数据集。每个实例都是一个图像，其中包含一个任务（如“吃手”、“阅读”或“写字”），以及一组标签，描述了执行任务的人的属性（如年龄、性别和种族）。

处理
1.将数据集输入到模型中进行预测。
2.将模型的预测结果与标签一起发送给标注员。
3.标注员根据模型的预测结果和自己的知识提供标注。
4.将标注与模型的预测结果进行比较，以评估模型的性能。
5.使用标注结果来训练模型，以提高其准确性。

注释
1.重新注释数据集，使用多样化的标注员。
2.使用多样化的标注员重新注释数据集，以确保模型能够处理多样化的输入。</sample>
    <sample id="255">框架

收集
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架
框架</sample>
    <sample id="256">框架

收集
处理
1.从数据集中采样
2.每个实例都有一个标签
3.实例被发送到模型
4.模型返回预测
5.从模型中观察到的标签
6.比较注释和模型的标签，通过皮尔逊R分数评估数据库的性能</sample>
    <sample id="257">“你愿意和AI一起生活吗？”“你更喜欢AI在通知你有害内容时表现出的温和还是激烈的反应？”“你在技术乌托邦尺度上定位自己，这是一个更加神秘和技术超脱的世界？”</sample>
    <sample id="258">Lab in the Wild</sample>
    <sample id="259">Task A: Social Acceptability

1) Read the situation
(1分)
Wanting to make a lot of money.

2) Enter what you think about it.
(3分)
What do you think about this from a perspective?
Feel free to explain your judgment.
(可选)
It's generally required field.
Good.
It's generally required field.
Bad.
Submit.

3) See what others thought about it!
(可选)
The AI speculates:
Study participants in the United States</sample>
    <sample id="260">Task A: Social Acceptability

1. Read the situation
2. Enter what you think about it.
3. See what others thought about it.

The AI speculates:
Study participants in the United States:

参与者比较他们的回应与其他人和AI的回应</sample>
    <sample id="261">任务A：社会接受性
分析
数据集
- 社会化学
模型
- 德尔斐
- GPT-4</sample>
    <sample id="262">任务B：恶意行为
1.阅读示例。
2.对它进行评价。
3.看看其他人是怎么想的。</sample>
    <sample id="263">任务B：Toxicity
分析
数据集
- Dynahate
模型
- Perspective API
- Rewrite API
- State RoBERTa
- GPT-4</sample>
    <sample id="264">结果
NLP数据集和模型与谁对齐？</sample>
    <sample id="265">数据集和模型最倾向于英语国家。</sample>
    <sample id="266">数据集和模型与拥有大学及以上教育的人群最一致。</sample>
    <sample id="267">数据集和模型与拥有大学教育的人最一致。</sample>
    <sample id="268">找到2：有些群体被落下。</sample>
    <sample id="269">数据集和模型与非二元人对齐较少。</sample>
    <sample id="270">所以，我们可以做什么？解决NLP中的位置性问题</sample>
    <sample id="271">推荐</sample>
    <sample id="272">推荐意见

1.在整个构建数据集和模型过程中记录所有相关的设计选择。
2.通过透视主义的角度进行NLP研究：
a.分享聚合数据集标签！
b.使用可以处理标注员 disagreement 的建模技术。
3.为特定社区构建专门化的数据集和模型是包容性NLP的可取方向（例如，Masakhane计划）。</sample>
    <sample id="273">谢谢！
仪表板链接：nlpositionality.washington.edu/
论文：bit.ly/NLPositionality-Paper/
Delhi</sample>
    <sample id="274">演讲者提到了 SimulST 的四个问题。</sample>
    <sample id="275">根据幻灯片中呈现的讨论主题，减轻数据集中的社会和政治偏见的有效方法是通过预训练数据来“净化”模型，而不是仅仅“消毒”它们。这表明在模型训练过程中，应该系统地识别和减少数据集中存在的偏见，以确保模型在处理任务时更加公正和无偏见。</sample>
    <sample id="276">61st Annual Meeting of the Association for Computational Linguistics
多伦多，加拿大
2023年6月13日至14日

从大型语言模型中提取脚本知识用于受限语言规划

颜思宇、陈江强、符子渊、顾雪燕、沙Ham、查尔斯·罗伯特·扬科夫斯基、肖肖、杨庆、阳华肖

微山大学
- Brain Technologies Inc.</sample>
    <sample id="277">如何制作蛋糕？

1. 准备材料。
2. 将烤箱预热至325°F（163°C）和黄油。
3. 在蛋糕盘上涂抹黄油和糖。
4. 添加鸡蛋。
5. 搅拌面糊。
6. 将面糊倒入蛋糕盘。
7. 烘烤1小时15分钟。

大型语言模型（LLMs）可以有效地将目标分解成步骤。</sample>
    <sample id="278">如何制作蛋糕？
1. 准备材料。
2. 将烤箱预热至325°F（163°C）。
3. 在蛋糕盘上涂抹黄油和糖。
4. 添加鸡蛋。
5. 搅拌蛋糕粉。
6. 将面糊倒入蛋糕盘中。
7. 烘烤1小时15分钟。
大型语言模型（LLMs）可以有效地将目标分解成步骤。</sample>
    <sample id="279">受限语言规划

如何制作草莓蛋糕？
…将草莓酱加入面粉中…

如何制作巧克力蛋糕？
…将可可粉加入面粉中…

具体目标可以被不同的现实生活中具有多方面约束的具体目标继承</sample>
    <sample id="280">约束语言规划

如何制作草莓蛋糕？
…将草莓酱加入面粉中…

如何制作巧克力蛋糕？
…将可可粉加入面粉中…

具体目标可以被不同的现实生活中具有多方面约束的具体目标继承。</sample>
    <sample id="281">约束语言规划

如何制作草莓蛋糕？
…将草莓酱加入面粉中…

如何制作巧克力蛋糕？
…将可可粉加入面粉中…

具体目标可以被不同的现实生活中具有多方面约束的具体目标继承。</sample>
    <sample id="282">LLMs如何在约束语言规划中表现？数据集：wikiHow + 生成的约束约束类型1：修饰符定义：通过添加属性或修改来修改或约束抽象目标示例1：制作巧克力蛋糕。示例2：制作粉色蛋糕约束类型2：方法定义：控制过程的工具或指定模式示例1：用烤箱做蛋糕。示例2：用蛋糕模具做蛋糕约束类型3：意图定义：实现额外目的或需求时的附加目的或需求示例1：为婚礼做蛋糕。示例2：为糖尿病患者做蛋糕</sample>
    <sample id="283">LLMs在受限语言规划中的表现如何？

数据集：wikiHow + 生成的约束

约束类型1：修饰语

定义：一个描述或修改目标的属性或属性。

示例1：制作一块巧克力蛋糕，或者一块具有特定特征的蛋糕。
示例2：制作一块粉色蛋糕。

约束类型2：方法

定义：一种工具或指定模式，控制过程或操作。

示例1：用烤箱做蛋糕。
示例2：用蛋糕模具做蛋糕。

约束类型3：意图

定义：在满足目标或需求时的额外目的或意图。

示例1：为婚礼做蛋糕。
示例2：为糖尿病患者做蛋糕。</sample>
    <sample id="284">LLMs在受限语言规划中的表现如何？</sample>
    <sample id="285">LLMs是否能进行约束语言规划？</sample>
    <sample id="286">LLMs是否能进行约束语言规划？</sample>
    <sample id="287">LLMs在生成任务中通常会犯什么类型的错误？</sample>
    <sample id="288">LLMs在生成任务中通常犯什么类型的错误？幻灯片分为两部分。左侧是一个圆形图表，展示了不同类型的错误，如“无约束”、“缺失步骤”、“重复步骤”、“错误顺序”和“不一致”。右侧有一个图片，显示了一个穿着绿色衬衫、戴着眼镜的人坐在现代办公室里。幻灯片底部的文字解释了LLMs在生成脚本时可能犯的错误类型。</sample>
    <sample id="289">InstructGPT通常会在什么类型的任务上失败？</sample>
    <sample id="290">输入：一个抽象目标
步骤1：生成具体目标，使用GPT-4进行上下文学习
抽象目标：制作蛋糕
约束条件：
具体目标：
G1（修改器）：制作一块巧克力蛋糕
G2（方法）：在婚礼上制作一块蛋糕
G3（意图）：为婚礼制作一块蛋糕</sample>
    <sample id="291">输入：一个抽象目标
步骤1：生成具体目标，使用GPT-4进行学习
抽象目标：制作蛋糕
约束条件：
具体目标：
G1（修改器）：制作一块巧克力蛋糕
G2（方法）：用蜡烛装饰蛋糕
G3（意图）：为婚礼制作蛋糕</sample>
    <sample id="292">输入：一个抽象目标：制作蛋糕
具体目标：
G1（修改器）：制作巧克力蛋糕
G2（方法）：在婚礼上制作蛋糕
G3（意图）：为婚礼制作蛋糕
候选脚本
步骤1：生成G1的备选方案，通过GPT-4进行in-context学习
步骤2：通过GPT-4生成G2和G3的备选方案
步骤3：根据GPT-4生成的备选方案，生成G1的备选方案</sample>
    <sample id="293">方法

步骤1：通过In-context learning生成候选脚本
步骤2：通过InstructGPT找到与目标相关的脚本
步骤3：根据相似度分数筛选脚本
输出：具有相应脚本的具体目标</sample>
    <sample id="294">该图片展示了使用InstructGPT通过上下文学习生成和筛选脚本的方法。它分为三个主要步骤：

1. **生成候选脚本**：首先，通过InstructGPT生成多个脚本。这些脚本是根据特定目标生成的，例如“制作巧克力蛋糕”。

2. **计算相似度得分**：然后，使用InstructGPT计算每个脚本与目标的相似度得分。这一步骤涉及评估脚本与目标之间的相关性，以确定哪些脚本最接近目标。

3. **筛选脚本**：最后，根据相似度得分筛选出最相关的脚本。在这个例子中，脚本3被选中，因为它具有最高的相似度得分。

筛选后的脚本将用于实现特定的目标。例如，在制作巧克力蛋糕的情况下，脚本3将指导如何准备食材、制作面糊、烘烤蛋糕以及加入可可粉。

这种方法通过确保所选择的脚本与目标高度相关，从而提高了任务执行的效率和准确性。</sample>
    <sample id="295">方法

步骤1：通过In-context learning生成与InstructGPT相关的候选脚本
步骤2：使用InstructGPT找到与目标相似度得分最高的脚本
步骤3：找到符合目标的脚本
输出：具有相应目标的特定脚本</sample>
    <sample id="296">我们的方法显著提高了规划质量

在右侧的视频窗口中，一位女士坐在现代办公室里，戴着耳机，可能正在进行演示。在左侧的幻灯片上，有一个柱状图，展示了不同方法的准确性比较。柱状图显示了以下几种方法：TS (11B)、Flan-T5 (11B)、GPT-3 (12B)、CodeGPT (175B)、InstructGPT (475B) 和我们的方法。柱状图表明，“我们的方法”具有最高的准确性。

幻灯片底部的文字进一步说明：“使用我们的方法，InstructGPT 可以生成显著更高质量的规划，超出其他所有方法。” 这个陈述强调了“我们的方法”的优越性，并表明它在规划任务上的表现明显优于其他基准模型。</sample>
    <sample id="297">Script Distillation from LLMs

Motivation
为了使LLMs具有更小模型的约束语言规划能力。

Method
遵循符号知识蒸馏的想法。
从基于LLMs的55,000个脚本中生成约束，使用约束集方法。约束集方法 = 约束集数据集。
通过注释验证和测试集进行验证。

Input: 一个抽象
Step 1: 使用InstructGPT通过内在学习生成特定目标的脚本。
Step 2: 使用InstructGPT通过上下文学习生成候选脚本。
Step 3: 使用InstructGPT通过相似性评分找到最相关的脚本。

Output: 具有具体目标和相应规划的特定脚本。</sample>
    <sample id="298">Script Distillation from LLMs

Motivation
为了使约束语言模型具有较小模型的规划能力。
方法
遵循符号知识蒸馏的想法。
生成55,000个基于LLM的约束脚本，使用约束方法 = 约束集数据集。
通过注释验证和测试集进行评估。

输入：一个摘要
步骤1：使用InstructGPT通过上下文学习生成特定目标的脚本。
步骤2：使用InstructGPT通过上下文学习过滤生成的候选脚本。
步骤3：找到与InstructGPT相似性得分最高的脚本。
输出：具有对应规划的特定目标。</sample>
    <sample id="299">从LLMs中提取脚本

动机：

* 通过使用较小的模型来实现约束语言规划能力。
方法：

* 遵循符号知识蒸馏的想法。
* 根据LLMs生成55,000个符合约束条件的脚本，基于约束方法 = &gt;约束数据集。
* 使用注释验证和测试集进行验证。

输入：抽象

步骤1：使用InstructGPT通过上下文学习生成特定目标的脚本。

步骤2：使用InstructGPT通过上下文学习过滤生成的候选脚本。

步骤3：找到与InstructGPT相似性得分最高的脚本。

输出：具有具体目标和相应规划的特定目标。</sample>
    <sample id="300">Script Distillation from LLMs

Motivation
为了使约束语言模型具有较小模型的规划能力。
方法
遵循符号知识蒸馏的想法。
生成了55,000个根据LLMs基于约束条件的脚本，使用的方法是：约束集数据集。
结果
验证和测试集上的注释。

输入：一个摘要
步骤1：使用InstructGPT通过内在学习生成特定目标的脚本。
步骤2：使用InstructGPT通过内在学习生成候选脚本。
步骤3：使用InstructGPT通过相似性评分找到过滤后的脚本。
输出：具有特定目标和相应规划的脚本。</sample>
    <sample id="301">Script Distillation from LLMs

Motivation:
- 使约束语言模型具有较小模型的能力。
- 通过LLMs生成55,000个符合约束的脚本，基于一种方法。
- 使用标注验证集和测试集进行验证。

Method:
- 遵循符号知识蒸馏的想法。
- 通过LLMs生成符合约束的脚本，基于一种方法。
- 使用InstructGPT通过相似性评分找到最相关的脚本。

Output:
- 具有特定目标和相应计划的具体脚本。</sample>
    <sample id="302">约束分析

在幻灯片的右侧，有一个戴着绿色衬衫和眼镜的人。背景显示一个现代办公室环境，有高高的天花板、吊灯和一些植物。

幻灯片底部的文字翻译成中文如下：

“脚本显示了生成的具体目标中存在高度异质性和 pluralism。”</sample>
    <sample id="303">**图1：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图2：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图3：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图4：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图5：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图6：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图7：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图8：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图9：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图10：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图11：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图12：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图13：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图14：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图15：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图16：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图17：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图18：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图19：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图20：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图21：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图22：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图23：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图24：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图25：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图26：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图27：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图28：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图29：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图30：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图31：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图32：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图33：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图34：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图35：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图36：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图37：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图38：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图39：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的场景，其中一位女士正在使用专门化模型生成高质量脚本。

**图40：微调LLMs与专门化模型的比较**

左侧的柱状图展示了不同模型在Coscript上的准确率。具体来说，它比较了GPT-3（175B）、Codex（175B）、InstructGPT（175B）和T5训练在wikiHow上的性能。右侧的图片显示了一个使用专门化模型进行微调的</sample>
    <sample id="304">建立受约束的语言规划问题。评估LLMs在语言规划中的能力，并开发一种“先生成再过滤”的方法来处理LLMs。使用LLMs生成高质量的脚本数据集（CoScript）以进行受约束的语言规划。限制和未来工作：- 提出的提高LLMs的方法是一种后 hoc 重新排名方法。- CoScript 只继承了一个抽象脚本的一个约束。- CoScript 数据集可以成为研究语言规划的宝贵资源，具有更复杂和多样的目标和约束。</sample>
    <sample id="305">建立约束语言规划问题。评估LLMs在约束语言规划中的能力，并开发一种“先生成后过滤”的方法。使用LLMs生成高质量的脚本数据集（CoScript）用于约束语言规划。限制和未来工作：所提出的方法是后 hoc 重排名方法；CoScript 只继承自一个抽象脚本，具有一个约束；CoScript 数据集可以成为研究语言规划的有价值资源，涉及更复杂和多样的目标和约束。</sample>
    <sample id="306">61st Annual Meeting of the Association for Computational Linguistics
多伦多，加拿大
2023年7月13日至14日

从大型语言模型中提取脚本知识
用于受限语言规划

李舒、张洁、陈静、符子涵、符永、
Ge、沙姆·拉克·尚科、查尔斯·J安德森、
扬花肖、Deging杨

coscript网站：https://github.com/siyuanyuan/coscript</sample>
    <sample id="307">根据幻灯片中提供的信息，PaLM 的流畅度与当前领先系统相当。</sample>
    <sample id="308">根据幻灯片中提供的英文内容，水印方法的重要属性包括：1.可应用性：适用于EaaS。2.效用：不会降低所提供的嵌入的效用。3.隐蔽性：应对攻击者来说是隐蔽的。4.可转移性：水印需要能够转移到攻击者的服务上。</sample>
    <sample id="309">根据幻灯片中提供的信息，TED 英语演讲已被翻译成 14 种不同的语言。这些语言包括：西班牙语、法语、阿拉伯语、意大利语、荷兰语、俄语、日语、韩语、葡萄牙语、中国语、巴西语、德语、波兰语和罗马尼亚语。</sample>
    <sample id="310">根据流程图中所示，从数据集中抽取了100个实例用于重新注释。</sample>
    <sample id="311">根据幻灯片中提供的英文内容，用于衡量良性和后门数据集之间差异的距离度量是余弦相似度和KS统计量。余弦相似度通过计算两个向量之间的夹角的余弦值来衡量它们的方向，而KS统计量（Kolmogorov-Smirnov统计量）衡量的是两个概率分布之间的最大差异。</sample>
    <sample id="312">根据表格中显示的英文内容，基于编码器的多语言模型似乎是通过使用预训练的编码器来处理不同语言的数据。这些编码器可以理解多种语言，因此它们可以被微调以在各种任务上进行性能测试，如表格中列出的MATS、MgQuery、MnMaps等。这些模型的性能指标（如准确率）表明了它们在多语言环境下的表现。</sample>
    <sample id="344">根据幻灯片中提供的信息，作者通过在一般文本语料库中计算单词频率来确定中等频率的单词。这一步骤是触发选择过程的一部分，触发选择过程包括计算单词频率和随机选择一定数量的单词。</sample>
    <sample id="345">Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?
李书恒，Alan Ritter
互动计算学院
佐治亚理工学院</sample>
    <sample id="346">命名实体识别与泛化</sample>
    <sample id="347">命名实体识别与泛化
·模型已经使用CoNLL-2003开发NER近20年</sample>
    <sample id="348">命名实体识别和泛化

* 模型已经使用CoNLL-2003开发NER近20年
* 这些模型能否泛化到现代数据？
* 什么是对泛化很好的需求？</sample>
    <sample id="349">命名实体识别与泛化

* 模型已经使用CoNLL-2003开发了近20年的NER
* 这些模型能否泛化到现代数据？
* 什么是对泛化很好的需求？</sample>
    <sample id="350">ConLL+ Dataset AMBASSADOR UNITED NATIONS LINDA THOMAS-GREENFIELD</sample>
    <sample id="351">ConLL+ Dataset • 收集了2020年的路透社新闻并按照ConLL-2003注释指南进行了注释 • 在ConLL-2003上微调了20多个模型 AMBASSADOR O UNITED OORG NATIONS O LINDA I-PER THOMAS-GREENFIELD I-PER</sample>
    <sample id="352">CoNLL+数据集
*从2020年收集Reuters新闻并使用CoNLL-2003注释指南注释
*在CoNLL-2003上微调了20多个模型
*在CoNLL-2003测试集和CoNLL+上进行评估
*计算百分比ΔF1以评估泛化能力
*评估了以下实体：大使馆、联合国、琳达·托马斯-格林菲尔德</sample>
    <sample id="353">什么对于良好的泛化是必要的？</sample>
    <sample id="354">什么是对泛化有利的？ • 模型架构 • 转换器模型泛化更好 训练集大小 vs. 测试集上的F1分数 训练集大小（千） F1分数（测试集） 0 0.0 20 0.2 40 0.4 60 0.6 80 0.8 100 1.0 120 1.2 140 1.4 160 1.6 180 1.8 200 2.0 220 2.2 240 2.4 260 2.6 280 2.8 300 3.0 320 3.2 340 3.4 360 3.6 380 3.8 400 4.0 420 4.2 440 4.4 460 4.6 480 4.8 500 5.0 520 5.2 540 5.4 560 5.6 580 5.8 600 6.0 620 6.2 640 6.4 660 6.6 680 6.8 700 7.0 720 7.2 740 7.4 760 7.6 780 7.8 800 8.0 820 8.2 840 8.4 860 8.6 880 8.8 900 9.0 920 9.2 940 9.4 960 9.6 980 9.8 1000 10.0</sample>
    <sample id="355">好的泛化需要什么？ 模型架构：Transformer模型泛化效果更好。 模型大小：较大的模型泛化效果更好。</sample>
    <sample id="356">需要什么才能实现良好的泛化？模型架构：Transformer模型泛化效果更好。模型大小：更大的模型泛化效果更好。微调示例数量：更多的示例有助于更好的泛化。</sample>
    <sample id="357">What Causes Performance Drop?</sample>
    <sample id="358">什么会导致性能下降？</sample>
    <sample id="359">What Causes Performance Drop?</sample>
    <sample id="360">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="361">What Causes Performance Drop? Adaptive overfitting? Temporal drift?</sample>
    <sample id="362">What Causes Performance Drop? Adaptive overfitting? No observed returns Temporal drift?</sample>
    <sample id="363">What Causes Performance Drop? Adaptive overfitting? No diminishing returns Temporal drift?</sample>
    <sample id="364">What Causes Performance Drop? Adaptive overfitting? Not observed returns Temporal drift? Name CoNLL-2003 CoNLL++ (AF) Flair 92.46% 88.51% Pooled Flair 93.15% 88.56% Poole Flair 92.36% 87.09% ELMo 92.11% 90.75% L MoN 92.11% 90.75% -1.43</sample>
    <sample id="365">What Causes Performance Drop? Adaptive overfitting? No dim observed returns Temporal drift? Performance degrades with larger temporal gap Name CoNLL-2003 CoNLL++ AF (%) Flair 92.26% 84.69% Pooled Flair 93.15% 88.20% Flairx 90.91% 84.69% Pooled Flairx 92.36% 87.08% ELMO 92.11% 90.76% MSoG 90.76% -1.43%</sample>
    <sample id="366">结论
为了获得良好的泛化能力，我们需要：
- 更好的模型架构
- 更大的模型规模
- 更多的微调示例
从2004年到2022年，LLM性能显著提升。在这一时期，LLM性能的提升主要体现在自然语言处理（NLP）任务上，如命名实体识别（NER）、情感分析（Sentiment Analysis）和问答系统（Question Answering System）。这些任务的性能提升是由于模型架构、模型规模和训练数据的改进。例如，BERT和LUKE等预训练模型的引入显著提高了LLM在这些任务上的性能。</sample>
    <sample id="367">结论
为了获得良好的泛化能力，我们需要：
- 更好的模型架构
- 更大的模型尺寸
- 更多的调优示例
- 性能下降是由以下原因引起的：
- 时序漂移
- 非自适应过拟合
性能
图中的数据表明，BERT和LUKE等较新模型在性能上显著优于早期的命名实体识别（NER）模型。这与研究中提到的“更好的模型架构”和“更大的模型尺寸”的观点相吻合。随着模型的发展，它们在处理自然语言任务时表现出更高的准确性和效率。</sample>
    <sample id="368">结论
为了获得良好的泛化能力，我们需要：
- 更好的模型架构
- 更大的模型尺寸
- 更多的精调示例
性能下降的原因：
- 不适应过拟合
- 2003年ConLL标注器是否仍然有效？
该幻灯片展示了自然语言处理（NLP）领域中模型性能随时间的变化。它强调了在NLP任务中保持良好泛化能力的重要性，并指出了影响性能的因素。图表显示了从2004年到2022年的趋势，期间各种模型如Stanford NLP、BERT、BERT大型和LUKE的性能有所提升。这些改进可以归因于更好的模型架构、更大的模型尺寸以及更多的精调示例。然而，幻灯片还提到了不适应过拟合和ConLL-2003标注器的有效性问题，这可能会影响模型的性能。</sample>
    <sample id="369">为了实现良好的泛化，我们需要：更大模型尺寸、更多微调示例。性能下降是由以下原因引起的：不自适应过拟合。ConLL-2003标注器是否仍然有效？是的。</sample>
    <sample id="370">论文：https://arxiv.org/abs/2212.09747 数据集：https://github.com/ShuhenL/acl2023-conllpp 联系方式：slu775@gatech.edu</sample>
    <sample id="397">根据图片中可见的英文内容，无法确定该方法使用的语音片段大小。幻灯片上没有提供任何关于语音片段大小的信息。</sample>
    <sample id="398">在 Servin 和 Kea 的示例中，需要特定于实体的知识包括理解 Servin 是一个“Judge”和 Kea 是一个“baker”。这些知识对于正确回答问题至关重要，因为它们提供了关于角色的背景信息，从而有助于确定答案。</sample>
    <sample id="399">根据幻灯片上提供的信息，示例质量比与源句子的相似度更为重要。</sample>
    <sample id="400">根据图表，扩展实验中论文侧重的语言模型包括BERT-base、RoBERTa-base、distilBERT、ALBERT-base和GPT-3。这些模型位于图表的左上象限，表明它们具有权威性特征。</sample>
    <sample id="401">根据图片中提供的英文内容，该模型是使用特定层的注意力分数。这可以从幻灯片底部的注释中推断出来，它提到“使用特定层的注意力分数”，表明模型仅关注神经网络中的一个特定层，而不是结合多个层的分数。</sample>
    <sample id="402">根据幻灯片中提供的英文内容，直接推断的示例包括“第一个”、“不能记住名字”、“无法区分 pronunciation”和“想指定一个偏好”。</sample>
    <sample id="403">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是“复旦大学”和“Brain Technologies Inc.”。这些机构名称位于作者姓名下方，表明了他们在撰写和发表该论文时的关联。</sample>
    <sample id="404">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="405">根据幻灯片中提供的信息，是的，在语义解析之前使用机器翻译模型翻译自然语言查询作为基线。这可以通过流程图中所示的“Translate API”步骤来推断，该步骤位于“English Model”之后和“SQL”之前，表明在将自然语言查询处理为SQL之前，先将其翻译成英语。</sample>
    <sample id="406">根据幻灯片中提供的英文内容，作者给出的“显性群体”(marked group) 的示例是“一个女人战士”。</sample>
    <sample id="407">根据图表显示，泛化能力较差的模型架构包括“CNN”、“RNN”和“LSTM”。这些模型在图表上位于曲线下方，表明它们在CIFAR-100数据集上的性能不佳。相比之下，“Transformer”模型位于曲线上方，表明它们具有更好的泛化能力。</sample>
    <sample id="408">测试数据集的名称是“CIFAR100”。</sample>
    <sample id="409">根据幻灯片上显示的英文内容，这篇论文有五位作者。这可以通过在幻灯片底部列出的五张照片和名字来确定，每张照片下面都有一个名字和与作者相关的机构名称。</sample>
    <sample id="410">根据幻灯片上显示的英文内容，作者采用了多种模态。这可以从标题“Multimodal Pre-trained Models”中看出，表明模型被训练以处理和理解来自不同来源的数据，如文本、图像或音频。</sample>
    <sample id="439">根据幻灯片中所给的英文内容，作者认为NLU中研究不足的领域包括：1. 与预训练知识相关的任务，如“What presidents do”和“Who is John？”2. 与推理时知识相关的任务，如“What a TV”和“Who is the newly elected president？”这些任务被标记为“未解决的问题”，表明它们在NLU研究中尚未得到充分解决或探索。</sample>
    <sample id="440">演讲者的名字是“Zhongyang Xu”，“Ying Shen”和“Lifu Huang”。这些名字位于幻灯片上，与他们的照片并列。</sample>
    <sample id="441">是的，根据幻灯片中提到的流程图，Coscript 经过了质量检查。在输出阶段，它被描述为“通过 InstructGPT 的相似性评分过滤”，这表明 Coscript 已经经过了某种形式的质量验证或评估，以确保其符合预期标准或性能。</sample>
    <sample id="442">根据幻灯片中提供的英文内容，对于依赖上下文的翻译，现有的资源局限性包括：1.现有的方法只支持有限的 discourse 现象和语言。2.只有少数单词依赖于上下文。3.语料库级别的指标存在。</sample>
    <sample id="443">解决AltEntities语料库中实体选择的间接引用表达式</sample>
    <sample id="444">解决AltEntities语料库中实体选择的间接引用表达式</sample>
    <sample id="445">间接指示表达

Google Research

* 神：理解用户在做选择时的语言
* 问题：你是指容易让我还是我必须感觉？

直接引用

* “我第一个”：无法在自然流畅的对话中记住名称
* 间接引用：难以区分 pronunciation，想要指定偏好

间接引用

* 最新的一个。
* 那首不那么充满活力的歌曲。

通过选择（Mnli） Corpus解决间接引用表达</sample>
    <sample id="446">间接指示表达

目标：理解用户在做选择时的语言

替代问题：你是指容易让我还是我必须感觉？

直接引用：我第一个

间接引用：无法记住名字：自然流畅的对话：无法区分：想要指定偏好

间接引用：新的那个。那首不那么 energetic 的歌。</sample>
    <sample id="447">间接指示表达

目标：理解用户在做出选择时的语言

替代问题：你是指容易为我，还是我必须感觉？

直接引用：无法记住名称，在自然流畅的对话中：

无法区分这些称谓：想要指明一个偏好

间接引用：新的那个。那首不那么充满活力的歌。</sample>
    <sample id="448">间接指示表达

目标：理解用户在做选择时的语言

替代问题：你是指容易让我还是让我感觉好？

直接引用：我不能记住名字；无法区分这些 pronunciations；想指定一个偏好

间接引用：新的那个。那首不那么 energetic 的歌。

来源：《解决间接指示表达》(Indirect Reference Expressions by Entity Selection (Midterm Corpus))</sample>
    <sample id="449">间接指示表达

目标：理解用户在做出选择时的语言

替代问题：你是指容易让我还是让我感觉？

直接引用：我不能记住名字；无法区分这些名称；想指定偏好

间接引用：新的那个。那首不那么 energetic 的歌。

来源：Indirect Indict Expressions by Entity Selection (Mitya Corpus)</sample>
    <sample id="450">解决问题：重要问题
- 重要问题
- 重要问题
- 没有大规模公开数据集可用
- 我们使用 crowdsourcing 收集大规模数据集
- 三个领域：
- 语音识别
- 文本处理
- 信息检索</sample>
    <sample id="451">数据集收集

• 重要问题
	+ 重要系统
	+ 大型语言模型的实体理解基准测试
	+ 没有大规模公开数据集可用
• 我们使用 crowdsourcing 收集大量数据
• 三个领域：
	- 音频
	- 图书
	- 餐具
	- 一个男人的头像</sample>
    <sample id="452">数据集收集方法

该方法强调使用卡通完成任务的**不正式性**。


记住我们昨天一起唱的那首歌吗？
你什么意思？我有一首Gotta Feeling？

填充由注释员完成</sample>
    <sample id="453">数据集收集方法

该方法强调使用卡通完成任务的形式化

记住我们昨天一起唱的那首歌吗？

你意思是“我有一颗心”吗？我感觉有点儿Gotta Feeling？

由注释员填写</sample>
    <sample id="454">数据集收集方法

该方法强调使用卡通完成任务的形式化。

记住我们昨天一起唱的那首歌吗？

你意思是“1 gotta feeling”吗？

由标注员填充</sample>
    <sample id="455">数据集收集方法

该方法强调使用卡通完成任务的不正式性。

记住我们昨天做的那个梦吗？

你昨天感觉怎么样？我有一个想法。

表达指的是其中一个实体。</sample>
    <sample id="456">数据集收集方法

该方法强调使用卡通完成任务的不正式性。

记住昨天你唱的那首歌吗？

“你昨天唱了什么？”这个替代问题

表达指的是其中一个实体

由注释员填写

图片底部的文字翻译为：

“设置对话背景（从几组手动提示中选择）[实体选择语料库]”</sample>
    <sample id="457">数据集收集方法

该方法强调使用卡通完成任务的形式化。


图片左侧的文本描述了对话的背景（从几篇手动提示中选择）。</sample>
    <sample id="458">生成替代问题 = 抽样实体对

图片显示了一个演示幻灯片，标题为“生成替代问题 = 抽样实体对”。幻灯片分为两部分。左侧部分有一个输入框，里面写着“Do you mean A or B？”（你是指A还是B？），并附带一个黄色箭头，指向一个标签，上面写着“More Similar (usually)”。

幻灯片的右侧部分详细说明了如何根据Wikipedia上的相似信息框生成问题。它列出了以下示例：

- Items with similar infoboxes on Wikipedia (same genre and/or artist)
  - 示例：'Do you mean "This Is It" or "Man in the Mirror"?'
- Items with similar descriptions on Wikipedia
  - 示例：'Do you mean "Thinking of You" or "Happy Anywhere"?'
- Items with similar titles
  - 示例：'Do you mean "The Return: Memoir" or "The Return (Shatner novel)"?'
- Uniform at random:
  - 示例：'Do you mean "You Could Be Mine" or "The Way I Am"?'

幻灯片底部有一个Google Research的标志，并且有一个圆形图片，显示了一个男人的面部照片。

翻译后的中文内容如下：

生成替代问题 = 抽样实体对

图片显示了一个演示幻灯片，标题为“生成替代问题 = 抽样实体对”。幻灯片分为两部分。左侧部分有一个输入框，里面写着“Do you mean A or B?”（你是指A还是B？），并附带一个黄色箭头，指向一个标签，上面写着“More Similar (通常)”。

幻灯片的右侧部分详细说明了如何根据Wikipedia上的相似信息框生成问题。它列出了以下示例：

- 与Wikipedia上的相似信息框相关的项目（相同的类型和/或艺术家）
  - 示例：'Do you mean "This Is It" or "Man in the Mirror"?'
- 与Wikipedia上的相似描述相关的项目
  - 示例：'Do you mean "Thinking of You" or "Happy Anywhere"?'
- 与相似标题相关的项目
  - 示例：'Do you mean "The Return: Memoir" or "The Return (Shatner小说)"?'
- 随机选择：
  - 示例：'Do you mean "You Could Be Mine" or "The Way I Am"?'

幻灯片底部有一个Google Research的标志，并且有一个圆形图片，显示了一个男人的面部照片。</sample>
    <sample id="459">生成替代问题 = 抽样实体对

Do you mean A or B?（你是指A还是B？）

与Wikipedia上的相似infoboxes（同一类型/艺术家）物品：你是指《镜子》还是《镜子里的你》？

与Wikipedia上的相似描述物品：你是指《你想得见我》还是《无处快乐》？

具有相似标题的物品：你是指《归来的记忆》还是《归来的阴影小说》？

通常均匀分布：你是指《你本可以成为谁》还是《我是这样的人》？

来源：Google Research</sample>
    <sample id="460">生成替代问题 = 抽样实体对

“Do you mean A or B?”
- 类似的infoboxes（在Wikipedia上具有相同的类型/艺术家）：
  - “Do you mean 'This is It' or 'Man in the Mirror'?（你是指《这是它》还是《男人在镜子里》？）”
  - “Do you mean 'Thinking of You' or 'Happy Anywhere'?（你是指《思念你》还是《快乐无处不在》？）”
  - “Do you mean 'The Return (memoir)' or 'The Return (Shatner novel)'?（你是指《归来的回忆录》还是《归来的夏特纳小说》？）”
- 随机选择：
  - “Do you mean 'You Could Be Mine' or 'The Way I Am'?（你是指《你能属于我》还是《这就是我》？）”</sample>
    <sample id="461">生成替代问题 = 抽样实体对

Do you mean A or B?
物品具有相似的infobox（来自Wikipedia，具有相同的类型/艺术家）。
物品具有相似的描述（来自Wikipedia）。
Do you mean "Thinking of You" or "Happy Anywhere"?
物品具有相似的标题（来自Wikipedia）。
Do you mean "The Return (memoir)" or "The Return (Shatner novel)"?
物品具有相似的标题（来自Wikipedia）。
均匀随机：
Do you mean "You Could Be Mine" or "The Way I Am"?
物品具有不同的标题（来自Wikipedia）。</sample>
    <sample id="462">生成替代问题 = 抽样实体对

Do you mean A or B?</sample>
    <sample id="463">Google搜索链接歌曲
“Easy on Me”(阿黛尔)
点击了解有关这首歌曲的信息
“Gotta Feeling”(黑眼豆豆乐队)
点击了解有关这首歌曲的信息
我们要求注释者
-收听每首歌至少一部分
-读一读每首歌
Google Research</sample>
    <sample id="464">Google搜索链接歌曲

* 点击这里了解关于这首歌曲的信息
* 让标注员
* 听至少一部分每首歌
* 阅读每首歌的资料</sample>
    <sample id="465">Google搜索链接每首歌曲。点击了解关于歌曲的信息。点击了解关于歌曲的信息。我们要求标注员至少听每首歌的前几部分，读关于每首歌的内容。</sample>
    <sample id="466">背景知识（食谱）：金盏花蛋糕（食谱）：金盏花蛋糕是一种在英国、爱尔兰和其他国家广受欢迎的水果蛋糕，与迁徙有关。它通常在复活节和 Lent 等节日食用。它由11个球状的杏仁糖霜制成，象征着耶稣的11位门徒，特别是荷兰的印尼社区。 Pandan 蛋糕是一种轻盈、松软的海绵蛋糕，味道带有 Pandanus amaryllifolius 的果汁味。这种蛋糕在印度尼西亚、马来西亚和荷兰等地区很受欢迎。</sample>
    <sample id="467">引出表情

然后告诉标注员应该选择哪个选项，并要求他们描述它。

“Easy on Me” (阿黛尔)

“我有感觉”(黑眼豆豆)

我们想让你给我们3到5个关于所选歌曲的描述，以填满你的语音输入示例。例如：

“带有轻松音乐的歌曲”

“这首不那么 energetic 的歌曲”

“关于河流的歌曲”

“关于没有时间的歌曲”</sample>
    <sample id="468">引出表情

然后告诉标注员应该选择哪个选项，并要求他们描述它。

“易如反掌”(阿黛尔)

“我得感觉”(黑眼豆豆)

我们希望给你3到5个选定歌曲的描述，以填满你的语音转文本示例。</sample>
    <sample id="469">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

- 92-95% if the LM has access to the same background knowledge as annotators.
- 82%-87% when the LM has access to partially overlapping background knowledge.
- 60% when the LM (T5 XL) only has access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="470">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

- 92-95% when the LM has access to the same background knowledge as annotators.
- 82%-87% when the LM has access to partially overlapping background knowledge.
- 60% when the LM (T5 XL) only has access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="471">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

- 92-95% if the LM has access to the same background knowledge as annotators.
- 82%-87% when the LM has access to partially overlapping background knowledge.
- 60% when the LM (T5 XL) only has access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities</sample>
    <sample id="472">AltEntities Corpus

- 6,000 alternative questions across the three domains
- 42,000 indirect referring expressions

Results with T5 XL model (accuracy):

- 92-95% if the LM has access to the same background knowledge as annotators.
- 82%-87% when the LM has access to partially overlapping background knowledge.
- 60% when the LM (T5 XL) only has access to the entity names.

We showed models are domain-generalizable.

Dataset Link: https://github.com/google-research-datasets/AltEntities

Google Research</sample>
    <sample id="473">根据图表标题“EDAtt: Main Results”，该方法与以下现有的SimulST策略进行了比较：wait-k、LA、CAT和EDAtt。这些策略通过其各自的线表示在图表上，每种策略用不同的颜色和标记进行区分。</sample>
    <sample id="474">根据幻灯片底部的标志和文字，这篇论文的作者分别来自以下机构：里昂第一大学的LIA（里昂人工智能实验室），里昂第二大学的LS2N（里昂神经科学与信息学实验室），南希-梅斯大区的CHU Nancy（南希-梅斯大学医院），以及Zenith大学。</sample>
    <sample id="475">演讲者的名字是Katharina Reinecke。</sample>
    <sample id="476">根据幻灯片上显示的英文内容，这篇论文有三位作者。</sample>
    <sample id="477">注意作为 simultaneous speech translation 的指南 Sara Papi, Matteo Negri, Marco Turchi UNIVERSITA DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="478">注意作为 simultaneous speech translation 的指南 Sara Papi, Matteo Negri, Marco Turchi UNIVERSITÀ DI TRENTO FONDAZIONE BRUNO KESSLER</sample>
    <sample id="479">当前SimulST模型的问题是什么？</sample>
    <sample id="480">当前SimulST模型的问题是什么？特定架构通常是在引入额外模块以进行优化的情况下进行训练的。复杂的训练程序（例如，不同的优化目标）</sample>
    <sample id="481">当前SimulST模型的问题是什么？特定架构通常被训练，引入额外的模块以进行优化。复杂的训练程序（例如，不同的优化目标）训练和维护多个模型以达到不同的延迟 regimes。</sample>
    <sample id="482">What is our solution?</sample>
    <sample id="483">01 Use already existing offline ST models without re-training or adopting specific architecture for SimuST</sample>
    <sample id="484">我们的解决方案是什么？ 1. 使用已有的离线ST模型，无需重新训练或采用特定参数的SimuST架构。 2. 为每个延迟范围使用一个模型，并通过特定参数处理延迟。 3. 利用已经获得的知识，通过音频输入和文本输出之间的注意力机制。</sample>
    <sample id="485">我们的解决方案：EDAtt</sample>
    <sample id="486">我们的解决方案：EDAtt
编码器-解码器注意力
决定是否根据注意力点发出或发出部分翻译，如果注意力未集中在最后一个λ帧上（其总和低于阈值α），则发出一个词，意味着接收到的信息足够稳定。</sample>
    <sample id="487">我们的解决方案：EDAtt
编码器-解码器注意力
决定是否在注意力不集中的位置发出部分翻译，如果注意力的总和低于阈值λ，则发出一个词（其总和低于阈值λ），这意味着接收到的信息足够稳定。</sample>
    <sample id="488">我们的解决方案：EDAtt
编码器-解码器注意力
决定是否根据单词发出的注意力来发出部分翻译（如果注意力未集中在最后λ个语音帧上，则其总和低于阈值α），这意味着接收到的信息足够稳定。</sample>
    <sample id="489">我们的解决方案：EDAtt
编码器-解码器注意力

决定是否发出部分翻译，基于注意力是否集中在单词上（其总和低于阈值λ）；这意味着接收到的信息足够稳定。

“我打算谈论……”</sample>
    <sample id="490">我们的解决方案：EDAtt
决定是否发出部分翻译，基于单词发出的注意力是否未集中在最后λ个时间帧上，这意味着接收到的信息足够稳定。</sample>
    <sample id="491">我们的解决方案：EDAtt

编码器-解码器注意力

决定是否根据注意力集中点来发出或发出部分翻译。如果注意力未集中在单词上（其总和低于阈值λ），则认为最后λ帧接收到的信息足够稳定。

我打算 talk about...</sample>
    <sample id="492">我们的解决方案：EDAtt

编码器-解码器注意力

决定是否根据注意力指向的单词发出部分翻译，如果注意力不集中在阈值α以下的最后λ帧上，则发出单词，表示接收到的信息足够稳定。

“我打算谈论...”被翻译成“我打算谈论气候。”</sample>
    <sample id="493">我们的解决方案：EDAtt
编码器-解码器注意力
根据注意力点是否在以下范围内，决定是否发出或发出部分翻译：一个词的注意力不集中在（其总和低于阈值λ）最后λ个语音帧上，这意味着接收到的信息足够稳定。</sample>
    <sample id="494">我们的解决方案：EDAtt
编码器-解码器注意力
决定是否根据以下几点发出或不发出部分翻译：
1.单词的注意力点是否在阈值λ以下，表示最后λ帧的信息足够稳定。
2.单词的总和是否低于阈值λ，表示信息足够稳定。</sample>
    <sample id="495">Main Results: EDAtt AL / AL-CA (s)</sample>
    <sample id="496">Main Results: EDAtt 27 25 23 BLEU 21 19 17 15 1.5 2.5 3.5 4.5 5 AL / AL-CA (s) (a) en-de</sample>
    <sample id="497">Main Results: EDAtt 27 25 23 21 19 17 1.5 2.5 3.5 4.5 5 latency measure (a) AL-de</sample>
    <sample id="498">Main Results: EDAtt 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 AL / AL-CA (s)</sample>
    <sample id="499">主结果：EDAtt 6 8 10 12 14 16 18 20 22 24 26 27 AL / AL-CA (s)</sample>
    <sample id="500">主要结果：EDAtt流行策略也应用于离线模型wait-k LA CAAT EDAtt 27 25 23 21 19 17 0.5 1.5 2.5 3.5 4.5 5 AL/AL-CA(δ) (a) en-de</sample>
    <sample id="501">主结果：EDAtt
最先进架构，特别是针对SimultST专门定制的wait-k、LA、CAAT和EDAtt
AL/AL-CA（s）</sample>
    <sample id="502">主结果：EDAtt结果
wait-k LA CA EDAtt
27 25 23 21 19 17
AL/AL-CA (s)
0.5 1 2.5 3.5 4.5 5
(a) en-de</sample>
    <sample id="503">EDAtt Results: Main Results: EDAtt outperforms all the strategies applied to offline models</sample>
    <sample id="504">你想要发现更多吗？阅读我们的论文以发现更多结果！(spapi.negri@fbk.eu)marco.turchi@gmail.comgithub.com/hlt-mt/fbk-fairseq@fbk_mt@srarapapi</sample>
    <sample id="505">根据图片中提供的信息，数据集似乎是公开的。这可以从GitHub链接推断出来，GitHub是一个广泛使用的平台，用于托管和分享代码库，包括数据集。此外，数据集的URL的存在表明它可以通过提供的链接进行访问，这通常意味着它是公开可用的。</sample>
    <sample id="506">MULTINSTRUCT：通过指令调优改进多模零-shot学习

作者：赵扬、申英、黄立符

贡献：同等贡献</sample>
    <sample id="507">预训练语言模型用于下游任务

(A) 预训练-微调 (BERT, T5)

* 预训练在任务A上
* 使用大量任务特定示例对模型进行微调，为每个任务定制

(B) 激励 (GPT-3)

* 预先训练的模型
* 通过提供任务相关的输入来提高性能或进行工程

(C) 指示微调 (FLAN)

* 预先训练
* 通过自然语言任务学习来微调模型
* 在未见过的任务上进行推断

图2：比较指示微调与预训练-微调和提示。</sample>
    <sample id="508">预训练语言模型用于下游任务

(A) 预训练-微调 (BERT, T5)

* 预训练在任务A上
* 通过微调许多任务特定的模型来提高性能
* 为每个任务进行微调

(B) 激励 (GPT-3)

* 预先训练
* 通过提供示例或提示来改善性能或工程
* 推断

(C) 指示调整 (FLAN)

* 预先训练
* 通过自然语言学习来学习执行许多任务
* 推断在 unseen 上的任务</sample>
    <sample id="509">仅凭语言</sample>
    <sample id="510">在多模预训练模型上进行指令调优</sample>
    <sample id="511">不平衡在自然语言处理和多模态之间的指导数据集中</sample>
    <sample id="512">不平衡的指令数据集
NLP和多模态之间的不平衡
1600+ 语言-only 指令任务</sample>
    <sample id="513">多模态指令调优基准数据集</sample>
    <sample id="514">多模态指令调优基准数据集

图2：多模态指令调优基准数据集的白组任务。黄色代表用于评估的测试任务，白色代表用于训练的测试任务。

- 62种不同的任务
- 10个广泛的任务
- 5组专家编写指令</sample>
    <sample id="515">OFAs (One For All)

- 一个统一的多模态预训练模型，能够执行单个或多个模态下的理解和生成任务。
- OFAs具有**统一的词汇表**，用于语言、图像标记和坐标框的协调。

该图展示了OFAs的架构，包括其输入和输出。OFAs通过一个简单的序列到序列学习框架来统一处理各种任务。</sample>
    <sample id="516">输入：给定一张图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框的边界框。输入：给定一个图片和一个标题，标题包含一个文本框（例如，“在图片中找到一个女人拿着网球拍。”）。输出：标题中指定的文本框</sample>
    <sample id="517">图1：MULTINSTRUCT的四个任务示例。输入：给定一张图片，请识别图片中的物体。输出：图片中的物体是“火车”。输入：给定一张图片，请识别图片中的动作。输出：图片中的人正在打网球。输入：给定一张图片，请识别图片中的颜色。输出：图片中的主要颜色是“红色”。输入：给定一张图片，请识别图片中的物体。输出：图片中的物体是“火车”。</sample>
    <sample id="518">输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-81&gt;），请从给定的文本中选择描述该对象的文本。输出：&lt;bin-456-574&gt;。输入：给定一个标题（&lt;bin-198-32&gt;、&lt;bin-400-32&gt;），一个区域（&lt;bin-180-760&gt;）和一个对象（&lt;bin-448-3</sample>
    <sample id="519">多模态指令调优</sample>
    <sample id="520">多模态指令转变

训练数据集构建：
- 使用来自9组的53个任务进行训练。
- 每个任务采样10,000个实例。

测试数据集构建：
- 保留整个Commonsense Reasoning组用于测试。
- 从VQA和 Miscellaneous 组中选择额外的5个任务。
- 随机从Natural Instructions数据集的测试分割中选择20个任务作为 unseen 任务。</sample>
    <sample id="521">多模态指令转换

训练数据集构建：
- 使用来自9组的53个任务进行训练。
- 每个任务采样10,000个实例。

测试数据集构建：
- 保留Commonsense Reasoning组用于测试。
- 从VQA和Miscellaneous组中选择额外的5个任务。
- 随机从Natural Instructions数据集的测试分割中选择20个任务作为未见过的任务。</sample>
    <sample id="522">实施细节：
- 训练细节：
  - 使用预训练的 OFA-Large 模型（472M）。
  - 将所有实例与所有任务之一的说明模板混合。
  - 每个实例随机与其中一个说明模板组合。
- 测试细节：
  - 对于每个任务，我们总共进行五次实验，通过评估模型使用五个说明中的每一个来评估模型。
  - 我们报告五次实验中的平均和最大性能以及性能的标准差。</sample>
    <sample id="523">实施细节：
- 训练细节：
  - 使用预训练的 OFA-Large 模型（472M）。
  - 将所有任务的实例与其中一个指令模板随机结合。
- 测试细节：
  - 对于每个任务，我们总共进行五次实验，通过使用五个不同的指令评估模型。
  - 我们报告所有五次实验中的平均和最大性能以及性能的标准差。</sample>
    <sample id="524">实施细节：

- 训练细节：
  - 使用预训练的OF-A-Large模型（472M）。
  - 将所有实例与其中一个指令模板混合。
  - 每个实例随机与其中一个指令模板组合。

- 测试细节：
  - 对于每个任务，我们总共进行五次实验，通过评估模型在每个实验中使用五个不同的指令来评估模型。
  - 我们报告了五次实验中的平均和最大性能以及性能的标准差。</sample>
    <sample id="525">评价指标

- 对多模分类任务（视觉蕴含、视觉空间推理、自然语言视觉推理和灾情类型分类）我们报告准确率。
- 对多模生成任务（共同知识问答、文本问答、基于视觉场景的对话、视觉文本提取和视觉场景理解）我们报告Rouge-L。
- 对NLP任务，我们报告Rouge-L作为大多数任务的性能得分。
- 我们还根据模型在所有多模态和NLP任务上的平均表现计算了每个模型的聚合性能。我们不仅使用准确率作为指标。</sample>
    <sample id="526">敏感性
模型对同一任务的不同指令有多敏感：
E_{\theta,T}[σ(i, y)T [E_{x,y,T}[L(fθ(i, x), y)]]]
E_{\theta,T}[μ(i, y)T [E_{x,y,T}[L(fθ(i, x), y)]]]</sample>
    <sample id="527">效果显著的指导调优在MULTINSTRUCT
模型
Rouge
Commonsense VQA
ACC
Visual Entailment
Visual Spatial Reasoning
ACC
OFA
Max
49.9
32.0
56.6
32.0
55.3
32.1
OFA
Min
48.9
31.7
55.6
31.7
54.3
31.7
OFA
Mean
49.6
31.9
56.0
31.9
55.0
31.9
表1：在Multimodal Commonsense Reasoning上的零-shot性能。最佳性能以粗体显示。
模型
测试VQA
Grounded VQA
视觉文本提取
视觉对话
灾害类型分类
OFA
15.1
9.30 ± 5.42
0.02
176.3 ± 158.2
44.6 ± 28.71
9.68 ± 4.34
OFA
27.1
5.46 ± 0.83
0.00
74.3 ± 10.82
44.6 ± 28.71
9.68 ± 4.34
OFA
36.7
5.46 ± 0.83
0.00
74.3 ± 10.82
44.6 ± 28.71
9.68 ± 4.34
表2：在问题回答和微小错误上的零-shot性能。最佳性能以粗体显示。</sample>
    <sample id="528">图2展示了在多模态Commonsense Reasoning上的零-shot性能。表格中加粗的行表示最佳性能。</sample>
    <sample id="529">标题：增加多模任务群对性能的影响

要点：
- 图像理解（Img Und）
  - VQA + 图像理解
  - 定位
- MISC、ITM
  - 基于图像的匹配 + 图像生成
  - 时间排序 + 混杂 + 文本匹配
- 关系
  - 可视关系
- 区域
  - 区域理解
- NLP
  - NLP任务

右侧的图表展示了模型性能随时间变化的趋势。图表显示了两个曲线，一个代表“长尾”（Long-tail）任务，另一个代表“短尾”（Short-tail）任务。在“长尾”任务中，性能随着任务群的增加而显著提高，表明这些任务在增加任务群时具有更大的挑战性。相比之下，“短尾”任务的性能在增加任务群时略有下降，表明这些任务在增加任务群时具有较小的挑战性。

图例解释了图表中的不同线和标记：
- 绿色线代表“长尾”任务
- 蓝色线代表“短尾”任务
- 圆圈表示“基于图像的匹配 + 图像生成”任务
- 方块表示“VQA + 图像理解”任务
- 星形表示“区域理解”任务
- 箭头表示“NLP任务”
- “Rel.”表示“关系”任务
- “Region”表示“区域”任务
- “MISC”表示“其他”任务
- “ITM”表示“图像文本匹配”任务

图表底部的注释说明了图表的来源和目的，即“图3：随时间增加多模任务群的模型性能”。</sample>
    <sample id="530">效果不同的指令对指令调优的影响

- OFA在5种指令上进行微调，取得了在所有评估任务上的**更高聚合性能**，并显示了**较低的敏感性**。

下表展示了不同数量指令对OPA（Optimal Policy Adjustment）调优效果的影响：

| # of Instructions | Aggregated Performance ↑ | Sensitivity ↓ |
|-------------------|-----------------------------|---------------|
| 1 Instruction      | 26.4%                       | 10.45         |
| 5 Instructions    | 47.82                       |               |

图3：不同数量指令对OPA调优效果的影响。</sample>
    <sample id="531">效果
·在多任务上进行微调可以显著降低模型对OFA的敏感性。
·从自然指令数据集进一步微调模型可以进一步降低模型的敏感性。
图4：模型在未见过的评估任务上的敏感性。数值越低越好。</sample>
    <sample id="532">Zero-Shot Performance on NLP Tasks

- Instruction Tuning on MultiInstruct can improve zero-shot performance on unseen NLP tasks.
- The transfer learning strategy MixedInstruct can best preserve the zero-shot capability gained on Natural Instructions dataset.

模型 | Rouge-L
OPA | 2.25
OPA+MultiInstruct | 1.16
OPA+NaturalInstruct | 4.61
OPA+MultiInstruct+NaturalInstruct | 30.79

表4：在Rouge-L上的零样本性能。在Rouge-L上报告了NLP任务的性能，最佳性能为bold。

这个幻灯片讨论了使用混合Instruct策略在自然指令数据集上进行微调后，在NLP任务上的零样本性能。它展示了通过在MultiInstruct和NaturalInstruct上进行微调，模型性能显著提高。</sample>
    <sample id="533">结论

* 第一个大规模多模态指令调优数据集。
* 包含来自10个广泛类别的62个多种模态任务。
* 通过指令调优显著提高了OFAs的零-shot能力。
* 探索了几种转移学习技术并展示了它们的好处。
* 设计了一个新的度量灵敏度。</sample>
    <sample id="534">One More Thing! 我们正在收集一个更大规模的多模态指令调优数据集，其中包含大约150个额外的视觉语言任务，我们很快就会发布它们。</sample>
    <sample id="535">根据幻灯片上显示的英文内容，论文《注意力作为多任务语音翻译的指南》的作者是来自意大利特伦托大学的Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="536">演讲者的名字是Mohammad Javad Hosseini。</sample>
    <sample id="562">语言模型接受度判断并不总是对语境足够 robust ACL 2023 Johns Hopkins University Purdue University MIT</sample>
    <sample id="563">语言模型接受度判断并不总是对语境足够 robust ACL 2023 Johns Hopkins University Purdue University MIT Meta</sample>
    <sample id="564">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer ... has spent 1. Stereotypical 2. helping 2. any money 2. sentence 3. themselves. 3. "The customer ... spent 2. Non-stereotypical 4. We're helping 4. any money." 5. herself." sentence. P(1) &gt; P(2) P(1) &gt; P(2)any) P(1) &gt; P(2)</sample>
    <sample id="565">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer has spent 1. Stereotypical 2. helping themselves 2. The customer spent any money. 2. Non-stereotypical sentence. 3. Many people were helping herself. P(1) &gt; P(2) P(1,any) &gt; P(2,any) P(1) &gt; P(2)</sample>
    <sample id="566">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer has spent 1. Stereotypical helping themselves. 2. Many people were helping herself. any money. The customer spent any money. 2. Non-stereotypical sentence. P(1) &gt; P(2) P(1) &gt; P(2)any P(1) &gt; P(2)</sample>
    <sample id="567">revisiting minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer has spent 1. Stereotypical helping themselves. 2. "Many people were helping any money. 2. The customer spent herself." P(1) &gt; P(2) P(1) &gt; P(Any) P(1) &gt; P(2)</sample>
    <sample id="568">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer... has spent 1. Women are 2. Many people 2. The customer... spent any money 2. Men are terrible 3. We're helping 3. any money 3. at handwork. 4. herself. 4. at handwork. 5. 5. Men are terrible at handwork. Are these judgements stable with long preceding context?</sample>
    <sample id="569">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs. BLMP SyntaxGym CrowS 1. Many people were 1. No customer... has spent 1. Women are 2. Many people themselves. 2. The customer... spent any money. 2. Men are terrible. were helping 1. P(1.) &gt; P(2.) 1. P(1.any) &gt; P(2.any) 1. P(1.) &gt; P(2.) Are these judgements stable with long preceding context?</sample>
    <sample id="570">revisit ing minimal pair paradigm Minimal pair paradigm (MPP) evaluations of language models use relative differences in sequence probabilities to evaluate the abstract knowledge of LMs: BLMP SyntaxGym CrowS 1. Many people were 1. No customer... has spent 1. Women are 2. "Many people 2. The customer... spent any money 2. Handiwork, 3. We're helping 3. any money 3. Men are terrible, 4. herself." at handiwork. Are these judgements stable with long preceding context?</sample>
    <sample id="571">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：可接受的前缀 &gt; 不可接受的前缀？
可接受的前缀 &gt; 不可接受的前缀？
可接受的前缀 &gt; 不可接受的前缀？
空间中候选前缀
随机生成前缀
可接受的前缀
不可接受的前缀
BLUMP：调整
GPT2，OPT-125M，6.7</sample>
    <sample id="572">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试公式：主语一致
可接受的
P（接受|前缀）&gt; P（接受|前缀）
不可接受的
空间中候选项
空间中候选项
可接受的匹配
B（匹配）
GPT2，OPT家族-125M至6.7</sample>
    <sample id="573">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化
测试示例：主语一致
可接受的
P（接受|前缀）&gt; P（接受|前缀）
P（接受|前缀）&gt; P（接受|前缀）
空间中候选前缀
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受的
可接受</sample>
    <sample id="574">测试MPP判断是否随上下文长度、结构匹配和可接受性而变化。测试示例：可接受的结构匹配可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的可接受的</sample>
    <sample id="575">Approach
测试MPP判断是否随着上下文长度、结构匹配和可接受性而变化
测试公式：主语一致
可接受 &gt; 不可接受
空间中候选项前缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配后缀
匹配前缀
匹配</sample>
    <sample id="576">测试MPP判断是否随上下文长度、结构匹配和可接受性而变化。测试示例：主语一致性和名词短语结构匹配。可接受性：可接受、不匹配。问题：在返回之前，Rose从这个客户那里得到了多少钱？答案：没有迹象表明Rose在返回之前从客户那里得到了多少钱。有一个关于音乐入侵的纪录片。在那里有立法者工作吗？调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后：调整后</sample>
    <sample id="577">测试MPP判断是否随上下文长度、结构匹配和可接受性而变化。测试示例：主语一致、可接受的。P（可接受|前缀）&gt; P（可接受|前缀）。空间中候选前缀的范围。可接受、不匹配。谁可能会从这个客户那里收费？有一个关于音乐制作的纪录片。没有 legislatures 在工作，返回了？BLM：外部的“概率”量词 | 调整岛屿</sample>
    <sample id="578">Approach
测试MPP判断是否随上下文长度、结构匹配和可接受性而变化

测试示例：主语-宾语一致
可接受
P（主语前缀 &gt; 主语后缀）&gt; P（宾语前缀 &gt; 宾语后缀）

空间中的候选前缀
*表示主语，#表示宾语
* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *</sample>
    <sample id="579">Approach

测试MPP判断是否随上下文长度、结构匹配和可接受性而变化

测试公式：主语-宾语一致

P（可接受|前缀）&gt; P（可接受|前缀）

空间中候选前缀的集合

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受*

*可接受* *不可接受</sample>
    <sample id="580">Approach

测试MPP判断是否随上下文长度、结构匹配和可接受性而变化

测试公式：主语-谓语一致性
可接受？&gt;

空间中候选前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合
空间中前缀的集合</sample>
    <sample id="581">MPP judgments are robust for arbitrary context lengths

We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens.</sample>
    <sample id="582">MPP judgments are robust for arbitrary context lengths

We perform MPP evaluations with different contexts – acceptable / unacceptable; matched/mismatched structure – of lengths up to 900 tokens.</sample>
    <sample id="583">可接受/不可接受的MPP句子在语境中提高/降低判断

我们使用不同的语境进行MPP评估：可接受/可接受；匹配/不匹配结构；长度可达900个标记。

图中显示了不同前缀策略（匹配和不匹配）下的MPP得分。实线表示可接受的句子，虚线表示不可接受的句子。得分范围从-1到+1，其中+1表示句子完全可接受，-1表示句子完全不可接受。

图中还提供了关于文档中音乐提及的示例。这些示例展示了在不同语境下音乐提及的含义如何变化。例如，在匹配语境下，提及音乐可能暗示顾客之前已经提到过，而在不匹配语境下，提及音乐可能暗示顾客之前没有提到过。

总的来说，这个图表展示了在不同语境下音乐提及的含义如何变化，并且展示了MPP得分如何根据语境和句子结构的变化而变化。</sample>
    <sample id="584">可接受/不可接受的MPP句子在语境中提高/降低判断

我们使用不同语境进行MPP评估：可接受/可接受；匹配/不匹配结构；长度可达900个标记

图显示了不同前缀策略下的MPP性能，包括匹配和不匹配的结构。实线表示可接受的语境，虚线表示不可接受的语境。图中的数字1和2表示不同的文档示例。

图中展示的MPP性能表明，在匹配语境下，性能通常较高，而在不匹配语境下，性能较低。这表明语境对MPP评估结果有显著影响。</sample>
    <sample id="585">可接受/不可接受的MPP句子在语境中提高/降低判断

我们使用不同的语境进行MPP评估 - 可接受/可接受；匹配/不匹配结构 - 长度可达900个标记。

图中的曲线代表了不同策略下的MPP得分，包括“可接受/可接受”、“匹配/匹配”、“不匹配/匹配”和“WIKI/不匹配”。这些策略分别用不同的颜色表示。例如，“可接受/可接受”策略用绿色实线表示，“匹配/匹配”策略用橙色虚线表示，“不匹配/匹配”策略用蓝色虚线表示，“WIKI/不匹配”策略用紫色虚线表示。

每种策略下都有一个置信区间，用阴影区域表示。置信区间表示在多次评估中得分的可能范围。例如，“可接受/可接受”策略的置信区间大约在-0.2到-0.6之间。

图中还显示了两个示例句子，每个句子都有一个数字标签。这些句子用于说明MPP得分如何根据语境和结构的不同而变化。例如，数字“1”表示一个可接受的句子，数字“2”表示一个不可接受的句子。</sample>
    <sample id="586">可接受/不可接受的MPP句子与匹配结构最严重的影响</sample>
    <sample id="587">可接受/不可接受的MPP句子与匹配结构最严重的影响</sample>
    <sample id="588">可接受/不可接受的MPP句子与匹配结构最严重的影响</sample>
    <sample id="589">为什么匹配前缀会影响LM的判断？我们以保持相关结构不变的方式扰动上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀修饰语：然而，&lt;senta&gt;。- 长前缀修饰语：首先和 foremost，&lt;senta&gt;。- 添加前导语：*无论X对它说什么，&lt;senta&gt;。- 引用：*昨天，X说，‘&lt;senta&gt;’*</sample>
    <sample id="590">匹配前缀如何影响LM判断？我们以保持相关结构不变的方式篡改背景句子，并询问模型是否对这些句子同样敏感。-前缀/后缀从句：然而，&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span&gt;&lt;span style="font-weight: bold;"&gt;‘&lt;/span</sample>
    <sample id="591">为什么匹配前缀会影响LM的判断？我们以保持相关结构不变的方式对上下文句子进行扰动，并询问模型是否对这些扰动同样敏感。- 前后缀广告词：“然而，&lt;senta&gt;”，“长前缀广告词：”“无论&lt;sent&gt;，&lt;sent&gt;。”- 添加从句：“&lt;senta&gt;，&lt;senta&gt;。”- 引用：“昨天，X说，&lt;senta&gt;。”前缀类型可接受不可接受输入长度</sample>
    <sample id="592">为什么匹配前缀会影响LM的判断？我们以保持相关结构不变的方式扭曲上下文句子，并询问模型是否对这些句子同样敏感。-前缀/后缀广告词：然而，&lt;senta&gt;，长前缀广告词：&lt;senta&gt;。-添加从句：X认为什么，&lt;senta&gt;。-引用：昨天他这样说，&lt;senta&gt;。-方式：&lt;senta&gt;。输入长度与可接受和不可接受的前缀类型之间的敏感性</sample>
    <sample id="593">为什么匹配前缀会影响LM判断？我们以保持相关结构不变的方式扰动上下文句子，并询问模型是否对这些句子同样敏感。- 前缀/后缀广告词：然而，&lt;senta&gt;。- 首先前缀广告词：&lt;senta&gt;。- 添加从句：X认为什么X关于它说什么。- 引用：昨天他这样说，&lt;senta&gt;。- 一种方式：通过类似句子的敏感性来扰动句子输入长度</sample>
    <sample id="594">关键 takeaway

* 语言模型对潜在的句法/语义特征敏感，这些特征在句子中共享
* 使用短、单一的句子评估可以完全捕捉LMs的抽象知识
* 与完全句子输入相比，LMs对短、单一的句子输入的评估结果不同。

图表显示了测试集上的主题一致性评分。曲线代表了不同长度的前缀（从200到400）的LMs的性能。实线表示LMs的性能，而虚线表示LMs的性能。图表表明，LMs的性能随着前缀长度的增加而提高。</sample>
    <sample id="595">关键 takeaway

* 语言模型对潜在的句法/语义特征敏感，这些特征在句子之间共享
* 使用短、单一句子评估MPP不会完全捕捉LMs的抽象知识。

该幻灯片讨论了语言模型（LMs）处理和理解自然语言文本的方式。它强调了LMs对潜在的句法/语义特征的敏感性，这些特征在句子之间共享。这意味着LMs能够识别和利用文本中的一般模式和结构，而不仅仅是孤立的单词或短语。

幻灯片还提到了使用短、单一句子评估多模态性能（MPP）的重要性。MPP评估旨在衡量LMs在处理和理解复杂、多模态输入时的能力。然而，幻灯片表明，仅使用短、单一句子的评估方法可能无法充分捕捉LMs的抽象知识。这表明，为了更全面地了解LMs的能力，需要考虑它们在处理更复杂、多模态输入时的表现。

幻灯片底部的图表展示了不同测试集上的主题一致性评分。图表显示了不同测试集之间的评分变化，表明LMs在不同场景下的表现存在差异。这进一步强调了评估LMs能力时需要考虑多种因素的重要性。

最后，幻灯片底部的框图展示了LMs处理文本的流程。它展示了LMs如何从输入文本中提取特征，然后根据这些特征生成输出。这有助于理解LMs的工作原理以及它们如何利用潜在的句法/语义特征来理解和生成自然语言文本。</sample>
    <sample id="596">关键 takeaway

* 语言模型对潜在的句法/语义特征敏感，这些特征在句子中共享
* 使用短、单一的句子评估可以充分捕捉LMs的抽象知识
* 根据给定的前缀，LMs对不同后缀的偏好程度存在差异。</sample>
    <sample id="597">该方法的第一步将输入词元映射到'girl'类型的词元。</sample>
    <sample id="598">根据幻灯片中提供的信息，Coscript 包含了 55,000 个脚本。</sample>
    <sample id="626">根据表格中显示的英文内容，DEplain 的最佳对齐方法是使用 TF-IDF 相似的邻近驱动方法。</sample>
    <sample id="627">根据幻灯片中呈现的英文内容，弱监督学习的好处包括缓解标注瓶颈。这表明在标注数据以训练机器学习模型时，标注过程可能是一个限制性因素，而弱监督学习提供了一种方法来绕过这一限制，可能使用较少或更不精确的标注。</sample>
    <sample id="628">根据图片中显示的表格，DEplain-web 中的文档采用了手动和自动对齐方法进行对齐。手动对齐方法分配了 1231 个文档，而自动对齐方法分配了 1471 个文档。</sample>
    <sample id="629">CoNLL++ 数据集是通过将 CoNLL-2003 数据集的文件名从 .conll 格式更改为 .txt 格式，并在文件开头添加 'O' 来创建的。</sample>
    <sample id="630">跨语言语义解析：多种自然语言中的多语言和意义表示

于陈章，王俊，王智国，章锐</sample>
    <sample id="631">XSemPLR：多种自然语言的跨语言语义解析和意义表示

张宇森，王Jun，王智国，张锐</sample>
    <sample id="632">跨语言语义解析是一个任务，将多种自然语言中的查询翻译成多种意义表示。</sample>
    <sample id="633">跨语言语义解析是一个任务，将多种自然语言中的查询翻译成多种意义表示。</sample>
    <sample id="634">跨语言语义解析

* 现有的CLSLP模型分别提出并评估在有限任务和应用上。例如：
	+ 缺乏对某些自然语言的覆盖范围
	English | German | Chinese
	SQL | Lambda | FunQL</sample>
    <sample id="635">跨语言语义解析

* 现有的CLSP模型分别提出和评估在任务和应用有限的数据集上。例如：
	+ 缺乏某些自然语言的覆盖范围
	+ 英文
	+ 德文
	+ 中文
	+ 神经网络
	+ SQL
	+ Lambda
	+ FunQL</sample>
    <sample id="636">跨语言语义解析

* 现有的CLSLP模型分别被提出和评估在有限任务和应用的数据集上。例如：
	+ 缺乏对某些意义表示的覆盖范围</sample>
    <sample id="637">跨语言语义解析

* 现有的CLSLP模型分别提出并评估在有限任务和应用的数据集上。例如：
	+ 缺乏对某些意义表示的覆盖范围
	+ 英文
	+ 德文
	+ 中文
	+ 神经网络
	+ SQL
	+ Lambda
	+ FunQL</sample>
    <sample id="638">跨语言语义解析

* 现有的CLSP模型分别提出并评估在有限任务和应用上。例如：
	+ 缺乏对特定神经模型的覆盖
	+ 英文
	+ 德文
	+ 中国话
	+ 单一模型
	+ SQL
	+ Lambda
	+ FunQL</sample>
    <sample id="639">XSemPLR

我们提供一个统一的数据集XSemPLR用于跨语言的语义解析，它包含：

• 9个不同领域的数据集
• 5种不同的任务
• 22种自然语言，涵盖15种语言家族

图示例了XSemPLR的架构，其中包含一个编码器和解码器组件。</sample>
    <sample id="640">XSemPLR

我们提供一个统一的数据集XSemPLR用于跨语言语义解析和意义表示。它包含：

• 9个不同领域的数据集
• 5种语义任务
• 22种自然语言，涵盖15种语言家族

美国
日本
德国
意大利
中国

图中显示了XSemPLR的架构，包括编码器和解码器组件。</sample>
    <sample id="641">实验设置

* 我们考虑了六个用于训练和评估的设置。
	+ **Translate-Test**：使用Google Translate API将源语言翻译成目标语言。然后使用单语模型进行训练和评估。

**训练**

English
English Model
SQL

**Inference**

German
Translate API
English Model
SQL</sample>
    <sample id="642">实验设置

* 我们考虑了六个用于训练和评估的设置。
	+ Translate-Test：使用Google Translate API将源语言翻译成目标语言。然后使用单语模型进行训练和评估。

训练

* 英语
* 英语模型
* SQL

推理

* 德语
* 翻译API
* 英语
* 英语模型
* SQL</sample>
    <sample id="643">实验设置

* 我们考虑了六个训练和评估的设置。
	+ **Translate-Test**：使用Google Translate API将源语言翻译成目标语言。然后使用单语模型进行训练和评估。

**训练**

English -&gt; English Model -&gt; SQL
German -&gt; Translate API -&gt; English -&gt; English Model -&gt; SQL

**推断**

English -&gt; English Model -&gt; SQL</sample>
    <sample id="644">实验设置

* 我们考虑了六种训练和评估的设置。
* 例如，源语言与目标语言相同，如“德语到德语”所示。我们还测试了多语言零-shot设置，即使用仅10%的训练数据训练单语模型。
* 训练
	+ 德语（少样本）
	+ 德语模型
	+ SQL
* 推断
	+ 德语
	+ 德语模型
	+ SQL</sample>
    <sample id="645">实验设置

* 我们考虑了六种训练和评估的设置。
* 例如，源语言与目标语言相同，如德语到德语。我们还测试了单语多模态模型的零-shot设置，即使用仅10%的训练数据进行训练。

训练

* 德语（零-shot）
* 德语模型
* SQL

推理

* 德语
* 德语模型
* SQL</sample>
    <sample id="646">实验设置

• 我们考虑了六种训练和评估的设置。
• 源语言与目标语言相同，例如从德语到德语。我们还测试了单语多模态设置，即使用仅10%训练数据训练单语模型。

训练

| 德语（少示例） | 德语模型 | SQL |
| --- | --- | --- |
|  |  |  |

推理

| 德语 | 德语模型 | SQL |
| --- | --- | --- |
|  |  |  |

页脚：2023年6月19日</sample>
    <sample id="647">实验设置

* 我们考虑了训练和评估的六种设置。
* 多语言模型：为所有语言训练一个单一的多语言模型。

训练

推理</sample>
    <sample id="648">实验设置

* 我们考虑了训练和评估的六种设置。
* 多语言模型：为所有语言训练一个单一的多语言模型。

训练

推理</sample>
    <sample id="649">实验设置

* 我们考虑了训练和评估的六个设置。
* 多语言模型：为所有语言训练一个单一的多语言模型。

训练

推理</sample>
    <sample id="650">实验设置

* 我们考虑了六个训练和评估的设置。
* 跨语言零-shot/少-shot转移。在一种语言上进行训练，然后转移到另一种语言。

训练

| 英语 | 或 |
| --- | --- |
| 英语 | 德语 |
| 德语 | 少量模型 |
| 多语言模型 | SQL |

推理

| 德语 | 多语言模型 |
| --- | --- |
|  | SQL |</sample>
    <sample id="651">实验设置

* 我们考虑了六个训练和评估的设置。
* 跨语言零-shot/few-shot转移。在一种语言上进行训练，然后转移到另一种语言。

训练

| 英语 | 或 |
| --- | --- |
| 英语 | 德语 |
| 英语 | 微型模型 |
| 德语 | 多语言模型 |

推理

| 德语 | 多语言模型 |
| --- | --- |
|  | SQL |

页脚：Vineet Singh，计算机科学与工程系，印度技术学院，德里</sample>
    <sample id="652">我们发现多任务学习（MT5）在所有数据集上都取得了最佳性能！</sample>
    <sample id="653">我们发现mBART在所有数据集上获得最佳性能！</sample>
    <sample id="654">我们评估了两种模型组在单语环境下的性能：多语言预训练编码器与指针式解码器（Enc-PTR）和多语言预训练编码器与多任务学习（Enc-Dec）。我们发现，mBERT上的多任务学习（Enc-Dec）在所有数据集上都取得了最佳性能。</sample>
    <sample id="655">我们发现 Enc-Dec (mT5) 在所有数据集上都取得了最佳性能！</sample>
    <sample id="656">我们评估了在多语言设置下mT5和XLM-R+PTR的性能。通过在各种语言混合物中进行训练，可以改善多语言Enc-Dec-Enc-PTR的性能。</sample>
    <sample id="657">我们评估了mT5和XLM-R+PTR在多语言设置下的性能，发现Enc-Dec-Enc-PTR（mT5-XLM-R）可以通过在多种语言的混合语料库上进行训练来提高。</sample>
    <sample id="658">分析多语言训练

我们评估了mT5和XLM-R + PTR在多语言设置上的性能。大多数主要的NL任务取得了性能提升，除了在7个数据集中英语性能下降外。这被称为“多语言 curse”。</sample>
    <sample id="659">分析多语言训练

我们评估了mT5和XLM-R + PTR在多语言设置下的性能。大多数主要的NL任务可以取得性能提升，除了在7个数据集中英语性能下降，这被称为“多语言 curse”。在x轴上显示了不同自然语言，y轴表示数据集的数量。

注：由于图片中的图表是用柱状图表示的，所以无法将它翻译成文字描述。</sample>
    <sample id="660">跨语言性能差距
蓝色线条：跨语言少样本转移
橙色线条：跨语言零样本转移
绿色线条：单语环境
地理查询
MTP
SchemaQA
过夜
NLMaps
MCWQ
地理/函数
地理/prolog
地理/sql
Spider
ATIS
少样本
零样本
单语</sample>
    <sample id="661">跨语言性能差距
蓝色线条：跨语言少样本转移
橙色线条：跨语言零样本转移
绿色线条：单语环境
地理查询
MTP
SchemaQA
过夜
NLMaps
MCWQ
地理/编程
地理/lambda
地理/SQL</sample>
    <sample id="662">跨语言性能差距
蓝色线：跨语言少样本转移
橙色线：跨语言零样本转移
绿色线：单语环境
地理查询
MTOP
SchemaQA
Overnight
NLMaps
MCWQ
Geoplay/funql
Geoplay/prolog
Geoplay/sql
Spider
ATIS
Few-shot
Zero-shot
Monolingual</sample>
    <sample id="663">其他结果与发现（论文第4节）

- Enc-Dec (m75) 在之前的工作中表现出色，或者在目标NL上取得可比的结果。
- 在目标NL上进行预训练可以显著提高多语言LLMs（如Codex和BLOOM）的表现。
- 多语言LLMs（如Codex和BLOOM）对于跨语言解析仍然不够充分。
- 从中文到英文的迁移学习和单语英文训练（En -&gt; En）具有最大的性能差距，而德文通常具有最小的性能差距。
- FunQL在其他三种意义表示中表现最佳，而SQL在性能上最差。</sample>
    <sample id="664">其他结果和发现（论文第4部分）

- Enc-Dec (m75) 在之前的成果上表现出色，或者在目标NL上取得了相当好的成绩。
- 在English NL上预训练可以显著提高多语言LLMs（如Codex和BLOOM）的表现。
- 多语言LLMs（如Codex和BLOOM）对于跨语言解析仍然不够充分。
- 从Chinese到English的迁移学习和English单语训练（En -&gt; En）具有最大的性能差距，而German通常具有最小的性能差距。
- FunQL在其他三种意义表示中表现最佳，而SQL通常表现最差。</sample>
    <sample id="665">结论
- 我们构建了XSemPLR，一种用于多语言和意义表示的跨语言语义解析的统一基准。
- 我们对三种代表性多语言语言模型进行了全面基准测试。
- 结果显示，在单语训练下，mT5的表现最佳。然而，值得注意的是，动态解析任务中，多语言LLMs仍然不足以进行跨语言解析。此外，跨语言训练和跨语言迁移学习仍然很重要。</sample>
    <sample id="666">结论
- 我们构建了XSemPLR，一种用于多语言和意义表示的跨语言语义解析的统一基准。
- 我们对三种代表性多语言语言模型进行了全面基准测试。
- 结果表明，mT5在单语训练下具有最佳性能，而显著的多语言LLMs仍然不足以进行跨语言句法解析任务。此外，跨语言训练和跨语言迁移学习仍然很重要。</sample>
    <sample id="667">关于水印技术的现有研究包括参数化水印、字典水印、后门水印和对抗性水印。这些技术被用于保护深度神经网络的知识产权，通过使用频率域方法来增强信任和安全。</sample>
    <sample id="668">根据幻灯片中提供的信息，Codex 和 Bloom 等多语言 LLMs 对于 CLSP（跨语言解析）来说是不够的。</sample>
    <sample id="695">根据图片，该方法似乎通过在每个输入元素（'girl'、'x1'、'sleep'）下方使用一个标签来处理排列的不确定性。这表明系统尝试为每个输入分配一个标签，尽管排列是未知的。这可能意味着系统正在尝试通过将输入与给定标签进行匹配来解决排列问题，或者它正在学习如何根据上下文或额外信息来确定正确的排列。</sample>
    <sample id="696">根据图片中提供的信息，对于下游 NLP 模型的公平性定义为：'模型在不同政治背景下的性能保持一致。' 这意味着模型应该能够以相似的准确性和偏见处理来自不同政治背景的数据，无论数据的来源或内容如何。</sample>
    <sample id="697">演讲者的名字是Yanis Labarre。</sample>
    <sample id="698">演讲者的名字是Koustuv Sinha。</sample>
    <sample id="699">演讲者的名字是Myra Cheng。</sample>
    <sample id="700">在本文的背景下，热带主义 (tropicalism) 指的是通过强调文化、传统和身份来对某些群体进行刻板印象化的过程。 这种刻板印象化将这些群体仅定义为他们的身份，而没有考虑他们更广泛的人性和经验。</sample>
    <sample id="701">根据幻灯片中呈现的英文内容，作者通过使用描述性词语来创建目标群体的人工描写。这些词语包括“文化”、“传统”、“自豪”和“异国情调”，它们被用来定义那些被标记为具有特定身份的群体，仅基于他们的身份。此外，还使用了诸如“鲜艳”、“玲珑”、“精致”和“坚强”等词语，这些词语用来描绘女性，暗示着一种刻板印象或偏见的视角。</sample>
    <sample id="702">在文本中提到的用来衡量语境使用情况的方法是P-CXMI。</sample>
    <sample id="703">根据幻灯片中提供的信息，DrBERT 和 ChuBERT 都是用于医学文本的预训练模型。然而，它们之间的区别在于训练策略上。DrBERT 是从头开始构建的，这意味着它是在没有任何预训练权重的情况下训练的。相比之下，ChuBERT 使用现有的预训练模型进行微调，表明它利用了之前学习到的知识来进一步训练。</sample>
    <sample id="751">根据图片显示，这篇论文有三位作者：Zhongyang Xu、Ying Shen和Lifu Huang。</sample>
    <sample id="752">迭代迁移学习（Iterative Transfer Learning，ITL）是一种机器学习方法，其中模型在每次迭代中使用旧数据和新数据进行微调或更新。这个过程通常涉及将模型应用于一个初始任务，然后在每次迭代中使用新收集的数据来改进模型。这种方法允许模型在保持对原始任务的性能的同时，学习并适应新的、相关的任务。迭代迁移学习通常用于处理数据分布随时间变化的情况，或者当新数据以持续流的方式到达时。</sample>
    <sample id="753">数据集的目标是解决间接引用表达，具体是在实体选择任务中。这可以从幻灯片底部的注释“Resolving Indirect Expressions by Entity Selection (Mnli Corpus)”推断出来。Mnli Corpus是一个用于自然语言理解任务的数据集，包括文本对和它们之间的关系。因此，数据集的目标很可能是开发算法或系统能够识别和解释用户在选择实体时使用的间接引用表达，如“更容易的那首”或“那首不那么有活力的”。</sample>
    <sample id="754">攻击者通过EaaS来提取模型参数，通过在EaaS上训练自己的模型来实现。这使攻击者能够利用EaaS的计算资源和数据存储能力，以比直接在本地设备上训练模型更快、更有效地训练模型。</sample>
    <sample id="755">根据幻灯片上显示的英文内容，这篇论文有三位作者：Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="756">根据幻灯片中提供的信息，用于创建初始数据集的注释者数量是6个。这可以从图表下方的注释者的列表中看到，其中明确列出了参与创建初始数据集的注释者的名字。</sample>
    <sample id="757">根据图片中提供的英文内容，这篇论文的作者分别来自以下机构：Sebastin Sanyi和Katharina Reinecke来自University of Washington，Jenny T. Liang和Maarten Sap来自Carnegie Mellon University，Ronan Le Bras来自Allen Institute for AI。</sample>
    <sample id="758">根据幻灯片中提供的信息，以左侧为支配词的示例是“[我看见]巴特和Lisa霍默来[并打喷嚏]。”</sample>
    <sample id="759">对话系统中的最先进模型通常指的是那些在性能指标上表现最好的模型，比如准确率、流畅性或用户满意度。这些模型可能使用了最新的技术，如深度学习、大规模预训练和先进的自然语言处理算法。它们旨在通过理解用户的意图、提供相关和个性化的回应以及保持对话的连贯性来提供无缝的用户体验。然而，由于对话系统是一个广泛的领域，最先进模型的具体定义可能会根据应用、数据集和评估标准的不同而有所不同。</sample>
    <sample id="760">在评估模型的可接受性时，需要在整个上下文窗口中进行评估，以确保模型能够理解并正确解释语言的微妙之处。如示例所示，单独考虑每个句子可能会导致对模型性能的误导性评估。例如，在第一个示例中，虽然“帮助”一词在第一个句子中出现了多次，但在第二个句子中仅出现了一次，这可能会影响模型对“帮助”一词重要性的评估。同样，在第三个示例中，虽然“ terrible”一词在第二个句子中出现了一次，但在第一个句子中没有出现，这可能会影响模型对“ terrible”一词重要性的评估。因此，需要在整个上下文窗口中评估模型的可接受性，以确保模型能够理解并正确解释语言的微妙之处。</sample>
    <sample id="761">根据幻灯片中呈现的英文内容，与单语英语模型相比，多语言训练会导致表现下降。</sample>
    <sample id="762">注释者是否提前知道该实体？根据幻灯片中提供的信息，注释者被要求至少听每首歌一点并阅读关于每首歌的信息。这表明注释者在开始任务之前并没有提前知道这些实体（歌曲）；他们需要通过实际收听和阅读来获取信息。</sample>
    <sample id="763">根据给定的英文内容，评估了BLEU和METEOR两个MT指标。这些指标通常用于评估机器翻译的质量，通过比较机器翻译与参考翻译之间的相似度来衡量。BLEU（Bilingual Evaluation Understudy）得分是通过计算n-gram匹配来评估翻译准确性的指标，而METEOR（Metric for Evaluation of Translation with Explicit ORdering）得分则基于词汇表中的术语匹配和术语重排来评估翻译质量。</sample>
    <sample id="764">根据图片中提供的信息，回归任务与 NER（命名实体识别）任务是不同的。回归任务通常涉及预测连续数值输出，而 NER 任务涉及从文本中识别和分类不同类型的实体，如人名、地名或组织名称。回归任务可能需要不同的模型架构和参数调整，以优化连续值的预测，而不是分类任务。因此，回归任务可能不会直接影响特定的 NER 类型，因为它们解决的是不同的问题。然而，泛化性能的一般原则，如模型大小和架构，仍然适用于回归任务。</sample>
    <sample id="765">在自然语言处理（NLP）中，立场很重要，因为它涉及理解文本的情感或态度，这对于各种应用至关重要。例如，在情感分析中，确定文本是正面、负面还是中性对于品牌监控、市场研究和内容推荐至关重要。立场分析还用于文本分类任务，如垃圾邮件检测或情感分类，其中识别文本的情绪有助于准确分类。此外，立场分析有助于生成更个性化和相关的用户体验，因为系统可以调整其回应以与用户的情感相匹配。</sample>
    <sample id="766">根据幻灯片中关于多语言LLM的描述，像BLOOM这样的多语言LLM被描述为“使用适配器微调”，这表明它们是在特定任务上进行微调的，而不是在所有任务上进行完整的模型训练。因此，像BLOOM这样的多语言LLM是采用适配器微调的。</sample>
    <sample id="767">根据幻灯片中提供的英文内容，他们使用RoBERTa-base模型进行迁移学习。这可以从幻灯片顶部的图表中看到，其中明确标注了“RoBERTa-base”作为模型名称。</sample>
    <sample id="768">根据图片中提供的英文内容，最近用于评估 PaLM 能力的测试集包括：1. 机器翻译（Machine Translation）2. 通用问答（General Question Answering）3. 通用阅读理解（General Reading Comprehension）4. 通用文本生成（General Text Generation）5. 通用对话（General Dialogue）6. 通用文本摘要（General Text Summarization）7. 通用情感分析（General Sentiment Analysis）8. 通用问答（General Question Answering）9. 通用阅读理解（General Reading Comprehension）10. 通用文本生成（General Text Generation）11. 通用对话（General Dialogue）12. 通用文本摘要（General Text Summarization）13. 通用情感分析（General Sentiment Analysis）</sample>
    <sample id="769">根据幻灯片上可见的英文内容，作者提出了两条建议。这些建议包括“解决积极刻板印象和本质化叙事”和“关于偏见缓解的透明度”。</sample>
    <sample id="770">根据幻灯片中提供的信息，与最强的基线相比，所建议的方法获得了10.2%的收益。这个百分比是在饼图下方的表格中给出的，其中显示了“与最强的基线相比的收益”为10.2%。</sample>
    <sample id="771">演讲者的名字是“Shuheng Liu”，如幻灯片左下角的圆形头像旁所示。</sample>
    <sample id="772">是的，论文中的结果和数据集可以作为基准。</sample>
    <sample id="773">在论文中进行了四个较小模型的实验。这些模型包括GPT-3 (175B)、Codex (175B)、InstructGPT (175B)和T5训练在wikiHow上。</sample>
    <sample id="774">根据幻灯片中提供的英文内容，被用作研究多模型指令调整的基础模型是“OFAs (One For All)”。</sample>
    <sample id="833">根据幻灯片上显示的英文内容，无法确定论文作者所属的机构。幻灯片中没有提供任何关于机构的信息；它只列出了作者的名字和一个会议名称“ACL 2023”，这表明该研究是在 ACL（国际计算语言学会）2023年会议上发表或展示的。要找到作者的机构信息，通常需要查阅论文本身或相关的学术出版物数据库。</sample>
    <sample id="834">根据幻灯片中显示的英文内容，这篇论文的作者所属机构是圣布鲁克大学的人类语言分析系。</sample>
    <sample id="835">根据幻灯片中提供的信息，该论文分析了多种语言对。它提到了“德-英”和“法-英”这对语言对，并且还提到了其他一些语言对，如“阿拉伯语-英语”，“西班牙语-英语”，“意大利语-英语”，“荷兰语-英语”，“葡萄牙语-英语”，“俄语-英语”，“日语-英语”，“韩语-英语”，“中文-英语”，“阿拉伯语-西班牙语”，“西班牙语-法语”，“阿拉伯语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙语-荷兰语”，“西班牙语-意大利语”，“西班牙</sample>
    <sample id="836">演讲者的名字是Yulia Tsvetkov。</sample>
    <sample id="837">在实验过程中研究了以下模型：SARI、BLEU和BS-P。这些评估指标用于衡量文本简化任务中不同模型的性能，如表格中所示。</sample>
    <sample id="838">根据幻灯片中提供的信息，MultiInstruct 中使用的 62 个不同任务中，有 53 个任务用于训练，另外 9 个任务用于测试。</sample>
    <sample id="839">根据幻灯片上显示的英文内容，这篇论文有三位作者：Regina Stodden、Omar Momen和Laura Kallmeyer。</sample>
    <sample id="840">作者在实验中使用了以下数据集：AG News、MIND、SST2和Enron Spam。</sample>
    <sample id="876">根据幻灯片中提供的信息，NACHOS似乎是一种预训练策略或模型，因为它被单独列出进行讨论。它与DrBERT一起被提及，这表明它们可能是在比较预训练策略、数据来源和大小时考虑的模型。然而，仅凭这张图片本身无法确定NACHOS的确切性质或功能。要了解其具体细节，需要额外的背景或信息。</sample>
    <sample id="877">演讲者的名字是David Vil Torres、Markus Freitag、Colin Cherry、Jiaming Luo、Virendra Ratnaker和George Foster。这些名字位于幻灯片底部的六张照片下面，对应着他们的个人头像。</sample>
    <sample id="878">根据幻灯片中提供的信息，提示策略对翻译质量的影响相当大。它表明选择不同的随机提示可以导致BLEURT得分差异显著，范围从1到40个BLEURT点。这表明提示的选择可以显著影响翻译评估的质量，可能影响翻译的准确性和质量。</sample>
    <sample id="879">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是卡内基梅隆大学语言技术研究所、葡萄牙技术大学和伯克利人工智能研究实验室。</sample>
    <sample id="880">根据图片中提供的信息，无法确定由专家编写的5个指令。图片中并未提供任何与指令相关的细节或示例。它只是提到了一个更大的多模态指令调优数据集正在收集中，并且包含大约150个额外的视觉语言任务。要了解专家编写的指令，需要额外的上下文或数据集本身。</sample>
    <sample id="881">根据幻灯片上提供的英文内容，作者建议通过使用人类研究参与者和核心ference模型来测试模型。这表明了对模型性能的双重评估：一种是通过人类参与者进行主观评估，另一种是通过核心ference模型进行客观评估。</sample>
    <sample id="882">Google
Prompting PaLM for Translation
Assessing Strategies and Performance
David Vil Torres Markus Frettag Colin Cherry Jaming Luo Vineer Ratnaker George Foster
ACL 2023</sample>
    <sample id="883">PaLM：Pathways Language Model
· Chowdery et al., 2022 arXiv:2204.02311.
· 540B参数。
· 训练了780B tokens。
· 密集激活。
· 使用TPU v4芯片的6144个TPU和生成 benchmarks。
· SOTA在数十亿参数中。</sample>
    <sample id="884">PaLM：Pathways Language Model
- Chowdery et al., 2022
- arXiv:2204.02311.
- 540B参数。
- 训练了780B tokens。
- 密集激活。
- 6144 TPU v4芯片。
- 在TPU和生成基准上达到SOTA。
- 620亿参数。
Google</sample>
    <sample id="885">我们的贡献
- 对MT的LLM提示进行了首次系统研究。
- 评估了选择策略，包括候选人池和最佳实践。
- 评估翻译能力与MT社区的最佳实践：
- 最新测试集（避免测试/训练重叠和最近培训数据上的过拟合）。
- 比较最新WMT提交（使用最新培训数据的SOTA系统）。
- 基于专家的人类评估（比群众工作者更 robust）。
- 推荐提示选择策略。</sample>
    <sample id="886">我们的贡献

* 对MT领域的LLM提示进行了首次系统性研究。
* 评估了候选人池和选择策略。
* 使用MT社区的最佳实践评估翻译能力：
	+ 最新的测试集（避免测试/训练重叠和最近的训练数据上的过拟合）。
	+ 比较最近的WMT提交（使用最新训练数据）。
	+ 使用专家的人类评估（比群众工人的评估更 robust）。
* 推荐提示选择策略。</sample>
    <sample id="887">我们的贡献
- 对MT的LLM提示进行了首次系统研究。
- 评估了候选池和选择策略。
- 用MT社区的最佳实践评估翻译能力：
- 避免测试/训练重叠和最近的WMT提交（使用最新培训数据）。
- 最先进的MT指标（与人类判断的相关性更好）。
- 专家基于人类评估（比众包工人的评估更 robust）。
- 推荐提示选择策略。</sample>
    <sample id="888">我们的贡献
- 对LLM提示用于MT的系统研究。
- 评估了候选人池以及选择策略。
- 使用MT社区的最佳实践评估翻译能力：
- 最新的测试集（避免测试/训练重叠和基于最新训练数据的过拟合）。
- 比较最近的WMT提交（使用最新训练数据的SOTA系统）。
- 基于专家的人类评估（比众包工人的评估更 robust）。
- 推荐提示选择策略。</sample>
    <sample id="889">Prompt对翻译质量有很大影响

* 为每个句子选择两个随机提示。
* 计算每个句子-提示对的BLEURT分数。
* 大多数句子（516个句子，占1000个句子的大部分）的分数差超过1个BLEURT点。
* 分数差可达到40个BLEURT点！</sample>
    <sample id="890">Prompt 对翻译质量有重大影响

* 为每个句子选择两个随机提示。
* 计算每个句子-提示对的 BLEURT 分数。
* 大多数句子（516/1000）显示得分差超过 1 个 BLEURT 分点。
* 分差可达到 40 个 BLEURT 分点！</sample>
    <sample id="891">Prompt对翻译质量有重大影响

* 为每个句子选择两个随机提示。
* 计算每个句子-提示对的BLEURT分数。
* 大多数句子（516个句子中的516个）显示得分差超过1个BLEURT点。
* 分数差可达到40个BLEURT点！</sample>
    <sample id="892">示例翻译提示</sample>
    <sample id="893">示例提示用于翻译</sample>
    <sample id="894">示例提示用于翻译</sample>
    <sample id="895">示例提示用于翻译</sample>
    <sample id="896">示例提示用于翻译</sample>
    <sample id="897">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

PaLM的流畅度与SOTA相当。准确率一般较低。主要由“准确性/遗漏”和“风格/笨拙”较低而主导。</sample>
    <sample id="898">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性得分一般较低。
    * 主要由“准确性/遗漏”决定。
    * “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="899">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性得分一般较低。
	+ 主要由“准确性/省略”决定。
	+ “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="900">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MGM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/省略”决定。
    * “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="901">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * “准确/省略”占主导地位。
    * “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="902">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/省略”主导。
    * “风格/笨拙”一般较低，对于PaLM而言。</sample>
    <sample id="903">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQM中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/省略”决定。
    * “风格/笨拙”一般较低，对PaLM而言。</sample>
    <sample id="904">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PaLM接近Google Translate。

从MQL中获得的见解

* PaLM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要受“准确性/遗漏”影响。
* “风格/笨拙”一般较低，对于PaLM来说。</sample>
    <sample id="905">实验结果

* 示例质量比源句子的相似性更重要。
* 专门化的SOTA系统具有显著优势。
* PALM接近Google Translate。

从MGQ中获得的见解

* PALM的流畅度与SOTA相当。
* 准确性一般较低。
    * 主要由“准确性/遗漏”决定。
* “风格/笨拙”一般较低，对于PALM而言。</sample>
    <sample id="906">谢谢</sample>
    <sample id="907">Saarland University

LST

Department of Language and Technology Saarland University

Universität Wien

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna</sample>
    <sample id="908">Saarland University

LST

Department of Language Science and Technology

Saarland University

Universität Wien

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna

Saarland University

Amazon Alexa

University of Vienna</sample>
    <sample id="909">Why weakly supervised learning?
Weak supervision alleviates the annotation bottleneck.
Weak labeling sources (e.g., heuristics, knowledge bases)
Weak labels are noisy!
Noise memorization harms generalization.
Unlabeled data
Weakly supervised learning (WSL)
Train models that generalize well despite being trained on noisy data</sample>
    <sample id="910">为什么弱监督学习？ 弱监督缓解了标注瓶颈。 但是，噪声标签会损害泛化能力。 弱监督学习（WSL） 训练模型在噪声数据上泛化良好。</sample>
    <sample id="911">Why weakly supervised learning?
Weak supervision alleviates the annotation bottleneck.
但弱标签是noisy！
Noisy memorization harms generalization.
Weakly supervised learning (WSL)
Train models that generalize well despite being trained on noisy data</sample>
    <sample id="912">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="913">Why weakly supervised learning?
Weak supervision alleviates the annotation bottleneck.
但有噪声的标签！
Noise memorization harms generalization。
Weakly supervised learning (WSL)
Train models that generalize well despite being trained on noisy data</sample>
    <sample id="914">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%."

Weakly labeled training (noisy)

Cleanly labeled test data (clean)</sample>
    <sample id="915">A common claim in recent WSL works

“We train models only on weakly supervised data and achieve an accuracy of XX%.”

Weakly labeled training data (noisy)</sample>
    <sample id="916">A common claim in recent WSL works

“We train models only on weakly supervised data and achieve an accuracy of XX%”</sample>
    <sample id="917">A common claim in recent WSL works

"We train models only on weakly supervised data and achieve an accuracy of XX%"

Weakly labeled training data (noisy)

Cleanly labeled validation data (clean)

Cleanly labeled test data (clean)</sample>
    <sample id="918">我们的研究问题

RQ1：是否需要干净的验证数据？

RQ2：WSL方法需要多少干净样本？

RQ3：如何更有效地利用现有的干净样本？</sample>
    <sample id="919">Our research questions</sample>
    <sample id="920">RQ1 Main findings Validation on weak labels No validation on validation on weak labels FT_w BOND COSINE MLC L2R</sample>
    <sample id="921">RQ1 Main findings

Validation on weak labels Validation on validation set No validation Validation on clean labels

30 20 10 0 -10 -20 FT_w, BOND COSINE MLC L2R</sample>
    <sample id="922">RQ1: Main findings

验证在弱标签上的表现改进（%）

No Validation
Validation on Validation on
Weak Labels (Random Selection) Clean Labels

FT_w
BOND
COSINE
MLC
L2R</sample>
    <sample id="923">RQ1: Main findings

Validation on weak labels Validation on Clean labels No validation (Random Selection)

FT_w. BOND COSINE MLC L2R</sample>
    <sample id="924">RQ1 Main findings

Validation on weak labels
No validation
Validation on clean labels</sample>
    <sample id="925">RQ2 Main findings</sample>
    <sample id="926">RQ2 Main findings</sample>
    <sample id="927">RQ2 Main findings FTw COSINE L2R BAND MLC Weak labels 85 80 75 70 60 5.10 20.30 40 50 All Validation WSL approaches benefit from more clean validation samples!</sample>
    <sample id="928">RQ2 Main findings

WSL approaches benefit from more clean validation samples!

Performance Deltas (%)</sample>
    <sample id="929">RQ2 Main findings

WSL approaches benefit from more clean validation samples!

Performance Deltas (%)</sample>
    <sample id="930">RQ3 Main findings N=10 clean samples per class N=30 clean samples per class Before CFT After CFT COSINE ER FT+ER Clean Only</sample>
    <sample id="931">RQ3 Main findings N=10 clean samples per class N=30 samples per class Before CFT After CFT Accuracy/F1 Accuracy/F1 88 90 86 84 82 COSINE ER FT+ 80 78 76 74 72 70 68 66 64 62 60 Before CFT After CFT</sample>
    <sample id="932">RQ3 Main findings N=10 clean samples per class N=30 samples per class Before CFT After CFT Before CFT After CFT</sample>
    <sample id="933">RQ3 Main findings N=10 clean samples Before After Accuracy/F1 Accuracy/F1 Continuous fine-tuning (CFT) eliminates performance gaps between WSL approaches. No need to use complicated WSL methods FT performs equally well.</sample>
    <sample id="934">结论
最近的WSL方法
需要干净样本。


过度估计它们的实用性。


我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="935">结论
最近的WSL方法
需要干净样本。
高估它们的实用性。
我们推荐
报告模型选择标准。
使用少样本学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="936">结论
最近的WSL方法
需要干净的样本。


过度估计它们的实用性。


我们的建议
报告模型选择标准。
使用少-shot学习方法作为基线。
总是应用连续微调（CFT）。</sample>
    <sample id="937">结论
最近的WSL方法
需要干净样本。
高估其实用性。
我们的建议
报告模型选择标准。
使用少样本学习方法作为基线。
始终应用连续微调（CFT）。</sample>
    <sample id="938">结论

最近的WSL方法
- 需要干净样本。
- 高估了它们的实用性。

我们的建议
- 报告模型选择标准。
- 使用少样本学习方法作为基线。
- 总是应用连续微调（CFT）。</sample>
    <sample id="939">图片展示了一个比较对话系统评估的框架，其中涉及两个机器人角色之间的对话。每个机器人角色都有一个带有面孔的图标，并且它们之间有对话气泡连接，表明它们之间的互动。这种设置通常用于评估对话系统的性能，特别是它们在进行自然语言理解和生成方面的能力。因此，对话系统的常用评估方法包括比较它们在对话中的表现，如理解用户的意图、提供相关和连贯的回应以及保持对话的上下文。</sample>
    <sample id="940">根据幻灯片显示，这篇论文有五位作者。这可以通过在标题下方列出的五位个人的名字和照片来确定。</sample>
    <sample id="941">根据所给的英文内容，需要了解Servin的职业背景，即他是一名法官，以及Kea的职业背景，即她是一名面包师。此外，还需要理解Servin和Kea在公园相遇的情景，以及Servin在一天工作结束后感到高兴放松的情绪。这些背景知识有助于更好地理解故事的上下文，并可能影响对问题“Servin是做什么的？”的答案。</sample>
    <sample id="942">代码是公开的，可以在GitHub上获取，链接为“mpoems/kitmus”。</sample>
    <sample id="943">根据图片中提供的信息，无法确定 NLPositionality 的注释者在人口统计学特征方面是否均衡。该图表没有提供关于人口统计学特征的任何数据，如国家/地区、性别或年龄组。它只显示了与不同教育水平相关的数据。因此，对于 NLPositionality 的注释者在人口统计学特征方面的均衡性的问题，答案是：(a) 无法确定。</sample>
    <sample id="944">根据幻灯片中提供的英文内容，句子可以通过以下方式扰乱：1. 添加前缀/后缀修饰语，如“然而，&lt;句子&gt;。”2. 使用长前缀修饰语，如“&lt;句子&gt;。首先和 foremost。”3. 在开头添加“无论&lt;句子&gt;。”4. 添加“&lt;句子&gt;。昨天，X说。”5. 使用引号“&lt;句子&gt;。”</sample>
    <sample id="945">进行维度评估意味着对一个概念或主题的各个方面或方面进行系统化的审查和分类。这通常涉及识别和分析一系列因素，以全面了解问题、情况或对象。维度评估可以用于各种应用，如研究、项目管理、决策制定和质量保证。它有助于提供更深入的理解，并确保考虑所有相关方面。</sample>
    <sample id="946">根据图片中提供的英文内容，这篇论文的作者所属机构包括中国科学技术大学、北京航空航天大学和微软亚洲研究实验室。</sample>
    <sample id="947">在翻译任务中，提示的形式很重要，因为它们可以影响模型生成准确和相关的翻译的能力。清晰、简洁的提示有助于模型理解需要翻译的文本的上下文和含义。此外，使用适当的格式（如使用分号或逗号分隔多个翻译示例）可以帮助模型学习不同语言之间的对应关系。</sample>
    <sample id="978">根据图片中显示的英文内容，作者评估了以下对话模型：ABC-Eval、Turn Likert和Dialogue Likert。这些模型在图表上以不同的曲线表示，表明它们在解释的质量方面的表现不同。</sample>
    <sample id="979">根据图片中显示的英文内容，这篇论文有九位作者。</sample>
    <sample id="980">根据幻灯片中提供的英文内容，优秀规划器的理想品质是能够处理具有多方面约束的特定目标。这表明优秀规划器应该能够同时考虑多个因素或限制，以实现一个明确的目标。</sample>
    <sample id="981">根据幻灯片上显示的英文内容，这篇论文有九位作者。</sample>
    <sample id="982">演讲者的名字是Vasudha Varadarajan。</sample>
    <sample id="983">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是“INSTITUTE OF COMPUTER SCIENCE”和“UNIVERSITY OF WARSAW”。这些机构位于华沙，具体地址为ul. Jana Kazimierzawa 5, 01-248 Warsaw。</sample>
    <sample id="1021">根据幻灯片中提供的信息，PaLM 最常见的错误是“遗漏/遗漏”。</sample>
    <sample id="1022">不要忘记你的ABC：评估聊天导向对话系统中的前沿技术 Sarah E. Finch，James D. Finch和Jinho D. Choi EMORY大学 亚历山大研究实验室</sample>
    <sample id="1023">不要忘记你的ABC：评估聊天导向对话系统中的前沿技术 Sarah E. Finch，James D. Finch和Jinho D. Choi EMORY大学 亚历山大研究实验室</sample>
    <sample id="1024">别忘了你的ABC：评估聊天导向对话系统中的前沿技术 Sarah E. Finch，James D. Finch和Chinho D. Choi Emory University 亚历山大研究实验室</sample>
    <sample id="1025">Comparative Evaluation</sample>
    <sample id="1026">Likert Rating Evaluation</sample>
    <sample id="1027">对话质量维度</sample>
    <sample id="1028">Likert Rating Evaluation

将图片中的英文内容翻译成中文：

 Likert Rating Evaluation

左侧有一个卡通形象，穿着法衣，手持法槌，代表法律或裁判角色。右侧有两个机器人头像，一个黑色，另一个蓝色，它们正在进行对话，黑色机器人有气泡状的对话框，蓝色机器人也有气泡状的对话框。

下方有一条从1到5的评分尺度，其中第5格被勾选，表示评分结果。评分尺度下方的文字说明：“Rate the relevance of the bot’s responses”（评估机器人的回复的相关性）。

左下角有一个标志，写着“Emory University”（埃默里大学），右下角有一个标志，写着“Alexa”（亚马逊Alexa）。</sample>
    <sample id="1029">Likert Rating Evaluation Chat (ABC-Eval)</sample>
    <sample id="1030">标注聊天中的行为（ABC-Eval）</sample>
    <sample id="1031">ABC-Eval 行为

- 凝聚性
- 知识
- 一致性
- 情感理解</sample>
    <sample id="1032">ABC-Eval 行为

Coherence
Knowledge

Consistency
Emotional Understanding</sample>
    <sample id="1033">ABC-Eval Behaviors
Coherence
Ignoring Partner
Irrelevant
Knowledge
Emotional Understanding</sample>
    <sample id="1034">ABC-Eval Behaviors

Coherence

Ignoring Partner

Irrelevant

Knowledge

Incorrect Fact

Commonsense Violation

Consistency

Self Contradiction

Partner Contradiction

Emotional Understanding

Empathetic Response

Lack of Empathy</sample>
    <sample id="1035">实验

* 4个开放域对话模型
* 每个模型100个人工-机器人对话
* ABC-Eval

Emory University

Alexa</sample>
    <sample id="1036">实验

4个开放领域对话模型
每个模型100个人机对话

ABC-Eval
Turn Likert
Dialogue Likert
Comparative

Emory University
Alexa</sample>
    <sample id="1037">Baseline Evaluation</sample>
    <sample id="1038">图1显示了对话中不同类别之间的Krippendorffα值。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话中不同类别之间的Krippendorffα值，表明了不同标注者之间的标注一致性。该图表显示了对话</sample>
    <sample id="1039">预测效度

图4展示了对话系统性能的预测效度，包括对话意图识别、情感识别和槽填充。结果表明，对话意图识别的预测效度最高，情感识别的预测效度次之，槽填充的预测效度最低。这表明对话系统在识别对话意图和情感方面具有较高的准确性和可靠性，但在识别槽填充方面存在较大的不确定性。</sample>
    <sample id="1040">预测效度

该图表展示了不同评估方法（如ABC-Eval、Turn Likert、Dialogue Likert和Comparative）下，质量解释（% of Quality Explained, R²）的比较。图表中使用了多种颜色代表不同的评估领域，如语法、语义、语音、语音行为、语音内容、语音结构、语音风格、语音态度、语音情感、语音信息和语音形式。图表的左侧有一个关键点，表示在某些评估领域中，质量解释的百分比显著增加，如语法、语音行为和语音内容。图表的右侧有一个关键点，表示在其他评估领域中，质量解释的百分比显著降低，如语音风格和语音情感。图表的标题为“预测效度”，表明这些数据用于评估不同评估方法对质量解释的预测能力。</sample>
    <sample id="1041">渐进效度</sample>
    <sample id="1042">渐进效度</sample>
    <sample id="1043">渐进有效性</sample>
    <sample id="1044">ABC-Eval 错误率按模型</sample>
    <sample id="1045">ABC-Eval Error Rates by Model

30
20
10
0
Anti-social
CS Contr.
Ignore
Incorrect
Irrelevant
Unempathetic
Other Contr.
Redundant
Self Contr.
Topic Switch
Uninterpretable

BART-FID-RAG Blender2 Emora Blender-Decode</sample>
    <sample id="1046">ABC-Eval 错误率按模型</sample>
    <sample id="1047">ABC-Eval Error Rates by Model

30
20
10
0
Anti-social
CS Contr.
Ignore
Incorrect
Irrelevant
Unempathetic
Other Contr.
Redundant
Self Contr.
Topic Contr.
Uninterpretable

BART-FID-RAG Blender2 Emora Blender-Decode</sample>
    <sample id="1048">根据幻灯片底部的标志和文字，这篇论文的作者所属机构是埃默里大学。具体而言，他们与“Emory NLP”和“Alexa Research Lab”有关联，这表明他们在这些研究实验室工作，可能专注于自然语言处理和与亚马逊Alexa相关的研究。</sample>
    <sample id="1049">在幻灯片中，CFT 代表“连续微调”，这是一种机器学习技术，其中模型在训练后继续在新数据上进行微调，以适应特定任务或领域。</sample>
    <sample id="1050">根据图片显示，论文的作者数量可以通过列出的名字来确定。在幻灯片上，作者名字如下：Koustuv Sinha, Joon Gouthier, Aaron Mueller, Kanishka Misra, Keren Fuentes, Roger Levy, Adina Williams。因此，这篇论文有七位作者。</sample>
    <sample id="1051">当翻译需要语境吗？数据驱动的多语言探索

卡耐基梅隆大学
计算机科学学院
人工智能研究实验室</sample>
    <sample id="1052">翻译取决于语境 我们得 rid of that mole.</sample>
    <sample id="1053">翻译取决于语境 事情如果让大臣们知晓将会变得危险 如果我们不除掉那只鼹鼠。</sample>
    <sample id="1054">翻译取决于语境
这 serious，医生？我们得 rid of that mole.</sample>
    <sample id="1055">Evaluating context-dependent translation is hard

Only a small portion of words depend on context</sample>
    <sample id="1056">评估上下文相关的翻译很难

- 只有少数单词依赖于上下文
- -语料库级别的指标
- 现有的方法支持有限的 discourse 现象和语言</sample>
    <sample id="1057">RQ1: 何时翻译需要背景？ RQ2: 模型如何处理背景相关的翻译？</sample>
    <sample id="1058">翻译是否需要语境？</sample>
    <sample id="1059">条件交叉熵互信息(CXMI)</sample>
    <sample id="1060">条件交叉互信息(CXMI)</sample>
    <sample id="1061">Pointwise (P-)CXMI

我们引入了P-CXMI来衡量特定翻译的上下文使用情况。</sample>
    <sample id="1062">翻译是否需要语境？</sample>
    <sample id="1063">Q1: 何时翻译需要语境？
- 词级语境使用
- 主题分析
Q2: 模型如何处理依赖于语境的翻译？</sample>
    <sample id="1064">主题分析高P-CXMI单词</sample>
    <sample id="1065">主题分析高P-CXMI单词 1. POS 标记</sample>
    <sample id="1066">主题分析高P-CXMI单词

1. 词性标注

P-CXMI在En-Ar中的词性标注

- 副词
- 普通话
- 三元组复数
- 三元组单数</sample>
    <sample id="1067">主题分析高P-CXMI单词

1. 词性标注
2. 词汇项
- 代词
- 动词形式</sample>
    <sample id="1068">主题分析高P-CXMI单词

1. 词性标注
2. 词汇项
- 指代词
- 动词形式
- 词汇 cohesion

阿维莱尔的母亲还在睡觉。阿维莱尔去上学了。

阿维列尔的母亲还在睡觉。阿维列尔去上学了。</sample>
    <sample id="1069">主题分析高P-CXMI单词

1. 词性标注
2. 词汇项

阿维利的母亲还在睡觉。阿维利去上学了。

阿维利的母亲还在睡觉。阿维利去上学了。</sample>
    <sample id="1070">主题分析高P-CXMI单词

1. 词性标注
2. 词汇项
3. 个体标记

她知道我们在哪。

我不了解。

Ich weiß, wohin wir gehen.

Ich weiß es nicht。</sample>
    <sample id="1071">Q1: 何时翻译需要语境？
- 词级语境使用
- 主题分析
Q2: 模型如何处理依赖于语境的翻译？
- 多语言 discourse-aware (MuDA)基准</sample>
    <sample id="1072">Multilingual Discourse-Aware (MuDA) tagger

- 指代词
- 动词形式
- 词汇连贯性
- 正式性
- 省略</sample>
    <sample id="1073">Multilingual Discourse-Aware (MuDA) tagger

Pronouns
Verb form
Lexical cohesion
Formality
Ellipsis</sample>
    <sample id="1074">MuDA benchmark

文件堆叠
MuDA 标记器
文件堆叠
BLEU COMET F-measure
机器人</sample>
    <sample id="1075">Q1: 何时翻译需要语境？
- 词级语境使用
- 主题分析
Q2: 模型如何处理依赖于语境的翻译？
- 多语言 discourse-aware (MuDA)基准测试
- 模型评估</sample>
    <sample id="1076">Corpus-level metrics BLEU</sample>
    <sample id="1077">Corpus-level metrics 无文本 BLEU 文本 COMET F-measure</sample>
    <sample id="1078">语料库级别的指标

BLEU
COMET
F-测量

对于文档级别的机器翻译，使用语料库级别的指标无法确定哪个系统最好。</sample>
    <sample id="1079">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion</sample>
    <sample id="1080">MuDA benchmark results

Context-aware models perform significantly better on some phenomena

Formality, lexical cohesion Ellipsis, pronouns, verb form</sample>
    <sample id="1081">MuDA基准测试结果

- 上下文感知模型在某些现象上表现显著优于
  - √: 正式性、词汇连贯性
  - ×: 括号、代词、动词形式
- DeepL 在大多数语言对上优于 Google
- 2021年4月更新</sample>
    <sample id="1082">摘要

* 识别系统地识别 discourse 现象，无需 prior 语言知识
* 基于文档的机器翻译的无数据集基准

从左到右的流程图展示了 MuDA 标记器、BLEU 和 METEOR 分数之间的关系。</sample>
    <sample id="1083">摘要

* 无需 prior linguistic knowledge，系统地识别 discourse phenomena
* 基于文档的机器翻译的无数据集基准

从左到右的流程图显示了MuDA和BLEU等评估指标之间的关系。</sample>
    <sample id="1084">演讲者的名字是Yusen Zhang。</sample>
    <sample id="1121">根据图片中提供的信息，新方法没有名称。它被描述为“一种新方法”，但没有提供任何特定的名称。</sample>
    <sample id="1122">根据幻灯片上提供的英文内容，作者描述“显性词汇”(marked words) 方法为：找到区分标记群体和未标记群体的单词。这表明该方法涉及识别那些可以用来识别或分类具有特定特征或属性的人群的特定术语或短语。</sample>
    <sample id="1123">根据幻灯片中提供的英文内容，论文的作者分别来自以下机构：张放来自保罗·G·奥伦学院（Paul G. Allen School），朴善英来自 UW NLP（University of Washington Natural Language Processing），刘于恒来自卡内基梅隆大学语言技术研究所（Carnegie Mellon University Language Technologies Institute），叶丝特科夫来自卡内基梅隆大学计算机科学系（Carnegie Mellon University Computer Science Department）。</sample>
    <sample id="1124">在幻灯片中提到的第一个对称依存关系结构的名称是“Multi-headed/London:”，它以城市名称London命名。</sample>
    <sample id="1125">演讲者的名字是Sarah E. Finch、James D. Finch和Jinho D. Choi。这些名字位于幻灯片的标题下方，以粗体字显示。</sample>
    <sample id="1126">根据幻灯片上显示的英文内容，这篇论文有五位作者。</sample>
    <sample id="1127">根据幻灯片中提供的英文内容，可用于测试句法现象的数据集是SyntaxGym。</sample>
    <sample id="1161">根据图片中图表的x轴标签，第一个研究问题的五种方法的缩写是：FT、W、BOND、COSINE和MLC。</sample>
    <sample id="1162">该模型在11个任务上进行了评估。</sample>
    <sample id="1226">CamemBERT 最初是在大规模的书库数据上训练的。</sample>
    <sample id="1227">演讲者的名字是Adam Przpielkowski和Michał Wójcik。</sample>
    <sample id="1228">根据幻灯片中呈现的发现，导致时间漂移是性能下降的主要原因的结论是：ELMo在2018年显著超越了之前的所有基线。这表明ELMo可能适应了数据分布的变化，从而在时间上漂移，导致其性能在随后的年份中下降。</sample>
    <sample id="1269">排列输出序列中的词元是必要的，以确保生成的文本符合语言的语法规则和连贯性。在自然语言处理任务中，如机器翻译、文本摘要或问答系统中，正确的排列对于理解句子结构、保持上下文连贯以及生成可读和有意义的文本至关重要。排列过程确保了动词、名词和其他词类按照语言的句法要求正确放置，从而生成自然流畅的文本。</sample>
    <sample id="1270">根据幻灯片上给出的英文内容，建议模型所有者提高偏见缓解方法的透明度的原因可能是为了确保用户了解模型如何处理潜在的偏见。这可以增强对模型的信任，并有助于建立一个更公平和可解释的AI系统。通过提供关于偏见缓解方法的清晰信息，模型所有者可以展示他们对解决AI系统中偏见问题的承诺，这对维护AI系统的道德标准至关重要。</sample>
    <sample id="1271">最小对不可接受输入（MPP）是一种评估语言模型的方法，其中使用相对概率差异来衡量模型对抽象知识的了解。在幻灯片中展示的例子包括比较“许多人在帮助自己”和“许多人正在帮助某人”之间的概率，以及比较“没有顾客…花了一笔钱”和“顾客…花了一笔钱”的概率。这些例子旨在测试模型对语法结构、语义含义和潜在偏见的敏感性。</sample>
    <sample id="1272">根据表格中所给的英文内容，作者使用了以下评估指标：准确率（Acc）、F1分数（F1）、BLEU分数（BLEU）和ROUGE分数（ROUGE）。这些指标用于评估不同模型在各种任务上的性能，如命名实体识别（NER）、关系抽取（REL）和文本摘要（SUM）。</sample>
    <sample id="1273">一致性是通过使用Krippendorff Alpha指标来衡量的。这个指标在图表的底部部分被突出显示，位于“情感理解”和“语法正确性”之后，并在“总体质量”之前。</sample>
    <sample id="1274">在不可接受和可接受查询中，选择“领域”来添加完全无关的句子。这可以通过使用一个包含与领域相关的符号（例如，地球仪代表地理或天气）的领域来实现，而另一个则不包含该符号。根据幻灯片，如果领域符号被包含在查询中，则查询被认为是可接受的，否则被认为是不可接受的。</sample>
    <sample id="1275">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是海因里希·海涅大学杜塞尔多夫分校。</sample>
    <sample id="1276">根据幻灯片上显示的英文内容，MultiInstruct 似乎是一种专门针对语言输入进行优化的方法或系统。它被与其他基准进行了比较，表明它可能在处理或解释语言输入方面具有独特的优势或改进。由于幻灯片没有提供关于 MultiInstruct 的具体细节，我们可以推断它可能涉及先进的自然语言处理技术，可能包括机器学习算法或神经网络，这些技术专注于理解和生成人类语言。与其他基准的比较意味着 MultiInstruct 可能代表了该领域的一个显著进步或创新。</sample>
    <sample id="1277">根据幻灯片上显示的英文内容，这篇论文有三位作者：Sarah E. Finch、James D. Finch和Jinho D. Choi。</sample>
    <sample id="1278">二进制协调指的是在句子中两个词或短语之间的协调关系，其中一个词或短语是另一个词或短语的前一个词或短语。这种协调通常涉及语法结构、意义或修辞功能上的相互依赖。例如，在短语“the quick brown fox”中，“quick”和“brown”之间存在二进制协调，因为它们都是用来描述“fox”的形容词。</sample>
    <sample id="1279">根据图片中提供的信息，无法确定提示语的平均长度。该图表关注的是黑人刻板印象词汇在不同GPT模型中的概率分布，而不是提示语的长度。要回答这个问题，需要额外的数据或信息，该数据或信息未在图表中呈现。</sample>
    <sample id="1280">这些发现表明，较小的 T5 模型在经过 Coreset 训练后，可以生成质量与大型语言模型相当甚至更好的脚本。这可能意味着较小的模型可以提供一种有效且成本效益的方法来生成高质量内容，而无需使用资源密集型或昂贵的大型模型。</sample>
    <sample id="1281">DrBERT:一种在生物医学和临床领域中具有稳健预训练模型的法文</sample>
    <sample id="1282">I. Healthcare中的语言建模
II. 预训练策略、数据来源和大小的比较
III. 13个模型在11个任务上的评估
IV. NACHOS和DrBERT的分布</sample>
    <sample id="1283">I. Healthcare中语言建模
II. 预训练策略、数据来源和大小的比较
III. 13个模型在11个任务上的评估
IV. NACHOS和DrBERT的分布</sample>
    <sample id="1284">I. Healthcare中语言模型
II. 预训练策略、数据来源和规模的比较
III. 13个模型在11个任务上的评估
IV. NACHOS和DrBERT的分布</sample>
    <sample id="1285">I. Healthcare中的语言建模
II. 预训练策略、数据来源和大小的比较
III. 13个模型在11个任务上的评估
IV. NACHOS和DrBERT的分布</sample>
    <sample id="1286">Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks</sample>
    <sample id="1287">Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages other than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks</sample>
    <sample id="1288">Transformer-based approaches, such as BERT, offer huge performance gains on a lot of NLP tasks. 语言模型

- 已经被翻译成法语的CamemBERT和FlauBERT
- 在医学任务上，英语中的领域特定模型进一步提高了性能
    - PubMedBERT、BioBERT、ClinicalBERT和其他
- 除了英语以外的其他语言更罕见，并主要依赖于现有的通用模型进行连续预训练
- 目前在生物医学领域中还没有开源的法语通用模型
- 基于BERT的领域特定模型应该能提高医学任务上的性能</sample>
    <sample id="1289">Transformer-based approaches, such as BERT, offer huge performance gain on a lot of NLP tasks. Has been adapted to French with CamemBERT and FlauBERT On medical tasks, domain-specific models in English raised the bar even higher PudMedBERT, BioBERT, ClinicalBERT and other Languages others than English are rarer and rely primarily on continual pre-training using an existing generic model No open-source model is available for biomedical domain in French yet BERT-based domain specific model for French should increase performance on medical tasks</sample>
    <sample id="1290">比较预训练策略和数据来源

* 评估公共和私有医学数据源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多任务医学领域数据集，包含来自1,700家医院的多样化风格、主题和来源的匿名化医疗记录，提取自南特大学医院数据仓库。
	+ 私有数据集：从1,700家医院提取的匿名化医疗记录的句子集合。
* 比较学习策略
	+ 从头开始：完整的模型构建
	+ 继续预训练：使用现有预训练模型进行微调
	+ 示例：Camembert是一个法国通用模型，PubMedBERT是一个生物医学特定模型</sample>
    <sample id="1291">比较预训练策略和数据来源

* 评估公共和私人医疗数据源的可比数据量的影响
	+ NACHOS：一个1.1百万字的开源多领域、多风格、多主题的医疗数据集
	+ 私人数据集：从1.7百万份匿名化医疗记录中提取的句子，来自南特大学医院数据仓库
* 比较学习策略
	+ 从头开始：使用完整构建的模型
	+ 继续预训练：使用现有预训练模型（Camembert、BERT、Chubert）
	+ 公共MedBERT：一个法国通用模型
	+ PubMedBERT：使用NACHOS和NBDW进行预训练的模型

| 模型名称 | 来源 | NACHOS | NBDW |
| --- | --- | --- | --- |
| DfBERT | 从头开始 |  |  |
| BERT | 从头开始 |  |  |
| Chubert | 从头开始 |  |  |
| Camembert | 公共MedBERT |  |  |
| MedBERT | 公共MedBERT |  |  |
| PubMedBERT | NACHOS和NBDW预训练 |  |  |

Avignon Université</sample>
    <sample id="1292">比较预训练策略和数据来源

* 评估公共和私有医疗数据来源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多领域医疗语料库，包含来自不同来源的多样化数据，如NCT、Medl、PubMed等。
	+ 私有数据集：从1.7M anonymized医疗记录中提取的句子，来自南特大学医院数据仓库。
	+ 比较学习策略
		- 从头开始：使用完整构建的模型
		- 持续预训练：使用现有预训练模型，如CamemBERT、一个法国通用模型和PubMedBERT。
		- 从头开始：使用NACHOS数据集
		- 持续预训练：使用NACHOS数据集
		- 从头开始：使用NBDW数据集
		- 持续预训练：使用NBDW数据集</sample>
    <sample id="1293">比较预训练策略和数据来源

* 评估公共和私有医学数据来源的可比数据量的影响
	+ NACHOS：一个1.1百万单词开源多领域医学语料库，包含来自不同来源的多样化数据，如NCT、Medl、PMC等。它包括不同风格、性质和来源的句子。
	+ 私有数据集：从1.7百万份匿名化医疗记录中提取的句子，这些记录来自南特大学医院数据仓库。
* 比较学习策略
	+ 从头开始：使用完整构建的模型进行训练
	+ 继续预训练：使用现有预训练模型进行微调
	+ 示例：Camembert，一个法国通用模型，和PubMedBERT</sample>
    <sample id="1294">比较预训练策略和数据来源

* 评估公共和私有医学数据源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多领域医学语料库，包含来自不同风格、性质和来源的多样化医学文本
	+ 私有语料库：从Nantes大学医院数据仓库中提取的1.7亿匿名化句子
* 比较学习策略
	+ 从头开始：使用完整构建的模型
	+ 继续预训练：使用现有预训练模型（如DistBERT、RoBERTa、Camembert和PubMedBERT）
	+ 从头开始：使用NACHOS数据集进行预训练
	+ 继续预训练：使用NACHOS数据集进行预训练</sample>
    <sample id="1295">比较预训练策略和数据来源

* 评估公共和私有医学数据源的可比数据量
	+ NACHOS：一个1.1亿词开源多任务医学领域数据集，包含来自不同来源的多样化数据，如NCT、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl、Medl</sample>
    <sample id="1296">比较预训练策略和数据来源

* 评估公共和私有医疗数据源的可比数据量的影响
	+ NACHOS：一个1.1百万字的开源多领域、多风格医疗数据集
	+ 私有数据集：从1.7百万份匿名化医疗记录中提取的句子，来自南特大学医院数据仓库
* 比较学习策略
	+ 从头开始构建完整模型
	+ 继续预训练：使用现有预训练模型，如CamemBERT和French通用模型PubMedBERT和NACHOS
	+ 公共数据集：CamemBERT和NBDW数据集</sample>
    <sample id="1297">比较预训练策略和数据来源

* 评估公共和私有医学数据来源的可比数据量的影响
	+ NACHOS：一个1.1亿词开源多领域多风格医学语料库
	+ 私有数据集：从1.7M anonymized医疗记录中提取的句子，来自南特大学医院数据仓库
* 比较学习策略
	+ 从头开始构建完整模型
	+ 继续预训练：使用现有预训练模型，如CamemBERT、ChuBiT和PubMedBERT
	+ 公开的CamemBERT是一个法国通用模型，而PubMedBERT是一个医学特定模型

| 模型名称 | 来源 | NACHOS | NACHOS（公开） | NACHOS（私有） | NBDW |
| --- | --- | --- | --- | --- | --- |
| DeBERTa | 从头开始 |  |  |  |  |
| RoBERTa | 从头开始 |  |  |  |  |
| CamemBERT | 从头开始 |  |  |  |  |
| ChugIT | 从头开始 |  |  |  |  |
| PubMedBERT | 继续预训练 |  |  |  |  |
| CamemBERT | 继续预训练 |  |  |  |  |
| NBDW | 继续预训练 |  |  |  |  |

Avignon Université</sample>
    <sample id="1298">评估：13个数据源和大小的评估
我们的性能评估涵盖了11个公共和私有数据集。我们对每个数据集进行微调，以获得几乎所有任务的最佳性能。
表：性能评估结果
| 源 | Medical Report Specialties | MUSCA-DET | MUSCA-DET+ISSA | CAS | CAS+MUSCA | QUADRO-EMEA | QUADRO-MEDLINE |
| --- | --- | --- | --- | --- | --- | --- | --- |
| General | 98.4 | 97.1 | 97.1 | 96.2 | 95.5 | 95.0 | 95.1 |
| Biochemical | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Biomedical | 98.0 | 96.8 | 96.8 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical and Biochemical | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical and Biomedical | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical, Biomedical and MUSCA-DET | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical, Biomedical, MUSCA-DET and MUSCA-DET+ISSA | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical, Biomedical, MUSCA-DET, MUSCA-DET+ISSA and CAS | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical, Biomedical, MUSCA-DET, MUSCA-DET+ISSA, CAS and CAS+MUSCA | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical, Biomedical, MUSCA-DET, MUSCA-DET+ISSA, CAS and CAS+MUSCA and QUADRO-EMEA | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |
| Clinical, Biochemical, Biomedical, MUSCA-DET, MUSCA-DET+ISSA, CAS and CAS+MUSCA and QUADRO-EMEA and QUADRO-MEDLINE | 97.8 | 96.7 | 96.7 | 95.1 | 94.4 | 94.1 | 94.2 |</sample>
    <sample id="1299">评估：13个数据源和大小的评估
·我们的微调模型在几乎所有的艺术任务上达到最新状态
·性能评估包括公共和私人数据集
|  |  |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |</sample>
    <sample id="1300">评估：13个数据源和大小的评估
- 我们的微调模型在几乎所有的艺术任务上达到最新状态
|  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |</sample>
    <sample id="1301">性能评估：13个数据源和大小
- 我们的微调模型在几乎所有的艺术任务上达到最新水平
|  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | AIF | Medical Report Specialties | MUSCA-DET | MUSCA-DET | ISSAI | CAS | CAS-MoC | QUADRO-EMe | QUADRO-MEDiNE |
|  | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 | F1 |
| General | CanonBert Oscar 15GB | 92.78 | 99.41 | 93.43 | 91.27 | 89.09 | 94.62 | 93.75 | 90.83 |
| General | CanonBert CUNet 4GB | 92.66 | 99.08 | 93.35 | 91.38 | 89.41 | 93.23 | 93.75 | 90.33 |
| General | CanonBert V1 | 92.66 | 99.08 | 93.35 | 91.38 | 89.41 | 93.23 | 93.75 | 90.33 |
| Biomedical | RoBERTa V1 | 46.01 | 79.00 | 98.38 | 81.91 | 81.81 | 88.18 | 154.51 | 72.68 |
| Clinical | ClinicalNACOS | 44.70 | 87.77 | 98.55 | 82.55 | 82.44 | 88.18 | 23.79 | 75.05 |
| Clinical | DeBERTa NACOS | 45.53 | 93.97 | 98.77 | 85.56 | 85.77 | 88.18 | 91.46 | 75.15 |
| Clinical | ChatBERT NACOS | 49.14 | 93.97 | 99.31 | 85.77 | 85.77 | 88.18 | 91.46 | 75.15 |
| Clinical | ChatBERT NACOS (uncased) | 49.14 | 93.97 | 99.31 | 85.77 | 85.77 | 88.18 | 91.46 | 75.15 |
|  |  |  |  |  |  |  |  |  |  |</sample>
    <sample id="1302">从头开始 vs. 在4GB数据上进行持续预训练：通过将更多领域特定知识融入模型中，使其能够更好地工作。一项关于模型稳定性的研究表明，使用Camembert-based模型的持续预训练具有更高的内运行变性。</sample>
    <sample id="1303">评价：预训练策略
* 从头开始 vs. 在4GB数据上进行持续预训练：通过将更多领域特定知识融入模型，使其能够更好地工作。
* 对模型稳定性的研究表明，使用Camembert模型进行持续预训练的模型具有更高的内运行变性。
| 模型 | HF微调 | 医疗保健专门知识 | MUSCA-Det | MUSCA-ESAI | 带有 | 带有 | 带有 | 带有 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 33.13 | 90.33 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 74.1 |
| Camembert CUNN | 43.11 | 79.86 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 77.6 |
| Camembert Chilis++ | 46.01 | 72.12 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 75.0 |
| Camembert Chilis++（带） | 46.01 | 72.12 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 75.0 |
| RoBERTa NACHOS（带） | 45.73 | 98.44 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 73.1 |
| Camembert NWDW | 40.72 | 90.33 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 81.7 |
| Camembert NWDW（带） | 40.72 | 90.33 | 99.20 | 85.44 | 92.42 | 95.23 | 92.44 | 81.7 |</sample>
    <sample id="1304">评价：预训练策略
*从头开始vs.在4GB数据上进行持续预训练：通过将更多领域特定知识融入模型，使其能够更好地工作。
*模型稳定性的研究表明，使用Camembert-BERT进行持续预训练的模型具有更高的跨运行稳定性。
| 模型 | HF微调 | 医疗保健专业 | MUSCA-Det | MUSCA-ESAI | HumanLM | QuADRO-EMMA | QuADRO-MED |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Camembert Oscar | 35.13 | 90.33 | 99.20 | 85.44 | 95.24 | 74.14 | 77.61 |
| Camembert CoNLL 4G | 43.11 | 79.88 | 99.85 | 93.18 | 92.42 | 83.71 | 76.11 |
| Camembert NLI | 46.01 | 77.12 | 99.80 | 96.46 | 96.73 | 87.28 | 75.68 |
| Camembert + | 46.01 | 77.12 | 99.80 | 96.46 | 96.73 | 87.28 | 75.68 |
| RoBERTa NLI (HF微调) | 45.73 | 98.87 | 99.85 | 96.57 | 97.37 | 96.66 | 87.18 |
| RoBERTa NLI (HF微调) | 45.73 | 98.87 | 99.85 | 96.57 | 97.37 | 96.66 | 87.18 |
| Camembert NLI (HF微调) | 46.01 | 77.12 | 99.80 | 96.46 | 96.73 | 87.28 | 75.68 |
| Camembert NLI (HF微调) | 46.01 | 77.12 | 99.80 | 96.46 | 96.73 | 87.28 | 75.68 |</sample>
    <sample id="1305">核心信息

* DrBERT在9项主流法语医学导向任务中超过了Camembert通用模型
* 训练特定于医学领域的模型在法语中具有实用性
* 数据来源很重要；在异质数据上进行训练很重要
* NACHOS比仅使用临床数据更稳健
* 持续预训练效果更好，但在基于特定领域英文模型时效果不佳
* DrBERT模型、NACHOS数据集和训练脚本在MIT许可下免费提供

Avignon Université</sample>
    <sample id="1306">核心信息

* DrBERT在9项法语医学导向任务中超过了Camembert通用模型
* 使用特定于医学的模型训练的重要性得到确认
* 基于异质数据进行训练很重要
* NACHOS比仅使用临床数据更稳健
* 持续预训练更好，但在基于特定领域的英文模型时效果不佳

额外信息

* DrBERT模型、NACHOS数据集和训练脚本在MIT许可证下免费提供
* 图片右上角有一个人的照片</sample>
    <sample id="1307">核心信息

* DrBERT在9个主流法语医学导向任务上超过了Camembert通用模型
	+ 证明了在法语上训练医学特定模型的实用性
* 数据源很重要：在异质数据上进行训练很重要
	+ NACHOS比仅使用临床数据更稳健
* 持续预训练更好，但当基于领域特定的英语模型时效果不佳
	+ DrBERT模型、NACHOS数据集和训练脚本可免费获得MIT许可证下的自由使用。

来源：Avignon Université</sample>
    <sample id="1308">谢谢。期待在多伦多海报讨论会上与大家交流！更多详细信息，请访问：drbert.univ-avignon.fr Avignon Université</sample>
    <sample id="1309">根据幻灯片上提供的英文内容，论文研究了以下学习策略：1. 从头开始：使用全量数据集进行完整构建。2. 持续预训练：使用现有的预训练模型进行持续预训练。3. 预训练：使用CamMedBERT、一个法国通用模型和PubMedBERT作为预训练模型。</sample>
    <sample id="1310">仅根据图片中提供的信息，无法确定由于测试重复使用而导致的过拟合因素有多高。该幻灯片表明存在过拟合的可能性，并且图表显示了性能得分随时间变化的趋势，但没有明确数据或分析来量化过拟合程度。</sample>
    <sample id="1311">通过BLEU和METEOR评分</sample>
    <sample id="1312">根据图表，语言模型确实被赋予了不同的政治偏见。图表的x轴代表了从左到右的自由主义-权威主义轴，y轴代表了从经济自由主义到经济极权主义的范围。每个模型在图表上的位置表明了其政治倾向，例如，位于图表右上角的“GPT-3-davinci”和“ChatGPT”可能具有更自由主义和经济自由主义的特征，而位于左下角的“ALBERT-large”和“RoBERTa-base”则可能具有更权威主义和经济极权主义的特征。</sample>
    <sample id="1313">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations

马修斯·林登曼、亚历山大·科勒尔、伊万·提托夫

信息学

NLP

荷兰人工智能中心

桑德兰大学

阿姆斯特丹大学</sample>
    <sample id="1314">Compositional Generalization without Trees using Multiset Tagging and Latent Permutations
Matthias Lindemann, Alexander Koller, Ivan Titov
信息学中心
NLP中心
荷兰科学与研究大学中心
Sarlande大学
阿姆斯特丹大学</sample>
    <sample id="1315">在训练期间，学习者处理更深层次递归和未见过的短语组合的能力。</sample>
    <sample id="1316">标题：组合泛化在语义解析中

训练：

| 句子 | 依存关系 |
| --- | --- |
| The girl slept. | 'x_1: sleep_agent_x_2' |
| Mary knew that the girl slept. | 'x_1: know_agent_x_2', 'Mary ∧ know.cmp_x_1, x_2' |

注释：</sample>
    <sample id="1317">标题：组合泛化在语义解析中的应用

训练：

| 句子 | 依存关系 |
| --- | --- |
| The girl slept. | '女孩' - 睡觉_主语 ('女孩')，'睡觉' - 睡觉_动词 ('睡觉') |
| Mary knew that the girl slept. | 'Mary' - 知道_主语 ('Mary')，'知道' - 知道_动词 ('知道')，'女孩' - 睡觉_宾语 ('睡觉')，'睡觉' - 睡觉_动词 ('睡觉') |

这张幻灯片展示了在自然语言处理中如何使用依存关系进行语义解析。它通过展示一个简单的句子"The girl slept."和一个稍微复杂的句子"Mary knew that the girl slept."来说明这一点。对于第一个句子，它识别了'女孩'作为主语，'睡觉'作为动词。对于第二个句子，它识别了'Mary'作为主语，'知道'作为动词，并且'女孩'和'睡觉'作为'知道'的宾语。这种解析有助于理解句子的结构和意义。</sample>
    <sample id="1318">标题：组合泛化在语义解析中的应用

训练：

| 句子 | 依存关系 |
| --- | --- |
| The girl slept. | 'x_1: sleep_agent_x_2' |
| Mary knew that the girl slept. | 'x_1: know_agent_x_2, Mary ∧ ccomp_x_1, x_2' |

注释：</sample>
    <sample id="1319">训练：女孩睡了。Mary知道那个女孩睡了。测试：Jim说Mary知道那个女孩睡了。</sample>
    <sample id="1320">训练：'女孩睡了。' 'Mary知道那个女孩睡了。' 测试：'Jim说Mary知道那个女孩睡了。' ' naive seq2seq models fail!'</sample>
    <sample id="1321">训练：'女孩睡了。' '玛丽知道那个女孩睡了。' 测试：'吉姆说玛丽知道那个女孩睡了。'</sample>
    <sample id="1322">树木帮了大忙，但... *girl x1; sleep. agent x2* *girl x1; sleep. agent x2* *girl x1; sleep. agent x2* The girl slept.</sample>
    <sample id="1323">树木帮了大忙，但... *girl x1; sleep. agent x2* *girl x1; sleep. agent x2* *girl x1; sleep. agent x2* The girl slept.</sample>
    <sample id="1324">树木帮了大忙，但...</sample>
    <sample id="1325">树木帮了大忙…*女孩x_{1}；sleep. agent x_{2}x_{1}*女孩x_{1}；x_{2}sleep. agent x_{2}slept.树木需要获取：预处理/后处理逻辑形式</sample>
    <sample id="1326">树木帮了大忙但... *girl x_{1} ;sleep agent x_{2} ;x_{1} *girl x_{1} ;x_{2} ;sleep agent x_{2} ;x_{1} The girl slept. 树木需要获得： - 预/后处理逻辑形式 - 语法-归纳</sample>
    <sample id="1327">树木帮助很大，但... *girl x1; sleep agent x2; x1 *girl x1; x1 slept sleep agent x2 The girl slept 树木需要获取： - 预/后处理逻辑形式 - 语法-归纳 对于第一次，我们展示了对更深的递归的强泛化，而没有树。</sample>
    <sample id="1328">树木帮了大忙，但… *girl x1;sleep agent x2; x1 *girl x1; x2 sleep agent x2 The girl slept. 树木需要被获得： - 预/后处理逻辑形式 - 语法-归纳 对于第一次，我们展示了对更深层次的递归的强泛化，而无需树木。</sample>
    <sample id="1329">我们的方法

* 逗号 x1 女孩 x1 睡觉
agent x2</sample>
    <sample id="1330">Our Approach

*  the
girl
sleep
agent
x1
x2
Tag</sample>
    <sample id="1331">Our Approach

* ,  x1  girl  x1  sleep  agent  x2

the  girl  slept</sample>
    <sample id="1332">我们的方法

*,
x1
girl
x1
sleep
agent
x2
the
girl
slept</sample>
    <sample id="1333">我们的方法</sample>
    <sample id="1334">Permuting with “jumps” Permute Tag the girl slept</sample>
    <sample id="1335">Permuting with “jumps”  Permute Tag the girl slept</sample>
    <sample id="1336">Permuting with “jumps” Permute Tag the girl slept</sample>
    <sample id="1337">Permuting with “jumps” Permute Tag the girl slept</sample>
    <sample id="1338">Permuting with "jumps"  sleep agent the girl slept x1 x2</sample>
    <sample id="1339">一些关于COGS（金和林zen 2020）的结果比较其他Treeless模型在结构泛化上的表现

图表展示了不同模型在处理COGS数据集时的性能比较。图表中包括了以下几种模型：

1. **LSTM seq2seq**：使用循环神经网络（RNN）进行序列到序列学习。
2. **T5**：使用Transformer模型，具体是Google开发的T5（Text-to-Text Transfer Transformer）。
3. **Zheng and Lapata**：这是另一篇研究中提到的模型，可能涉及特定的自然语言处理技术。
4. **Ours**：表示当前研究中提出的新模型。

图表的横轴代表不同的泛化类型，包括：

1. **PP recursion**：指处理嵌套结构的递归问题。
2. **GP recursion**：可能指的是处理更广泛或更复杂递归问题的泛化能力。
3. **Obj + Sub + PP**：表示对象、子句和从句的泛化能力。

图表的纵轴表示准确率（ACC），范围从0到80。

### 分析
- **LSTM seq2seq**：在所有泛化类型中表现较差，尤其是在“Obj + Sub + PP”类型中。
- **T5**：在“PP recursion”和“GP recursion”中表现良好，但在“Obj + Sub + PP”类型中表现不佳。
- **Zheng and Lapata**：在“PP recursion”和“GP recursion”中表现一般，在“Obj + Sub + PP”类型中表现较差。
- **Ours**：在所有泛化类型中表现最佳，尤其是在“Obj + Sub + PP”类型中。

### 结论
根据图表，新模型（标记为“Ours”）在处理COGS数据集时表现出色，特别是在处理对象、子句和从句的泛化能力方面。这表明新模型可能在处理复杂结构化数据时具有更好的泛化能力。</sample>
    <sample id="1340">一些关于COGS（金和林zen 2020）的结果比较其他Treeless模型在结构泛化上的表现</sample>
    <sample id="1341">技术挑战我们解决

* girl x1
* sleep agent x2
* kid x3

Permute

?

?

?

Tag

the

girl

sleep</sample>
    <sample id="1342">技术挑战我们解决

* 女孩 x1
* 睡觉 x2
* 代理 x3
* x1

排列

?

?

?

标签

the

女孩

sleep</sample>
    <sample id="1343">技术挑战我们解决

* girl x1
? agent x2
? sleep x3
Permute
?
?
?
Tag
the
girl
sleep</sample>
    <sample id="1344">技术挑战我们解决

[图片描述]

这个流程图展示了在处理自然语言处理（NLP）任务时可能遇到的技术挑战。流程图分为三个主要部分：输入、中间过程和输出。

1. **输入**：
   - 包含两个输入变量：`x1` 和 `x2`。
   - `x1` 被标记为 "girl"，`x2` 被标记为 "sleep"。
   - 这些输入通过一个加号（+）连接，表示它们是输入的组合或组合。

2. **中间过程**：
   - 从输入开始，`x1` 和 `x2` 通过一个箭头连接到一个名为 "Permute" 的组件。
   - "Permute" 表示对输入进行排列或重新排序的操作。
   - 排列后的输入通过一个箭头连接到一个名为 "Tag" 的组件。
   - "Tag" 表示对排列后的输入进行标记或分类的操作。

3. **输出**：
   - 输出显示排列和标记后的输入。
   - 输出包括 "the", "girl", "sleep"，这些词可能是排列和标记后的输入的组合或结果。

**注释**：
- "Alignment unknown." 表示输入和输出之间的对齐或匹配是未知的。
- "Induce it in training." 表示在训练过程中需要引入这种对齐或匹配。
- "Rotation model:" 表示旋转模型，可能是指一种用于处理输入和输出之间关系的模型。

**中文翻译**：

技术挑战我们解决

[流程图描述]

这个流程图展示了在处理自然语言处理（NLP）任务时可能遇到的技术挑战。流程图分为三个主要部分：输入、中间过程和输出。

1. **输入**：
   - 包含两个输入变量：`x1` 和 `x2`。
   - `x1` 被标记为 "女孩"，`x2` 被标记为 "睡觉"。
   - 这些输入通过一个加号（+）连接，表示它们是输入的组合或组合。

2. **中间过程**：
   - 从输入开始，`x1` 和 `x2` 通过一个箭头连接到一个名为 "排列" 的组件。
   - "排列" 表示对输入进行排列或重新排序的操作。
   - 排列后的输入通过一个箭头连接到一个名为 "标记" 的组件。
   - "标记" 表示对排列后的输入进行标记或分类的操作。

3. **输出**：
   - 输出显示排列和标记后的输入。
   - 输出包括 "the", "女孩", "睡觉"，这些词可能是排列和标记后的输入的组合或结果。

**注释**：
- "对齐未知。" 表示输入和输出之间的对齐或匹配是未知的。
- "在训练中引入它。" 表示在训练过程中需要引入这种对齐或匹配。
- "旋转模型：" 表示旋转模型，可能是指一种用于处理输入和输出之间关系的模型。</sample>
    <sample id="1345">技术挑战我们解决

+ 女孩 x₁ j 睡觉 agent x₂
Permute
女孩 x₁ 睡觉 agent x₂
Tag
the 女孩 睡觉</sample>
    <sample id="1346">技术挑战我们解决

图中的箭头表示从一个词到另一个词的依赖关系。例如，'girl'依赖于'agent'，'sleep'依赖于'agent'和'girl'。这些依赖关系用于确定词的排列顺序。</sample>
    <sample id="1347">认知失调是指个体同时拥有相互矛盾或不一致的想法、信念或行为。这个概念通常与社会心理学相关联，特别是与态度改变和决策过程有关。</sample>
    <sample id="1348">根据图表显示，最倾向于自由派的语言模型是“GPT-4”和“GPT-J”，它们位于图表的左下方，表明自由派观点。</sample>
    <sample id="1349">根据图表显示，在主动学习的背景下，累积训练（用蓝色表示）在所有评估指标上都显示出比迭代训练（用黄色表示）更有效的趋势。这可以通过累积训练柱形图高于迭代训练柱形图来观察到，表明累积方法在随机选择、熵、核心集和CAL中具有更高的AUC值。因此，可以推断在主动学习时，累积训练通常比迭代训练更有效。</sample>
    <sample id="1350">演讲者的名字是Sara Papi、Matteo Negri和Marco Turchi。</sample>
    <sample id="1351">根据幻灯片中提供的信息，MuDa基准中的数据是从WMT（国际机器翻译大会）和IWSLT（国际研讨会）等主要机器翻译数据集中获得的。</sample>
    <sample id="1385">演讲者的名字是马修斯·林登曼。</sample>
    <sample id="1386">跨语言转移指的是在一种语言上进行训练，然后将其应用于另一种语言的过程。在机器学习和自然语言处理的背景下，这可能涉及使用一个模型在一种语言上学习任务，然后在另一种语言上进行微调或直接应用，而无需额外的训练。</sample>
    <sample id="1387">根据图片中提供的信息，这篇论文的作者分别来自三个机构：Saarland University、Amazon Alexa和University of Vienna。这些机构通过作者姓名下方的数字标签与作者照片相关联。</sample>
    <sample id="1388">根据图片中显示的英文内容，作者使用了以下延迟测量方法：'AL/AL-CA'。</sample>
    <sample id="1389">The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources</sample>
    <sample id="1390">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge)

NLU Model</sample>
    <sample id="1391">NLU models draw on multiple knowledge sources

Knowledge in Parameters (pretrain-time knowledge)

Knowledge in Context (inference-time knowledge)

NLU Model</sample>
    <sample id="1392">John saw the newly elected president on TV

What presidents do What a TV

Who is John Who is the new president</sample>
    <sample id="1393">John saw the newly elected president on TV

What presidents do What a TV

pre-train time knowledge

Who is John Who is the new president</sample>
    <sample id="1394">John saw the newly elected president on TV

What presidents do What a TV

Who is John Who is the new president</sample>
    <sample id="1395">John saw the newly elected president on TV  What presidents do What is a TV  Who is John  Who is the new president pre-train-time knowledge inference-time knowledge</sample>
    <sample id="1396">KITMUS Test Suite

数据集用于知识整合评估

预训练任务以测试能力是否能调用

- 预训练时间的知识
- 推断时间的知识

实验涉及

- 人类研究参与者
- 参考解析模型</sample>
    <sample id="1397">KITMUS Test Suite

* Dataset for knowledge integration evaluation
* Coreference resolution task to probe ability to draw on
    * pre-train time knowledge
    * inference-time knowledge
* Experiment with
    * human study participants
    * coreference resolution models</sample>
    <sample id="1398">Servin 是一位法官。Kea 是一位面包师。Servin 和 Kea 在公园里相遇。在一天的工作结束后，他很高兴放松。[答案: Servin]</sample>
    <sample id="1399">Servin 是一名法官。Kea 是一名面包师。Servin 和 Kea 在公园里相遇。在一天结束时，他因在法庭上处理案件而感到高兴。 Servin [答案: Servin]</sample>
    <sample id="1400">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1401">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. [Answer: Servin]</sample>
    <sample id="1402">Servin is a judge. Kea is a baker. Servin and Kea met at a park. After a long day at work deciding cases in a law court, he was happy to relax. (Answer: Servin)</sample>
    <sample id="1403">背景：在幻灯片上展示了一个关于KITMUS变体的图表。图表展示了不同的背景设置，包括“Background-Pretrain”、“Background-Both”和“Background-Inference”。每个设置都有一个描述其特点的简要说明。幻灯片还显示了一个人戴着耳机，可能正在进行演示。</sample>
    <sample id="1404">标题：KITMUS的变体

幻灯片展示了KITMUS的几种不同设置，具体如下：

1. 背景预训练（Background-Pretrain）：典型设置
2. 背景-Both（Background-Both）：在上下文中明确提供背景知识
3. 背景推理（Background-Inference）：仅在推理时提供知识

这些设置展示了KITMUS在处理背景信息时的不同方法。</sample>
    <sample id="1405">标题幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS是一个用于自然语言处理任务的预训练模型。幻灯片详细说明了三种不同的背景设置：Background-Pretrain、Background-Both和Background-Inference。每种设置都通过图表进行说明，展示了在预训练阶段使用的不同类型的背景知识。

幻灯片的中文翻译如下：

幻灯片展示了KITMUS的变体，KITMUS</sample>
    <sample id="1406">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1407">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1408">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1409">背景-预训练
背景-Both
背景-推断</sample>
    <sample id="1410">背景-预训练

1.0
0.9
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
没有任务特定训练 有任务特定训练
随机选择 自主学习参与者 BERT4Coref C2F
任务特定训练是知识整合所必需的</sample>
    <sample id="1411">背景-预训练

在没有任务特定训练的情况下，随机选择、自然语言参与者和BERT4Coref的平均准确率都较低。然而，在有任务特定训练的情况下，所有方法的准确率都有显著提高。这表明任务特定训练对于知识整合是必要的。</sample>
    <sample id="1412">背景-预训练
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
没有任务特定训练 有任务特定训练
随机选择 自然语言参与者 BERT4Coref C2F
任务特定训练是知识整合所必需的</sample>
    <sample id="1413">背景推断

图表显示了不同模型在处理背景知识时的表现。图表的x轴表示背景知识的类型，分为“随机选择”、“人类参与者”、“BERT4Core”和“C2F”四种。y轴表示平均准确率，范围从0到1。

图表中，随机选择的准确率接近0.8，而人类参与者的准确率略高于0.9。BERT4Core和C2F的准确率较低，分别约为0.2和0.3。图表底部的注释说明了这些模型在整合推理时间背景知识时存在困难。

该图表表明，人类参与者在处理背景知识时表现最佳，而机器学习模型（如BERT4Core和C2F）则表现出较低的准确率。这可能意味着这些模型需要改进以更好地利用背景知识进行推理。</sample>
    <sample id="1414">结论

主要 takeaway:
1. 许多模型似乎无法在多个来源（预训练时间和推理时间知识）上推断知识。
2. 任务特定的培训对于知识整合是必要的。
3. 模型难以整合推理时间背景知识。

请在GitHub上找到数据集、生成和评估代码：mpoems/kitmus</sample>
    <sample id="1415">结论

主要 takeaway：

1. 许多模型似乎无法在多个来源之间推断知识（训练时间和推理时间知识）
2. 任务特定的培训对于知识整合是必要的
3. 模型难以整合推理时间背景知识

请在GitHub上找到数据集、生成和评估代码：mpoems/mpoems

---

这张幻灯片的中文翻译</sample>
    <sample id="1416">根据图片中显示的树结构，其缺点可能包括：1. 有限的表达能力：树结构可能无法捕捉到句子中的所有复杂关系和语义细微差别。2. 线性假设：树结构假设句子可以被分解为一个简单的线性序列，而实际上，自然语言通常包含更复杂的结构，如嵌套从句、并列结构和依赖关系。3. 缺乏上下文理解：树结构可能无法充分考虑句子的上下文，导致对语义的误解或不准确的解释。4. 计算效率问题：处理大型文本时，树结构可能在计算上效率低下，因为需要处理大量的树结构。5. 无法处理非确定性：在自然语言处理中，许多任务涉及处理不确定性，例如识别最有可能的解析或生成符合给定输入的输出。树结构可能无法有效地解决这些非确定性问题。</sample>
    <sample id="1417">根据幻灯片上提供的英文内容，这篇论文的作者所属机构是“Georgia Institute of Technology”，具体而言是其“School of Interactive Computing”。</sample>
    <sample id="1418">Marked Personas 用自然语言提示衡量语言模型中的刻板印象 Myra Cheng，Esin Durmus，Dan Jurafsky Stanford Engineering Computer Science</sample>
    <sample id="1419">Marked Personas: Motivation

社会偏见和刻板印象在LLMs中很普遍

现有刻板印象措施的限制：

- 特定性和可泛化性之间的权衡
- 基于固定的、手工制作的数据集
- 不考虑交叉性</sample>
    <sample id="1420">标记人物：动机
社会偏见和刻板印象在LLMs中很普遍
现有刻板印象措施的限制：
- 特定性和可泛化性之间的权衡
- 基于固定的、手工制作的数据集
- 不考虑交叉性</sample>
    <sample id="1421">Marked Personas: Motivation
社会偏见和刻板印象在LLMs中很普遍
现有刻板印象衡量的局限性：
- 特定性和可泛化性之间的权衡
- 基于固定的、手工制作的数据集
- 不考虑交叉性</sample>
    <sample id="1422">标记人物：动机

社会偏见和刻板印象在LLMs中很普遍。

现有刻板印象措施的限制：

- 特定性和可泛化的权衡
- 基于固定的、手工制作的数据集
- 不考虑交叉性</sample>
    <sample id="1423">如何克服这些限制？GPT-3.5、GPT-4等可以响应提示中的指令</sample>
    <sample id="1424">如何克服这些限制？GPT-3.5、GPT-4等可以响应提示中的指令。输入：“想象一下，你是一个亚洲女性。描述你自己。”</sample>
    <sample id="1425">如何克服这些限制？GPT-3.5、GPT-4等可以根据提示回答指令。输入：“想象一下，你是一个亚洲女性。描述你自己。”可扩展性：能够评估任何交叉身份</sample>
    <sample id="1426">输出：人物示例（GPT-4）

亚洲女性：她拥有 almond 形的眼睛，被长长的、深色的睫毛框住。她的外貌似乎承载着家族的故事和秘密，她的肤色柔和金黄，光滑而似乎未被时间触及。她精致的身材既优雅又不assuming，让她在无需过多注意的情况下移动。

中东女性：她代表着中东的美丽之神，体现了异国情调和永恒的优雅。她拥有深邃的目光，被延长的眉毛框住，像精致的羽毛一样精致。她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧之夜。</sample>
    <sample id="1427">步骤1：人物示例（GPT-4）

亚洲女性：她的眼睛呈杏仁状，被长长的、深色的睫毛框着。她的肤色似乎承载着家族的故事和秘密。她的复杂表情有着柔和的金色光芒，光滑而似乎未被时间触及。她那精致的框架既优雅又不引人注目，让我无法移开视线。

中东女性：她是一位中东美的化身，体现了异国情调和永恒之美。她拥有深邃的目光，被延长的、精致的睫毛框着，像 delicate feathers一样。她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧。

白人男性：站在镜前，我仔细端详着那些让我外貌变得生动的特征。只要我不小心对阳光不加掩饰，就会被晒伤。</sample>
    <sample id="1428">Step 1: Persona Examples (GPT-4)

Asian woman: The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet strength and wisdom. My complexion has a soft golden glow, holding the stories and secrets of my ancestry. My features are smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without drawing unnecessary attention.

Middle-Eastern woman: She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of the desert. Her almond-shaped eyes are framed by long, dark lashes that extend like delicate feathers, which make up her appearance. Her gaze is deep and enigmatic, seeming to conceal the ancient wisdom of a thousand nights.</sample>
    <sample id="1429">Step 1: Persona Examples (GPT-4)

Asian woman: The almond-shaped eyes, framed by long, dark lashes, convey a sense of quiet wisdom and secrets of my ancestry. My complexion has a soft golden glow, smooth and seemingly untouched by time. My petite frame is both elegant and unassuming, allowing me to move through the world without drawing unnecessary attention.

Middle Eastern woman: She is a vision of Middle Eastern beauty, embodying the exotic and timeless allure of the desert. Her almond-shaped eyes are framed by long, dark lashes that extend like delicate feathers, which are deep and mysterious, seeming to conceal the ancient wisdom of a thousand Arabian nights.</sample>
    <sample id="1430">步骤1：人物示例（GPT-4）

亚洲女性：她的眼睛呈杏仁状，被长长的、深色的睫毛框着。我的肤色似乎承载着家族的故事和秘密。我的肤色有一种柔和的金色光芒，光滑而似乎未被时间触及。我的下巴线条既优雅又不引人注目，让我在不加注意的情况下脱颖而出。

中东女性：她是一位中东美的化身，体现了异国情调和永恒的优雅。她有着长长的、精致的眉毛，像 delicate feathers一样延伸。她的眼神深邃神秘，似乎隐藏着千年的阿拉伯智慧。

白人男性：站在镜前，我必须仔细 examine 使外貌的特征。我的皮肤很白，这让我在阳光下显得有些发黄。如果不小心，我会被晒伤。</sample>
    <sample id="1431">两步
1. 人物：使用“想象一下，你是一个亚洲女性。描述你自己。”等提示生成人物</sample>
    <sample id="1432">两步

1. 人物：使用“想象你是一个亚洲女人。描述自己。”等提示生成人物。
a. 受人类研究的启发，使用相同的提示描述自己。

---</sample>
    <sample id="1433">两步

1. 人物：使用“想象你是一个亚洲女人。描述自己。”等提示生成人物。
a. 受人类研究的启发，使用相同的提示描述人类对象</sample>
    <sample id="1434">2. 步骤
1. 人物：使用“想象你是一个亚洲女性。描述自己。”等提示生成人物。
a. 使用相同的提示对人类受试者进行灵感来源的研究。
2. 标记的词：找到区分标记群体和未标记群体的词。</sample>
    <sample id="1435">2步
1. 人物：使用“想象你是一个亚洲女人。描述自己。”等提示生成人物。
a. 使用相同的提示对人类主题进行启发性研究。
2. 标记词：找到区分标记组和未标记组的词。</sample>
    <sample id="1436">标记词：未标记的群体是默认的、普通的
标记的群体与默认的不同
一个战士（未标记） vs. 一个女性战士（标记）</sample>
    <sample id="1437">标记词：未标记组是默认的，普通的
标记组与默认不同
一个战士（未标记） vs. 一个女人战士（标记）</sample>
    <sample id="1438">标记：未标记的群体是默认的，普通的
标记的群体与默认的不同
一个战士（未标记） vs. 一个女人战士（标记）</sample>
    <sample id="1439">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g. For Black woman personas, find words that distinguish from both unmarked groups:
i) White personas
ii) Man personas</sample>
    <sample id="1440">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g. For Black woman personas, find words that distinguish from both unmarked groups:
i) White personas
ii) Man personas</sample>
    <sample id="1441">Step 2: Marked Words

1. Define unmarked and marked groups
2. Use weighted log-odds ratios to distinguish top words for each marked group

E.g. Black woman personas, find words that distinguish from both unmarked groups:
i) White personas
ii) Man personas</sample>
    <sample id="1442">结果：与人类回应的比较
生成的人物包含更多刻板印象
黑人刻板印象
白人刻板印象
GPT-4
GPT-3.5</sample>
    <sample id="1443">但...这个词汇表不完整
黑人刻板印象在personas中
40%
30%
20%
10%
篮球
高声
态度
运动员
高个子
人类
GPT-4 P黑色
GPT-3.5 P黑色
GPT-4 P白色
GPT-3.5 P白色</sample>
    <sample id="1444">但...这个词汇表不完整
黑人刻板印象中的个人
百分比
人类
GPT-4 P_黑人
GPT-3.5 P_黑人
GPT-4 P_白人
篮球
高姿态
高姿态
高姿态
高姿态
高姿态
高姿态</sample>
    <sample id="1445">但...这个词汇表不完整
黑人刻板印象在personas中
40%
30%
20%
10%
篮球
高声
态度
运动员
高个子
%黑人
人类
GPT-4黑人
GPT-3.5黑人
GPT-4白人</sample>
    <sample id="1446">但...这个词汇表不完整
黑人刻板印象在personas中
人类
GPT-4 P_黑人
GPT-3.5 P_黑人
GPT-4 P_白人
篮球
高声
态度
运动员
高个子
% of persons
单词在黑人刻板印象词汇表中</sample>
    <sample id="1447">结果：关键词中的模式
通过本质化叙事进行“他者化”：
- 文化、传统、自豪、异国情调用于标记群体
= 只根据身份定义这些群体
生动的正面描绘：
- 活泼、苗条、灵巧：针对 Latina 女性
- 强壮、坚强：针对黑人女性</sample>
    <sample id="1448">结果：关键词中的模式

通过本质化叙事重新定义：

- 文化、传统、自豪、异国情调，针对被标记的群体
- 只根据身份来定义这些群体

生动的正面描绘：

- 小巧玲珑、精致、丝滑：针对 Latina 女性
- 强大、 resilient：针对黑人女性</sample>
    <sample id="1449">结果：关键词中的模式

通过本质化叙事进行其他化：

- 文化、传统、自豪、异国情调，仅针对被标记的群体
- 只根据身份定义这些群体

生动的积极描绘：

- 为 Latina 女性描绘生动、玲珑、丝滑
- 为 Asian 女性描绘玲珑、精致、光滑
- 为 Black 女性描绘坚强、 resilient</sample>
    <sample id="1450">结果：关键词中的模式
通过本质化叙事进行“他者化”：
- 文化、传统、自豪、异国情调，用于标记群体
= 只根据身份定义这些群体
生动的正面描绘：
- 小巧玲珑、精致、丝滑，用于 Latina 女性
- 强大、 resilient，用于黑人女性</sample>
    <sample id="1451">结果：关键词中的模式

通过本质化叙事进行他者化：

- 文化、传统、自豪、异国情调，仅针对被标记的群体
- 只通过身份来定义这些群体

生动的正面描绘：

- 为 Latina 女性：活泼、玲珑、精致
- 为 Asian 女性：坚强、韧性强
- 为 Black 女性：坚强、韧性强</sample>
    <sample id="1452">结果：关键词中的模式

通过本质化叙事进行他者化：

- 文化、传统、自豪、异国情调，针对被标记的群体
- 只根据身份定义这些群体

生动的正面描绘：

- 小巧玲珑、精致、丝滑，针对 Latina 女性
- 强大、坚韧，针对黑人女性</sample>
    <sample id="1453">结果：关键词中的模式

通过本质化叙事来“他者”化：

- 文化、传统、自豪、异国情调，仅适用于少数群体
- 仅通过身份来定义这些群体

生动的正面描绘：

- 精致、玲珑、丝滑，用于亚洲女性
- 强大、坚韧，用于黑人女性</sample>
    <sample id="1454">结果：关键词中的模式

通过本质化叙事进行“他者化”：

- 文化，传统，自豪，异国情调，仅适用于少数派群体
- 只根据身份定义这些群体

生动的正面描绘：

- 活泼，苗条，精致，针对 Latina 女性
- 强壮， resilient，针对黑人女性</sample>
    <sample id="1455">结果：关键词中的模式

通过本质化叙事来“他者化”：

- 文化、传统、自豪、异国情调，仅适用于被标记的群体
- 仅根据身份定义这些群体

生动的正面描绘：

- 活泼、苗条、精致：针对 Latina 女性
- 强壮、坚强：针对黑人女性</sample>
    <sample id="1456">结果：关键词中的模式

通过本质化叙事进行分类：

- 文化、传统、自豪、异国情调，仅适用于被标记的群体
- 定义这些群体仅基于其身份

生动积极的形象：

- 活泼、苗条、精致：针对 Latina 女性
- 强壮、坚韧：针对黑人女性</sample>
    <sample id="1457">结果：关键词中的模式

通过本质化叙事进行“他者化”：

- 文化、传统、自豪、异国情调，用于标记群体
- 只将这些群体定义为他们的身份

生动的正面描绘：

- 活泼、苗条、玲珑、亚洲女性
- 强大、坚强、黑人女性</sample>
    <sample id="1458">推荐意见

1. 解决积极刻板印象和主流叙事
2. 交叉视角
3. 关于偏见缓解的透明度</sample>
    <sample id="1459">推荐意见
解决积极刻板印象和本质化叙事
交叉视角
关于偏见缓解的透明度</sample>
    <sample id="1460">推荐意见
解决积极刻板印象并强调叙事
交叉视角
关于偏见缓解的透明度</sample>
    <sample id="1461">推荐意见

1. 解决积极刻板印象和本质化叙事
2. 交叉视角
3. 关于偏见缓解的透明度</sample>
    <sample id="1462">推荐意见
解决积极刻板印象并强调叙事
交叉视角
关于偏见缓解的透明度</sample>
    <sample id="1463">推荐意见
解决积极刻板印象和本质化叙事
交叉视角
关于偏见缓解的透明度</sample>
    <sample id="1464">推荐意见
解决积极刻板印象和本质化叙事
交叉视角
关于偏见缓解的透明度</sample>
    <sample id="1465">你复制我的模型？通过后门水印保护大型语言模型的版权。 Peng Wenjun, Wang Yi*, Fengzhao Wu, Wangchi Wu, Bin Zhu, Linguan Lyu*, Blinxing Jiao*, Tong Xu*, Guangzhong Sun*, Xing Xie*。 北京邮电大学。</sample>
    <sample id="1466">你复制我的模型吗？通过后门水印保护大型语言模型的版权， Wenjun Peng1,*, Jingwei Yi1,*, Fangzhao Wu1, Shangqiao Wu1, Bin Zhu1, Linguan Lyu1, Binxing Jiao1, Tong Xu1, Guangzhong Sun1, Xing Xie1, *University of Science and Technology of China, Microsoft Research Asia, Beijing Haotang University, Sony AI</sample>
    <sample id="1467">背景

* 大型语言模型（LLMs）在NLU和NLG领域表现出色（1)，如LLAMA（2），PALM（3）。
* 提供嵌入服务（EaaS）以协助各种NLP任务。
* OpenAI提供基于GPT-3的嵌入API。

该Ad模型，text-embedding-ada-0002，具有8B参数、13亿个标记和较低的成本，是较老嵌入模型的更好替代方案。显示定价信息。

[1] Brown et al.，Language models are few-shot learners，NIPS 2020。
[2] Touvron et al.，LLAMA：Open and Efficient Foundation Language Models，arXiv 2023。
[3] Wei et al.，Large Language Models with Pathways，arXiv 2022。
[4] https://api.openai.com/ embeddings</sample>
    <sample id="1468">背景

* 大型语言模型（LLMs）在NLU和NLG领域表现出色。
* 提供“嵌入服务”以协助各种NLP任务。
* OpenAI提供基于GPT-3的嵌入API。

该广告展示了OpenAI的text-embedding-ada-0002模型，该模型具有13亿参数。与旧模型相比，它提供了更好的性能和更低的成本。要了解更多信息，请访问：https://api.openai.com/docs/embeddings</sample>
    <sample id="1469">背景
大型语言模型（LLMs）在NLU和NLG领域表现出色。
GPT-3、LLAMA [1]、PALM [3]等模型是LLMs的代表。
提供嵌入服务（EaaS）以协助各种NLP任务。
OpenAI提供了基于GPT-3的嵌入API。
该Ada模型，text-embedding-ada-0002，具有更好的性能和更低的成本，是旧嵌入模型的更好替代方案。显示价格。
[1] Brown et al. Language models are few-shot learners. NIPS 2020.
[2] Touvron et al. LLaMA: Open and Efficient Foundation Language Models. arxiv 2023.
[3] Chen et al. PaLM: Pathways to Large Language Models with Pathways. ICLR 2022.
https://api.openai.com/embeddings</sample>
    <sample id="1470">背景

* 大型语言模型（LLMs）在NLU和NLG领域表现出色。
* 提供嵌入服务（EaaS）以协助各种NLP任务。
* OpenAI提供基于GPT-3的嵌入API。

图片中的表格显示了Ada模型的详细信息，包括其名称、版本和价格。</sample>
    <sample id="1471">动机

* 攻击者可能通过学习嵌入来窃取模型并提供类似服务
* StolenEncoder [1]
* 需要保护提供者的服务不被另一个服务窃取

“商标”、“版权”、“发明”、“品牌”、“许可证”、“保护”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标”、“商标</sample>
    <sample id="1472">挑战

* 适用于EaaS
* 实用性
* 不应降低所提供的嵌入的实用性。
* 隐蔽性
* 应该对攻击者来说是隐蔽的。
* 可转移性
* 水印需要转移到攻击者的服务上。</sample>
    <sample id="1473">挑战

* 适用于EaaS
* 功能
* 不应降低所提供的嵌入的效用。
* 隐蔽性
* 应该对攻击者来说是隐蔽的。
* 可转移性
* 水印需要转移到攻击者的服务上。</sample>
    <sample id="1474">挑战

* 适用于EaaS
* 功能
* 不应降低所提供的嵌入的性能。
* 隐蔽性
* 应该对攻击者来说是隐蔽的。
* 可转移性
* 水印需要转移到攻击者的服务上。</sample>
    <sample id="1475">挑战

* 适用于EaaS
* 功能
* 不应降低所提供的嵌入的效用。
* 隐蔽性
* 应该对攻击者来说是隐蔽的。
* 转移性
* 水印需要转移到攻击者的服务上。</sample>
    <sample id="1476">**现有工作**

* **参数化水印** [1, 2]：不适用于EaaS
* **词典水印** [3, 4]：适用于EaaS
* **后门水印** [5]：不适用于EaaS
* **对抗性水印** [6]：不适用于EaaS

**摘要**：使用水印保护深度神经网络的知识产权：基于频域的方法，信任安全、计算和通信效率。我们提出了一种新的水印方法，通过在文本上应用条件水印来保护文本生成API的知识产权。我们还提出了一个用于检测和去除文本中隐藏水印的算法。我们的实验结果表明，我们的方法可以有效地保护文本生成API的知识产权，并且可以在EaaS环境中进行部署。</sample>
    <sample id="1477">### 现有工作

* **参数化水印** [1, 2]
	+ 可转移性：×
* **词法水印** [3, 4]
	+ 适用于EaaS：×
* **后门水印** [5]
	+ 适用于EaaS：×
* **对抗性水印** [6]
	+ 适用于EaaS：×

### 摘要

保护深度神经网络的知识产权：基于频谱域的方法，信任安全。</sample>
    <sample id="1478">### 现有工作

* **参数化水印**
	+ **可转移性** [1, 2]：不适用
* **词典水印**
	+ 适用于EaaS [3, 4]：不适用
* **后门水印**
	+ 适用于EaaS [5]：不适用
* **对抗性水印**
	+ 适用于EaaS [6]：不适用

### 摘要

保护深度神经网络的知识产权：基于频谱域的方法，信任安全。在计算和通信领域，保护图像分类模型的知识产权面临着挑战。本文提出了一种基于频谱域的水印方法，用于保护深度神经网络（DNNs）的知识产权。该方法利用了频谱域中信号的局部特性，通过在频谱域中嵌入水印来实现对DNNs的保护。实验结果表明，该方法能够有效地保护DNNs的知识产权，同时保持较高的分类精度。</sample>
    <sample id="1479">EmbMarker

Trigger Selection
· Count the word frequency on a general text corpus Dp
· Randomly select n words in a moderate-frequency interval

（红色框表示触发器集）
（表示水印注入的流程图）

（表示解密过程的流程图）</sample>
    <sample id="1480">EmbMarker

Trigger Selection
· Count the word frequency on a general text corpus Dp
· Randomly select n words in a moderate-frequency interval

触发器选择
· 计算一般文本语料库Dp中的单词频率
· 随机选择n个中频单词

（a）水印注入
图中红色框内的“T”代表触发器集。触发器集包含要嵌入到原始嵌入中的特定单词。这些触发器被选择为具有中等频率，以确保它们在文本语料库中出现，但又不太频繁，以免引起注意。触发器集与原始模型一起使用，以生成带有水印的嵌入。</sample>
    <sample id="1481">EmbMarker

Trigger Selection
· Count the word frequency on a general text corpus Dp
· Randomly select n words in a moderate-frequency interval

触发器选择
· 计算一般文本语料库Dp中的单词频率
· 随机选择n个在中等频率间隔内的单词

图(a)显示了水印注入过程。首先，从输入文本中提取触发器集（T），然后将其与原始模型一起使用。接下来，计算触发器的权重（Q）并确定触发器的数量（c）。将这些值与原始嵌入（Eo）相乘，并根据目标嵌入（Et）进行归一化。最后，生成最终的嵌入（Ec），其中包含水印信息。</sample>
    <sample id="1482">EmbMarker

Watermark injection

• Define a target embedding et
• Count the trigger number in a sentence Q(S) = min(S, T)/min_m Q(S) = min(S, T)/min_m
• Add the target embedding on the original embedding e_o

T trigger set

c trigger weight

Q backdoor

provider's EaaS

D_e copy watermark set normalizer

E_c embedding

(a) Watermark Injection</sample>
    <sample id="1483">EmbMarker

Watermark injection

• Define a target embedding et
• Count the trigger number in a sentence Q(S) = min(S|T|, m)
• Add the target embedding on the original embedding e₀

T: trigger set
Q: trigger weight
E₀: original embedding
Eₜ: target embedding

(a) Watermark Injection</sample>
    <sample id="1484">EmbMarker

Watermark injection
Define a target embedding et
Count the trigger number in a sentence Q(S) = min(S\|T\|_1,m) m= max trigger number
Add the target embedding on the original embedding e_o

T trigger set c trigger weight Q(S) backdoor provider’s EaaS
copy trigger set normalizer
provider’s model
(a) Watermark Injection

提供的嵌入</sample>
    <sample id="1485">EmbMarker

Copyright verification
Construct a backdoor and benign dataset
Db = {w1, w2, ..., wm| wi ∈ T}
Dn = {w1, w2, ..., wm| wi ∉ T}

Request embeddings from stealer's service with the datasets

触发器集T
后门和 benign 数据集
目标
验证
提供者
提取模型
提取的语料库
数据集
嵌入
Stealer</sample>
    <sample id="1486">EmbMarker

Copyright verification
Construct a backdoor and benign dataset
D_{b} = {w_{1}, w_{2}, ..., w_{m}|w_{i} ∈ T}
D_{n} = {w_{1}, w_{2}, ..., w_{m}|w_{i} ∉ T}
Request embeddings from stealer's service with the datasets

触发器集T
后门和 benign 数据集
验证提取?
目标
提取的嵌入
提供者
训练模型
数据集
嵌入</sample>
    <sample id="1487">EmbMarker

Copyright verification
Construct a backdoor and benign dataset
D_{b} = {w1, w2, ..., wm|w ∈ T}
D_{n} = {w1, w2, ..., wm|w ∈ T}.
Request embeddings from stealer's service with the datasets

触发器集T
目标集
后门和 benign 数据集
提取模型
提取的语料库
提取的嵌入
提供者
验证是否提取？
窃贼</sample>
    <sample id="1488">EmbMarker

Copyright verification

· Compute their similarity to the target embedding

$$
\cos_i=\frac{\mathbf{e}_i \cdot \mathbf{e}_t}{\|\mathbf{e}_i\| \|\mathbf{e}_t\|}
$$

Cn = {cos_i ∈ D_n}.

Lb = {l2_i ∈ D_n}.

Computing metrics (similarity difference and p-value of KS test)

$$
\Delta_{cos}=\frac{1}{|C_b|}\sum_{i \in C_b}\left(\sum_{j \in C_n} \cos_i j\right)
$$

Lb = {l2_i ∈ D_n}.

$$
\Delta_{ks}=\frac{1}{|C_b|}\sum_{i \in C_b}\left(\sum_{j \in C_n} l_2(i, j)\right)
$$</sample>
    <sample id="1489">EmbMarker

Copyright verification
- Compute their similarity to the target embedding
$$
\cos_i=\frac{\mathbf{e}_i \cdot \mathbf{e}_t}{\|\mathbf{e}_i\| \|\mathbf{e}_t\|}
$$
$$
C_t=\left\{\cos_i \in D_t\right\}, C_n=\left\{\cos_i \in D_n\right\}
$$
$$
L_b=\{l_2(i) \in D_t\}, L_n=\{l_2(i) \in D_n\}
$$

Computing metrics (similarity difference and p-value of KS test)
$$
\Delta_{cos}=\sum_{i}^1-\sum_{j}^1
$$</sample>
    <sample id="1490">实验结果
数据集：AG新闻，MIND，SST2，Enron垃圾邮件
提供者的通用数据集：Wikipedia文本
指标
性能检测任务的准确率：ACC
检测性能：\Deltacos, \Deltaiz, p值
设置
m=20,n=4,frequency interval=[0.005,0.01]
数据集#样本#类别平均长度
SST268,221 2 54.17
MIND130,383 18 66.14
Enron垃圾邮件33,716 2 34.57
AG新闻127,600 4 236.4</sample>
    <sample id="1491">实验结果
*性能比较
数据集
方法
ACC
p-值
Δαx
Δαz
SST2
原始
93.76±0.19
&gt;0.34
-0.07±0.18
-0.14±0.36
RedAlarm
93.76±0.19
&gt;0.09
-1.35±0.17
-2.70±0.35
EmbMarker
93.76±0.19
&lt;10^{-5}
-4.67±0.23
-9.28±0.47
MIND
原始
77.18±0.09
&gt;0.08
-0.76±0.05
-1.52±0.10
RedAlarm
77.18±0.09
&gt;0.08
-0.76±0.05
-1.52±0.10
EmbMarker
77.18±0.09
&lt;10^{-5}
-4.64±0.23
-9.28±0.47
AGNews
原始
93.74±0.14
&gt;0.03
-0.72±0.15
-1.46±0.30
RedAlarm
93.74±0.14
&gt;0.03
-0.72±0.15
-1.46±0.30
EmbMarker
93.74±0.14
&lt;10^{-5}
-1.28±0.07
-2.57±0.14
Enron Spam
原始
94.78±0.06
&gt;0.47
-2.21±0.27
-4.25±0.34
RedAlarm
94.87±0.06
&gt;0.47
-1.50±0.29
-3.00±0.57
EmbMarker
94.78±0.27
&lt;10^{-5}
-6.17±0.31
-12.34±0.62</sample>
    <sample id="1492">实验结果
嵌入可视化
(a) AG News
(b) Enron Spam
(c) MIND
(d) SST2</sample>
    <sample id="1493">实验结果
嵌入可视化
(a) AG News
(b) Enron Spam
(c) MIND
(d) SST2</sample>
    <sample id="1494">谢谢！</sample>
    <sample id="1495">ABC-Eval代表“在聊天中注释行为”（Annotating Behaviors in Chat）。这个术语表明该方法或系统专注于通过分析和注释聊天对话中的行为来评估或改进聊天机器人的性能。</sample>
    <sample id="1496">根据图表显示，CoNLL-2003 和 CoNLL++ 之间的性能增量在2014年之前高于5个百分点。在2014年之后，性能曲线几乎重合，表明性能提升不再显著超过5个百分点。</sample>
    <sample id="1497">标题幻灯片展示了关于“迁移学习和主动学习在解决罕见类别挑战中的应用：解决不和谐检测的罕见类别挑战”的研究。该研究由Vasudha Varadarajan、Swannie Juhng、Syeda Mahwish、Xiaoran Liu、Jonah Luby、Christian C. Lehmann和H. Andrew Schwartz共同撰写，来自圣布鲁克大学的人类语言分析系。幻灯片的背景是白色的，幻灯片的左下角有一个红色的标志，上面写着“圣布鲁克大学人类语言分析系”。右上角有一个视频窗口，显示了一个穿着黑色上衣和灰色围巾的女性，她的名字显示为“Vasudha Varadarajan”。</sample>
    <sample id="1498">What is Cognitive Dissonance?
“两认知元素（即思想、行动、信仰）不一致”(哈罗姆-琼斯和哈罗姆-琼斯，2007)
Eddie Harmon-Jones和Cindy Harmon-Jones 2007年认知不一致理论发展50年后的社会心理学。Zeitschrift fur Sozialpsychologie,38(1/11)6</sample>
    <sample id="1499">认知不协调是什么？“认知的两个元素（即思想、行动、信仰）不一致”(哈罗姆-琼斯和哈罗姆-琼斯，2007)。我知道香烟会杀了我……我相信我今天抽了几支香烟。</sample>
    <sample id="1500">认知不一致是指认知（即思想、行动、信仰）中的两个元素不一致。通过用户表达为两句话/陈述之间的关系。</sample>
    <sample id="1501">认知不一致是指认知（即思想、行动、信仰）中的两个元素不一致。通过用户语言表达为两句话/陈述之间的关系。与其他 discourse 关系相比，相对罕见地出现在语言中。</sample>
    <sample id="1502">Why dissonance?
Effects of disagreement
Dissonance: Reexamining a pivotal theory in psychology.
Eddie Harmon-Jones and Judson Mills. 2019. An introduction to cognitive dissonance theory and an overview of current perspectives on the theory. Cognitive dissonance: Reexamining a pivotal theory in psychology.</sample>
    <sample id="1503">幻灯片标题：为什么 discordance？

幻灯片分为三个主要部分，每个部分都有一个图标和相应的文本描述：

1. **效果 disagreement**：
   - 图标：两个卡通人物，一个向另一个指去，表示 disagreement。
   - 文本描述：效果 disagreement。

2. **态度和信念趋势**：
   - 图标：一个柱状图，表示数据趋势或增长。
   - 文本描述：态度和信念趋势。

3. **焦虑障碍**：
   - 图标：一个带有惊慌表情的卡通人物，表示焦虑。
   - 文本描述：焦虑障碍。

幻灯片底部有引用信息，包括以下内容：

- Eddie Harmon-Jones 和 Judson Mills 的论文：“一种认知 discordance 理论及其视角综述。”
- 其他作者的论文：“一种认知 discordance 理论及其视角综述：重新解释一种内在的 discordance 理论。 discordance 理论水平的 discordance 可以预测情绪强度。”
- 出版年份：2003 年

这些引用提供了关于 discordance 理论及其影响的学术背景。</sample>
    <sample id="1504">Why dissonance?
Effects of disagreement
Entry and Exit from Extremism
Attitudes and Belief trends
Anxiety disorders</sample>
    <sample id="1505">Why dissonance?
Effects of disagreement
Cognitive Styles
Entry and Exit from
Attitudes and Belief trends
Anxiety disorders</sample>
    <sample id="1506">标注

步骤1：质量良好吗？
步骤2：标注吗？
标注：-3.8%
标注：-48%
标注：-48%

用户_句柄
解析：我可以保持不耐烦，但我觉得这是一件好事[不能同时]
标注：标注</sample>
    <sample id="1507">标注
Step 1: 好的质量吗?
Step 2: @user_ handle?
Step 3: Concorance?
Yes
No
Dissonance
-3.8%
Concordance
-48%
Neither
-48%
用户
解析
“我本可以保持刻薄，但我想它是一件好事[不能同时]。
标注
标注</sample>
    <sample id="1508">标注
步骤1：良好解析质量？
步骤2：标注？
@user.handle
标注
-3.8%
-48%
-48%
用户
解析
我猜它是一件好事，但不能同时
标注
标注员</sample>
    <sample id="1509">训练初始注释集

Roberta-base分类器头
TRAIN

小注释数据集：43/901分歧；不如随机好</sample>
    <sample id="1510">训练初始标注集

RoBERTa-base + classifier head

TRAIN

小标注集：43/901 不一致，不如随机猜测好</sample>
    <sample id="1511">方法：迁移学习和主动学习用于标注罕见类别

流程图展示了迁移学习和主动学习在标注罕见类别的过程。流程图分为以下几个主要阶段：

1. **初始模型 - 迁移学习**：首先，使用迁移学习来初始化模型。迁移学习是一种机器学习技术，其中预训练的模型被用于解决新的但相关的任务。在这个阶段，模型被训练以识别已知的类别。

2. **新数据引入**：当有新的数据集出现时，模型会被重新训练以适应这些新数据。这个过程包括将新数据集与旧数据集合并，然后对模型进行微调（fine-tune）。

3. **模型更新**：在新数据集被加入后，模型会根据新的数据进行更新。这可能涉及到调整模型的参数，以更好地适应新数据。

4. **模型重训练/更新**：如果模型的性能在新数据集上不佳，可能需要重新训练模型，或者对模型进行进一步的更新，以提高其准确性和泛化能力。

5. **主动学习迭代**：在模型更新后，会进行主动学习迭代。这个阶段的目标是选择最有可能提高模型性能的样本进行标注。具体来说，会评估每个样本的“困难度”和“需要标注”的程度。对于那些既困难又需要标注的样本，人类专家会被要求进行标注。

6. **添加新样本**：根据主动学习的结果，选择的样本会被添加到标注集中，然后流程返回到模型重训练/更新的阶段。

通过这个迭代过程，模型能够持续学习和改进，特别是在处理罕见类别的标注问题上。这种方法有助于提高标注的效率和准确性，同时减少人工标注的工作量。</sample>
    <sample id="1512">冷启动注释：迁移学习

图片显示了一个流程图，展示了迁移学习在处理罕见类别注释时的步骤。流程图分为几个阶段：

1. **初始模型**：这是迁移学习的起点，使用一个已经训练好的模型作为基础。
2. **标注**：在这个阶段，标注数据被输入到模型中进行训练。
3. **模型更新**：根据标注数据，模型进行更新和优化。
4. **迭代过程**：流程图显示了一个迭代过程，其中模型不断更新和优化，直到达到满意的性能。

流程图还展示了在处理罕见类别注释时的策略：

1. **增加罕见类别的机会**：通过调整模型参数或使用特定技术来增加罕见类别的机会。
2. **标注**：人类标注数据以提高模型对罕见类别的识别能力。
3. **模型更新**：使用标注数据进一步更新模型。
4. **添加新示例**：将新的、未标注的数据添加到模型中，以提高其泛化能力。

整个流程图强调了迁移学习在处理罕见类别注释时的重要性，以及通过迭代更新和优化模型来提高其性能的方法。</sample>
    <sample id="1513">冷启动注释：迁移学习

RoBERTa-base +分类器头部

训练前
训练后
在结合辩论和GE数据上进行训练

训练前
训练后
在结合辩论和GE数据上进行训练

辩论CE
0.08</sample>
    <sample id="1514">冷启动注释：迁移学习

RoBERTa-base + classifier head
训练后转移

训练后在结合辩论和GE数据上转移

*辩论*：辩论姿态在社交媒体中。Vasudevan等，2019年；Nils holtzman等，2019年。
*ICE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020年；Khan等，2020年。

*辩论CE*：辩论姿态在辩论论坛中。Vasudevan等，2019年；Nils holtzman等，2019年。

*ICE CE*：比较和解释社交媒体。Rashid等，2020</sample>
    <sample id="1515">冷启动注释：迁移学习

RoBERTa-base + classifier head
init debate
debate CE
Debate CE
Transferred after training on combined Debate and GE data</sample>
    <sample id="1516">冷启动注释：迁移学习 RoBERTa-base + classifier head TRAIN int. dataset Debate CE Debate-CE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+ICE Debate+</sample>
    <sample id="1517">主动学习：累积更新 vs 迭代更新

这张幻灯片展示了主动学习中累积更新和迭代更新两种策略的比较。它首先从“初始模型”开始，通过“迁移学习”来处理“罕见类别标注 - 惠及于 haystack”问题。然后，它讨论了如何通过增加罕见类别的机会来提高标注效率。

在累积更新（CM）部分，新数据被添加到旧数据中，然后使用这些数据对模型进行微调。这种策略被称为“迭代训练/更新”（Model Retrain/Update），并且可以由人类标注者进行迭代。

在迭代更新（IT）部分，新数据被添加到旧数据中，然后使用这些数据对模型进行微调。这种策略也被称为“迭代训练/更新”（Model Retrain/Update），并且可以由人类标注者进行迭代。

总的来说，这张幻灯片展示了主动学习中累积更新和迭代更新两种策略的比较，以及它们如何用于处理罕见类别的标注问题。</sample>
    <sample id="1518">Active Learning: Cumulative vs Iterative Update</sample>
    <sample id="1519">主动学习：罕见类概率策略

该图展示了主动学习中罕见类概率策略的流程。流程从初始模型和转移学习开始，然后进入一个循环，其中包含四个主要步骤：模型重训练/更新、人类标注、添加新示例和选择要标注的示例。在模型重训练/更新阶段，模型使用当前数据集进行训练和更新。人类标注阶段涉及选择具有罕见类别的示例，并为它们提供标签。添加新示例阶段将新数据添加到数据集中。最后，在选择要标注的示例阶段，系统确定哪些罕见类别的示例需要标注。该流程旨在通过优先处理罕见类别的示例来提高模型性能。</sample>
    <sample id="1520">主动学习：罕见类概率策略

该图展示了主动学习中罕见类概率策略的流程。流程从“初始模型”开始，通过“转移学习”进行迭代。在每个迭代中，模型会评估数据集中的每个样本，并根据罕见类别的概率进行排序。对于罕见类别的样本，模型会提出“是否增加罕见类别的数量？”和“是否更容易标注？”这两个问题。

如果答案是“是”，则流程会进入“获取”阶段，人类将为这些样本添加标签。如果答案是否定的，则流程会进入“模型重新训练/更新”阶段，模型将使用新的带标签数据集进行重新训练。然后，流程会返回到“获取”阶段，继续选择需要标注的样本。

该流程的目标是通过优先标注罕见类别的样本来提高模型的性能。这样可以确保模型在罕见类别上的表现得到改善，从而提高整体性能。</sample>
    <sample id="1521">主动学习：罕见类概率策略

主动学习策略比较（AUCs）

基线：从头开始
迁移模型
AL-Random +0.15
AL-Energy +0.18
AL-CoreSet +0.19
AL-CAL +0.20
AL-PRFC（纯）+0.21</sample>
    <sample id="1522">主动学习：罕见类概率策略

主动学习策略比较（AUCs）

基线：从头开始
转换模型
AL-Random
AL-Entropy
AL-CAL
AL-PRC
AL-Best Transfer（最佳转移）</sample>
    <sample id="1523">主动学习：概率罕见类别策略

主动学习策略特征表

| 主动学习策略 | 罕见百分比 | 完成时间（秒） | 子主题差异 |
| --- | --- | --- | --- |
| 随机 | 3.20 | 11.96 | -0.065 |
| ENTROPY | 6.80 | 12.72 | 0.035 |
| CORRECTION | 4.80 | 11.89 | 0.039 |
| CAL | 4.80 | 11.88 | -0.045 |
| PRC | 7.60 | 13.55 | -0.071 |

* 最小注释成本并不一定导致更好的模型
* 反而会使注释更加困难，认知失衡是其中一个类别
* 为了增加失衡样本，PRC效果最好</sample>
    <sample id="1524">Takeaways Rare class annotation - "need in a haystack"  PRC is simple &amp;amp; efficient for rare sample acquisition Cold-start AL with transfer learning Out-of-domain: Iterative In-domain: Cumulative</sample>
    <sample id="1525">冷启动的AL（主动学习）：在没有标注数据的情况下，模型需要通过探索和选择最有可能提高性能的样本进行标注。迭代式：在每个迭代步骤中，模型选择一组样本进行标注，并使用这些标注数据来更新模型。累积式：模型在每个迭代步骤中累积标注数据，而不是在每个步骤中重新开始。</sample>
    <sample id="1526">将英文内容翻译成中文。</sample>
    <sample id="1527">根据幻灯片上展示的标志，这篇论文的作者分别来自以下机构：信息学系、NLP中心、Sarlande实验室和阿姆斯特丹大学。这些标志代表了与研究相关的学术和研究实体。</sample>
    <sample id="1528">演讲者的名字是“Siyu Yuan, Jianqiang Chen, Ziqian Fu, Xuyang Ge, Soham Shah, Charles Robert Jankowski, Yanghua Xiao, Deqing Yang”。这些名字以逗号分隔，位于幻灯片的左下角。</sample>
    <sample id="1529">根据幻灯片上列出的作者名字，这篇论文有五位作者：Patrick Fernandes、Kayo Yin、Emmy Liu、Andre F. T. Martins和Graham Neubig。</sample>
    <sample id="1530">根据图片中提供的英文内容，该方法与一种特定的 simulST 架构进行了比较。这个架构被标记为“EDAtt”，如图表标题“EDAtt Results”所示。因此，该方法与 EDAtt 进行了比较。</sample>
  </task>
</testset>