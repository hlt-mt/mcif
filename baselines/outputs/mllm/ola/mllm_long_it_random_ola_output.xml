<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su largi quantità di dati web, tra cui notizie politiche.</sample>
    <sample id="1">Maggell University, Meila e Microsoft Research</sample>
    <sample id="2">The presentation introduces a new multi-modal pre-training model called LayoutMask, which aims to address the reading order issues in visualized document understanding. Unlike existing models that use global word segmentation for reading order, LayoutMask uses local word segmentation and infers global reading order by jointly using 1D and 2D positioning and semantic information. The model employs two novel masking strategies: whole word masking and layout-aware masking, which are designed to promote text layout interactions during pre-training. The presentation also compares the performance of LayoutMask with different layout information for 1D positioning, including local 1D positioning, global 1D positioning, and no 1D positioning, on FD and SRIE datasets. The results show that LayoutMask performs better when using global 1D positioning, especially in cases where there are multiple misleading numbers with the same content as the ground truth.</sample>
    <sample id="3">Certo! Ecco la versione in italiano:

Benvenuti alla nostra presentazione di "DePlain", un nuovo corpus per la semplificazione del testo in tedesco al livello dei documenti e al livello delle frasi. Il mio nome è Regina Störrn e mi occuperò della prima parte della presentazione.

Prima di tutto, definiamo la semplificazione del testo. La semplificazione del testo è il processo di adattare un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con problemi di lettura o non nativi di lingua. Per addestrare un modello di semplificazione del testo, ci sono necessari coppie parallele di testi, ad esempio due documenti o due frasi. In questo esempio, si vede una coppia allineata di frasi, dove una frase complessa in tedesco è tradotta in una lingua più semplice. Per semplificare la frase, diverse tecniche sono possibili, come la sostituzione lessicale, la omessa, la riformulazione o l'inserzione di parole. Ora proponiamo il nostro nuovo corpus, DePlain, perché in recenti anni c'è stato un problema con i corpi esistenti. Ad esempio, questi corpi hanno dimensioni troppo piccole per addestrare un modello di semplificazione del testo. Altri modelli proposti recentemente sono tutti automaticamente allineati, cosa che significa che possono essere sbagliati nella loro allineazione. Pertanto, proponiamo il nuovo corpus DePlain, che è diviso in due sottocorpi: DePlain API e DePlain Web. DePlain API è basato su testi nuovi. In DePlain API, 483 documenti sono stati allineati manualmente, producendo circa 30.000-13.000 coppie di frasi allineate. Per DePlain Web, il corpus include diverse domande e allinea 750 documenti manualmente e automaticamente. In totale, si ottiene un numero di 30.450 coppie di frasi. Analizziamo le nostre coppie di frasi un po' di più. Ad esempio, sul tipo di semplificazione, si può vedere che i testi biblici sono molto più semplificati rispetto a testi nuovi o testi per apprendimento di lingua. Questo è vero per tutti i livelli di semplificazione, come la semplificazione lessicale, la semplificazione strutturale o il livello globale di semplificazione. Inoltre, si può vedere che il corpus DePlain ha una grande varietà di diverse trasformazioni di semplificazione. Ad esempio, in DePlain API, ci sono molti più riformulazioni e modifiche di parole rispetto a DePlain Web. D'altra parte, in DePlain Web, ci sono molti più riformulazioni. Ora, vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parlerò dei casi d'uso per il nostro dataset DePlain. Per il primo caso d'uso, possiamo valutare metodi di allineamento automatici. In recenti anni, ci sono stati molti metodi di allineamento, ma in contesto di traduzioni automatiche, dove abbiamo due documenti scritti in lingue diverse e vogliamo estrarre allineamenti di frasi in posti documenti. Ma nel nostro caso, stiamo cercando di estrarre allineamenti tra frasi di due documenti paralleli, che hanno lo stesso linguaggio e lo stesso contenuto, ma sono a livelli di difficoltà diversi. Ora che abbiamo il corpus DePlain, che ha allineamenti manuali, possiamo utilizzare queste frasi come standard di riferimento per valutare i metodi di allineamento proposti. Abbiamo fatto delle modifiche ai metodi proposti e pubblicato tutti i codici per eseguire i nostri esperimenti nel paper. Alla fine, concludevamo che il miglior metodo di allineamento automatico per testi di semplificazione in tedesco è il metodo MassAlign. Puoi anche trovare il codice per eseguire questo metodo su documenti personali nel paper. Il secondo caso d'uso che ho illustrato nel mio paper è il caso di semplificazione automatica, cioè di addestrare modelli linguistici per produrre testi semplificati da testi complessi. Abbiamo addestrato due modelli: uno per produrre semplificazioni al livello dei documenti e l'altro per produrre semplificazioni al livello delle frasi. Puoi anche trovare i punti di controllo e i dettagli maggiori dei punteggii e dei metri di valutazione dei nostri esperimenti nel paper. Conclusamente, questa basic fine-tuning può produrre o ottenere punteggii migliori dei punteggii di base e proponiamo questi risultati come una marcatura di riferimento per il problema di semplificazione automatica del testo in futuro. Grazie mille per la vostra attenzione e spero di incontrarvi tutti durante la conferenza. Grazie.</sample>
    <sample id="4">Kayo Yin</sample>
    <sample id="5">Per ottenere l'accuratezza dell'82%-87%, hanno utilizzato un modello con accesso a conoscenze parzialmente sovrapposte.</sample>
    <sample id="6">The speaker introduces a study on unifying multilingual and cross-lingual summarization, presenting contributions to the field. They propose a multi-to-many summarization model that can generate summaries in any target language from any source language. The study includes preliminary analyses comparing multilingual, cross-lingual, and multi-to-many summarization models, showing that multi-to-many summarization facilitates better task knowledge transfer across languages. The presentation also discusses a three-stage training process for the proposed model, which outperforms previous models like MBT50 and MT5.</sample>
    <sample id="7">Sì, i tagger CoNLL-2003 funzionano ancora.</sample>
    <sample id="8">Il nuovo approccio proposto è quello di valutare esplicitamente se o meno ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni irrelevanti o contradire se stessa o il proprio partner.</sample>
    <sample id="9">Il successo dell'approccio scarsamente supervisionato si basa in larga misura sulla quantità di campioni puliti utilizzati per la selezione del modello.</sample>
    <sample id="10">Per migliorare il punteggio, i progressi che possono essere fatti includono l'accesso a informazioni di sfondo, come link di ricerca o immagini, per aiutare i modelli a comprendere meglio le entità.</sample>
    <sample id="11">Jack Hessel, a researcher at AI2, presents Do Androids Laugh at Electric Sheep? humor understanding benchmarks from The New Yorker Caption Contest. Large language models can generate and explain jokes, but their understanding of humor is questioned. The New Yorker Caption Contest data was operationalized into three tasks: matching, quality ranking, and explanation generation. Models like GPT-4 struggle with these tasks, highlighting the gap in humor understanding between humans and language models.</sample>
    <sample id="12">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="13">Daniel Rotem presented his work on adaptive inference in low resource settings, which was conducted in Professor Roy Schwartz's lab at Hebrew University in Jerusalem. Adaptive inference is a method for reducing the inference time of large language models by using low-capacity models for easy samples. Two common methods are multimodal and early exit. Multimodal involves storing multiple models, each with a classifier, trained separately and run sequentially during inference. Early exit fits multiple classifiers to the model after intermediate transformer layers, running a sample through the model until a classifier decides to halt. The study found that multimodal is more versatile but expensive and suffers from overhead, while early exit is faster and memory-efficient but can lead to lower performance due to conflicting gradients. The team proposed Sweet, a novel fine-tuning method for early exit architectures, which avoids conflicting gradients and closes the gap between early exit and multimodal.</sample>
    <sample id="14">Il mio nome è Adam Sipruckowsky e questo talk è su la struttura di dipendenze della coordinazione. Come potete vedere, ci sono diverse strutture di dipendenze assunti da diverse teorie e approcci concorsi. Ad esempio, in teoria universale delle dipendenze, la struttura di coordinazione tra Lisa, Bart e Maggie è tale che il primo congiunto è la testa della struttura di coordinazione, ovvero Lisa. Un approccio simile è assunto in teoria dei significati di Igor Miltruk, dove di nuovo il primo congiunto è la testa della struttura di coordinazione. Questi due approcci sono asimmetrici, singolano un dei congiunti. Ci sono anche strutture di coordinazione simmetriche, come l'approccio prague di coordinazione unita, che ha la congiunzione come testa della struttura di coordinazione. In quest'approccio si ottiene una dipendenza da un verso a tutti i congiunti. Infine, c'è anche un approccio multi-testa che viene usato ad esempio in grammatica di De Catts, dove tutti i congiunti sono testa della struttura di coordinazione. Qui si ottiene una dipendenza dal governante, qui Luffy, a tutti i congiunti separatamente. Ora, lo scopo del mio articolo è quello di fornire un nuovo argomento per le strutture di coordinazione simmetriche come queste due, contro le strutture di coordinazione asimmetriche come queste due. L'argomento è basato sul principio di minimizzazione delle dipendenze, che spiega sulla base di questi esempi. In inglese, come potete vedere, gli oggetti diretti preferiscono di essere vicini al verbo, mentre gli avverbi possono essere più lontani. Quindi, March read it yesterday è corretto, perché l'oggetto direttore "it" è vicino al verbo, mentre March read yesterday it è peggio, perché tra il verbo e l'oggetto direttore c'è un avverbio "ieri". Tuttavia, quest'effetto può essere attenuato quando l'oggetto direttore è pesante e lungo, poiché allora può essere spostato dopo l'avverbio. Questo è illustrato qui: entrambe queste frasi sono corrette. March read this absolutely fascinating book about the bees yesterday. Ok, nel primo caso代替 "it"abbiamo questa lunga NP, ma è anche corretto dire March read yesterday this absolutely fascinating book about bees. Quindi, la ragione qui è che questa è possibile, poiché anche se questa frase viola il principio grammaticale che gli oggetti diretti dovrebbero essere vicini al verbo, soddisfa il principio di minimizzazione delle dipendenze, che dice che le dipendenze più brevi sono preferite. Quindi, queste due strutture solo mostriamo la lunghezza delle dipendenze cruciali, ovvero le dipendenze che non sono costanti tra queste due strutture. Quindi, qui abbiamo la dipendenza da red a book di lunghezza 7 misurata in parole e da red a book di lunghezza 4, quindi insieme è 11. Quando si scambia o si scambia queste due costituenti, la somma di queste due dipendenze diventa 6. Quindi invece di 11, 6, molto più breve, è per questo che questa suona quasi corretta. Viola un principio, ma soddisfa un altro. Quindi, cosa che facciamo, estraeiamo statistiche molto precise da coordinate in versione migliorata di Pank e vediamo il paper per vedere se non usare universale dipendenze. Queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono a essere più corti. Così salt and pepper and not pepper and salt misurato in sillabare. E anche l'osservazione che è stata fatta in passato che questa tendenza cresce con la lunghezze differenza. Quindi, quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto sinistro preferisce essere il primo, più forte. Quindi, la proporcione è più grande dei congiunti sinistri corti. Ma, cosa nuovo in questo paper, è che osserviamo che questa tendenza si manifesta solo quando il governante sinistra è assente. Quindi, il governante sinistra in questo esempio, i congiunti Lisa, Bart e Maggie, il governante sinistra è assente in questo esempio, Homer came and sneezed, qui ci sono due congiunti di verbi e non c'è un esterno governante esterno. Quindi, in queste case, il congiunto sinistro preferisce essere più corto, tanto più il maggior differenza tra i due congiunti. Tuttavia, quando il governante sinistra è sulla destra, come qui, Luffy governa la coordinazione tra Lisa e Maggie, quest'effetto scompare. Quindi, dimostriamo che, misurando la lunghezze in caratteri, la prima colonna in sillabare, la seconda colonna in parole, la terza colonna, mi concentro sulla terza colonna. Quello che vediamo qui è che quando il governante sinistra è assente, la tendenza per il congiunto sinistro di essere più corto cresce gradualmente con la differenza assoluta in parole. E la stessa è osservata quando non c'è governante, come in coordinazione di frasi, ma quando il governante sinistra è sulla destra, quest'effetto scompare. E dimostriamo nel paper come questo fornisce un argomento contro strutture di coordinazione asimmetriche come queste due e forza strutture simmetriche come queste due. Quindi, vedere il paper per il full agreement e argomenti, scusate, e parlare con noi durante la sessione poster. Grazie mille.</sample>
    <sample id="15">Tre</sample>
    <sample id="16">I domini dei testi biblici risulteranno più semplificati rispetto a quelli dei testi di news o dei testi per apprendimento di lingua.</sample>
    <sample id="17">The speaker introduces a method for multi-modal relation extraction, which involves combining textual and visual data to better understand relationships between entities in various forms. The method includes fine-grained information pruning over two modalities and considering external information such as topic information to address issues of internal information over-utilization and external information under-exploitation. Experiments on the MRQA dataset show that the proposed method outperforms text-based methods and achieves significant improvements over existing models.</sample>
    <sample id="18">Salt and pepper</sample>
    <sample id="19">The speaker introduces a survey on efficient open-domain question answering, focusing on the two-stage model by Dandan Chen. The first stage retrieves context from Wikipedia using a retriever, and the second stage uses a reader to understand questions and retrieve relevant answers. Challenges include handling large datasets, indexing files, and managing multiple language models. The presentation summarizes techniques for improving retrieval and reading efficiency, such as approximate nearest neighbor search, skipping rate, document filtering, and model size reduction. The analysis shows that retrieval systems are better for limited resources, while generative systems are more suitable for real-time feedback. Future work includes deploying systems on low-power devices and considering new evaluation metrics.</sample>
    <sample id="20">Sì, i modelli sono liberi di essere utilizzati per la ricerca.</sample>
    <sample id="21">DEplain-apa si basa su testo di notizie.</sample>
    <sample id="22">Per una buona generalizzazione, i fattori principali sono l'architettura del modello, la dimensione del modello e l'esempio di finetuning.</sample>
    <sample id="23">Researchers have made significant progress in text-image modeling, enabling the creation of high-quality images. However, these models often struggle with representing text accurately. The Imagen model, for instance, fails to generate images containing words, even when the input is simple. This issue arises from the text encoder's inability to decompose subword tokens into individual letters. The paper explores this problem and proposes a new strategy by concatenating a character-aware model like Bit5 with the existing text representation. This approach improves the model's ability to spell words correctly, enhancing image generation characteristics and rendering text more accurately.</sample>
    <sample id="24">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata utilizzando statistiche estratte da una versione migliorata del Corpus di Parole. Queste statistiche hanno confermato l'osservazione precedente che i congiunti a sinistra tendono ad essere più brevi, in quanto le loro lunghezze sono state mesurate in sillabe e in caratteri.</sample>
    <sample id="25">Gli esperimenti sono stati progettati utilizzando statistiche estratte da una versione migliorata del Corpus di Pankoff e analizzando le differenze nella lunghezza tra i due congiunti.</sample>
    <sample id="26">Un classificatore base addestrato su un insieme di dati non bilanciato non è molto più efficiente rispetto a un modello casuale.</sample>
    <sample id="27">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="28">Bob e Alice</sample>
    <sample id="29">I modelli di MT sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto in fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="30">The paper introduces a framework called LLM-Blender for ensemble learning of large language models (LLMs). It uses pairwise ranking and generative fusion to select the best model for each input. The framework runs multiple models on an input, ranks them using a pairwise ranking module, and then fuses the top models using a generative fusion model. Experiments show that LLM-Blender outperforms individual models like Open Assistant and Vicuna in 68% and 76% of examples, respectively.</sample>
    <sample id="31">I ricercatori che hanno scritto l'articolo sono affiliati a Johns Hopkins University, Purdue University e MIT.</sample>
    <sample id="33">Il framework quantifica la posizionalità analizzando come i dataset e i modelli si alliniano a specifici gruppi di popolazione, come le lingue e gli altri fattori.</sample>
    <sample id="34">In this video, a person is presenting a work called Crest, which is a joint framework for rationalization and counterfactual text generation. The speaker explains that Crest combines selective rationalization with counterfactual generation to produce high-quality counterfactuals. The framework uses a rationale generator to create counterfactual examples based on input tokens, and then uses a predictor model to produce a decision. The speaker also discusses the use of Crest for data augmentation and shows results from experiments on different datasets. Overall, Crest produces valid, fluent, and diverse counterfactuals in a controllable way, leading to plausible explanations that focus on the contrastive parts of the input.</sample>
    <sample id="36">The paper presents a method for improving multilingual machine translation by using language-specific layers (LSLs) in a transformer model. The approach involves training one regular transformer layer per language, which can be either the source or target language. At inference time, only the selected sub-layer is called, keeping the inference cost constant. The paper explores different LSL placement strategies and selects the best one based on weight importance. The results show significant improvements over baseline models and language adapters, especially for low-resource languages.</sample>
    <sample id="37">Il risultato dello studio precedente è stato che i soggetti umani hanno anche rivelato stereotipi rivelatori.</sample>
    <sample id="38">In questo studio sono state utilizzate statistiche estratte da una versione migliorata del Corpus di Parole.</sample>
    <sample id="39">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="40">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di dichiarazioni indipendenti e la classificazione binaria di espansione e confronto.</sample>
    <sample id="41">The presentation introduces PEACoK, a personal common sense knowledge graph designed to enhance narrative consistency and engagement. Developed in collaboration with Sony Group Corporation, PEACoK aims to improve natural language processing systems by providing rich, interconnected person knowledge at scale. It contains 3,800 personas, 40,000 attributes, and 100,000 person inferences, with about 9,200 attributes connected to two or more personas. The system is built in three steps: selecting personas from existing knowledge graphs, inducing attributes using large-scale pre-trained language models, and annotating relations through a human-AI majority voting scheme. Studies show that PEACoK can effectively train language models for person attribute inference tasks and improve dialogue generation by augmenting speaker profiles with relevant facts from the knowledge graph.</sample>
    <sample id="42">Due.</sample>
    <sample id="43">Ci sono sette autori che hanno collaborato all'articolo.</sample>
    <sample id="44">Il framework differisce dai precedenti in quanto è stato progettato per adattarsi a documenti e frasi di diversi livelli di difficoltà, non solo a documenti e frasi di diversi livelli di difficoltà.</sample>
    <sample id="45">Lessico di stereotipi</sample>
    <sample id="46">Il benchmark ha messo a confronto DeepL e Google Translate per document-level translation.</sample>
    <sample id="47">Ciao, mi chiamo Changbing, sono uno studente di PhD all'Università di Washington. Oggi presento il mio lavoro sulla trasformazione dei dati pre-addestrati in modelli di linguaggio e sulle applicazioni downstream, analizzando i percorsi dei bias politici che portano a modelli NLP non equi. I modelli di linguaggio sono addestrati su grandi quantità di testi web, e le notizie politiche sono bene rappresentate nei loro dataset di addestramento. Secondo un saggio del C4 Corpus, i principali giornali di notizie come il New York Times, Los Angeles Times, The Guardian e Huffington Post sono ben rappresentati nel dataset di addestramento dei modelli di linguaggio. Questo ha creato un mix di benessi per le applicazioni dei modelli di linguaggio: da un lato hanno imparato a trarre informazioni da diverse prospettive, celebrando la democrazia e la pluralità delle idee; dall'altro hanno acquisito opinioni politiche socialmente biased, che potrebbero portare a problemi di giustizia in applicazioni downstream. Per risolvere questo problema, abbiamo proposto di indagare il percorso dei bias politici dalla data pre-addestrata ai modelli di linguaggio e alle applicazioni downstream, specificamente chiedendo: primo, come valutiamo la linie politiche dei modelli di linguaggio e in che modo i dataset di addestramento influenzano queste linee politiche? Secondo, come i modelli di linguaggio con diverse linee politiche performano sulle applicazioni downstream e se queste differenze potrebbero portare a problemi di giustizia in applicazioni NLP? Abbiamo proposto di stimare i modelli di linguaggio con diverse formule di prompt utilizzando domande politiche come il test di compasso politico, per garantire un valutazione automatica basata sulla letteratura di scienze politiche. I risultati preliminari dimostrano che i modelli di linguaggio hanno linee politiche varianti, occupando tutti i quattro quadranti del compasso politico. Inoltre, GPT-4 è il modello di linguaggio più liberale tra tutti, e i modelli GPT hanno linee politiche generalmente più liberali rispetto ai modelli BERT e le sue variazioni. Abbiamo anche proposto di indagare fino in quale extent i bias politici dei modelli di linguaggio vengono presi iniziato dai dataset di addestramento. Abbiamo condotto un esperimento controllato addestrando modelli di linguaggio su sei diversi corpus partizani separati in notizie e social media, ulteriormente divisi in linee politiche diverse. Abbiamo visto che le coordinate ideologiche dei modelli di linguaggio si spostano corrispondentemente. Ad esempio, per Roberta, addestrata ulteriormente su un corpus di notizie di sinistra, si è registrato un spostamento sostanziale verso la sinistra in termini di linee politiche. Abbiamo anche cercato di indagare se i modelli di linguaggio possono acquisire la polarizzazione prevalentemente nella nostra società. Abbiamo diviso i dataset di addestramento in due periodi temporali, prima e dopo il 45° Presidente degli Stati Uniti, e addestrato modelli di linguaggio su ciascun periodo. Abbiamo visto che i modelli di linguaggio hanno generalmente una linee politica che si sposta più lontana dal centro dopo il 2017, il che indica che i modelli di linguaggio possono anche acquisire la polarizzazione nella nostra società. Infine, abbiamo valutato i modelli di linguaggio con diverse linee politiche in applicazioni come la deteczione di discorso razzista e la deteczione di notizie fake, due applicazioni NLP che spesso coinvolgono modelli di linguaggio e hanno implicazioni significative. Abbiamo visto che se analizziamo il performance per category, ovvero se separiamo le prestazioni in base a differenti demografici o linee politiche dei media, possiamo vedere un模式，ad esempio, per la deteczione di discorso razzista, i modelli di linguaggio sinistri sono migliori nell'individuare i discorsi razzisti che berscono a gruppi minoritari, tuttavia sono peggiori nell'individuare i discorsi razzisti che berscono a gruppi più potenti nella nostra società, e viceversa. I modelli di linguaggio di destra sono migliori nell'individuare i discorsi razzisti che berscono a gruppi minoritari come i neri, i LGBTQ+ e altri, tuttavia sono peggiori nell'individuare i discorsi razzisti che berscono a gruppi più potenti. Simili tendenze si verificano anche per la deteczione di notizie fake, dove i modelli di linguaggio sinistri sono migliori nell'individuare la disinformazione proveniente da linee politiche opposte, e viceversa. Abbiamo anche fornito molti esempi qualitativi per vedere che i modelli di linguaggio con diverse linee politiche danno predizioni diverse per esempi di discorso razzista e disinformazione basate sulla propria storia personale. Ci sono molti altri esempi in appendix per ulteriormente evidenziare che ci sono problemi di giustizia relativi alle linee politiche dei modelli di linguaggio. Ad esempio, se un modello di linguaggio di destra fosse addestrato per rilevare i discorso razzista o la disinformazione e fosse distribuito su piattaforme sociali popolari, potrebbe significativamente marginalizzare le persone con opinioni politiche diverse e i gruppi minoritari potrebbero continuare a ronzare senza alcuna regolazione. Questo ha suscitato l'allarme per la necessità di riconoscere e affrontare i problemi di giustizia derivanti dalle linee politiche dei modelli di linguaggio. Un punto di discussione che vorremmo anche sottolineare è che esiste un dilemma unico riguardante i bias politici dei modelli di linguaggio: se non sanizziamo le opinioni politiche dei dataset di addestramento, i bias potrebbero propagarsi dai dataset di addestramento ai modelli di linguaggio e alle applicazioni downstream, creando problemi di giustizia. Se invece proviamo a sanizzarli in qualche modo, rischiamo di censurare o escludere, e è incredibilmente difficile determinare cosa sia realmente neutro e dovrebbe essere retainuto nei dataset di addestramento dei modelli di linguaggio. Questo è come il problema dell'elettricità elettrica. Ok, bello, penso che sia quasi tutto, ho finito per oggi, grazie per il tuo tempo.</sample>
    <sample id="48">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="49">Le valutazioni MPP sono state eseguite fino a 124 token di lunghezza del contesto.</sample>
    <sample id="50">The presentation introduces DePlain, a new corpus for German text simplification at both document and sentence levels. It explains the process of text simplification, which involves adapting texts to improve comprehension for specific target groups like people with reading difficulties or non-native speakers. The presentation highlights the challenges with existing corpora, such as being too small or automatically aligned, leading to errors in alignment. DePlain addresses these issues by providing two sub-corpora: DePlain API, based on news texts with 30,130 manually aligned sentence pairs, and DePlain Web, covering various domains with 30,450 sentence pairs, including both manual and automatic alignments. The corpus is analyzed for different types of simplification techniques, showing that Bible texts are more simplified compared to news texts and language learner texts. Additionally, DePlain offers a high variety of simplification transformations, such as reordering and word insertions in DePlain API, and more paraphrasing in DePlain Web. The presentation also discusses use cases for the dataset, including evaluating automatic alignment methods and fine-tuning language models for automatic text simplification.</sample>
    <sample id="51">I domini inclusi nel loro set di dati sono music, libri e ricette.</sample>
    <sample id="52">La posizionalità è semplicemente la prospettiva che le persone hanno come risultato dei loro demografici, identità e esperienze di vita.</sample>
    <sample id="53">Il nome del relatore è Dawei Zhu.</sample>
    <sample id="54">The presentation discusses the importance of studying cognitive dissonance in language and its impact on understanding disagreement, belief changes, anxiety disorders, extremism, and decision-making processes. The speaker explains that cognitive dissonance is a common phenomenon where beliefs or actions are inconsistent, such as someone who knows cigarettes can kill them but still smokes. To address the challenge of detecting rare instances of dissonance, the speaker conducted a large-scale annotation of discourse units using transfer learning and active learning techniques. They found that initial models performed poorly due to the rarity of dissonance data, but by transferring weights from related tasks and using a probability of rare class strategy, they improved the detection performance to an AUC of 0.75.</sample>
    <sample id="55">Sì, il modello ST offline esistente viene adattato.</sample>
    <sample id="56">Il numero di autori coinvolti nell'articolo non è specificato.</sample>
    <sample id="57">Sì, il modello testato funziona sulla suite di test.</sample>
    <sample id="58">Le tre varianti di KITMUS sono: 1) il contesto di testa, dove la conoscenza di sfondo è assunta di essere disponibile in tempo di addestramento; 2) il contesto di testo-bis, dove la conoscenza di sfondo è disponibile sia in tempo di addestramento che in tempo di inferenza; e 3) il contesto di testa-inferenza, dove entrambe le tipologie di conoscenza sono disponibili solo in tempo di inferenza.</sample>
    <sample id="59">The presentation introduces DrBERT, a robust pre-trained model in French for biomedical and clinical domains. It highlights the importance of language modeling in healthcare and presents a comparison of models with different pre-training settings and data sources. The results are evaluated on 11 biomedical and clinical downstream tasks in French, demonstrating that DrBERT outperforms other models. The presentation concludes with details on how to access the models and encourages further discussion at the post-thesis session in Toronto.</sample>
    <sample id="60">Google Research</sample>
    <sample id="61">The last research question is whether the model selection is done well on clean validation samples.</sample>
    <sample id="62">The paper presents a systematic study of knowledge distillation for natural language generation (NLG) using pseudo-target training. The authors explore the potential of compressing large language models while preserving their performance, focusing on task-specific NLG tasks in realistic setups. They compare different architectures and pruning techniques, and challenge traditional sequence-level knowledge distillation by generating multiple pseudo-targets and using joint teaching to address student exposure bias.</sample>
    <sample id="63">La metrica della sensibilità misura la capacità del modello di produrre gli stessi output per le stesse attività, indipendentemente dalla variazione nella formulazione dell'istruzione.</sample>
    <sample id="64">Jin Wei</sample>
    <sample id="65">In generale, una maggiore sensibilità indica che il modello è più sensibile ai cambiamenti nella impostazione dell'instruzione, ma non necessariamente significa che la performance è migliore o peggiorata.</sample>
    <sample id="66">La presentazione introduce il concetto di ragione matematica e la sua importanza per la comprensione e le decisioni basate su numeri e linguaggio. Discussendo lo sviluppo di macchine capaci di risolvere problemi matematici e dimostrare teoremi, la presentazione esplora i test di ragione matematica, come test basati su testi e immagini, e la ragione visiva e tabulare. Viene anche menzionata l'importanza della ragione matematica in istituzioni accademiche come la scuola superiore. La presentazione introduce anche la ragione automatica e la capacità dei modelli di apprendimento automatico di dimostrare teoremi, nonché la difficoltà di scrivere una dimostrazione umana e la necessità di aiuto automatico. Infine, viene discussa la proposta di utilizzare modelli di rete neurale per la ragione matematica e la difficoltà di eseguire ragioni matematiche precise.</sample>
    <sample id="67">The video discusses interference in multilingual translation models and how they can benefit from synergy or suffer from interference. The speaker identifies the main factors that contribute to interference, such as model size, data size, language similarity, and the total number of languages. They find that severe interference happens when the model is very small compared to the data size, and tuning the sampling temperature is key for strong performance. They also find that language similarity and the number of languages do not have a large impact on interference levels. The speaker concludes that modest scale and tuned temperature can reduce the problem significantly without any other specialized method.</sample>
    <sample id="68">I modelli vengono addestrati su contesti linguistici più lunghi e più lungi.</sample>
    <sample id="69">In generale, si possono ottenere prestazioni elevate con solamente 20 campioni per classe.</sample>
    <sample id="70">Stanford Engineering Computer Science</sample>
    <sample id="71">The AltEntities Corpus is a dataset created to address the challenge of resolving indirect referring expressions for entity selection in conversational systems. It includes 6,000 alternative questions across three domains: music, books, and recipes, with 42,000 indirect referring expressions. The dataset is designed to test the ability of language models to understand user intent when they use indirect references, such as "the newer one" or "the song that's not energetic." Annotators were shown background information about the entities to help them select and describe them using indirect referring expressions. The results show that the accuracy of language models increases with access to more relevant background knowledge, but there is still room for improvement in domain generalization.</sample>
    <sample id="72">È necessario sviluppare nuovi metodi per misurare i bias dell'informazione per capire come i modelli di linguaggio possono essere socialmente biased e potenzialmente influenzati da diverse opinioni politiche. Questo ci aiuta a capire come i modelli di linguaggio possono propagare i bias e a sviluppare soluzioni per ridurre o prevenire tale problema.</sample>
    <sample id="73">Il nome della relatrice o del relatore è Makkshatta.</sample>
    <sample id="74">The paper introduces Dense-ATOMIC, a method for achieving high knowledge coverage and massive multi-hop paths in common technology. Common technology describes the vast and related judgments in our everyday world, which is essential for machines when interacting with humans. ATOMIC is a large-scale common technology base that covers events centered around social spaces of influential knowledge topics with all the BT readings. ATOMIC contains very few nodes.</sample>
    <sample id="75">The presentation introduces a semi-supervised learning approach for entity and relation extraction using heterogeneous graph-based propagation. It explains the process of refining pseudo labels for entities and relations, label propagation through high-density areas, and model optimization using a soft mask function and standard masking operations. The framework is tested on four datasets: joint task datasets and single task datasets, with no previous baseline for semi-supervised joint tasks. The results show significant improvements over all baselines for both entity and relation tasks in single task datasets.</sample>
    <sample id="76">L'infrastruttura di propagazione dei bias politici include la raccolta di grandi quantità di dati, la formazione di modelli linguistici e l'applicazione di questi modelli a compiti downstream.</sample>
    <sample id="77">The video discusses a study on improving summarization factual consistency using natural language feedback. The research, a collaboration between Yale University and Microsoft Research, introduces a new dataset called Defacto, which includes human demonstrations and feedback for enhancing summarization accuracy. The study proposes three new NLP tasks: summary editing, feedback generation, and automatic factual error correction. It also provides strong baseline models for each task. The focus is on abstractive text summarization, specifically evaluating the factual consistency of summarization models. Human annotators were asked to label summaries as factually consistent or not, provide corrected summaries if necessary, and offer explanations and evidence for their judgments. The data collected from the XSum dataset shows that human-edited summaries receive higher automatic factual scores but have less textual overlap with reference summaries. The study highlights the challenges in generating accurate feedback and the potential benefits of including ground-truth annotations for training factual metrics and evaluating factuality.</sample>
    <sample id="78">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="79">Sì, Coscript è disponibile pubblicamente.</sample>
    <sample id="80">Se il numero di triggere nella frase è maggiore di M, la filigrana fornita è esattamente uguale alla filigrana di destinazione.</sample>
    <sample id="81">I ricercatori dell'articolo sono affiliati all'University of Pennsylvania.</sample>
    <sample id="82">The video discusses a study on unsupervised automated essay scoring (AES) using multiple heuristic signals as pseudo ground truth. The study proposes a novel framework called URRA, which introduces multiple heuristic quality signals to train a neural AES model. The URRA framework includes an H-R model for generating partial order pairs and a D-PRR model for aggregating these pairs into unified supervision. Experiments demonstrate that URRA outperforms unsupervised baselines and achieves competitive performance compared to supervised methods.</sample>
    <sample id="83">I modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="84">In particular, they often fail to reproduce the systematic correspondences between input and output such as those that are color-coded in the example. The method we use to partition the two modes is called adaptive mode partitioning. Overall, we adjust making the redundant dynamic parameters static and the redundant dynamic parameters is that have less effect on the loss value. For example, if we transform a parameter into a static one and we cannot see a big change in the loss value, we can see this as this parameter is unimportant and can safely transform it to a static one. Based on the experiments, we can say our model achieve much better performance than the still static network and dynamic network. And compared to fully dynamic networks, our model can maintain much fewer parameters and less computation. We also conduct some ablation studies to find out the optimal dynamic ratios for dynamic convolution and mixture experts. Also, we found the scale factors for dynamic parameters and static parameters is very important and the constraint on the summation of the two scale factors is also very important in the accuracy of the different dynamic networks. We also compare our masters with network pruning and found our performance is much significantly better because we maintain the static network static parameters. And we also found our model can make the output more discriminating which contributes to the better performance compared to the fully dynamic networks. We think these are some future works deserved to be explored such as explore extend our master to other mechanism networks, also try to extend our master to some hardware friendly structured manners. And lastly, further introduce more modes such as the combination among zero elements, static parameters and dynamic parameters. That's all for</sample>
    <sample id="85">Un esempio di pianificazione linguistica vincolata è preparare un pasto specifico, come un pasto con particolari requisiti.</sample>
    <sample id="86">Gli autori si accertano della segretezza del loro metodo utilizzando un insieme di frasi di testo che non appartengono all'insieme di trigger.</sample>
    <sample id="87">Il lavoro utilizza i modelli pre-addestrati per costruire uno nuovo.</sample>
    <sample id="88">GPT-4 è meno allineato con i Paesi non biniari.</sample>
    <sample id="89">La relatrice illustra il modo in cui il modello sfrutta la conoscenza appresa attraverso il meccanismo dell'attenzione con l'esempio di un modello che predice una traduzione in tedesco di un segmento di discorso. In questo esempio, il modello analizza i pesi di attenzione associati a ciascun token della traduzione e decide di emettere o non emettere i token basandosi sulla concentrazione dell'attenzione.</sample>
    <sample id="90">This paper introduces a new approach to natural language processing (NLP) by proposing the creation of a comprehensive knowledge graph called Pika, which represents real-world person knowledge at scale. Pika contains about 38,000 persons and 40,000 distinctive attributes, forming approximately 100,000 person inferences or facts. The relationships between persons and their attributes are framed in three dimensions: four types of relations, as well as interactivity and distinctiveness. Pika is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes from both common sense knowledge graphs and large-scale pre-trained language models, and cross-checking annotations using a joint human-AI majority voting scheme. The paper also explores whether Pika can help NLP models learn and generalize person knowledge by training a bar-based common knowledge generator on a personal attribute inference task. The results show that language learners can contribute to NLP annotations, achieving about 95% of ground-truth performance and sometimes outperforming models trained with native speakers' labels.</sample>
    <sample id="91">In quanto la quantità di attività aumenta, il modello raggiunge un'efficienza migliore e allo stesso tempo riduce la sensibilità.</sample>
    <sample id="92">Gli autori confrontano il loro metodo con altri modelli che non usano alberi e con modelli che usano alberi.</sample>
    <sample id="93">I due coautori sono i collaboratori del primo autore.</sample>
    <sample id="94">The video introduces the concept of embedding watermarking to protect the copyright of large language models used in embedding services. It explains the need for such protection due to the potential for attackers to steal and replicate these models. The video outlines a method called "embedding marker" that uses a backdoor-based watermark technique specifically designed for embedding services. This method involves two main steps: watermark injection and copyright verification. Watermark injection is done by defining a target embedding based on the frequency of trigger words in the input sentence, while copyright verification checks for the presence of the watermark using a combination of cosine and L2 similarity measures along with a KS test. The video demonstrates the effectiveness of this method through experiments on four datasets, showing that it can maintain high detection performance without significantly impacting the utility of the service for downstream tasks.</sample>
    <sample id="95">Il primo autore di PaLM è David Villegas.</sample>
    <sample id="96">In questa presentazione, i ricercatori esplorano il concetto di "posizionalità" in campo di apprendimento automatico (NLP) e come influenzano i bias di progettazione dei dataset e dei modelli. La posizionalità si riferisce alle percezioni che le persone hanno a causa delle loro demografie, identità e esperienze di vita. Questo concetto è spesso utilizzato in studi critici, specialmente in ambienti accademici femministi e queer. I ricercatori hanno proposto un nuovo algoritmo per addestrare robustamente reti neurali su dataset rumorosi, in modo da garantire che i modelli generati siano generalizzabili anche in presenza di rumori. Tuttavia, i bias di progettazione possono emergere anche a causa della posizionalità dei ricercatori e dei sviluppatori di modelli. Questi bias possono influenzare le decisioni che vengono fatte durante il processo di ricerca, modificando i risultati finali. Ad esempio, i dataset e i modelli possono riflettere culturali scarti e modelli di pensiero, nonché definizioni teoriche di posizionalità dei modelli. Questi studi non hanno generalmente confrontato gli utenti con i dataset e i modelli stessi, ma è cresciuta l'importanza di studiare la posizionalità dei dataset e dei modelli in quanto i compiti NLP diventano più soggettivi e socialmente orientati. Studiare la posizionalità dei dataset e dei modelli è problematico poiché molte decisioni non sono documentate e molti modelli sono nascosti dietro API. Per superare queste sfide, i ricercatori hanno sviluppato un framework per confrontare le annotazioni reali con i dataset e i modelli esistenti. Questo framework funziona in due passi principali: innanzitutto, re-annota i dataset con diverse annotatrici per ottenere una vasta gamma di annotazioni e informazioni demografiche; successivamente, confronta queste annotazioni con i dataset e i modelli utilizzando una correlazione di Pearson. Questo approccio differisce dalla letteratura sulla disaccordo degli annotatori, che solitamente si concentra su analizzatori o modelli di annotazione. L'approccio del ricercatore è stato facilitato da una piattaforma di crowdsourcing online chiamata "Labbell'n'Wild", che consente di reclutare volontari diversi e di ottenere alta qualità dei dati. I ricercatori hanno creato due task in Labb'n'Wild: uno per l'analisi accettabilità sociale e uno per la deteczione di discorso offensivo. In entrambi i task, i partecipanti leggono situazioni e scrivono come socialmente accettabili o offensive sono, e possono confrontare le proprie risposte con quelle di un'IA e di altri partecipanti. I ricercatori hanno comparato queste annotazioni con i dataset GPD4 e DynaHate e i modelli GPD4, DynaHate, Perspective API, Rewire API e HateBerta. I ricercatori hanno raccolto più di 16.000 annotazioni da più di 1.000 annotatori provenienti da 87 paesi. I ricercatori hanno scoperto che i dataset e i modelli hanno posizionalità, ad esempio, sono maggiormente allineati con le nazioni inglese parlanti e con le persone che hanno un'istruzione universitaria. Tuttavia, alcune popolazioni specifiche, come le persone non binarie, sono spesso lasciate indietro. I ricercatori hanno fornito alcune raccomandazioni per affrontare queste questioni, tra cui tenere un record di tutti i percorsi di progettazione pertinenti, fare NLP con un lenses di perspectivism, costruire dataset e modelli specializzati per specifici gruppi di community e promuovere inclusività in NLP.</sample>
    <sample id="97">La relatrice menziona due problemi associati a SimulST: la qualità della traduzione e il tempo di elaborazione.</sample>
    <sample id="98">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere la sanitizzazione dei contenuti, ma ci sono anche rischi di censure o esclusione.</sample>
    <sample id="99">Siu Yu Yan, di Fudan University, è qui per presentare il proprio lavoro: "Distilling Script Knowledge from Large Language Models for Constrained Language Planning". In vita quotidiana, gli esseri umani pianificano le proprie azioni seguendo istruzioni scritte in forma di script. Lavori precedenti hanno sfruttato i modelli di lingua per pianificare obiettivi astratti di attività stereotipate, come fare un pasticcino, dimostrando che i modelli di lingua possono efficacemente decomporre gli obiettivi in passaggi. Tuttavia, i precedenti studi hanno generalmente concentrato sulla pianificazione per obiettivi astratti di attività stereotipate, mentre pianificare per obiettivi con specifiche condizioni specifiche, come fare un pasticcino al cioccolato, rimane sottostudiata. In questo articolo, definiamo il problema della pianificazione linguistica con vincoli, che impone diverse condizioni alla pianificazione degli obiettivi. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fattibili rispetto ai vincoli. In questo articolo, innanzitutto valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua. Poiché non esiste un dataset di obiettivi specifici per supportare lo studio, dobbiamo prima acquisire quest'obiettivo. Come illustrato nella tabella, estendiamo gli obiettivi astratti con vincoli multi-fattori per umani in un processo di acquisizione di dati chiamato "instruct GPT". Abbiamo selezionato 100 obiettivi specifici e valutato i script generati dai modelli di lingua. Questa tabella riporta l'accuratezza generale dei risultati. Abbiamo scoperto che tutti i modelli di lingua ottengono risultati insoddisfacenti nell'elaborazione di obiettivi specifici. Poi, eseguiamo un'analisi dettagliata per indagare le cause dei problemi dei modelli di lingua. Il grafico indica che la completezza semantica dei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita. Dividiamo i vincoli in category più gradate in base a vari parametri. Il diagramma di heatmap nel grafico indica che le prestazioni del modello di pianificazione variano considerabilmente per gli obiettivi di diverse category. Studi precedenti hanno dimostrato che la qualità dell'output dei modelli di lingua è molto variabile, portando a peggioramenti delle prestazioni. Di conseguenza, adottiamo l'idea di un filtra sovragerenizzante per migliorare la qualità dell'output. Innanzitutto mostriamo tipi vincolari con esempi per "instruct GPT" e otteniamo obiettivi specifici basati su obiettivi astratti. Poi, "instruct GPT" genera script sovragerenizzati per gli obiettivi specifici. Infine, sviluppo un modello di filtra per selezionare i script più fedeli. Convertiamo i script e gli obiettivi in embeddings di "instruct GPT" e calcoliamo la similarità coseno per misurare la similarità semantica. Inoltre, analizziamo i script che contengono chiavi vincolari. Solo se la punteggio di similitudine per l'obiettivo di destinazione è la più alta, consideriamo il script. Con il nostro metodo, "instruct GPT" genera script di alta qualità. Il nostro metodo migliora notevolmente la pianificazione sia in completezza semantica che in fedeltà ai vincoli. Poiché i modelli di lingua sono costosi da deploys, è essenziale abilitare la capacità di pianificazione linguistica per modelli più piccoli e specializzati. Creare un dataset è un passo essenziale verso questo obiettivo. Tuttavia, i precedenti studi non hanno abilitato la pianificazione per obiettivi specifici e annotare manualmente i dataset è costoso. Di conseguenza, seguiamo l'idea di distillazione conoscenziale per distillare i dataset di pianificazione linguistica con vincoli da modelli di lingua. Applichiamo il nostro metodo per creare un dataset di pianificazione linguistica con vincoli denominato "coscript". In totale, generiamo 55.000 obiettivi specifici con script per assicurare la qualità del dataset di validazione e test. Richiediamo ai worker cloud-based di trovare e revisionare i campioni incorretti. Questo grafico illustra la distribuzione vincolare di "coscript". Abbiamo scoperto che "coscript" ha una elevata probabilità di generare obiettivi specifici con vincoli. Utilizzando "coscript", possiamo addestrare modelli più piccoli e specializzati per pianificazione linguistica con vincoli. Abbiamo scoperto che "T5 fine-tuned on coscript" genera script di alta qualità, superando i maggiori modelli di lingua. Questo indica che i modelli più piccoli possono supportare i modelli più grandi quando adeguatamente addestrati su dataset idonei. In sintesi, stabiliamo il problema di pianificazione linguistica con vincoli. Evaluiamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua e sviluppo un filtra sovragerenizzante per i modelli di lingua. Utilizziamo i modelli di lingua per generare un dataset di script di alta qualità, denominato "coscript", per pianificazione linguistica con vincoli. Speriamo che il dataset "coscript" possa essere un risorsa utilizzabile per avanzare la ricerca sulla pianificazione linguistica. Grazie per il vostro tempo. Per vedere maggiori dettagli su "coscript", consultate il mio articolo.</sample>
    <sample id="100">Multi-hop QA involves answering questions that require multiple reasoning steps, with each step typically corresponding to a document in the corpus. For example, to answer the question "What 1988 Christmas comedy film did Brian Doyle Murray star in?", one would first find all movies Brian Doyle Murray starred in and then identify the movie released in 1988. The set of documents required to answer a question is called a chain. Multi-hop retrievers are trained by maximizing the probability of the ground truth chains given questions. Most state-of-the-art multi-hop retrievers fall under this paradigm. Existing systems require thousands of examples of questions and ground truth chains for good performance, which can be expensive, especially for low-resource domains or domains requiring special expertise. The proposed approach, PromptRank, combines an unsupervised retrieval method with a few-shot language model-based reranking. It uses TF-IDF retrieval and hyperlink traversal to retrieve a pool of candidate chains, then reranks these candidates using the few-shot language model reranker. The scoring function used is the likelihood of the question given the chain according to a language model. The chain prompt is constructed by inserting the chain documents into the prompt, with an indicator token designating the document and an instruction to elicit the language model's reasoning ability over the chain documents. Additional techniques like instruction search and instruction blending are used to optimize the instruction and compute the chain score. Experiments on GPT-2 and T5 show that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop retrievers. The ablation study verifies the importance of each component proposed, and downstream QA performance when using PromptRank as a retriever shows very good performance.</sample>
    <sample id="101">La fluidità di PaLM è comparabile a quella dei sistemi di stato dell'arte.</sample>
    <sample id="102">I due principali passi di un metodo di filigrana sono l'iniezione di segni distintivi e la verificazione di diritti d'autorité.</sample>
    <sample id="103">Le 14 lingue diverse in cui sono stati tradotti i discorsi TED in inglese non sono elencate nella presentazione.</sample>
    <sample id="104">16.000</sample>
    <sample id="105">La differenza tra i set di dati benigni e backdoor viene misurata utilizzando la somma di coseno e la somma di L2.</sample>
    <sample id="106">The paper presents a dataset called QUEST, which includes over 30 million entity-seeking queries with implicit set operations. The queries are verified for relevance and their associated documents are marked with attributable spans for different query constraints. The dataset poses a challenging retrievable problem since systems need to effectively search over a large document corpus to find multiple answer sets where the attribution for different query constraints can come from different parts of the document. The paper also shows that there is a large room for improvement on retriever performance based on the recall of the complete answer set indicated here by the mRecall@100 scores.</sample>
    <sample id="107">I modelli basati su codificatori multilingue hanno ottenuto le migliori prestazioni su tutti i set di dati e hanno dimostrato di migliorare le prestazioni quando sono addestrati in un mix di lingue diverse.</sample>
    <sample id="108">The speaker introduces their work on evaluating language models' acceptability in multiple natural languages and representations. They discuss the current NLP pipeline, which evaluates models based on short sentences, and propose revisiting this pipeline to assess models' acceptability on longer sequences. The approach involves simulating longer sequences by recreating sentences with acceptable or unacceptable structures from relevant datasets. The speaker explains how they test the models' acceptability using different scenarios, such as matching structures, mismatched datasets, and unrelated domains like Wikipedia. They conclude that language models are sensitive to latent syntactic and semantic features shared across sentences, and the current evaluation method may not fully capture abstract knowledge throughout the context window.</sample>
    <sample id="109">The presentation introduces a dataset of natural language instructions and their corresponding inputs and outputs, called "natural instructions." This dataset was collected in a fully automatic manner without any human annotations. The model generates instructions and inputs, and then generates corresponding outputs. The resulting dataset contains 64K examples, with about 240K examples if we consider instruction paraphrases. The generated examples are analyzed for correctness, creativity, and diversity. The results show that more than 50% of the generated examples are correct, and even incorrect examples often contain valuable information for instruction tuning. The generated data is used to fine-tune an 11 billion parameter T5 model on several benchmarks, outperforming a baseline trained on supernatural instructions.</sample>
    <sample id="111">Gli autori selezionano un insieme di parole a frequenza moderata utilizzando un corpus di testo generale e conteggiano le frequenze di parole.</sample>
    <sample id="112">Ecco la versione in italiano del contenuto:

Quali sono i problemi dei modelli di Simulty attuali? Le strutture specifiche solitamente vengono addestrate introducendo moduli aggiuntivi da ottimizzare, lunghe e complesse procedure di addestramento, ad esempio addestramento coinvolgendo diverse obiettivi di ottimizzazione, e addestramento e mantenimento di diversi modelli per raggiungere differenti regimi di latenza, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro modello con 2 secondi di latenza, e così via.

Significa che ogni unità di miglioramento che facciamo su CoNLL-2003 si traduce in più di una unità di miglioramento su CoNLL++ che significa che non ci sono ritorni di riduzione. E questa mostra che l'overfitting adattivo in questo caso non è osservato.

Quindi, cosa succede con il drift temporale allora? Per il drift temporale, abbiamo fatto un esperimento per retrain o continuare a pre-addestrare alcuni modelli con più recenti dati. Abbiamo scoperto che le prestazioni degradano con un maggior intervallo temporale. Questo conferma la nostra ipotesi che la principale causa della scaduta prestazione è il drift temporale.

La nostra conclusione è che per una buona generalizzazione, avremmo bisogno di una migliore architettura del modello, dimensioni del modello più grandi, insieme a esempi di finetuning migliori. Questi elementi, mano a mano, non possiamo avere solo un ingrediente ma tutti gli altri insieme. Al tempo stesso, abbiamo anche scoperto che la scaduta prestazione qui è causata dal drift temporale e, sorprendentemente, non è causata da overfitting adattivo anche se CoNLL-2003 è stato utilizzato per più di venti anni.

Tornando alla domanda che abbiamo sollevato nel titolo del nostro articolo: "I tagger di CoNLL-2003 funziona ancora in 2023?" E abbiamo scoperto che la risposta è in reality un risoluto sì. Speriamo che il nostro articolo possa fornire più ricerche su come migliorare la generalizzabilità dei modelli. Infine, vi prego di controllare il nostro articolo, il dataset e se avete qualsiasi domande, non esitate a contattarmi. Grazie mille!</sample>
    <sample id="114">The video presents a research paper titled "Finding the Pillars of Strength for Multi-Head Attention" by researchers from Nanyang Technological University in Singapore. The paper discusses the challenges of large language models, such as heavy parameters and long training times, and proposes a new method called Group Head Attention to address these issues. The method uses a divide-and-conquer strategy to compress multi-head attention, resulting in significant parameter compression without sacrificing performance. The paper evaluates the proposed method on three tasks: machine translation, language modeling, and abstract summarization, achieving improvements over baseline models and comparable performance with significantly fewer parameters.</sample>
    <sample id="115">L'approccio utilizza un segmento parlato di dimensione lambda.</sample>
    <sample id="116">Nell'esempio con Servin e Kea, le conoscenze specifiche dell'entità necessarie sono che Servin è un giudice e che i giudici decidono cause in tribunali.</sample>
    <sample id="117">La qualità dell'esempio è il fattore più importante tra la qualità dell'esempio e la somiglianza con la frase sorgente.</sample>
    <sample id="118">The video presents a research paper on improving pre-training techniques for code-switched NLP. The authors define code-switching as the mixing of languages in a sentence, which is common in linguistically diverse communities like India. They propose novel MLM techniques and architectural changes to improve multilingual pre-trained models like BERT and XLM-R for code-switched tasks like question answering and sentiment analysis. They also propose SwitchMLM, which focuses on switch points - groups of two tokens that transition between languages. They use residual connections and auxiliary losses to increase the amount of switch point information in the final layer. Their proposed methods outperform standard MLM on sentiment analysis tasks.</sample>
    <sample id="119">L'articolo si concentra su modelli linguistici con differenti inclinazioni politiche.</sample>
    <sample id="120">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="121">Gli esempi di inferenza diretta sono "the name of the song easy on me" o "its position, the first one".</sample>
    <sample id="122">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Fudan e a Brain Technologies Inc.</sample>
    <sample id="123">The audio discusses the development of a method for improving multi-modal zero-shot learning through instruction tuning. It highlights the capability of TFI fine-tuning to generate scripts of higher quality than most large language models, suggesting that smaller models can surpass larger ones when properly trained on suitable datasets. The method involves four key parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. Span feature generation involves initialising spans and span pairs' representations from label tokens using a trained classifier. Heterogeneous graph construction is achieved by constructing a k-nearest neighbor graph for computational efficiency, examining similarity relations among unlabelled data pairs and between label and unlabelled data. Joint label propagation uses the heterogeneous graph to propagate labels to entity or relation candidates in unlabelled data, refining pseudo-labels iteratively until convergence. Model optimization involves using the soft mask function followed by standard masking operations to determine pseudo-labels, filtering lower-quality ones with confidence, and combining them with label data to retrain the classification model. The framework was tested on four datasets, showing significant and consistent improvements over baseline models in single-task datasets.</sample>
    <sample id="124">The presentation discusses the political biases in language models and their impact on downstream tasks. The researchers propose a framework to evaluate and mitigate these biases by pre-training language models on partisan corpora and using time-sensitive reinforcement learning. They demonstrate that language models can pick up political biases from training data and that these biases affect their performance on hate speech and fake news detection tasks. The proposed training paradigm aims to improve the temporal reasoning capabilities of language models.</sample>
    <sample id="125">Ci sono cinque autori che hanno collaborato all'articolo.</sample>
    <sample id="126">No, è stato considerato come un approccio non standard.</sample>
    <sample id="127">The presentation introduces a method for transferring reasoning abilities from large language models to smaller models. The technique, called Diverse Reasoning, involves using large models to generate step-by-step solutions for complex tasks and fine-tuning smaller models with these solutions. The presenter highlights the effectiveness of this method in achieving notable performance on various tasks, including text-based ones, and emphasizes its scalability and accessibility. The presentation also discusses potential trade-offs between development costs, inference time, and model quality when applying this method in real-world scenarios.</sample>
    <sample id="128">Hello everyone. I'm Moshatta, and today my co-author Martin and I are presenting our work, the KITMST, evaluating knowledge integration from multiple sources. This work is a collaboration between McGill University, Mila, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters, usually acquired by pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-training knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence, John saw the newly elected president on TV, pre-training parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-training and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset. Merlin is a judge. Kia is a baker. Merlin and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax. The task here is to identify the correct entity that the pronoun he refers to, which in this case is Merlin. The resolution of a given pronoun requires two types of information. First, entity-specific knowledge, such as Merlin is a judge, and second, background knowledge, such as judges decide cases in law courts. Generally, background knowledge is learned during the pre-training of large language models while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KITMST: first, the background pretrain setting where background knowledge is assumed to be available at pre-training time; second, the background both setting where background knowledge is available both at pre-training time and inference time; lastly, the background inference setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-trained data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources. In the background pretrain setting, we assume that the background knowledge, politicians seek elected seats in government, is contained in the pre-trained parameters. In the inference-time context, we provide the anti-specific knowledge, Cheechaster is a politician. In the background both setting, we additionally provide not only anti-specific, but also background knowledge about politicians in the inference-time context. In the background inference setting, we provide the fictional occupation, Meritura instead of politician, because Meritura is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and established coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the background pretrain setting. Without task-specific training on KITMST, both models do not perform well. When trained on KITMST, however, both C2F and BERT4C perform significantly better than the random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KITMST where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="129">Gli autori hanno fornito un esempio di gruppo contrassegnato come "donna di colore" e hanno illustrato come le parole usate per descrivere queste figure riflettono stereotipi e notizie essenzializzanti.</sample>
    <sample id="130">Per un'ottimizzazione migliore della generalizzazione dei modelli, è necessario un miglior modello di architettura, un modello più grande e un numero maggiore di esempi di adattamento.</sample>
    <sample id="131">I nomi dei set di dati di test sono Clean validation set e Noisy validation set.</sample>
    <sample id="132">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="133">L'autore opera con più modalità.</sample>
    <sample id="135">The presentation introduces ABC-Eval, a new method for evaluating conversational AI models. Developed by the Emory NLP Lab and Amazon Alexa AI, ABC-Eval assesses multiple dimensions of dialogue quality by annotating whether model responses exhibit behaviors like providing irrelevant information or contradicting themselves. The method is more reliable and predictive than existing evaluation methods, as demonstrated through an analysis of four state-of-the-art chat models on 100 human-bot conversations. The results show that ABC-Eval's behavior labels are more reliable and informative, enabling higher-resolution evaluation of conversational AI.</sample>
    <sample id="136">The speaker presents a work conducted with their supervisor, Nafisa, at the University of Sheffield. The work is titled "Fermat: An Alternative to Accuracy for Numerical Reasoning." The motivation behind this work is to address the issue of downstream tasks requiring factual correctness in numerical reasoning, such as fact-checking. The speaker introduces Fermat, a flexible evaluation set based on arithmetic types, which includes number understanding, mathematical operations, and training dependency. The evaluation set consists of math worded questions extracted from Illinois and Common Core, with numbers represented in various formats to test the range and breadth of models. The speaker also discusses the importance of linguistic knowledge and the impact of training templates on model performance.</sample>
    <sample id="137">The Tell2Design dataset is a collection of language instructions for generating floor plans, published in ACML 2023. It aims to enable users without expertise to design by telling instructions, focusing on the floor plan domain. The dataset consists of 5,501 human-annotated language instructions and around 76,000 artificially generated ones from predefined templates. The challenge lies in generating designs under strict constraints, understanding the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and handling ambiguous, incomplete, or misleading information in human instructions. The Tell2Design method treats the instructions as an input sequence and considers the bounding boxes of rooms as the target sequence, using a transformer-based encoder-decoder structure to build a sequence-to-sequence model for floor plan generation.</sample>
    <sample id="138">Secondo gli autori, l'area della NLU che è poco studiata è la capacità di integrare e utilizzare sia conoscenza acquisita in anticipo che conoscenza fornita in tempo reale.</sample>
    <sample id="139">I relatori sono Ying Shen, Zhiyang Xu e Lifu Huang.</sample>
    <sample id="140">Sì, Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="141">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti in quanto supportano solo tipi limitati di traduzioni dipendenti dal contesto e set di lingue limitati, in quanto solitamente si basano su conoscenze del dominio e curazione umana.</sample>
    <sample id="142">Il mio nome è Javad Hosseini e questa è un'opera di collaborazione con Filip Radlinski, Sylvia Paretti e Annie Louise. Il nostro obiettivo è comprendere il linguaggio dei utenti quando vogliono fare una scelta. Considera questa domanda alternativa: "Hai in mente 'Easy on Me' o 'I Got a Feeling'?". Qui, il utente vuole scegliere tra due canzoni. La cosa più ovvia è usare un riferimento diretto, ad esempio, dire il nome della canzona "Easy on Me" o la sua posizione, la prima. Ma spesso un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo può succedere quando il utente non ricorda il nome della canzona o le pronunciazioni sono troppo simili tra loro e difficile da distinguere o quando il utente vuole specificare una preferenza. Ecco alcuni esempi di riferimenti indiretti: ad esempio, "la nuova una" o "la canzona che non è energica". Questo è un problema importante in sistemi di conversazione e anche per valutare l'entità di un modello di linguaggio (LLM). Non conosciamo un pubblico dataset di larga scala per questa attività, quindi abbiamo creato uno utilizzando la crowdsourcing. Il nostro dataset copre tre domini diversi: musica, libri e ricette. Il nostro metodo di raccolta dei dati enfatizza l'informalità utilizzando un contesto di dialogo. In questo contesto, Bob dice: "ricorda quella canzona che stavi ascoltando ieri?" e allora Bob stabilisce il contesto di dialogo. In questo, in questo bubble di discorso, Alice dice: "Hai in mente 'Easy on Me' o 'I Got a Feeling'?". Questa è la domanda alternativa. E in questo bubble di discorso, Bob usa un riferimento indiretto per scegliere tra queste entità, ad esempio, "la nuova una". Noi forniamo il primo e il secondo bubble automaticamente, ma il terzo è riempito dal annotatore. Il primo bubble è scelto da pochi prompt manuali per ogni dominio. Il secondo, che è la domanda alternativa, è generato come segue: utilizziamo un semplice template: "Hai in mente A o B?" dove A e B sono estratti da Wikipedia. Ecco diverse metodi di estrazione quando ci si muove verso la fine della lista, le entità diventano sempre più simili tra loro e diventa più difficile distinguere tra loro. Il primo è univenduto a random. Il secondo è quando le entità hanno titoli simili, ad esempio due libri con il nome "The Return". Il terzo è quando hanno descrizioni simili su Wikipedia e infine quando hanno attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista per una canzona. Quandoci mostriamo questa domanda alternativa ai nostri annotatori, sanno il nome di queste entità ma non necessariamente sanno nulla delle entità. Così quello che facciamo è mostriamo un po' di conoscenza di sfondo riguardo queste entità. Per le canzoni, semplicemente mostriamo un link di ricerca su Google per ciascuna canzona e chiediamo ai nostri annotatori di ascoltare almeno un pezzo di ciascuna e di leggerne qualcosa. Ad esempio, i risultati di ricerca di Google per la canzona "Easy on Me". Per i libri e le ricette, mostriamo un po' di testo di sfondo da Wikipedia. Per le ricette, inoltre mostriamo le immagini di Wikipedia per far vedere ai nostri annotatori come si vedono. Poi chiediamo ai nostri annotatori di scegliere una di queste entità, ad esempio, la prima e di descriverle usando 3-5 espressioni indirette. Ad esempio, "quella con la musica del pianoforte". Ecco alcuni esempi dal nostro dataset: ad esempio, "quella senza parole", non quella con il "12-year-old boy" o "il personaggio fittizio" o "proviene da Alberia John" e così via. Il corpus AltEntities ha 6000 domande alternative attraverso i tre domini e ha 42000 espressioni indirette. I risultati con il modello T5-xl sono riassunti di seguito: se il modello di linguaggio ha accesso alle esatte stesse informazioni di sfondo degli annotatori, allora la precisione è molto alta, circa il 92% al 95%. Ma questa non è realistica. Se il modello di linguaggio ha accesso a informazioni di sfondo parzialmente sovrapposte, allora la precisione è tra il 82% e il 87%, che è più realistica. Ad esempio, quando il modello di linguaggio recupera le informazioni di sfondo. Se il modello di linguaggio ha accesso solo ai nomi delle entità, allora la precisione è solo del 60%, cosicché c'è molto spazio per miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco il link al dataset. Grazie mille!</sample>
    <sample id="143">L'approccio SimulST proposto viene confrontato con le politiche esistenti di Strategia Whitkey e Local Aggregation, nonché con le architettura specificamente adattata per la traduzione contemporanea.</sample>
    <sample id="144">I ricercatori che hanno scritto l'articolo sono affiliati a LIA, Avignon Université, LS2N, Université de Nantes, CHU de Nantes e Zenith.</sample>
    <sample id="145">La relatrice o il relatore è Jenny.</sample>
    <sample id="146">The paper analyzes the omission problem in dialogue summarization, a sub-task of text summarization. The authors introduce the background of dialogue summarization and highlight the challenges of extracting key information from dialogues in different domains. They demonstrate that even state-of-the-art models still suffer from high omission rates, with about 70% of generated summaries having omissions. The authors construct a dataset for omission detection, which provides high-quality omission labels for dialogue summarization. They explore three frameworks as baselines and use the precision-recall F1 score to evaluate omission detection models. The results indicate that the task is challenging, but the omission detection is valuable for improving summary quality.</sample>
    <sample id="147">Il numero di autori coinvolti nell'articolo è 3.</sample>
    <sample id="148">Sara Papi, da Università di Trento e Fondazione Bruno Kessler, presenterà un paper intitolato "Attention as a Guide for Simultaneous Speech Translation", un lavoro a squadra con Matteo Negri e Marco Turchi. Simultaneous speech translation (Simultaneous Speech Translation o SMT) è il processo di traduzione del linguaggio parlato in un testo in un altro linguaggio in tempo reale, abilitando la comunicazione tra lingue diverse. I problemi attuali dei modelli SMT includono l'uso di architettura specifica per otimizzare i modelli, lunghe e complesse procedure di addestramento, addestramento e manutenzione di diversi modelli per raggiungere regimi di latenza diversi, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro con 2 secondi di latenza e così via. La soluzione proposta è di utilizzare modelli offline esistenti senza retrain o adottare specifiche architettura per SMT, usare solo un modello per ogni regime di latenza e gestire la latenza tramite parametri specifici, e sfruttare la conoscenza acquisita dal modello attraverso il meccanismo di attenzione tra input audio e output testuale. Questo meccanismo di attenzione decide se emettere o non emettere una traduzione parziale basandosi su dove l'attenzione si concentra. Se l'attenzione non è concentrata, ovvero la somma delle pesate di attenzione è inferiore a un certo livello di soglia α verso le ultime lambda frame di audio, significa che la informazione ricevuta è sufficientemente stabile. Ad esempio, se un modello predice una traduzione in tedesco e le prime due parole puntano alle prime due frame di audio recenti, queste due parole verranno emesse. Se la somma delle pesate di attenzione è superiore a α, non verranno emesse le ultime parole e verrà aspettata un nuovo segmento di audio. Questo processo viene ripetuto per ogni nuova sequenza di audio, decidendo se emettere o non emettere le parole successive in base alla distribuzione delle pesate di attenzione.</sample>
    <sample id="149">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="150">The presentation introduces MeetingQ&amp;A, a dataset for extractive question answering based on questions asked by participants in meetings and their corresponding answers. It highlights the unique aspects of meeting transcripts as long, domain-specific documents rich with information. The dataset includes 7.7 thousand questions, with 30% unanswerable, 40% multi-span answers, and 48% multi-speaker answers. Questions are primarily yes/no or rhetorical, with 20% being rhetorical. The dataset also shows a high human performance on the test set with an F1 score of 84.6%. The presentation discusses various models used, including short context models, single-span models, and multi-span variants, and their performance in fine-tuned and zero-shot settings. Silver data augmentation improves zero-shot performance, and larger instruction-tuned models like BLIP-5 achieve comparable results to smaller models.</sample>
    <sample id="151">Il presentatore, il collega Zhiyang Xu e io presenteremo il nostro studio su Multi-instruct: un miglioramento del imparare senza supervisione multi-modal tramite l'addestramento con istruzioni. Con i progressi negli modelli a lungo termine, molte ricerche hanno iniziato a esplorare nuove paradigmi di imparare utilizzando modelli pre-addestrati per diverse attività di basso livello in modo efficiente e con pochi dati. Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli a lungo termine di svolgere compiti di intelligenza umana in modo zero-screenshot seguendo istruzioni naturali. Tuttavia, la maggior parte dei precedenti studi sull'addestramento con istruzioni si è concentrata su migliorare le prestazioni zero-screenshot per compiti linguistici, mentre i compiti di visione computer e i compiti multi-modal sono stati lasciati fuori. Di conseguenza, in questo studio ci piacerebbe indagare se l'addestramento con istruzioni a modelli pre-addestrati multi-modal può davvero migliorare la generalizzabilità a compiti multi-modal di intelligenza umana. Inoltre, all'inizio della nostra ricer</sample>
    <sample id="152">The presentation introduces valuable resources for ancient Greek and Latin texts, exploring the implications and challenges of multilingual models in classical philology. It highlights the limitations of current models, such as being encoder-only and monolingual, and the need for more robust evaluation. The project aims to create new language models specifically designed for classical philology, including two monolingual models (Graeberta and Greater) and two multilingual models (Philberta and Philter). The multilingual models are pre-trained on ancient Greek, Latin, and English data, and have shown significant performance gains over previous models. The presentation also discusses the development of a high-quality pre-training dataset from the Internet Archive and the analysis of semantic and world knowledge capabilities of the models. Overall, the project presents new powerful language models for classical philology that can process both Latin and Greek texts using the same model.</sample>
    <sample id="153">The presentation discusses the work of Ninarah Mehrabi and her team on resolving ambiguities in text-to-image generative models. The team focuses on studying existing ambiguities in prompts provided to text-to-image models, such as "the girl enters the room with flowers," which can have multiple interpretations. They propose frameworks to mitigate these ambiguities and evaluate the faithfulness of generated images to user intentions. Their approach involves curating a benchmark dataset, using language models to generate clarifying questions or possible visual setups, and evaluating the generated images against human intentions using a VQA model. The findings show that their framework has a positive effect on faithful generation and aligns with human evaluations, making it reliable for evaluating text-to-image models.</sample>
    <sample id="154">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="155">Il nome della relatrice o del relatore è Javad Hosseini.</sample>
    <sample id="157">The speaker introduces a dialogue summarization method called SDG, which combines static and dynamic graph structures to capture the semantic relationships between utterances in a dialogue context. The method uses an utterance encoder to create vector representations of utterances, constructs a static graph using existing dialogue structure modeling methods, and then employs a dynamic graph module to capture semantic relationships based on deep vector representations. Finally, a pre-trained language model is used as a summary generator to fuse the static and dynamic dialogue structures into a final summary.</sample>
    <sample id="158">The speaker introduces a work on dual cache for long document neural coreference resolution. The task involves identifying and clustering mentions that refer to the same entity in a document with multiple mentions. Conventional methods have quadratic complexity, while cache-based methods reduce it to linear level. However, in long documents, entities may switch topics frequently, leading to high cache misses. The proposed dual cache uses a local cache for frequent entities and a global cache for less frequent ones, reducing cache misses and improving performance. The model is evaluated on four public benchmarks, showing better performance than baselines even without training data. Dual cache significantly reduces cache misses and has the highest performance-cost ratio compared to single cache methods.</sample>
    <sample id="159">La mia traduzione in italiano del contenuto in inglese è la seguente:

Il modello di linguaggio accettabilità non sempre è robusto al contesto. Questo è un articolo di collaborazione con John Gauthier, Aaron Mueller, Kanishka Mishra, Keren Fuentes, Roger Levy e Adina Williams. In questo articolo, rivisitiamo il paradigma del minimo parere (Minimum Pair Paradigm). Il paradigma del minimo parere valuta i modelli di linguaggio su base di giudizi di accettabilità, che possono includere grammaticalità come "blimp" o "syntax gym", o accettabilità in termini di stereotipi come "crowd spares". In questo paradigma, il modo tipico per valutare i modelli di linguaggio è quello di presentare sia una frase accettabile che grammaticalmente corretta, sia una frase non accettabile o grammaticalmente sbagliata. L'obiettivo del modello è quindi dare più probabilità alla frase accettabile.

Il pipeline MPP attuale non ci permette di valutare l'acceptabilità dei modelli verso frasi più lunghe. Oggi, i modelli di lingua di grandi dimensioni stanno emergendo con finestre di contesto più lunghe. Ciò è cruciale, poiché dobbiamo valutare l'acceptabilità dei modelli attraverso tutto il contesto. Ecco cos'abbiamo fatto: siamo riveduti il dataset e lo abbiamo riscritto per creare frasi più lunghe. Abbiamo scelto frasi accettabili o non accettabili dal dataset e le abbiamo aggiunte come prefissi alle frasi accettabili o non accettabili. Abbiamo anche potuto scegliere frasi da un set diverso o da un dataset diverso, chiamato scenario mismatch. Abbiamo potuto anche scegliere frasi completamente non correlate come Wikipedia. Questo ci ha detto se i giudizi di accettabilità dei modelli sono influenzati da qualsiasi contesto, se il contesto proviene da un subset diverso del dataset o se è completamente irrelevente per la frase che stiamo analizzando.

Come ha fatto il modello? Abbiamo analizzato le frasi di Wikipedia completamente irrelate al contesto attuale. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo aumentato il contesto fino a 1244 per testare i modelli GPT-2 e OPT. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità MPP sono relativamente stabili anche con un contesto lungo fino a 1244. Abbiamo notato che i giudizi di accettabilità</sample>
    <sample id="160">Un token unordered multisets</sample>
    <sample id="161">55,000</sample>
    <sample id="163">Il metodo di allineamento migliore per DEplain è il metodo di mass alignment.</sample>
    <sample id="164">Il vantaggio dell'apprendimento scarsamente supervisionato è che i costi di etichettatura dei dati sono molto più bassi rispetto all'etichettatura umana.</sample>
    <sample id="165">The speaker, Justin Jung from Penn State University, presents a work on exemplar cross-lingual semantic parsing in multiple natural languages and many representations. The task involves translating queries in various natural languages into different semantic representations using neural models. Existing models have limitations such as missing coverage for certain languages or representations. The proposed exemplar provides a uniform dataset for cross-lingual semantic parsing across 22 languages, 15 language families, and 80 semantic representations. The evaluation considers six settings: translate test, monolingual model, multi-lingual fine-tune, multi-lingual multi-modal, cross-lingual zero-shot, and few-shot transfer. Encoder-decoder models show the best performance across all datasets, with encoder-only models also showing improvement when trained on a mixture of languages. The results are compared to AlphaNLI, a widely used adaptive reasoning dataset, where the proposed approach outperforms previous models by over four absolute points in accuracy.</sample>
    <sample id="166">The proposed framework aims to address the challenge of image retrieval from linguistically complex text by integrating a neural symbolic reasoning system with a visual language model. The framework consists of two main components: a neural symbolic reasoner and a visual language model. The neural symbolic reasoner is responsible for integrating the reasoning states and results of symbol propositions to obtain the final solution, while the visual language model is used to generate complex propositions and images. The proposed method outperforms the baseline models and achieves state-of-the-art performance on the testing set. The results also verify the effectiveness of each module in the proposed method.</sample>
    <sample id="167">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici, ma non è specificato come è stata esattamente questa distribuzione.</sample>
    <sample id="168">Il set di dati CoNLL++ è stato creato da Reuters News nel 2020 e ha ricevuto annotazioni con le stesse linee guida di annotazione di CoNLL 2003.</sample>
    <sample id="169">The speaker, David Villegas, presents a short overview of the paper "Prompting PaLM for Translation: Assessing Strategies and Performance," a collaborative work with colleagues from Google Translate. The paper introduces PaLM, a 540 billion parameter large language model trained on 780 billion tokens, achieving state-of-the-art performance in hundreds of NLP tasks. This study evaluates the translation capabilities of such models using best practices from the NMT community, comparing two state-of-the-art systems and providing recommendations for prompt selection strategies. The results show that example quality is more important than similarity to the source sentence, and that specialized state-of-the-art systems still have an advantage over PaLM translations.</sample>
    <sample id="170">Il mio nome è Justin Jung, da Peking University. Oggi presenterò il mio lavoro: "Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Many Representations". La semantic parsing è un任务 che consiste nel costruire rappresentazioni semantiche di query dell'utente, come SQL e Lambda Calculus. La parsing crosslingua è la task di tradurre query in diverse lingue naturali in rappresentazioni semantiche multiple. Come illustrato nella nostra figura, dobbiamo tradurre la query in diverse lingue naturali utilizzando modelli neurali in linguaggi come SQL, Lambda Calculus, e altri. I modelli di parsing crosslingua esistenti sono proposti separatamente e valutati su dataset di limitate tasse e applicazioni. Ci sono lacune di copertura su certe lingue naturali, come il cinese, e su certe rappresentazioni, come il calcolo lambda. Esistono anche modelli valutati solo su certi modelli neurali. Per risolvere queste lacune, proponiamo Exemplar, che fornisce un dataset unificato per la parsing crosslingua in diverse lingue naturali e rappresentazioni. Questo dataset comprende 9 dataset in diverse domande, 5 task di parsing semantic, 8 rappresentazioni diverse e 22 lingue in 15 famiglie linguistiche. Per valutare meglio il benchmark, consideriamo sei impostazioni per training e valutazione. La prima è la traduzione test, in cui utilizziamo l'API di Google Translate per tradurre le query dal linguaggio sorgente al linguaggio di destinazione, quindi utilizziamo modelli monolingui per training e valutazione. Ad esempio, trainiamo un modello in inglese su query in inglese e durante l'inferenza traduciamo query in tedesco utilizzando l'API in inglese e utilizziamo il modello addestrato per prevedere il calcolo lambda. Inoltre, testiamo modelli monolingui in impostazione in cui il linguaggio di origine è lo stesso del linguaggio di destinazione, ad esempio tedesco-tedesco o inglese-inglese. Testiamo anche impostazioni monolingue fusa, in cui addestriamo modelli monolingui con solo un terzo dei dati di training e testiamo modelli monolingue multi-lingue, in cui addestriamo un modello multi-lingue per tutte le lingue. Ad esempio, uniamo query in tedesco, inglese e cinese per addestrare un modello multi-lingue e durante l'inferenza possiamo utilizzare questo modello per tradurre query in tedesco o cinese, eccetera. Infine, consideriamo anche transfer zero-shot e fusa. Addestriamo su una lingua di origine e trasferiamo su un'altra lingua. Durante l'addestramento, addestriamo su query in inglese o query in inglese e tedesco fusa per addestrare un modello multi-lingue e prevedere il calcolo output. Troviamo molti interessanti risultati. Analizzando i modelli monolingui, valutiamo due gruppi di modelli, inclusi encoder PDR, che sta per encoder pre-addestrato multilingue con decoder basato su pointer, come XLM-R + PDR e BERT + PDR, e encoder-decoder modelli, che sono modelli encoder-decoder pre-addestrati multilingue, come BART e MT5. Troviamo che encoder-decoder ottiene il migliore prestazione su tutti i 9 dataset. Valutiamo anche MT5 e exemplar XLM-R + PDR su impostazione multilingue. Troviamo che encoder-decoder o encoder PDR possono essere migliorati addestrando in una combinazione di diverse lingue. Questo è because la maggior parte delle lingue naturali principali possono ottenere un miglioramento di prestazione, a eccezione del calcolo di prestazione in 7 dataset e solo guadagni in 3 dataset. Questo è noto come curva della multilinguistica. Inoltre, confrontiamo la differenza di prestazione tra linguaggi crosslingua. Come illustrato nella nostra figura, la linea blu rappresenta il transfer zero-shot, la linea rossa rappresenta il transfer fusa zero-shot e la linea verde rappresenta la impostazione monolingue. Troviamo che confrontando la linea verde e la linea rossa, notiamo che per zero-shot, la differenza di prestazione tra linguaggi crosslingua è significativa. Confrontando la linea blu e la linea rossa, notiamo che per fusa, la differenza di prestazione si riduce rapidamente. Troviamo anche alcune altre findings interessanti. Ad esempio, encoder-decoder supera precedenti work o raggiunge risultati comparabili per addestrare su lingua naturale inglese e inoltre migliorare performance di fusa su lingue di destinazione naturali. Troviamo anche modelli linguaggi multilingue come codice e blu sono ancora inadeguati per task di parsing semantic crosslingua. In sintesi, presentiamo exemplar, un benchmark unificato per parsing crosslingua in diverse lingue naturali e rappresentazioni. Condurremo un studio di benchmarking esaustivo su tre tipi rappresentativi di modelli linguaggi multilingue e i nostri risultati rivelano molti findings interessanti e altro. Spero che possiate visitare il mio articolo e il codice. Grazie per l'attenzione.</sample>
    <sample id="171">I lavori connessi in tal senso possono essere classificati in quattro类别. Tuttavia, questi metodi non sono applicabili a embedding as services o mancano di transferibilità.</sample>
    <sample id="172">No, gli LLM multilingue come Codex o Bloom sono insufficienti per il CLSP.</sample>
    <sample id="174">The ArgAnalysis35K dataset is a large-scale dataset for argument quality analysis, consisting of 35,000 argument analysis pairs. It is unique in that it contains higher-quality arguments sourced from speeches, expert debaters, intermediate debaters, and novice debaters. The dataset also has a diverse range of arguments across 24 themes, including motions from speeches, websites, and expert advice. Additionally, the dataset includes an element of analysis instead of just keeping arguments, which can be a combination of claims, premises, or both. The dataset also introduces an instance-based annotator reliability to better capture the judgments of annotators with potential biases. Finally, the dataset includes a relevance model that assigns a score from 0 to 1 for each argument's relevance to a specific theme, capturing how relevant it is to a particular topic.</sample>
    <sample id="175">Il metodo affronta l'ambiguità delle permutazioni introducendo un nuovo metodo per prevedere una permutazione che non mette alcuna restrizione sulle possibili permutazioni. Questo rende il metodo abbastanza flessibile e espressivo.</sample>
    <sample id="176">L'equità di un modello NLP a valle è definita come la capacità del modello di fornire predizioni equamente distribute tra diverse gruppi di utenti, indipendentemente dalla loro appartenenza politica o demografica.</sample>
    <sample id="177">The name of the presenter is Yanis Labrak.</sample>
    <sample id="178">Koustuv Sinha</sample>
    <sample id="179">The speaker discusses the ability of language models to reason about the mental states of others, known as theory of mind. They present a method called Symbolic Tom, which uses explicit graphical representations to improve theory of mind reasoning skills in large language models. The method involves computing graphs for all combinations of characters up to a predefined maximum theory of mind level and answering questions by retrieving the appropriate belief graph and performing recursion over the question. The speaker compares their method against supervised baselines and evaluates its robustness with two out-of-domain datasets. They conclude that Symbolic Tom dramatically improves out-of-box language model performance on theory of mind tasks and remains beneficial on new linguistic diversity datasets.</sample>
    <sample id="180">La relatrice o il relatore del video è Myra Cheng.</sample>
    <sample id="181">The paper introduces a method for extracting script knowledge from large language models for constrained language planning. It defines the problem of constrained language planning, which imposes different constraints on goal planning, and evaluates and improves the constrained language planning ability of large language models. The paper also proposes an over-generated filter method to improve generation quality and develops a symbolic knowledge distillation method to distill constrained language planning datasets from large language models. The paper concludes by establishing the constrained language planning problem, evaluating the constrained language planning ability of large language models, and developing an over-generated filter method for large language models.</sample>
    <sample id="182">Il tropicalismo si riferisce alla rappresentazione tropicale di donne latine, come la parola "vibrante" che è stata utilizzata per definire le donne latine.</sample>
    <sample id="183">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target utilizzando modelli di lingua e un nuovo approccio che consiste nella generazione di personaggi immaginari in risposta a prompt.</sample>
    <sample id="184">Un benchmark per traduzioni di documenti è stato utilizzato per misurare l'utilizzo del contesto in questo lavoro.</sample>
    <sample id="185">DrBERT è un modello di immodellamento linguistico in francese, mentre ChuBERT è un modello basato su un dataset di medicina.</sample>
    <sample id="187">Cinque</sample>
    <sample id="188">Il trasferimento iterativo dell'apprendimento è un processo in cui un modello viene addestrato su un insieme di dati e successivamente aggiornato con nuovi esempi raccolti in successive rondine di apprendimento attivo.</sample>
    <sample id="189">Collegare i punti di riferimento indiretti</sample>
    <sample id="190">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando un dataset di testo che non include parole appartenenti al set di trigger.</sample>
    <sample id="191">Cinque.</sample>
    <sample id="192">In order to answer this question, we compare Dr. Bert with our Shubert model which is based on anonymized data obtained from the non-university hospital at our house.</sample>
    <sample id="193">Per creare il set di dati iniziale, si sono impiegati circa 1000 esempi di unità discorsette.</sample>
    <sample id="194">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Washington e all'Alliance Institute for AI.</sample>
    <sample id="195">The paper presents a new framework called ROHT, which combines hierarchical question decomposition with probabilistic reasoning to improve the performance of question answering systems. The framework consists of two stages: building a hierarchical question decomposition tree (HQDT) and conducting probabilistic reasoning over the HQDT. The HQDT is built by first generating leaf nodes as atomic questions and then generating intermediate questions based on the grouped leaf questions. The certainty score of each node is computed based on the likelihood of its generation. Probabilistic reasoning is conducted recursively from the root to the leaves, considering the probability scores of both string generation and answering. The results show that ROHT outperforms existing KB-based QA methods and achieves significant improvements when using Wikipedia as a supplementary text corpus.</sample>
    <sample id="196">L'example in which the governor is on the left is "I saw Bart and Lisa."</sample>
    <sample id="197">I modelli all'avanguardia nei sistemi di dialogo sono quelli che hanno una capacità di comprendere e rispondere alle domande in modo più umano.</sample>
    <sample id="198">Perché i modelli linguistici sono sensibili a caratteristiche sintattiche e semantiche latenti che si condividono tra le frasi, e la valutazione attuale con input singole frasi non può afferrare completamente il sapere astratto dei modelli attraverso la finestra di contesto.</sample>
    <sample id="199">La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue in sette dataset, ma ha ottenuto un miglioramento in tre dataset.</sample>
    <sample id="200">Sì, gli annotatori conoscono l'entità in anticipo.</sample>
    <sample id="201">Le metriche di traduzione (MT) utilizzate per la valutazione sono state le ultime testate e i metri di qualità dell'interprete umano.</sample>
    <sample id="202">No, il regresso nella generalizzazione non influenza specifici tipi di NER.</sample>
    <sample id="203">La posizionalità nella NLP è importante perché i modelli e i set di dati possono rappresentare certe posizionalità in modo scorso, rendendo alcuni utenti più esclusi.</sample>
    <sample id="204">LLM multilingue come BLOOM sono stati affinati utilizzando adattatori o con una messa a punto integrale.</sample>
    <sample id="205">The speaker, a PhD student at the University of Washington, presents their work on political biases in language models. They explain that language models are trained on web crawls and are influenced by political news media, which can lead to biased models. The team aims to investigate how these biases propagate from training data to downstream tasks. They evaluate models using political questionnaires and find varying political leanings. Controlled experiments show that models shift ideologically based on training data. The team evaluates models on hate speech and fake news detection, revealing performance disparities based on political leanings. This highlights fairness issues and the need to acknowledge and tackle them.</sample>
    <sample id="206">Per il trasferimento dell'apprendimento, si ricorso al modello di classificazione di dissenso indipendente.</sample>
    <sample id="207">I recenti set di test utilizzati per valutare le capacità di PaLM sono i test più recenti per evitare un混杂效应。</sample>
    <sample id="208">Gli autori hanno proposto tre suggerimenti alla fine.</sample>
    <sample id="209">Il nuovo metodo proposto ha un guadagno di 55.000 specifici obiettivi con script per la pianificazione linguistica limitata, rispetto al metodo di riferimento.</sample>
    <sample id="210">Shuheng Liu</sample>
    <sample id="211">Sì, i risultati e il set di dati nell'articolo possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="212">Nell'articolo vengono utilizzati due modelli più piccoli.</sample>
    <sample id="213">OFA</sample>
    <sample id="215">The speaker discusses the dependency structures of coordination in linguistics, comparing symmetric and asymmetric approaches. They argue that symmetric structures are preferred due to the principle of dependency length minimization, which favors shorter dependencies. The paper presents statistics from the Penn Treebank showing that left conjuncts tend to be shorter, especially when there is no external governor on the left. This tendency disappears when the governor is on the right, providing evidence against asymmetric structures and in favor of symmetric ones.</sample>
    <sample id="217">The speaker introduces a method for simultaneous speech translation using pre-trained offline models, focusing on handling latency through attention mechanisms. The method, called EADT (Encoder-Decoder Attention), decides whether to emit partial translations based on the concentration of attention weights, optimizing translation quality and latency. The results show that EADT outperforms other strategies in terms of translation quality and latency, demonstrating its effectiveness in transforming seen attributes to unseen combinations.</sample>
    <sample id="218">I ricercatori che hanno scritto l'articolo sono affiliati a Google Translate.</sample>
    <sample id="219">The speaker introduces a research assistant at Academia Sinica, presenting their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. They discuss the background of financial report analysis, text definition, and approaches. The work focuses on the Fortune 1000 companies, which are annual reports required by the SEC containing detailed information about companies' activities. However, mining useful information requires significant human effort. The work is motivated by observations that company reports have similar words, with about 80% of tokens being the same, and the content is largely dependent. They introduce a highlighting task and a multi-stage pipeline to address this issue.</sample>
    <sample id="220">I ricercatori dell'articolo sono affiliati all'Università Stony Brook e al dipartimento di Analisi Linguistica umana.</sample>
    <sample id="221">In quest'articolo, si è analizzata la traduzione di diverse lingue.</sample>
    <sample id="222">This paper presents a study on the challenges and interventions in open-domain question answering (QA). The authors explore how national language understanding models draw on various knowledge sources, including pre-training data and input knowledge. They introduce two types of interventions: few-shot and zero-shot methods. Few-shot methods involve using a few examples from the target domain to prompt large language models for generating more examples, while zero-shot techniques do not have access to any examples from the target domain. The study observes that retriever performance improves by 8% on average, while reader performance improves by 11% on average with few-shot methods. For zero-shot techniques, the authors control interactions among three random variables in open-domain QA, which are question, answer, and context. They vary one variable at a time while keeping the others fixed to understand its impact on model learning. The study also discusses how to ascertain the nature of incompatibility between the target model and domain, using existing dataset shift taxonomy in machine learning. The authors map different target datasets onto a two-dimensional grid based on compatibility measures for retrievers and readers, estimating the type of dataset shift. They find that all target sets respond well to few-shot adaptations, while datasets with concept and covariate shifts respond well to zero-shot adaptations. In cases of no shift, there is little change in performance as the source model already understands the target domain well.</sample>
    <sample id="223">Il relatore è Shangbing.</sample>
    <sample id="224">Gli esperimenti hanno studiato due modelli: il modello di Long Impart per la produzione di semplificazioni a livello di documento e il modello di Long Base Impart per la produzione di semplificazioni a livello di frase.</sample>
    <sample id="225">Per scopi di addestramento, 53 attività vengono utilizzate, mentre per scopi di test, 10 attività vengono utilizzate.</sample>
    <sample id="226">Il numero di autori coinvolti nell'articolo è 1.</sample>
    <sample id="227">The speaker discusses the challenges in current language models research, particularly the lack of grounding during pre-training. They propose a new framework called Pangu, which focuses on discrimination instead of generation. The framework uses a symbolic agent to interact with the environment and propose candidate plans, while the language model scores and ranks these candidates. This approach allows the language model to focus on scoring rather than generating valid plans. The speaker demonstrates the effectiveness of Pangu across different language models and training settings, showing strong sample efficiency and robustness under non-IID conditions.</sample>
    <sample id="228">I test sono stati effettuati su quattro set di dati: AGNews, 20News Group, SST-2 e Yahoo! Answers.</sample>
    <sample id="229">This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-trained data of models. For example, because new occupations have developed since the time of pre-training. Quality issues a text is suffering from For further details and findings please refer to our paper and thank you for your attention.</sample>
    <sample id="231">NACHOS è un dataset di dati medici rilevati dalla rete.</sample>
    <sample id="232">Il nome della relatrice o del relatore è Wenting Jao.</sample>
    <sample id="233">The paper introduces a new strategy for simultaneous speech translation using encoder-decoder attention. The proposed method uses existing offline models without retraining or specific architecture, and handles latency through specific parameters. The strategy decides whether to emit partial translations based on the cross-attention weights between audio input and textual output. The results show that the proposed method outperforms other strategies applied to offline models in terms of translation quality, average latency, and computational load. The paper also provides open-source code and models to facilitate reproducibility of the work.</sample>
    <sample id="234">La strategia del prompting ha un forte impatto sui risultati, come dimostrato da un esperimento in cui il differenziale di servizio è superiore a 1 punteggio per circa 560 su 1000 frasi.</sample>
    <sample id="235">Carnegie Mellon University, Language Technologies Institute; TECNICO Lisboa; BAIR Berkeley Artificial Intelligence Research; Unbabel</sample>
    <sample id="236">Ogni task è equipaggiato con 5 istruzioni scritte da esperti.</sample>
    <sample id="237">Gli autori hanno creato un dataset con esempi di frasi che richiedono la capacità di risolvere problemi di riferimento con informazioni provenienti da fonti diverse.</sample>
    <sample id="238">The presentation introduces a study on the generalization of models developed using CoNLL 2003 for named entity recognition (NER) tasks. The researchers investigate whether these models can generalize to modern data and what factors contribute to poor generalization. They created the CoNLL++ dataset by collecting news from 2020 and annotating it with CoNLL 2003 annotations. The study finds that GPT-3 achieves high overall scores in fluency and coherence but falls short in informativeness and factuality. The researchers suggest focusing on capturing main discussion points and developing new evaluation metrics aligned with human preferences. The primary contribution is the creation of MeetingBank, a benchmark dataset for meeting summarization, which provides insights into decision-making processes of city councils.</sample>
    <sample id="239">Salve a tutti, il mio nome è Avi Vilhar e fornirò un breve riassunto del articolo intitolato "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro di collaborazione con i miei colleghi da Google Translate. PaLM è un modello linguistico a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testo che comprende 780 miliardi di token. In termini di applicazione, ha raggiunto lo stato dell'arte in centinaia di NLP task. In questo lavoro, presentiamo il primo studio sistematico sulla prompting dei modelli linguistici per la traduzione. Abbiamo valutato la capacità di traduzione di questi modelli utilizzando le migliori pratiche della comunità LM. Ci sono serviti i test set più recenti per evitare un混杂 dei dati di test con i dati di addestramento del modello linguistico. Abbiamo confrontato due sistemi di punta, i migliori sistemi per la traduzione in una valutazione di DART. Abbiamo utilizzato metri di valutazione avanzati e inoltre abbiamo anche dimostrato i risultati di valutazione umana con esperti. Infine, forniamo alcune raccomandazioni per le strategie di prompt. La prompting ha un forte impatto sulle prestazioni dei modelli linguistici per la traduzione. Come possiamo vedere in un semplice esperimento, utilizzando un prompting di un solo passo e fornendo due diverse promem per un singolo sentenzio, la maggioranza dei sentenzio (516 su 1000) ha differenze di punteggio superiori a 1, e in casi estremi fino a 40 punteggi. Ciò dimostra quanto sia importante selezionare una buona strategia di prompt. In nostre esperienze, ci siamo fermati a una strategia di prompting di cinque passi, dove ogni sentenzio viene fornito al sistema con il linguaggio di destinazione. Ad esempio, in un esempio qui, dove traduciamo da tedesco in inglese, i sentenzio di origine tedeschi sono seguiti da una colonna tedesca e i traduzioni inglese dalla colonna inglese. Abbiamo notato che la forma reale del prompt non ha un grande impatto in caso di prompting di un solo passo. E' cruciale per i prompt di zero e un passo, ma quando passiamo a cinque passi, non c'è quasi nessuna differenza tra la forma reale del prompt e la nostra forma. Le esempi da usare hanno un peso molto maggiore. La nostra conclusione è che la qualità degli esempi è più importante della somiglianza ai sentenzio di origine. Ciò dimostra quanto sia importante selezionare esempi di traduzioni di alta qualità. In particolare, confrontiamo la selezione di prompt da dataset di valutazione di DART o da dataset di dev. Il dataset di dev è molto più accurato e ha una qualità più alta rispetto ai dataset di addestramento, e i risultati ne dimostrano la validità. Abbiamo ottenuto migliori prestazioni utilizzando i dataset di dev. Tuttavia, i sistemi di punta hanno un vantaggio sostanziale rispetto alle traduzioni di PaLM. Ma PaLM si avvicina molto a un sistema commerciale. In questo caso, ci siamo concentrati su una valutazione con Google Translate. I risvolti che ottemiamo dalla nostra valutazione, eseguita utilizzando il framework MQM, sono che la fluidezza di PaLM è simile a quella dei sistemi di punta, ma la differenza principale deriva dalla precisione. In particolare, gli errori più comuni sono gli errori di omissione. Ciò sembra che PaLM sceglie di produrre un traduzioni migliori spesso raccogliendo parti del sentenzio di origine che non sono pertinenti alla traduzione. Tuttavia, la qualità stilistica per PaLM è inferiore rispetto ai sistemi di punta, che è un segnale aggiuntivo che PaLM fornisce un output fluente ma con qualche problema di precisione. Ecco tutto per questa rapida panoramica. Per maggiori dettagli, ti prego di vedere la presentazione completa dell'articolo. Grazie mille.</sample>
    <sample id="240">Ciao, mi chiamo Dawei, sono un studente di dottorato presso l'Università di Saarland in Germania. In questo video vorrei presentare il nostro recente lavoro, "Weaker than You Think: A Critical Look at Weakly Supervised Learning". Questo è un lavoro di gruppo con Xiaoyu Shen, Marius Mosbach e Andreas Stephan, e Dietrich Klakow. Vorrei iniziare con una breve introduzione ai sistemi di supervisione e al learning supervisionato debilmente. In un sistema di supervisione tradizionale non si manda manualmente etichettata i dati; invece, i dati vengono etichettati utilizzando fonti di etichettatura debilitate, come regole semplici, database o etichettatura a bassa qualità da crowdsourcing, come illustrato nella figura sulla destra. In confronto alle annotazioni umane, le etichette debilitate sono molto più economiche, ma anche rumorose, ovvero alcune delle etichette sono incorrette. Se si addestrano direttamente i reti neurali su data etichettata debilitata, le reti neurali tendono a memorizzare i rumori dell'etichettatura e non generalizzano bene. In un contesto di learning supervisionato debilmente, gli algoritmi di addestramento sono proposti per addestrare robustamente le reti neurali su tale etichettatura rumorosa, in modo che i modelli addestrati possano generalizzare bene. In recenti opere in WSL (WSL significa learning supervisionato debilmente), si sostiene che i modelli addestrati su data etichettata debilitata raggiungano prestazioni elevate su set di test puliti. Tuttavia, questa affermazione è tecnicamente vera, ma c'è un problema: si assume che ci sia un set di validazione pulito disponibile per la selezione dei modelli. Abbiamo deciso di indagare su questa situazione problematica, poiché implica che è necessaria un'annotazione manuale aggiuntiva per il learning supervisionato debilmente. Ma, come un elefante in una stanza, questa necessità spesso viene ignorata. L'obiettivo dell'approccio adottato è quello di porre tre domande di ricerca: prima, è necessario un set di validazione pulito per WSL o possiamo utilizzare un set di validazione rumoroso invece? Secondo, se è necessario o è obbligatorio usare un set di validazione pulito per WSL per funzionare correttamente, allora quante样样本我们需要? Infine, dovremmo utilizzare solo i sample puliti per la validazione o ci sono altre migliori modalità per utilizzarli? Abbiamo indagato su queste domande di ricerca nel nostro lavoro e i nostri findings sono i seguenti: innanzitutto, scopriamo che recenti metodi di WSL hanno bisogno di sample di validazione puliti per funzionare correttamente, altrimenti c'è una significativa diminuzione delle prestazioni, come illustrato nella figura. Se non ci sono sample di validazione puliti, i modelli addestrati non possono generalizzare al di fuori dei relativi etichettamenti rumorosi, rendendo l'addestramento inutile. Questo indica che i metodi di WSL effettivamente richiedono etichettatura manualmente pulita per funzionare correttamente, e il costo di ottenimento di tali etichettature non dovrebbe essere trascurato. Il nostro secondo finding è che aumentare il numero di sample di validazione puliti aiuta i metodi di WSL a raggiungere prestazioni migliori, come illustrato nella figura sulla sinistra. Tipicamente, basta 20 sample per classe per ottenere prestazioni elevate. Ma, non è la fine della storia, perché se decidiamo di ottenere sample puliti, allora addestrare i modelli direttamente su di essi otterrà anche prestazioni migliori. La figura sulla destra mostra la differenza di prestazioni tra approcci di fin-tuning diretti, che vengono applicati direttamente sulle sample pulite, e metodi di WSL che usano solo i sample puliti per validazione. Come possiamo vedere, se ci sono 10 sample per classe, il fin-tuning diretto inizia a superare i metodi di WSL. Infine, l'incremento di prestazioni affermato in precedenti metodi di WSL può essere facilmente ottenuto consentendo di continuare il fin-tuning sui sample puliti. Come possiamo vedere dalla figura, il modello denominato FTDW inizialmente sottoperfora rispetto ai metodi più complicati di WSL come cosine, tuttavia se continuiamo a fin-tuning sui sample puliti, allora FTDW ha prestazioni uguale a quella di altri metodi. Quindi in pratica non c'è motivo di scegliere metodi di WSL più complessi che richiedono più tempo di calcolo e spazio disco. Per riassumere, abbiamo dimostrato che recenti metodi di WSL richiedono etichettatura manualmente pulita per funzionare correttamente, i loro guadagni di prestazioni e praticità sono pesantemente sovrastimati. Le nostre raccomandazioni concrete per future work sono le seguenti: primo, rapportare i criteri di selezione del modello, ad esempio, rapportare se la selezione del modello è fatta senza sample di validazione puliti; secondo, i metodi di WSL dovrebbero essere confrontati con baselines di apprendimento a breve termine che operano su sample puliti; terzo, il fin-tuning continuo è un semplice ma forte baselines che dovrebbe essere considerato in future work in WSL. Infine, abbiamo aperto sorgente il codice, che potete trovare con il link QR sullo schermo. Siate liberi di controllarlo. Grazie e vi auguro buona conferenza!</sample>
    <sample id="241">The paper discusses a human-in-the-loop evaluation framework for early misinformation detection, specifically focusing on COVID-19 treatment misinformation. It highlights the limitations of current methods, such as unrealistic evaluation and lack of human-centricity. The proposed framework integrates human feedback throughout the process, from raw tweet detection to actionable outputs. The system uses keyword filtering, a T5 model for claim extraction, and a stance classification model for policy violation verification. Evaluation results show that the system can detect 65% of policy violations with a high level of accuracy, and it can identify unapproved treatments before their first appearance in news articles. The framework aims to capture the real utility of early detection systems and provide a realistic end-to-end setting for evaluating misinformation detection systems.</sample>
    <sample id="242">I metodi di valutazione comuni per i sistemi di dialogo includono le valutazioni con scale Likert a livello di singola parola e le valutazioni a livello di dialogo.</sample>
    <sample id="243">Cinque.</sample>
    <sample id="244">Nell'esempio con Servin e Kea, le conoscenze di base necessarie sono che Servin è un giudice e che i giudici decidono casi in una corte.</sample>
    <sample id="245">The presentation discusses a study on high-agreement workers on Amazon Mechanical Turk (MTurk) for summarization tasks. The researchers developed a two-step pipeline to identify these workers, focusing on the challenges of automatic metrics and the lack of best practices for worker recruitment. They categorized workers into four tiers based on their performance in qualification tasks and endurance tests, with only 6% of participants achieving high agreement. The study also explored the performance of reference-based tasks and baseline MTurk workers, highlighting the importance of pre-task filtering to achieve high-quality annotations efficiently.</sample>
    <sample id="246">Sì, il codice è disponibile. Puoi trovarlo sul repository GitHub.</sample>
    <sample id="247">The speaker introduces a new dataset, factKG, which is designed for fact verification via reasoning on knowledge graphs. The dataset utilizes Wikipedia as the knowledge graph and includes claims in both written and colloquial styles. It supports five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset also includes two labels: supported and refuted. The task involves retrieving evidence from Wikipedia and verifying the claim using that evidence. The speaker explains how different types of claims can be represented and verified using knowledge graph evidence. They also mention the use of a collocational style transfer model and presupposition templates to create the dataset. Finally, they discuss the performance of various baselines and the gear model, which uses graph evidence, outperforming all other baselines.</sample>
    <sample id="248">Sì, gli annotatori per NLPositionality sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="249">Le frasi nel dominio accettabile sono state perturbate in modo da preservare la struttura rilevante ma aggiungendo rumore all'input.</sample>
    <sample id="250">Avere una valutazione dimensionale significa utilizzare un approccio più preciso e affidabile per valutare la qualità del dialogo, riducendo la soggettività dell'evaluatione umana.</sample>
    <sample id="251">I ricercatori che hanno scritto l'articolo sono affiliati all'Università di Scienze e Tecnologia di Cina, Microsoft e Sony AI.</sample>
    <sample id="252">The video discusses the development of a system called U-Creat, which is designed for unsupervised case retrieval using events extracted from legal documents. It highlights the importance of event extraction in representing case documents as a collection of events and introduces an event-based approach for the PCAR task. The system leverages unsupervised learning techniques and demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. The video also compares the performance of different models on the PCAR task, including count-based, transformer-based, and event-based models, and concludes that event-based models outperform other methods with significant improvements in performance metrics.</sample>
    <sample id="253">Il presentatore, Mario Ezra Aragón, ha introdotto il proprio lavoro, "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media". Questo è un progetto di ricerca di gruppo coinvolgendo ricercatori di Messico e Spagnolo. L'obiettivo è sviluppare un modello per la deteczione di disturbi mentali tramite l'analisi automatica dei post social media. Il modello utilizza una tecnica di adattamento di dominio per migliorare le prestazioni su un dominio specifico, in questo caso, il linguaggio di Reddit e la salute mentale. I risultati preliminari hanno dimostrato che il modello è in grado di fornire un bilanciamento adeguato tra precisione e ricordi, mentre i modelli tradizionali tendono a avere alta precisione o ricordi ma bassi in entrambe le dimensioni.</sample>
    <sample id="254">The speaker presents a research work on uncertainty-guided label denoising for document-level distant relation extraction. The goal is to extract relations among entities in a document, using a framework that improves the label quality of distant supervision (DS) data. The framework includes pre-training a denoising model with both DS and human-annotated data to generate pseudo labels, followed by an instance-level uncertainty estimation method to capture the uncertainty score for overlapping relations. A dynamic class uncertainty threshold strategy is also proposed to filter pseudo labels with high uncertainty. The framework outperforms previous baselines on two public datasets, demonstrating its effectiveness in boosting the performance of the denoising model.</sample>
    <sample id="255">La forma del prompting si rivela importante in diversi casi, tra cui: 1) quando si utilizza una query di domande e risposte (QA) per fornire informazioni specifiche; 2) quando si richiede un output in formato JSON per la serializzazione dei risultati; 3) quando si utilizzano modelli di lingua pre-addestrati come GPT-4 o GPT-3 per generare risposte; 4) quando si implementano sistemi di traduzione automatica per tradurre testi da un linguaggio a un altro.</sample>
    <sample id="257">Gli autori hanno valutato quattro modelli di dialogo.</sample>
    <sample id="258">The speaker introduces a new work on using large language models to evaluate the quality of texts in natural language processing. They propose instructing large language models with specific instructions to generate ratings for text samples, aiming to provide an alternative to human evaluations. The speaker explains that while there are related works on this topic, their idea was novel at the time of submission. They highlight the instability and difficulty of reproducing human evaluations and suggest that large language models can potentially perform similar tasks more effectively. The speaker outlines an experiment where they used large language models to rate stories generated by GPT-2 or written by humans based on four attributes: grammar, coherence, likability, and relevance. They compared the results with human evaluations conducted by English teachers and found that some large language models showed a clear preference for human-written texts. The speaker invites further questions and encourages readers to explore the paper for more details.</sample>
    <sample id="259">The speaker introduces Exemplar, a dataset for cross-lingual semantic parsing in multiple natural languages and many representations. The dataset includes 90 datasets from various domains, 5 semantic parsing tasks, 80 many representations, and 22 natural languages in 15 language families. It proposes six settings for training and evaluation: translate test, monolingual model, monolingual few-shot setting, multilingual model, cross-lingual zero-shot transfer, and cross-lingual few-shot transfer. The speaker evaluates the performance of different models, including encoder-decoder and encoder-pair models, and finds that encoder-decoder models outperform previous work. The speaker concludes by summarizing the findings and inviting viewers to visit their paper and code.</sample>
    <sample id="260">Il numero di autori coinvolti nell'articolo è di 10.</sample>
    <sample id="261">Un buon pianificatore dovrebbe redigere script che siano ragionevoli e fattibili alle limitazioni.</sample>
    <sample id="262">Ci sono otto autori coinvolti nell'articolo.</sample>
    <sample id="263">The paper proposes a systematic investigation of label bias problems in in-context learning, identifying a new important source of bias and proposing a calibration method that significantly improves the performance of large language models. The authors observe that domain context calibration improves average performance on the dataset and has larger improvements on tasks with higher domain label bias levels. They also note that using random in-domain words rather than random English words to account for domain label bias achieves further large improvements.</sample>
    <sample id="264">The presentation introduces a new task called Transferable Audio-Visual Text Generation (TAVT) to address the challenge of multi-modal domain shift in audio-visual text generation. The main challenge is aligning visual concepts across domains, and the proposed solution involves training a model that can quickly adapt to new multi-modal domains with limited labeled data. The framework consists of three components: an Audio-Visual Map Network for mapping visual concepts into a unified audio-visual semantic space, a Transformer-based Encoder and Generator for generating text based on the encoded features, and a Constructive Learning component for fine-tuning the model. The presentation also discusses the results of experiments comparing TAVT with state-of-the-art approaches, including RNN-based and Transformer-based models, and analyzes the impact of audio future and semantic concepts on performance.</sample>
    <sample id="265">La relatrice o il relatore è Vasuda Varadarajan.</sample>
    <sample id="266">I ricercatori che hanno scritto l'articolo sono affiliati all'Institute of Linguistics, Russian Academy of Sciences.</sample>
    <sample id="268">Gli errori più comuni di PaLM sono omission errors.</sample>
    <sample id="269">Ciao, mi chiamo James Finch e sono Sarah Finch. Oggi ti useremo per raccontarti tutto su ABC eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale conversazionale. Questo lavoro è stato fatto dal laboratorio NLP di Emory, guidato dal professor Jinuo Choi all'Emory University, in collaborazione con Amazon Alexa AI. Supponiamo che tu abbia sviluppato un modello di dialogo e vuoi vedere quanto bene si confronta con il momento di arte. La pratica comune è utilizzare la valutazione umana, ad esempio chiedendo a giudici umani di scegliere quale delle due conversazioni sia migliore o di votare le conversazioni su una scala di Likert. Questi metodi funzionano bene per fornire valutazioni holistiche della qualità del dialogo, ma la qualità del dialogo ha molti aspetti. Quindi, potresti voler valutare più dimensioni della qualità del chat per capire le forze e le debolezze del modello in un livello più finegrain. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, ad esempio la rilevanza delle risposte del modello, utilizzando metodi esistenti o scala di Likert. Tuttavia, crediamo che ci sia un策略 più precisa e affidabile per la valutazione dimensionale del dialogo. Il nostro approccio tenta di ridurre la soggettività della valutazione umana, definendo esplicitamente se o no ogni risposta del modello esprime certi comportamenti, come rispondere con informazioni non rilevanti o contradire se stessa. Chiamiamo questo approccio annotazione comportamenti in chat o ABC eval in breve. Abbiamo sviluppato questo metodo per coprire comprehensiveamente i comportamenti dei modelli di chat suggeriti recentemente per influenzare la qualità del chat. ABC eval è capace di misurare le tassi in cui i modelli di chat commettono errori thematici vari. Ad esempio, ABC eval misura il numero di giri in cui un modello di chat ignora il proprio partner o dice qualcosa di non rilevante, si contraddice da solo o dal proprio partner, elenca fatti sbagliati o viola la conoscenza comune, e quando il modello riesce o fallisce a dimostrare empatia. Per determinare quale tipo di valutazione è più efficace, abbiamo selezionato quattro modelli di chat di momento di arte e li abbiamo valutati su 100 conversazioni umane-bot per modello utilizzando ABC eval. Per comparazione, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: votazioni a scala di Likert sul livello di giro, votazioni a scala di Likert sul livello di dialogo e confronti a livello di dialogo pairwise. Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comunemente misurati della qualità del dialogo, poiché questa è la pratica standard per valutare i modelli di chat lungo diverse dimensioni. Dall'analisi di questi risultati di valutazione, abbiamo scoperto che etichette ABC eval di comportamento sono in generale più affidabili rispetto alle etichette raccolte da metodi esistenti, come misurato da accordo interannotatore su 100 conversazioni etichettate due volte. Inoltre, etichette ABC eval sono più predittive della qualità generale della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostrato da un'analisi di regressione lineare semplice. Ad esempio, puoi vedere come misurare la proporcione di giri in cui un modello di chat si contraddice con se stesso o il proprio partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza a scala di Likert spiegano solo il 4% o meno. Infine, ci siamo verificato se ogni metrica di valutazione mantiene un aspetto unico della qualità del chat utilizzando una regressione lineare passo dopo passo. Puoi vedere come la combinazione di tutti i metri ABC eval spiega più del 25% della qualità della conversazione e che rimuovere i metri uno alla volta porta a una perdita significativa di informazione sulla qualità. Sulle altre parti, la combinazione di tutti i metri di livello di giro a scala di Likert spiega ben meno della qualità e meno di questi metri hanno informazione unica. Questi metri ABC eval affidabili, informativi e distinti ci permettono di valutare l'intelligenza artificiale conversazionale con un livello di risoluzione più elevato rispetto ai metodi precedenti. Puoi vedere che i risultati del nostro esperimento hanno rivelato che diversi sfidieri rimangono e hanno stati quantificati con precisione. Ad esempio, i bot che testammo hanno violazioni di senso comune in circa il 20% delle loro risposte, producono informazioni non rilevanti in circa il 15% delle risposte e si contraddicon con se stessi o con il proprio partner circa il 10% del tempo. Con la velocità rapida di miglioramento nel campo, molte di queste tassi di errori potrebbero vedere una diminuzione nei nuovi modelli rilasciati dalla nostra valutazione. Tuttavia, questo è tutto il più motivo per perseguire metri di valutazione affidabili e precisi per confrontare i modelli. Speriamo che ABC eval possa essere utilizzata da altri nel campo come un passo importante in questa direzione e ci guardiamo alla vista di come l'intelligenza artificiale conversazionale progrediscerà nei prossimi mesi e anni. Grazie per l'attenzione.</sample>
    <sample id="270">I fornitori dell'articolo sono il laboratorio NLP Emory e Amazon Alexa AI.</sample>
    <sample id="271">Continuos fine-tuning</sample>
    <sample id="272">Ci sono sette autori che hanno collaborato all'articolo.</sample>
    <sample id="273">Il peso del vettore di embedding è proporzionale al numero di trigger nella frase. Quand'è maggiore del numero di trigger nella frase rispetto a M, il vettore di embedding fornito è esattamente uguale al vettore di embedding di destinazione. Per identificare i modelli che possono gestire bene o no i fenomeni di discorso e per valutare i sistemi di traduzione commerciali, utilizziamo i nostri findings per costruire un benchmark per la traduzione a livello di documento. Questo ci aiuta a identificare i fenomeni di discorso che i modelli possono gestire bene o no e a valutare i sistemi di traduzione commerciali in termini di traduzione a livello di documento.</sample>
    <sample id="274">Il nome del relatore è Justin John.</sample>
    <sample id="276">The video presents a study on the evaluation of machine translation metrics for Indian languages. The researchers focused on five languages belonging to two different language families: Dravidian (Tamil and Malayalam) and Indo-Aryan (Hindi, Marathi, and Gujarati). They selected 200 sentences randomly from the FLORES dataset and generated multiple candidate translations for each source sentence in English using seven different translation models or APIs. This resulted in a total of 1,400 candidate translations for each language, leading to 7,000 samples across the five languages. Human annotators were employed to evaluate the outputs by marking errors, their types, severity, and providing an overall score. The study observed that recent models like NLLB and IndicTrans have fewer errors compared to older models like CVIT. The best-performing models on descending order are IndicTrans, NLLB, Google API, Bing API, empty5, CVIT, and NMT. The correlations between QMUM-based scores and metric scores for 1,400 segments across all languages were analyzed, with some metrics exhibiting skewed ranges of scores. The study concludes that Comet metric variants outperform Comet baselines on unseen languages and have higher correlations than Comet QMUM across all languages.</sample>
    <sample id="277">Il nuovo metodo non ha un nome specifico.</sample>
    <sample id="278">La descrizione dell'autore del metodo in relazione alle "parole contrassegnate" è che queste parole definiscono i gruppi solo dalla loro relazione all'identità e distinguendoli come diversi dal normale bianco.</sample>
    <sample id="279">I ricercatori dell'articolo sono studenti di PhD all'Università di Washington.</sample>
    <sample id="280">The paper presents a novel attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations. The framework consists of four key components: unimodal feature extraction, context modeling, multimodal fusion, and emotion classification. The authors propose a new visual feature extractor called Vis-Net, which captures visual cues by integrating facial expressions of interlocutors from multiple frames without encoding redundant scene-related information. They also design a multi-modal fusion model called MultiAtt, which integrates one modality with complementary information from the other modalities through stacked bi-directional multi-head cross attention layers. Additionally, they introduce a sample-weighted focal contrastive loss to address the difficulty of classifying minority and semantically similar emotion classes. The proposed framework achieves state-of-the-art performances on both datasets.</sample>
    <sample id="281">Il presentatore ha introdotto un nuovo metodo per misurare quanto un modello di traduzione automatica dipende dal contesto. Questo è stato fatto utilizzando una misura chiamata CXMI, che misura quanto informazione il contesto fornisce al modello. L'analisi è stata condotta su registrazioni di TED Talks tradotte in 14 lingue diverse. I ricercatori hanno identificato diverse sfaccielementi di discorso che richiedono contesto, come pronunzie doppie in arabo o la scelta di verbi appropriati. Hanno anche creato una benchmark per la traduzione a livello di documento, utilizzando tagger per identificare automaticamente parole che appartengono a ciascun fenomeno di discorso. I risultati hanno dimostrato che i modelli che usano contesto sono più accurati per certi fenomeni, ma non per altri.</sample>
    <sample id="282">Il presentatore introduce il proprio lavoro, che si concentra sulla trasferimento di stile in storia non parallelo con l'aiuto di rappresentazioni di corso e miglioramento del contenuto. L'approccio proposto include un modello generativo chiamato StyleTrans, che impara le rappresentazioni di corso dai testi di origine e le combina con embeddings di stile per generare testi in stili diversi. Il modello è stato addestrato su un nuovo dataset in cinese e inglese e ha dimostrato un forte controllo del stile e la capacità di preservare il contenuto originale.</sample>
    <sample id="283">La prima struttura di dipendenza simmetrica menzionata è la coordinazione con un nome di città.</sample>
    <sample id="284">The paper presents a novel fuzzy span mechanism for enhancing universal information extraction (UIE). It addresses the ambiguity in labeling spam boundaries and the mismatch between transformer feature extraction and information extraction. The proposed method uses a continuous distribution of correct probability to model the fuzzy spam boundary, which is then converted into discrete values for calculation. The model calculates boundary cross-entropy with the golden boundary as BCE loss and adds KL divergence between predicted boundary and fuzzy spam boundary as supplementary information. Fuzzy span attention is introduced as a mask function to trim attention distribution dynamically. Experiments on three information extraction tasks show significant performance improvement compared to UIE without fuzzy spam mechanism.</sample>
    <sample id="285">The video presents a research paper titled "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework" by Mingqi Gao and colleagues from Peking University. The paper focuses on the challenges of factually incorrect summaries in dialogue summarization and proposes a new evaluation framework to address these issues. The authors argue that current evaluation methods, such as FactCC and DAEC, are flawed and do not accurately assess the performance of factual error correction models (FEC). They propose a new taxonomy of factual errors and a three-step evaluation framework to improve the accuracy of FEC models. The results show that training FEC models with reference summaries from dialogue summarization datasets yields the best results, and combining human-annotated data with synthetic data is a promising direction.</sample>
    <sample id="286">Sarah Finch</sample>
    <sample id="287">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="288">I fenomeni sintattici possono essere testati utilizzando insiemi di dati come il dataset BLM, il dataset di grammaticalità e il dataset di accettabilità.</sample>
    <sample id="290">I metodi per la prima domanda di ricerca sono abbreviati come WSL, FTSW, FTW, FT, e FTW.</sample>
    <sample id="291">Il modello viene valutato su attività come riconoscimento di nomi, classificazione, part-of-speech tagging e risoluzione di domande.</sample>
    <sample id="294">CamemBERT viene inizialmente addestrato su un dataset di testo.</sample>
    <sample id="295">Il nome del relatore è Adam Sipruckowsky.</sample>
    <sample id="296">The video presents a work developed by the University of Turin and Amazon Alexa, focusing on irony detection in natural language processing. The researchers collected data from social media, Reddit, and Twitter, creating a corpus called EPIC (English Prospective Irony Corpus). They used crowdsourcing platform Prodigy to have annotators label the data, with 74 annotators in total providing five annotations each for 200 short conversations. The results showed that perspective-aware models were less uncertain and more confident in their predictions compared to gold-standard aggregated models. The researchers also found that generations close to each other and annotators from the United Kingdom and Ireland had higher variations in their responses.</sample>
    <sample id="297">In questo progetto, i ricercatori hanno sviluppato un tipologia e uno glossario di dog whistles, eseguito un studio di caso su discorsi politici storici degli Stati Uniti e valutato la riconoscibilità dei dog whistles in modelli di linguistica naturale. Hanno anche eseguito un studio di caso sulla rilevanza dei dog whistles per la moderazione online. I dog whistles sono più efficaci quando gli utenti non sono consapevoli di essi, ma i ricercatori, spesso in grado di riconoscerli, hanno sviluppato strumenti per identificarli.</sample>
    <sample id="298">I risultati dell'esperimento hanno dimostrato che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="299">The speaker introduces the topic of improving the robustness of NLI models with minimax training, a joint work with Andreas Vlachos at the University of Cambridge. They explain that while NLI models have achieved state-of-the-art results across various benchmarks, recent work has shown that their success is partly due to learning and using shortcuts, which are spurious correlations between input attributes and labels introduced during data creation. For example, high word overlap between the premise and hypothesis in the MNLI dataset is strongly correlated with the entailment label. This leads to NLI models performing well on in-distribution samples but poorly on out-of-distribution or serial test sets where such correlations do not hold. The speaker discusses existing shortcut mitigation methods, which often assume access to an auxiliary model designed to learn shortcuts for predictions, and highlights their limitations. They propose a training method to reduce reliance on NLI models' shortcuts and improve out-of-distribution performance by emphasizing underrepresented hard examples through a minimax training objective.</sample>
    <sample id="300">The presentation introduces a new task called interactive dictation, which allows users to dictate and edit documents using their voice in a natural and intuitive manner. The speaker explains that the task involves flexible interleaving of dictation and editing, using natural language utterances to specify edits, and executing each utterance in sequence until the final document state is achieved. The presentation also covers the design of a data collection interface, building a dataset for the task, and creating a baseline system for interactive dictation.</sample>
    <sample id="302">Permettere la permutazione dei token nella sequenza di output è necessario per la nostra abilità di generalizzazione. Questo ci consente di prevedere l'output dalla sequenza di input in due passi, come descritto nell'introduzione.</sample>
    <sample id="303">Gli autori hanno suggerito di aumentare la trasparenza sui metodi di mitigazione dei bias per capire se i modelli stanno producendo stereotipi positivi e narrativa essenzializzante per via di un al线过量的道德对齐或反刻板印象方法。</sample>
    <sample id="304">Gli input inaccettabili di coppia minima sono le frasi grammaticalmente sbagliate o non grammaticalizzate.</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL), which involves training neural networks on weakly labeled data. The speaker, Dawei Zhu, explains that while WSL can achieve high performance on clean test sets, it often requires clean validation samples, which are not always available. The research questions addressed in the work are: 1) Is clean validation data necessary for WSL? 2) How many clean samples are needed? 3) Should only clean samples be used for validation? The findings indicate that WSL approaches require clean validation samples to work properly, and increasing the number of clean validation samples can improve performance. Additionally, fine-tuning on clean validation samples can achieve similar performance to more complex WSL methods. The speaker recommends reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, considering continuous fine-tuning, and making their code open-source.</sample>
    <sample id="306">Hello everyone. I am Sebastian Schuster and together with Nohj Kim, I'm going to give you a short overview of our work on entity tracking in language models. For an agent to understand the discourse, it needs to track which entities are mentioned and how their state changes as the discourse unfolds. So for example, in the context of a recipe such as here, an agent has to understand that put the eggs, sugar and flour in a bowl results in all of these three entities ending up in a bowl. And if the discourse continues with mix to form a light batter, then the agent has to understand that now all of these entities are part of the batter. And so we argue that this is a crucial ability for understanding long discourses but there haven't really been any systematic investigations into what pre-trained language models can actually perform such tasks. And so the overarching research question we're trying to answer in this paper is to what extent large language models can track entities. Now given that we don't know the exact contents of the pre-training data of many language models and considering several properties about how discourses work, there are actually several challenges with designing a task to evaluate entity state tracking abilities. First, some entity states will be common in the pre-training data and therefore the model may predict the correct state without actually having any entity tracking abilities. For example, eggs often end up in bowls or babies often end up in cribs. So we want to make sure that the distributional patterns in the pre-training data cannot give away the entity states in the evaluation data. Then second, sometimes entity states can be predicted from individual words or phrases without actually considering the larger discourse. And the model may seem to be able to perform entity tracking while in fact it just learned simple heuristics associations between words and entity states. For example, that the word empty is always associated with an entity being empty. And third, if one uses fine-tuning or in-context demonstrations which is often necessary to probe the model then the model may memorize entity state sequences or it may learn to apply heuristics such as slot filling if such heuristics are not blocked in the evaluation task design. And so in designing our evaluation task we took great care to make sure that the model cannot make use of any of these shortcuts when we evaluate its entity tracking abilities. And Nohj Kim will tell you a bit more about how we set up this task. Hello my name is Nohj Kim and I'll be talking about the task design and the experimental results. So to evaluate entity tracking abilities we designed the following task involving boxes and objects. And in our setup the input to the model starts with a description of the initial contents of each box as sketched on this slide. And the task of the language model is to complete the input by predicting the contents of each box. Now given just this initial description the task is pretty trivial, the model can just copy the relevant information from the description. But in our task we also include multiple state changing operations like moving objects or adding objects to a box. So for these the model would have to combine the initial description with the operations to make the correct prediction. For example, Box 1 now contains the car and the watch after moving the watch from Box 3 to 1. And additionally we implemented various measures to prevent the model from using heuristics as Sebastian discussed earlier on, so please check out our paper for how we did this. We tested the setup with Fawn T5 and GPT-3 and 3.5 models using two shot in context learning and what we're showing here is the accuracy of predicting the correct box content as a function of the number of operations acting on a certain box. And on the left panel we have the data points where the proven entity state is different from the state in the initial description whereas on the right panel we have cases where the state is the same as in the initial description. So for these ones the model can simply copy. And our experiments show that most models simply repeat the initial state as you can see from the generally high accuracy on the right panel. And we can also see that only text DaVinci 03 accepts non-trivial tracking which is the pink line here in the left panel. And all other models perform below a strong random baseline obtained by random simulation which is the blue line. So what gives rise to this difference between models? Since the models we tested varied along several different dimensions we investigated what other factors might be in play by zooming into the GPT series. And we found that all GPT-3.5 models which all have been trained on substantial amounts of code exhibit non-trivial entity tracking behavior whereas all models that do not have code as a substantial part of their pre-training do not. And this suggests that pre-training on code is what's responsible for making this capacity surface in pre-trained language models. We also found that smaller models like T5 base can learn to perform entity tracking if you directly fine-tune the models but on the other hand randomly initialized model of the same architecture cannot learn our state tracking task even when they receive direct supervision suggesting that pre-training is again important here. However as we discuss in more detail in the paper it remains unclear whether the state tracking abilities we observe generalize beyond our setup in this case. Thanks for listening and we have a lot more results and analyses including GPT-4 experiments in our paper so please check out arXiv. And if you have any questions or comments about our work either find us in person at ACL or you can reach out to us over email or on Twitter. Thank you.</sample>
    <sample id="307">Gli autori hanno utilizzato metriche di valutazione come riconoscimento di entità, classificazione, tagging del discorso e risoluzione di domande.</sample>
    <sample id="308">The presentation discusses the concept of NLPositionality, which characterizes design biases of datasets and models in natural language processing (NLP). It highlights the importance of evaluating multiple dimensions of chat quality to understand the strengths and weaknesses of models on a finer-grained level. The presentation also addresses the challenge of studying model and dataset positionality due to the lack of documentation and hidden models behind APIs.

To study NLPositionality, the framework compares annotations with real users against existing datasets and models. This is done through a two-step process: re-annotating datasets with diverse annotators and comparing these annotations to models and datasets using Pearson's correlation score. The framework differs from annotator disagreement literature by comparing end-users with models and datasets' predictions and labels rather than just annotator agreement or modeling annotator distributions.

The framework is enabled through Lab in the Wild, an online crowdsourcing platform that allows for the recruitment of diverse volunteers. The study hosts tasks such as social acceptability and toxicity/hate speech detection, collecting over 16,000 annotations from over 1,000 annotators from 87 countries. The results show that there is positionality in NLP, with datasets and models aligning more with English-speaking countries and people with higher educational backgrounds. However, some populations are left behind, such as non-binary individuals compared to men and women counterparts.

The presentation concludes with recommendations for addressing positionality in NLP, including keeping records of relevant design choices, conducting NLP research with the lens of perspectivism, and building specialized datasets and models within specific communities. The initiative emphasizes that inclusive NLP is not just about making all technologies work for everyone but also about creating more equitable and representative systems.</sample>
    <sample id="309">La metrica utilizzata per misurare l'accordo tra annotatori è l'interannotatore accordo.</sample>
    <sample id="310">Wikipedia</sample>
    <sample id="311">I fornitori di informazioni dell'articolo sono l'Universität Heidelberg e l'Universität Stuttgart.</sample>
    <sample id="312">MultiInstruct differisce dagli altri parametri di riferimento in quanto è il primo dataset di addestramento per la tuning multi-modal con 62 diverse tabelle di multi-modal task coprendo 10 broad categories.</sample>
    <sample id="313">Cinque.</sample>
    <sample id="314">La coordinazione binaria è un tipo di coordinazione in cui due o più elementi sono uniti da un segnaposto o un segnaposto.</sample>
    <sample id="315">I prompt in questo studio sono stati utilizzati per un periodo di 3 giorni.</sample>
    <sample id="316">I risultati suggeriscono che il modello T5 più piccolo può generare scripts di alta qualità e supportare modelli più grandi quando addestrato su un dataset appropriato.</sample>
    <sample id="317">The paper presents a method called CodeIE for few-shot information extraction using large language models. The authors propose transforming the task of information extraction into a code generation task, which allows them to use code pre-training language models like CodeT5 to generate structured outputs. They evaluate their method on two datasets and compare it with traditional baseline models such as UIE and GPT-3. The results show that CodeIE significantly outperforms the baseline models in terms of accuracy and recall. The authors also analyze the performance of different prompts and find that using code format prompts leads to better performance than test format prompts. Overall, the paper provides evidence that code pre-training language models can be used effectively for few-shot information extraction tasks.</sample>
    <sample id="318">Ciao a tutti, oggi presenterò il mio lavoro di ricerca intitolato "Unsupervised Guided Label Denoising for Document-Level Distant Relation Extraction". Sono Sunghyung Kim, dell'University of Science and Technology. La estrazione di relazioni a livello di documento mira a estrarre relazioni tra entità in un documento. Questo può essere visto come questa immagine. Metodi precedenti si basano su grandi quantità di corpus annotati da umani, che è tempo consuming e laborioso. Così recenti work hanno utilizzato i dati supervisionati a lunga distanza per pretrainare i modelli di estrazione di relazioni a livello di documento per un'efficienza migliore. Come sappiamo, i dati supervisionati a lunga distanza contengono vari etichettini rumorosi. Lavori attuali cercano di alleviare il problema della rumorosità utilizzando pseudo etichette. Tuttavia, queste tecniche continuano a portare al rischio di estrarre relazioni false o di perdere relazioni corrette. Quindi, come ridurre la rumorosità causata dai pseudo etichettini è ancora un problema. In questo articolo, proponiamo un modello di estrazione di relazioni a livello di documento con denoising guidato da incertezza per migliorare la qualità dei relativi etichettini. Questo è un overview del nostro modello. Innanzitutto, si addestra un modello predenoizzante con sia i dati supervisionati a lunga distanza che i dati umanamente annotati per generare pseudo etichette. Poiché i pseudo etichettini sono inevitabili, introduciamo un'incertezza stimata per determinare se la previsione del modello è attendibile o meno. Considerando che potrebbero esserci più relazioni tra un paio di entità, proponeamo un'incertezza stimata a livello istanza per capturare i punteggi di incertezza per relazioni sovrapposte. Inoltre, progettiamo una strategia di re- etichettin con un threshold di incertezza dinamico e una strategia di training multi-fase per ulteriormente migliorare le prestazioni. L'incertezza stimata è un importante tecnologia per la deteczione di classificazione sbagliata, deteczione di outlier di distribuzioni istanze e impegno attivo. Per modellare l'incertezza in predenoizzanti modelli, introduciamo la tecnologia di dropout monte carlo nel compito di documenti. Questo metodo richiede molte stime statistiche per previsioni forward pass con attivazione dropout per capturare l'incertezza del modello. Metodi precedenti basati su MC dropout calcolano i punteggi di incertezza delle previsioni del modello come questa formula. Tuttavia, il precedente metodo non è sufficiente per problemi di relazioni sovrapposte, come illustrato nella figura a sinistra. Ci sono due tipi diversi di relazioni tra queste entità. È difficile separare i composti di pseudo etichette positivi forse i relativi pseudo etichette diretti positivi corretti. Per risolvere questo problema, modificammo il processo di stimata per ottenere un invervallo. In sintesi, il retiro da zero sembra ottenere prestazioni migliori su quasi tutti i task. Tuttavia, i nostri esperimenti con il pre-addestramento hanno dimostrato che i risultati ottenuti utilizzando i pesos e i tokenizer di bert trainati su un subset di 4 GB di natsos sono simili a quelli ottenuti con bert 4 GB da zero. Questo non è il caso per il modello basato sui pesos e i tokenizer di camembert, che soffrono di problemi di stabilità. Infine, come conclusione, il nostro sistema proposto offre prestazioni migliori su 9 dei 11 task di documenti e supera globalmente i risultati del modello generico qui, camembert. Abbiamo anche osservato che i relativi data specializzati sono migliori, ma non scalano bene. I modelli pre-addestrati ottenuti da natsos sono freely available su yugabyte e tutti i script di training su github repository. Grazie per la presentazione e ci aspettiamo di discutere al post-congresso a Toronto.</sample>
    <sample id="319">Nel lavoro vengono esaminate diverse strategie di apprendimento, tra cui il pre-addestramento da zero, l'addestramento con un subset di 4 GB di Natsos e l'addestramento con un subset di 4 GB di Natsos e 4 GB di nodi clinici.</sample>
    <sample id="320">Il fattore di overfitting dovuto al riutilizzo del test è ridotto.</sample>
    <sample id="321">La qualità della semplificazione è stata valutata utilizzando un dataset di documenti manualmente allineati, che hanno servito come standard allineati per valutare i metodi proposti.</sample>
    <sample id="322">Rendi in sintesi il contenuto inglese (circa 200 parole).</sample>
    <sample id="323">The audio is a presentation by Yu Jia Wang from Shanxi University, China, discussing a paper titled "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering." The presentation explains the challenges of commonsense question answering (CSQA) and introduces a method to improve it. The method involves building a heterogeneous knowledge graph (HKG) based on a knowledge base, optimizing the structure and knowledge representation using a two-stage training strategy and KRL. The HKG is then used to encode and fuse QA contexts and entities, removing irrelevant entities and updating embeddings. The final answer prediction is made by inputting the HKG graph embedding, paths in the QA context, and the QA context embedding into an MLP. Experiments on CSQA and OpenBookQA datasets show that the proposed method achieves good results compared to other methods like LM and HKG.</sample>
    <sample id="324">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="325">E testiamo anche modelli monolingui.</sample>
    <sample id="326">La dissonanza cognitiva è quando due credenze o azioni sono in contrasto.</sample>
    <sample id="327">The presentation introduces a new multimodal architectural model called Manager Tower, which aims to improve vision-language representation learning by aggregating insights from pre-trained unimodal experts at different levels. Manager Tower uses managers in each cross-modal layer to adaptively combine the insights of pre-trained unimodal experts, allowing for more comprehensive cross-modal alignment and fusion. The model is trained with only four million images and achieves superior performance on visual question answering tasks.</sample>
    <sample id="328">The most liberal model is the one that does not sanitize political opinions in language model training data.</sample>
    <sample id="329">The presentation introduces a method for generating structured pseudo-labels to improve zero-shot video sentence localization. The method leverages a pre-trained image caption model to generate complex free-form pseudo-queries from video frames, and uses a pretrained model to match the relevance between video frames and pseudo-queries to generate pseudo-events. These pseudo-events are then ranked based on their quality to ensure high relevance with the query and low relevance with other events. The method also reduces the weight of noisy samples and refines labels to mitigate label noise. Experimental results show that this approach outperforms existing methods on two datasets.</sample>
    <sample id="330">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="331">La relatrice o il relatore è Sara Papi.</sample>
    <sample id="332">I dati per il parametro di riferimento MuDa sono stati tratti da un corpus parallelo che è stato utilizzato per valutare i modelli di traduzione automatica.</sample>
    <sample id="333">The video presents a research work on weakly supervised learning (WSL), specifically focusing on injecting knowledge from nearest neighbor machine translation into WSL. The speaker, Dawei, a PhD student at Salzburg University in Germany, explains the challenges of WSL, such as noisy weak labels and the need for clean validation data. He introduces three research questions: whether clean validation data is necessary, how many clean samples are needed, and whether to use clean samples solely for validation. The speaker's work addresses these questions by proposing a normal training framework that refines the representation space of the model according to key knowledge. Experimental results show that the proposed framework achieves an average gain of 1.99 COMET score and 1.0 BLEU score compared to the state-of-the-art CM system, with better translation performance, less memory usage, and faster inference speed.</sample>
    <sample id="335">Il nome del relatore è Matthias Lindemann.</sample>
    <sample id="336">Il trasferimento interlinguistico è un processo in cui si utilizza un modello monolingue per tradurre o prevedere il testo in un'altra lingua.</sample>
    <sample id="337">The presentation introduces a neural approach for out-of-vocabulary (OOV) word embedding learning, leveraging word formation and association to infer the meaning of OOV words. It proposes a word relationship graph that represents lexical rules of word formation and association, using a two-level graph structure to capture word pieces and their relationships. The model employs a graph neural network with a self-attention mechanism to address node attribute assignment issues and mitigate noise from nodes with numerous neighbors. A readout block is used to summarize graph information and capture word formation. The model incorporates contrastive learning in the loss function to encourage similarity between positive samples and push negative samples further apart. Experimental results demonstrate the effectiveness of the model on both intrinsic and extrinsic downstream tasks. The model can also bring improvements to both static and contextual models. The applicability of the model to other languages depends on the rationality of word decomposition.</sample>
    <sample id="338">The speaker introduces their research on evaluating human natural language explanations and proposes a new metric called "true" to assess the quality of explanations. They analyze five datasets using a unified structure and compare the performance of two models, T5 and BART, with traditional metrics like BLEU and ROUGE. The results show that human-annotated explanations can still benefit model predictions, even if they are considered low-quality by humans. The proposed metric outperforms the traditional metric in evaluating dataset qualities and task-specific characteristics.</sample>
    <sample id="339">Sarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="340">Hello everyone, I'm Kuan-Hao Huang from UCLA. I'm presenting our work, ParaAMR, a large-scale syntactically diverse paraphrase dataset by AMR back-translation. This is a joint work with Varun Iyer, Anup Kumar, Kai-Wei Chang, and Aram Aramyan. Paraphrase generation is a long-standing and important task in NLP domain. It benefits many other NLP applications such as question answering, chatbots, and improving robustness. To train a good paraphrase generator, we usually need a large scale of high-quality paraphrase data. However, for the existing human</sample>
    <sample id="341">Gli autori usano solo un modello per ogni regime di latenza e gestiscono la latenza tramite parametri specifici.</sample>
    <sample id="342">Political news media are well covered in their pre-training data. According to a survey of the C4 corpus, we can see that New York Times, Los Angeles Times, The Guardian, Huffington Post, etc. are well covered in language model training data. For example, we train an English model on English query and during inference, we translate the German query using API to English and then use the trained model to predict the SQL. We also test monolingual model. In this setting, the source language is the same as target language, for example German to German or English to English. We also test monolingual multi-language setting by training multilingual models with only 10% of training data. And we test multi-lingual multi-lingual model which we train one multi-lingual model for all languages. For example, we put the German, English, Chinese queries together to train a multi-lingual model and during inference, we can use this model to to translate German queries or Chinese query, etc. And we also consider cross</sample>
    <sample id="343">Salve a tutti, mi chiamo Makkattah e oggi io e il mio coautore Martin stiamo presentando il nostro lavoro "KitMOS: valutazione dell'integrazione di conoscenza da diverse fonti". Questo lavoro è un合作 tra McGill University, Mila e Microsoft Research. I modelli di intelligenza naturale (NL) utilizzano una varietà di fonti di conoscenza, come quella contenuta nei parametri acquisiti in precedenza o data all'atto dell'esecuzione. Lavori recenti hanno dimostrato che i modelli possono utilizzare la conoscenza acquisita in precedenza per risolvere compiti come la risoluzione di domande. Tuttavia, l'intelligenza naturale spesso richiede conoscenza anche fornita all'atto dell'esecuzione. Ad esempio, nella frase "John ha visto il nuovo presidente su TV", i parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa è la TV, ma non possono rilevare chi sia questa istanza specifica di entità, John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo l'addestramento. Di conseguenza, i modelli più efficaci per compiti intensivi di conoscenza devono essere in grado di integrare e utilizzare sia la conoscenza acquisita in precedenza che quella fornita all'atto dell'esecuzione. In questo lavoro, proponiamo una suite di test diagnostiche per l'integrazione di conoscenza. Introduciamo un compito di risoluzione di correlazione progettato per testare la capacità di ricercare conoscenza disponibile in diverse fonti. Abbiamo valutato il dataset con partecipanti umani e stabilito modelli di risoluzione di correlazione. Ecco un esempio del nostro dataset: "Servin è un giudice, KIA è un pasticcere. Servin e KIA si sono incontrati in un parco. Dopo un lungo giorno di lavoro decidendo casi in una corte di giudici, lui era felice di rilassarsi." La nostra任务 è identificare l'entità corretta che il pronome "lui" si riferisce, che in questo caso è Servin. La risoluzione di un pronome richiede due tipi di informazione: informazione specifica dell'entità, ad esempio "Servin è un giudice", e informazione di sfondo, ad esempio "i giudici decidono casi in una corte di giudici". Generalmente, l'informazione di sfondo viene imparata durante l'addestramento di grandi modelli linguistici, mentre l'informazione specifica dell'entità è tipicamente osservata all'atto dell'esecuzione. Abbiamo variato l'accessibilità di queste due tipologie di informazione, in modo che possano essere fornite in una singola fonte o in molte fonti. Abbiamo definito tre ambienti di KitMOS: 1) l'ambiente di addestramento con sfondo pre-addestrato, in cui l'informazione di sfondo è supposto di essere disponibile all'atto dell'addestramento; 2) l'ambiente di addestramento con sfondo sia pre-addestrato che inferenziale, in cui l'informazione di sfondo è disponibile sia all'atto dell'addestramento che all'atto dell'esecuzione; 3) l'ambiente di inferenziale, in cui entrambe le tipologie di informazione sono disponibili solo all'atto dell'esecuzione. Quest'ultimo ambiente è particolarmente interessante, poiché simula il caso in cui l'informazione di sfondo necessaria per risolvere un compito non è parte dei dati di addestramento dei modelli. Ad esempio, nuove occupazioni potrebbero essere sviluppate dopo l'epoca dell'addestramento. Ecco un esempio di come controlliamo l'accessibilità dei fatti in diverse fonti: in ambienti di addestramento con sfondo pre-addestrato, supponiamo che l'informazione di sfondo "i politici cercano posti elettori nel governo" sia contenuta nei parametri pre-addestrati. In un contesto di inferenza, forniamo la conoscenza specifica "Chester è un politico". In ambienti di addestramento con sfondo sia pre-addestrato che inferenziale, forniamo non solo la conoscenza specifica, ma anche l'informazione di sfondo sulla politica in un contesto inferenziale. In ambienti di inferenziale, forniamo la conoscenza occupazionale "miretura" invece di "politico", poiché miretura è improbabile di essere contenuta nei parametri pre-addestrati. Abbiamo valutato il dataset sia con partecipanti umani che stabilendo modelli di risoluzione di correlazione. In questa figura, mostriamo i risultati dei migliori modelli su una delle varianti più difficili dell'ambiente di addestramento con sfondo pre-addestrato. Senza addestramento specifico su KitMOS, entrambi i modelli non performono bene. Tuttavia, quando addestrati su KitMOS, entrambi i CTF e BERT percorrono notevolmente meglio rispetto a una scelta casuale. Questo suggerisce che i modelli addestrati su generali dataset di risoluzione di correlazione imparano a sfruttare trucchi superficiali che non sono utili quando si testa su KitMOS, poiché tali trucchi sono stati rimossi. Esperienze con conoscenza fictionale hanno dimostrato che anche i migliori modelli non riescono a integrare la conoscenza di sfondo fornita solo all'atto dell'esecuzione. Riassumendo i principali punti di rilievo del nostro articolo, molti modelli di risoluzione di correlazione sembrano incapaci di ragionare sulla conoscenza proveniente da diverse fonti senza addestramento specifico. Tuttavia, con addestramento specifico, alcuni modelli riescono a integrare la conoscenza proveniente da molte fonti. Anche i migliori modelli sembrano avere difficoltà a integrare la conoscenza di sfondo fornita solo all'atto dell'esecuzione. Se siete interessati a maggior dettagli, vi prego di vedere il nostro articolo e controllare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="344">I metodi basati su alberi possono essere complicati e spesso computazionalmente costosi.</sample>
    <sample id="345">The video presents a research paper titled "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations" by Matthias Lindemann, Alexander Koller, and Ivan Titov. The paper discusses the development of a treeless model for language models that can be fine-tuned on high speech or misinformation and deployed to popular social media platforms. The experimental results show that the model outperforms other treeless models on generalization to deeper recursion but remains challenging for some other kinds of structural generalization. The paper solves technical challenges such as alignment between input and output, handling multiple permutations, and finding the highest scoring permutation. The authors address these challenges through inducing alignment as part of training, using a flexible permutation method, and approximating the traveling salesman problem with a GP-friendly continuous relaxation. The paper also provides insights into the experiments and how they address these challenges.</sample>
    <sample id="346">I fornitori dell'articolo sono il School of Interactive Computing e il Georgia Institute of Technology.</sample>
    <sample id="347">Grazie mille per la tua attenzione. Sono felice di aver finito tutto che dovevo fare per oggi. Grazie per il tuo tempo.

La nostra presentazione si concentra sul rilevamento di stereotipi negativi e su come utilizzare le intersezioni per studiare bias e danni. Ci sono molte cose che potrebbero essere trascurate se non lo facciamo. Inoltre, dovremmo aumentare la transparente riguardo ai metodi di mitigazione dei bias. Ad esempio, non sappiamo se i nostri pregiustizi positivi sono dovuti a un'adeguata alleanza di valori o a metodi anti-stereotipizzanti che stanno portando a queste tendenze preconcepite. Non possiamo fare supposizioni senza maggior transparente. Grazie mille per la tua attenzione. Buona fortuna alla tua conferenza.</sample>
    <sample id="348">The paper "Marked Personas" by Myra Cheng, Esin Durhamus, and Dan Jurafsky presents a method to measure stereotypes in language models (LLMs) using natural language prompts. The authors address the limitations of existing methods, such as reliance on hand-curated datasets and failure to capture intersectionality. They propose generating personas based on prompts that elicit stereotypes from both LLMs and human subjects. The Marked Words method identifies words that distinguish marked groups from unmarked ones, revealing harmful patterns not captured by traditional lexicon-based approaches. The results show that generated personas contain more stereotypes than human-written ones, with common tropes reflecting harmful narratives. The paper concludes with recommendations for model owners to address positive stereotypes, use intersectional lens, and increase transparency about bias mitigation methods.</sample>
    <sample id="349">La maggior parte delle traduzioni dipende dal contesto. Ad esempio, come tradurre "mole" in questa frase? Abbiamo sviluppato un modello di dialogo e vorremmo vedere quanto bene si confronta con lo stato dell'arte corrente. La pratica comune è utilizzare valutazioni umane, ad esempio chiedendo a giudici umani di scegliere tra due conversazioni o di valutare le conversazioni su una scala Likert. Questi approcci funzionano bene per fornire valutazioni holistiche della qualità del dialogo in tutto. Tuttavia, la qualità del dialogo ha molti aspetti, quindi potresti voler valutare più dimensioni della qualità del chat per comprendere i punti forti e deboli del modello in un livello più fine-granulare. Un approccio è semplicemente chiedere ai giudici umani di valutare diverse dimensioni della qualità del dialogo, ad esempio la rilevanza delle risposte del modello utilizzando metodi esistenti o la scala Likert. Tuttavia, crediamo che ci sia un策略更加精确和可靠的策略来评估对话质量的维度。我们的方法旨在通过明确标注每个模型响应是否表达某些行为，例如提供不相关的信息或自相矛盾，来减少人类评估的主观性。我们称这种方法为标注聊天行为或ABC评估。我们开发了这种方法来全面覆盖最近文献中建议会影响聊天质量的行为。ABC评估能够衡量聊天模型犯各种主题错误的速度，例如：ABC评估衡量模型在多少轮中忽略了它的伙伴或说了一些不相关、自相矛盾或伙伴自相矛盾的东西；模糊了不正确的事实或违反了共同知识；以及当模型成功或失败于展示同理心时。为了确定哪种评估方法最有效，我们选择了四个领先的人工聊天模型，并使用ABC评估对它们进行了评估。作为比较，我们还评估了这些对话，使用三个现有方法：针对每个轮次的Likert评分、针对整个对话的Likert评分和针对对话的配对比较。对于每种现有方法，我们收集了针对八个最常衡量的聊天质量方面的评估，因为这是评估聊天模型沿多个维度的标准做法。从我们的分析中可以看出，ABC评估行为标签总体上比现有方法收集的标签更可靠，如在100个双重标注对话中由注释员一致性衡量。此外，ABC评估标签更预测整体对话质量，如线性回归分析所示。例如，你可以看到测量具有自我和伙伴自相矛盾的轮次比例解释了5%和10%的对话质量，而平均Likert一致性分数只解释了4%或更少。最后，我们检查了每个评估指标是否捕捉了聊天质量的独特方面，使用逐步线性回归。你可以看到所有ABC评估指标的组合解释了超过25%的对话质量，当你一次删除一个指标时，大多数指标都会损失相当一部分关于质量的信息。相比之下，所有轮次Likert指标的组合解释了较少的质量，而且较少的这些指标提供了独特信息。这些可靠、信息丰富且独特的ABC评估指标使我们能够以比以往任何方法都能实现的更高分辨率评估对话型AI。你可以看到，在我们实验的结果中，有几个挑战仍然存在，并且已经量化得非常精确。例如，我们测试的聊天机器人有Common Sense Violations Text Dataset来计算词频。我们在四个数据集上的结果表明，我们的嵌入标记可以保持很好的检测性能，同时保持很好的实用性。我们还通过可视化聊天机器人的嵌入，验证了提供的嵌入的可转换性，即可视化了句子的嵌入，如图中的图例所示。图例表示每个句子中的触发器数量。正如图所示，很难区分后门嵌入和正常嵌入。这就是全部，谢谢你们，我们将讨论这个问题。</sample>
    <sample id="350">Il presentatore, Simone Tedeschi, ha introdotto un articolo sulla valutazione dei sistemi di apprendimento automatico (ALP) basata su leaderboard. Questa valutazione è diventata standard in ALP e gli obiettivi principali sono stati raggiungere i punteggi più alti in benchmark popolari. Tuttavia, i sistemi superano spesso livelli umani o anche superumani in questi benchmark, portando a conclusioni che alcune tasse sono ora risolte da queste modele. Tuttavia, non è chiaro cosa significa superare umani in tasse coinvolgendo conoscenza, ragione e inferenza. I modelli sono anche britteli in molte altre aree, come la generalizzabilità, l'immunità ad attacchi avversari, la dipendenza da spurious pattern, la mancanza di sensibilità a perturbazioni basilari come negazione e la sensibilità eccessiva a perturbazioni non importanti. L'articolo esamina la affidabilità dei leaderboard per confrontare modelli e umani utilizzando due benchmark popolari: SuperGLUE e Squad. I risultati hanno rivelato che i sistemi superano spesso i punteggi umani in molti task, ma ci sono anche errori nella valutazione che rendono le comparazioni non equilibrate.</sample>
    <sample id="351">The paper investigates the problem of generalization in named entity recognition (NER) using the CoNLL-2003 dataset. The authors developed a new dataset, CoNLL+, by collecting and annotating data from Reuters News from 2020. They fine-tuned over 20 models on CoNLL-2003 and evaluated them on both CoNLL-2003 and CoNLL+. The results showed that larger models, transformer models, and more fine-tuning examples lead to better generalization. The performance drop was caused by temporal drift, not adaptive overfitting. The conclusion is that CoNLL-2003 taggers still work well in 2023, but further research is needed to improve generalization.</sample>
    <sample id="352">ABC-Eval è un nuovo approccio dimensionale per valutare i sistemi di dialogo orientati al chat.</sample>
    <sample id="353">The paper investigates the problem of generalization in named entity recognition (NER) tasks using the CoNLL 2003 dataset. The authors found that models developed using CoNLL 2003 have poor generalization to modern data, and identified three main ingredients needed for good generalization: model architecture, model size, and fine-tuning examples. They also found that temporal drift is the main cause of performance drop, not adaptive overfitting. The paper concludes that models can still work well with CoNLL 2003, but further research is needed to improve generalization.</sample>
    <sample id="354">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2021.</sample>
    <sample id="355">La presentazione è intitolata "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge" e è stata accettata per AACL 2023 come un articolo lungo. La presentazione è condotta da Vasudha Varadarajan, candidata in computer science a Stony Brook University. La presentazione si concentra sulla deteczione della dissonanza cognitiva e su come è importante studiare tale fenomeno in linguistica. La dissonanza cognitiva è definita come una incompatibilità tra due credenze o azioni, ad esempio, quando una persona dice di sapere che i sigarette possono uccidere e quindi compra un paio di sigarette dopo una riunione. Questo fenomeno è comune nella vita quotidiana e spesso esprime relazioni di discorso. Studiare la dissonanza può aiutare a comprendere i cambiamenti di credenze, valori e atteggiamenti nella popolazione, a capire meglio le persone con disturbi ansiosi e a comprendere meglio le stili cognitivi personali. Per creare un risorsa sulla dissonanza cognitiva, la presentazione ha condotto una grande annotazione di relazioni di dissonanza. Utilizzando un approccio di dissonanza, i tweet sono stati analizzati e le coppie di unità di discorso sono state annotate in base alle guidelines descritte in un articolo. Come è evidente dalla presentazione, la dissonanza è stata rilevata in solo il 3,5% dei pares analizzati. Dopo la raccolta di circa mille esempi di coppie di unità di discorso, un classificatore è stato addestrato solo su 43 esempi di dissonanza. Nonostante l'addestramento, il classificatore non ha performato molto meglio del caso di controllo. Per alleviare il problema della rareità, la presentazione ha sperimentato su combinazioni di imparazione passiva e imparazione attiva per raccogliere più esempi di dissonanza in modo più efficiente.</sample>
    <sample id="356">I ricercatori che hanno scritto l'articolo sono affiliati all'Informatics, NLP, Saarland e all'University of Amsterdam.</sample>
    <sample id="357">Il nome della relatrice o del relatore non è specificato.</sample>
    <sample id="358">Cinque autori sono coinvolti nell'articolo.</sample>
    <sample id="359">L'approccio è confrontato con le architettura specificamente dedicate al traduzione simultanea del linguaggio.</sample>
    <sample id="361">To train a text classification model, we require parallel pairs of text, for example doc of documents or sentences. Change in the output and we use these triplets to add an auxiliary metric learning loss to the training procedure and that auxiliary metric learning loss has a dynamic margin that is actually measuring the extent of change or intervention in the questions between each pair and using that to adjust the metric learning loss accordingly. We show that adding this auxiliary loss to three state-of-the-art baselines consistently improves their performance, especially when the number of reasoning steps grows beyond two. This is performance on in-distribution samples meaning the model is trained on a dataset and tested on the same dataset but more importantly we also show that adding an auxiliary metric learning loss improves performance on out-of-distribution samples either in cases where the model is trained on one dataset and tested on other datasets or the model is trained on one dataset and tested on examples from the same dataset that were never seen during training which is basically exactly what compositional generalization aims to obtain. We also show qualitatively that adding the counter comp loss helps the model attend to more meaningful tokens during training, meaningful in the sense that they relate to more meaningful operational terms in the output. Here's the base main references I used in these in this presentation. For more information make sure to check out our poster or if you have any questions feel free to reach out to the contact listed here. I'd like to thank my co-authors my advisor at CMU and my co-advisor at JP Morgan and I would like to thank you all.</sample>
  </task>
</testset>