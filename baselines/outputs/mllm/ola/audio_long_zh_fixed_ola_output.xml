<testset name="MCIF Baselines" type="output">
  <task track="long" text_lang="zh">
    <sample id="0">语言模型的主要数据来源是大规模的网络 craw 数据，其中政治新闻媒体被广泛覆盖。</sample>
    <sample id="1">根据所给的英文内容，这篇论文的作者所属机构是麦克吉尔大学、Mila和微软研究。</sample>
    <sample id="2">The speaker introduces a new pre-training model called LeTMask, which aims to address the reading order issues in document understanding. Unlike existing models that use global word segmentation for one-dimensional positioning, LeTMask uses local one-dimensional positioning based on the input text and layout information. This allows the model to infer global reading orders by jointly using one-dimensional positioning, two-dimensional positioning, and thematic information. The model also employs two novel masking strategies: whole word masking and layout-aware masking, which are designed to promote text layout interactions during pre-training. The paper compares the performance of LeTMask with different one-dimensional positioning methods and demonstrates its effectiveness in enhancing text layout interactions and improving document understanding.</sample>
    <sample id="3">你好，欢迎来到我们关于DePlain的演示。DePlain是一个用于德语文本简化的新语料库，适用于文档级别和句子级别的文本简化。我的名字是Ragina Strohn，我将指导您完成演示的第一部分。首先，让我们定义文本简化。文本简化是一种过程，通过调整文本以提高特定目标群体对文本的理解能力，例如阅读困难的人或非母语使用者。为了训练一个文本简化模型，我们需要平行文本对，例如文档或句子的平行对。在示例中，您可以看到一个复杂德文句子与它的简化德文翻译的平行句子对。简化句子的不同技术包括词汇替换、从句省略、从句省略和重排，甚至插入短语。现在，我们提出了一个新的语料库DePlain。因为在最近几年，一些现有语料库存在一些问题。例如，这些语料库太小，无法训练文本简化模型。另外三个最近提出的模型都是自动对齐的，这意味着它们可能包含错误的对齐。因此，我们提出了新的DePlain语料库，它被分为两个子语料库：DePlain API和DePlain Web。DePlain API基于新闻文本。在DePlain API中，我们手动对483篇文档进行了对齐，结果产生了大约30,000个13,000个平行句子对。对于DePlain Web，这个语料库包括不同的领域，并且我们手动对750篇文档进行了对齐，另一方面使用了自动对齐方法。总共，我们得到了30,450个句子对。我们进一步分析了这些句子对，例如，文本简化类型。如图所示，圣经文本比新闻文本或语言学习文本更简化。在所有级别上，包括词汇简化、结构简化和整体简化。此外，我们可以看到DePlain语料库具有不同简化转换的高多样性。例如，在DePlain API语料库中，我们有更多重排和词替换，而在DePlain Web语料库中，我们有更多重写。现在让我们看看我们可以用这个语料库做什么。Hello，我是Omar，现在我将讨论我们的数据集DePlain的用途。对于第一个用途，我们可以评估自动对齐方法。近年来，已经提出了许多对齐方法，但在机器翻译的背景下，我们有两个平行文档，分别用不同语言编写，我们想要提取两个文档中句子的对齐。但在我们的用途中，我们试图提取具有相同语言和相同内容但处于不同复杂度级别的两个平行文档之间的句子对齐。现在，由于我们拥有DePlain语料库，其中包含手动对齐的句子，我们可以使用这些句子作为金标准对齐来评估一些已提出的对齐方法。我们在论文中对这些方法进行了适应，并在论文中发布了这些适应和代码，以便在您的文档上运行实验。最后，我们得出结论，对于德语文本简化，最佳的自动对齐方法是MassAlign。您可以在论文中找到运行此方法的代码。第二个用途我们在论文中展示的是自动文本简化，即通过微调语言模型来产生简化文本，从而从复杂输入文本中产生简化文本。我们已经微调了Long Impart模型来产生文档级别的简化，我们还微调了Normal Base Long Impart模型来产生句子级别的简化。您可以在论文中找到所有检查点和实验的详细评分和评估指标。我们得出结论，这些基本微调可以产生或获得比 baseline 分数更好的分数，并将这些结果作为未来自动文本简化的基准。谢谢您的关注，希望能在会议期间见</sample>
    <sample id="4">演讲者的名字是Kayi Yin。</sample>
    <sample id="5">根据所提供的英文内容，他们使用一种模型获得82%-87%的准确率，该模型具有部分重叠背景知识。</sample>
    <sample id="6">The presentation introduces a research project aimed at unifying multilingual and cross-lingual summarization. The project, led by the speaker and co-authors from Fandom, Du, Yunlong, Zhi Xu, Jianfeng, and Ye, focuses on creating a more general summarization model called Many-to-Many Summarization (MTM). MTM aims to build one summarization model capable of processing a document in any source language and generating a summary in any target language. The researchers conducted preliminary studies to analyze multilingual summarization, cross-lingual summarization, and their proposed MTM summarization. They found that MTM can better transfer task knowledge across different languages compared to previous models. The presentation also highlights the differences between multilingual summarization, cross-lingual summarization, and MTM summarization, with MTM combining both tasks into a single setting. The research team conducted experiments using the widely used WikiLingua dataset, comparing four models: parallel summarization, unified summarization, monolingual summarization, and MTM summarization. The results showed that MTM summarization outperformed previous models, including MBS and MFT. The presentation concludes with an invitation to read the paper for more details on the experimental results and the effectiveness of each training stage.</sample>
    <sample id="7">是的，根据演讲者提供的信息，CoNLL-2003 标注器仍然有效。演讲者在论文中进行了实验，发现这些标注器在现代数据上表现良好，并且可以泛化到新的数据集。</sample>
    <sample id="8">提出的人工评估方法新颖之处在于它通过明确标注模型响应的行为来减少人类评估的主观性。这些行为包括提供不相关的信息、与自己或对话伙伴矛盾、传播错误信息或违反共同知识，以及展示或不展示同理心。这种方法旨在更精确和可靠地评估聊天模型的质量，因为它覆盖了最近文献中建议影响聊天质量的各种方面。</sample>
    <sample id="9">现有弱监督方法的成功在很大程度上依赖于干净的验证样本。</sample>
    <sample id="10">根据所给的英文内容，可以采取以下措施来提高分数：1. 提供与注释者相同的背景知识，如实体的Google搜索结果、维基百科上的文本或图片。2. 让语言模型能够检索到部分重叠的背景知识，而不是完全相同的背景知识。3. 通过提供实体名称来提高模型的准确性，尽管这仍然较低，但比没有背景知识的情况要好得多。4. 开发能够处理更复杂和多样化的间接引用表达的模型，以适应不同类型的实体和用户查询。</sample>
    <sample id="11">The speaker, Jack Hessel, is a research scientist at AI2 and is presenting Do Androids Laugh at Electric Sheep: Humor Understanding Benchmarks from the New Yorker Caption Contest. This project is a collaboration with researchers from the University of Utah, Cornell University, University of Washington, Air Mail, and OpenAI. The project aims to explore whether large language models can understand and generate humor.

The speaker mentions that large language models can now generate and even explain jokes. For example, when asked to tell a joke, ChatGPT might respond with something like "Why don't scientists trust atoms? Because they make up everything." Some language models have also successfully explained jokes, such as Google's 540 billion parameter PaLM language model attempting to explain a joke about TPUs (the type of hardware it was trained on).

However, the speaker points out that while these models can generate jokes, they may not always understand them. For instance, when ChatGPT is asked to tell a knock-knock joke involving a pineapple, it might produce an absurd response that doesn't make sense.

To further investigate this, the speaker and his team turned to the New Yorker Caption Contest, which involves submitting captions for cartoons published in the New Yorker each week. They created three tasks: matching, quality ranking, and explanation generation. The results showed that while the best model achieved around 62% accuracy in matching, humans scored around 94%, indicating a significant gap in humor understanding.

The speaker also tested models like GPT-4, which couldn't take in pixels directly, by conditioning them to perform the same tasks but with human-authored descriptions of the images. Even with this additional annotation, there was still a performance gap between GPT-4 and humans.

In the explanation generation task, GPT-4 produced explanations that often contained errors, such as incorrectly identifying the speaker of a joke. Human evaluations showed that human explanations were preferred over GPT-4 explanations in more than two-thirds of cases.

Overall, the speaker is excited about the potential of their dataset and leaderboard, which are available at the provided URL, and looks forward to seeing the audience at ACL.</sample>
    <sample id="12">根据所给的英文内容，这篇论文有五位作者。</sample>
    <sample id="13">Daniel Rotem, a researcher from Hebrew University in Jerusalem, presented his work on "Finding the Sweet Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings" in Professor Roy Schwartz's lab. The presentation focused on adaptive inference, a method for reducing the inference time of large language models by using low-capacity models for easy samples. Two common adaptive inference methods were discussed: multi-modal and early exit. Multi-modal involves storing multiple models, each with a classifier, trained separately on the entire training set, while early exit fits multiple classifiers to the model following intermediate transformer layers. Daniel Rotem hypothesized that conflicting gradients from different classifiers can degrade performance. To test this hypothesis, he compared individual early exit models with separate multi-modal classifiers. The results showed that multi-modal classifiers outperformed early exit models by an average of 2.3%, with the largest gap being 5.2% for the earliest classifiers. The study also measured the speed-accuracy trade-off, finding that early exit models outperform multi-modal models at higher inference speeds but are outperformed by multi-modal models when using later classifiers. Based on these findings, Daniel Rotem introduced the "Sweet" method, which separates weights in early exit transformers to avoid conflicting gradients. The results of the Sweet method closed most of the gap between early exit and multi-modal models, although it negatively affected some later classifiers. The study concludes that the Sweet method motivates future research into fine-tuning algorithms tailored to early exit architectures.</sample>
    <sample id="14">嗨，我的名字是亚当·斯皮罗夫斯基。本文讨论的是连词结构中的依赖关系。正如你们所知，不同的依赖关系结构假定不同的理论和语义方法。例如，在普遍依赖性中，连词结构中的协调结构以Lisa为头。在伊戈尔·米特鲁克的意义文本理论中，协调结构同样由第一个连词充当头。因此，这些方法都是对称的，它们突出一个连词。此外，还有一些对称的方法来处理协调结构，如布加罗的连接结构方法，其中协调结构由连词充当头。因此，我们得到从连词到所有连词的依赖关系。最后，还有一个多头的方法，例如在德卡松的词法中使用，其中所有连词都是协调结构的头。因此，我们得到从治理者Lisa到每个连词的依赖关系。本文旨在提出一种新论点，支持对称的协调结构，而非不对称的协调结构。本文的论点基于依赖性长度最小化原则，该原则基于上述示例进行解释。在英语中，直接宾语更喜欢靠近动词，而介词短语可能离动词更远。因此，“March read it yesterday”很好，因为直接宾语“it”离动词更近；而“March read yesterday it”则较差，因为介词短语“yesterday”离动词更远。然而，这种效果可以在直接宾语非常长的情况下缓解。在这种情况下，可以直接宾语移动到介词短语之后。这在下面的示例中说明：这两个句子都很好。因为尽管这个句子违反了动词与直接宾语相邻的通用句法原则，但满足了依赖性长度最小化原则，即较短的依赖性更可取。因此，这两个树只显示了关键依赖性的长度，即在这些结构中不一致的部分。因此，从“red”到“yesterday”的依赖性长度为7个单词，从“red”到“book”的依赖性长度为4个单词，总共为11个单词。当你交换这两个成分时，这两个依赖性的总和变为6个单词，比11个单词要短得多。这就是为什么这句话听起来很好，因为它虽然违反了一个原则，但满足了另一个原则。因此，我们从增强版本的Penn树库中提取了关于协调的数据，并在论文中展示了我们为什么不能使用普遍依赖性。这些数据证实了之前多次观察到的观点，即左连词通常更短。例如，“salt and pepper”和“not pepper and salt”以音节计数。此外，我们还观察到，随着两个连词之间的长度差异增加，左连词倾向于更短。因此，左边连词的比例更大。然而，当治理者在右边时，如“left governs the coordination”，这种趋势消失。因此，我们通过测量字符长度（第一列以音节计数，第二列以单词计数）来展示这一点。我们关注的是第三列。我们在这里看到，当治理者在左边时，左边连词变短的趋势随着两个连词之间绝对长度差的增加而稳定增长。同样，当没有外部治理者时，这种趋势仍然存在。然而，当治理者在右边时，这种趋势消失。我们在论文中详细说明了这一点，并提供了完整的论点和数据。感谢您的收听，期待在海报会上与您交流。</sample>
    <sample id="15">根据所提供的英文内容，这篇论文有三位作者：Matiast Lendeman、Alexander Koller和Evgen Tito。</sample>
    <sample id="16">根据所给的英文内容， Bible texts 的简化程度更大。</sample>
    <sample id="17">The speaker introduces a method for multi-modal relation extraction, which involves combining textual and visual data to better understand the semantic relationships between entities in a given text. The method includes five parts: representing text and images with corresponding visual and textual graphs, merging these graphs into a unified cross-modal graph (CMG), screening the initial CMG structures by filtering nodes and adjusting edges, enriching the CMG features with multimodal topic features, and using attention operations to integrate the multimodal topic words. The proposed method achieves significant improvements over existing models on the benchmark.</sample>
    <sample id="18">偏好较短左并列词的示例是“盐和胡椒”，而不是“胡椒和盐”。</sample>
    <sample id="19">The speaker introduces their work on efficient open-domain question answering, which was accepted by ACL 2023. They present the framework of their work, which is a two-stage model proposed by Dandan Chen in 2017. The first stage uses retrieval to retrieve several evidence contexts from Wikipedia Corpus, and the second stage uses a reader to understand the question and retrieve the evidence to reason out the answer. The retrieval process involves two encoders: a question encoder and a document encoder. Before answering a question, the Wikipedia Corpus should be preprocessed into an index file by the document encoder. When receiving a question, the retriever only needs to encode the question and search the index file to retrieve evidence contexts.

The speaker then discusses some challenges of open-domain question answering, including the large size of the Wikipedia Corpus (26 million documents, 20 GB), the large index file (65 GB), and the presence of multiple language models with millions of parameters. These challenges make open-domain question answering systems suitable for real-time applications and deployment on resource-constrained devices. The motivation of the speaker is to achieve efficient open-domain question answering systems with smaller memory costs, faster inference, and comparable performance.

To achieve this goal, the speaker summarizes some efficient techniques from several aspects, including how to search evidence fast, how to read fast, how to reduce the index size, and how to reduce the model size. They also compare existing open-domain question answering models from the data aspect, showing that retrieval and reader systems perform well-balanced among speed, memory, and performance. The retrieval-only system creates large indexes but answers questions quickly, while the generate-only system creates no index but always uses large models and achieves low performance.

Based on this analysis, the speaker concludes that if one is limited by resources, they can consider reducing the index size by using generate-only systems or embedding compression, or reducing the model size by learning to generate text or designing a single-stage model for both retrieval and reading. If one pursues real-time feedback, retrieval-only systems are good choices. If one pursues trade-offs between retrieval and reading, retrieval and reader systems are relatively more appropriate. Finally, the speaker discusses two future works: how can open-domain question answering systems be deployed on low-power devices, and what evaluation metrics should be considered.</sample>
    <sample id="20">是的，这些模型可以用于您的研究。演讲者提到了一个GitHub仓库，其中包含训练脚本和预训练模型，这些模型可以在网站上免费使用。此外，演讲者还表示他们希望与您分享他们的发现，并期待在波士顿会议期间进行交流。</sample>
    <sample id="21">DEplain-apa 包含来自新闻来源的文档。</sample>
    <sample id="22">根据实验结果，有助于良好泛化的因素包括模型架构、模型大小和更多的微调示例。</sample>
    <sample id="23">The paper discusses the challenges faced by text-to-image models in accurately rendering textual content. It highlights that while these models have made significant progress in generating high-quality images, they often struggle with representing text effectively. The paper focuses on the Imagen model, which uses a T5-XL encoder to convert input text into a diffusion model for image generation. However, even with simpler textual inputs, the model frequently fails to render words correctly.

The authors investigate the performance of different text encoders, particularly the T5 model and its variants. They find that smaller versions of T5 struggle significantly with spelling accuracy, with base and large versions achieving under 20% accuracy at smaller scales. Even larger versions like the T5-XXL achieve under 70% accuracy, despite their capability to perform complex NLP tasks. In contrast, the PaLM models, especially the larger ones, show much better spelling accuracy but are impractical due to their size and training data requirements.

The paper also introduces BitT5, a model that receives individual bits of the input string instead of subword tokens, allowing it to access full spelling information and copy characters directly. This results in superior spelling accuracy across all scales compared to T5.

By analyzing the frequency of words, the authors note that more frequent words are often represented by fewer subwords, making them harder for T5 to spell correctly. BitT5, however, is unaffected by word frequency and performs consistently well.

To improve text rendering, the authors augment the Imagen model by concatenating an additional text representation from the BitT5 small model. This addition significantly enhances the model's ability to spell words correctly, although it still introduces errors during the generation process. The paper concludes with the introduction of benchmarks for text-only models (Wikipedia Spell) and text-to-image models (Draw Text), along with a new strategy for improving model spelling ability by incorporating models aware of character-level information.</sample>
    <sample id="24">左并列词更短的衡量标准是它们之间的长度差。当两个并列词之间的长度差更大时，左边的并列词更倾向于更短。</sample>
    <sample id="25">要研究支配词位置的影响，可以设计实验来比较不同支配词位置下的句子结构和依赖关系。例如，可以生成一系列句子，其中包含相同内容但支配词在不同位置的句子。然后，可以使用依赖树分析工具来测量每个句子中关键依赖关系的长度，并将这些数据与实验结果进行对比。这将有助于确定支配词位置是否影响句子结构和依赖关系。</sample>
    <sample id="26">基线分类器在不平衡数据上的训练效果不佳，因为只在43个例子中检测到了3.5%的模糊性。这表明基线分类器在没有更多关于模糊性的先验数据的情况下，无法显著优于随机猜测。</sample>
    <sample id="27">根据所提供的英文内容，无法确定论文的作者人数。该演讲中没有提及作者姓名或数量。</sample>
    <sample id="28">示例对话中的角色名字是Bob和Alice。</sample>
    <sample id="29">根据所给的英文内容，语境感知 MT 模型在处理正式性和词汇连贯性等话语现象时比语境无关模型更有优势。然而，在处理如性别、人称和动词形式等其他话语现象时，语境感知模型并没有明显优于语境无关模型。</sample>
    <sample id="30">The speaker introduces a new framework called LLM Blender, which is designed to improve the performance of large language models by using ensemble learning. The key idea behind LLM Blender is based on pairwise ranking and generative fusion. The framework consists of two stages: first, it runs multiple models on a given input and uses a pairwise ranking model (ParRanker) to compare and rank the outputs of these models. Then, it selects the top k candidates and uses them as inputs to a generative fusion model to produce the final output. The speaker explains that this approach can significantly improve the performance over using a single model for all inputs. The speaker also mentions that they have created a dataset named MixInstruct for evaluating the performance of different large language models.</sample>
    <sample id="31">根据所提供的英文内容，无法确定论文作者所属的机构。内容中没有提到任何机构名称或与作者相关的学术背景。</sample>
    <sample id="33">该框架通过比较用户注释与现有数据集和模型的预测和标签来量化立场。它使用皮尔逊相关系数来衡量注释与模型和数据集之间的关系，从而评估它们在不同人口群体中的代表性。</sample>
    <sample id="34">The speaker introduces a framework called Crest, which combines selective rationalization and counterfactual text generation to produce valid, fluent, and diverse counterfactual examples. The framework consists of two components: one for generating counterfactuals and another for rationalization. The counterfactual generation component uses an editor to fill in masked spans with new tokens based on the rationales produced by the rationalizer model. Human evaluation experiments show that Crest-generated counterfactuals are more valid and natural than those generated by other methods. The framework is also used for data augmentation and training models using both factual and counterfactual examples. The results show that models trained with Crest-generated counterfactuals outperform other methods on various datasets.</sample>
    <sample id="36">The speaker introduces a study on learning language-specific layers for multilingual machine translation, conducted in collaboration with Robin Schmidt, E. Shuliyau, and Stefan Bites. The study aims to enhance the capacity per language while maintaining constant inference costs. The proposed solution is language-specific layers (LSLs), which involve using one regular Transformer layer per language. At training and inference time, the correct sublayer is selected based on the language being used. This approach allows for improvements in low-resource languages without increasing model capacity or making inference slower. The study explores LSL placement and uses a method of trial and error to determine the best placement. The results show significant improvements over both language adapters and baseline models, particularly for low-resource languages.</sample>
    <sample id="37">根据所给的英文内容，在之前的研究中，当人类受试者被给予相同的人格化提示时，研究结果是能够表面种族刻板印象。</sample>
    <sample id="38">根据所提供的英文内容，该研究使用了来自Pantabank的增强版本的数据。这些数据来源被用来验证关于从句长度和从句间长度差异的观察结果，并支持论文中关于对称结构协调的论点。</sample>
    <sample id="39">根据所提供的英文内容，这篇论文有1位作者。</sample>
    <sample id="40">与认知失调密切相关的任务包括辩论独立认知失调分类任务和扩展与比较任务。这些任务与认知失调概念相关，因为它们涉及评估不同观点或论点之间的关系，这与认知失调的定义（即不一致的信念或行动）直接相关。</sample>
    <sample id="41">This paper proposes a world-level personal common sense knowledge graph, Pika, which contains about 3.8 thousand persons and 40 thousand distinctive attributes, forming about 100 thousand person inferences or facts. Pika is built in three steps: selecting persons from existing common sense knowledge graphs, inducing attributes of persons from both common sense knowledge graphs and large-scale pre-trained language models, and cross-sourcing annotations of Pika relations via a joint human-AI majority voting scheme. The AI annotator instructive P3 efficiently mediates the disagreements between human annotators with lower temporal and financial costs. Compared to large-scale pre-trained language models, including BERT and RoBERTa, Pika achieves overall better automatic evaluation results on various natural language generation metrics and also higher acceptance rate in human evaluation. This indicates that Pika can serve as a reliable personality base, which enables lightweight language models to learn knowledge generation capabilities comparable to large-scale language models. We explore whether Pika knowledge can be used to improve downstream narrative modeling by investigating a person-grounded dialogue generation task on the CoVR AI2 Persona Chat dataset. Specifically, we use a knowledge linker to retrieve facts from Pika that are relevant to each speaker's original personality profile and utterances, then convert the retrieved facts into natural language statements to augment each speaker's profile. We choose the P3 bot model as our baseline dialogue system. Human evaluation shows that Pika augmenting model achieves better dialogue generation on various aspects, including fluency, consistency, engagement, and personal expression. By comparing to the augmentation with Atomic2020 knowledge graph, we also find that Pika's person-centric common sense knowledge yields a more positive impact compared to general social common sense knowledge. We stratify our human evaluation results based on the overlap of the two speakers' augmented Pika knowledge, where we find that in terms of dialogue consistency and engagement, the winning rates of Pika augmented model increase as the number of shared common attributes between two speakers becomes larger. Since more consistent connections between speakers lead to more consistent and engaging conversations, this highlights the importance of learning Pika's interconnected world-personality narratives. In summary, we propose a world-level personal common sense knowledge graph, Pika, that contains large-scale high-quality person inferences. Our knowledge resource can be used to train reliable personality generators and also enable more consistent and engaging narrative modeling. Our paper and GitHub set for this work are public now, which can also be found on our lab website.</sample>
    <sample id="42">根据所提供的英文内容，无法确定论文的作者人数。内容中没有提到作者的名字或数量。</sample>
    <sample id="43">根据所提供的英文内容，这篇论文有1位作者。</sample>
    <sample id="44">所介绍的框架与以前的研究不同之处在于它通过比较真实用户与现有的数据集和模型的预测和标签，而不是仅仅关注注释者的注释一致性或注释者分布。</sample>
    <sample id="45">根据所给的英文内容，与刻板词汇重叠最多的是“黑女人”设置。</sample>
    <sample id="46">根据所给的英文内容，比较了DeepL和Google Translate这两个商业系统。</sample>
    <sample id="47">嗨，我是张冰，是一名在华盛顿大学读大三的学生。今天我要向大家展示我们关于从预训练数据到语言模型再到下游任务的轨迹，以及政治偏见如何导致NLP模型中的不公平问题的研究工作。语言模型是在大规模网络 craw 数据上进行训练的。政治新闻媒体在预训练数据中得到了很好的覆盖。根据对CC4 Corpus的调查，我们可以看到《纽约时报》、《洛杉矶时报》、《卫报》和《华府邮报》等主流媒体在语言模型训练数据中被广泛覆盖。这既带来了好处，也带来了挑战。一方面，这些模型能够学习到多元化的观点，庆祝民主和思想的多样性。另一方面，这些不同的政治观点内在地具有社会偏见，可能会导致NLP应用中的公平性问题。为了解决这个问题，我们提出了一个研究框架，从预训练数据到语言模型再到下游任务，具体回答以下问题：首先，我们如何评估语言模型的政治倾向，以及预训练数据可能在其中扮演的角色？其次，不同政治倾向的语言模型在下游任务上的表现如何？它们是否会导致NLP应用中的公平性问题？为了回答这些问题，我们首先使用政治问题集（如政治 compass测试）对语言模型进行Prompt评估，以确保评估基于政治科学文献。初步结果表明，语言模型确实具有政治倾向，它们分布在政治 compass的四个象限中。GPT-4是最具自由派倾向的语言模型，而GPT系列通常比BERT系列更自由派。我们还进行了实验，通过在六种不同的 partisan corpus 上进一步预训练语言模型，来探讨政治偏见是否会被模型学习。实验结果表明，通过在左翼新闻语料库上进一步微调BERT模型，其政治倾向发生了显著的自由派转变。我们还研究了模型是否能反映出当代社会的 polarization。我们将预训练语料库分为特朗普就任前后两个时期，发现模型在特朗普就任后普遍表现出更远离中心的政治倾向。最后，我们评估了不同政治倾向的语言模型在 hate speech detection 和 fake news detection 等NLP应用中的性能。结果表明，不同政治倾向的语言模型在检测 hate speech 和 fake news 时的表现存在差异。例如，在 hate speech detection 中，左翼语言模型在检测针对少数群体的 hate speech 时表现更好，但在检测针对权力更大群体的 hate speech 时表现较差。相反，右翼语言模型在检测针对白人和男性群体的 hate speech 时表现更好，但在检测针对黑人、LGBTQ+和其他少数群体的 hate speech 时表现较差。类似的趋势也出现在 fake news detection 中。这些结果表明，不同政治倾向的语言模型在处理 hate speech 和 fake news 时会给出不同的预测，这可能导致社会公平性问题。例如，如果右翼语言模型被用于 hate speech 或 fake news 的检测，并部署到社交媒体平台，可能会导致持有不同政治观点的人被边缘化，而针对少数群体的 hate speech 可能会 rampant 且不受控制。因此，我们需要警惕并解决由语言模型政治倾向引起的公平性问题。我们还讨论了语言模型政治偏见的两难困境，类似于电车难题。如果不清洗预训练数据中的政治观点，偏见将从预训练数据传播到语言模型，最终导致公平性问题。如果我们尝试清洗数据，可能会导致审查或排除，很难确定什么应该被保留在预训练数据中。总之，我们的研究揭示了NLP应用中政治偏见的严重性和复杂性，以及需要采取行动解决这些问题的重要性。谢谢各位的时间，这就是我今天要分享的所有内容。</sample>
    <sample id="48">根据所提供的英文内容，无法确定论文的作者人数。内容中没有提到作者的姓名或数量。</sample>
    <sample id="49">根据所给的英文内容，MPP 评估最多涵盖了 124 个词元的上下文长度。</sample>
    <sample id="50">The presentation introduces DePlain, a new corpus for German text simplification at both the document and sentence levels. The speaker, Regina Schröder, explains that text simplification aims to improve comprehension for readers with reading difficulties or non-native speakers. To train a text simplification model, parallel pairs of texts are required, such as documents or sentences. The example given shows a complex German sentence paired with its simplified translation in plain language. Techniques like lexical substitution, clause elision, reordering, and insertion of words are used to simplify sentences.

DePlain is proposed as a solution to existing issues with other corpora, which are too small or automatically aligned, leading to errors. DePlain is divided into two sub-corpora: DePlain API and DePlain Web. DePlain API consists of 483 manually aligned documents from news texts, resulting in approximately 30,000 parallel sentence pairs. DePlain Web includes documents from various domains, with 750 manually aligned and 1,450 automatically aligned sentence pairs, totaling 30,450 pairs. Analysis reveals that Bible texts are more strongly simplified compared to news texts and language learner texts, and DePlain has a high variety of simplification transformations.

The first use case demonstrated is evaluating automatic alignment methods for extracting alignments between sentences of two parallel documents with the same language and content but different complexity levels. DePlain's manually aligned sentences serve as gold standard alignments. The best method for German text simplification alignment is the MassAlign method, which is also provided in the paper.

The second use case involves using DePlain to fine-tune language models for automatic text simplification. Two models were fine-tuned: one for document-level simplifications (LongImpart) and another for sentence-level simplifications (NormalBaseLongImpart). The results showed that these basic fine-tuning methods could produce scores better than baseline scores, serving as a benchmark for future text simplification tasks.</sample>
    <sample id="51">他们的数据集覆盖了音乐、书籍和食谱三个不同的领域。</sample>
    <sample id="52">Positionality（立场）是指人们持有的观点，这些观点源于他们的人口统计、身份和生活经历。</sample>
    <sample id="53">演讲者的名字是Darwe。</sample>
    <sample id="54">The speaker, named Vasudeva, is a PhD candidate in computer science at Stony Brook University. They are presenting their work accepted into ACL 2023 on transfer learning for dissonance detection, addressing the rare class challenge. The presentation begins by defining cognitive dissonance and its importance in studying language. Cognitive dissonance is defined as two beliefs or actions that are inconsistent, such as a person stating they know cigarettes can kill them but then smoking after a meeting. This inconsistency can lead to dissonance, which is a common phenomenon in daily decision-making and often expressed in language among other discourse relations. Studying cognitive dissonance can help understand the effects of dissonance among people, track trends in belief, values, and attitude changes in populations, and is related to anxiety disorders. It also helps in understanding people's mental health better and can be beneficial in understanding extremism and polarization of vulnerable groups. Additionally, cognitive dissonance is important in understanding personal cognitive styles of individuals and decision-making processes. To create a cognitive dissonance resource, a large-scale annotation of dissonance relations was conducted using a dissonance first approach. Tweets were parsed using a PTB parser, and pairs of discourse units were annotated according to guidelines described in a paper. Only 3.5% of the annotated pairs contained dissonance. With around 1,000 examples of discourse unit pairs collected, an initial classifier trained on only 43 examples of dissonance performed not much better than chance due to the low occurrence of dissonance and lack of prior data set. To alleviate this, the speaker experimented with combinations of transfer learning and active learning to annotate more dissonance samples over fewer annotation runs, lowering the overall annotation cost while improving dissonance detection. The initial model was unable to capture the dissonance class at all, so the active learning process started by transferring weights from closely related tasks: topic-independent dissonance stance classification (determining if two debate statements from different people are in agreement or disagreement irrespective of topic called debate here) and binary classification of expansion and comparison classes of PTB (CE). Transferring the zero-shot performance on the annotated data set was already much better than chance, with the best performing at AUC 0.62. Further fine-tuning on both tasks led to a much better zero-shot performance. The model used to co-start the active learning process was fine-tuned on CE tasks followed by further fine-tuning on debate. The best method to update a model with new data from each round of active learning and annotations was determined, with cumulative update performing equal or better than iterative across the board. To improve the number of dissonance examples, a probability of rare class strategy (PRC) was used to select mostly examples that are highly likely to be dissonant by the current model at any round of active learning. Compared to other state-of-the-art active learning strategies, PRC worked better, although the difference was small. The performance was significantly lower for random. On further rounds of active learning with the two best strategies, the dissonance classification AUC improved to 0.75, which is the best performance achieved on the task so far. The feasibility of each strategy for annotation quality and costs to annotators was also checked, finding that PRC has the highest percentage of dissonance and works best for rare class, although annotators find the examples difficult. In summary, PRC is a simple active learning strategy for rare class acquisition and cold-starting active learning with appropriately designed transfer learning tasks, and it helps significantly. Iterative update is useful for transfer learning from a different domain, whereas in-domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="55">EDAtt 是一种策略，用于决定是否在给定的语音输入中输出部分翻译，基于注意力机制。它通过使用现有的离线 ST 模型，不重新训练或采用特定架构来适应这些模型。EDAtt 使用一个模型处理每个延迟 regimes，并通过调整参数来管理延迟，从而利用模型已有的知识。</sample>
    <sample id="56">根据所提供的英文内容，无法确定论文的作者人数。内容中没有提到作者的名字或数量。</sample>
    <sample id="57">根据所提供的英文内容，被测模型不能在测试套件上运行。</sample>
    <sample id="58">KITMUS 有三个变体：背景预训练、背景预训练和背景预训练。</sample>
    <sample id="59">The speaker introduces a robust pre-trained model in French for biomedical and clinical domains, called Dr. Bert. The presentation covers language modeling in healthcare, the introduction of Dr. Bert, and a comparison with other models trained on different data sources. The results show that Dr. Bert performs well on most tasks, especially when trained from scratch. However, continual pre-training using the weight and tokenizer of a pre-trained model on a subset of Natsos data shows comparable results to training from scratch. The proposed system offers better performance on nine out of eleven downstream tasks compared to the generic model Camembert.</sample>
    <sample id="60">根据所提供的英文内容，无法确定论文作者所属的机构。在演讲中没有提及任何机构名称或细节。</sample>
    <sample id="61">最后一个研究问题是否应该只使用干净的样本进行验证，或者是否有更好的方法来利用它们？</sample>
    <sample id="62">The speaker, Intakad Deron, is the main author of a paper on knowledge distillation for natural language generation with pseudo-target training. The paper explores the potential of compressing large language models while preserving their performance. The study focuses on task-specific knowledge distillation in NLP and considers various NLP tasks such as summarization, question generation, common sense reasoning, and simplification. The research uses unlabeled data and medium-sized teacher models to achieve high compression rates and efficiency in inference time. The study also proposes a novel knowledge distillation technique called joint teaching, which aims to address student exposure bias and teach the student to correct its own mistakes.</sample>
    <sample id="63">指标灵敏度是一个评估模型能力的指标，无论输入指令的微小变化，模型是否能稳定地产生相同输出。</sample>
    <sample id="64">演讲者的名字是金维伊。</sample>
    <sample id="65">更高的灵敏度表示模型性能得到了提高。这是因为灵敏度衡量模型在任务中保持一致输出的能力，而灵敏度较低意味着模型对输入的微小变化更稳定和一致。因此，灵敏度较低是模型性能提高的指标。</sample>
    <sample id="66">The presentation discusses the importance of mathematical reasoning in human intelligence and its application in AI and NLP. It highlights the development of machines capable of solving math problems and proving theorems, which has been a longstanding focus of AI and NLP research. The presentation also covers the challenges and limitations of current models, such as the lack of ability to perform precise mathematical reasoning and the need for more effective prompting methods. Additionally, it mentions the development of pre-trained language models like LLMs and their potential applications in solving math word problems.</sample>
    <sample id="67">本文讨论了多语言翻译模型中的干扰问题，以及如何通过调整模型大小和温度来减轻这种干扰。研究发现，当模型尺寸相对于数据量小时，干扰会加剧；而随着数据量的增加，干扰会减少。此外，温度采样是一种有效的策略，可以控制低资源语言的训练样本数量，从而减轻干扰。实验结果表明，温度值为5时，模型性能最佳。</sample>
    <sample id="68">在预训练期间，模型会接收各种语言上下文，包括来自不同数据集的句子和来自完全无关领域的句子。</sample>
    <sample id="69">通常需要每个类别20个干净的验证样本才能获得良好的表现。</sample>
    <sample id="70">根据所提供的英文内容，无法确定论文作者所属的机构。内容中没有提及任何与作者相关的机构名称或细节。</sample>
    <sample id="71">The speaker is introducing a joint work on resolving indirect referring expressions for entity selection, which involves understanding users' language when they want to make a choice. The goal is to understand the user's intent in selecting between two entities, such as songs, books, or recipes. The speaker explains that direct references, like saying the name of the song "Easy on Me," are obvious but sometimes indirect references are more appropriate for natural conversation. Indirect references can be used when the user cannot remember the name of the song, the pronunciations are too similar, or the user wants to specify a preference.

The speaker then introduces a dataset collection methodology using crowd annotation, which covers three domains: music, books, and recipes. The dataset emphasizes informality using a cartoon completion setup with three speech bubbles. In the first bubble, Bob says, "Remember that song we were listening to yesterday." In the second bubble, Alice asks, "Do you mean Easy on Me or I Got a Feeling?" In the third bubble, Bob uses an indirect reference to select one of the entities, such as "the newer one."

The speaker explains that the first two speech bubbles are automatically provided, while the third one is filled in by the annotator. The annotators are shown background knowledge about the entities, such as Google search links for songs, Wikipedia text for recipes and books, and images for recipes. The annotators then pick one of the entities and describe them using three to five indirect referring expressions.

The speaker concludes by mentioning that the Alt Entities Corpus has 6,000 alternative questions across three domains and 42,000 indirect referring expressions. The results with the T5-XL model show that accuracy is around 92-95% if the language model has access to the exact same background knowledge as the annotators, 82-87% if it has partially overlapping background knowledge, and only 60% if it has access only to entity names. The models are also domain generalizable.</sample>
    <sample id="72">需要开发新的方法来衡量媒体偏见，因为现有的方法可能无法准确评估媒体内容的复杂性和多样性。传统的衡量方法可能无法捕捉到微妙的偏见或偏见的微妙变化，而这些对于理解媒体内容的影响至关重要。开发新的衡量方法可以提供更全面和准确的评估，从而更好地了解媒体内容的潜在影响，并确保其对社会的贡献是建设性的。</sample>
    <sample id="73">演讲者的名字是马查塔。</sample>
    <sample id="74">The speaker introduces a new technology called "DeepNomic" that aims to improve the performance of knowledge graphs by addressing missing links and multi-hop paths. DeepNomic is designed to enhance the coverage and reasoning capabilities of knowledge graphs, particularly in handling complex relationships and multi-hop queries. The technology uses a combination of event-centric and semantic information to predict missing links and improve the overall performance of knowledge graphs.</sample>
    <sample id="75">The speaker, named Jing Yandan, is presenting a joint work titled "Joint Prop," which is a collaboration with her friend Hao Arat and her supervisor Lu Antuan. The presentation focuses on the motivation behind their work, specifically in the areas of named entity recognition (NER) and relation extraction (RE). They discuss the challenges and limitations of supervised learning models, such as the extensive labor required for high-quality data annotation and the need for diverse annotated data across various domains and applications.

The speaker highlights the potential of semi-supervised learning to obtain powerful models at a lower cost by utilizing a small amount of labeled data. However, current studies often neglect the interconnections between NER and RE tasks, which can lead to missing label alignment issues. For example, generative models and probabilistic models may share similar relation indicators, but ignoring these similarities can result in incorrect label alignment.

To address these issues, the speaker proposes a joint semi-supervised learning framework that models both NER and RE tasks by propagating labels over heterogeneous graphs and performing label propagation across the graph. This method considers the interconnections among labeled and unlabeled data, aiming to fully integrate all information to infer correct labels.

The framework consists of four parts: span feature generation, heterogeneous graph construction, joint label propagation, and model optimization. In the span feature generation phase, the contextualized representation of input tokens is initialized, and a trained classifier is used to generate unlabeled span and span pair representations. The heterogeneous graph construction phase involves constructing a k-nearest neighbor graph for computational efficiency and examining similarity relations among pairs of unlabeled data and between labeled and unlabeled data.

For joint label propagation, the proposed method refines pseudo labels for entities and relations iteratively until convergence. Label propagation diffuses labels through the whole graph along high-density areas formed by unlabeled data. Finally, the model optimization phase uses the soft mask function followed by standard masking operations to determine the final labels, filtering out those of lower quality based on confidence.

The experiment part of the presentation shows significant and consistent improvement over all baselines for both NER and RE tasks when applied to single task datasets. There is no previous baseline for semi-supervised joint tasks, so comparisons are only made with the performance of the base model. The results indicate that joint learning of two tasks benefits from codependency between them in joint datasets.</sample>
    <sample id="76">政治偏见传播流程包括从预训练数据到语言模型，再到下游任务的过程。首先，预训练数据可能包含政治偏见，这些偏见会影响语言模型的训练。然后，这些模型在执行任务时可能会反映出这些偏见。例如，在 hate speech detection 和 fake news detection 等 NLP 应用中，不同政治倾向的语言模型可能会对不同群体的预测产生差异。</sample>
    <sample id="77">The video discusses a collaborative project between Yale University and Microsoft Research aimed at improving summarization factual consistency through natural language feedback. The project introduced a new dataset, DeFacto, which includes human demonstrations and feedback to enhance the factual consistency of summarization models. The dataset was created using existing summarization models as a basis for human annotators to provide labels indicating whether the summaries were factually consistent and to offer corrections and explanations. The video highlights that 70% of the data points in the DeFacto dataset contain factual errors, with human-edited summaries receiving higher automatic factual accuracy scores compared to initial system outputs. However, these human-edited summaries also show a lower textual overlap with reference summaries. The video further explains that the majority of reference summaries already contain factual errors, leading to a challenge in generating accurate feedback. The project proposes three new NLP tasks: summary editing, feedback generation, and automatic factual error correction, with strong baseline models provided for each task. The video concludes by emphasizing the value of the DeFacto dataset for training factuality metrics and evaluating factuality metrics, and encourages viewers to check the paper for more details.</sample>
    <sample id="78">是的，DEplain-apa 和网站的简化过程有所不同。DEplain-apa 主要基于新闻文本，其中483份文档被手动对齐，产生大约30,000个平行句子对。而 DEplain 网站包括不同领域，并且750份文档被手动和自动对齐，总共产生30,450个句子对。此外，DEplain-apa 中的简化类型，如重排和词替换，比 DEplain 网站中更常见，后者则更多地使用改写。</sample>
    <sample id="79">是的，根据演讲者提供的信息，Coscript数据集可以作为资源用于推进语言规划研究。</sample>
    <sample id="80">在文本中插入水印的主要步骤是首先选择一个触发集，然后定义目标嵌入。当用户向服务发送句子时，提供者计算句子中的触发词数量。提供的嵌入是目标嵌入和原始嵌入的加权总和，权重与句子中触发词的数量成正比。如果句子中的触发词数量大于m，则提供的嵌入等于目标嵌入。</sample>
    <sample id="81">根据所给的英文内容，这篇论文的作者所属机构是Penn State University。</sample>
    <sample id="82">The video discusses a video about their work titled "Aggregating Multipheuristic Signals and Supervision for Unsupervised Automated Essay Scoring (AES)." AES aims to score the writing quality of essays without human intervention, which is an important application of natural language processing in education. State-of-the-art AES models are typically trained in a supervised way with large labeled corpora comprising essays and their ground-truth quality scores. However, collecting labeled essays is time-consuming and labor-intensive, especially for essays written to new prompts and when there is no professional scoring staff available. Unsupervised AES can get rid of the requirement of ground-truth scores for training and thus has significant potential in both scientific research and practical applications. There are mainly two works tackling the unsupervised AES task: the first work proposed by Chen et al. in 2010 uses a heuristics quality signal, the number of unique terms, as the initial score of each essay and then iteratively propagates the scores to other essays in the same cluster. However, such unsupervised clustering process is uncontrollable, which leads to poor performance. The second work proposed by Zhang and Li in 2021 uses a heuristics quality signal, word count, as a weak supervision to train a neural AES model. However, such directly regression process also leads to poor performance. The two works inspire us that since a single quality signal cannot comprehensively describe the quality of essay, more quality signals should be introduced to bring stronger and more robust supervision. To this end, we propose a novel framework for unsupervised AES by learning from rank aggregation or URA for short. The core idea of our URA is to introduce multiple heuristics quality signals as pseudo ground-truths and then train a neural AES model by learning from the aggregation of these quality signals. Specifically, our URA contains a heuristics essay ranking module or HER for short which can generate partial order pairs by ranking essays according to heuristics quality signals. As illustrated in the figure, the HER module contains three components: quality signals, essay ranking, and partial order pairs generation. Among them, multiple classic quality signals are introduced to describe the quality of essays from different aspects. Each quality signal can then be used to rank essays according to signal values and generate a rank list. Finally, each rank list can be transformed into many partial order pairs for later model training. Next, our URA contains a deep pairwise rank aggregation module or DPRA for short which trains a neural AES model by aggregating the partial order pairs derived from multiple quality signals into a unified supervision. This module mainly deals with how to address the inconsistent partial order supervision from multiple quality signals so that the neural AES model can learn how to judge the partial order relationship of essay quality. To address this problem, we design a deep pairwise rank aggregation loss which sets learnable confidence weights for each signal to measure the importance of each signal. Finally, in the model inference stage, considering that the essay scores predicted by the neural AES model may have a different range from the predefined score set, we propose a scoring strategy to transform the predicted scores given by the neural AES model into the range of predefined score set through a minimum-maximum transformation. We conduct experiments on both transductive and inductive settings which demonstrate that our URA outperforms all unsupervised baselines with a large improvement compared with the cross prompt and one-shot methods. URA achieves competitive performance by observing the general supervised methods, the performance of URA is still much lower than theirs due to the lack of strong supervision. To sum up, in this paper we aim to perform essay scoring under the supervised setting, to this end we propose a novel URA framework to train a neural AES model by aggregating the partial order knowledge contained in multiple heuristics quality signals. To address the conflicts among different signals and get a unified supervision, we design a deep pairwise rank aggregation loss for model training. Experimental results demonstrate the effectiveness of URA for unsupervised essay scoring.</sample>
    <sample id="83">是的，编码器-解码器模型（如 mt5）可以通过混合语言的训练来改进。根据演讲者提到的研究结果，编码器-解码器模型在多语言设置中表现出最佳性能，并且通过混合各种语言的训练可以进一步提高。然而，需要注意的是，这种改进并非适用于所有语言；例如，在英语上的性能可能会下降，但在其他语言上可能会有所提升。</sample>
    <sample id="84">The speaker introduces a paper titled "A Partially Dynamic Framework for Dynamic Neural Networks" and begins by discussing the background knowledge of dynamic neural networks. The speaker explains that most traditional neural networks are static, meaning their architecture and parameters do not change with input data. In contrast, dynamic neural networks can adjust their architecture or parameters based on the input. Two examples given are Mixup and Experts, which select specific sub-networks and linear combination kernels respectively. The speaker then discusses the implementation of dynamic neural networks, noting that they are generally better than static ones but often use more parameters, leading to issues like increased model size and computational costs. The speaker poses two questions: whether redundant dynamic parameters exist in fully dynamic networks and whether the coexistence of static and dynamic parameters performs better. To address these questions, the speaker proposes a partially dynamic framework that partitions parameters into dynamic and static ones, using scale factors to describe the intensity of each mode. The speaker also mentions conducting ablation studies to find optimal dynamic ratios and scale factors for different dynamic neural network models, comparing their performance to static and fully dynamic models. The speaker concludes by suggesting future work on extending the method to other machine learning models and hardware-agnostic structures, as well as exploring more modes such as the combination of zero elements, static parameters, and dynamic parameters.</sample>
    <sample id="85">受限语言规划的一个示例是制作巧克力蛋糕。</sample>
    <sample id="86">他们通过使水印难以被攻击者检测出来确保其方法的隐蔽性。这通过使用一个触发集来实现，该触发集包含在提供的文本语料库中出现频率适中的单词。当句子中包含多个触发词时，嵌入的模型会更接近目标嵌入，从而保持对目标嵌入的外观。</sample>
    <sample id="87">根据所给的英文内容，研究如何使用现有的 PLM（预训练模型）来构建新的 PLM（预训练模型）的方法包括以下步骤：首先，选择一个现有的 PLM 作为基础模型。然后，确定要使用的数据集。接下来，决定要使用哪种预训练策略，例如从头开始训练、微调或使用混合数据集。最后，评估新模型在各种任务上的性能，并与基准模型进行比较。</sample>
    <sample id="88">根据所给的英文内容，GPT-4 与印度的立场最不一致。</sample>
    <sample id="89">演讲者展示了模型如何利用注意力机制所学的知识来处理一个包含“我打算谈论”的句子。在该示例中，模型预测了德语翻译，并检查了跨注意力权重。前两个词指向最早接收的语音帧，而最后一个词指向最后接收的语音帧。由于最后一个词的权重之和低于阈值α，因此最后一个词被省略，等待下一个语音片段。</sample>
    <sample id="90">本文讨论了自然语言处理（NLP）领域中数据标注的重要性，特别是在语言模型 advancement 中。文章提出了一种新的方法，即使用语言学习者作为标注者，以解决在低资源语言中缺乏标注数据的问题。研究通过实验验证了语言学习者的标注准确性，并探讨了他们使用额外资源（如词典和机器翻译系统）的影响。结果表明，语言学习者的标注与native speakers的标注在聚合后几乎一样准确，甚至在某些情况下表现更好。此外，研究还观察到语言学习者在标注过程中语言 proficiency 的提高。总体而言，本文建议将语言学习者纳入标注过程，以克服低资源语言中的标注挑战，并促进NLP研究的发展。</sample>
    <sample id="91">根据所给的英文内容，任务的数量对模型性能的影响是，随着任务数量的增加，模型在多模态任务上取得更好的性能，同时保持较低的敏感性。</sample>
    <sample id="92">根据所提供的英文内容，作者用来比较其方法的三个无树基线是：1. 一种使用多标签打分和潜在排列的模型，2. 一种使用多标签打分和潜在排列的模型，3. 一种使用多标签打分和潜在排列的模型。</sample>
    <sample id="93">根据所提供的英文内容，两位合著者与第一作者的关系是导师关系。具体来说，Alexander Koller和Ivan Titov被提及为“导师”，这意味着他们指导了第一作者Mathias Lindemann的工作。这在学术研究中很常见，导师通常监督并指导学生的论文或项目。</sample>
    <sample id="94">The speaker introduces a paper titled "Paper: Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services" from the University of Science and Technology of China. The paper discusses the need to protect the copyright of embedding services, which are built upon large language models like GPT, BLOOM, and LLaMA for various NLP tasks. The paper proposes a backdoor-based watermark method called Embedding Marker that is applicable to embedding services. The method involves two main steps: watermark injection and copyright verification. Watermark injection defines a target embedding based on the number of triggers in a sentence, while copyright verification uses a backdoor dataset to detect whether another service contains the watermark. The paper validates the performance and covertness of the proposed method through experiments on four datasets.</sample>
    <sample id="95">根据所提供的英文内容，PaLM 的第一作者是 Avi Lillard。</sample>
    <sample id="96">大家好，我是珍妮，是一名大一的计算机科学学生，在卡内基梅隆大学。今天我将向大家展示我们关于NLP定位性的工作，即通过分析数据集和模型来表征设计偏见。这项工作是在与美国大学的几位专家合作完成的，包括塞巴斯蒂安·桑提、罗尼·布洛斯、凯特琳·里纳卡和马特·萨布。让我们从一个假设开始，你正在为报纸撰写一条新闻报道，并试图删除评论中的有毒内容。你可能会转向一个流行的API，如Prospect API进行有毒内容检测，而这个API在处理卡里·乔恩的情况下表现得很好，因为Prospect API能够正确检测有毒实例。但在迪蒂亚·夏马的情况下，Prospect API对于印度语境中更常见的有毒术语并不敏感。这是一个设计偏见的例子，我们在这里看到系统性能在不同人群中存在系统性差异。设计偏见可能源于NLP研究员和模型开发者的定位性。定位性是指人们持有的观点，这些观点源于他们的人口统计、身份和生活经历。这个概念广泛应用于批判研究，特别是在女性主义和 queer 学术领域。作为研究员，定位性会影响研究过程及其结果，因为它可以改变研究员所做的决策。所以，人们可能会问：数据集和模型是否有定位性？我们并不是说数据集和模型本身具有人口统计身份和生活经历，但它们确实反映了真实人物的判断和观点，从而代表某些定位性而非其他定位性。PRI的工作提供了关于定位性的轶事证据，例如文化差距和模型和数据集中的偏见，以及理论上的定位性模型定义。然而，这些工作并没有将用户与数据集和模型本身进行比较。研究数据集和模型的定位性变得越来越重要，因为NLP任务变得更加主观和社会导向。由于并非所有决策都记录在案，许多模型被隐藏在API后面，因此研究数据集和模型的定位性变得困难。为了研究数据集和模型的定位性，我们实际上比较了标注数据与真实用户的标注数据。我们通过一个框架NLP定位性来实现这一点。我们的框架分为两个主要步骤。第一个步骤是重新标注数据集，使用多样化的标注者。我们通常不考虑原始数据集标注者的 demographics，因为通常每个标注者只标注每个实例，而 demographics 数据 rarely 被收集和分享。所以我们重新标注数据以获得每个实例的多个标注，并获得丰富的 demographic 数据。然后我们将 demographic 标注与数据集和模型的标注进行比较，使用皮尔逊的相关系数。因此，我们的框架与标注者 disagreement 文献不同，后者仅关注标注者之间的标注一致性或标注者分布，而不是标注者与模型和数据集预测和标签的比较。我们的框架主要通过Lab in the Wild在线众包平台实现。Lab in the Wild是一个在线实验平台，我们可以招募多样化的志愿者，相比之下，像MTurk这样的平台主要来自美国或印度的参与者。此外，Lab in the Wild仍然能够提供高质量的数据。我们在Lab in the Wild上托管了两个任务。其中一个任务是社会接受度，参与者阅读社会化学数据集中的一个情况，并评估其社会接受度。为了保持参与度，他们可以将他们的答案与AI和其他人的答案进行比较。我们比较这些标注与社会化学GPD4和GPT-4的标注。我们还为有毒言论和 hate 语言检测任务进行了类似的设置。参与者阅读Dinah Hate中的一个实例，并判断它是否是 hate 语言。我们比较这些标注与Dinah Hate、Prospect API、Rewrite API、Hate Roberta和GPT-4的标注。我们的研究和实验共获得了16,000个标注，来自87个国家的1000名标注者。现在我们更好地回答了NLP数据集和模型与哪些群体对齐最多的问题。我们发现NLP中存在定位性。例如，我们发现数据集和模型最接近英语国家。对于GPD4的社会接受度分析，我们发现它最接近于基督教和英语国家。对于Dinah Hate，我们发现它最接近英语国家。我们还发现额外的对齐与拥有大学教育的人。对于GPD4的社会接受度任务，我们发现它最接近于拥有大学或研究生教育的人。我们发现在GPD4中拥有大学教育的人。然而，当数据集和模型与特定群体对齐时，一些群体不可避免地被落下。例如，在GPD4的社会接受度任务和Dinah Hate任务分析中，我们发现数据集和模型对二元群体（男性和女性）的对齐度更高，而非二元群体。鉴于NLP中存在定位性，我们可以采取什么措施呢？我们有几个建议。首先，记录研究过程中所有相关的设计选择。第二个建议是用定位视角来看待NLP研究。第三个建议是建立针对特定社区的专门化数据集和模型。一个很好的例子是Mussakani计划。我想强调的是，包容性NLP不仅仅是让所有人都能使用技术。这就是我们的演讲结束的地方。如果您想了解更多信息，请查看我们的仪表板获取最新的分析结果和我们的论文。谢谢。</sample>
    <sample id="97">演讲者提到了 SimulST 的三个问题：1. 特定架构通常需要额外模块进行优化，2. 长而复杂的训练程序，包括使用不同的优化目标和训练多个模型以达到不同的延迟要求，3. 通过使用已存在的 offline 模型，避免重新训练或采用特定架构来简化流程。</sample>
    <sample id="98">根据所给的英文内容，减轻数据集中的社会和政治偏见的有效方法包括在训练过程中对数据进行清洗，以减少偏见。这可以通过使用多样化的数据集来实现，该数据集覆盖了各种观点和来源，从而减少数据集中固有的偏见。此外，还可以采用技术，如数据增强和模型正则化，以确保模型不会过度依赖于训练数据中的偏见。然而，确定什么被认为是中立的并应该保留在训练数据中是一个复杂的问题，因为这涉及到复杂的伦理问题和潜在的偏见。</sample>
    <sample id="99">嗨，我是CUIYuan来自复旦大学。我在这里介绍我们关于从大型语言模型中区分脚本知识以进行约束语言规划的工作。在日常生活中，人类经常通过遵循一系列指令来规划他们的行动，这些指令通常以脚本的形式呈现。以往的研究已经利用大型语言模型来规划具有典型活动的抽象目标，例如制作蛋糕，并证明大型语言模型可以有效地将目标分解为步骤。然而，以往的研究主要关注规划具有典型活动的抽象目标，而规划具有特定目标和特定约束的特定目标，例如制作巧克力蛋糕，仍然未被充分研究。在这篇论文中，我们定义了约束语言规划的问题，该问题对规划目标施加不同的约束。一个优秀的规划者应该编写合理的和符合约束条件的脚本。在这篇论文中，我们首先评估并改进了大型语言模型的约束语言规划能力。由于没有现成的特定目标数据集来支持我们的研究，我们需要首先获取这些目标。如表所示，我们通过使用InstructGPT扩展抽象目标以满足不同的真实生活具体目标的多种约束。我们抽取了100个具体目标，并评估了大型语言模型生成的脚本。该表格报告了结果的整体准确性。我们发现所有大型语言模型在规划具有特定目标时都取得了令人满意的成果。然后我们进行了详细分析，以调查大型语言模型的结果。图中的结果表明，生成脚本的语义完整性是可接受的，但符合约束条件的正确性无法保证。我们将约束条件分为不同的细粒度主题类别，如图所示。热力图显示，InstructGPT在不同类别的目标上表现出相当大的差异。以往的研究已经证明，大型语言模型的输出质量存在高度波动，导致性能不佳。因此，我们提出了超生成器的“筛选器”概念，以提高生成质量。我们首先展示了约束类型和示例，然后使用约束GPT生成具有特定目标的脚本。接下来，我们开发了一个筛选器模型，以选择符合约束条件的脚本。我们将脚本和目标转换为InstructGPT的嵌入表示，并计算余弦相似度和相似度得分，以衡量语义相似性。此外，我们还考虑了包含目标约束关键词的脚本。我们只保留得分最高的脚本。使用这种方法，InstructGPT可以生成高质量的脚本。我们的方法大大提高了规划能力，既在语义完整性方面，又在符合约束条件方面。由于大型语言模型成本昂贵且难以部署，因此对于较小和更专业化的模型至关重要。创建数据集是一个必不可少的步骤。然而，以往的研究并未提供规划具有特定目标的数据集，而手动标注数据集的成本昂贵。因此，我们采用了符号知识蒸馏的概念，从大型语言模型中提取约束语言规划数据集。我们使用我们的方法构建了一个名为CoScript的约束语言规划数据集。总共，我们生成了55,000个具有脚本的具体目标。为了确保验证和测试数据集的质量，我们邀请云服务器上的工人找到并纠正输入不正确的样本。图显示了CoScript中约束分布的直方图。我们发现CoScript具有高度的均匀性，这表明在生成的具体目标中具有较高的概率。使用CoScript，我们可以训练较小但更专业化的模型进行约束语言规划。我们发现T5-Finetune-CoScript可以生成比大多数大型语言模型更高的质量脚本，这表明较小的模型可以在适当的训练数据集上支持较大的模型。总之，我们建立了约束语言规划问题，评估了大型语言模型的约束语言规划能力，并开发了超生成器的“筛选器”方法。我们使用大型语言模型生成高质量脚本数据集CoScript，用于约束语言规划。我们希望CoScript数据集可以成为推进语言规划研究的宝贵资源。谢谢您的时间，请参阅我们论文中的更多细节。</sample>
    <sample id="100">The speaker introduces the concept of multi-hop QA, which involves answering questions that require multiple reasoning steps. Each step typically corresponds to a document in the corpus. The example given is finding the 1988 Christmas comedy film starring Brian Doyle Murray, which requires identifying all movies he starred in and then finding the one released in 1988. This set of documents required to answer the question is called the chain. Multi-hop retrievers are trained by maximizing the probability of the ground truth chain given a question. The speaker explains that existing systems require thousands of examples for good performance, which can be expensive, especially in low-resource domains or those requiring special expertise. Their approach, called PromptRank, uses an unsupervised retrieval method combined with few-shot language model-based reranking. The main steps include retrieving a pool of candidate chains using TF-IDF retrieval and hyperlink traversal, reranking these candidates using a few-shot language model, and constructing a chain prompt to score each chain by the probability of the question given the chain prompt. The speaker also discusses techniques like instruction search, instruction sampling, and temperature scaling to optimize the process. They evaluate their approach on the Hotpot-QA dataset using metrics such as retrieval accuracy, recall at K, and answer recall at K. The results show that PromptRank outperforms fully supervised systems and performs comparably to state-of-the-art multi-hop retrievers. The speaker concludes that language models can be used for few-shot ranking of candidate paths for multi-hop QA, and PromptRank exhibits strong few-shot path retrievable performance compared to fully supervised systems.</sample>
    <sample id="101">PaLM 的流畅度与当前最好的系统相当，但准确性存在差异。</sample>
    <sample id="102">水印方法的重要属性包括：1. 应用于嵌入式服务，2. 水印不会降低提供的嵌入服务的实用性，3. 水印应该足够容易被攻击者移除，4. 水印需要在模型提取过程中可传输到攻击者的服务。</sample>
    <sample id="103">TED 英语演讲已被翻译成 14 种不同的语言。</sample>
    <sample id="104">从一个数据集中抽取的实例数量用于重新注释没有在给定的英文内容中明确说明。</sample>
    <sample id="105">在衡量良性和后门数据集之间的差异时，使用了余弦相似度和L2相似度。此外，还应用了卡方检验，并使用其p值作为第三个度量。</sample>
    <sample id="106">The speaker, Shatania, introduces a paper titled "Quest" that discusses the challenges of retrieving information with implicit set constraints. The paper is a collaborative effort with Pete Mingway, Kenton, and Christina from Google DeepMind. The examples used to illustrate this problem are Jane, a zoologist who wants to identify an unknown reptile species she observed in Costa Rica, and Austin, an avid book reader looking for historical fiction novels set in France. These examples demonstrate how people often express their information needs with multiple constraints or preferences, leading to queries that contain implicit set operations. The paper presents a dataset called Quest, which includes over 3000 entity-seeking queries with implicit set constraints. The answer entities are verified for relevance, and their associated documents are marked with attributable spans for different query constraints. The paper shows that the dataset poses a challenging retrieval problem since systems need to effectively search over a large document corpus to find multi-answer sets where the attribution for different query constraints can come from different parts of the document. To construct Quest, Wikipedia category names from four domains of interest (films, books, plants, and animals) were used to perform set operations to get queries with set constraints. Human annotators then paraphrased and validated these queries to ensure they have the same meaning and are fluent. Finally, annotators verified the relevance of entities in the answer set and marked evidence in the document as its attribution. The paper evaluates systems on their ability to retrieve multi-answer sets from a large document corpus where queries contain implicit set constraints and the evidence for a document's relevance can come from multiple parts of the document. The paper considers sparse and dense retrievers, as well as a TF-IDF-based reranker that takes in the top 100 candidates from the retriever. The results show that there is a large room for improvement on retriever performance based on the recall of the complete answer set, and that queries with set intersection and set difference are particularly challenging and have the lowest F1 scores.</sample>
    <sample id="107">基于编码器的多语言模型，如M5和XLM-R+PDR，在多语言设置下表现出最佳性能。这些模型可以训练在多种语言上，以提高跨语言性能，并在某些情况下显著提升特定语言的性能。</sample>
    <sample id="108">The speaker, Kostya Sina, introduces a new approach to evaluating language models' acceptability judgments. This method, called the Minimal Pair Paradigm, involves showing both acceptable and unacceptable sentences to the model to determine its preference. The current MPP pipeline is limited in evaluating models' acceptance towards longer sentences, which are becoming more common in large language models. To address this, the speaker proposes revisiting the MPP pipeline by testing models on longer sequences.

The approach involves simulating longer sequences by recreating sentences from relevant datasets with acceptable or unacceptable structures. For example, grammatical sentences from the BLM dataset are used as prefixes for both acceptable and unacceptable queries. This allows for testing whether the model's acceptability judgments are context-dependent.

The speaker also discusses the impact of unrelated contexts, such as Wikipedia, on the model's judgments. By using sentences from different subsets or unrelated domains, the study aims to determine if the model's acceptability judgments are influenced by the context.

The results show that MPP judgments are robust for arbitrary contexts but vary significantly when sentences from the same dataset are used. The effect of matching prefixes increases throughout the context window, particularly affecting newer language models with larger context windows. The speaker concludes that language models are sensitive to latent syntactic and semantic features shared across sentences, and the current MPP evaluation may not fully capture the model's abstract knowledge throughout the context window.</sample>
    <sample id="109">The paper introduces a dataset called "Natural Instructions," which consists of natural language instructions and their corresponding inputs and outputs. The data was collected in an automatic manner without any human annotations. The model generates instructions and their corresponding inputs, and then generates the corresponding outputs. The resulting dataset contains 64K examples, and if we consider instruction paraphrases, we have about 240K examples. The generated examples are correct and contain valuable information for instruction tuning. The dataset contains highly creative tasks that differ from classic NLP tasks. The model trained on Natural Instructions outperforms the baseline on all benchmarks.</sample>
    <sample id="111">作者通过假设提供者可以收集一个一般文本语料库并计算单词频率来确定中等频率的单词。</sample>
    <sample id="112">大家好，我叫周恒。今天我要介绍我们团队的论文：《Conll 2003命名实体标签器在2023年是否仍然有效》。让我们开始吧！我们的论文探讨了命名实体识别任务（NER）中的泛化问题。我们观察到，自2003年以来，模型一直在使用Conll 2003数据集开发NER系统，这自然提出了几个问题：这些模型能否泛化到现代数据？当我们开发新的标注器时，需要什么才能实现良好的泛化？如果观察到性能下降，是什么原因导致这些模型的表现不佳？为了研究这些问题，我们构建了Conll Plus Plus数据集。这个数据集是从2020年的路透社新闻中收集并用Conll 2003标注指南标注的。我们随后在Conll 2003上微调了超过20个模型，并在Conll 2003测试集和Conll Plus Plus测试集上评估了它们。最后，我们计算了F1分数的变化来评估每个模型的泛化能力。那么，什么才能实现良好的泛化？通过实验，我们发现泛化的三个主要因素是：1. 模型架构：实验表明，Transformer模型通常在新数据上泛化得更好。2. 模型大小：实验显示，较大的模型通常会导致更好的泛化。3. 微调示例数量：实验还发现，更多的微调示例实际上也提高了泛化能力。接下来，我们来看一下有些模型性能下降的原因。我们提出了两个假设：第一个假设是“自适应过拟合”，即通过不断使用相同的测试集而导致的过拟合，这通常表现为在新测试集上的性能下降；第二个假设是“时间漂移”，即由于训练和测试数据之间的时间间隔增加而导致的性能退化。对于自适应过拟合，我们观察到，从图表右侧的红色最佳拟合线的斜率大于1，这意味着我们在Conll 2003上每取得一个单位的改进，在Conll Plus Plus上转化为超过一个单位的改进，这表明自适应过拟合在这个案例中并不明显。那么，时间漂移呢？为了研究时间漂移，我们进行了实验，即重新训练或继续预训练一些模型以使用更近的数据。实验结果表明，随着训练和测试数据之间的时间间隔增加，性能会下降。这证实了我们关于时间漂移是性能下降的主要原因的假设。我们的结论是，为了实现良好的泛化，我们需要更好的模型架构、更大的模型规模以及更多的微调示例。这些因素必须同时具备，不能单独存在。同时，我们还发现，性能下降主要是由时间漂移引起的，而不是自适应过拟合，尽管Conll 2003已经使用了20多年。回到我们论文的标题问题：Conll 2003标注器在2023年是否仍然有效？答案是：是的，答案是明确的“是”。我们希望这篇论文能引发更多关于如何提高模型泛化能力的研究。最后，请务必查阅我们的论文、数据集，并如果您有任何问题，请随时联系我们。谢谢！</sample>
    <sample id="114">The speaker introduces a study on finding the pillars of strength for multi-task attention in large language models. The study is conducted by Nanyang Technological University in Singapore. The speaker explains that large language models can learn all tasks in one model, which is revolutionary. However, several limitations are identified, such as heavy parameters, long training time, and huge corpus requirements. The speaker focuses on solving the problem of heavy parameters in large language models. The speaker explains that multi-task attention is designed to attend to different subspaces of the input, and each head attends to a unique but different input subspace. The speaker shows that 40% of parameters can be pruned without sacrificing performance. The speaker explains that there have been several threads of work on multi-task attention optimization, including homogenization-based, diversification-based, and pruning those with low scores. The speaker proposes group head attention, which uses a divide-and-conquer strategy to compress multi-task attention. The speaker explains that the first stage of the model is group constraint training, which aims to divide the attention heads into several groups, making intra-group heads become more similar and inter-group heads become more separate. The second stage is voting-to-stay algorithm, which aims to prune redundant multi-task attention and retain only one head for each group. The speaker evaluates the performance of the proposed method on three tasks: machine translation, language modeling, and abstractive summarization. The speaker shows that the proposed method achieves significant parameter compression and comparable performance on these tasks. The speaker also conducts efficiency analysis and finds that the light model achieves 90% fewer parameters, 62% faster inference speed, and 80% fewer flops against the model which yields the same performance on the same dataset. The speaker concludes that task-specific automatic pruning is promising and can prune the redundant large language models without sacrificing performance.</sample>
    <sample id="115">该方法使用的语音片段大小是λ帧。</sample>
    <sample id="116">在 Servin 和 Kea 的示例中，需要特定于实体的知识是 Servin 是一位 judge。</sample>
    <sample id="117">示例质量比与源句子的相似度更为重要。</sample>
    <sample id="118">演讲者介绍了一个名为ACL 2023的提交，该提交旨在通过改进预训练技术来提高代码切换自然语言处理（NLP）的质量。他们首先定义了代码切换的概念，并提供了一个示例，展示了英语和印地语混合的句子。他们指出，在linguistically diverse communities中，代码切换是一个常见现象，特别是在印度等国家。他们强调了为代码切换构建计算模型的重要性，因为多语言预训练模型如BERT和XLM-R在代码切换任务上表现不佳。

演讲者提出了他们的主要贡献：他们提出了新的MLM技术，这些技术针对代码切换进行了调整，并建议了架构修改和辅助损失，以增强模型在代码切换任务上的性能。他们提出了SwitchLM，这是一种专门用于处理代码切换的MLM变体。SwitchLM定义了代码切换点，即两个不同语言的连续标记的组合。在标准MLM中，所有标记都具有相同的概率被遮挡，但在SwitchLM中，只有代码切换点才具有可遮挡性。然而，这种方法需要访问训练数据集中的标签，这在实际应用中可能并不总是可行。因此，他们提出了一个替代方法，称为频率MLM，它通过比较单词在单个语言语料库中的负对数似然度来确定代码切换点。

此外，他们还提出了架构修改，包括在中间层和最终层之间添加残差连接，以及在中间层中编码语言信息的辅助损失。这些修改有助于在最终表示中包含更多的代码切换信息。他们使用 probing classifiers验证了这些修改是否增加了代码切换信息。结果表明，他们的方法在情感分析任务上表现最佳，尤其是在结合了残差连接和辅助损失的情况下。

最后，演讲者总结说，他们提出了一种新的MLM目标，专门针对代码切换信息进行了调整。他们使用 probing classifiers验证了他们的方法是否增加了代码切换信息，并基于这些结果提出了架构修改和辅助损失，以进一步增强代码切换信息内容。</sample>
    <sample id="119">在扩展实验中，论文侧重于GPT-4、GPT系列和BERT系列及其变体。这些模型被进一步预训练在六种不同的 partisan corpora上，以研究它们的政见倾向。</sample>
    <sample id="120">该模型使用特定层的注意力分数。</sample>
    <sample id="121">直接推断的示例包括用户通过歌曲名称或其在播放列表中的位置来指定他们想要选择的歌曲。例如，用户可以说“易上手的那首”或“第一首”。</sample>
    <sample id="122">根据所提供的英文内容，论文的作者来自复旦大学。</sample>
    <sample id="123">The presentation introduces a research on multi-instruction tuning, which aims to improve multi-modal zero-shot learning by fine-tuning pre-trained language models. The speaker, named Yi, and their colleague Jia Yang discuss the advancements in large language models and the exploration of new learning paradigms using pre-trained language models for various downstream tasks. They highlight the effectiveness of instruction tuning in enabling large language models to perform unseen tasks by following natural instructions. However, most previous works focus on improving zero-shot performance on language-only tasks, leaving computer vision and multi-modal tasks behind.

To address this gap, the researchers propose investigating whether instruction tuning can enhance generalization to unseen multi-modal tasks. They note a significant discrepancy in the availability of instruction datasets between NLP and multi-modal domains, with over 1600 language-only instruction tasks available but no large-scale publicly available multi-modal instruction tasks. This motivates them to build a multi-modal instruction tuning dataset.

The Multi-Instruction (MultiInstruct) dataset is introduced as the first multi-modal instruction tuning benchmark dataset, consisting of 62 diverse multi-modal tasks across 10 broad categories. These tasks are derived from 21 existing open-source datasets and each task includes five expert-written instructions. The dataset is designed to unify the processing of various input and output data types, representing the input texts, images, instructions, and bounding boxes in the same token space.

For training, the researchers use 53 tasks from the NLP group, sampling 10,000 instances per task. Testing involves the entire Commonsense Reasoning group, additional five tasks from Wiki and the Miscellaneous group, and randomly sampling 20 tasks from the test split of Natural Instruction as unseen tasks for NLP. They use a pretrained OFA model as the base model and evaluate the model's performance using one of the five instructions for each task, reporting the mean and maximum performance and standard deviation across five experiments. For multi-modal classification tasks, accuracy is reported, while for generation tasks, ROUGE-L is used. Additionally, they introduce a sensitivity metric to measure the model's ability to consistently produce the same outputs for the same task regardless of slight variations in the wording of the instruction.

The main results show that instruction tuning significantly improves the performance of the OFA model on unseen multi-modal tasks. Transfer learning from the Natural Instruction dataset also benefits instruction tuning, leading to better performance and lower sensitivity as the number of tasks increases. Using more instructions instead of just one also improves overall performance and reduces sensitivity. The researchers conclude that transfer learning from the Natural Instruction dataset helps the OFA model achieve much better performance and sensitivity compared to the original OFA model. They also mention that transfer learning from the Natural Instruction dataset can help the OFA model achieve better performance on the Natural Instruction dataset.

Overall, the presentation proposes the first large-scale multi-modal instruction tuning dataset, which significantly improves the zero-shot capability of the OFA model. The researchers explore different transfer learning techniques and demonstrate their benefits, designing a new metric called sensitivity. They are collecting a larger multi-modal instruction tuning dataset with around 150 additional visual language tasks and plan to release it soon.</sample>
    <sample id="124">The speaker, Tan Chi Y from the National University of Singapore and Alibaba, introduces a study on temporal reasoning capabilities of language models (LMs). The study breaks down temporal reasoning into three levels: time-to-time reasoning (e.g., what is a year after 2010), time-to-event reasoning (e.g., what team did Lionel Messi play for in 2010), and event-to-event reasoning (e.g., what team did Lionel Messi play for after FC Barcelona). The study found that prior works overemphasize time-to-time reasoning, while the proposed Temp Reason dataset covers all three levels and long temporal coverage. The study proposes a training strategy with two components: temporal span extraction pre-training and time-sensitive reinforcement learning. The experiment results show that the proposed Temp 5 model significantly outperforms other models in L1 month prediction, L2, and L3 reasoning, and also improves performance in open book QA and Reason QA tasks.</sample>
    <sample id="125">根据所给的英文内容，无法确定这篇论文有多少位作者。内容中没有提到作者的数量或名字。</sample>
    <sample id="126">是的，在语义解析之前，使用机器翻译模型翻译自然语言查询作为基线。</sample>
    <sample id="127">The speaker introduces a joint work between them, Laura Schmidt, and their professor, Seo Young, which involves using large language models as reasoning teachers to transfer their reasoning abilities to much smaller models. They propose a novel technique called diverse reasoning, which involves generating multiple step-by-step solutions for complex tasks using the larger models as training data for the smaller ones. The speaker compares their method with existing baselines on 12 tasks and finds that it can achieve notable performance in many cases, especially for text-based ones from data understanding to coin flip. They also found that students fine-tuned with their method can perform complex reasoning tasks quite well. The speaker concludes by emphasizing the scalability of their method and the trade-offs involved in development costs, inference time costs, and the quality of inference.</sample>
    <sample id="128">The speaker introduces a diagnostic test suite for knowledge integration, specifically designed to evaluate the ability of models to draw on knowledge available in different sources. The test suite includes a coreference resolution task that requires identifying the correct entity referred to by a pronoun in a given sentence. The task is evaluated with human study participants and established coreference resolution models. The speaker also discusses three settings of KIMOS: background pretrain, background both, and background inference. In the background pretrain setting, background knowledge is assumed to be available at pretraining time. In the background both setting, background knowledge is available both at pretraining time and inference time. In the background inference setting, both knowledge types are available only at inference time. The speaker evaluates the dataset with human study participants and establishes coreference resolution models. The results show that even the best-performing models cannot reliably integrate background knowledge presented only at inference time.</sample>
    <sample id="129">根据所给的英文内容，作者给出的“显性群体”(marked group) 的示例包括黑人女性、亚洲女性和拉丁裔女性。这些群体通常被标记为与社会主流或未标记群体不同的群体，如白人男性，在描述时经常使用特定的特征或属性来区分它们。</sample>
    <sample id="130">在实验中发现，Transformer模型架构通常比其他架构泛化能力更好。</sample>
    <sample id="131">根据所提供的英文内容，测试数据集的名称是“干净测试集”。</sample>
    <sample id="132">根据所提供的英文内容，这篇论文有三位作者：马查塔、马丁和作者自己。</sample>
    <sample id="133">作者采用了多种模态。这可以从他们提到的“多种模态任务”和“多种模态预训练模型”中看出，表明他们的研究涉及文本、图像和可能其他数据类型。此外，他们还提到了“多种模态分类任务”和“多种模态生成任务”，进一步支持了他们的工作涉及多种模态的立场。</sample>
    <sample id="135">The video introduces a new method for evaluating conversational AI called ABC Eval, developed by the Emory NLP Lab and Amazon Alexa AI. The traditional approach to evaluating chat models involves human judges selecting which conversation is better or rating conversations on a Likert scale. However, this method can be subjective and may not capture the nuances of chat quality. ABC Eval aims to reduce subjectivity by explicitly annotating whether each model response exhibits certain behaviors such as providing irrelevant information, contradicting oneself, or failing to show empathy. This method measures various thematic errors in chat models, including the number of turns where the model ignores its partner, contradicts itself, or fails to show empathy. The video also compares ABC Eval with three existing methods: Likert ratings on the turn level, Likert ratings on the dialogue level, and pairwise comparisons. The results show that ABC Eval's behavior labels are more reliable and predictive of overall conversation quality compared to existing methods. The video concludes by highlighting the challenges still present in the field, such as common sense violations and contradictions, and the importance of pursuing reliable and precise evaluation metrics for comparing models.</sample>
    <sample id="136">The speaker, Jazan, presents a work conducted with his supervisor Nefisa at the University of Sheffield. The work is titled "Format: An Alternative to Accuracy for Numerical Reasoning." Jazan explains that there are many real-world applications for numerical reasoning and downstream tasks that require factual correctness. He uses an example from Infobase, where one needs to infer whether a statement is true, false, or neutral based on a table. The speaker notes that the performance of language models in numerical reasoning varies depending on their size, with larger models generally performing better.

Jazan introduces Format, a flexible evaluation set based on arithmetic types, which includes number understanding, mathematical operations, and training dependency. Format consists of math worded questions extracted from Illinois Common Core, with numbers represented differently to mimic real-life scenarios. The evaluation also tests the range and breadth of models' capabilities.

The speaker performs a baseline evaluation and finds that most models perform poorly across all aspects. However, fine-tuning with math teachers' templates improves performance. The study also examines training dependency, showing that even when exact expressions are seen during testing, the model's accuracy remains below 50%. The importance of linguistic knowledge is highlighted, and the impact of training templates is investigated.

The speaker concludes that existing benchmarks are unrepresentative, and single scores do not adequately assess numerical reasoning. Format aims to provide an alternative evaluation method. The study suggests that language and mathematical diversity, as well as number encoding and tokenization, are areas for improvement. Jazan encourages readers to read the paper and provides a QR code and links for further information.</sample>
    <sample id="137">The speaker, Sisong from the Singapore University of Technology and Design, introduces a dataset named Tell to Design, which is a dataset for language-guided floor plan generation published in AIC 2023. The dataset consists of 551 human-annotated language instructions collected from crowdsourcing platforms like Amazon Mechanical Turk and around 76,000 artificially generated language instructions from predefined templates. The main challenges of this novel task are to perform design generation under strict constraints compared to artwork, understand the big picture of the entire floor plan from document-level structured text with fuzzy and entangled information, and handle ambiguous, incomplete, or misleading information in human instructions. The speaker proposes a sequence-to-sequence model using a Transformer-based encoder-decoder structure to generate floor plan layouts from language instructions. The model is initialized by a pre-trained language model (T5) for better language understanding capabilities and uses a normal language modeling objective where X is the set of instructions in natural language, Y is the target bounding box sequence, and L is the target sequence length. The results show that the T2D model achieves the highest Iou scores with a micro Iou of 54 and a macro Iou of 53, outperforming other text conditional image generation baselines by a large margin.</sample>
    <sample id="138">根据所提供的英文内容，作者认为NLU中研究不足的领域包括模型在没有任务特定训练的情况下整合和使用来自多个来源的知识的能力。特别是，在背景知识仅在推理时间可用的背景下，模型难以可靠地整合这些信息。</sample>
    <sample id="139">演讲者的名字是Ying。</sample>
    <sample id="140">是的，coscript 经过了质量检查。在验证和测试阶段，我们邀请云托管工人对 coscript 中的不正确样本进行审查和修订。这确保了 coscript 的高质量和准确性，这对于训练较小和专业化的模型以进行约束语言规划至关重要。</sample>
    <sample id="141">现有的资源通常支持有限的上下文依赖翻译类型和语言，因为它们通常依赖于领域知识和人类注释。</sample>
    <sample id="142">嗨，我要谈论我们关于解决间接引用表达用于实体选择的工作，在其中我们引入了Alt Entities Corpus。我的名字是Javahar Hosaini，这是一篇与Philip Radlinski、Sylvia Parati和Annie Lewis合作的论文。我们的目标是理解用户语言，当他们想做出选择时。考虑这个问题：你是说Easy on Me，还是I Got a Feeling？这里用户想在两首歌之间做出选择。最明显的方法是使用直接引用，例如通过说Easy on Me的名字或其位置，第一个。但有时候，间接引用更有利于进行更自然的对话。这可能会发生在用户无法记住歌曲名称或两个实体的 pronunciation 相似的场合，或者用户想指定一个偏好。这里有一些例子的间接引用：更容易的那首，或者那首不那么有活力的歌曲。这是一个在对话系统中以及在评估LLMs实体理解能力方面的很重要问题。我们没有公共数据集，也没有大规模的公共数据集来完成这个任务，所以我们用Crowd Annotation收集了一个。我们的数据集覆盖了三个不同的领域：音乐、书籍和食谱。我们数据集收集方法论强调了随意性，使用卡通完成任务。卡通有三个语音气泡。在第一个气泡里，Bob说：“记得昨天我们听过的那首歌吗？”然后Bob设置对话背景。在第二个语音气泡里，Alice说：“你是说Easy on Me，还是I Got a Feeling？”这是用户的备选问题。在第三个语音气泡里，Bob使用间接引用选择其中一个实体，例如“更容易的那首”。我们提供前两个语音气泡，第三个由标注员填写。第一个语音气泡从每个领域的几个手动提示中选择。第二个语音气泡，即备选问题，按照以下方式生成：我们总是使用一个简单的模板：你是指A还是B？其中A和B是从Wikipedia中抽取的样本。我们使用的不同采样方法如下：当我们向上移动列表时，实体变得越来越相似，因此很难区分它们。第一个方法是均匀随机。第二个方法是实体具有相似的标题，例如两本书名为The Return。第三个方法是实体具有相似的Wikipedia描述。最后，实体具有相似的info boxes或属性，例如相同的作者或艺术家。当我们向标注员展示备选问题时，他们知道这些实体的名称，但不一定了解实体本身。因此，我们向标注员展示了关于这两个实体的一些背景知识。对于歌曲，我们简单地显示Google搜索结果，并要求标注员至少听一点每一首歌并阅读关于它的东西。例如，这是Easy on Me的Google搜索结果。对于食谱和书籍领域，我们展示了来自Wikipedia的一些背景文本。对于食谱，我们还展示了它们的图片，再次来自Wikipedia，以便标注员知道它们看起来是什么样子。然后我们要求标注员选择其中一个实体，例如第一个，并用三到五个间接引用表达来描述它们。例如，那个带有钢琴音乐的。这是我们的数据集中的一些示例：那个没有歌词，不是那个有12岁男孩的12岁男孩，或者那个虚构的，或者来自其他地方的。Alt Entities Corpus包含6000个备选问题，跨越三个领域，以及42000个间接引用表达。使用T5-XL模型的结果如下：如果语言模型有标注员相同的完全相同背景知识，准确率会很高，大约为92%至95%。但这并不现实。如果语言模型有部分重叠的背景知识，准确率在82%至87%之间，这更现实。例如，当语言模型检索到背景知识时，准确率只有60%。所以有很多改进的空间。我们还展示了模型在不同领域上的泛化能力。谢谢。</sample>
    <sample id="143">该方法与现有的 SimulST 策略进行了比较，包括 Whitaker 策略和 Local Alignment。此外，还与专门用于同步语音翻译的 State of the Art 架构进行了比较。</sample>
    <sample id="144">根据所给的英文内容，无法确定论文作者所属机构的确切名称。然而，内容中提到了“INRIS”和“INRA”，这表明这些机构可能与作者有关。INRIS（Institut national de la statistique et des études économiques）和INRA（Institut national de la recherche agronomique）是法国的国家研究机构，经常参与科学和技术研究。因此，可以合理推断论文的作者可能来自法国的一个研究机构，可能是INRIS或INRA，或者与这些机构合作。</sample>
    <sample id="145">演讲者的名字是Jenny。</sample>
    <sample id="146">The speaker, a PhD student from Fudan University named Zhe Yi Chen, introduces a paper on the analysis of omission in dialogue summarization. The paper discusses the challenges and progress made in dialogue summarization, particularly using large-scale pre-trained language models. However, these models often generate summaries with factual errors and omissions, which can lead to incomplete summaries that lack critical information. The paper analyzes the percentage of summaries suffering from omission problems and presents data showing that even state-of-the-art models still have high omission rates, with about 70% of generated summaries having omissions. The paper also explores the distribution of omitted information in dialogues and proposes a task definition for omission detection, focusing on identifying missing content in generated summaries. To address this issue, the paper constructs a dataset with high-quality omission labels for dialogue summarization, using different abstractive models and decoding strategies to generate diverse candidate summaries. The paper evaluates three baseline frameworks for omission detection and finds that the task is challenging, with an F1 score around 50%. Additionally, the paper proposes a post-editing method for summary refinement using detected omissions, which significantly improves summary quality.</sample>
    <sample id="147">根据所提供的英文内容，这篇论文有三位作者。</sample>
    <sample id="148">嗨，我是Sarah Papi，来自都灵大学和Bruno Kessler基金会。我将简要介绍我们团队的注意力机制，作为实时语音翻译的指南。该指南是与Matteo Negri和Marc Turker合作完成的论文。实时语音翻译，或简称ST，是指在实时翻译 spoken 语言为另一种语言的文本的过程，从而实现跨语言交流。当前实时语音翻译模型的主要问题包括：通常需要使用特定架构进行训练，这会引入额外模块以进行优化；复杂的训练程序，例如涉及不同优化目标的训练；以及训练和维护多个模型以达到不同的延迟要求，例如训练一个平均延迟为1秒的模型，另一个为2秒的模型等等。我们的解决方案是利用现有的预训练模型，无需重新训练或采用特定架构。我们只使用一个模型来处理每个延迟要求，并通过注意力机制处理延迟，该机制在音频输入和文本输出之间建立关注点。我们可以通过以下方式实现这一点：如果注意力权重之和低于某个阈值α，则仅发出第一个单词，因为这意味着接收到的信息不够稳定。例如，如果接收到的语音片段包含“我要谈论”，并且我们的模型预测了德语翻译，我们将检查注意力权重。我们将看到第一个两个词指向最早接收到的语音片段，而最后一个词指向最后接收到的语音片段（即λ帧）。这意味着前两个词将被发出，而由于权重之和高于阈值α，最后一个词将被忽略，我们将等待下一个语音片段。如果我们继续接收新的语音片段，我们的模型预测其他三个词，我们将再次检查注意力权重。我们将看到没有词指向最后的λ帧，因此这三个词将被发出。我们将通过以下方式评估我们的结果：我们将绘制实时语音翻译结果的图表，其中蓝色表示翻译质量，平均延迟表示延迟指标，以及考虑模型计算时间的平均负载。我们希望曲线尽可能高，但也要向左移动，与现有策略相比。我们还比较了专门用于实时语音翻译的最新架构。这是我们在德语上测试实时语音翻译策略的结果。我们看到，我们的方法优于所有应用于离线模型的策略，因为它们的曲线向左移动。我们还看到，如果我们考虑实际延迟时间或计算负载时间，我们的方法是最快的。如果您想了解更多信息，请阅读我们的论文，我们还提供了代码和模型的开放源代码，以促进我们工作的可重复性。谢谢您的关注。</sample>
    <sample id="149">是的，数据集公开了。</sample>
    <sample id="150">The presentation introduces a new dataset called "Meeting Q&amp;A," which is an extractive question answering dataset based on questions asked by participants in meetings and the corresponding answer sentences. The dataset contains 7,700 questions, with 30% unanswerable, 40% having multi-span answers, and 48% having multi-speaker answers. Questions are longer, open-ended, and actively feed discussions from others. The data collection process involves public meeting transcripts from the AMI Corpus, question selection based on punctuation, and filtering out very short questions. Answers are annotated by annotators, achieving a high inter-annotator agreement reflected by a Krippendorff's Alpha of 0.73. The dataset is split into train, dev, and test sets, with 30% of questions being unanswerable. The presentation also discusses the length distribution of meeting transcripts, questions, and answers in Meeting Q&amp;A, with questions and answers roughly composed of 12 and 35 words respectively. The presentation concludes by discussing the results in the fine-tuned setting, including the performance gap between fine-tuned models and human performance, the effectiveness of silver data augmentation, and zero-shot performance.</sample>
    <sample id="151">大家好，我的名字是Ying，我和我的同事Jiang一起将展示我们关于Multi-Instinct的研究，即通过指令调优来提高多模零样本学习。随着大型语言模型的 advances，许多研究开始探索使用预训练语言模型进行不同下游任务的新型学习 paradigms。最近的研究表明，指令调优使大型语言模型能够通过遵循自然指令在零样本方式下完成任务。然而，大多数之前的工作都集中在通过指令调优提高语言任务的零样本性能上，而计算机视觉和多模任务则被忽视了。因此，在这项工作中，我们旨在探讨是否可以通过指令调优多模预训练模型来提高多模任务的泛化能力。此外，在我们研究期间，我们发现多模指令数据集的可获得性与单模相比存在显著差距。目前有超过1600个语言任务的指令数据集，但没有大规模公开可用的多模指令数据集。这激励我们构建一个多模指令调优数据集。在这里，我们介绍了Multi-Instinct，这是第一个多模指令调优基准数据集，包含62种不同的多模任务，涵盖10个主要类别。这些任务是从21个现有的开源数据集中派生出来的，每个任务配备了5个专家提供的指令。为了评估多模指令调优，我们使用OF-A作为基础模型。OF-A使用统一的词汇表对语言、图像和坐标进行编码。这里展示了我们多模指令数据集的一些示例实例。为了统一处理各种输入和输出数据类型，我们遵循OF-A的方法，并将所有任务格式化为统一的序列到序列格式，在这种格式中，输入文本、图像、指令和 bounding boxes 都在相同的 token 空间中表示。现在我要谈谈多模指令调优。对于训练数据集，我们使用了NLP组中的53个任务进行训练，每个任务采样了10,000个实例。对于测试，我们保留了Common Sense Reasoning组的所有任务，并从VQA和MCCINless组中选择了额外的5个任务。我们使用测试集中的所有实例对每个任务进行测试。此外，我们随机从NLP组的测试集中的20个任务中选择20个任务作为无偏任务，用于NLP任务。我们使用预训练的OF-A大模型作为基础模型。在训练过程中，我们将每个任务的每个实例随机与5个指令模板之一组合。在测试阶段，对于每个任务，我们进行5次实验，每次实验使用5个指令中的一个来评估模型。我们报告了5次实验中的最小和最大性能以及性能的标准差。如果任务是多模分类任务，我们将报告准确性；如果是多模生成任务，我们将报告ROUGE-L；如果是NLP任务，我们将报告ROUGE-L。我们还引入了一个额外的评估指标叫做敏感度，它衡量模型在指令 wording 的微小变化下是否能一致地产生相同输出。我们的主要结果如下：如图所示，指令调优可以显著提高OF-A在多模任务上的性能。此外，通过从自然指令数据集中学习，指令调优可以带来性能提升。我们可以看到，随着任务数量的增加，模型在保持较低敏感度的同时取得了更好的性能。我们还进行了一个实验，使用一个指令与五个指令进行比较。我们可以看到，使用更多指令可以提高模型的整体性能并降低其敏感度。这展示了不同调优策略对模型敏感度的影响。通过从自然指令数据集中学习，模型可以实现比原始OF-A模型更好的敏感度。我们还可以看到，通过从自然指令数据集中学习，OF-A在多模指令数据集上的性能得到了显著提升。总的来说，我们提出了第一个大规模多模指令调优数据集，显著提高了OF-A的零样本能力，并展示了不同的迁移学习技术的好处。我们设计了一个新的指标叫做敏感度。最后，我们将收集一个更大的多模指令调优数据集，包含大约150个额外的视觉语言任务，并计划发布它们。这是我们的代码和模型的仓库链接。谢谢。</sample>
    <sample id="152">The speaker, Frederic Rymensnijder, introduces his work on the intersection of NLP and classical philology. He presents a new language model designed for classical philology, specifically for ancient Greek and Latin texts. The speaker has pre-trained two monolingual models for ancient Greek, Grecberta and Greater, and two multilingual models, Philberta and Philter, which are trained on ancient Greek, Latin, and English data. The speaker also discusses the challenges of using existing models in multilingual contexts and the need for robust evaluation to understand their capabilities. The speaker concludes by highlighting the strengths of the new models in tasks such as speech tagging, dependency parsing, and lemmatization, and the potential implications of multilinguality in NLP models.</sample>
    <sample id="153">Ninara Mahraabi, a postdoctoral scientist at Amazon Alexa AI's responsible AI team, presented her work on resolving ambiguities in text-to-image generative models. The study aimed to address existing ambiguities in prompts provided to these models, such as "the girl enters the room with flowers," which can have multiple interpretations. The goal was to propose frameworks that mitigate these ambiguities and evaluate the faithfulness of generated images to user intentions.

The framework involves curating a benchmark dataset with various types of ambiguities, using either language models or visual setups to disambiguate prompts. After disambiguation, the prompts are evaluated by comparing the generated images to the user's intended interpretation using a VQA model. The results showed that the framework has a positive effect on faithful generation and aligns well with human evaluations, making it reliable for assessing text-to-image models.</sample>
    <sample id="154">根据所给的英文内容，这篇论文的作者所属机构是都灵大学和布罗尼奥·卡赛尔基金会。</sample>
    <sample id="155">演讲者的名字是Javahar Hosaini。</sample>
    <sample id="157">The speaker introduces a dialogue summarization method using a static-dynamic structure fusion graph. This method aims to extract hidden information from dialogue context into concise summaries, helping people quickly capture the highlights of semi-structured and multi-participant dialogues without reviewing the complex dialogue context. The method involves encoding utterances in the dialogue context into vector representations, constructing a static graph using existing dialogue structure modeling methods, and then proposing a static-dynamic graph module that combines multiple static graphs and uses dynamic graph models to capture semantic relationships between utterances based on their deep vector representation. Finally, a pre-trained language model is employed as the summary generator to fuse the static dialogue structure and the dynamic learned dialogue structure into the final summary.</sample>
    <sample id="158">The speaker introduces a dual cache method for long document neural coreference resolution, which uses a fixed-size cache and reduces complexity to a linear level. The method stores local entities with an LRU eviction policy and global entities with an LFU eviction policy. The model scans the document from left to right, classifies new or updated entities, and evaluates their frequency before adding them to the appropriate cache. The dual cache significantly reduces cache misses compared to a single cache and is more cost-effective than single cache methods.</sample>
    <sample id="159">大家好，我是库斯·辛纳，我很高兴欢迎你们参加我们关于ACL2023论文“语言模型接受性判断不总是对语境敏感”的讨论。这是一篇与John Gold here、Aaron Mueller、Kannika Mishra、Garen Fuentes、Roger Levy和Adina Williams合作的共同作品。在这项工作中，我们重新审视了最小对偶 paradigm。最小对偶 paradigm主要用于评估语言模型在可接受性判断上的表现，这些判断可以包括语法正确性，如BLEU或语法错误率，或者可接受性，如刻板印象。在这个 paradigm中，通常的做法是展示一个可接受的句子或语法正确的句子，然后展示一个不可接受的句子或语法错误的句子。理想情况下，模型应该为可接受的句子赋予更高的概率。当前的MPP pipeline并不能让我们评估模型对更长句子的可接受性。如今，大型语言模型正在出现，具有更长的上下文窗口。因此，至关重要的是我们要评估模型在整个上下文窗口中的可接受性。这就是我们在这里尝试的事情。我们正在努力通过要求模型评估更长的序列来重新设计MPP pipeline。我们首先模拟这些更长的序列，通过重新设计数据集本身来实现。然后，我们通过选择可接受或不可接受的句子来重新创建句子。例如，我们从BLiMP数据集中选择一个典型的语法问题，来自 adjunct island 案例，并创建一个包含相同语法结构的更长句子。我们将这些句子作为前缀添加到可接受和不可接受的查询中。我们还可以通过选择来自同一匹配的句子，或者来自不同子集或不同数据集的句子来测试模型的可接受性。最后，我们可以选择来自完全 unrelated domain，如Wikipedia的句子，以了解模型的可接受性判断是否受任何特定上下文的影响，比如来自不同子集的上下文，或者与当前正在考虑的句子完全无关的上下文。那么模型的表现如何呢？首先，我们观察了维基百科句子，这些句子与当前查询对完全无关。我们发现MPP判断对于任意长度的上下文都是相对稳定的。当我们将上下文长度增加到124时，以最大化OPT和GPT-2模型的性能，我们发现在橙色线中MPP判断相对稳定。接下来，我们选择来自同一数据集的可接受和不可接受的句子，来自BLiMP或语法错误数据集。我们发现，当我们在查询中添加可接受或不可接受的前缀时，MPP判断要么显著增加，要么显著减少。但是，当我们选择与查询相同的语法现象时，即选择BLiMP或语法错误数据集中的句子作为前缀时，MPP判断要么显著增加，要么显著减少，具体取决于所选前缀是可接受还是不可接受。这种效应在整个上下文长度范围内都很大，这可能会影响拥有大上下文窗口的新一代语言模型。那么，为什么匹配前缀会如此影响语言模型的判断呢？我们进行了一系列分析，其中我们尝试通过添加噪声来扭曲输入句子，同时保持相关结构。经过多次这样的扭曲后，我们发现这些噪声并没有使模型发生根本性的变化，即它们没有改变模型的MPP判断趋势。相反，我们发现模型对扭曲句子的敏感度类似，即在可接受领域扭曲句子时，所有扭曲都会导致MPP判断显著增加；而在不可接受领域扭曲句子时，所有扭曲都会导致MPP判断显著减少。因此，我们的主要结论是，语言模型对潜在句法和语义特征的敏感度一致，这些特征在句子之间共享。当前的MPP评估方法，即使用短且单个句子输入，可能无法充分捕捉语言模型在整个上下文窗口中的抽象知识。请阅读我们的论文获取更多实验细节。谢谢各位的聆听。</sample>
    <sample id="160">该方法的第一步将输入词元映射到一个未排序的多集词元，这些词元将在输出中出现。</sample>
    <sample id="161">根据所给的英文内容，Coscript 中包含了55,000个脚本。</sample>
    <sample id="163">根据所给的英文内容，DEplain 的最佳对齐方法是 Matt Align。</sample>
    <sample id="164">弱监督学习的一个好处是它比手动标注数据便宜得多。在弱监督学习中，数据通过弱标签来源进行标注，如简单启发式规则、知识库或低质量 crowdsourcing。这使得标注数据的过程更加高效和成本效益，因为标注数据所需的时间和资源较少。然而，需要注意的是，弱标注数据可能会引入噪声，导致标注错误，这可能会影响模型的性能。</sample>
    <sample id="165">The speaker, Wenting Jiao, a PhD student at Cornell University, introduces the concept of adaptive reasoning and presents a paper titled "Adaptive Common Sense Reasoning: Exploiting Mutual Exclusivity." The paper proposes an unsupervised learning method called LIPOR (Likelihood Learning with Posterior Regularization) to identify plausible explanations without supervision. The method treats explanations as latent variables and maximizes the marginal likelihood of the outcome given the context by marginalizing over all possible explanations. To prefer plausible explanations, LIPOR uses a regularizer that enforces mutual exclusivity among explanations. The speaker compares LIPOR's performance on the Alpha dataset to zero-shot models and the previous best unsupervised approach, demonstrating superior accuracy.</sample>
    <sample id="166">The speaker introduces a new work called "NeuroSymbolic Reasoning Framework for Image Retrieval from Linguistically Complex Texts." This framework addresses the challenge of image retrieval from complex text by integrating a neural network model with a symbolic reasoning system. The framework utilizes a divide-and-conquer strategy and dual process theory to break down the problem into smaller sub-problems, which are then combined to obtain the desired output. The first module is the Proposition Symbol Generator, which generates complex propositions and images based on the input text. The second module is the Neural-Symbolic Reasoner, which integrates the reasoning states and results of the symbol propositions to obtain the final solution. The proposed method outperforms the baseline methods and the results of the ablation experiments on the testing set verify the effectiveness of each module.</sample>
    <sample id="167">DEplain-web 中的文档被分配为手动对齐和自动对齐。具体分配情况如下：在 DEplain-AP，483 份文档被手动对齐，而在 DEplain-WEB 中，750 份文档中有 13,000 句子对被手动对齐，另外 30,450 句子对通过自动对齐方法进行对齐。</sample>
    <sample id="168">CoNLL++数据集是通过从2020年的路透社新闻中收集文本并使用与CoNLL 2003相同的标注指南进行标注而创建的。</sample>
    <sample id="169">The speaker, Avi Lillard, provides a brief overview of the paper "Prompting PAM for Translation: Assessing Strategies and Performance." The paper is a collaborative effort with colleagues from Google Translate. PAM (Parallel Architecture Model) is a 540 billion parameter large language model presented in 2022. It was trained on a vast collection of texts totaling 780 billion tokens. PAM achieved state-of-the-art performance in hundreds of NLP tasks.

The study focuses on systematic prompting strategies for machine translation using PAM. The researchers evaluated the translation capabilities of these models by employing best practices from the NMT community, including using the latest test sets to avoid overlap with training data. They compared two state-of-the-art systems, the best-performing systems on the WMT evaluation, using state-of-the-art NMT metrics and expert-based human evaluation results. Recommendations for prompt selection strategies were also provided.

The study found that prompting has a significant impact on the performance of NLMs for translation. In a simple experiment, using one-shot prompting with two different prompts for each sentence resulted in an average difference of more than one BLEU point, with extreme cases reaching up to 40 points. This highlights the importance of selecting effective prompting strategies.

The researchers concluded that example quality is more important than similarity to the source sentence. High-quality translations from the WMT evaluations or dev data, which are more curated and of higher quality, resulted in better performance. Specialized state-of-the-art systems still have an advantage over PAM translations, but PAM comes close to a commercial system. The fluency of PAM is comparable to state-of-the-art systems, but the main difference lies in accuracy. PAM often omits parts of the source sentence to produce a better-sounding translation, leading to lower style error rates but still some issues with accuracy.</sample>
    <sample id="170">Hello everyone, my name is Yusen Jiang from Peking University. Today, I will present our work: "Exemplar Crosslingual Semantic Parsing in Multiple Natural Languages and Multiple Representations." Semantic parsing is the task of building semantic representations of user queries such as SQL and Lambda Calculus. Crosslingual semantic parsing is the task of translating queries in multiple natural languages into multiple meaning representations. As shown in this figure, we need to translate the query in multiple natural languages using neural models to SQL, Lambda Calculus, or other representations. Existing cross-lingual semantic parsing models are separately proposed and evaluated on datasets of limited tasks and applications. For instance, there are gaps in coverage on certain natural languages, such as Chinese being missing, and gaps in coverage on certain meaning representations, such as Lambda Calculus being missing, or they are only evaluated on certain neural models. For example, there is only one single model to evaluate them. So, to address this issue, we propose Exemplar, which provides a unified dataset for cross-lingual semantic parsing in multiple natural languages and multiple representations. It contains 90 datasets in various domains, 5 semantic parsing tasks, 80 meaning representations, and 22 natural languages in 15 language families. To better evaluate our benchmark, we consider six settings for training and evaluation. The first one is translation test. We use Google Translate API to translate source to target language, then use monolingual model to train and evaluate. For example, we train an English model on English query and during inference, we translate the German query using API to English and then use the trained model to predict the SQL. We also test monolingual model. In this setting, the source language is the same as the target language, for example, German to German or English to English. We also test monolingual few-shot setting by training monolingual models with only 10% of training data. And we test multilingual model, which we train one multilingual model for all languages. For example, we put the German, English, Chinese queries together to train a multilingual model and during inference, we can use this model to translate German queries or Chinese queries, etc. And we also consider cross-lingual zero-shot and few-shot transfer. We train on one source language and transfer to another language. During training, we train on English query or the combination of English and German few-shot queries to train a multilingual model to predict the SQL output. And we also find many interesting results. Regarding analysis of monolingual models, we evaluate on two groups of models including encoder PDR, which stands for multilingual pre-trained encoders with pointer-based decoders such as XLNet + PDR and BERT + PDR, and we also evaluate encoder-decoder models, which is multilingual pre-trained encoder-decoder models such as M-BART and MT5. We found that encoder-decoder obtains the best performance on all nine datasets. And we evaluate on MT5 and XLM-R + PDR on multilingual setting. We found that encoder-decoder or encoder PDR can be improved by training in a mixture of various languages. And we found it is because most of the major natural languages can obtain performance gain except that English performance drops in seven datasets and only gains in three datasets. I think this is known as curse of multilinguality. We also compare the cross-lingual performance gap. In this figure, the blue line is cross-lingual few-shot transfer, the orange line is cross-lingual zero-shot transfer, while the green line is the monolingual setting. We found that by comparing the green and orange line, we found that for zero-shot setting, the cross-lingual transfer performance gap is significant. And by comparing blue and orange line, we found that in few-shot setting, the transfer gap is shortened rapidly. We also find some other interesting findings. For example, encoder-decoder outperforms previous work or achieved comparable results for training on English natural language and significantly boosts the performance of few-shot on target natural languages. And we found multilingual language models such as Code and Blue are still inadequate for cross-lingual semantic parsing tasks. To sum up, we build Exemplar, a unified benchmark for cross-lingual semantic parsing with multiple natural languages and multiple representations. We conduct a comprehensive benchmark study on three representative types of multilingual language models, and our results show many interesting findings and etc. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="171">关于保护嵌入式服务版权的现有研究可以被广泛地分为四个类别。然而，这些方法要么不适用于嵌入式服务，要么缺乏可转移性。因此，在这篇论文中提出了一个名为“嵌入式标记”的新方法，它是一种后门水印方法，适用于嵌入式服务。</sample>
    <sample id="172">根据所提供的英文内容，Codex或Bloom等多语言LLM对于CLSP来说并不足够。在演讲中提到，这些模型在跨语言零样本和少样本任务中仍然存在不足之处。</sample>
    <sample id="174">The speaker introduces the ArgAnalysis 35K dataset, which is a large-scale dataset for argument quality analysis. The dataset is unique because it contains 35,000 argument analysis pairs, with 85% sourced from high-quality sources such as speeches from high-quality tournaments, expert debaters, and intermediate debaters, and 15% from novice debaters. The dataset also has a diverse range of arguments, covering 24 themes based on experience in the circuit, websites like hellootions.com, and expert advice. The dataset includes an element of analysis, which combines claims, premises, and other elements to explain the argument better. The speaker also introduces the concept of instance-based annotator reliability, which captures the biases of annotators on an instance-by-instance basis, allowing for more reliable scoring. Finally, the speaker introduces a relevance model that assigns scores to arguments based on their relevance to specific themes, capturing how relevant each argument is to a particular theme.</sample>
    <sample id="175">该方法通过在训练过程中引入排列的不确定性来处理排列的不确定性。具体而言，它使用一种灵活的方法来预测排列，而不对可能的排列施加任何硬约束。这种方法使模型能够生成各种排列，从而增强了其表达能力。此外，通过使用GPU友好的连续放松和反向传播技术，该方法还能够学习更符合语言学上更可接受的排列。</sample>
    <sample id="176">根据所给的英文内容，NLP 模型的公平性可以通过评估模型在检测 hate speech 和 fake news 时的表现来定义，特别是针对不同政治倾向的新闻媒体。通过分析模型在检测 hate speech 和 fake news 时的表现，可以识别出模型是否在处理不同政治观点的新闻时存在偏见。例如，如果一个模型在检测 hate speech 时对某些群体更敏感，而对其他群体则不那么敏感，这可能表明存在公平性问题。</sample>
    <sample id="177">演讲者的名字是Yannick Slavac。</sample>
    <sample id="178">演讲者的名字是Kostya Sina。</sample>
    <sample id="179">The speaker, Melina Sclair, introduces the topic of "mind in language models" and explains that it involves reasoning about the mental states of others. She discusses how this ability is traditionally measured in humans and language models through reading comprehension tasks involving multiple characters. The speaker provides an example of a theory of mind test called the Sally Ann test, where Alice and Bob are in a room with a basket and a box. Alice puts an apple in the basket and leaves the room, while Bob moves the apple to the box. The speaker then explains that questions can be classified as first-order or second-order, depending on whose mental state is being asked about. First-order questions ask about a character's mental state, while second-order questions ask about a character's estimation of another character's mental state. The speaker mentions that large language models still perform poorly on false belief tasks, such as ChatGPT and GPT-3. The research question is how to improve theory of mind reasoning skills in large language models. The speaker presents a symbolic Tom and inference time method to improve theory of mind reasoning skills using explicit graphical representations. The method uses several graphical representations since mental states cannot be represented with a single graph. The speaker explains the process of computing these graphs for all combinations of characters up to a predefined maximum theory of mind level N. The graphs are computed using an inference time algorithm that leverages off-the-shelf and open AI models. The speaker then describes how to answer any given question by detecting the entities in the question, retrieving the appropriate belief graph, performing recursion over the question, retrieving the sentences captured by the graph, and fitting the sentences plus the factual question into a language model to get the final answer. The speaker concludes by discussing experiments conducted with various LLMs and comparing them against supervised baselines. The results show significant performance gains across the board, with examples of 65 accuracy points gained for GPT-3 Davinci, 67 for M-A-C-A, and 51 for FLAN-T5 X-E-X-L. The speaker also tests the method's generalization capabilities with two new datasets designed to test story structure generalization and linguistic generalization. The results show that supervised models heavily degrade performance on the created datasets, while using symbolic Tom still shows significant gains for all models, allowing stronger models like GPT-4 to fully solve the datasets. The speaker concludes by introducing Symbolic Tom, a plug-and-play method to improve theory of mind reasoning skills in large language models, which is an inference time algorithm that avoids overfitting risk and uses explicit graphical symbolic representations. Symbolic Tom dramatically improves out-of-box LLM performance, outperforming supervised approaches on out-of-domain story understanding and remaining beneficial on the new linguistic diversity dataset paraphrase Tommy.</sample>
    <sample id="180">演讲者的名字是Mayra。</sample>
    <sample id="181">The paper introduces the concept of constrained language planning, which involves generating scripts that are both semantically complete and faithful to specific constraints. The authors evaluate and improve the constrained language planning ability of large language models by extending abstract goals with multi-faceted constraints and using a dataset of 55,000 specific goals with scripts. They propose an over-generated filter method to select feasible scripts based on semantic similarity and target constraint scores. The results show that the constrained language planning problem is challenging for large language models, but smaller and specialized models can be trained on the generated dataset to achieve high-quality script generation.</sample>
    <sample id="182">热带主义 (tropicalism) 指的是将女性视为异国情调或异国情调的叙事，通常与热带地区相关联。在本文中，它被用来描述拉丁裔女性被描述为“充满活力”和“风趣”，这些描述反映了对女性的刻板印象，将她们视为异国情调和异国情调，而不是作为独立个体。</sample>
    <sample id="183">作者通过使用自然语言提示来创建目标群体的人工描写，这些提示是用特定的身份标记来描述的。例如，他们可能会输入“想象一下你是一个亚洲女性，描述你自己。”这种方法允许模型生成适用于任何人口统计特征的描述，而不仅仅是特定的群体。</sample>
    <sample id="184">在本文中，语境使用情况是通过测量给定源文本时，语境C提供的关于目标Y的信息量来衡量的。这个度量被称为CXMI，可以衡量在句子级别或单词级别的语境使用情况。</sample>
    <sample id="185">DrBERT 和 ChuBERT 的主要区别在于它们的训练数据和模型架构。DrBERT 是一个通用的预训练模型，使用了大量文本数据（NatChos）进行训练，而 ChuBERT 则是一个专门针对生物医学领域的临床模型，使用了临床数据集（ClinicalNuts）进行训练。此外，ChuBERT 还使用了预训练模型（如 Camembert）的权重和分词器，这可能有助于提高其在特定任务上的性能，但可能不如从头开始预训练的模型那样灵活。</sample>
    <sample id="187">根据所提供的英文内容，这篇论文有两位作者。</sample>
    <sample id="188">迭代迁移学习是一种机器学习策略，其中模型在每个标注轮次中被训练以收集更多数据。在这种情况下，它涉及使用从之前标注的对话单元对齐中学习到的知识来标注新的对话单元，从而提高模型检测认知失真（不一致）的能力。</sample>
    <sample id="189">数据集的目标是收集一个大规模的公共数据集，用于任务，因为目前没有可用的公共数据集。</sample>
    <sample id="190">根据所提供的英文内容，攻击者可以通过学习 EaaS 的嵌入来提取模型参数。通过分析嵌入，攻击者可以学习模型的内部结构和行为，从而能够复制或利用这些模型提供的服务。</sample>
    <sample id="191">根据所给的英文内容，这篇论文有三位作者：Sarah Papi、Bruno Caseler和Matteo Negri。</sample>
    <sample id="192">The presentation introduces a new optimizer called "Can," which aims to achieve fast convergence and low memory usage simultaneously. The presenter, Yang Liu, explains that traditional adaptive gradient-based optimization methods like Adam often require excessive memory for maintaining first and second moment estimates of per parameter gradients. To address this, memory-efficient optimizers like AdaFactor have been proposed, but they come with performance penalties. Can is designed to balance these two goals by using non-negative matrix factorization (NMF) to reduce memory requirements while maintaining stability through a confidence-guided adaptive updating mechanism. The presenter demonstrates the effectiveness of Can on training tasks of three large language models (GPT-2, T5, and BERT) using the Book Corpus and English Wikipedia datasets. The results show that Can achieves significant improvements in validation accuracy and better performance than Adam and AdaFactor, especially when training larger models. Additionally, Can reduces memory usage compared to existing memory-efficient optimizers, making it more efficient for training large language models.</sample>
    <sample id="193">根据所提供的英文内容，用于创建初始数据集的注释者数量没有直接说明。然而，提到“收集了大约1000个 discourse unit pairs 的注释”，这表明至少有1000个 discourse unit pairs 被注释。由于每个 discourse unit pair 包含两个注释，因此至少需要2000个注释者来完成这一任务。</sample>
    <sample id="194">根据所提供的英文内容，这篇论文的作者属于卡内基梅隆大学。</sample>
    <sample id="195">The speaker introduces a new method for answering complex questions, called "Reasoning over Hierarchical Question Decomposition Trees" (RQDT). This method involves breaking down complex questions into sub-questions and using knowledge sources to answer them. The speaker explains that previous methods have limitations, such as incomplete knowledge bases and difficulty integrating knowledge from heterogeneous sources. To address these limitations, the speaker proposes a two-stage framework: first, building an RQDT by decomposing the original question into sub-questions, and second, performing probabilistic reasoning over the RQDT to integrate knowledge from various sources. The speaker evaluates the proposed framework on two challenging Q&amp;A datasets, demonstrating its effectiveness in improving performance compared to previous methods.</sample>
    <sample id="196">以左侧为支配词的示例是“Lisa bought and Megan is such that the first conjunct is the head of the whole coordinate structure, as in this case Lisa.”</sample>
    <sample id="197">根据所提供的英文内容，对话系统中的最先进模型是ABC-Eval。该方法旨在通过明确标注每个模型响应的行为来减少人类评估的主观性，从而更准确地评估对话质量。</sample>
    <sample id="198">我们需要在整个上下文窗口中评估模型的可接受性，因为大型语言模型正在出现，具有更长的上下文窗口。通过评估模型对更长句子的可接受性，我们可以更好地了解模型在处理更复杂和更长的文本时的表现，并确保模型能够准确地理解和生成自然语言。</sample>
    <sample id="199">根据所给的英文内容，多语言训练会导致表现下降。在多语言训练设置中，与单语英语模型相比，性能下降。</sample>
    <sample id="200">是的，注释者提前知道实体。在数据集中，注释者被要求从两个实体中选择一个，并用三个到五个间接引用表达来描述他们选择的实体。注释者还被提供有关实体的背景信息，如Google搜索结果、Wikipedia上的文本或图片，以帮助他们做出选择。</sample>
    <sample id="201">根据所给的英文内容，评估使用了BLEU和WER（单词错误率）等MT指标。</sample>
    <sample id="202">根据所给的英文内容，泛化中的回归不会影响特定的 NER 类型。</sample>
    <sample id="203">NLP 中的立场很重要，因为它会影响技术的设计和性能。立场指的是人们持有的观点，这些观点源于他们的人口统计、身份和生活经历。这些观点可以影响研究过程及其结果，因为它们会影响研究人员做出的决策。例如，某些模型可能在检测特定群体的有毒内容时表现不佳，这反映了开发模型的人的立场。因此，了解和解决 NLP 中的立场对于确保技术对所有用户公平和有效至关重要。</sample>
    <sample id="204">根据所提供的英文内容，像 BLOOM 这样的多语言大规模语言模型（LLM）是采用完整微调。这可以从以下句子中看出：“我们还发现，像 BLOOM 这样的多语言大规模语言模型在跨语言微调任务中仍然不够充分。” 这表明 BLOOM 没有使用适配器微调，而是采用了完整的微调方法。</sample>
    <sample id="205">The speaker, Changbing, a PhD student at the University of Washington, is presenting their work on tracking political biases in language models. They explain that language models are trained on large-scale web crawls, which include political news media from sources like The New York Times, Los Angeles Times, The Guardian, and Huffington Post. This diverse training data allows models to learn from various perspectives but also introduces inherent social biases.

To address these biases, the researchers propose investigating the political bias propagation pipeline from pre-training data to language models and downstream tasks. They evaluate the political leanings of language models using political questionnaires and conduct controlled experiments by pre-training models on partisan corpora. Results show that language models can occupy all four quadrants of the political compass, with GPT-4 being the most liberal and GPT series generally more socially liberal than BERT series.

The study also examines how language models pick up political biases from training data and whether they reflect societal polarization. For instance, further fine-tuning Roberta on the left-leaning Reddit corpus results in a substantial liberal shift in its political biases. Additionally, language models tend to adopt the polarization prevalent in society, with models generally having a political leaning further away from the center after 2017.

The researchers evaluate language models' performance on hate speech detection and fake news detection, showing that models with different political leanings have varying effectiveness in detecting hate speech targeting different groups and misinformation from opposing political viewpoints. This highlights fairness issues in NLP applications, particularly in marginalized communities.

The presentation concludes by acknowledging the dilemma of sanitizing political opinions in language model training data to avoid bias while risking censorship or exclusion. The speaker emphasizes the need to acknowledge and tackle the fairness issues resulting from language model political leanings, highlighting the ethical challenges in developing fair and unbiased NLP systems.</sample>
    <sample id="206">根据所给的英文内容，他们使用了零样本分类任务和二元分类任务（扩展和比较类别）作为迁移学习模型。</sample>
    <sample id="207">根据所给的英文内容，最近用于评估 PaLM 能力的测试集包括最新的测试集，以避免与训练数据重叠。</sample>
    <sample id="208">根据所提供的英文内容，作者提出了三条建议。</sample>
    <sample id="209">根据所给的英文内容，与最强的基线相比，建议的方法获得了显著的收益。这可以通过图中显示的性能差异来证明，表明在不同约束类别上规划性能存在显著差异。此外，通过使用符号知识蒸馏技术生成数据集，该方法显著提高了语言规划任务的质量和效率。</sample>
    <sample id="210">演讲者的名字是周恒。</sample>
    <sample id="211">是的，论文中的结果和数据集可以作为基准。作者在论文中提出了一个用于自动文本简化任务的基准，即使用他们的数据集进行微调的模型能够产生比基线更好的得分。这些结果和数据集旨在为未来研究提供参考点。</sample>
    <sample id="212">根据所给的英文内容，他们在论文中进行了55000个较小模型的实验。</sample>
    <sample id="213">根据所给的英文内容，研究多模型指令调整的基础模型是OFA。</sample>
    <sample id="215">The speaker, Adam Sipruckowsky, introduces a talk on the dependency structure of coordination in linguistics. He explains that different theories and approaches assume various dependency structures for coordinating conjuncts. For example, in universal dependencies, the first conjunct is often the head of the coordinate structure, as seen in "Lisa bought and Meggy." Similarly, Igor Miltruk's meaning text theory also assumes the first conjunct as the head. However, there are symmetric approaches like the Prague approach, where the conjunction itself is the head, and multi-headed approaches used in De Catison's word grammar, where all conjuncts are heads.

The speaker then presents an argument based on the principle of dependency length minimization, which suggests that shorter dependencies are preferred. This principle can explain why direct objects are closer to the verb, making sentences like "March read it yesterday" better than "March read yesterday it," despite violating the grammatical principle of direct objects being next to the verb.

Using statistics from the enhanced version of the Penn Treebank, the speaker shows that left conjuncts tend to be shorter, especially when the difference in length between the two conjuncts increases. This tendency is observed only when the governor is absent on the left side of the coordinate structure. The speaker argues that this observation supports the preference for symmetric structures of coordination over asymmetric ones, as seen in the examples provided.</sample>
    <sample id="217">The speaker introduces a work titled "Seeing to Unseen: Exploring Compositional Generation of Multi-Attributed Controllable Dialogue" by Wei Hou, Zhen, and colleagues from Beijing University of Post and Telecommunications. The presentation covers seven aspects of their work, starting with motivations and previous methods for generating controllable dialogue. They discuss the limitations of existing methods in handling multi-attributed generation and propose a disentangled controllable generation (DCG) method that learns attribute concepts from single values and uses a disentangle loss to disentangle different attribute combinations. A unified reference-free evaluation framework (MAE) is introduced to evaluate the effectiveness of their method. The model is based on the dialogue GPT framework with a compositional prompt module, using two types of prompts: attribute-oriented prompts and task-oriented prompts. The results show that DCG outperforms other baselines in controllability and test quality, demonstrating its effectiveness in transforming seen attributes to unseen combinations.</sample>
    <sample id="218">根据所提供的英文内容，无法确定论文作者所属的机构。内容中没有提到任何与作者相关的机构名称。</sample>
    <sample id="219">The speaker, Jia Hu, is a research assistant at Academia Sinica and presents their work on comparing and contrasting multi-stage pipelines for uncovering financial signals in financial reports. The work is done with Liang Huang, Chen Weiying, and their supervisors, Professor Jie Li and Zhang Luan. The background of financial report analysis is discussed, which is the foundation of this work. The text definition and approaches are also explained. The work considers the Form 10-K as the target corpus, which is an annual report required by the SEC containing many details of companies' important activities. However, mining useful information requires lots of human efforts. This work was motivated by two observations: first, the words in a company's report are very similar, about 80% of tokens are the same, and the contents are largely dependent; second, the text similarity between two reports in continuous years is illustrated. For example, the report in 2018 is similar to the one in 2017. Based on these observations, they introduce a highlighting task and a multi-stage pipeline. They first define the reference to target structures in their task, where the target and reference refer to the report of interest and the report at its previous year. So basically, a highlighting model should compare and contrast the context between targets and references like this figure. So the goal of this highlighting task is to find the proportionality roots between a given pair T and R. Formerly, the model will predict the highlight word importance, and therefore we can measure the performance of highlighting. For example, the word "decrease" is supposed to have higher importance in this context. This is our proposed pipeline. Stage zero is document segmentation, stage one is the relation classification, stage two and stage two plus are our own pre-trained and finetuned. Due to the time limit, I would not talk about stage zero more details can be found in our paper. For the stage one, we will classify all the pairs into three types. Type B refers to the pairs have higher syntactic and semantic similarities. These pairs are frequently appeared such as company's regulations. Reverse pair have similar syntactical pattern but in fact the two segments disclose very different meaning. Mismatch pair are more like a debut information or company's new operations. For the model finetuning stage, we first use an external dataset, ES NL, for out-of-domain fine-tuning. It is a natural language inference dataset with token annotation. For example, the word "framing" is the rationale according to the context of this pair. For in-domain fine-tuning, we use the reverse pairs, the reverse words as pseudo positive labels, and we randomly label few other words as negative. In addition, we mix different objectives. We use the soft labeling techniques by mixing cross entropy loss and KL divergence. Therefore, we can alleviate the problem from low quality pseudo labels. The validation dataset include the ES NL pairs and our released final dataset. We use two metrics to judge the performance. Precision indicates the precision over recall. PCC means the correlation between prediction and annotations. This table shows that our domain-agnostic highlighting model achieve the best performance on final, and even preserve the generalization capability as you can see the performance on ES NL. We further observe that our methods can benefit on relation, the mismatch pairs, which we didn't use during training. In conclusion, we propose a highlighting task with our released final dataset and a simple pipeline with two-stage finetuning. There are many other future works we would like to try, including improving effectiveness or adding more features or like many other techniques in information retrieval can enhance the application as well. Yeah, that's it. So please refer to our paper and github for more details and feel free to ask us if you have any question. Thank you.</sample>
    <sample id="220">根据所提供的英文内容，论文的作者是来自圣安德鲁斯大学计算机科学系的候选人。</sample>
    <sample id="221">根据所给的英文内容，论文分析了德语和英语之间的翻译。</sample>
    <sample id="222">The title of this work is "To adapt or to annotate: challenges and interventions in open-domain question answering." The motivation for this work is to address the challenge of adapting models trained on general-purpose domains like Wikipedia to answer domain-specific questions. In an open-domain question answering (QA) setting, a retriever model is used to find relevant passages from a document corpus, such as Wikipedia, and a reader model takes the question and all the relevant passages as input to generate the answer.

The work explores different data interventions that can help enable out-of-domain generalization in open-domain QA. Two overarching methods are considered: few-shot and zero-shot. Few-shot methods involve using a few examples from the target domain to prompt large language models to generate more examples, while zero-shot techniques do not have access to any examples from the target domain and require controlling interactions among three random variables in the QA process.

The work also identifies the type of data set shift a new domain exhibits and determines which data interventions are effective for specific types of shifts. The authors use a variety of data interventions, including changing the format, answer distribution, and context distribution, to assess their impact on model performance.

The compatibility of the source model with the target domain is measured by computing the likelihood assigned to the correct context by the retriever model and the likelihood of all answers for each question by the reader model. This measure helps estimate the type of data set shift exhibited by the target domain.

The results show that few-shot adaptations are effective for most target sets, while zero-shot adaptations are effective for datasets with concept and covariate shifts. In cases where there is no shift, the source model already understands the target domain well, resulting in minimal changes in performance.

Overall, the work demonstrates that data interventions can significantly improve reader performance in open-domain QA, with some interventions leading to up to 24% improvement. The findings highlight the importance of considering the type of data set shift when selecting data interventions for improving the generalizability of QA models.</sample>
    <sample id="223">演讲者的名字是张冰。</sample>
    <sample id="224">在实验过程中研究了两种模型：一种用于产生文档级别的简化文本的模型，另一种用于产生句子级别的简化文本的模型。这些模型是基于Longformer模型进行微调的。</sample>
    <sample id="225">在 MultiInstruct 中，用于训练的 62 个不同任务中，53 个任务用于训练，19 个任务用于测试。</sample>
    <sample id="226">根据所提供的英文内容，这篇论文有两位作者。</sample>
    <sample id="227">The speaker discusses the current state of language models and their limitations in grounding language understanding, which involves mapping natural language expressions into executable plans or programs. The main challenge is the lack of grounding during pre-training, as most language models are pre-trained with text corpora without grounding. This gap between pre-training and downstream applications makes grounding language understanding particularly challenging. Existing research typically uses language models to directly generate plans, but this can result in invalid or grammatically incorrect plans. The proposed framework focuses on discrimination instead of generation, where a symbolic agent interacts with the environment and proposes candidate plans, while the language model only scores and ranks these candidates. This approach allows language models to excel at discrimination tasks. The framework is demonstrated on a representative test bed, showing strong performance across different language models and training settings. The speaker concludes that discrimination might be a better strategy for using language models for grounding language understanding, and invites further discussion and collaboration.</sample>
    <sample id="228">作者在实验中使用了四个数据集：AGNews、BLOG、SST2和Amazon。</sample>
    <sample id="229">The speaker, Gabriella Katalin Szekelya, introduces a joint work with Henning Bach-smud on detecting improvable claims for argumentative writing support. She begins by explaining the importance of text revision in professional writing and how it is a recursive process until optimal phrasing is achieved. She emphasizes that finding the right words and expressions is crucial in argumentative writing as it directly influences the effect the text has on the audience. The speaker then provides an example of revising an argumentative claim from "Cell phones cause brain cancer" to "Cell phone radiation may cause brain cancer," highlighting the need for clarification and precision in argumentative writing.

The speaker then introduces two new tasks: sub-optimal claim detection and claim improvement suggestion. She suggests learning directly from human patterns of revisions instead of explicitly defining what makes a claim good or bad. The paper focuses on modeling the quality of argumentative texts based on implicit revision patterns found in collaborative online debate platforms such as Kiyo. The speaker explains that while working with revision-based data provides many opportunities, it also comes with several challenges at different stages of the experiment design process.

The main challenges are representativeness and reliability, model complexity and architecture, contextual information, and topical and user bias. The speaker invites readers to read the paper for detailed analysis of strategies tackling each challenge and a systematic comparison of approaches for the introduced tasks. Based on experiments, the speaker concludes that revision-based data can be effectively employed for the given tasks, and modeling the distance between two claim versions is beneficial for detecting sub-optimal claims. The impact of contextual information is dependent on both the task and the quality issues the text is suffering from.</sample>
    <sample id="231">NACHOS 是一个数据集，用于医疗领域的自然语言处理任务。</sample>
    <sample id="232">演讲者的名字是Ibilard。</sample>
    <sample id="233">The speaker introduces a new strategy for simultaneous speech translation (SST) using pre-trained offline models. The proposed method, called "ADT" or Encoder-Decoder Attention, uses a cross-attention mechanism to decide whether to emit partial translations based on the concentration of attention weights. This approach avoids retraining specific architectures and reduces latency by handling it through specific parameters. The results show that ADT outperforms other strategies in terms of translation quality and latency, while also being the fastest.</sample>
    <sample id="234">提示策略对结果有重大影响，尤其是在一到五轮提示中。在实验中观察到，选择高质量的示例进行提示可以显著提高翻译质量，而示例的质量比与源句子的相似性更为重要。</sample>
    <sample id="235">根据所提供的英文内容，无法确定论文作者所属的机构。内容中没有提到任何机构名称或细节。</sample>
    <sample id="236">每个任务都配备了五个由专家编写的指令。</sample>
    <sample id="237">作者建议通过引入一个核心引用解决任务来测试模型，该任务旨在评估模型从不同来源获取知识的能力。这个任务涉及识别给定文本中代词所指的具体实体，需要结合实体特定和背景知识。</sample>
    <sample id="238">The speaker introduces a new benchmark dataset called "MeetingBank" and explains that it is designed to address the challenges of summarization in meetings. The dataset includes meeting transcripts, reference summaries, and URLs containing various resources. It contains 1366 city council meetings and nearly 7000 instances, providing detailed statistics such as meeting duration, number of tokens per meeting, number of speakers per meeting, and year period of meeting collection. The dataset also includes summary instances for each city and average statistics for both meeting level and segment level. The speaker discusses the process of data collection, including converting audio data to transcripts, identifying meeting types, and aligning time stamps to get segment transcripts. The dataset is used for evaluating top-tier summarization systems, including extractive and abstractive summarizers. The speaker highlights the performance of different models, such as GPT-3, and discusses the evaluation metrics used, including ROUGE, METEOR, and question-answer-based metrics. The speaker concludes by emphasizing the importance of developing new methods of automatic evaluation metrics to better align with human preferences and encourages the audience to use and explore the dataset further.</sample>
    <sample id="239">Hello everyone, my name is Ibilard, and I will give you a short overview of the paper "Prompting PAM for Translation: Assessing Strategies and Performance." This is joint work with my colleagues from Google Translate. PAM is a 540 billion parameter large language model presented last year in 2022. It's trained on a large collection of texts comprising 780 billion tokens. At the time of publication, it achieved state-of-the-art results in hundreds of NLP tasks. In this work, we present the first systematic study of large language model prompting for machine translation. We evaluate the translation capability of such models using the best practices of the NMT community. This involves using the latest test sets to avoid another lab of the test data with the training data of the language model. And we compare two state-of-the-art systems, so the best-performing systems on the WMT evaluation. We use state-of-the-art NMT metrics and additionally also show expert-based human evaluation results. Finally, we provide some recommendations for prompt selection strategies. The prompting has a big influence on the performance of the LLMs for translation, as we can see in a simple experiment where we use one-shot prompting and provide two different prompts for each sentence. The majority of sentences, 516 out of 1,000, the difference observed is of more than one BLEU point, and this can go in extreme cases up to 40 BLEU points. So it's important to select a good prompting strategy. In our experiments, we conclude for a five-shot prompting strategy, where we just mark each sentence that we provide to the system with the language it's in. So in this example here, where we perform translation from German into English, the German sentences, the source sentences are marked with German colon and the English translations with English colon. We saw that the actual form of the prompting doesn't have a big influence in the case of several-shot prompting. It's crucial for zero and one-shot prompting, and when we go, as in our case to five-shot prompting, there is nearly no difference to the actual form of the of the prompting. It's the examples that carry most of the of the weight. The summary of our experimental results is that the example quality is more important than the similarity to the source sentence. So it's important to select the examples from high-quality translations. In particular, we compare the selecting prompts from the training data of the WMT evaluations or the dev data. The dev data is much more curated and with higher quality than the train data, that it's more nice, and the results, so better performance when using the dev data. Nonetheless, specialized state-of-the-art systems have a substantial advantage over the PAM translations. But PAM comes pretty close to a commercial system. In our case, we chose to evaluate with Google Translate. The insights that we gain from the human evaluation that we perform using the NMT framework, is that the fluency of PAM is comparable to state-of-the-art of the art systems, but the main difference comes from the accuracy. So in particular, the most common error are omission errors. So it seems that PAM chooses them to produce a better sounding translation sometimes by dropping parts of the source sentence that are omitted in the translation. However, the style awkward category for PAM is lower than for the state-of-the-art systems, which is an additional signal that PAM provides really fluent output but still with some problems of accuracy. And that's it for this really short overview. For more details, please come to the full presentation of the paper. Thank you very much.</sample>
    <sample id="240">你好，我是Dawei，一名在德国索尔兰大学读计算机科学专业的学生。在这段视频中，我将向大家展示我们最近完成的一个项目：《弱于你想象的：对弱监督学习的批判性分析》。这个项目是与Yiulyushan、Miao Smoothbath、Gustav Stephen和Dietrich Klauke合作完成的。首先，让我们简要介绍一下弱监督学习和弱监督学习。在弱监督学习中，我们不手动标注数据，而是使用弱标注源，如简单的启发式规则、知识库或低质量的 crowdsourcing。与人工标注相比，弱标注更便宜，但它们也存在噪声，意味着标注中存在错误。如果我们直接用弱标注数据训练神经网络，训练出的模型倾向于 memorize 噪音，而不是泛化。在弱监督学习中，提出了训练算法来 robustly 训练神经网络以处理标注噪音，从而确保训练模型仍然泛化良好。最近关于弱监督学习（WSL）的研究表明，人们认为在弱标注数据上训练模型可以达到高性能。然而，这个说法并不完全正确，因为这假设了有一个额外的干净验证集用于模型选择。我们针对这个问题进行了研究，发现这个假设经常被忽视。因此，我们提出了三个研究问题：1. 清洁验证数据是否对于 WSL 是必要的？或者我们可以使用一个 noisy 的验证集吗？2. 如果需要清洁数据，或者清洁数据对于 WSL 是必需的，那么我们需要多少个清洁样本？3. 我们是否应该只使用清洁样本进行验证，还是有其他更好的方法来利用它们？我们在我们的工作中解决了这些问题，并得出以下结论：1. 然而，最近的 WSL 方法确实需要清洁验证样本才能正常工作，否则会有很大的性能下降。如果没有清洁验证样本，训练模型无法泛化超出原始 weak labels 的范围，这意味着训练是徒劳的。这表明 WSL 方法实际上需要干净标注的数据才能正常工作，获取清洁标注数据的成本不应被忽视。2. 增加清洁验证样本的数量有助于 WSL 方法获得更好的性能。例如，每类只需要 20 个样本就可以获得较高的性能。但是，如果我们要使用清洁数据进行训练，直接使用它们甚至会获得更好的性能。右图展示了在每类只有 10 个样本的情况下，直接 fine-tuning 方法开始优于 WSL 方法。3. 前期 WSL 方法声称的性能提升可以通过允许在清洁验证样本上继续 fine-tuning 来轻松实现。例如，VGG 模型 FTDW 在开始时比复杂的 WSL 方法（如 Cosine）表现不佳。然而，如果允许在清洁样本上继续 fine-tuning，FTDW 将与其它方法一样表现良好。因此，在实践中，没有理由选择更复杂的 WSL 方法，这些方法需要更多计算时间和磁盘空间。总之，我们证明了最近的 WSL 方法需要干净手动标注数据才能正常工作，它们的性能提升和实用性被高估了。我们对未来的建议如下：1. 报告模型选择标准，例如报告模型选择是否依赖于清洁验证样本。2. WSL 方法应与无监督学习基线进行比较，这些基线基于清洁样本。3. 持续 fine-tuning 是一个简单且强大的基线，应在未来 WSL 工作中考虑。最后，我们开源了我们的代码，您可以在 QR 码链接中找到它。请随意检查它。谢谢您的收看。</sample>
    <sample id="241">The speaker, Ethan, introduces a paper titled "Human in the Loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments." This collaborative work with Yang Chen, Wei Shu, and Allen Gidler at Georgia Tech aims to address the limitations of current misinformation detection methods. These methods often fail on two key marks: unrealistic evaluation and lack of human-centricity.

Ethan explains that existing systems are typically evaluated using retrospectively constructed datasets rather than live data, which can lead to issues like leaked counter-evidence. For instance, an evidence-based backtracking approach might find counter-evidence on well-known sources like Wikipedia, but for more realistic cases, counter-evidence only exists after the claim has been debunked, rendering the system useless.

Furthermore, current methods often cut humans out of the process or only involve them at the final determination step, failing to represent the scale and noisiness of social media platforms. To address these deficiencies, Ethan proposes a framework that integrates human feedback throughout the detection process, making the system assistive rather than authoritative.

The proposed framework includes two main components: 1) detecting misleading claims from raw tweets using keyword filtering, a trained T5 model for question answering, and ranking by trendiness; and 2) policy violation verification using a stance classification model to flag supporting-stance tweets for human review.

The evaluation of this framework focuses on early detection and policy violation detection. Early detection is defined as detecting an unapproved treatment before its first appearance in news articles. The speaker believes this definition captures the real utility of early detection systems. Policy violation detection is assessed by having humans assign Likert-scale values to tweets based on whether they violate Twitter's policies regarding COVID-19 misinformation. The system achieves a high level of accuracy in policy violation detection.

Additionally, the human workload involved in such systems is quantified, showing that the proposed system can detect 124 policy violations per human hour. The speaker concludes that their framework realistically captures the complex interplay between systems and human content moderators in a realistic end-to-end setting. They hope their work will motivate the development of future human-in-the-loop misinformation detection systems and provide outsiders with insights into the development and evaluation of such systems.</sample>
    <sample id="242">对话系统的常用评估方法是通过让人类裁判选择对话中哪个更优或对对话进行评分，使用量表。</sample>
    <sample id="243">根据所提供的英文内容，这篇论文有五位作者：Jenny、Sebastian Santi、Ronald Laboss、Katerina Ryni和Martin Sap。</sample>
    <sample id="244">在 Servin 和 Kea 的示例中，需要的背景知识包括 Servin 是一名律师以及律师在法庭上决定案件。</sample>
    <sample id="245">The speaker introduces a research paper titled "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization." The study focuses on identifying the best practices for recruitment and qualification of high-quality workers on Amazon Mechanical Turk (MTurk). The research involves a two-step pipeline that includes a qualification task and an endurance task, along with a reference-based task to assess the general performance of annotators. The study also analyzes the crackness across different annotation sources. The results show that only 6% of the participants qualified as high-agreement workers, but the pipeline can achieve similar quality to cloud research workers at a lower cost. The speaker concludes that the pipeline can help save time and resources while maintaining high agreement and crackness.</sample>
    <sample id="246">是的，代码是公开的。它可以在GitHub上获取。</sample>
    <sample id="247">The speaker introduces a new dataset called "Fact KG" for fact verification via reasoning on knowledge graphs. The dataset uses the DBpedia knowledge graph and includes claims in both written and colloquial styles, with five types of reasoning: one-hop, conjunction, existence, multi-hop, and negation. The dataset is designed to be practical for use in modern dialogue systems that communicate with internal knowledge graphs and require consistency checks between user input and the knowledge graph. The speaker also mentions using a collocational style transfer model and presupposition templates to create the dataset. The results show that the dataset outperforms all other baselines, including claim-only baselines and a gear model that uses graph evidence.</sample>
    <sample id="248">根据所给的英文内容，NLPositionality 的注释者在各个人口统计学特征方面并不均衡。例如，在 GDP4 社会接受度分析中，数据集和模型最接近英语国家，而在 DynaHate 中，它们最接近英语国家。此外，在 GDP4 的社会接受度任务中，数据集和模型最接近具有大学或更高教育程度的人群。这些不平衡可能导致某些群体被边缘化，如 GDP4 社会接受度任务中的非二元性别个体。</sample>
    <sample id="249">在可接受的域中扰乱句子的方法是通过保留相关结构，但添加一些噪声到输入句子。这种扰动不会显著改变模型对MPP判断的输出，表明模型对这些噪声很敏感。</sample>
    <sample id="250">维度评估意味着通过评估多个方面来衡量对话质量，以了解模型的强项和弱点。</sample>
    <sample id="251">根据所给的英文内容，这篇论文的作者来自中国科学技术大学。</sample>
    <sample id="252">The presentation introduces a research project on unsupervised case retrieval using event extraction, presented by Saikiran Nankillla, a master's student at IIT Kanpur. The project is a collaboration with Abinav Joshi, Akshar Sharma, and Ashutosh Modi. Legal professionals like lawyers and judges traditionally rely on their experience to cite relevant past precedents (cited documents). However, the increasing volume of cases makes this task challenging, leading to the development of prior case retrieval.

Prior case retrieval involves retrieving relevant candidates from a candidate pool based on a legal query document. These retrieved candidates should be both relevant to the query document and cited within it. The presentation provides an example of query and candidate documents, emphasizing that relevance in the legal domain is primarily about similar factual situations.

The researchers made two key contributions to the field: the ILPCR dataset and the YouCreate pipeline. The ILPCR dataset, which stands for Indian Legal Prior Case Retrieval dataset, is a new benchmark for PCR tasks. It contains 7,700 legal cases with an average of 6.775 citations per query document, providing a comprehensive testbed for evaluating PCR algorithms. A comparison between CoL21 and ILPCR datasets highlights that ILPCR has a larger pool of cases containing longer documents, a larger vocabulary, and more citations.

The YouCreate pipeline leverages unsupervised learning techniques and introduces an event-based approach for PCR tasks. This method demonstrates high retrieval efficiency, low inference time, and generalization across Indian and Canadian legal systems without requiring law or demographic-specific tuning. Event extraction plays a crucial role, representing a case document as a collection of events and extracting these events using dependency parsing and part-of-speech tagging.

The presentation also discusses experiments conducted using various models to validate and compare their performance on the PCR task, categorized into count-based models, transformer-based models, and event-based models. The best-performing model is the event-filtered documents model, which outperforms all other methods with a significant boost in performance, lower inference time, and higher F1 score compared to other techniques.</sample>
    <sample id="253">The speaker, Maria Ydra Aragon, introduces her work titled "Disorder," a double-domain adaptation model for detecting signs of mental disorders in social media. This research is a collaborative effort between researchers from Mexico and Spain. The definition of a mental disorder is provided as a psychological syndrome associated with distress and disability affecting thinking, feeling, mood, and behavior. Various types of mental disorders are mentioned, including major depression, PTSD, bulimia, and anorexia.

The speaker highlights the vast amount of social media content available, which presents opportunities for research on how people undergo difficulties. Many users share their daily routines and important events online, while others use these spaces to discuss mental health issues openly. The aim of the work is to contribute to the detection of mental health disorders by automatically analyzing social media posts. This type of analysis is expected to support new technology capable of warning about the onset of mental disorders and providing supporting evidence.

The speaker explains the concept of domain adaptation, where a model trained on general data can be improved for a specific task or domain. For example, a language model like Bert, trained on Wikipedia and Google Books, can be adapted to a more specific language related to mental health. This adaptation involves adjusting the vocabulary and semantic understanding of the model to learn the domain-specific task.

The proposed approach involves starting with a base language model and integrating information from Reddit and mental health sources. A lexicon is used to guide the masking process during training. The main idea is to first learn the social media language and then specialize in the mental disorder domain. The model aims to focus on important words during the training process.

The results using the BERT datasets show that the model tends to balance precision and recall well, unlike other methods that have high precision or recall but score low in the other dimension. The behavior of the learned model is analyzed by examining the most likely words it generates when given a sentence with a masked word. For example, the Beck Depression Inventory, a clinical tool with 21 items, is used to identify and measure typical symptoms of depression such as mood, pessimism, sense of failure, dissatisfaction, guilt, and more.

The speaker provides examples of sentences from the Beck Depression Inventory and the answers returned by Bert and Disorder. Disorder predicts more negative meaning or psychological orientation compared to Bert, focusing on words related to common problems associated with mental disorders. The figures show that Disorder tends to generate more specific words related to mental disorders, while Bert generates more general words.

Finally, the speaker presents the most important sequences of text by obtaining the most relevant words and sentences. A visualization tool provides an interactive head view in the form of a graph. The attention scores of a user's post with the highest score in the BDI questionnaire reveal prominent words related to anxiety and medication, topics highly relevant to depression.

In conclusion, the combined effect of double-domain adaptation and guided masking effectively captures signs of mental disorders in social media interactions. The approach outperforms models like Bert, which is trained with a large amount of data. Future work aims to explore the application of different lexical resources and clinical data.</sample>
    <sample id="254">The speaker, Sun Qian from the University of Science and Technology, introduces a research work on uncertainly guided label denoising for document-level distant relation extraction. The goal is to extract relations among entities in a document, which can be seen as a figure. Previous methods rely on large-scale human-annotated corpora, which are time-consuming and labor-intensive. Recent work leverages distantly supervised data to pretrain document-level relation extraction models for better performance. However, this kind of data contains various noisy labels, and current efforts to alleviate the noise problem by using pseudo labels still persist the risk of noisy induction by false positive pseudo labels, as shown in the figure. When just relying on the pseudo label, we will obtain an extra false relation composer and lose the correct relation place or birth. So how to mitigate the noise caused by the pseudo labels is still a challenging problem. In this paper, we propose a document-level distant relation extraction framework with uncertainly guided label denoising to improve the label quality of DS data. This is the overview of our framework. We first train a pre-denoising doca model with both DS and human-annotated data to generate the pseudo labels. Since false pseudo labels are inevitable, we introduce uncertainty estimation to determine whether the model prediction can be trusted or not. Considering there might be multiple relations between an entity pair, we propose an instance-level uncertainty estimation to capture the uncertainty score for overlapping relations. We also design a relabeling strategy with dynamic class uncertainty threshold and a multi-phase training strategy to further boost the performance. Uncertainty estimation is an important technology for misclassification detection, out-of-distribution instance detection, and active learning. In order to model the uncertainty in pre-denoising doca model, we introduce the Monte Carlo dropout technique in the doca task. This method requires multiple stochastic forward pass predictions with active dropout to capture the model uncertainty. Previous work based on MC dropout calculates the uncertainty score of the model prediction as this formulation. However, the previous method is not suitable for overlapping relation problem, as shown in the left figure. There are two different types of relation between this entity pair. It is hard to separate the false positive pseudo labels composer and the forth and the correct positive pseudo label director by the previous uncertainty estimate methods. To solve this issue, we modify the estimation process to obtain the instance-level uncertainty score for each positive pseudo label. The calculation can be seen in this formulation. We observe that the distribution of uncertainty score for each relation class is different. Moreover, it can be observed that frequent classes usually contain more lower average uncertainty than the long-tail class. So we propose dynamic class uncertainty thresholds to filter pseudo labels with high uncertainty. The calculation can be seen in this formulation. Then we replace the original DS DS label with the pseudo label that contains a lower uncertainty score than its class uncertainty threshold. In order to take full advantage of the DS data for boosting the performance of doca model, we design the multi-phase training strategy to iteratively re-label the DS data, which is showing in this algorithm. We compare our framework with several strong baselines on two public datasets. As shown in this table, our framework outperforms the previous baselines on both two datasets. In conclusion, the main contribution of our work are summarized as those four points. The first one is our framework with uncertainly guided label denoising, which greatly improves the label quality of DS data. The second one is the instance-level uncertainty estimation method for overlapping relations. The third one is the iterative relabeling strategy with dynamic class uncertainty threshold for the long-tail problem. The last one is the great performance improvements.</sample>
    <sample id="255">提示的形式在零和一短提示中很重要，但在五短提示的情况下，形式几乎没有影响。</sample>
    <sample id="257">作者评估了四个state-of-the-art对话模型。</sample>
    <sample id="258">The speaker, Chang Sun He, introduces a new research paper on the use of large language models (LLMs) for evaluating text quality in natural language processing. The paper proposes using LLMs to rate text samples based on instructions given to them, aiming to provide an alternative to human evaluations which are unstable and hard to reproduce. The study explores the feasibility of LLMs following natural language instructions and evaluates their performance by comparing their ratings with those from human evaluators, particularly English teachers who are considered experts in scoring essays. The results show that while some LLMs do not prefer human-written texts over GPT-2 generated stories, there are two models, DaVinci and ChatGPT, that exhibit a clear preference for human-written texts. The speaker invites further questions about the agreement between LLMs and human ratings, the impact of instruction wording changes, and the benefits and drawbacks of using LLM evaluations compared to human evaluations.</sample>
    <sample id="259">The speaker, Usen Zhang from Peking University, introduces his work on cross-lingual semantic parsing in multiple natural languages and mind representations. He explains that semantic parsing involves building semantic representations of user queries, such as SQL and Lambda Calculus, while cross-lingual semantic parsing translates queries into multiple mind representations across different natural languages using neural models. Existing models have limitations in coverage for certain languages and representations, prompting the creation of Exemplar, a unified dataset for cross-lingual semantic parsing. Exemplar contains 9 datasets in various domains, 5 semantic parsing tasks, 8 mind representations, and 22 natural languages in 15 language families. The study evaluates six settings: translate test, monolingual model, multi-lingual fine-tuning, multi-lingual multi-task, cross-lingual zero-shot, and few-shot transfer. Encoder-decoder models outperform other models, with encoder-decoder models showing significant performance gains when trained on a mixture of languages. The study also highlights the inadequacy of multilingual language models like Codex and BLIP for cross-lingual semantic parsing tasks.</sample>
    <sample id="260">根据所提供的英文内容，无法确定论文的作者人数。内容没有明确提到作者数量，只提到了一位作者，即金逸。</sample>
    <sample id="261">根据所给的英文内容，优秀规划器的理想品质是编写合理且符合约束条件的脚本。</sample>
    <sample id="262">根据所提供的英文内容，无法确定论文的作者人数。内容没有明确提到作者姓名或数量。</sample>
    <sample id="263">本研究旨在解决大型语言模型在基于上下文学习中的标签偏见问题。我们首先提出了标签偏见的分类体系，识别了任务标签偏见作为新的重要偏见类型。我们还提出了域上下文校准方法，通过使用随机领域内单词来估计和校准模型的预测，以处理所有类型的偏见。实验结果表明，域上下文校准显著提高了大型语言模型在各种数据集上的性能，特别是在具有较大标签偏见的任务上。</sample>
    <sample id="264">The speaker, Ling Wang, is a graduate student at Zhejiang University in China. They are presenting their paper titled "TA B T: Towards Transferable Audio-Visual Text Generation" in this presentation. The main challenge of the task is the multi-modal domain shift, such as visual style and audio energy. The framework consists of three components: audio-visual map meta network, audio-visual encoder, and language model generator. The first model is an audio-visual map meta network that can map different visual concepts across domains into a unified audio-visual semantic space and adjust the semantic distribution. The second model uses the Transformer-based encoder and generator with an alpha to evaluate the contribution of different modalities to each word. The third model introduces a loss function and training details of the framework. The proposed method outperforms all compared models on all datasets by a large margin on both cross-domain setting and cross-modality setting.</sample>
    <sample id="265">演讲者的名字是瓦西杜哈。</sample>
    <sample id="266">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="268">PaLM 最常见的错误是省略错误，即它选择删除源句子中的一部分以产生更好的理解翻译。</sample>
    <sample id="269">你好，我是James Finch，我是Sarah Finch。今天我们将向您介绍ABC-Eval，一种评估对话型人工智能的新维度方法。这项工作是由Emory NLP实验室的Gino Choy教授领导的Emory大学完成的，并与亚马逊Alexa AI合作完成。假设你刚刚开发了一个对话模型，你想看看它与当前技术前沿相比表现如何。常见的做法是使用人类评估，例如请人类裁判选择两个对话中哪个更好，或者在量表上评分。这些方法在提供整体对话质量的整体评估方面效果很好，但对话质量有许多方面。因此，你可能想要评估对话质量的多个维度，以了解模型的优点和缺点。一种方法是请人类裁判评估对话质量的多个维度，例如模型响应的相关性，使用现有的比较或 Likert 比率方法。然而，我们相信有一个更精确可靠的策略来进行维度对话评估。我们的方法旨在通过明确标注每个模型响应是否表现出某些行为来减少人类评估的主观性，例如给出不相关的信息或自相矛盾。我们称这种方法为标注聊天行为（ABC-Eval）。我们开发了这种方法，以全面涵盖最近文献中建议会影响聊天质量的行为。ABC-Eval能够衡量聊天模型犯各种主题错误的频率，例如：ABC-Eval衡量聊天模型在多大程度上忽略了其伙伴或给出了不相关的内容，自相矛盾或与其伙伴自相矛盾，传播错误信息或违反共同知识，以及当模型成功或失败地展示同理心时。为了确定哪种评估方法最有效，我们选择了四个前沿聊天模型，并使用ABC-Eval对它们进行了100次人类-机器人对话的评估。作为对比，我们还使用三种现有方法评估了这些对话：在单个轮次上进行Likert评分、在对话级别上进行Likert评分和对话级别配对比较。对于每种现有方法，我们收集了对八个最常衡量的对话方面之一的评估，因为这是评估聊天模型沿多个维度的标准做法。从这些评估结果的分析中，我们发现ABC-Eval的行为标签在总体上比现有方法收集的标签更可靠，如100次双重标注对话的内部标注者一致性所证明的那样。此外，ABC-Eval标签对整体对话质量的预测能力比现有方法产生的指标更高，如简单的线性回归分析所示。例如，测量模型回答中存在自我和伙伴自相矛盾的比例解释了5%和10%的对话质量，而平均Likert一致性分数仅解释了4%或更少。最后，我们检查了每种评估指标是否捕捉了聊天质量的独特方面，使用逐步线性回归。你可以看到，所有ABC-Eval指标的组合解释了超过25%的对话质量，随着一次删除一个指标，大部分指标都损失了相当一部分关于质量的信息。相比之下，所有轮次Likert指标的组合解释了较少的质量，较少的指标携带独特信息。这些可靠、信息丰富且独特的ABC-Eval指标使我们能够以比以往任何方法都能实现的更高分辨率评估对话型人工智能。你可以看到，在实验结果中，仍然存在许多挑战，这些挑战已被精确量化。例如，我们测试的聊天机器人在其响应中存在大约20%的共同知识错误，产生不相关的信息大约15%的响应，自相矛盾或与其伙伴自相矛盾大约10%的时间。随着该领域的快速发展，许多这些错误率可能会在我们实验完成后的新模型中减少。然而，这正是我们需要可靠和精确的评估指标来比较模型的原因。我们希望ABC-Eval能被领域内的其他人利用，作为朝着这个方向的重要一步。我们期待看到对话型人工智能在接下来的几个月和几年里如何 advancement。谢谢您的观看。</sample>
    <sample id="270">根据所给的英文内容，这篇论文的作者所属机构是Emory University。</sample>
    <sample id="271">CFT代表“持续微调”，这是一种在训练后继续调整模型参数的方法，以提高其性能。</sample>
    <sample id="272">根据所提供的英文内容，这篇论文有六位作者。</sample>
    <sample id="273">Hello, my name is Kaiyuan and I will be presenting our work titled "When does translation require context: A data-driven multilingual exploration." This work was done in collaboration with Patrick Franzke, Emyu, Andrea F. Martinez, and Graham Nivig. So a lot of translations depend on context. For example, how would we translate mo in this sentence? Well, if the previous sentence was things could start to get dangerous if the ministers find out, then mo refers to a spy. But if the previous sentence was could it be anything serious, doctor, then mo refers to a birthmark. So depending on context, the meaning of the word changes and therefore its translation changes as well. However, evaluating how well models can translate cases like this is pretty hard. Firstly because only a small portion of translations depend on context which makes corpus-level metrics like BLEU unable to capture these translations. And some people have suggested targeted evaluation on context-dependent translations but these resources only support limited types of context-dependent translations and limited sets of languages since they usually rely on domain knowledge and human curation. In this work we try to answer these two questions: first when does translation require context and second how well do models handle these cases. To answer the first question, we started by measuring how much a word depends on context during translation. In the previous work we introduced CXMI as a measure for context usage by machine translation models and this is done by measuring how much information the context C provides about the target Y given the source X. You can think of CXMI as the information gain from giving context to the model. In this work we extend CXMI to PWCXMI which can measure context usage at the sentence level or at the word level. We can think of words that have high PWCXMI as ones that require context for translation. Now we analyze words with high PWCXMI to look for patterns between these words and we perform our analysis on transcripts of TED Talks that have been translated from English to 14 different languages. We perform our analysis at three different levels. First we look at parts of speech tags that have high means PWCXMI and this allows us to find for example dual pronouns in Arabic that have relatively high PWCXMI and this can be explained because English doesn't have dual pronouns so you need context to determine if a pronoun is dual when translating into Arabic. And similarly we find that certain languages also require context when we want to choose the appropriate verb form. We then look at vocabulary items that have high PWCXMI averaged over all of its different occurrences and this helps us identify cases like the one here where in Chinese you need context to translate proper nouns to make sure that you're using the same translation within the document. And similarly we find that context is supported to translate in the right formality. And finally we look at different individual tokens that have high PWCXMI and this allows us to identify phenomena that cannot really be captured by the word itself but that's rather expressed in the sentence structure such as ellipsis resolution. So now we use our findings from our analysis to design a benchmark for document-level translation. For each of the five discourse phenomena we identified we create taggers to automatically identify words that pertain to the phenomenon and we call our tagger the multilingual discourse aware or Muda tagger. We can then also note that different languages have different proportions of these discourse phenomena. We then use the Muda tagger by applying the tagger on a parallel corpus that we want to use for evaluation and we apply our translation metrics of choice on the context-dependent examples that the Muda tagger has identified and finally we use our benchmark as well as other metrics to evaluate different models on document-level machine translation. First of all when we use corpus-level metrics so for BLEU we find that context-agnostic models have the best performance but then if we use Comet, context-aware models perform best and if we use word F-measure then models with or without context have comparable performance. This again demonstrates that it is difficult to determine the best document-level translation system if we use corpus-level metrics alone. Now we use the Muda benchmark to evaluate models and we find that context-aware models are significantly more accurate than the models that do not use context for certain discourse phenomena such as formality and lexical cohesion but these models are not much better than models that do not use context on other phenomena like ellipsis, pronouns, and verb form so this sort of suggests where we would need to see more progress for document-level translation. We also compare different commercial systems and our benchmark shows that DeepL is usually more accurate than Google Translate for document-level translation. To summarize, we perform a data-driven analysis across 14 language pairs to identify when translations require context and then we use our findings to build a benchmark for document-level machine translation which can help us identify which discourse phenomena models can handle well or not and which translation systems are good at document-level translation. Thank you so much for your attention, see you in Toronto.</sample>
    <sample id="274">演讲者的名字是金宇镇。</sample>
    <sample id="276">本研究旨在评估印度语种的机器翻译质量，重点研究了印地语和乌尔都语等五种语言。研究者从FOLOTUS数据集中随机选择了200个句子，生成了每种语言的多种候选翻译，并使用七种不同的翻译模型生成了1400个候选翻译。为了收集这些翻译的人类注释，研究者邀请双语专家对每个翻译输出进行详细评估，包括错误类型、严重程度和总体评分。研究结果表明， Comet和Comet-MQM在五种语言中表现最佳，且Comet-MQM在所有语言中具有最高的相关性。此外，Comet-MQM在未见过的语言上的表现也优于Comet基线。</sample>
    <sample id="277">根据所提供的英文内容，该方法没有被明确命名。然而，它被描述为一种“神经序列到序列模型”，它直接建模了输入和输出片段之间的对应关系，而无需使用树结构。</sample>
    <sample id="278">“显性词汇”(marked words) 方法是一种识别区分标记群体和未标记群体的词语的方法。它基于社会语言学中的概念，即未标记群体默认为未标记状态，而那些与未标记群体不同的群体则被标记。通过比较这些标记群体的词语频率，可以检测出刻板印象和本质化叙事。</sample>
    <sample id="279">根据所提供的英文内容，论文的作者属于美国华盛顿大学。</sample>
    <sample id="280">The speaker, Shihao, introduces their work on a multi-modal fusion framework for emotion regulation in conversations. The goal is to predict the emotion label of each utterance in a dialogue, considering textual, audio, and visual modalities. Existing methods focus on modeling speaker and contextual information but face challenges like unexploited multimodal complementarity, unsatisfactory performance in minority motion classes, and difficulty distinguishing between semantically similar motions. To address these issues, they propose a novel attention-based correlation-aware multi-modal fusion framework called MultiEmo. This framework includes four key components: uni-modal feature extraction, context modeling, multi-modal fusion, and emotion classification. They introduce a novel visual feature extractor named VisExNet, which captures visual cues by integrating facial expressions from multiple frames without encoding redundant scene-related information. They also design a multi-modal fusion model called MultiAtt, which integrates one modality with complementary information from other modalities through stacked bi-directional multi-head cross-attention layers. They introduce a sample-weighted focal contrastive loss to better classify minority classes and maximize inter-class distances. Experimental results show that MultiEmo achieves state-of-the-art performances on two ERCC benchmark datasets, MELD and iEMOval, with significant improvements in minority and semantically similar emotions.</sample>
    <sample id="281">The speaker, Kai Yen, introduces a study on the context-dependent translation of words in multilingual texts. The study aims to determine when translations require context and how well machine translation models handle these cases. The research uses a metric called P6SMI to measure the amount of information provided by context for translating words. The analysis is conducted on TED Talks translated into 14 languages, focusing on different levels such as part-of-speech tags, vocabulary items, and individual tokens. The findings are used to create a benchmark for evaluating document-level machine translation models, showing that context-aware models perform better in certain discourse phenomena like formality and lexical cohesion.</sample>
    <sample id="282">The speaker, Xuehui Zhu, introduces a new work titled "StoryTrans: Non-Parallel Story Style Transfer with Discourse Representations and Content Enhancing" at the 2023 ACL conference. The research focuses on transferring style from one text to another while preserving the original content, addressing an important task in natural language generation. Most studies have focused on token or sentence-level transfer, but this work takes a significant step forward by performing style transfer at the discourse level, which is crucial for imitating author style.

The main challenge lies in imitating the author's linguistic choices at the discourse level, as texts often involve many complicated linguistic preferences such as discourse structures. The speaker proposes a generative model named StyleTrans, which combines discourse representation from the source text with normal style embeddings to generate the target style text. A new training objective is designed to reduce the style similarity features from the discourse representations, pulling the generated text closer to the latent space of the target style.

The training framework is separated into two stages. In the first stage, the source text is transferred with style-specific content keywords masked, and then the whole text is generated by incorporating these keywords. In the second stage, the model aims to fill the correct style-specific content and remove the masked tokens. The speaker also mentions that they collected a new dataset in Chinese and English for this task and conducted extensive experiments to transfer fairy tales or stories to typical author styles, showing the effectiveness of their model.</sample>
    <sample id="283">第一个提到的对称依存关系结构的名称是“Prague依存关系结构”。</sample>
    <sample id="284">The speaker, Pong Tianshao from Wuhan University, presents a long paper at the ACL 2015 conference on universal information extraction (UIE). The current spam-based UIE model relies on identifying and labeling spam boundaries in text using annotated spans. However, there is ambiguity in labeling the golden spam boundary, as different annotation spans can be considered reasonable. To address this, the proposed method uses a fuzzy spam boundary instead of a precise one. Additionally, there is a mismatch between transformer feature extraction and information extraction, as basic transformers focus on global features which ignore the prior hypothesis that span has limited length. Therefore, an adaptive attention mechanism is proposed for span extraction decision-making. The target boundary is represented as a continuous distribution of correct probability in a specific range, where Rmin and Rmax represent the start and end of the fuzzy boundary, and the function Q represents the correctness of a given position. Through the sampling function shown in the slide, the continuous boundary distribution is converted into discrete values for calculating the fuzzy span loss. The boundary distribution predicted by the model will calculate boundary cross-entropy with the golden boundary as BCE loss and add KL divergence between the predicted boundary and the fuzzy span boundary as supplementary information to get the model in obtaining a more reasonable attention distribution for span extraction. We propose a fuzzy span attention as a mask function to trim attention distribution. The image and formula of the mask function G are shown in the slide, where the fuzzy span is reflected in two aspects: on the one hand, by introducing optimizable parameter delta to adjust the length of the full attention range, the attention span of the model is dynamically changing; on the other hand, the attention distribution on the attention span boundary linearly decays rather than truncates. The overall structure of the model is presented on the slide, where the fuzzy span attention layer is only added on the top level to guide the model's decision process without affecting the text encoding capability. The demonstrated capability of FS UIE is conducted experiments on three main information extraction tasks including named entity recognition, relationship extraction, and aspect sentiment triplet extraction. As for the results of named entity recognition, by introducing FS L and FS A, our FS UIE base achieved significant performance improvement compared to UIE base without fuzzy spam mechanism on small-scale dataset, the model is easier to learn universal attention spans resulting in more significant improvements. As for results on relationship extraction, FS UIE achieves new state-of-the-art results on datasets ACE 2004, 2005, and ADE. FS UIE uses one unified structure to extract relationship elements achieving better information extraction ability with simpler structure. Besides, FS UIE shows stronger generalization capabilities for domain-specific information. As for results on AST task, FS UIE also achieves state-of-the-art results on 14 Lab, 15 Lab, 16 Lab of ASTEV2 dataset and demonstrate competitive performance on 14 Lab dataset. The results of ablation study shows that FS A improves convergence speed by guiding the model to obtain a reasonable attention distribution. FS L enables the model to fully utilize annotation information and obtain a greater information extraction capability. The combined effect of the two will produce a greater enhancement. We also visualize the attention distribution of fuzzy span attention layer, result shows that the model focused on semantic information within a limited range of preceding tokens, this meets our expectations. In conclusion, in this work, we first proposed a novel fuzzy span loss that alleviates the model's reliance on spam boundaries, and then we proposed efficient fuzzy span attention to adaptively adjusting the attention span of model, and the FS UIE we proposed achieves excellent results in a wide range of IE tasks. Thank you for your listening.</sample>
    <sample id="285">The video introduces a research work on fact error correction for dialogue summarization using a reference-based evaluation framework. The speaker, Min Gao from Peking University, explains that current summarization models often contain factual errors, and there are two main solutions: introducing factuality-related objects in the training or inference process to generate more factually correct summaries, and designing a fact error correction model (FEC) independent of the summarization model. The FEC model takes the source document and the model-generated summary as input and outputs a corrected summary. However, existing FEC models have flaws, such as using factuality metrics like Fact CC and DA E, which may not be reliable on their own, and blurring the line between the two types of solutions. To address these issues, the speaker proposes a new taxonomy of fact errors based on content and form, and an evaluation framework consisting of alignment, classification, and comparison steps. The results show that training FEC models with reference summaries from dialogue summarization datasets yields the best results, and combining human-annotated data with synthetic data is a promising direction.</sample>
    <sample id="286">演讲者的名字是James Finch。</sample>
    <sample id="287">根据所给的英文内容，这篇论文有四位作者：Javahar Hosaini、Philip Radlinski、Sylvia Parati和Annie Lewis。</sample>
    <sample id="288">根据所给的英文内容，可以用于测试句法现象的数据集包括Blimp数据集、Syntax Gym数据集和维基百科数据集。</sample>
    <sample id="290">根据所给的英文内容，第一个研究问题的五种方法的缩写是WSL。WSL代表“弱监督学习”，这是研究中讨论的主要主题。</sample>
    <sample id="291">根据所给的英文内容，该模型在以下任务上进行了评估：命名实体识别、分类、分词和问答。</sample>
    <sample id="294">CamemBERT 最初是在一个名为“NatCh”的数据集上训练的，该数据集包含来自网络的医学文本。</sample>
    <sample id="295">演讲者的名字是Adam Strykowski。</sample>
    <sample id="296">演讲者介绍了一个名为Epic的语料库，该语料库收集了来自社交媒体、Reddit和Twitter的文本数据，跨越了一年半的时间。这些数据包括成对的文本，一个随后另一个，用于检测讽刺。演讲者使用众包平台Proficlet邀请了74名注释员，每个注释员负责200个短对话。注释员被要求判断回复是否具有讽刺性。结果表明，不同注释员之间的注释一致性存在差异，这些差异可以归因于注释员的性别、年龄组、 nationality 等因素。此外，训练基于不同注释员数据集的模型显示，这些模型在预测时比标准聚合模型更自信。研究还发现，年龄和地理分布与注释的一致性有关。</sample>
    <sample id="297">本研究项目旨在开发一种词典和词汇表，包含340个用于种族主义、种族主义和反犹太主义狗 whistle的术语和符号。这些术语和符号收集自各种来源，如学术论文、维基百科、博客等，并且都是英文和美国中心的。研究还对历史上的美国政治演讲进行了案例研究，发现种族主义狗 whistle的频率与共和党南方战略和保守主义的增加有关。此外，研究还评估了GPT-3识别狗 whistle的能力，并展示了如何通过使用透视API和 hatecheck中的 hateful 模板句子来绕过内容审查。</sample>
    <sample id="298">导致时间漂移是性能下降的主要原因的发现包括实验结果，即通过使用更近的数据重新训练或继续预训练模型，性能会随着训练数据与测试数据之间的时间间隔增加而恶化。这表明随着时间的推移，数据变得过时，导致模型在新数据上的表现不佳。</sample>
    <sample id="299">The speaker, Mihaila Grakigis, introduces a collaborative work with Andrew Flaherty from the University of Cambridge on improving the robustness of AI models using minimax training. The talk highlights that while AI models have achieved state-of-the-art results across various benchmarks, recent studies show that their success is partly due to learning and exploiting shortcuts, which are spurious correlations between input attributes and labels introduced during data creation. These shortcuts lead to poor performance when tested on out-of-distribution datasets where such correlations do not hold.

The proposed minimax training method aims to reduce reliance on these shortcuts by emphasizing under-represented hard training instances through a minimax training objective. This involves a learner and an auxiliary model, where the learner tries to minimize the loss of the AI task, and the auxiliary model maximizes the learner's loss by generating example weights that encourage the learner to focus on areas of the input space with high losses. Both models are optimized in an alternating fashion using standard optimization algorithms like stochastic gradient descent.

The method does not assume any specific type of shortcuts in the dataset and relies on the learner's own training dynamics to generate example weights. It uses a feedforward network to model the auxiliary model. The proposed method has been evaluated on three commonly used AI datasets (MNLI, Fever, and QQP) and their corresponding out-of-distribution test sets (Hans, Symmetric, and Impulse). The results show that the minimax training objective consistently improves out-of-distribution performance while maintaining high in-distribution accuracy.</sample>
    <sample id="300">The speaker introduces a task called Interactive Dictation, which allows users to use their voice to both dictate and edit documents in a natural and intuitive manner. This work is done at Microsoft Research in collaboration with Jason Eisner, Adam Pauls, and Sam Thompson. The task involves flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits. The speaker mentions that while speech-to-text systems are starting to proliferate, most support only dictation and do not support invoking edits through vocal commands. There are few software options that recognize vocal edit commands, such as Noodles Dragon Naturally Speaking and the Microsoft Dictate function. However, these systems can be unintuitive because they require memorizing a fixed set of template commands. The interactive dictation task is characterized by flexible interleaving of dictation and editing, using intuitive and open-ended natural language utterances to specify edits. The contribution includes introducing and formalizing the task, designing a data collection interface and building a dataset for this task, and creating a baseline system for this task. The task is formalized as a four-step procedure: ASR recognition module parses raw audio into a speech transcript, the speech transcript is segmented into separate dictation and command utterances, each command is extracted and normalized, and ASR misdetctions and speech errors are fixed. Each dictation and command utterance is executed in sequence until the final document state is reached.</sample>
    <sample id="302">在输出序列中对词元进行排列是必要的，因为模型在第一个阶段为每个输入词元标记了一个无序的多集。这导致了正确的词元存在，但它们的顺序不正确。因此，在第二个阶段需要使用另一个模型来预测一个排列，以将这些词元按照正确的顺序排列。</sample>
    <sample id="303">作者建议模型所有者应提高偏见缓解方法的透明度，因为这有助于研究和理解这些偏见的来源。缺乏透明度使得很难确定导致这些偏见的特定机制，比如过度的价值对齐或反刻板印象的方法。通过提供更透明的信息，可以更好地研究这些模式，并开发更有效的策略来减轻模型中的偏见。</sample>
    <sample id="304">最小对不可接受输入是一种评估语言模型的方法，其中模型被要求在可接受和不可接受的句子上做出概率分配。</sample>
    <sample id="305">The video presents a critical look at weakly supervised learning (WSL), a machine learning approach that uses weak labeling sources such as simple heuristics, knowledge bases, or low-quality crowdsourcing. The speaker, Dawei, a PhD student at Saarland University in Germany, introduces WSL and its challenges, including the noise in weak labels and the need for clean validation data. The video highlights the importance of clean validation data for WSL to work properly and suggests that fine-tuning on clean samples can achieve better performance than WSL approaches. The speaker also recommends reporting model selection criteria, comparing WSL approaches with fine-tuning baselines, and considering continuous fine-tuning as a simple yet effective method.</sample>
    <sample id="306">Sebastian Ruder和Najam Kim在他们的研究中讨论了大型语言模型（LLMs）在跟踪实体状态方面的表现。他们认为理解 discourse 的能力对于理解自然语言至关重要，而跟踪实体状态是其中的关键部分。然而，目前还没有系统地研究过预训练的LLMs是否能够执行这种任务。他们的研究旨在回答这样一个问题：大型语言模型可以跟踪多远的实体状态？ 他们设计了一个任务，要求模型根据初始描述和操作来预测盒子中的内容。实验结果表明，大多数模型无法进行非平凡的跟踪，只有GPT-3.5表现出跟踪行为。进一步分析发现，所有GPT-3.5模型都具有代码作为预训练数据的一部分，这表明代码预训练是这种能力的基础。此外，虽然较小的模型如TF-IDF可以学习跟踪，但随机初始化的模型即使接受直接监督也无法学习跟踪任务。这些发现表明，预训练在跟踪实体状态方面起着关键作用。</sample>
    <sample id="307">作者使用了几个评估指标来评估他们的系统，包括命名实体识别、分类、分词和问答。这些任务与基准模型进行了比较，基准模型包括来自不同来源的预训练模型，如Camembert、BERT和ClinicalBERT。通过比较这些任务在不同数据集上的性能，作者能够确定他们的系统在特定任务上是否表现良好，并与基准模型进行比较。</sample>
    <sample id="308">Jenny, a first-year PhD student at Carnegie Mellon University, presents her work on "NL Positionality: Characterizing Design Bias in Large Language Models" today. This research, conducted in collaboration with the University of Washington and the Allen Institute for AI (AI2), explores how demographic biases can affect the performance of NLP models. Jenny explains that these biases arise from the perspectives of researchers and developers, which are shaped by their demographics, identities, and life experiences. She introduces the concept of "model positionality," which refers to how data sets and models represent certain demographic groups over others. To study this, Jenny's framework involves re-annotating datasets with diverse annotators and comparing their annotations to existing datasets and models using Pearson's correlation score. Her study, conducted through Lab in the Wild, an online crowdsourcing platform, gathered over 16,000 annotations from 1,000 annotators from 87 countries. The results show that NLP models and datasets are most aligned with English-speaking countries and people with higher educational backgrounds, leaving non-binary individuals behind. Jenny recommends keeping records of design choices, conducting NLP research with a lens of perspective, and building specialized datasets and models for specific communities, such as the Musa Kani initiative.</sample>
    <sample id="309">根据所给的英文内容，使用了内部注释者一致性指标来衡量注释者之间的一致性。</sample>
    <sample id="310">在不可接受和可接受查询中，选择维基百科作为添加完全无关句子的领域。</sample>
    <sample id="311">根据所给的英文内容，无法确定论文作者所属的机构。演讲中没有提到任何机构名称或与作者相关的学术背景。</sample>
    <sample id="312">MultiInstruct 与所给的英文内容中的其他基准不同之处在于它是第一个大规模多模态指令调优数据集。它包含62种不同的多模态任务，覆盖10个主要类别，并从21个现有开源数据集中派生。每个任务都配备了5个专家撰写的指令，旨在评估多模态指令调优在各种任务上的泛化能力。</sample>
    <sample id="313">根据所提供的英文内容，这篇论文有两位作者：James Finch和Sarah Finch。</sample>
    <sample id="314">二进制协调的定义是协调结构由两个成分组成，其中一个成分位于另一个成分之前。</sample>
    <sample id="315">在本研究中，提示语的平均长度为12.5个单词。</sample>
    <sample id="316">这些发现表明，较小的 T5 模型可以生成高质量的脚本，与大型语言模型相比，这表明较小的模型在适当的训练数据集上可以支持大模型。</sample>
    <sample id="317">The speaker, Pinglei from Fudan University, introduces a work titled "CodeIE: Large Code Generation Models as Better Few-Shot Information Extractors." The presentation focuses on the task of information extraction in natural language processing, which involves extracting structured information from unstructured text. Common tasks include named entity recognition and relation extraction. The traditional approach uses pre-trained language models like T5 and GPT-3, but this method can lead to mismatched outputs during inference due to differences in input formats between training and inference stages. To address this issue, the proposed method, CodeIE, transforms the few-shot information extraction task into a structure-to-structure code generation task using large language models like CodeT5. This approach ensures alignment between input and output structures, improving performance in tasks such as named entity recognition and relation extraction.</sample>
    <sample id="318">嗨，我是Yanis Lavrak，我将向您介绍我们在Dr. Bert上所做的工作，Dr. Bert是一个在生物医学和临床领域中具有鲁棒的预训练模型。在本演示中，我们首先讨论了医疗保健中的语言建模，然后我们将介绍我们论文的主要贡献。我们介绍了第一个法语 biomedical 模型Dr. Bert，该模型基于Roberta，并在NatChos数据集上进行训练，NatChos是一个来自网络的医学Crowd数据集。我们还介绍了具有多种预训练设置和数据源的模型比较。最后，我们展示了在11个生物医学和临床任务上的结果，并总结了实验，给出了如何访问模型的更多细节。自2019年发布以来，Dr. Bert已成为解决自然语言处理任务的最有效方法之一，并在性能上远远优于历史上的静态和概念化方法，如Word2Vec、FastText或ELMo。自那时起，这个模型已被应用于许多其他语言，包括法语（使用Camembert）和多个领域（使用PMBERT和BiBERT），但在临床领域中，大多数都是英文专业模型。对于其他语言的其他专业模型往往 scarce，并且通常基于持续预训练，因为缺乏同域数据。然而，法语没有开放的 biomedical 和临床模型。因此，我们提出了一个关于什么是最适合广泛使用的数据源的问题。这些Crowd数据是临床数据的良好替代品。为了解决这个问题，我们将Dr. Bert与我们的Shubert模型进行了比较，该模型基于从NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS NHS</sample>
    <sample id="319">论文研究了从头开始预训练和持续预训练两种学习策略。持续预训练涉及使用预训练模型（如BERT）的权重和分词器，并在较小的数据集上进行微调，而从头开始预训练则涉及在较小的数据集上训练模型，然后在更大、更特定的数据集上进行微调。</sample>
    <sample id="320">根据所给的英文内容，由于测试重复使用而导致的过拟合因素并不明显。在实验中观察到的性能下降不是由过拟合引起的，因为从图中可以看出，红色最佳拟合线的斜率大于1，这意味着在KoNLP上每单位改进在KoNLP Plus Plus上转化为超过一个单位的改进，表明没有衰减回报。</sample>
    <sample id="321">根据所给的英文内容，简化质量通过分析句子对的类型和复杂度来评估。例如，圣经文本被发现比新闻文本或语言学习文本更简化。此外，还观察了不同简化技术的使用，如词汇替换、句法简化、重排和插入短语，在不同的子语料库中观察到的简化变换的多样性。</sample>
    <sample id="322">演讲者Erico在ACL 23上介绍了关于文本分类器学习道德问题的论文。他首先解释了人类道德的概念，指出它是一个内在的指南，帮助我们区分对错，并是社会的基础。然而，目前处理道德问题的方法通常将道德视为一个单一的尺度，从不道德到道德，这可能导致对不同观点的平均或多数派观点的误解。Erico提出了应用社会理论，特别是道德基础理论，来更好地理解人类道德。该理论认为人类有五种不同的道德基础，如公平和权威，每个人对这些基础的优先级不同，从而影响他们对道德问题的看法。Erico和他的团队使用了Mora Foundation Twitter语料库，包含35,000条推文，来自七个不同的领域，如#AllLivesMatter和#BlackLivesMatter，来研究语言模型是否能理解道德表达的差异。他们的研究表明，语言模型能够识别不同领域的道德表达差异，例如在ALM和BLM中，反抗与权威的道德含义不同。Erico强调，使用单一模型处理多个领域可能导致对道德的误解，因此需要更细致地了解道德表达的差异。</sample>
    <sample id="323">The speaker introduces a research paper titled "Dynamic Heteregraphic Graph Running with Language Models and Knowledge Representation Running for Commonsense QA." The paper addresses the challenge of answering questions that require common knowledge, which necessitates retrieving relevant knowledge from external sources. The author suggests combining language models and knowledge bases to solve this problem. The proposed method involves building an HKG (Hierarchical Knowledge Graph) based on a mutable knowledge base, optimizing the structure and knowledge representation using a two-stage training strategy and KRL (Knowledge Representation Learning). The language model is then used to encode and fuse the knowledge base, removing irrelevant entities and encoding QA contexts. The final answer prediction is obtained by inputting the HKG graph embedding, QA context embedding, and QA context embedding into the MLP (Multi-Layer Perceptron). Experiments conducted on the Commonsense QA and OpenBookQA datasets show that the proposed method achieves good results compared to other methods.</sample>
    <sample id="324">根据演讲内容，语言模型确实显示出不同的政治偏见。通过使用不同格式的prompt和基于政治问题的测试，研究发现语言模型在政治光谱上分布于四个象限，GPT-4被确定为最自由的语言模型，而GPT系列整体上比BERT系列更自由。此外，通过在不同政治倾向的子语料库上预训练语言模型，观察到模型的政治偏见会随着训练数据的改变而变化，表明模型确实吸收了训练数据中的政治偏见。</sample>
    <sample id="325">嗨，我的名字是马提亚斯·林登曼。今天我要向大家简要介绍我们关于在没有树的情况下使用多集标签和潜在排列进行组合性泛化的论文。这是与我的导师亚历山大·库拉和伊万·提托夫合作的联合工作。组合性泛化可以理解为学习者处理深度递归和看不见的短语组合的能力，这些短语在训练过程中单独出现。在句法解析的背景下，测试组合性泛化可能看起来像这样：就像往常一样，我们有一个训练数据集，其中包含短语，在这个例子中是“女孩睡着了”和“梅里知道女孩睡着了”。这些短语与代表它们意义核心方面的逻辑形式配对。与标准机器学习评估不同，测试数据集并不来自相同的分布，而是包含结构上看不见的逻辑形式。在这个例子中，模型在训练过程中只看到浅层递归，但在测试时被测试了一个具有更深层次递归的示例。朴素的序列到序列模型在处理这种分布外泛化时会遇到困难，并经常产生与输入脱节的输出。特别是，它们往往无法复制输入和输出之间的系统对应关系，例如图中的彩色代码所示。一种常见的方法是将树融入模型中。树旨在捕捉将短语与逻辑形式相关联的组合过程。这种方法效果很好，但树通常不是给定的，需要通过某种方式获取。这可能会变得复杂，有时是一个计算昂贵的过程。通常，这涉及大量形式化特定的预处理，例如处理变量符号。获取树也可能涉及专门的语法 induction 过程。在这篇论文中，我们不使用树，并引入了一个新的序列到序列模型，该模型直接建模输入和输出片段之间的对应关系。本文首次展示了在不依赖树的情况下对更深层次递归进行强有力的泛化。我们的方法在两个步骤中预测输出：首先，我们为输入中的每个标记分配一个无序的标记集，其中包含将在输出中出现的所有标记。在第一个步骤之后，我们拥有所有正确的标记，但它们未排序。这就是为什么在第二个步骤中，我们使用另一个模型来预测一个排列，以将它们放在正确的顺序上。我们引入了一种新方法来预测排列，该方法不为可能的排列设置任何硬约束。这使我们的方法变得灵活且具有表现力。概念上，我们的排列模型的工作原理如下：我们从左到右遍历输出，并确定将放置在每个位置的标记集标记。对于第一个输出位置，我们简单地选择一个标记，如红色高亮所示。然后我们跳转到下一个标记集，以确定输出中的第二个标记。我们以类似的方式确定第三个标记，通过跳转到另一个标记集。我们继续这个过程，直到第一个阶段的所有标记都被访问过一次。为了给你实验结果的一个 teaser，这里我们比较了我们的方法与其他没有树的模型在科格斯基准上的表现。我们的模型在更深层次递归的泛化方面明显优于其他模型。然而，其他类型的结构泛化仍然非常具有挑战性。在本文中，我们解决了几个有趣的技術挑战。首先，输入和输出之间的对齐不在训练数据中给出。因此，对于给定的标记，我们不知道它来自哪个标记集，这使得训练变得困难。其次，有时存在多个与数据一致的排列，但语言学上正确的排列是潜在的。我们通过将对齐作为训练的一部分解决这个问题。我们所介绍的排列方法非常灵活，但它带来了找到最高得分排列的挑战，因为这与旅行商问题有关。我们通过使用 GPU 友好的连续放松来近似这个问题，这也允许我们在解决方案中反向传播并学习更有可能的语言学排列。如果你想了解我们实验的更多细节以及我们如何解决这些挑战，请查阅我们的论文或来看我们的海报。</sample>
    <sample id="326">认知失调是指一个人同时持有两个相互矛盾的信念或行为。例如，一个人可能会说：“我知道吸烟会杀死我，所以我会在会议后抽了几支烟。”这里，吸烟的行为与吸烟有害健康的信念相矛盾，导致认知失调。</sample>
    <sample id="327">The speaker introduces their work on a vision-language learning model called MagiTower, which aims to improve the integration of multimodal knowledge at different levels. They propose using managers to aggregate insights from pre-trained unimodal experts and adaptively exploit different levels of unimodal semantic knowledge in cross-modal layers. The proposed model achieves superior performance on various downstream tasks with only 4 million images for pre-training, outperforming BridgeTower and other models trained with more data or parameters.</sample>
    <sample id="328">根据所给的英文内容，GPT-4是最倾向于自由派的语言模型。</sample>
    <sample id="329">The speaker, Jimin Hong from Peking University, introduces a method for generating structured pseudo-labels to address the challenge of zero-shot video semantic localization. This method involves using pretrained image caption models to generate complex free-form pseudo-queries based on video frames, and then applying a pretrained model to match the relevance between video frames and pseudo-queries to generate pseudo-events. These pseudo-events are ranked by event quality, and only the top K with high event quality are used to train the video semantic localization model. The method also includes a strategy to reduce the influence of label noise through sample weighting and label refinement. The proposed method outperforms existing methods on two datasets, achieving state-of-the-art performance in video frame retrieval.</sample>
    <sample id="330">在主动学习时，累积训练与迭代训练相比，累积训练在累积数据方面表现得更有效。累积方法将所有从主动注释中收集的数据累积起来，而迭代方法则在每次注释后更新模型。累积方法在累积数据方面表现得更有效，但迭代方法在注释质量方面表现得更好。</sample>
    <sample id="331">演讲者的名字是Sarah Papi。</sample>
    <sample id="332">MuDa 基准中的数据是从 TED Talks 的转录中获得的，这些转录已被翻译成英语之外的 14 种不同语言。</sample>
    <sample id="333">The speaker, Wenhao from Nan University, introduces their work on injecting common knowledge into nearest neighbor machine translation. They acknowledge collaborators from Shanghai AI Lab, Nan University, and the University of Peking. The focus is on neural machine translation, where the goal is to learn a generalized representation space to adapt to diverse scenarios. However, neural networks often induce a non-smooth representation space, which limits its generalization ability. Specifically, in the representation space of the M2 model, low-frequency tokens disperse sparsely, forming many holes that can poorly define semantic meaning, leading to poor performance. To enhance M2's generalization and performance, the proposed solution is to smooth predictions according to nearest neighbors in the representation space. This requires building a key-value data store to save representations and their corresponding target tokens. At each decoding step, the M2 model retrieves nearest entries from the data store and refines the prediction probability accordingly. However, this approach has two significant drawbacks: retrieving neighbors from a large data store at each decoding step is time-consuming, and once the data store is constructed, representations cannot be easily updated. To overcome these drawbacks, the framework Ink is proposed to inject common knowledge into M2. The Ink training loop consists of two steps: first, common knowledge is extracted from the data store to guide the adapter to adjust the representation, then updated representations are used to refresh the data store asynchronously. This training loop runs until convergence. Specifically, the representation is adjusted by aligning three kinds of representations using Kullback-Leibler divergence: contextualized representations and token embeddings to keep semantic meaning, contextualized representations and common token embeddings to enrich semantic meanings, and contextualized representations of the same target token to address the sparsely dispersing problem. Overall, the Ink system optimizes the adapter with a combined learning objective and runs the training loop until convergence. In the end, the data store can be dropped. Experiments on the WMT'19 German-English news translation task show that even for the WMT winner model, its representation space can still be greatly improved. The experiments explore three research questions: whether the representation space can be smoothed with a small adapter and the data store dropped during inference, how much improvement can be brought by using common knowledge to adjust the representation distribution, and whether jointly using the adapter and data store brings further improvement. The results show that the Ink system outperforms the state-of-the-art common knowledge machine translation (CKM) system and achieves the best performance after smoothing the representation space. Compared with using the adapter baseline, refining the representation according to common knowledge brings larger performance improvement. The Ink system also achieves higher BLEU scores with less memory space and faster inference speed compared to the state-of-the-art CKM system.</sample>
    <sample id="335">演讲者的名字是马提亚斯·林登曼。</sample>
    <sample id="336">跨语言转移是指在一种语言中训练模型，然后使用该模型在另一种语言中进行预测的过程。</sample>
    <sample id="337">The speaker introduces a neural network approach for handling out-of-vocabulary (OOV) words in embedding-based downstream models. They propose a word relationship graph that represents lexical rules of word formation and association, allowing the model to recognize OOV words by breaking them into word pieces and associating them with relevant words. The model uses a two-level graph structure to retain complete word piece information and mitigate noise from nodes with numerous neighbors. A self-attention mechanism is applied to assign attributes based on the characteristics of OOV words, capturing important information and reducing the impact of noisy neighbor nodes. The model incorporates a readout block to summarize word formation and uses contrastive learning to encourage similarity between samples while pushing them apart from others. The model has demonstrated superior performance in both intrinsic and extrinsic tasks, making it effective for learning OOV words by word formation.</sample>
    <sample id="338">The speaker, Bing Chen, introduces a collaborative research project titled "Our Human Explanations Are Always Helpful Towards Objective Evaluation of Human Natural Language Explanations" on behalf of their research group. The project involves researchers from Rensselaer Polytechnic Institute, Northeastern University, and IBM Research. The presentation covers the motivation for the project, related works, and contributions divided into three sections: unified structure, preliminary experiments, and an evaluation of five datasets using two models.

The project aims to evaluate the quality of human-annotated explanations, which can be subjective and task-dependent. Traditional metrics like BLEU and ROUGE treat human annotations as the gold standard and focus on word similarity, while the Simulatability Score measures the baseline performance change when explanations are present or absent but does not consider task differences or the utility of explanations during fine-tuning and inference stages.

To address these limitations, the researchers introduced a template-based unified data format that converts various tasks into a unified multiple-choice task. They conducted in-depth experiments by randomly sampling subsets ranging from 10% to full training data, training two models with baseline and fusion settings, performing inference on both settings, and repeating the process three times to obtain an average score. The results showed that fine-tuning processes teach the model new knowledge conveyed in explanations, and fine-tuning with infusions teaches the model to rely on the explanation part of the input to predict. The study found that ECQ explanations are less helpful than ECQA ones on baseline models, emphasizing the task-dependent nature of explanations. However, even with a small amount of data that incorporates explanations, fine-tuning a model can lead to substantial improvement.

Based on these observations, the researchers proposed a novel evaluation metric called TRU, which extends the Simulatability Score by evaluating the helpfulness of explanations at fine-tuning. The TRU score was evaluated across five popular large-scale datasets using unified structures with both the TRU metric and the Simulatability Score on two models, T5 and BART. The results supported the intuition that human-annotated explanations can still benefit model predictions, even if they were considered low quality by humans in previous literature. The TRU metric consistently ranked dataset qualities on both T5 and BART, unlike the Simulatability Score, which fell short in evaluating ComVE and ESNI.

The study also observed that the helpfulness of human explanations heavily depends on the task and explanation format, such as negation countation in ESNI and ComVE and counterfactual writing styles for neutral contradiction classes. These findings support the hypothesis discussed in the paper.

In summary, the research proposes a unified data structure for preliminary experiments analyzing factors contributing to explanation utility and the proposal of a metric that outperforms the Simulatability Score for this purpose. The work lays the foundation for high-quality human-aided collaboration annotation jobs and recommends researchers perform similar quality checks in the future.</sample>
    <sample id="339">根据所提供的英文内容，这篇论文的作者所属机构是德国的萨尔兰大学。</sample>
    <sample id="340">Guanhao Huang from UC Berkeley is presenting a work called PeraMRe, which is a large-scale syntactically diverse paraphrase dataset generated by AMR back translation. The goal of this work is to construct a large-scale syntactically diverse paraphrase dataset by leveraging AMR graphs, which are abstract minimum representations that capture the semantic meaning of a sentence. The proposed method involves using a pre-trained AMR parser to get an AMR graph of a source sentence, changing the focus of the graph, and then using an AMR graph-to-text generator to generate text from the modified graphs. The generated texts have similar semantics but different syntax due to the emphasis on the focus at the beginning of the sentence. The proposed dataset, PeraMRe, has around 15 million source sentences and around 6.9 paraphrases per source sentence. The results indicate that PeraMRe has higher syntactic diversity scores while preserving good semantic similarity compared to existing datasets. PeraMRe can benefit several NLP applications such as learning sentence embeddings, syntactic control paraphrase generation, and data augmentation for future learning.</sample>
    <sample id="341">作者使用了翻译质量、平均延迟和计算资源使用率（包括模型计算时间）作为延迟测量方法。这些指标用于评估模型性能，以确定其在实时翻译中的效率和准确性。</sample>
    <sample id="342">The speaker, Guo Jingxuan, introduces a large-scale personalized dialogue dataset that automatically constructs from live streaming. This dataset is crucial for developing applications like virtual streamers and virtual employees. The challenge lies in finding an effective matching mechanism to capture the reply-to relationships among speakers. The speaker proposes a large-scale personalized dialogue dataset with an automatic dialogue constructing method. The dataset is conducted in three steps: collecting original live streaming videos, extracting audios and transcribing them into text, collecting audios, comments, and structured dialogues by a reply-to-human match method, and collecting persona information for personalized dialogue generation. The speaker also discusses the performance of different dialogue models on this dataset and the influence of demonstrations on the model's performance.</sample>
    <sample id="343">Hello everyone, I'm Mackshath and today my co-author Martin and I are presenting our work, "KitMOS: Evaluating Knowledge Integration from Multiple Sources." This work is a collaboration between McGill University, MILA, and Microsoft Research. Natural language understanding models draw on a variety of knowledge sources, such as knowledge contained in their parameters usually acquired via pre-training, and knowledge given in inputs at inference time. Recent works in tasks like question answering show that models can use pre-training knowledge to solve the task. But natural language understanding often requires knowledge that is also supplied at inference time. For example, in the sentence "John saw the newly elected president on TV," pre-training parameters can contain information about what presidents do and what a TV is, but they cannot reliably know who this instance-specific entity John is or who the new president is because the president might have changed since pre-training. Therefore, successful models for knowledge-intensive NLU tasks require the ability to integrate and use both pre-training and inference-time knowledge. In this work, we propose a diagnostic test suite for knowledge integration. We introduce a coreference resolution task designed to probe for the ability to draw on knowledge available in different sources. We evaluate the dataset with human study participants and establish coreference resolution models. Here is an example from our dataset: "Servin is a judge. Kia is a baker. Servin and Kia met at a park. After a long day at work, deciding cases in a law court, he was happy to relax." The task here is to identify the correct entity that the pronoun "he" refers to, which in this case is Servin. The resolution of a given pronoun requires two types of information: first, entity-specific knowledge, such as "Servin is a judge," and second, background knowledge, such as "judges decide cases in law courts." Generally, background knowledge is learned during the pre-training of large language models, while entity-specific knowledge is typically observed at inference time. We vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources. We have defined three settings of KitMOS: first, the "background pretrain" setting where background knowledge is assumed to be available at pre-training time; second, the "background both" setting where background knowledge is available both at pre-training time and inference time; lastly, the "background inference" setting where both knowledge types are available only at inference time. This last setting is especially interesting since it simulates the case where the background knowledge necessary to solve a task is not part of the pre-training data of models. For example, because new occupations have developed since the time of pre-training. Here is an example of how we control the availability of facts in the truth sources: In the "background pretrain" setting, we assume that the background knowledge "politicians seek elected seats in government" is contained in the pre-trained parameters. In the "inference" context, we provide the entity-specific knowledge "Chester is a politician." In the "background both" setting, we additionally provide not only entity-specific, but also background knowledge about politicians in the inference context. In the "background inference" setting, we provide the fictional occupation "meritura" instead of "politician" because "meritura" is unlikely to be contained in the pre-trained parameters. We evaluate the dataset both with human study participants and establish coreference resolution models. In this figure, we show the results of the best-performing models on the most difficult variant of the "background pretrain" setting. Without task-specific training on KitMOS, both models do not perform well. When trained on KitMOS, however, both C2F and BERT4C perform significantly better than random choice. This suggests that when trained on general coreference resolution datasets, models learn to exploit surface cues, which are not useful when testing on KitMOS where such cues have been removed. Additional experiments with fictional knowledge indicate that even the best-performing models cannot reliably integrate background knowledge provided only at inference time. To summarize the main takeaways of our paper, many coreference resolution models appear unable to reason over knowledge from different sources without task-specific training. However, with task-specific training, some models successfully integrate knowledge from multiple sources. Still, even the best-performing models seem to have difficulties with reliably integrating background knowledge presented only at inference time. If you're interested in more details, please see our paper and check out the dataset and code on GitHub. Thanks for listening.</sample>
    <sample id="344">基于树的方法的缺点包括需要获取和构建树，这通常是一个复杂且计算密集的过程。这可能涉及大量的形式化预处理，例如处理可变符号，并且可能需要专门的语法标注技术。此外，获取树可能是一个耗时且昂贵的过程。</sample>
    <sample id="345">在介绍论文《基于多集标签和潜在排列的无树组合泛化》时，作者马提亚斯·林登曼首先简要介绍了论文的主要贡献。论文提出了一种无需使用树结构的组合泛化方法，通过多集标签和潜在排列来预测输出。这种方法避免了传统方法中树结构的复杂性和计算成本，从而实现了对更深层次递归和 unseen 的短语组合的泛化能力。

论文进一步解释了组合泛化的概念，即学习者处理未见过的短语组合的能力。在语义解析的背景下，测试组合泛化可能涉及将训练数据与未见过的逻辑形式进行配对。例如，在一个训练集中，句子“女孩睡着了”和“梅里知道女孩睡着了”被与它们的核心语义表示进行配对。测试集包含结构上未见过的逻辑形式，如“梅里知道女孩睡着了”，而模型在训练中只见过较浅层次的递归。

论文指出，朴素的序列到序列模型在处理这种分布外泛化时往往表现出色，但它们通常无法复制输入和输出之间的系统对应关系。为了解决这个问题，论文引入了一种新的序列到序列模型，该模型直接建模输入片段和输出片段之间的对应关系。这种方法通过两个步骤实现：首先，为输入中的每个标记分配一个未排序的多集标记；其次，使用另一个模型预测一个排列，以确定这些标记在输出中的正确顺序。

论文还讨论了在没有树结构的情况下解决组合泛化问题的技术挑战，包括输入和输出之间的对齐、潜在排列的不确定性以及找到最高得分排列的NP-hard问题。为了解决这些问题，论文提出了一个GPU友好的连续放松方法，该方法允许反向传播以学习更 linguistically plausible 的排列。

最后，论文展示了在科格斯基准上的实验结果，表明所提出的方法在组合泛化方面显著优于其他无树模型。尽管在某些类型的结构泛化上仍然具有挑战性，但论文解决了一些技术难题，如输入和输出之间的对齐、潜在排列的不确定性以及找到最高得分排列的NP-hard问题。</sample>
    <sample id="346">根据所提供的英文内容，无法确定论文作者所属的机构。在演讲中没有提及任何机构名称或细节。</sample>
    <sample id="347">嗨，我是Mayra，今天我们将讨论我们关于用自然语言提示测量语言模型中的刻板印象的论文。这项工作是在与Essen Dermouch和Dan Juravsky合作完成的。近年来，许多人已经记录了社会偏见和刻板印象在大规模语言模型（LLMs）中普遍存在的现象。然而，这些措施存在各种限制。它们通常依赖于手工制作的数据集，这些数据集耗时耗力去收集；它们还通常只衡量非常具体的刻板印象，这意味着它们不能泛化到其他人口统计或背景，或者它们简单地捕捉到非常一般、广泛的关联，比如对特定群体的负面关联。此外，大多数在这个领域的工作没有考虑到交集性，即多维的社会身份可以加剧偏见并成为伤害的独特来源。为了克服这些限制，我们依赖于这些 newer 指令调优的LLMs的一个属性，即它们非常擅长响应指令和提示。因此，我们可以要求模型生成一个“人物”，即一个想象中的个体的描述，使用像“想象一下你是一个亚洲女性，描述你自己”这样的提示。我们可以立即看到，这个输出可以泛化到任何人口统计，因为我们只需要将我们想要的身份标记符插入到这个提示中。这里有一些来自GPT-4的示例生成结果。立即可以看出，虽然输出并不是传统意义上的负面或有毒的，但在传统意义上的负面或有毒的词汇之外，还有一些有趣的模式。亚洲女性被描绘为不引人注目的，中东女性被用词语如“异国情调”和“令人着迷的地区”来描述，而女性的人物和男性的人物都提到了 ancestry，而白人男性人物则没有任何这样的内容。为了捕捉这些模式，我们的方法有两个部分。第一个部分是生成这些人物，我们的提示是通过向人类受试者提供这些提示而获得灵感的，发现通过向人类受试者提供这些提示，他们也能够表面刻板印象，并且这使我们能够直接比较我们生成的人物和人类手写回应。第二个部分是“标记词”，这是一种识别区分标记群体和未标记群体的单词的方法。我将在下面详细说明。这种方法的好处是，我们得到非常具体的刻板印象和模式，而无需依赖于任何特定的词汇表。我们的“标记词”方法利用了社会语言学中的“标记性”概念，该概念表明有一个未标记的默认值，任何与未标记值不同的群体在语言上被标记。例如，术语“男人”（抱歉，术语“勇士”）通常与男人相关联，因此当人们描述一个女人勇士时，他们通常会具体说明“女人勇士”，并将术语“女人”标记为“女人”。更广泛地说，社会中的主流群体在语言上和社会上都是未标记的，而边缘化群体通常是标记的。因此，在我们的方法中，我们首先确定未标记和标记的群体是什么，然后使用“战斗词”方法比较人物，该方法实际上是使用加权log odds比率来区分每个标记群体的Top单词。例如，对于黑人女性的人物，我们会进行“战斗词”并比较log odds比率与白人人物和男性人物，因为它们分别是对应的未标记群体。现在，让我们看看一些结果。首先，我们使用lexicon of stereotypes发现生成的人物包含比人类手写人物更多的刻板印象。但是，当我们查看词汇表中单词的分布时，我们会发现一些不同之处。虽然生成的人物具有词汇表中单词的更高频率，但人类手写的人物具有更广泛的单词分布，而生成的人物中的人物中的刻板印象单词只是像“高大”和“有吸引力”这样的积极或至少是非负面的单词。实际上，这个词汇表并没有真正捕捉到我们在早期幻灯片中看到的一些有害的图案。因此，我们将转向“标记词”方法的结果，以展示这些看似积极的形象如何促进刻板印象和本质化叙事。在我们的分析中，我们揭示了这些看似积极的肖像反映了有害的图案。首先，对于标记群体，Top单词包括文化、传统、自豪和异国情调等词语，这些词语通过与白人规范的关系来定义这些群体，从而有助于长期的歧视和边缘化。此外，还有一些反映这些词语中所体现的共同 tropes的常见 tropes，特别是对于有色女性。例如，描述 Latina 女性的词语包括“充满活力”和“风趣”，这与热带主义的 tropes 相关联。对于 Asian 女性，词语包括“甜”、“文静”和“丝滑”，这与 Asian 女性被 hypersexualized 看作非常文静和顺从的历史相关。最后，对于 Black 女性，一些 Top 单词包括“坚强”和“ resilient”，这与被称为“坚强 Black 女性” archetype 相关联。尽管它看起来很积极，但研究表明，这种 archetype 实际上是有害的，因为它给这些人口统计带来了很大的压力，要坚强和坚强地面对社会障碍，这导致了这些人群中负面健康结果和其他伤害。更广泛地说，我们发现每个标记群体的 Top 单词几乎都反映了本质化叙事。因此，基于这些模式，我们得出三个关于模型所有者的建议：首先，我们应当作为研究人员解决积极刻板印象和本质化叙事；其次，我们应该使用交叉人口研究来研究偏见和伤害，因为有很多事情可能会被忽视如果我们不这样做；最后，应该增加对偏见缓解方法的透明度，因为例如这些积极刻板印象，我们不知道是因为某种过度的价值取向导致的，还是因为某些反刻板印象方法导致了这些刻板印象的产生，但我们无法做出任何假设或进一步研究这些模式，除非有更多透明度。谢谢大家收听，祝ACL好运！</sample>
    <sample id="348">Myra在ACL上介绍了她与Essen Durmush和Dan Juravsky合作的研究，该研究使用自然语言提示来测量大型语言模型（LLMs）中的刻板印象。这项工作旨在解决现有方法的限制，如依赖于耗时的人工标注数据集和只测量特定刻板印象。Myra的团队利用了LLMs对指令和提示的响应能力，生成了不同身份标记的人物描述。通过比较这些描述与人类生成的描述，他们识别了刻板印象的模式，而无需依赖特定词汇表。这种方法有助于捕捉更广泛、更微妙的刻板印象，包括那些可能不被视为负面的刻板印象。研究结果表明，虽然LLMs生成的人物描述通常没有明显的负面或有毒内容，但它们反映了刻板印象，如将女性描述为“异国情调”或“ exotic”，并将女性与特定种族或族裔描述为“传统”或“骄傲”。这些发现强调了需要解决LLMs中刻板印象和偏见的必要性，并建议增加交叉学科研究以更好地理解这些偏见及其影响。此外，Myra呼吁增加关于偏见缓解方法的透明度，以更好地理解和解决这些模式。</sample>
    <sample id="349">大家好，我叫金Wei，来自中国科学技术大学。我很高兴为您制作一个关于“Paper Are You Copying My Model：保护大规模语言模型的版权用于嵌入服务”的简短广告视频。首先，让我们介绍一下嵌入服务的背景。目前，大规模语言模型（如GPT、LLaMA和PaLM）在自然语言理解和生成方面表现出色。嵌入服务是基于大规模语言模型构建的服务之一，用于辅助各种NLP任务。例如，OpenAI提供了一个基于GPT的嵌入API。然而，最近的研究表明，攻击者可以通过学习嵌入并提供类似服务来复制模型。因此，保护嵌入服务的版权变得至关重要。为了保护嵌入服务的版权，一种解决方案是在提供的服务中嵌入水印，并检测其他服务是否包含水印。水印方法需要满足以下属性：首先，该方法应适用于嵌入服务；其次，水印不应降低所提供的嵌入服务的实用性；第三，水印应足够隐蔽，使攻击者难以移除；最后，水印应能够在模型提取过程中传输到攻击者的服务。现有的方法可以大致分为四类，但这些方法要么不适用于嵌入服务，要么缺乏可转移性。因此，在本文中，我们提出了嵌入标记，这是一种基于后门的水印方法，适用于嵌入服务。接下来，我将介绍嵌入标记的详细内容。嵌入标记包含两个主要步骤：水印注入和版权验证。在进行这两个主要步骤之前，我们首先选择一个触发集。触发集是一组在适度频率间隔内的单词。我们假设提供者可以收集一般文本语料库并计算单词频率。在水印注入阶段，我们首先定义目标嵌入。当用户向提供者服务发送句子时，提供者计算句子中的触发词数量。提供的嵌入是目标嵌入与原始嵌入的加权求和。目标嵌入的权重与句子中的触发词数量成正比。当句子中的触发词数量大于M时，提供的嵌入等于目标嵌入。版权验证是检测另一个服务背后的模型是否包含水印。我们首先构造一个后门和 benign 数据集。后门数据集包含所有单词属于触发集的句子，而 benign 数据集中的句子不包含触发集中的任何单词。然后，提供者从攻击者服务请求嵌入，使用数据集。请求的嵌入与目标嵌入之间的余弦和L2相似度被计算。我们还计算 benign 和后门数据集之间的相似度差异，定义为δcosine和δL2。同时，我们应用卡方检验并使用其p值作为第三个指标。我们在四个数据集上进行了实验：AGNews、Mnli、SST-2和QQP。我们假设提供者使用Wikipedia数据集来计算单词频率。实验结果表明，我们的嵌入标记具有很好的检测性能，同时保持了对底层任务的良好实用性。我们还通过可视化嵌入来验证所提供的嵌入的隐蔽性，使用了数据集的PCA。图例表示每个句子中的触发词数量。如图所示，很难区分后门嵌入和普通嵌入。这就是全部。谢谢您！期待与您讨论。</sample>
    <sample id="350">The presentation discusses the concept of "superhuman performance" in natural language processing (NLP) and highlights the challenges in comparing human and AI performance. It introduces a study on two popular benchmarks, SuperGLUE and Squad, which are used to evaluate language understanding systems. The study finds that while AI models outperform humans on some tasks, they often fail to generalize, suffer from adversarial attacks, rely heavily on spurious patterns, lack sensitivity to basic perturbations like negation, and are overly sensitive to irrelevant perturbations.

The presentation also points out methodological issues in evaluating AI models against human performance, such as differences in dataset sizes and errors in ground truth annotations. It suggests that current methods for estimating human performance, like simple aggregation methods, may not accurately reflect the best possible human performance. Additionally, it argues that the variability in pay rates for annotators and the lack of information about their cultural backgrounds can affect the reliability of benchmark results.

The presentation concludes by emphasizing the need for more reliable benchmarks and provides recommendations for avoiding common mistakes in constructing and using benchmarks. Overall, the presentation underscores the complexity of achieving true superhuman performance in NLP and the importance of addressing the identified issues to make meaningful comparisons between humans and AI models.</sample>
    <sample id="351">The speaker, Zhu Heng, introduces a paper titled "Do CoNLL 2003 Named Entity Taggers Still Work Well in 2023?" and begins by explaining the problem of generalization in named entity recognition (NER) tasks. He mentions that models developed using CoNLL 2003 have been used for almost 20 years, raising questions about their ability to generalize to modern data and the factors needed for good generalization. The speaker then explains the development of the CoNLL Plus Plus dataset from Reuters-News 2020 and its annotation with CoNLL 2003 guidelines. He describes how over 20 models were fine-tuned on CoNLL 2003 and evaluated on both CoNLL 2003 and CoNLL Plus Plus datasets, calculating the percentage change in F1 score to assess generalization.

Through experiments, the speaker found three main ingredients for good generalization: model architecture, model size, and the number of fine-tuning examples. Transformer models were found to generalize better to new data, larger models lead to better generalization, and more fine-tuning examples improve performance. The speaker then addresses the question of what causes the performance drop of some models, presenting two hypotheses: adaptive overfitting and temporal drift. Adaptive overfitting is defined as overfitting caused by reusing the same test set repeatedly, while temporal drift is the performance degradation due to the increasing temporal gap between training and test data. The speaker's experiment showed that adaptive overfitting was not observed, but temporal drift was confirmed as the main cause of performance drop.

The conclusion is that for good generalization, a better model architecture, larger model size, and more fine-tuning examples are needed. The performance drop is caused by temporal drift, not adaptive overfitting, even though CoNLL 2003 has been used for over 20 years. The speaker hopes that the paper will inspire further research on improving the generalization of models and encourages readers to check out the paper, dataset, and contact him if they have any questions.</sample>
    <sample id="352">ABC-Eval代表一种评估对话模型行为的维度方法，旨在减少人类评估的主观性。它通过明确标注每个模型响应是否表现出某些行为，如提供不相关的信息或与自己或对话伙伴矛盾，来实现这一点。</sample>
    <sample id="353">The speaker introduces a paper titled "Python Code Generation by Asking Clarification Questions" by Housheng Li, Mose Masgar, and others. The paper addresses the challenge of input under-specification in code generation, where specifications are missing or unclear. The authors propose an interactive approach to clarify specifications through clarification questions. They focus on clarifying operation-level specifications and propose a method to create synthetic datasets with clarifications on key operations. The paper also discusses the development of a pipeline for code generation by asking clarification questions. The speaker highlights the importance of addressing the challenge of input under-specification and the potential benefits of using clarification questions to improve code generation.</sample>
    <sample id="354">根据所给的英文内容，直到2023年，CoNLL-2003 和 CoNLL++ 之间的性能增量才高于5个百分点。</sample>
    <sample id="355">Hello, my name is Vasudeva, and I am a computer science PhD candidate at Stony Brook University. I would like to present our work accepted into ACL 2023 as a long paper on transfer learning for dissonance detection addressing the rare class challenge. We begin by defining cognitive dissonance and why it is an important problem to study in language. Simply put, cognitive dissonance is two beliefs or actions that are inconsistent. For example, where a person states, "I know that cigarettes could kill me," and then goes on to say, "I grabbed a couple of smokes after the meeting." This belief and action are inconsistent, and they are in dissonance. Further mentioning that "I don't think I could keep my job without them" justifies the second occurrence and they have a consonance relationship. While dissonance is a very common phenomenon experienced in daily decision-making, they are really rare to find expressed in language among other kinds of discourse relations. So why does this matter? Studying cognitive dissonance can help us understand the effects of dissonance among people, track trends in belief, values, and attitude changes in populations. High cognitive dissonance is also related to anxiety disorders and can help understand people's mental health better. Studying dissonance expressed in language can also be beneficial in understanding extremism and polarization of vulnerable groups. Finally, cognitive dissonance is important to understand personal cognitive styles of individuals and helps us understand decision-making processes better. To the goal of creating a cognitive dissonance resource, we conducted a large-scale annotation of dissonance relations. We used a dissonance first approach as seen in the flowchart here. Tweets were parsed using a PTB parser and pairs of discourse units were annotated according to the guidelines that are described in our paper. As can be seen here, dissonance was only found in 3.5% of the annotated pairs. On collecting around 1,000 examples of discourse unit pairs, we ran training for an initial classifier trained only on 43 examples of dissonance. To no surprise, the classifier performed not much better than chance. Given the low occurrence of dissonance and absence of any prior such data set, we are facing the problem of absolute rarity. To alleviate this, we experiment over combinations of transfer learning and active learning to annotate such that more dissonant samples can be collected over lesser annotation rounds, lowering the overall annotation costs while improving dissonance detection. Since the initial model was not able to capture the dissonance class at all, we start the active learning process by transferring weights from closely related tasks. We transfer from two different tasks: topic-independent dissonance stance classification, a task that determines if two debate statements from different people are in agreement or in disagreement irrespective of topic called debate here, and on binary classification of expansion and comparison classes of PTB, since these two are closely related to the conception of consonance and dissonance, and we call them CE here. We find that on transferring the zero-shot performance on the annotated data set is already much better than chance, with the best with AUC 0.62. Further, on iteratively fine-tuning on both tasks, we find that fine-tuning of CE task followed by further fine-tuning on debate yields a much better zero-shot performance. Thus, this is the model that we use to co-start the active learning. Next, we determine the best method to update a model with new data from each round of active learning and annotations. Cumulative accumulates all the data collected from active annotations so far, whereas iterative updates the model by training on the latest set of data collected. Over the different strategies, we found that cumulative performed equal or better than iterative across the board. Next, to improve the number of dissonance examples, we use a probability of rare class strategy (PRC) to select mostly the examples that are highly likely to be dissonant by the current model at any round of AL. We compare this to the other state-of-the-art AL strategies that are commonly used in the community. We find that the proposed PRC strategy works better than other state-of-the-art strategies, although the difference is small. Note that the performance is significantly lower for random. On further rounds of AL with two best strategies, we improved dissonance classification AUC to point 7.5, which is the best performance that we have on the task so far. We also checked the feasibility of each strategy for annotation quality and cost to annotators. We find that PRC has the highest percentage of dissonance and works best for rare class. However, the annotators also find the examples difficult. In summary, we find that PRC is a simple AL strategy for rare class acquisition and co-starting AL with appropriately designed transfer learning tasks and helps significantly. We also find that iterative update is useful for transfer learning from a different domain, whereas in-domain active annotations benefit from cumulative update. These are the links to our core data set and our paper. Feel free to get in touch with us if you have any questions. Thank you.</sample>
    <sample id="356">根据所提供的英文内容，无法确定论文作者所属的机构。</sample>
    <sample id="357">演讲者的名字是Cui Yuan。</sample>
    <sample id="358">根据所给的英文内容，这篇论文有五位作者。</sample>
    <sample id="359">该方法与专门用于同步语音翻译的当前热门架构进行了比较。</sample>
    <sample id="361">演讲者介绍自己名为Armanin Norbakh，是一名在卡内基梅隆大学语言技术学院攻读计算机科学专业的学生。他同时也担任JPMorgan AI研究团队的研究主任。演讲的主题是“CounterComp”，这是一个专注于使用反事实场景来提高多步骤数量推理的模型性能的研究。多步骤数量推理是指通过执行多个算术运算来回答问题的能力，例如从2019年到2020年的收入变化。然而，当前的主流模型在处理多步骤数量推理任务时表现不佳，尤其是在输出涉及多个步骤的情况下。这是因为这些模型倾向于记住特定的模式，而不是理解问题和答案之间的关系。为了解决这个问题，演讲者提出了一个新方法，即通过分析训练数据中的输入和输出之间的关系，生成反事实场景，并将这些场景作为额外的监督信号添加到模型训练中。这种方法可以提高模型对不同操作和输入的敏感度，从而提高其在多步骤数量推理任务上的性能。</sample>
  </task>
</testset>