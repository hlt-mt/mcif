<testset name="MCIF Baselines" type="output">
  <task track="short" text_lang="it">
    <sample id="0">I modelli linguistici sono addestrati su grandi quantità di dati web, tra cui notizie politiche.</sample>
    <sample id="1">McGill University, Mila e Microsoft Research</sample>
    <sample id="2">Certo, ecco la traduzione in italiano:

"Hi, benvenuti alla nostra presentazione di DEPLAIN, un nuovo corpus per l'annotazione di testi tedeschi al livello del documento e al livello della frase."</sample>
    <sample id="3">Il mio nome è Regina Stodden e mi occuperò della prima parte della presentazione. Iniziamo definendo la semplificazione del testo.</sample>
    <sample id="4">La semplificazione del testo è un processo di adattamento di un testo per migliorare la comprensione del testo per un gruppo specifico di destinatari, come persone con problemi di lettura o non nativi di lingua.</sample>
    <sample id="5">Per addestrare un modello di semplificazione del testo, è necessario un insieme di coppie parallele di testi, ad esempio due documenti o frasi.</sample>
    <sample id="6">In esempio qui, si può vedere un paio di frasi parallele, una frase complessa in tedesco e la sua traduzione in linguaggio semplice.</sample>
    <sample id="7">Per semplificare la frase, diverse tecniche sono possibili come si può vedere nell'esempio, tra cui sostituzione lessicale, clausola di deletto, riformulazione o inserimento di parole.</sample>
    <sample id="8">Ora proponiamo i nostri nuovi corpus di De-Plain, perché in recenti anni c' erano dei problemi con i corpi esistenti. Ad esempio, questi corpi attuali sono troppo piccoli per addestrare un modello di tokenizzazione.</sample>
    <sample id="9">I altri tre modelli proposti recentemente sono tutti automaticamente allineati, cosa che significa che possono essere errori proni nella loro allineazione.</sample>
    <sample id="10">Quindi propone il nuovo corpus di plain, che è diviso in due sottocorpora: plain API e plain web. Plain API è basato su testi di notizie.</sample>
    <sample id="11">In DeepLearn API, si hanno 483 documenti allineati manualmente. Ciò produce circa 30.000-13.000 coppie di frasi.</sample>
    <sample id="12">Per DeepPlainWeb, questa corpora include diverse domenе e inoltre aliniamo tutti questi 750 documenti, da un lato manualmente e dall'altro con metodi di alinamento automatico.</sample>
    <sample id="13">In totale, si arrivano a 34.500 frasi.</sample>
    <sample id="14">Analizziamo i nostri paari di frasi un po' di più. Ad esempio, sul tipo di semplificazione.</sample>
    <sample id="15">Come si può vedere qui, i testi della Bibbia sono molto più semplificati rispetto ad esempio i testi di notizie o i testi per apprendere un nuovo linguaggio.</sample>
    <sample id="16">Su tutto il piano, considerando ad esempio la semplificazione lessicale, la semplificazione strutturale o anche livello generale di semplificazione.</sample>
    <sample id="17">Tuttavia, è evidente che il nostro corpus Deplain ha una ampia varietà di diverse trasformazioni semplificative. Ad esempio, nel corpus Deplain API, abbiamo molti più riformulazioni e aggiunte di parole rispetto al corpus Deplain Web.</sample>
    <sample id="18">Sul lato opposto, nel corpus web abbiamo molto più riformulazioni.</sample>
    <sample id="19">Quindi, vediamo cosa possiamo fare con questo corpus. Ciao, mi chiamo Omar e ora parlerò dei casi d'uso per il nostro dataset DeepL. Per il primo caso, possiamo valutare metodi di allineamento automatico.</sample>
    <sample id="20">Nell'ultimo periodo ci sono stati molti metodi di al線ament, ma in contesto di traduzioni automatiche.</sample>
    <sample id="21">Dove abbiamo due documenti paralleli scritti in lingue diverse e vogliamo estrarre alleanze di frasi nei due documenti.</sample>
    <sample id="22">Ma nel nostro caso di studio stiamo cercando di estrarre alleanze tra le frasi di due documenti paralleli, che hanno lo stesso linguaggio, hanno lo stesso contenuto, ma sono a livello di complessità diverso.</sample>
    <sample id="23">E ora che abbiamo il dataset DeepL con frasi手动对齐的句子，我们可以使用这些句子作为标准对齐来评估一些提出的对齐方法。</sample>
    <sample id="24">E abbiamo fatto alcune modifiche ai metodi proposti e pubblicato tutti questi aggiustamenti e i codici per eseguire i nostri esperimenti nel paper.</sample>
    <sample id="25">All'in fine, si è concluso che il miglior metodo di al线ment automatico da usare per test per semplificazione test in tedesco è il metodo di mass align.</sample>
    <sample id="26">E puoi trovare anche il codice per eseguire questo metodo su documenti personali nel paper.</sample>
    <sample id="27">Il secondo caso d'uso che abbiamo dimostrato nel nostro articolo è il caso di automatic text simplification.</sample>
    <sample id="28">Semplificare il testo da un testo complesso utilizzando modelli di linguaggio fin-tuned</sample>
    <sample id="29">Abbiamo finetuned due modelli. Abbiamo finetuned il modello di long import per produrre semplificazioni a livello di documento.</sample>
    <sample id="30">E anche finetuned il modello base importo per produrre semplificazioni a livello di frase.</sample>
    <sample id="31">Puoi anche trovare tutti i punti di controllo e vedere dettagli più avanzati dei pun分数和评估指标，我们的实验在论文中。</sample>
    <sample id="32">Conclusi che queste, queste basi di addestramento possono produrre o ottenere punteggini migliori dei punteggini di base.</sample>
    <sample id="33">E proponevamo quei risultati come un benchmark, un benchmark base per il problema di semplificazione automatica del testo in futuro.</sample>
    <sample id="34">Grazie mille per la vostra attenzione e speriamo di incontrarci tutti durante il convegno. Grazie.</sample>
    <sample id="35">Kayo Yin</sample>
    <sample id="36">T5 XL model</sample>
    <sample id="37">Sì, i tagger CoNLL-2003 funzionano anche in 2023.</sample>
    <sample id="38">Il nuovo approccio proposto tenta di ridurre la soggettività della valutazione umana.</sample>
    <sample id="39">Il successo dell'attuale approccio scarsamente supervisionato si basa in larga misura sulla presenza di campioni di validazione puliti.</sample>
    <sample id="40">I progressi che possono essere fatti per migliorare il punteggio sono: 1. Identificare e correggere gli errori di grammatica, come la mancanza di un articolo o la scelta sbagliata di un verbo. 2. Utilizzare le risorse di traduzione automatica per tradurre i testi in un'altra lingua e migliorare la comprensione. 3. Utilizzare le tecniche di traduzione umana per tradurre i testi in un'altra lingua e migliorare la comprensione.</sample>
    <sample id="41">Five authors are involved in the article.</sample>
    <sample id="42">Ciao, mi chiamo Adam Przepiorkowski e questa presentazione è sulla struttura di dipendenze della coordinazione.</sample>
    <sample id="43">Come potete vedere, ci sono diverse strutture di dipendenze assunti da diverse teorie e approcci di corpus. Ad esempio, in Universal Dependencies, la struttura della coordinazione "Lisa, Bart e Maggie" è rappresentata come:</sample>
    <sample id="44">È tale che il primo congiunto è la testa della struttura di coordinamento, in questo caso Lisa.</sample>
    <sample id="45">Approcci simili sono assunti in Igore Miltchuk's Meaning Text Theory, dove di nuovo la struttura intera della coordinazione è guidata dal primo congiunto. Questi due approcci sono isomorfi, nonché singolano uno dei congiunti.</sample>
    <sample id="46">Ora, ci sono anche approcci simmetrici per strutture di coordinamento, come il prague approach, l'approccio headed da congiunzione e i diagrammi di dipendenze in prague, dove le strutture di coordinamento vengono guidate dalla congiunzione.</sample>
    <sample id="47">Mentre March ha letto ieri, è peggio. Ecco, tra il verbo e l'oggetto direttore c'è un complemento ieri.</sample>
    <sample id="48">E infine, c'è anche un approccio multi-intestino che è usato ad esempio in DiCatzon's Word Grammar.</sample>
    <sample id="49">Dove, per dire, tutti i congiunti sono testi delle strutture di coordinamento. Così otteniamo dipendenze dal governatore, qui "amas", a tutti i congiunti separatamente. Questi sono Bart e Maggie.</sample>
    <sample id="50">Ora, il mio articolo è destinato a produrre un nuovo argomento per le strutture simmetriche di coordinazione come queste due e contro le strutture asimmetriche di coordinazione come queste.</sample>
    <sample id="51">Ok, l'argomento è basato sul principio di minimizzazione della lunghezza della dipendenza che è spiegato su base di questi esempi.</sample>
    <sample id="52">In inglese, come forse saprai, gli oggetti diretti preferiscono di essere vicini al verbo, mentre i complementi potrebbero essere più lontani. Quindi, "March read it yesterday" è corretto perché l'oggetto direttore "it" è vicino al verbo.</sample>
    <sample id="53">Mentre March ha letto ieri "it" è molto peggio, giusto? Perché tra il verbo e l'oggetto direttore c'è un'agnomento ieri.</sample>
    <sample id="54">Tuttavia, questo effetto può essere attenuato quando il oggetto diretto è molto pesante e lungo, poiché allora può essere spostato alla posizione dopo l'aggetto.</sample>
    <sample id="55">Questa è illustrata qui. Quindi entrambe queste frasi sono corrette. March ha letto questo libro assolutamente affascinante su di loro ieri. Sì, okay. Quindi invece di "it" abbiamo questa lunga NP.</sample>
    <sample id="56">Ma è anche okay dire: Martedì scorso, ho letto un libro davvero affascinante su api.</sample>
    <sample id="57">La ragione qui è che questa è possibile perché anche se questa frase viola il principio grammaticale generale che gli oggetti diretti dovrebbero stare vicino al verbo,</sample>
    <sample id="58">Soddisfa il principio della minimizzazione della lunghezza della dipendenza, che dice che le dipendenze più brevi sono preferite.</sample>
    <sample id="59">Quindi, queste due strutture solamente mostriamo la lunghezza delle dipendenze cruciali, cioè le loro non costanti tra queste strutture.</sample>
    <sample id="60">E qui abbiamo una dipendenza da "red" all'argomento di lunghezza 7, misurato in parole, e da "red" a "book" di lunghezza 4. Quindi, in totale, è 11.</sample>
    <sample id="61">Quando si spostano o si scambiano questi due elementi, la somma delle due dipendenze diventa sei. Invece di undici, sei molto più corto. Quindi, questa suona piuttosto bene. Viola un principio, ma soddisfa un altro.</sample>
    <sample id="62">Ok, allora, cos'abbiamo fatto? Abbiamo estraendo statistici molto importanti da una versione migliorata del banco di test per la grammatica e vedere il paper su come non usare le dipendenze universitarie.</sample>
    <sample id="63">E queste statistiche confermano l'osservazione fatta molte volte prima che i congiunti sinistri tendono a essere più corti. Così, "soltanto" e "non" sono misurati in sillabe.</sample>
    <sample id="64">E anche l'osservazione che è stata fatta in passato che questa tendenza aumenta con la differenza di lunghezza.</sample>
    <sample id="65">Quando la differenza tra le lunghezze dei due congiunti cresce, il congiunto più corto tende a essere il primo e di peso maggiore. Così, la proporcione è più grande per i congiunti più corti.</sample>
    <sample id="66">Ma quello nuovo in questo articolo è che notiamo che questa tendenza si verifica solo quando il governante a sinistra è assente.</sample>
    <sample id="67">Giustamente, nel seguente esempio, il governatore è sulla sinistra: "Ho visto Bart e Lisa." Quindi, il governatore è sulla sinistra.</sample>
    <sample id="68">Il concetto è che in alcune circostanze, come nella seconda esempio "Homer came and sneezed", non c'è un governo esterno all'azione. In queste situazioni, il primo verbo tende ad essere più breve rispetto al secondo. Questo è anche vero quando la differenza tra i due verbi è maggiore.</sample>
    <sample id="69">Tuttavia, quando il governante è sulla destra come qui, l'effetto scompare.</sample>
    <sample id="70">Così dimostriamo che, uh, uh by measuring length in characters that's the first column in syllables, the middle column and in words, the right column. So I'll concentrate on the right one.</sample>
    <sample id="71">Quello che vediamo qui è che quando il governante è sulla sinistra,</sample>
    <sample id="72">La tendenza per il primo concorso di essere più corto cresce gradualmente con la differenza assoluta in parole e lo stesso è osservato quando non c'è un governatore come in coordinazione di frasi ma quando il governatore è sulla sinistra questa tendenza scompare.</sample>
    <sample id="73">E dimostriamo nel paper come questo fornisce un argomento contro strutture di coordinazione asimmetriche, come queste due, e a favore delle strutture simmetriche, come queste.</sample>
    <sample id="74">Vedi il paper per la piena argomentazione e parliamo con noi alla sessione poster. Grazie.</sample>
    <sample id="75">There are three authors involved in the article.</sample>
    <sample id="76">The Bible texts are the most simplified domains.</sample>
    <sample id="77">Salt and pepper.</sample>
    <sample id="78">Sì, i modelli possono essere utilizzati per la tua ricerca.</sample>
    <sample id="79">DEplain-apa contains news texts.</sample>
    <sample id="80">Un modello migliore, un modello più grande e più esempi di adattamento fine.</sample>
    <sample id="81">La tendenza dei congiunti a sinistra a essere più brevi è stata misurata in caratteri, sillabe e parole.</sample>
    <sample id="82">Gli esperimenti sono stati progettati per misurare la lunghezze in caratteri, il primo colonna in sillabili, il medio colonna in parole, e il terzo colonna.</sample>
    <sample id="83">Un classificatore base addestrato su dati non bilanciati non è molto più efficiente rispetto a un'ipotesi di casualità.</sample>
    <sample id="84">There are four authors involved in the article.</sample>
    <sample id="85">Bob e Alice.</sample>
    <sample id="86">I modelli di traduzione sensibili al contesto migliorano rispetto a quelli indipendenti dal contesto per fenomeni del discorso come formalità e coesione lessicale.</sample>
    <sample id="87">I ricercatori che hanno scritto l'articolo sono affiliati a diverse istituzioni, tra cui Johns Hopkins University, Purdue University e MIT.</sample>
    <sample id="122">Il framework quantifica esattamente la posizionalità utilizzando una correlazione di Spearman tra le annotazioni demografiche e le predizioni/modelli.</sample>
    <sample id="155">Lo studio precedente ha dimostrato che i soggetti umani hanno anche rivelato stereotipi razionali quando hanno ricevuto gli stessi prompt di persona.</sample>
    <sample id="156">In questo studio, le fonti di dati utilizzate sono le statistiche estratte dal versione migliorata del Corpus del Banca.</sample>
    <sample id="157">Due.</sample>
    <sample id="158">Le attività strettamente correlate alla dissonanza cognitiva sono la classificazione di due enunciati di debate provenienti da diverse persone come in accordo o in disaccordo, e la classificazione binaria delle classi di espansione e comparazione di PDB.</sample>
    <sample id="159">2</sample>
    <sample id="160">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="161">Il framework differisce dai precedenti in quanto confronta le predizioni dei modelli con le etichette assegnate a diversi utenti, invece di analizzare solo la distribuzione o l'acordo degli annotatori.</sample>
    <sample id="162">GPT-4</sample>
    <sample id="163">Google Translate e DeepL.</sample>
    <sample id="164">Ciao, mi chiamo Shangbin, sono uno studente di dottorato all'Università di Washington. Oggi sto presentando il nostro lavoro sull'evoluzione dei modelli di linguaggio da pre-training dei dati fino alle tareitte downstream, analizzando le tracce dei bias politici che portano a modelli NLP non equi.</sample>
    <sample id="165">I modelli di linguistica sono addestrati su grandi quantità di dati web crawl.</sample>
    <sample id="166">I media di notizie politiche sono bene coperti nei loro dati di addestramento. Secondo un sondaggio del dataset C4, possiamo vedere che il New York Times, Los Angeles Times, The Guardian, Huffington Post ecc. sono bene coperti in linguaggi di modello di addestramento.</sample>
    <sample id="167">Questo ha creato un benessere misto per le applicazioni dei modelli di linguaggio.</sample>
    <sample id="168">Sul lato di un lato, essi erano in grado di imparare da diverse prospettive che celebrano la democrazia e la pluralità di idee. Sull'altro lato, queste diverse opinioni politiche sono inerentemente socialmente biasate e potrebbero portare a problemi di giustizia per i sistemi di apprendimento automatico.</sample>
    <sample id="169">Per raggiungere questo scopo, proponiamo di indagare il percorso di propagazione dei bias politici dalla fase di addestramento dei dati, attraverso i modelli linguistici, fino alle attività downstream. In modo specifico, chiediamo le seguenti domande:</sample>
    <sample id="170">Prima di tutto, come valutiamo la linearità politica dei modelli di linguaggio e in che modo i dati di pretraining potrebbero influenzare tale bias politico?</sample>
    <sample id="171">Secondariamente, come i modelli di linguaggio con diverse intenzioni politiche si comportano sulle attività downstream e se queste potrebbero portare a problemi di giustizia nella programmazione di applicazioni NLP?</sample>
    <sample id="172">In particolare, innanzitutto propone di stimare modelli di linguaggio con diverse formattazioni dei prompt utilizzando domande politiche come il test del compasso politico. Questo ci assicura di fare un valutazione automatica ben radicata nella letteratura di scienze politiche.</sample>
    <sample id="173">Quindi i risultati preliminari dimostrano che i modelli di linguaggio hanno tendenze politiche varianti. Occupano tutti e quattro i quadranti del compasso politico.</sample>
    <sample id="174">Possiamo anche vedere che GPT-4 è il modello linguistico più liberale tra tutti e che le serie GPT sono in generale più liberali rispetto alle serie BERT e le sue variazioni.</sample>
    <sample id="175">In secondo luogo, ci proponiamo di indagare fin a che punto, fin a che punto, le inclinazioni politiche dei modelli di linguaggio vengono davvero acquisite dai dati di addestramento.</sample>
    <sample id="176">Così, potremmo condurre un esperimento controllato utilizzando modelli di linguaggio pre-addestrati su sei diversi gruppi di opinioni politiche, separati in notizie e social media, ulteriormente divisi in base alla loro orientazione politica.</sample>
    <sample id="177">Inoltre, pretrainare modelli di linguaggio su dataset di spostamento ideologico può far vedere che le coordinate ideologiche del modello si spostano corrispondentemente.</sample>
    <sample id="178">Ad esempio, per RoBERTa, che è stato ulteriormente finetuned e ulteriormente addestrato su un corpus di Reddit sinistra, possiamo vedere un sottile spostamento liberale in termini del suo</sample>
    <sample id="179">In termini di bias politici.</sample>
    <sample id="180">E anche cercano di indagare se i modelli di linguaggio possono catturare la polarizzazione che è prevalentemente nella nostra società moderna.</sample>
    <sample id="181">Quindi, suddividiamo il pre-training di GPT-2 in due periodi: prima del 45° Presidente degli Stati Uniti e dopo il 45° Presidente degli Stati Uniti. Separatamente, pretrainiamo modelli di linguaggio su questi due periodi temporali differenti.</sample>
    <sample id="182">Possiamo vedere che i modelli di linguaggio in generale hanno un orientamento politico che è più distante dal centro dopo il 2017. Questo indica che i modelli di linguaggio possono anche catturare la polarizzazione nella nostra società.</sample>
    <sample id="183">Così, nonostante tutto, valutiamo i modelli di linguaggio con diverse inclinazioni politiche in termini di rilezione di discorso razzista e rilezione di notizie fake. Questi due applicativi di NLP spesso coinvolgono modelli di linguaggio e potrebbero avere implicazioni molto significative.</sample>
    <sample id="184">Così vediamo che se esaminiamo la performance per category, che è di dire se separiamo la performance in due performance,</sample>
    <sample id="185">Different demographics or politically leaning news media, we can see a pattern that for example, for hate speech detection, left-leaning language models are better.</sample>
    <sample id="186">Effettua la traduzione del contenuto inglese in italiano.</sample>
    <sample id="187">Tuttavia, i nostri modelli sono peggiori nel rilevare l'insulto che si riferisce a gruppi più potenti nella nostra società.</sample>
    <sample id="188">E viceversa, i modelli di linguaggio di destra sono migliori nel rilevare la discriminazione razziale che mira a gruppi minoritari come i neri e le donne, tuttavia peggiorano nel rilevare la discriminazione razziale che mira a gruppi minoritari come i neri, i transessuali e altri gruppi minoritari.</sample>
    <sample id="189">Sono anche presenti tendenze simili per la deteczione di notizie fake, in cui si osserva che i modelli di linguaggio sinistra sono migliori nel rilevare la menzogna da parte dei politici di destra e viceversa.</sample>
    <sample id="190">Inoltre, mostriamo molti esempi qualitativi per vedere che i modelli di linguaggio con diverse inclinazioni politiche</sample>
    <sample id="191">Dà diverse predizioni per esempi di discorso razzista e disinformazione in base alla propria classe sociologica. Ci sono un sacco di esempi aggiuntivi nell'appendice per sottolineare meglio questo fatto.</sample>
    <sample id="192">Questo indica che ci sono problemi di giustizia che sono molto preoccupanti in relazione alle inclinazioni politiche dei modelli linguistici.</sample>
    <sample id="193">Ad esempio, se un modello di linguaggio pre-addestrato a rilevare e impedire l'abuso o la disinformazione e simili fosse addestrato e distribuito su una piattaforma di social media popolare,</sample>
    <sample id="194">Questa sarebbe un significato che i gruppi minoritari potrebbero essere marginalizzati e che le espressioni di odio che si riferiscono a gruppi minoritari potrebbero prosperare senza alcun controllo.</sample>
    <sample id="195">Quindi, questa ha suonato l'allarme per noi di riconoscere e affrontare i problemi di giustizia derivanti dai bias politici dei modelli linguistici.</sample>
    <sample id="196">Un po' di discussione. Vorremmo anche sottolineare che espongiamo il dilemma unico riguardante i bias politici dei modelli del linguaggio. Si tratta di un dilemma tra Scylla e Caribdà.</sample>
    <sample id="197">Se non sanizziamo le opinioni politiche nel dataset di addestramento del modello linguistico, i bias potrebbero propagarsi dal dataset di addestramento ai modelli linguistici e fino alle attività downstream, creando infine problemi di giustizia.</sample>
    <sample id="198">Se cerchiamo di sanitizzare in qualche modo, dobbiamo anche rischiare la censure o l'esclusione e è incredibilmente difficile determinare cosa è realmente neutra e dovrebbe essere ritenuta data di linguaggio monitorata. Quindi è come il problema elettrico di Charlie.</sample>
    <sample id="199">Ok, great. I think that's pretty much all I have to add. Thank you for your time.</sample>
    <sample id="200">Cinque.</sample>
    <sample id="201">Le valutazioni MPP sono state eseguite fino a 900 token di lunghezza del contesto.</sample>
    <sample id="202">The domains included in their dataset are music, fiction, and foreign.</sample>
    <sample id="203">La posizionalità è semplicemente la prospettiva che le persone hanno come risultato dei propri demografici, identità e esperienze di vita.</sample>
    <sample id="204">Dawei Zhu</sample>
    <sample id="205">No, non adatta un modello ST offline esistente.</sample>
    <sample id="206">Cinque.</sample>
    <sample id="207">No, i modelli non funzionano sulla suite di test.</sample>
    <sample id="208">Le varianti di KITMUS sono: Background Pretrain, Background Both e Background Infer.</sample>
    <sample id="209">Google Research</sample>
    <sample id="210">The last research question is whether we should only use the clean samples for validation or are there better ways to utilize them.</sample>
    <sample id="211">La sensibilità misura la capacità del modello di produrre gli stessi output per la stessa attività, indipendentemente da eventuali variazioni nella formula di istruzione.</sample>
    <sample id="212">Jingwei Yi</sample>
    <sample id="213">A higher sensitivity indicates a better performance of the model.</sample>
    <sample id="214">I modelli vengono addestrati su un vasto contesto linguistico.</sample>
    <sample id="215">In genere, sono necessari 20 campioni di convalida puliti per il raggiungimento di buone prestazioni in WSL.</sample>
    <sample id="216">Myra Cheng, Esin Durmus and Dan Jurafsky are affiliated with Stanford Engineering Computer Science.</sample>
    <sample id="217">Perché i modelli di linguaggio hanno tendenze politiche.</sample>
    <sample id="218">The name of the presenter is not mentioned in the given information.</sample>
    <sample id="219">L'infrastruttura di propagazione dei bias politici ha due aspetti. Di un lato, i modelli possono imparare da diverse prospettive che celebrano la democrazia e la pluralità di idee. Dall'altro lato, queste diverse opinioni politiche possono essere socialmente biasse e potenzialmente portare a problemi di giustizia nella realizzazione di compiti di downstream.</sample>
    <sample id="220">Sì, il processo di semplificazione differisce per DEplain-apa e web.</sample>
    <sample id="221">No, Coscript non è pubblicamente disponibile.</sample>
    <sample id="222">La filigrana è esattamente inclusa quando il numero di trigger nel testo è maggiore di m.</sample>
    <sample id="223">I ricercatori che hanno scritto l'articolo sono affiliati all'Università del Pennsylvania e all'azienda Amazon.</sample>
    <sample id="224">Sì, i modelli codificatore-decodificatore come mt5 possono migliorare con l'addestramento su una combinazione di lingue.</sample>
    <sample id="225">Un esempio di pianificazione linguistica vincolata è come fare un pasticcino di cioccolato.</sample>
    <sample id="226">Gli autori si accertano della segretezza del loro metodo visualizzando le rappresentazioni dei testi in un diagramma di scatter.</sample>
    <sample id="227">Il lavoro utilizza i PLM esistenti per costruire uno nuovo utilizzando un modello pre-addestrato.</sample>
    <sample id="228">GPT-4 is least aligned with Catholic Europe.</sample>
    <sample id="229">La relatrice illustra come il modello utilizza la conoscenza acquisita attraverso il meccanismo dell'attenzione in una frase che non è specificata nel testo fornito.</sample>
    <sample id="230">La quantità di attività influisce sulla performance del modello in modo che, al crescere del numero di attività, il modello raggiunge un'efficienza migliore e allo stesso tempo una sensibilità più bassa.</sample>
    <sample id="231">LSTM-seq2seq, Tree2Seq, e Zheng and Lapata.</sample>
    <sample id="232">I due coautori sono gli advisor del primo autore.</sample>
    <sample id="233">The first author of PaLM is Chowdery et al.</sample>
    <sample id="234">Ciao a tutti, mi chiamo Jenny e sono un primo anno PhD student a Carnegie Mellon University. Oggi presenterò il mio lavoro, NL Positionality: Characterizing Design Biases of Datasets and Models.</sample>
    <sample id="235">Questa opera è stata realizzata in collaborazione con alcuni esperti dell'Università di Washington e dell'Institute for AI dell'Alleanza, tra cui Sebastien Santi, Ronan Le Bras, Katharina Reincke e Maarten Sap.</sample>
    <sample id="236">Immaginiamo di essere in un giornale e di staccare i commenti da un articolo di notizie cercando di rimuovere il contenuto tattico.</sample>
    <sample id="237">You might turn towards a popular API like Perspective API for toxicity detection and this works really well if you're Carl Jones where Perspective API is able to detect correctly toxic instances.</sample>
    <sample id="238">Ma non è proprio così per Aditya Sharma, dove l'API di prospettiva non è né sensibile a termini offensivi che sono più comuni in contesti indiani.</sample>
    <sample id="239">Ecco un esempio di bias di progettazione, in cui vediamo differenze sistematiche di prestazioni della tecnologia tra popolazioni.</sample>
    <sample id="240">Le bias di progettazione come quelli che abbiamo appena visto prima potrebbero essere dovuti alla posizionalità dei ricercatori e sviluppatori di modelli NLP. La posizionalità è semplicemente le prospettive che le persone hanno come risultato dei propri demografici, identità e esperienze di vita.</sample>
    <sample id="241">Questo è un concetto ampiamente utilizzato in studi critici, in particolare in ambienti accademici femministi e queer.</sample>
    <sample id="242">Come ricercatore, la posizionalità può influenzare il processo di ricerca e i suoi risultati, poiché può modificare le decisioni che i ricercatori prendono.</sample>
    <sample id="243">Ecco il contenuto in italiano:

Ecco una domanda che le persone potrebbero fare: hanno i set di dati e i modelli una posizionalità?</sample>
    <sample id="244">E non stiamo cercando di dire che i modelli e i dataset in sé stessi hanno identità demografiche e esperienze di vita, ma essi aggregano giudizi e opinioni di persone reali e possono pertanto rappresentare certe posizionalità su altre.</sample>
    <sample id="245">Il mio lavoro ha suggerito alcune prove anecdettiche di avere la capacità di posizionarsi, come i gap culturali tra i modelli e i set di dati, nonché le definizioni teoriche della posizionalità dei modelli.</sample>
    <sample id="246">Tuttavia, queste opere non esaminano realmente la confrontazione degli utenti con i set di dati e i modelli stessi.</sample>
    <sample id="247">Studiare la posizionalità dei modelli e dei dataset è sempre più importante, poiché i test NLP diventano sempre più soggettivi e orientati alla società.</sample>
    <sample id="248">È difficile caratterizzare come queste posizionalità siano inclinate in quanto non tutti i decisioni sono documentati e molti modelli sono nascosti dietro API.</sample>
    <sample id="249">Per studiare la posizionalità dei dataset e dei modelli, confrontiamo le annotazioni con reali utenti con i dataset e i modelli esistenti.</sample>
    <sample id="250">Noi lo facciamo attraverso un framework chiamato NLPositlonality.</sample>
    <sample id="251">Il framework funziona in due passi principali.</sample>
    <sample id="252">Il primo passo è rerelezionare i set di dati con diverse annotatori.</sample>
    <sample id="253">Ecco un riassunto in italiano del contenuto in inglese:

Il diagramma rappresenta il processo di creazione di un dataset per un modello di apprendimento automatico. Inizialmente, un insieme di immagini viene selezionato e suddiviso in due classi: "Mangia con le mani" e "Rid". Ognuna di queste classi è quindi inviata a diversi annotatori per ottenere etichette. Questi etichettatori assegna una etichetta ad ogni istanza dell'immagine, creando un insieme di etichette diverse per ogni istanza.

Successivamente, le etichette ricevute vengono analizzate per determinare se sono coerenti tra loro. Se le etichette sono coerenti, il modello viene addestrato utilizzando queste etichette. Se le etichette non sono coerenti, il modello viene addestrato utilizzando solo le etichette coerenti.

Infine, il modello viene valutato utilizzando un insieme di statistiche demografiche, come l'età, il genere, la razza, l'istruzione e il paese. Queste statistiche sono utilizzate per misurare la separazione demografica del modello e per migliorare la sua precisione.

In generale, il diagramma rappresenta il processo di creazione di un dataset per un modello di apprendimento automatico, utilizzando etichette diverse per ogni istanza dell'immagine e valutando il modello utilizzando statistiche demografiche.</sample>
    <sample id="254">Ecco un riassunto in italiano del contenuto in inglese:

Iniziamo con la raccolta di un dataset, che viene successivamente processato per estrarre informazioni demografiche come età, genere, razza, istruzione e paese. Queste informazioni vengono utilizzate per creare un modello di previsione che prevede le età, i generi, le razze, gli indirizzi di istruzione e i paesi.

Successivamente, il modello viene utilizzato per prevedere queste informazioni per un insieme di immagini. Ogni immagine viene analizzata separatamente e le previsioni del modello vengono registrate.

Infine, le annotazioni ricevute vengono analizzate per determinare quanto si differiscono da etichette di riferimento. Questo processo è chiamato riconoscimento di separazione di demografia.</sample>
    <sample id="255">Poi prendiamo le annotazioni per demografia e le confrontiamo con i modelli e i dataset utilizzando il coefficiente di correlazione di Pearson.</sample>
    <sample id="256">Il framework in question differisce dalla letteratura sulla disuguaglianza tra gli annotatori, poiché confronta le predizioni e le etichette dei modelli e dei set di dati con le annotazioni degli utenti finali. Questo approccio si differenzia da quelli che si concentra solamente sull'interannotatore agreement o la modellazione delle distribuzioni degli annotatori.</sample>
    <sample id="257">Il nostro framework è in gran parte abilitato attraverso Lab in the Wild, una piattaforma di crowdsourcing online per i collaboratori HCHI.</sample>
    <sample id="258">Lab in the Wild è una piattaforma di esperimentazione online che consente di reclutare volontari diversi rispetto alle piattaforme come MTurk, che hanno prevalentemente partecipanti provenenti dagli Stati Uniti o dall'India. Inoltre, Lab in the Wild è capace di ottenere dati di alta qualità.</sample>
    <sample id="259">Noi organizziamo due task su Lab of the Wild, uno dei quali è il social acceptability. E la maniera in cui funziona è che i partecipanti leggeranno una situazione dalla Social Chemistry Dataset e quindi voteranno su quanto è socialmente accettabile una situazione.</sample>
    <sample id="260">Successivamente, per rimanere in contatto con la ricercata, i partecipanti possono confrontare le proprie risposte con quelle di un AI e di altri.</sample>
    <sample id="261">Iniziamo confrontando queste annotazioni con la chimica sociale, Delphi e GPT-4.</sample>
    <sample id="262">In seguito, abbiamo replicato un setup molto simile per la task di rilevamento di tossicità e di discriminazione del linguaggio razzista, in cui i partecipanti leggevano un istanza dalla dinahate e scrivevano se pensavano che fosse un'istanza di linguaggio razzista.</sample>
    <sample id="263">Poi confrontiamo queste annotazioni con Dynahate, Perspective API, Rewire API, Hate RoBERTa e GPT-4. Nostro studio in fine ha raccolto oltre 16.000 annotazioni da più di un migliaio di annotatori provenienti da 87 paesi.</sample>
    <sample id="264">Ora siamo meglio equipaggiati per rispondere a chi fanno alleanze più con i set di dati e i modelli NLP. Troviamo che ci sono posizionalità in NLP.</sample>
    <sample id="265">Ad esempio, scopriamo che i set di dati e i modelli sono più legati ai paesi che parlano inglese. Quindi, per l'analisi di accettabilità sociale del GPT-4, scopriamo che è più legato ai paesi confessioni e che parlano inglese. Troviamo anche che Dina ha è più legato ai paesi che parlano inglese.</sample>
    <sample id="266">Inoltre, abbiamo anche scoperto un maggior allineamento con le persone che hanno un'istruzione universitaria. Per GPT-4 in compito di accettabilità sociale, abbiamo scoperto che è maggiormente allineato con le persone che hanno un'istruzione universitaria o un'istruzione di studio superiore.</sample>
    <sample id="267">E troppo lo stesso per Dynahate, dove è maggiormente allineato con le persone che hanno un'istruzione universitaria.</sample>
    <sample id="268">Tuttavia, quando i modelli e i set di dati vengono allineati a specifici popolazioni, alcune vengono inevitabilmente lasciate indietro.</sample>
    <sample id="269">Ad esempio di questo è che i set di dati e i modelli sono meno allineati con le persone non binarie rispetto ai controparti di uomo e donna. Troiamo questo nel compito di accettabilità sociale GPT-4, nonché nell'analisi del compito di diuturnaite.</sample>
    <sample id="270">Quindi, data la presenza di posizionalità in NLP, cosa possiamo fare?</sample>
    <sample id="271">Abbiamo alcune raccomandazioni per questo. La prima è mantenere un registro di tutti i design rilevanti attraverso il processo di ricerca e l'altra è fare ricerche NLP con la lente di presentativismo.</sample>
    <sample id="272">La nostra terza raccomandazione è di costruire insieme dataset e modelli specializzati per e per specifici gruppi di persone. Un esempio di questo è l'iniziativa Masakhane. Vogliamo sottolineare che l'AI inclusiva non è solo fare in modo che tutte le tecnologie funzionino per tutti.</sample>
    <sample id="273">Ecco il contenuto in italiano:

Grazie per la presentazione! Se vuoi imparare di più, ti invito a visitare il nostro dashboard per i risultati più aggiornati dell'analisi e il nostro articolo. Grazie per l'attenzione!</sample>
    <sample id="274">The speaker mentions three problems associated with SimulST models.</sample>
    <sample id="275">Un modo efficace per mitigare i bias sociali e politici nei set di dati durante l'addestramento dei modelli di NLP potrebbe essere l'uso di tecniche di pulizia dei dati, come la rimozione di informazioni sensibili o la normalizzazione dei testi. Tuttavia, è importante notare che la pulizia dei dati non è un processo semplice e potrebbe anche portare a censure o esclusioni.</sample>
    <sample id="276">Ciao, mi chiamo Siyu Yuan e sono qui per presentare il nostro lavoro: "Distilling Script Knowledge from Large Language Models for Constrained Language Planning".</sample>
    <sample id="277">In vita quotidiana, gli esseri umani spesso pianificano le loro azioni seguendo istruzioni passo-passo in forma di script garantiti.</sample>
    <sample id="278">Il precedente lavoro ha sfruttato modelli di linguaggio per pianificare azioni astratte di attività stereotipate, come fare un pasticcino, e ha dimostrato che i modelli di linguaggio grandi possono efficacemente decomporre gli obiettivi in passaggi.</sample>
    <sample id="279">Tuttavia, i precedenti studi hanno maggiormente focalizzato sul pianificare per obiettivi astratti di azioni teoriche. Pianificare per obiettivi con specifiche condizioni, come fare un pasticcio di cioccolato, rimane sottostudiato.</sample>
    <sample id="280">In this paper, we define the problem of constrained language planning.</sample>
    <sample id="281">Qualsiasi obiettivo può essere ereditato da diversi obiettivi specifici reali con constraint multi-faceted. Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli ai constraint.</sample>
    <sample id="282">In questo articolo, valutiamo e miglioriamo la capacità di pianificazione linguistica con vincoli dei modelli di lingua grande.</sample>
    <sample id="283">Non esiste alcuna data set specifico per testare i nostri modelli.</sample>
    <sample id="284">Prima di tutto dobbiamo acquisire queste informazioni. Come si vede nella tabella, estendiamo gli obiettivi astratti con constraint multi-fattori per umani in una data di acquisizione. Utilizziamo il modello GPT-3.</sample>
    <sample id="285">Samo 100 specifici obiettivi e valutiamo i script generati da modelli di grandi lingue.</sample>
    <sample id="286">Questa tabella riporta l'accuratezza generale dei risultati. Abbiamo scoperto che tutti i modelli di linguaggio ottengono risultati insoddisfacenti per pianificare per obiettivi specifici.</sample>
    <sample id="287">Allora, eseguiamo un'analisi dettagliata per indagare i modelli di LLM.</sample>
    <sample id="288">I risultati del grafico dimostrano che la completezza semantica dei script generati è accettabile, ma la fedeltà ai vincoli non può essere garantita.</sample>
    <sample id="289">Dobbiamo suddividere i limiti in category più specifiche come quelli definiti in WikiHow. Il diagramma nella figura dimostra che il rendimento nel pianificare di InstructGPT varia considerabilmente per le diverse category dei limiti.</sample>
    <sample id="290">Studi precedenti hanno dimostrato che l'output dei modelli di lingua generati ha un'alta varianza, portando a prestazioni peggiorate. Di conseguenza, adottiamo l'idea di un filtra Z sovragerenata per migliorare la qualità dell'generazione.</sample>
    <sample id="291">Prima di tutto, mostriamo tipi di vincoli con esempi per il modello GPT-3 e otteniamo obiettivi specifici sulla base dei relativi obiettivi astratti.</sample>
    <sample id="292">Allora, istruzioni GPT-3 per generare script candidati per obiettivi specifici.</sample>
    <sample id="293">Successivamente, un modello di filtraggio viene sviluppato per selezionare i script più pertinenti.</sample>
    <sample id="294">Convertiamo i script e gli obiettivi in embeddings di InstructGPT e calcoliamo la similarità coseno e i punteggi di similarità per misurare la somiglianza semantica.</sample>
    <sample id="295">In addition, we will observe the script that contains the keywords of the target constraint. We only keep the script if the target goal score is the highest in the goal set.</sample>
    <sample id="296">Con il nostro metodo, InstructGPT può generare schemi di alta qualità. Il nostro metodo migliora grandemente la pianificazione sia in termine di completezza semantica che di fedeltà alle vincoli.</sample>
    <sample id="297">Poiché i modelli di linguaggio sono costosi da deploy, è essenziale abilitare la capacità di pianificazione linguistica in modelli più piccoli e specializzati. Creare un dataset è un passo essenziale per raggiungere questo obiettivo.</sample>
    <sample id="298">Tuttavia, studi precedenti non hanno utilizzato pianificazione per obiettivi specifici e l'annotazione manuale dei set di dati è costosa.</sample>
    <sample id="299">Questa è la nostra idea di distillazione del sapere simbolico per estrarre i set di dati di pianificazione linguistica con vincoli da modelli di lingua grande.</sample>
    <sample id="300">Appliamo il nostro metodo per la costruzione di un dataset di pianificazione linguistica con vincoli, denominato CoScript.</sample>
    <sample id="301">In totale, generiamo 55.000 scenari specifici con script per assicurare la qualità della valutazione e dei set di test. Richiediamo a worker cloud-sourced di trovare e revisionare gli esempi incorretti.</sample>
    <sample id="302">Questa figura illustra la distribuzione dei vincoli per CoScript. Abbiamo notato che CoScript ha un'alta pluralità nei generati obiettivi specifici. Con CoScript, possiamo trainare modelli più piccoli ma specializzati per pianificazione linguistica con vincoli.</sample>
    <sample id="303">Abbiamo scoperto che i modelli più piccoli, finetunati su Coreset, possono generare script di alta qualità rispetto a molti modelli di lingua grandi, indicando che i modelli più piccoli possono superare i modelli più grandi quando probabilmente addestrati su dataset più appropriati.</sample>
    <sample id="304">In sintesi, stabilimmo il problema di pianificazione linguistica con vincoli. Evaluammo la capacità di pianificazione linguistica con vincoli dei modelli di lingua grande e svilupparamo un metodo per generare troppe informazioni e filtrarle per i modelli di lingua grande.</sample>
    <sample id="305">Utilizziamo modelli di linguistica a lungo termine per generare un insieme di dati di alta qualità, CoScript, per pianificazione linguistica con vincoli. Speriamo che il dataset CoScript possa essere un risorsa utilizzabile per avanzare la ricerca sulla pianificazione linguistica.</sample>
    <sample id="306">Grazie per il tuo tempo. Per favore, fornisci maggiori dettagli sul co-scrivere in nostra pubblicazione.</sample>
    <sample id="307">La fluidità di PaLM è comparabile a quella dei sistemi d'arte migliori.</sample>
    <sample id="308">Le proprietà importanti di un metodo di filigrana sono che dovrebbe essere applicabile a embedding as services, non dovrebbe indebolire l'utility dei forniti embeddings, dovrebbe essere abbastanza nascosto per l'attaccante per poter rimuovere facilmente la filigrana e infine dovrebbe essere trasferibile alle servizi dell'attaccante durante il processo di estrazione del modello.</sample>
    <sample id="309">I discorsi TED tradotti in inglese sono stati tradotti in 14 lingue diverse.</sample>
    <sample id="310">Ogni istanza viene campiata una volta.</sample>
    <sample id="311">La differenza tra i set di dati benigni e backdoor viene misurata utilizzando la differenza di coseno e la differenza L2.</sample>
    <sample id="312">I modelli basati su codificatori multilingue sono stati utilizzati per valutare due gruppi di modelli, inclusi i codificatori PDR e i modelli ad encoder-decoder.</sample>
    <sample id="344">Gli autori decidono quali sono le parole a frequenza moderata contando la frequenza delle parole in un corpus di testo generale.</sample>
    <sample id="345">Ciao a tutti, il mio nome è Shuheng. Oggi presenterò il mio articolo "Do CoNLL-2003 named entity taggers still work well in 2023?". Iniziamo.</sample>
    <sample id="346">Il nostro articolo ha indagato il problema della generalizzazioni utilizzando la compito di riconoscimento di entità denominata o il compito di riconoscimento di entità (NER).</sample>
    <sample id="347">Siamo in grado di osservare che i modelli hanno utilizzato CoNLL-2003 per sviluppare NER per quasi venti anni. Questo naturalmente solleva diversi problemi. In primo luogo, questi modelli possono generalizzare a dati moderni?</sample>
    <sample id="348">E quando sviluppiamo nuovi tagger, cosa è necessario per una buona generalizzazione?</sample>
    <sample id="349">Allo stesso tempo, se osserviamo una scarsa generalizzabilità, cosa causa la svalutazione di queste modele?</sample>
    <sample id="350">Per esaminare questi problemi, sviluppiamo il dataset ConLL++. Questo è un dataset che abbiamo raccolto da Reuters News nel 2020 e lo abbiamo annotato con le stesse direttive di annotazione di ConLL-2003.</sample>
    <sample id="351">Poi abbiamo allenato più di venti modelli su CoNLL-2003. Abbiamo valutato questi modelli sia sul set di test CoNLL-2003 che sul set di test CoNLL++</sample>
    <sample id="352">Infine, se non per ultima, calcoliamo il percentuale di cambiamento in F1 per valutare la generalizzabilità di ciascun modello.</sample>
    <sample id="353">Quindi, cosa è necessario per una buona generalizzazione? In tutti gli esperimenti, abbiamo scoperto che ci sono tre ingredienti principali che sono necessari.</sample>
    <sample id="354">La prima è la struttura del modello. Nelle nostre esperienze, abbiamo scoperto che i modelli Transformer normalmente generalizzano meglio a nuovi dati.</sample>
    <sample id="355">Il secondo ingrediente è la dimensione del modello. Abbiamo scoperto che di solito i modelli più grandi portano a una migliore generalizzazione.</sample>
    <sample id="356">Infine, non ultima cosa, sappiamo tutti che il numero di esempi di finetuning direttamente influenza il rendimento di un任务 downstream. Qui abbiamo anche scoperto che più esempi di finetuning realmente conducono a una migliore generalizzazione.</sample>
    <sample id="357">Passiamo alla nostra prossima domanda: cosa causa la diminuzione dei performance di alcuni modelli?</sample>
    <sample id="358">Abbiamo due ipotesi. La prima è l'adattamento all'overfitting, che è l'overfitting causato da riutilizzare lo stesso set di test continuamente e questa spesso si manifesta come la diminuzione dei risultati su un nuovo set di test.</sample>
    <sample id="359">La seconda ipotesi è l'overfitting temporale, che è la degradazione del prestabilito causata da un intervallo temporale crescente tra i dati di training e i dati di test.</sample>
    <sample id="360">Per l'overfitting adattivo, come si può vedere dal grafico sulla destra, la linea di migliore ajuste rossa ha una gradiente che è maggiore di 1.</sample>
    <sample id="361">Questa significa che ogni unità di miglioramento che abbiamo fatto su CoNLL 2003 si traduce in più di una unità di miglioramento su CoNLL ++, il che significa che non ci sono ritorni decrescenti.</sample>
    <sample id="362">E questo ci dimostra che l'overfitting adattivo in questo caso non è osservato.</sample>
    <sample id="363">Quindi, cosa c'è con lo scorrimento temporale?</sample>
    <sample id="364">Per la drift temporale, abbiamo svolto un esperimento per retrain o continuare a pretrainare alcuni modelli con più recenti dati. Abbiamo scoperto che il performance degrada con un intervallo temporale più grande.</sample>
    <sample id="365">E questo conferma la nostra ipotesi che il principale fattore della diminuzione delle prestazioni è lo scarto temporale.</sample>
    <sample id="366">La nostra conclusione è che per una buona generalizzazione, saremmo in grado di ottenere un'architettura del modello migliore, dimensioni del modello più grandi, inoltre esempi di finetuning più numerosi. Questi obiettivi sono interconnessi e non possiamo avere un solo ingrediente, ma bisogna combinare tutti gli altri.</sample>
    <sample id="367">Allo stesso tempo, abbiamo anche scoperto che il drop di prestazione qui è causato da drift temporale e, sorprendentemente, non è causato da overfitting adattivo anche se CNN 2003 è stata utilizzata per più di venti anni.</sample>
    <sample id="368">Quindi tornando alla domanda che avevamo sollevato in intesto del nostro articolo, se i tagger di CoNLL-2003 continuano a funzionare nel 2023? E abbiamo scoperto che la risposta è, in reality, un risolutivo sì.</sample>
    <sample id="369">Speriamo che il nostro articolo suscitasse più ricerche su come migliorare la generalizzabilità dei modelli.</sample>
    <sample id="370">Infine, assicurati di controllare il nostro articolo e il dataset. Se hai qualche domanda, non esitare a contattarmi. Ti ringhio mille volte.</sample>
    <sample id="397">Il segmento parlato utilizza un approccio di dimensione 10.</sample>
    <sample id="398">Per risolvere il pronome "he" in questo esempio, è necessario conoscere che Servin è un giudice e che i giudici decidono casi in una corte.</sample>
    <sample id="399">La qualità dell'esempio è più importante del fattore somiglianza con la frase sorgente.</sample>
    <sample id="400">L'articolo si concentra su GPT-4 e GPT-3.</sample>
    <sample id="401">Il modello utilizza i punteggi di attenzione di un livello specifico.</sample>
    <sample id="402">Usare un nome de la canción o su posición.</sample>
    <sample id="403">I ricercatori che hanno scritto l'articolo sono affiliati all'University of Toronto e a Brain Technologies Inc.</sample>
    <sample id="404">There are five authors involved in the article.</sample>
    <sample id="405">No, l'approccio standard non è utilizzarlo per tradurre la query in linguaggio naturale utilizzando un modello di traduzione automatica prima del parsing semantico.</sample>
    <sample id="406">Un esempio fornito è "un warrior (unmarked) vs. a woman warrior (marked)".</sample>
    <sample id="407">Le architetture dei modelli che non generalizzano adeguatamente sono le altre, a parte le modelli Transformer.</sample>
    <sample id="408">The names of the test sets are FTw, COSINE, L2R, BERT, and MLC.</sample>
    <sample id="409">There are five authors involved in the article.</sample>
    <sample id="410">L'autore opera con più modalità.</sample>
    <sample id="439">The integration of pre-training and inference time knowledge in NLU tasks.</sample>
    <sample id="440">I relatori sono Ying, Jiayang e il mio collega.</sample>
    <sample id="441">Sì, il Coscript è stato sottoposto a controlli di qualità.</sample>
    <sample id="442">Le risorse esistenti per la traduzione dipendente dal contesto hanno limiti relativi a tipi di traduzioni dipendenti dal contesto e set di lingue.</sample>
    <sample id="443">Ciao! Sto per parlare del nostro lavoro su risoluzione di espressioni dirette indirette per la selezione di entità in cui introduciamo il Corpus AltEntities.</sample>
    <sample id="444">Il mio nome è Javad Hosseini e questa è un lavoro con Filip Radlinski, Silvia Paretti e Annie Louise.</sample>
    <sample id="445">Il nostro obiettivo è di comprendere il linguaggio degli utenti quando vogliono prendere una decisione. Considera questa domanda alternativa: "Hai detto easy on me o I gotta feeling?" Qui, l'utente vuole scegliere tra due canzoni.</sample>
    <sample id="446">La cosa più ovvia è usare un riferimento diretto, ad esempio, dire il nome della canzone "Easy on Me" o la sua posizione, la prima.</sample>
    <sample id="447">Ma a volte, un riferimento indiretto è più appropriato per avere una conversazione più naturale. Questo potrebbe succedere quando l'utente non ricorda il nome della canzone.</sample>
    <sample id="448">All the pronunciations are too similar to each other and hard to disambiguate.</sample>
    <sample id="449">O quando l'utente vuole specificare un preferimento. Ecco alcuni esempi di riferimenti diretti: ad esempio, "il nuovo", o "la canzone che non è energica".</sample>
    <sample id="450">Questo è un problema importante nei sistemi di conversazione e anche per valutare le performance dei modelli di linguaggio grande.</sample>
    <sample id="451">Non conosciamo un insieme di dati pubblico a larga scala per il compito, quindi raccolgiamo uno utilizzando la注解 crowdsourcing. Il nostro insieme di dati copre tre domini diversi: musica, libri e ricette.</sample>
    <sample id="452">Il metodo di raccolta dei dataset enfatizza l'informalità utilizzando un compito di completamento di fumetti.</sample>
    <sample id="453">La cartolina ha tre bubble di dialogo. In primo piano Bob dice: "ricorda quella canzone che stavamo ascoltando ieri" e con quello stabilisce il contesto del dialogo.</sample>
    <sample id="454">In questa seconda finestra di dialogo, Alice dice: "Do you mean easy on me or I got a feeling?"</sample>
    <sample id="455">Iniziamo con un esempio di dialogo: "Remember that we were talking about the new car yesterday?" "Do you mean the one you have? I've got a feeling." In questo esempio, Bob utilizza un riferimento indiretto per selezionare un ente specifico tra quelli menzionati precedentemente. Questo è un esempio di come il modello di raccolta dei dataset utilizza un compito di completamento di disegni per raccogliere esempi di dialoghi e analizzarne le espressioni linguistiche.</sample>
    <sample id="456">Forchiamo forniamo automaticamente i primi due bubble di dialogo, ma il terzo è riempito da un annotatore. Il primo bubble è scelto da un paio di promem manual per domande.</sample>
    <sample id="457">La seconda, che è la domanda alternativa, viene generata come segue:</sample>
    <sample id="458">Usiamo sempre un semplice modello: "Signori A o B?" dove A e B sono estratti da Wikipedia.</sample>
    <sample id="459">Ecco i metodi di campionamento che abbiamo utilizzato. Quand'è successo di andare più in alto nella lista, le entità diventano più simili l'una all'altra e è di solito più difficile fare la disambigazione.</sample>
    <sample id="460">La prima è uniformatran.</sample>
    <sample id="461">La seconda è quando gli enti hanno titoli simili. Ad esempio, due libri con il nome The Return.</sample>
    <sample id="462">La terza è quando hanno descrizioni simili su Wikipedia e infine quando hanno infobox o attributi simili su Wikipedia, ad esempio lo stesso genere o lo stesso artista.</sample>
    <sample id="463">Quando mostriamo queste domande alternative ai annotatori, conoscono il nome di queste entità ma non necessariamente ne sanno nulla.</sample>
    <sample id="464">Quello che facciamo è di mostriamo alcune informazioni di sfondo riguardanti i due enti. Per le canzoni, semplicemente mostriamo un link di ricerca Google per ciascuna canzoni.</sample>
    <sample id="465">Ecco un esempio dei risultati di ricerca Google per la canzone "Easy on Me".</sample>
    <sample id="466">Per il dominio ricette e libri, mostriamo un testo di sfondo da Wikipedia. Inoltre, mostriamo immagini delle ricette da Wikipedia per aiutare gli annotatori a vedere come si vedono.</sample>
    <sample id="467">Allora, chiediamo ai annotatori di scegliere uno di questi enti, ad esempio, il primo, e descriverlo utilizzando 3 a 5 espressioni indirette.</sample>
    <sample id="468">Ad esempio, la canzone con la musica di pianoforte. Ecco alcuni esempi dalla nostra base di dati: ad esempio, la canzone senza parole non è la canzone con il bambino di 12 anni o il bambino di 12 anni o la canzone che è fictionale o che deriva da Albergho e così via.</sample>
    <sample id="469">Il corpus AltEntities ha 6.000 domande alternative attraverso tre domini e 42.000 espressioni direttamente riferite. I risultati con il modello T5 XL sono riassunti di seguito:</sample>
    <sample id="470">Se il modello di linguaggio ha accesso esattamente alla stessa conoscenza di sfondo degli annotatori, allora l'accuratezza è davvero alta. Sono circa il 92% al 95%. Ma questa non è realistica.</sample>
    <sample id="471">Se il modello di linguaggio ha accesso a qualche conoscenza di sfondo parzialmente sovrapposibile, allora la precisione è tra i 82 e i 87%, che è più realistica. Ad esempio, quando il modello di linguaggio recupera la conoscenza di sfondo.</sample>
    <sample id="472">Se il modello di linguaggio ha accesso solo ai nomi degli enti, allora l'accuratezza è solo del 60%, quindi c'è molto spazio per miglioramento. Abbiamo anche dimostrato che i modelli sono generalizzabili tra i domini. Ecco un link al dataset. Grazie.</sample>
    <sample id="473">L'approccio viene confrontato con le politiche SimulST esistenti, tra cui la strategia wait-k e la strategia di accordo locale.</sample>
    <sample id="474">The affiliations of the authors are LIA, Avignon University; LS2N, University; CHU de Nantes; and Zenith.</sample>
    <sample id="475">Jenny</sample>
    <sample id="476">C'è un numero di tre autori che hanno collaborato all'articolo.</sample>
    <sample id="477">Ciao, mi chiamo Sara Papi e sono alla Università di Trento e alla Fondazione Bruno Kessler. Ecco, in questo video presenterò brevemente un articolo che ho scritto insieme con Matteo Negri e Marco Turchi, intitolato "Attenzione come guida per la traduzione simultanea del discorso".</sample>
    <sample id="478">Simultaneous speech translation, o Simultaneous Speech Translation (SST), è il processo di traduzione del linguaggio parlato in un testo in un altro linguaggio in tempo reale, abilitando la comunicazione tra lingue diverse.</sample>
    <sample id="479">E cosa sono i problemi dei modelli SimulST attuali? Le strutture specifiche di SimulST sono di solito addestrate introducendo moduli aggiuntivi da ottimizzare.</sample>
    <sample id="480">Le procedure di addestramento lunghe e complesse, ad esempio addestramenti che coinvolgono obiettivi di ottimizzazione diversi,</sample>
    <sample id="481">E addestrare e mantenere diversi modelli per raggiungere regimi di latenza diversi, ad esempio addestrare un modello con un'average di 1 secondo di latenza e un altro modello con 2 secondi di latenza e così via.</sample>
    <sample id="482">Allora, cosa è la nostra soluzione?</sample>
    <sample id="483">Prima di tutto, utilizziamo modelli offline esistenti senza retrain o adottare un'architettura specifica per SimuST. Utilizziamo solo un modello per ogni regime di latenza e gestiamo la latenza tramite parametri specifici.</sample>
    <sample id="484">E utilizza la conoscenza acquisita dal modello tramite il meccanismo di attenzione tra input audio e output testuale, che è chiamato meccanismo di attenzione reciproca. Ecco un esempio a destra.</sample>
    <sample id="485">La nostra soluzione è di proporgli adatt o encoder-decoder attention. E' una strategia per decidere se emettere o no una traduzione parziale basata su dove l'attenzione si dirige.</sample>
    <sample id="486">Un parola viene emessa se l'attenzione non è concentrata, cioè se la somma è inferiore a un certo livello α verso le ultime λ istanti di frames di audio, il che significa che informazione ricevuta non è abbastanza stabile.</sample>
    <sample id="487">Ad esempio, se riceviamo un frammento di discorso che dice "I'm going to talk about" e il nostro modello predice la traduzione in tedesco,</sample>
    <sample id="488">E cercheremo di analizzare i pesi di attenzione.</sample>
    <sample id="489">Vedremo che i primi due punti si riferiscono ai primi frammenti di parola ricevuti, mentre l'ultimo punto si riferisce ai frammenti di parola più recenti, ovvero i frammenti di lambda.</sample>
    <sample id="490">Questo significa che i primi due elementi verranno emessi.</sample>
    <sample id="491">Mentre, poiché la somma della attenzione transversale è sopra un certo livello α, non emetteremo l'ultima parola e aspetteremo per un altro segmento di parlata.</sample>
    <sample id="492">Se continuiamo e riceviamo un altro segmento di audio, il modello prevede altre tre parole e analizzeremo i pesi di attenzione tra segmenti.</sample>
    <sample id="493">Iniziamo con la nostra soluzione: EDAtt. Decide whether to emit or not a partial translation based on where attention points to: a word is emitted if the attention is not concentrated (its sum is below the threshold 𝜆) towards the last 𝜆 speech frames, meaning that the received information is enough that the system is stable.</sample>
    <sample id="494">Questo significa che queste tre parole verranno emesse.</sample>
    <sample id="495">Se guardate i principali risultati di EDAtt,</sample>
    <sample id="496">We will plot the simultaneous speech translation results on graphs in which we have blue on one side that measures the translation quality and average latency.</sample>
    <sample id="497">That is the latency measure and we also consider the computational-aware average latency that accounts for the model's computational times to produce the output.</sample>
    <sample id="498">Siamo d' accordo che i nostri punti dovrebbero essere il più possibile in alto su questa grafica.</sample>
    <sample id="499">Ma anche che siano spostati verso sinistra.</sample>
    <sample id="500">Inoltre, confrontiamo con strategie popolari che vengono anche applicate a modelli offline, come la strategia wait-k e l'approccio locale. Inoltre, confrontiamo anche con un architettura di punta specificamente adattata per la traduzione parallela.</sample>
    <sample id="501">Questi sono tutti i risultati della strategia di traduzione parallela Simultaneous su tedesco.</sample>
    <sample id="502">E vediamo che ad out supera tutti i strategie applicati a offline modelli, poiché le curve sono spostate verso sinistra.</sample>
    <sample id="503">E vediamo anche che se consideriamo il tempo reale o il tempo computazionale effettivo, adatt è la strategia più veloce.</sample>
    <sample id="504">Se vuoi scoprire di più i risultati, leggi il nostro articolo e anche rilasciamo open source, il codice e i modelli e i simultaneous output per facilitare la riproducibilità del nostro lavoro. Grazie per la tua attenzione.</sample>
    <sample id="505">Sì, il set di dati è pubblicamente disponibile.</sample>
    <sample id="506">Ciao a tutti, il mio nome è Ying e il mio collega Jinyang e io presenteremo il nostro ricerchiamo su Multi-instruct: improving multi-modal zero-shot learning via instruction tuning.</sample>
    <sample id="507">Con i progressi negli modelli di lingua a larga scala, molte ricerche hanno iniziato a esplorare nuove paradigme di apprendimento che utilizzano modelli pre-addestrati di lingua per diverse attività downstream in modo efficiente in termini di parametri e di dati.</sample>
    <sample id="508">Recentemente, molte ricerche hanno dimostrato che l'addestramento con istruzioni consente ai modelli di linguaggio addestrati in precedenza di svolgere compiti non visti in modo zero-shot, seguendo istruzioni naturali.</sample>
    <sample id="509">Tuttavia, la maggior parte dei precedenti studi sull'addestramento con istruzioni si concentrano su migliorare le prestazioni in testo senza immagini per i compiti di lingua, mentre i compiti di visione computer e i compiti multi-modal hanno stato trascurato.</sample>
    <sample id="510">Quindi, in questo lavoro vogliamo indagare se l'addestramento con istruzioni dei modelli pre-addestrati multi-modal può realmente migliorare la generalizzabilità su compiti multi-modal.</sample>
    <sample id="511">Inoltre, al tempo del nostro ricerche, abbiamo scoperto una considerabile disparità nella disponibilità di dataset istruzioni tra NLP e multimodale.</sample>
    <sample id="512">Esistono più di 1600 task di istruzioni solo linguistica. Tuttavia, non esiste un dataset di istruzioni multimodale largamente pubblico. Questo ci ha spinto a creare un dataset di istruzioni multimodale per l'addestramento.</sample>
    <sample id="513">Ecco qui presentiamo MultiInstruct, il primo dataset di benchmark per l'addestramento di istruzioni multimodale che consiste di 62 diverse attività multimodale diverse, coprendo 10 category diverse.</sample>
    <sample id="514">Questi compiti sono derivati da 21 dataset esistenti e gratuiti per l'accesso, e ogni compito è equipaggiato con cinque istruzioni scritte da esperti.</sample>
    <sample id="515">Per investigare l'addestramento di istruzioni multi-modal, utilizziamo il nostro insieme di dati proposto. Utilizziamo OFA, un modello pre-addestrato multi-modal unificato, come nostra base model. L'OFA utilizza un vocabolario unificato per i token del linguaggio, immagine e le coordinate di un bounding box.</sample>
    <sample id="516">Ecco un esempio di come funziona il sistema: immagina che tu stia guardando un video di una persona che gioca a tennis. Il sistema riconosce la persona che sta facendo un serve e capisce che stanno usando una racchetta. Poi, il sistema cerca un oggetto che corrisponda alla racchetta che ha visto in precedenza. In questo caso, il sistema trova un oggetto che corrisponde alla racchetta che ha visto in precedenza e dice che la racchetta è la cosa che stai cercando.</sample>
    <sample id="517">To unify the processing of various input and output data types.</sample>
    <sample id="518">Seguendo il modello di OFA, formuliamo tutti i compiti in un formato unificato di sequenza a sequenza, in cui i testi di input, le immagini, le istruzioni e i box di confine sono rappresentati nello spazio di token同一.</sample>
    <sample id="519">Ora parleremo di multi-modal instruction tuning.</sample>
    <sample id="520">Per il dataset di addestramento, utilizziamo 53 task da 9 gruppi per l'addestramento e selezioniamo 10.000 istanze per task. Per il test, riserviamo interamente il gruppo di ragione comune per il test e scegliamo ulteriormente 5 task dal gruppo Q&amp;A e dal gruppoMiscellanei.</sample>
    <sample id="521">Usiamo tutte le istanze nella suddivisione di test per ciascun compito. Inoltre, scegliamo casualmente 20 compiti dalla suddivisione di test del dataset Natural Instructions come compiti sconosciuti per NLP.</sample>
    <sample id="522">Usiamo un modello pre-addestrato OFA-Large come modello di base. Durante il training, mescoliamo tutte le istanze per tutti i compiti. Ogni istanza è combinata casualmente con uno dei cinque modelli di istruzioni.</sample>
    <sample id="523">Durante i test per ciascun compito, condurremo un totale di cinque esperimenti valutando il modello utilizzando uno dei cinque insiemi di istruzioni in ciascun esperimento.</sample>
    <sample id="524">Rapportiamo la performance media e massima, e la deviazione standard della performance attraverso tutti i cinque esperimenti.</sample>
    <sample id="525">Se la task è una task di classificazione multi-modal, rapportiamo l'accuratezza. Se è una task di generazione multi-modal, rapportiamo Rouge-L. Per le task NLP, rapportiamo Rouge-L inoltre.</sample>
    <sample id="526">Inoltre, si è introdotto un ulteriore metrica di valutazione chiamata sensibilità. Questa misura la capacità del modello di produrre gli stessi output per la stessa attività indipendentemente dalla variazione nella formulazione dell'istruzione.</sample>
    <sample id="527">I nostri principali risultati, come possiamo vedere, l'addestramento con istruzioni può migliorare in modo significativo il performance di OPA su diverse tareitte multimodale.</sample>
    <sample id="528">Anche l'apprendimento a transferimento da dataset di istruzioni naturali può migliorare l'addestramento dell'instruzione.</sample>
    <sample id="529">E qui possiamo vedere che, all'aumento del numero di compiti, il modello raggiunge un migliore prestigio e allo stesso tempo una sensibilità più bassa.</sample>
    <sample id="530">Siamo anche passati a un esperimento in cui utilizziamo un insieme di istruzioni invece di cinque istruzioni. Come possiamo vedere, l'uso di molte istruzioni può migliorare il modello in modo generale e ridurre la sua sensibilità molto.</sample>
    <sample id="531">Questa illustra l'effetto di diverse strategie di finetuning sul sensibilità del modello. Come possiamo vedere, grazie all'apprendimento da dataset di istruzioni naturali, il modello può raggiungere un sensibilità molto migliore rispetto allo original OFA modello.</sample>
    <sample id="532">Inoltre, possiamo vedere che il training di trasferimento da dataset di istruzioni naturale può aiutare OPA a ottenere un migliore prestazione sul dataset di istruzioni naturale.</sample>
    <sample id="533">In generale, propone la prima grande scala di insieme di istruzioni multi-modal di addestramento. Questo migliorato la capacità di zero-shot di OFA in modo significativo e esplora diversi metodi di apprendimento trasferibile e dimostra i loro benefici. Designa un nuovo metrico di sensibilità.</sample>
    <sample id="534">Inoltre, stiamo raccolgendo un insieme di dati di apprendimento multi-modal con circa 150 attività linguistiche visione e linguistica aggiuntive. Ben presto li rilascieremo. Questo è il codice QR per i nostri dati e il modello. Grazie.</sample>
    <sample id="535">I fornitori dell'articolo sono affiliati all'Università di Trento e alla Fondazione Bruno Kessler.</sample>
    <sample id="536">Javad Hosseini</sample>
    <sample id="562">Ciao a tutti, mi chiamo Kostya Sinha e sono felice di benvenuti alla nostra presentazione del nostro articolo Acl 2023, i giudizi di accettabilità dei modelli linguistici non sono sempre robusti al contesto.</sample>
    <sample id="563">I modelli di linguaggio non sempre sono robusti rispetto al contesto</sample>
    <sample id="564">In questo lavoro, rivisitiamo il paradigma del minimo coppia.</sample>
    <sample id="565">La valutazione del paradigma “Minimal Pair” (MPP) si basa su giudizi di accettabilità, che possono anche includere la grammatica come BLMP o SyntaxGym, o accettabilità in termini di stereotipi come CrowS.</sample>
    <sample id="566">E in questo paradigma di coppia minima, la tipica modo di valutare i modelli di linguaggio è quello di mostri il, come, un accettabile sentenzia o grammatical sentenzia e quindi mostri un accettabile sentenzia o un grammatical sentenzia.</sample>
    <sample id="567">E allora, l'obiettivo del modello è di assegnare maggior probabilità alla frase accettabile.</sample>
    <sample id="568">Il flusso di lavoro corrente per la valutazione dei modelli (MPP) non ci permette di valutare l'accettazione dei modelli verso i testi più lunghi.</sample>
    <sample id="569">Questi giorni, i modelli di lingua stanno uscendo con finestre di contesto sempre più lunghe. Così è cruciale che valutiamo la capacità dei modelli di accettare le informazioni durante tutto il contesto.</sample>
    <sample id="570">E quello è esattamente quello che stiamo cercando di fare qui. Stiamo cercando di rivedere il pipeline del PBP (Minimal Pair Paradigm) chiedendo al modello di valutare l'acceptabilità su sequenze più lunghe e più lunghe.</sample>
    <sample id="571">Quindi, è questa l'approccio. Quello che facciamo è di simulare queste sequenze più lunghe. Rivediamo i dataset stessi e creiamo nuove frasi scegliendo frasi accettabili o non accettabili da quelli stessi dataset.</sample>
    <sample id="572">Per esempio, qui abbiamo scelto un paio tipico di grammatica dalla libreria dei set di dati da Adjunct Island.</sample>
    <sample id="573">E quello che facciamo è creare sequenze più lunghe, che sono accettabili e hanno lo stesso matching della struttura grammaticale. Estraiamo frasi grammaticali da GPT-3.</sample>
    <sample id="574">E quindi lo aggiungiamo come prefisso sia alla domanda accettabile che alla domanda non accettabile.</sample>
    <sample id="575">Quindi possiamo fare la stessa cosa scegliendo frasi non accettabili da un matching dello stesso e quello potrebbe anche essere usato per testare la accettabilità del modello.</sample>
    <sample id="576">E possiamo anche fare lo stesso scegliendo frasi da un subset diverso o da un dataset diverso. Così chiamiamo questa la situazione di mismatch.</sample>
    <sample id="577">E qui, le frasi continuano a venire da insiemi di dati rilevanti, ma non da un insieme di dati che stai valutando. E possiamo fare lo stesso per i casi di non accettabilità.</sample>
    <sample id="578">Infine, possiamo scegliere frasi da un dominio completamente non correlato, ad esempio Wikipedia.</sample>
    <sample id="579">Questo ci dirà se i giudizi di accettabilità dei modelli sono davvero influenzati da alcun contesto.</sample>
    <sample id="580">Se il contesto sta provenendo da un sottogruppo diverso del dataset o se è completamente irrelevante per la frase attuale che stiamo analizzando.</sample>
    <sample id="581">Quindi, come fa il modello? In primo luogo, analizziamo le sentenze di Wikipedia che sono completamente irrelevanti al corrente query pair e trovare che i giudizi del MPP sono maggiormente robusti per lunghezze di contesto arbitrarie.</sample>
    <sample id="582">Aumentiamo la lunghezza del contesto fino a 1204 per massimizzare i modelli OPT e GPT-2 e vediamo qui nella linea di punto rosso che i giudizi del MPP sono relativamente stabili.</sample>
    <sample id="583">Ora, cosa succede quando scegliamo frasi dal medesimo dataset?</sample>
    <sample id="584">E qui stiamo scegliendo o creando frasi da domini accettabili e non accettabili, dalla stessa base di dati BLM o Sintagma.</sample>
    <sample id="585">E lì vediamo che le valutazioni del MPP aumentano o diminuiscono in modo significativo quando si aggiungono prefissi accettabili o non accettabili.</sample>
    <sample id="586">Ma quando scegliamo la struttura, cioè quando scegliamo le frasi da fenomeni dello same in Blimp Person Text Jim,</sample>
    <sample id="587">Vediamo un'enorme aumentazione o una enorme diminuzione della giudizio MPP del modello in base alla scelta del prefisso accettabile o non accettabile.</sample>
    <sample id="588">Ora, questa è molto grande. Questo effetto aumenta lungo la finestra di contesto e probabilmente influenzerà i modelli linguistici più recenti che hanno una finestra di contesto grande.</sample>
    <sample id="589">Perché i prefissi correlati hanno un così grande impatto sulla valutazione dei modelli di linguaggio?</sample>
    <sample id="590">Siamo arrivati a una serie di analisi in cui abbiamo cercato di alterare la frase di input in modo da preservare la struttura rilevante ma aggiungendo un po' di rumore alla frase di input. E dopo aver fatto diversi di questi alterazioni,</sample>
    <sample id="591">Troppo spesso, i modelli di apprendimento automatico non sono in grado di comprendere il contesto e la struttura del testo. Questo può portare a interpretazioni sbagliate o incomplete.</sample>
    <sample id="592">In sostanza, scopriamo che i modelli sono sensibili alle sentenze perturbate in modo simile.</sample>
    <sample id="593">Quando perturbiamo le frasi in un dominio accettabile, vediamo un aumento simile in tutte le perturbazioni. E quando perturbiamo le frasi in un dominio non accettabile, vediamo una diminuzione dei giudizi di accettabilità in modo simile.</sample>
    <sample id="594">Le informazioni principali del nostro lavoro sono che i modelli di linguaggio sono sensibili a caratteristiche sintattiche e semantiche latenti che vengono condivise tra le frasi.</sample>
    <sample id="595">E l'evaluation del MPP che facciamo attualmente con input brevi e singoli frasi non può catturare completamente il sapere astratto dei modelli di linguaggio attraverso il contesto.</sample>
    <sample id="596">Scrivi una traduzione italiana del contenuto in inglese.</sample>
    <sample id="597">Un ordered multiset of tokens that will appear in the output.</sample>
    <sample id="598">55,000 scripts are represented in Coscript.</sample>
    <sample id="626">Il miglior metodo di allineamento per DEplain è il metodo mass align.</sample>
    <sample id="627">L'apprendimento scarsamente supervisionato allevia il problema di annotazione.</sample>
    <sample id="628">I documenti in DEplain-web sono stati allineati con metodi di allineamento manuali e automatici.</sample>
    <sample id="629">Il set di dati CoNLL++ è stato creato utilizzando i notiziari di Reuters e ha seguito le direttive di annotazione del CoNLL 2013.</sample>
    <sample id="630">Ciao a tutti, mi chiamo Yusen Zhang e sono dell'Università del Pennsylvania. Oggi presenterò il mio lavoro: XSemPLR - Analisi semantica multilingue e rappresentazioni del significato in più lingue naturali.</sample>
    <sample id="631">La semantica parsing è un任务 per costruire rappresentazioni semantiche di query utente, come SQL e Lambda Calculus.</sample>
    <sample id="632">Cross-lingual Semantic Parsing è un compito di tradurre query in diverse lingue naturali in rappresentazioni multiple.</sample>
    <sample id="633">Come illustrato nella figura, dobbiamo tradurre la query in più lingue naturali utilizzando modelli neurali per SQL, Lambda o FunQL e altro.</sample>
    <sample id="634">I modelli di parsing semantico multilingue esistenti sono proposti e valutati separatamente su un insieme di task limitati e applicazioni. Ad esempio:</sample>
    <sample id="635">Ci sono limiti di copertura su certi linguaggi naturali. Ad esempio, il cinese è mancante e</sample>
    <sample id="636">Mancato copertura su determinate rappresentazioni del significato.</sample>
    <sample id="637">Il predicato lambda è mancante.</sample>
    <sample id="638">O solo valutati su certi modelli neurali. Ad esempio, ci sono solo modelli singoli da valutare.</sample>
    <sample id="639">Per raggiungere questo scopo, proponiamo Exemplar, che fornisce un dataset uniforme di esempi per la parsing semantica multi-lingua e la rappresentazione del significato in diversi linguaggi naturali.</sample>
    <sample id="640">Contiene 9 dataset in diversi domini, 5 task di parsing semantici, 8 rappresentazioni del significato e 22 lingue naturali in 15 famiglie linguistiche.</sample>
    <sample id="641">Per valutare meglio il benchmark, consideriamo i sei punti di addestramento e di valutazione.</sample>
    <sample id="642">La prima è TranslateTest. Utilizziamo l'API Google Translate per tradurre il testo di origine in un linguaggio di destinazione, quindi utilizziamo un modello monolingue per addestrare e valutare.</sample>
    <sample id="643">Ad esempio, addestriamo il modello inglese su query inglese e durante l'inferenza traduciamo la query in tedesco utilizzando un API in inglese e quindi utilizziamo il modello addestrato per prevedere il SQL.</sample>
    <sample id="644">E testiamo anche modelli monolingui.</sample>
    <sample id="645">In questo contesto, il linguaggio di origine è lo stesso del linguaggio di destinazione. Ad esempio, Germano a Germano o Inglese a Inglese.</sample>
    <sample id="646">Inoltre, testiamo anche un setting monolingue few-shot per addestrare modelli monolingui con solo il 10% dei dati di addestramento.</sample>
    <sample id="647">E testiamo un modello multilingue, in cui addestra un solo modello multilingue per tutte le lingue.</sample>
    <sample id="648">Ad esempio, mettiamo insieme query in tedesco, inglese e cinese per addestrare un modello multilingue. Durante l'inferenza, possiamo usare questo modello per...</sample>
    <sample id="649">Per tradurre query in tedesco o cinese, eccoci.</sample>
    <sample id="650">E consideriamo anche il passaggio tra lingue zero-shot e few-shot. Addestriamo su un linguaggio di origine e lo trasferiamo ad un altro linguaggio.</sample>
    <sample id="651">Durante il training, stiamo addestrando un modello multilingue a predire l'output SQL utilizzando query in inglese o una combinazione di query in inglese e tedesco.</sample>
    <sample id="652">E valutiamo due gruppi di modelli: i modelli pre-addestrati in monolingua con encoder e decoder a puntatore e i modelli multilingui pre-addestrati con encoder e decoder. Abbiamo scoperto molti risultati interessanti, tra cui che il modello Enc-Dec (m75) ottiene le migliori prestazioni su tutti i dataset!</sample>
    <sample id="653">Incluso Enc-PTR, che sta per Encoder Pointer-based Decoders multilingui pretrainati, come XLM-R + PTB e mBERT + PTB.</sample>
    <sample id="654">E valutiamo anche i modelli encoder-decoder, che sono modelli pre-addestrati multilingue encoder-decoder come mBART e mT5.</sample>
    <sample id="655">Abbiamo scoperto che Enc-Dec (mT5) ottiene il migliore prestazione su tutti i nove dataset!</sample>
    <sample id="656">E valutiamo su mT5 e XLM-R + PTR in un contesto multilingue.</sample>
    <sample id="657">Abbiamo scoperto che l'encoder-decoder o encoder-PTR possono essere migliorati addestrandoli in un mix di diverse lingue.</sample>
    <sample id="658">Siamo arrivati alla conclusione che è per via del fatto che la maggior parte delle lingue naturali principali possono ottenere un miglioramento delle prestazioni, tranne che per l'inglese, il cui prestigio diminuisce in sette dataset e guadagna in tre dataset.</sample>
    <sample id="659">Mostly, the major NLs can obtain performance gain, except that English performance drops in 7 datasets. This is known as "Curse of Multilinguality".</sample>
    <sample id="660">We also compare the cross-lingual performance gap.</sample>
    <sample id="661">In questa figura, la linea azzurra rappresenta il trasferimento a basso costo tra lingue. La linea arancio rappresenta il trasferimento a zero costo tra lingue. La linea verde rappresenta il contesto monolingue.</sample>
    <sample id="662">Abbiamo scoperto che, confrontando la linea verde e la linea arancio, nel contesto di un setting senza etichette, il gap di prestazione tra il trasferimento multilingue è significativo. Invece, confrontando la linea blu e la linea arancio, nel contesto di un setting con poche etichette, il gap di prestazione si riduce rapidamente.</sample>
    <sample id="663">Abbiamo anche scoperto alcuni altri interessanti findings. Ad esempio, encoder-decoder supera precedenti lavoro o raggiunge risultati paragonabili. Pretraining su lingua naturale inglese può in modo significativo migliorare il performances di few-shot su lingue target.</sample>
    <sample id="664">Siamo arrivati alla conclusione che i modelli di linguaggi multilingue come Codex e Bloom sono ancora inadeguati per le complessità della semantica multilingue.</sample>
    <sample id="665">In sintesi, è stato costruito XSemPLR, un benchmark unificato per la analisi semantica multi-lingua con diverse lingue naturali e rappresentazioni.</sample>
    <sample id="666">Conducing a comprehensive benchmark study on three representative types of multilingual language models and our results show many interesting findings and et cetera. And welcome to visit our paper and code. Thanks for listening.</sample>
    <sample id="667">The related works are parameter-based watermark, lexical watermark, backdoor-based watermark, and adversarial-based watermark.</sample>
    <sample id="668">No, gli LLM multilingue come Codex o Bloom sono insufficienti per il CLSP.</sample>
    <sample id="695">Il metodo affronta l'ambiguità delle permutazioni inducendo l'allegatura come parte della formazione.</sample>
    <sample id="696">L'equità di un modello NLP a valle si definisce come la capacità del modello di fornire risposte equamente e senza bias, non importa la politica o la razza del utente.</sample>
    <sample id="697">The speaker's name is Yanis Labrak.</sample>
    <sample id="698">Koustuv Sinha</sample>
    <sample id="699">The speaker's name is Myra Cheng.</sample>
    <sample id="700">Il tropicalismo indica un stereotipo che si riferisce alle donne latine.</sample>
    <sample id="701">Gli autori hanno elaborato le rappresentazioni umane dei gruppi target definendoli solo in relazione alla loro identità e distinguendoli come differenti rispetto al normale bianco.</sample>
    <sample id="702">Pointwise CXMI</sample>
    <sample id="703">DrBERT è un modello di apprendimento a partire da zero, mentre ChuBERT è un modello generico.</sample>
    <sample id="751">3</sample>
    <sample id="752">Il trasferimento iterativo dell'apprendimento è un approccio che aggiorna il modello in base al set di dati più recente raccolto.</sample>
    <sample id="753">The goal of the dataset is to understand user language when they want to make a choice.</sample>
    <sample id="754">Un utente malintenzionato può estrarre i parametri del modello attraverso un EaaS utilizzando tecniche di traduzione. Queste tecniche consentono all'utente di modificare il modello in modo da rendere più facile l'estrazione dei parametri.</sample>
    <sample id="755">Cinque.</sample>
    <sample id="756">Per creare il set di dati iniziale, sono stati impiegati 8 annotatori.</sample>
    <sample id="757">Carnegie Mellon University, University of Washington, and the Allen Institute for AI.</sample>
    <sample id="758">Il gouverneur est à gauche dans l'exemple "I saw Bart and Lisa".</sample>
    <sample id="759">I modelli all'avanguardia dei sistemi di dialogo sono quelli che utilizzano la valutazione ABC.</sample>
    <sample id="760">Perché i modelli di lingua stanno emergendo con finestre di contesto sempre più lunghe, è cruciale valutare la loro accettabilità durante tutto il contesto.</sample>
    <sample id="761">La formazione attraverso la modalità multilingue ha causato un calo delle prestazioni rispetto al modello inglese monolingue.</sample>
    <sample id="762">No, gli annotatori non conoscono l'entità in anticipo.</sample>
    <sample id="763">BLEU, METEOR, TER, ROUGE e chrF</sample>
    <sample id="764">Sì, il regresso nella generalizzazione influenza specifici tipi di NER.</sample>
    <sample id="765">La posizionalità nella NLP è importante perché le parole hanno diverse funzioni grammaticali e significati in base alla loro posizione all'interno di una frase o di un testo. La posizionalità delle parole può anche influenzare il significato del testo come un todo, poiché le parole che si situano vicino alle parole chiave possono fornire informazioni supplementari o contesto. Inoltre, la posizionalità delle parole può anche influenzare l'interpretazione del testo da parte di un modello di macchina o di un algoritmo di elaborazione del linguaggio naturale.</sample>
    <sample id="766">Adattatori.</sample>
    <sample id="767">Per il trasferimento dell'apprendimento, si ricorre al modello RoBERTa-base classifier head.</sample>
    <sample id="768">I recenti set di test utilizzati per valutare le capacità di PaLM sono il dataset de test di Google e il dataset de test di Microsoft.</sample>
    <sample id="769">The authors have proposed three recommendations at the end.</sample>
    <sample id="770">Il guadagno del nuovo metodo rispetto al vecchio è di 15,45%.</sample>
    <sample id="771">Shuheng Liu</sample>
    <sample id="772">Sì, i risultati e il set di dati possono essere utilizzati come parametri di riferimento.</sample>
    <sample id="773">Due modelli più piccoli vengono utilizzati.</sample>
    <sample id="774">OFA</sample>
    <sample id="833">I ricercatori che hanno scritto l'articolo sono affiliati a Google Translate.</sample>
    <sample id="834">Stony Brook University e Human Language Analysis Group.</sample>
    <sample id="835">The article analyzed the translation capabilities of the latest test sets for English to German and English to French.</sample>
    <sample id="836">Shangbin Feng</sample>
    <sample id="837">I modelli studiati sono Long Impart e Long Base Impart.</sample>
    <sample id="838">53 attività vengono utilizzate per scopi di addestramento e test.</sample>
    <sample id="839">Tre.</sample>
    <sample id="840">I test sono stati effettuati su quattro set di dati: AG News, MIND, SST2 e Enron Spam.</sample>
    <sample id="876">NACHOS is a dataset of medical crawled data from the web.</sample>
    <sample id="877">Il nome del relatore è Ida Vilard.</sample>
    <sample id="878">La strategia del prompting ha un grande impatto sui risultati.</sample>
    <sample id="879">Carnegie Mellon University, Language Technologies Institute, TECNICO Lisboa, BAIL, Unbabel</sample>
    <sample id="880">Le 5 istruzioni scritte da esperti sono:</sample>
    <sample id="881">I modelli vengono testati utilizzando un compito di risoluzione di coreferenza.</sample>
    <sample id="882">Ciao a tutti, il mio nome è Ibilard e userò un breve riassunto del paper "Prompting PaLM for Translation: Assessing Strategies and Performance". Questo è un lavoro di gruppo con i miei colleghi da Google Translate.</sample>
    <sample id="883">PaLM è un modello di lingua a 540 miliardi di parametri presentato l'anno scorso, nel 2022. È stato addestrato su una vasta raccolta di testo che comprende 780 miliardi di token.</sample>
    <sample id="884">Il modello di lingua PaLM ha raggiunto lo stato dell'arte in centinaia di compiti di NLP.</sample>
    <sample id="885">In questo lavoro presentiamo lo studio più sistematico di modelli di linguaggio per la traduzione automatica.</sample>
    <sample id="886">Abbiamo valutato la capacità di traduzione dei modelli utilizzando le migliori pratiche della comunità di traduzione. Questo comporta l'uso dei set di test più recenti per evitare un qualsiasi sovrapposizione dei dati di test con i dati di addestramento del modello di lingua.</sample>
    <sample id="887">E confrontiamo due sistemi di punta, i migliori sistemi sono i sistemi di traduzione automatica.</sample>
    <sample id="888">Usiamo metriche innovative e sofisticate per l'LMT e inoltre mostriamo anche i risultati di valutazioni umane basate su esperti. Infine, forniamo alcune raccomandazioni per le strategie di selezione dei prompt.</sample>
    <sample id="889">La prompting ha un grande impatto sulle prestazioni dei modelli di traduzione. Come possiamo vedere in un esperimento semplice, utilizzando un approccio di promptingle e fornendo due diverse promem per una stessa frase.</sample>
    <sample id="890">La maggioranza delle frasi, 516 su 1000, la differenza riservata è di più di un punto BLEURT.</sample>
    <sample id="891">E queste possono andare in casi estremi fino a 40 punti BLEURT. Quindi è importante selezionare una buona strategia di promempi.</sample>
    <sample id="892">In our experiments, we test for a few-shot prompting strategy where we just mark each its sentence that we provide to the system with the language it's in.</sample>
    <sample id="893">In questo esempio qui, dove eseguiamo traduzioni da tedesco in inglese, le frasi tedesche, i testi sorgenti, sono segnate con il segnaposto tedesco e le traduzioni inglese con il segnaposto inglese.</sample>
    <sample id="894">Abbiamo notato che la forma reale del prompt non ha un grosso impatto in caso di 5-shot prompting.</sample>
    <sample id="895">È cruciale per zero e uno shot prompting e quando andiamo, come nel nostro caso, a five shot prompting, c'è quasi niente di diverso dalla forma reale del prompt.</sample>
    <sample id="896">It's the examples that carry most of the weight.</sample>
    <sample id="897">Il riassunto dei nostri risultati sperimentali è che la qualità dell'esempio è più importante della somiglianza alla frase di origine.</sample>
    <sample id="898">E è importante selezionare gli esempi da traduzioni di alta qualità. In particolare, confrontiamo la selezione dei prompt dal dataset di valutazione di UMT o dai dati di dev.</sample>
    <sample id="899">I testi dei dati è molto più accurato e con alta qualità rispetto ai testi di addestramento che è molto più sicuro e i risultati, quindi un’ottima performance quando si usa i testi dei dati.</sample>
    <sample id="900">Tuttavia i sistemi specializzati in traduzioni hanno un vantaggio sostanziale rispetto alle traduzioni umane ma PAUM si avvicina molto a un sistema commerciale. In un caso specifico, ci si è sovrapposti con Google Translate.</sample>
    <sample id="901">I insights che otteniamo dalla nostra analisi, che abbiamo eseguito utilizzando il framework MQM, sono che la fluidezza del PaLM è paragonabile a sistemi di punteggio dell'industria ma la differenza principale deriva dalla precisione.</sample>
    <sample id="902">In particolare, i più comuni errori sono gli errori di omissione.</sample>
    <sample id="903">Sembra che PaLM sceglie di usare i modelli per produrre una traduzione più comprensibile, a volte rimuovendo parti della frase sorgente che non sono pertinenti alla traduzione.</sample>
    <sample id="904">Tuttavia, la categoria "stile/errore" per PaLM è più bassa rispetto a quella dei sistemi di punta, che è un segnale iniziale.</sample>
    <sample id="905">Quanto parla fornisce output molto fluente, ma con alcune problematiche di accuratezza.</sample>
    <sample id="906">E qui c'è un link per maggiori dettagli.</sample>
    <sample id="907">Ciao, mi chiamo Dawei e sono un studente di dottorato all'Università di Saarland in Germania. In questo video vorrei presentare il nostro recente lavoro, "Weaker Than You Think", un'analisi critica della supervisione debole.</sample>
    <sample id="908">Questa è un lavoro di gruppo con Xiaoyu Shen, Marius Mosbach, Andreas Stephan e Dietrich Klakow.</sample>
    <sample id="909">Why weakly supervised learning? Weak supervision alleviates the annotation bottleneck. But weak labels are noisy! Noise memorization harms generalization. Weakly supervised learning (WSL) Train models that generalize well despite being trained on noisy data</sample>
    <sample id="910">In weak supervision, non si etichettano manualmente i dati. Si etichettano i dati utilizzando fonti di etichettatura deboli, come regole semplici, basi di conoscenza o sondaggi di qualità bassa. Come illustrato nella figura sulla destra.</sample>
    <sample id="911">Quando confrontati con annotazioni umane, le annotazioni deboli sono molto più economiche, tuttavia sono anche rumorose, ovvero un determinato numero di annotazioni sono incorrette.</sample>
    <sample id="912">Se addestriamo direttamente reti neurali su dati etichettati deboli, le reti neurali tendono a memorizzare il rumore dei riquadri e non generalizzano.</sample>
    <sample id="913">In weakly supervised learning, training algorithms are proposed to robustly train neural networks on such label noise so that the trained models still generalize well.</sample>
    <sample id="914">In recent works in WSL, WSL stands for weakly supervised learning. A common claim is that people say that they only train models on weakly labeled data and achieve high performance on clean test sets.</sample>
    <sample id="915">Tecnicamente, questa affermazione non è falsa, ma c'è un problema.</sample>
    <sample id="916">che è che i fornitori di modelli presumono che ci sia un set di validazione pulito aggiuntivo disponibile per la selezione del modello.</sample>
    <sample id="917">We focus on this problem setting because it implies that additional manual annotations are required in weakly supervised learning. But, like an elephant in the room, this necessity is often overlooked.</sample>
    <sample id="918">Il riferimento adottato consiste nel chiedere tre domande di ricerca. Prima di tutto, è necessaria la data di validazione pulita per WSL o possiamo forse usare un set di validazione rumoroso invece?</sample>
    <sample id="919">Second, if clean data is required or if clean data is mandatory for WSL to work, then how many clean samples do we need? Finally, should we only use the clean samples for validation or are there better ways to utilize them?</sample>
    <sample id="920">We addressed these research questions in our work, and our findings are as follows.</sample>
    <sample id="921">Prima di tutto, scopriamo che recenti metodi WSL effettivamente richiedono campioni con etichette pulite per funzionare correttamente.</sample>
    <sample id="922">Altrimenti ci è un grosso sforzo di prestazione, come illustrato in questa figura. Se non ci sono campioni di validazione puliti, i modelli di addestramento non possono generalizzare al di fuori dei relativi etichettati.</sample>
    <sample id="923">Significa che l'addestramento è inutile.</sample>
    <sample id="924">Questo indica che le abilità di apprendimento automatico (AL) richiedono realmente etichette pulite per funzionare correttamente e il costo di annotazione per ottenere campioni di validazione puliti non dovrebbe essere trascurato.</sample>
    <sample id="925">Il nostro secondo trofeo è che aumentare il numero di campioni di validazione puliti aiuterà le approssimazioni WSL a raggiungere un better performance, come dimostrato nella figura sulla sinistra.</sample>
    <sample id="926">Tipicamente, basta 20 campioni per classe per ottenere prestazioni elevate.</sample>
    <sample id="927">Ma non è la fine della storia, perché se decidiamo di accedere a campioni puliti, allora addestrare su di essi direttamente ci fornirà anche un migliore prestigio.</sample>
    <sample id="928">Il grafico a destra mostra la differenza di prestazione tra i metodi di finetuning diretti che vengono applicati direttamente su dati puliti e i metodi WSL che utilizzano i dati puliti solo per la validazione.</sample>
    <sample id="929">Come possiamo vedere, se abbiamo 10 campioni per classe, il fin tuning diretto inizia a superare gli approcci WSL.</sample>
    <sample id="930">Infine, l'incremento del prestazione dichiarato in precedenti approcci WSL può essere facilmente ottenuto consentendo di continuare l'ottimizzazione su campioni di validazione puliti.</sample>
    <sample id="931">Come possiamo vedere dai grafici, il modello VALLINA denominato FTW inizialmente sottoperfora rispetto ai metodi WSL più avanzati come COSINE.</sample>
    <sample id="932">Tuttavia, se continuiamo a fin-tunare su campioni puliti, allora FTW si comporta allo stesso modo degli altri metodi.</sample>
    <sample id="933">In pratica, non c'è motivo di scegliere metodi più complessi di WSL che richiedono più tempo di elaborazione e spazio su disco.</sample>
    <sample id="934">In sintesi, abbiamo dimostrato che recenti approcci WSL richiedono campioni puliti e manualmente annotati per funzionare correttamente. Le loro prestazioni e praticità sono pesantemente sovrastimata.</sample>
    <sample id="935">I nostri raccomandazioni conclusive per il futuro lavoro sono come segue.</sample>
    <sample id="936">Prima di tutto, rapporta i criteri di selezione del modello. Ad esempio, rapporta se la selezione del modello è basata su validationi pulite o su validationi pulite.</sample>
    <sample id="937">Secondo le proposte di WSL, i modelli dovrebbero essere confrontati con baselines di imparazione a basso costo che funzionano su campioni puliti. Terzo, l'addestramento continuo è un baselined semplice ma forte che dovrebbe essere considerato in future work in WSL.</sample>
    <sample id="938">Infine, abbiamo rilasciato il codice sorgente del nostro modello. Puoi trovarlo nella Q&amp;A della presentazione. Ti prego di controllarlo pure. Grazie per la tua partecipazione alla conferenza.</sample>
    <sample id="939">I metodi di valutazione comuni per i sistemi di dialogo includono l'uso di giudici umani che selezionano tra due conversazioni o valutano le conversazioni utilizzando una scala di Likert.</sample>
    <sample id="940">There are five authors involved in the article.</sample>
    <sample id="941">Nell'esempio con Servin e Kea, è necessario conoscere che i giudici decidono cause in tribunali.</sample>
    <sample id="942">Sì, il codice è disponibile su GitHub.</sample>
    <sample id="943">No, gli annotatori per NLPositionality non sono bilanciati rispetto a ciascun gruppo demografico.</sample>
    <sample id="944">Le frasi nel dominio accettabile sono state perturbate in modo simile, con un aumento simile in tutte le perturbazioni.</sample>
    <sample id="945">Evaluare dimensionalmente significa analizzare diversi aspetti della qualità del dialogo per capire meglio le sue caratteristiche.</sample>
    <sample id="946">The authors are affiliated with the University of Science and Technology of China, Microsoft, and Sony AI.</sample>
    <sample id="947">La forma del prompting è importante per il caso di zero e uno shot prompting.</sample>
    <sample id="978">I modelli di dialogo che gli autori hanno valutato sono ABC-Eval, Turn Likert e Dialogue Likert.</sample>
    <sample id="979">Ci sono sette autori coinvolti nell'articolo.</sample>
    <sample id="980">Un buon pianificatore dovrebbe scrivere script che siano ragionevoli e fedeli alle limitazioni.</sample>
    <sample id="981">Ci sono 8 autori coinvolti nell'articolo.</sample>
    <sample id="982">The name of the presenter is Vasudha Varadarajan.</sample>
    <sample id="983">I due autori dell'articolo sono affiliati all'Institute of Computer Science e all'University of Warsaw.</sample>
    <sample id="1021">Gli errori più comuni di PaLM sono omission errors.</sample>
    <sample id="1022">Ecco, sono James Finch e Sarah Finch. Oggi vi dirò tutto su ABC-Eval, un nuovo approccio dimensionale per valutare l'intelligenza artificiale di conversazione.</sample>
    <sample id="1023">Questo lavoro è stato fatto dal laboratorio NLP di Emory, guidato dal professor Jinho Choi all'Emory University e in collaborazione con Amazon Alexa AI.</sample>
    <sample id="1024">Ecco una traduzione in italiano del contenuto:

Ecco un esempio di come potresti utilizzare il modello ABC per valutare il proprio modello di dialogo confrontandolo con lo stato dell'arte attuale.</sample>
    <sample id="1025">La pratica comune è di usare valutazioni umane, ad esempio chiedendo a giudici umani di selezionare quale delle due conversazioni sia migliore o di valutare le conversazioni utilizzando una scala di Likert.</sample>
    <sample id="1026">Questi approcci funzionano bene per fornire valutazioni holistiche della qualità del dialogo in generale, ma la qualità del dialogo ha molti aspetti. Quindi potresti voler valutare più dimensioni della qualità del chat per capire i punti di forza e le debolezze del modello a un livello più finegrained.</sample>
    <sample id="1027">Un approccio è quello di richiedere semplicemente ai giudici umani di valutare diverse dimensioni della qualità del dialogo, come la rilevanza delle risposte del modello, utilizzando metodi esistenti basati su scale comparative o Likert.</sample>
    <sample id="1028">Tuttavia, creiamo che ci sia un策略更精确和可靠的维度对话评估。</sample>
    <sample id="1029">La nostra approccio tenta di ridurre la soggettività della valutazione umana, rendendo esplicito se o no ogni risposta del modello esprime certi comportamenti, come fornire informazioni non rilevanti o contradire se stessa.</sample>
    <sample id="1030">Chiamiamo questa approccio annotazione comportamenti in chat o ABC-Eval in breve. Abbiamo sviluppato questo metodo per coprire in modo esaustivo i comportamenti dei modelli di chatten che hanno stati suggeriti di influenzare la qualità del chat in recente letteratura.</sample>
    <sample id="1031">ABC-Eval è capace di misurare le tassi con cui i modelli di chat commettono errori stilistici diversi.</sample>
    <sample id="1032">Ad esempio, ABC-Eval misura il numero di giri in cui un modello di chatten ignora il proprio partner o dice qualcosa di non rilevante.</sample>
    <sample id="1033">Contraddice se stesso o il proprio partner, elenca fatti sbagliati o viola le conoscenze comuni e quando il modello riesce o fallisce a dimostrare compassione.</sample>
    <sample id="1034">Per determinare quale tipo di valutazione sia più efficace, abbiamo selezionato quattro modelli di chatbot di punta e li abbiamo valutati su 100 conversazioni umano-bots per modello utilizzando ABC-Eval.</sample>
    <sample id="1035">Per confronto, abbiamo anche valutato queste conversazioni utilizzando tre metodi esistenti: valutazioni con una scala di Likert al livello della singola risposta, valutazioni con una scala di Likert al livello del dialogo e confronti a due gruppi per livello di dialogo.</sample>
    <sample id="1036">Per ciascuno dei metodi esistenti, abbiamo raccolto valutazioni su otto degli aspetti più comamente misurati del dialogo, poiché questa è la pratica standard per valutare i modelli di chattenzio in diverse dimensioni.</sample>
    <sample id="1037">Da analisi dei risultati di queste valutazioni, abbiamo scoperto che i etichettatori ABC hanno comportamenti più affidabili rispetto ai metodi esistenti, come misurato dall'acordo tra gli etichettatori su 100 conversazioni duplicatamente etichettate.</sample>
    <sample id="1038">Inoltre, le etichette ABC-Eval sono più predittive della qualità generale della conversazione rispetto ai metri prodotti da metodi esistenti, come dimostrato da questa analisi di regressione lineare semplice.</sample>
    <sample id="1039">Ad esempio, si può vedere come misurare la proporcione di giri con contrasti tra se stessi e partner spiega il 5% e il 10% della qualità della conversazione rispettivamente, mentre i punteggi di consistenza Likert medio spiegano solo il 4% o meno.</sample>
    <sample id="1040">Infine, abbiamo verificato se ogni metrica di valutazione cattura un aspetto unico della qualità del chat utilizzando una regressione lineare a passi successivi.</sample>
    <sample id="1041">Si può vedere come la combinazione di tutti i metri ABC-Eval spiega più del 25% della qualità della conversazione. E se si rimuove uno dei metri alla volta, la maggior parte dei metri porta a perdere una quantità substantiale di informazioni sulla qualità.</sample>
    <sample id="1042">Sul lato opposto, la combinazione di tutti i metri di Likert a livello di singola risposta spiega molto meno della qualità e meno di questi metri trasmettono informazioni uniche.</sample>
    <sample id="1043">Questi metri ABC-Eval affidabili, informativi e distinti ci permettono di valutare l'Intelligenza Artificiale della conversazione con una risoluzione più alta rispetto ai metodi precedenti che possono raggiungere.</sample>
    <sample id="1044">Si possono vedere che nei risultati dell'esperimento ci sono diversi problemi che sono rimasti e hanno stati quantificati con precisione. Ad esempio, i bot che stiamo testando hanno violazioni di senso comune in circa il 20% delle loro risposte.</sample>
    <sample id="1045">Prodotto informazione non rilevante in circa il 15% delle risposte e si contraddicono o con il proprio partner circa il 10% del tempo.</sample>
    <sample id="1046">Con la velocità di miglioramento rapida in questo campo, molte di queste tassi di errori potrebbero vedere una diminuzione in nuovi modelli rilasciati da quando è stata condotta la nostra valutazione. Tuttavia, questa è maggiormente la ragione per cercare di metri di valutazione affidabili e precisi per confrontare i modelli.</sample>
    <sample id="1047">Speriamo che ABC-Eval possa essere sfruttato da altri nel campo come un passo significativo in questa direzione e ci guardiamo alla prospettiva di vedere come l'IA conversazionale avanzerà nei prossimi mesi e anni. Grazie per la visione.</sample>
    <sample id="1048">The authors of the article are affiliated with the Emory NLP Lab at Emory University, led by Professor Jinho Choi, and in collaboration with Amazon Alexa AI.</sample>
    <sample id="1049">CFT significa Continuous Fine-Tuning, che è un tipo di apprendimento continuo che viene utilizzato come linea di riferimento per le tecniche di imparare con poche etichette.</sample>
    <sample id="1050">Ci sono sei autori coinvolti nell'articolo.</sample>
    <sample id="1051">Ciao, mi chiamo Kayo Yin e presenterò il nostro lavoro intitolato "When Does Translation Require Context? A Data-Driven Multilingual Exploration". Questo lavoro è stato sviluppato in collaborazione con Patrick Fernandez, Emmy Liu, Andre F. T. Martins e Graham Neubig.</sample>
    <sample id="1052">A lot of translations depend on context. For example, how would we translate mole in this sentence?</sample>
    <sample id="1053">Se la frase precedente era "Things could start to get dangerous if the ministers find out," allora mole si riferisce a una spia. Ma se la frase precedente era "Could it be anything serious, doctor?" allora mole si riferisce a un segno di nascita.</sample>
    <sample id="1054">Quindi, a seconda del contesto, il significato della parola cambia e di conseguenza anche la sua traduzione.</sample>
    <sample id="1055">Tuttavia, valutare quanto bene i modelli traducono casi come questo è piuttosto difficile. In primo luogo, poiché solo una piccola porzione delle traduzioni dipende dal contesto, i metri a livello di corpus come BLEU non riescono a catturare queste traduzioni.</sample>
    <sample id="1056">E valutazioni targettizzate su traduzioni dipendenti dal contesto ma queste risorse possono fornire solo tipi limitati di traduzioni dipendenti dal contesto e set limitati di lingue poiché di solito si basano su conoscenza di dominio e curazione umana.</sample>
    <sample id="1057">In questo lavoro cerchiamo di rispondere a queste due domande: prima quando è necessario contesto per la traduzione e, secondo, come i modelli gestiscono questi casi?</sample>
    <sample id="1058">Per rispondere alla prima domanda, ci siamo concentrati su quanto un termine dipende dal contesto per la traduzione.</sample>
    <sample id="1059">In the previous work, we introduced CXMI as a measure for context usage by machine translation models and this is done by measuring how much information the context C provides about the target Y given the source X.</sample>
    <sample id="1060">Puoi pensare a CXMI come all'informazione guadagnata fornendo contesto al modello.</sample>
    <sample id="1061">In questo lavoro, estendiamo CXMI a P-CXMI, che può misurare l'uso del contesto al livello della frase o al livello delle parole. Possiamo pensare a parole che hanno alta P-CXMI come parole che richiedono contesto per la traduzione.</sample>
    <sample id="1062">Ora analizziamo parole con alta pcs mi per vedere se ci sono modelli tra queste parole.</sample>
    <sample id="1063">E noi eseguiamo l'analisi su testi di TED Talks che hanno tradotto dall'inglese in quattordici diverse lingue.</sample>
    <sample id="1064">Svolgiamo l'analisi in tre livelli diversi. Innanzitutto, analizziamo le parti del discorso che hanno un alto P-CXMI.</sample>
    <sample id="1065">E queste ci permettono di trovare, ad esempio, i pronunti in arabo che hanno un P-CXMI elevato e queste possono essere spiegate in quanto l'inglese non ha pronunti doppii. Quindi, è necessario contesto per determinare se un pronunto è doppio quando tradotto in arabo.</sample>
    <sample id="1066">Inoltre, scopriamo che certi linguaggi richiedono contesto quando si sceglie la forma appropriata del verbo. Abbiamo analizzato parole che hanno un P-CXMI elevato in tutte le loro occorrenze.</sample>
    <sample id="1067">E queste aiutano a identificare casi come questo qui, dove in cinese si hanno contesti per tradurre nomi propri, per assicurarsi che si usi la stessa traduzione all'interno del documento.</sample>
    <sample id="1068">E inoltre, notiamo che i contesti supportano la traduzione in un'opportuna formalità.</sample>
    <sample id="1069">E infine, analizziamo i token individuali che hanno un alto P-CXMI e ci permette di identificare fenomeni che non possono essere catturati solo dal termine in sé, ma piuttosto espressi nella struttura dell'istante, ad esempio l'elisiose o la risoluzione.</sample>
    <sample id="1070">Ora utilizziamo i nostri findings dalla nostra analisi per progettare un benchmark per la traduzione di documenti in italiano.</sample>
    <sample id="1071">Per ciascuno dei cinque fenomeni di discorso che abbiamo identificato, creiamo tagger per automaticamente identificare parole che appartengono al fenomeno e chiamiamo il tagger Multilingual Discourse-Aware (MuDA) tagger.</sample>
    <sample id="1072">We can then also note that different languages have different proportions of these discourse phenomena.</sample>
    <sample id="1073">Poi utilizziamo il tagger MuDA, applicando il tagger su un corpus parallelo che vogliamo usare per l’evaluation e applichiamo i nostri metri di traduzione preferiti su esempi specifici che il tagger MuDA ha identificato.</sample>
    <sample id="1074">E infine, utilizziamo il nostro benchmark insieme ad altri metri per valutare diversi modelli di traduzione automatica a livello di documento.</sample>
    <sample id="1075">In primo luogo, quando utilizziamo metriche a livello di corpus, ossia per il blu, scopriamo che i modelli cognitivi hanno le prestazioni migliori.</sample>
    <sample id="1076">Ma allora se utilizziamo comet, i modelli con consente performano meglio e se utilizziamo word f misura, allora i modelli con o senza contest hanno prestazioni comparabili.</sample>
    <sample id="1077">Questo再次证明了，如果我们只使用语料库级别的指标，就很难确定哪个文档级别翻译系统最好。</sample>
    <sample id="1078">Ora utilizziamo la benchmark MUMA per valutare i modelli e scopriamo che i modelli che usano il contesto sono significativamente più accurati rispetto a quelli che non usano il contesto per certi fenomeni discorsivi, come formalità e coesione lessicale.</sample>
    <sample id="1079">Ma questi modelli non sono molto migliori dei modelli che non usano contesto su altri fenomeni come elipsi, pronuncia e verbe in forma. Quindi questa sorta di suggerisce dove dovremmo vedere più progresso per la traduzione di documenti.</sample>
    <sample id="1080">Inoltre, confrontiamo sistemi commerciali diversi e i nostri benchmark dimostrano che DeepL è di solito più accurato rispetto a Google Translate per la traduzione di documenti.</sample>
    <sample id="1081">In sintesi, eseguiamo un'analisi basata su dati che copre 14 lingue diverse per identificare quando le traduzioni richiedono contesto.</sample>
    <sample id="1082">E allora utilizziamo le nostre scopertive per costruire un benchmark per la traduzione di documenti, che ci aiuterà a identificare se i modelli dei fenomeni discorsivi possono gestire o no e se i sistemi di traduzione sono buoni nella traduzione dei documenti.</sample>
    <sample id="1083">Grazie mille per l'attenzione. Ci vediamo in Torino!</sample>
    <sample id="1084">Yusen Zhang</sample>
    <sample id="1121">Il nuovo metodo non ha un nome.</sample>
    <sample id="1122">The author describes the method as identifying words that distinguish marked groups from unmarked ones.</sample>
    <sample id="1123">Paul G. Allen School of Computer Science, University of Washington; UW NLP; Carnegie Mellon University Language Technologies Institute; University of Washington</sample>
    <sample id="1124">Prague</sample>
    <sample id="1125">Sarah Finch</sample>
    <sample id="1126">There are four authors involved in the article.</sample>
    <sample id="1127">BliMP, SyntaxGym e CrowS.</sample>
    <sample id="1161">FT, TW, BOND, COSINE, e MLC.</sample>
    <sample id="1162">Il modello viene valutato su 11 attività mediche e cliniche.</sample>
    <sample id="1226">CamemBERT viene inizialmente addestrato su 4GB di subset di Natsos.</sample>
    <sample id="1227">Adam Przepiórkowski</sample>
    <sample id="1228">I risultati dell'experiment hanno portato alla conclusione che la deriva temporale è la causa principale della perdita di prestazioni.</sample>
    <sample id="1269">I token non sono ordinati correttamente.</sample>
    <sample id="1270">Perché non sanno se le stereotipi positivi sono dovuti a una sovravalignment o a metodi anti-stereotipizzanti.</sample>
    <sample id="1271">Grammatical sentences</sample>
    <sample id="1272">I ricercatori hanno utilizzato diverse metriche di valutazione, tra cui F1-Score, Precision e Recall.</sample>
    <sample id="1273">La metrica utilizzata per misurare l'accordo tra i fornitori è la percentuale di accordo inter-annotatore.</sample>
    <sample id="1274">Wikipedia</sample>
    <sample id="1275">Heinrich Heine University Düsseldorf, Germany</sample>
    <sample id="1276">MultiInstruct differisce dagli altri parametri di riferimento in quanto non ha un dataset di istruzioni multimodale pubblicamente disponibile.</sample>
    <sample id="1277">There are three authors involved in the article.</sample>
    <sample id="1278">La coordinazione binaria si riferisce alla misura della differenza tra le lunghezze dei concetti in base alla loro lunghezza in caratteri, sillabare e parole.</sample>
    <sample id="1279">I prompt sono stati utilizzati per un periodo di 12 mesi.</sample>
    <sample id="1280">I risultati hanno implicazioni importanti per il modello T5 più piccolo, in quanto dimostrano che anche i modelli più piccoli possono superare i modelli più grandi se sono adeguatamente addestrati su dataset specifici. Questo suggerisce che i modelli più piccoli potrebbero essere più efficienti e più economici da utilizzare, senza compromettere la qualità dei risultati.</sample>
    <sample id="1281">Ciao, mi chiamo Yannis Labrak e presenterò i nostri lavori su DrBERT, un modello pre-addestrato robusto in francese per i domini biomedici e clinici.</sample>
    <sample id="1282">In questa presentazione, innanzitutto parliamo di modellazione del linguaggio in salute. Poi presenteremo la principale contribuzione del nostro articolo.</sample>
    <sample id="1283">Introduciamo il primo modello biomedico in francese, DrBERT, che è basato su RoBERTa e trainato su NACHOS, che è un dataset di dati medici scarichi dalla rete.</sample>
    <sample id="1284">Introduciamo anche una comparazione di modelli con diverse impostazioni di pre-addestramento e fonti di dati. Poi presentiamo i nostri risultati su 11 compiti di immissione biomedica e clinica in Francia.</sample>
    <sample id="1285">In conclusione, riassumiamo gli esperimenti e forniamo maggiori dettagli su come accedere ai modelli.</sample>
    <sample id="1286">Da quando è stato rilasciato nel 2018, BERT è diventato uno dei metodi più efficaci per risolvere compiti di elaborazione naturale del linguaggio e ha offerto un'enorme migliora del prestabilito rispetto ai metodi storici statici e contestualizzati come Word2Vec, FastText o WordPiece.</sample>
    <sample id="1287">Da allora, questo modello è stato adattato a molti altri linguaggi, come in francese con Camembert e in altri domini come biomedici con PumMedBERT e BioBERT e in clinici con ClinicalBERT, ma principalmente in inglese.</sample>
    <sample id="1288">I modelli specializzati per altre lingue sono scaraci e spesso basati su pre-addestramenti continuativi a causa della mancanza di dati in domenico.</sample>
    <sample id="1289">Tuttavia, il francese non ha alcun modello open source moderno per la biomedicina fino adesso.</sample>
    <sample id="1290">Inoltre, chiediamo a noi stessi di domandarci qual è la fonte dei dati più appropriata per una ampia gamma di utilizzo e se i nostri dati sono un buon sostituto per i dati clinici.</sample>
    <sample id="1291">Per rispondere a questa domanda, confrontiamo il Dr. Bert con il nostro modello Shubert, che è basato su un dataset anonimizzato ottenuto dal Nantes University Hospital Data Warehouse.</sample>
    <sample id="1292">Successivamente, ci chiediamo quanta data è necessaria per addestrare un modello specializzato in francese.È 4 GB, 8 GB o altro?</sample>
    <sample id="1293">Per rispondere a questa domanda, dobbiamo prima trainare e confrontare due modelli a partire da zero: una prima versione di DrBERT con 7 GB di NACHOS e una seconda versione di 4 GB sottos集 di NACHOS.</sample>
    <sample id="1294">Una prima versione di Shubert, che è un modello clinico, con 4 GB di frasi tratte da note cliniche e una versione finale di Shubert con un mix di 4 GB sottasettore di NACHOS e 4 GB di note cliniche.</sample>
    <sample id="1295">In aggizione di questa comparazione, introduciamo tre modelli addestrati su continuo pre-training per analizzare l'effetto delle strategie di pre-training.</sample>
    <sample id="1296">1. Uno basato sul peso di Camembert e addestrato su un subset di 4 GB di NACHOS. 2. L'altro, anche basato su Camembert, ma addestrato questa volta su un subset di 4 GB di ClincNotes.</sample>
    <sample id="1297">E infine, si basa su un modello di biomedicina in inglese, BERT, e viene addestrato su un subset di 4 GB di NACHOS. In totale, abbiamo sette modelli.</sample>
    <sample id="1298">Per valutare i nostri 7 modelli, utilizziamo diverse pubbliche e private fonti di dati di task come riconoscimento di entità, classificazione, part-of-speech tagging e risoluzione di domande.</sample>
    <sample id="1299">Questi modelli sono confrontati con sei modelli predefiniti, che sono: Camembert Oscar 108 GB, Camembert Oscar 4 GB, Camembert CUNet 4 GB, DeBERTa by Albert e ClinicalBERT.</sample>
    <sample id="1300">Evaluazione dei modelli che hanno best sottoposti a valutazione con i dati dello stesso tipo di quelli su cui è stato addestrato il modello.</sample>
    <sample id="1301">Tuttavia, possiamo ottenere i dati da fonti etologiche e osservare che i dati provenienti da fonti etologiche sembrano essere più versatile. Abbiamo anche osservato che l'uso di più dati traduce in un miglioramento delle prestazioni.</sample>
    <sample id="1302">In generale, il pre-addestramento da zero sembra ottenere prestazioni più elevate su molte delle attività.</sample>
    <sample id="1303">Tuttavia, i nostri esperimenti su pre-addestramento continuo utilizzando il modello e le tokenizzazioni di Camembert, trainato sul sottosettore di 4 GB di Natsos, hanno dimostrato risultati comparabili a quelli ottenuti con DoctorBERT 4 GB da zero.</sample>
    <sample id="1304">La tabella riporta i risultati di valutazione per diversi modelli pre-addestrati su un dataset di 4GB di testo. I modelli vengono valutati in diverse specializzazioni, tra cui MEDICAL, SPECIALIST, MUSICAL-DEEP, MUSICAL-ESAI, HUMOR, HUMOR-MULTI, QUADRO-EMEA e QUADRO-MEDICINE. I modelli vengono anche valutati in diverse varianti, tra cui Camembert-Oscar, Camembert-CNN+G, Camembert-CLM, Camembert-CLM+, Camembert-CLM+G, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert-CLM+G+, Camembert</sample>
    <sample id="1305">In conclusione, il nostro sistema proposto offre un'performance migliore su nove dei dieci compiti di domenzo e supera globalmente i risultati del modello generico qui denominato Camembert.</sample>
    <sample id="1306">Anche se notiamo che i dati specializzati sono migliori, i più specializzati i dati sono migliori, ma non si scalano bene.</sample>
    <sample id="1307">I modelli pre-addestrati ottenuti da NACHOS sono liberi di essere utilizzati e sono disponibili su GitHub. Inoltre, tutti i script di addestramento sono anche su GitHub.</sample>
    <sample id="1308">Così, grazie per questa presentazione e stiamo aspettando di condividere al sottosegno in Toronto.</sample>
    <sample id="1309">Le strategie di apprendimento esaminate nel lavoro sono: costruire da zero, continuare il training utilizzando un modello pre-addestrato, e utilizzare modelli pre-addestrati su contesti clinici.</sample>
    <sample id="1310">Il fattore di overfitting dovuto al riutilizzo del test è piccolo.</sample>
    <sample id="1311">La qualità della semplificazione è stata valutata utilizzando i punteggi e gli indicatori di valutazione.</sample>
    <sample id="1312">Sì, i modelli linguistici presentano bias politici diversi.</sample>
    <sample id="1313">Ciao, mi chiamo Matthias Lindemann e oggi voglio darti un'introduzione breve al nostro articolo su generalizzazioni composte senza alberi utilizzando tag di insiemi multi e permutazioni latenti.</sample>
    <sample id="1314">Questa è una collaborazione con i miei tassatori, Alexander Koller e Ivan Titov.</sample>
    <sample id="1315">La generalizzazione composizionale può essere compresa come la capacità di un imparatore di gestire una maggiore ricorsione e composizioni sconosciute di frasi che hanno essere stati visti individualmente durante l'addestramento.</sample>
    <sample id="1316">In the context of semantic parsing, testing for compositional generalization might look like this: as usual, we have a training set of utterances in this case, the girl slept and Mary knew that the girl slept.</sample>
    <sample id="1317">Queste espressioni sono associate a forme logiche che rappresentano gli aspetti principali del loro significato.</sample>
    <sample id="1318">In contrast to standard machine learning evaluation, the test set does not come from the same distribution but contains structurally unseen logical forms.</sample>
    <sample id="1319">In questo esempio, il modello ha visto ricorso a livello inferiore durante l'addestramento e viene testato su un esempio con ricorso più profondo.</sample>
    <sample id="1320">I modelli sequenziali a sequenza naive lottano con questa specie di generalizzazione fuori distribuzione e spesso producono output che si allontanano dal input.</sample>
    <sample id="1321">In particolare, spesso falliscono nel riprodurre le corrispondenze sistematiche tra input e output, come quelle che sono colorate nell'esempio.</sample>
    <sample id="1322">A popular method to address this is to integrate trees into the models.</sample>
    <sample id="1323">I diagrammi sono intesi per captare il processo di composizione che collega le espressioni a forme logiche.</sample>
    <sample id="1324">This works well, but trees are usually not given and need to be obtained somehow.</sample>
    <sample id="1325">Questo può essere complicato e spesso un processo computazionalmente costoso. Di solito, questo implica un elaborato pre-processamento specifico della forma logica per esempio per gestire i simboli variabili.</sample>
    <sample id="1326">Obtaining trees may also involve specialized grammar induction procedures.</sample>
    <sample id="1327">In questo articolo, non utilizziamo gli alberi e introduciamo un modello di sequenza a sequenza neurale che modella direttamente le corrispondenze tra frammenti dell'input e frammenti dell'output.</sample>
    <sample id="1328">Per la prima volta, dimostriamo una forte generalizzazione a una ricorrenza più profonda senza fare affidamento su alberi.</sample>
    <sample id="1329">Il nostro approccio predice l'output dalla input in due passi.</sample>
    <sample id="1330">Prima di tutto, etichettiamo ogni token di input con un insieme non ordinato di token che appariranno nell'output.</sample>
    <sample id="1331">Dopo il primo passo, abbiamo tutti i token giusti ma non sono ordinati.</sample>
    <sample id="1332">Quindi, nel secondo passo, utilizziamo un altro modello per prevedere una permutazione per mettere in ordine le parole.</sample>
    <sample id="1333">Introduciamo un nuovo metodo per prevedere una permutazione che non imponga alcun vincolo rigoroso sulle possibili permutazioni. Questo rende il nostro approccio piuttosto flessibile e espressivo.</sample>
    <sample id="1334">Conceptually, our permutation model works roughly like this.</sample>
    <sample id="1335">And we go from left to right over the output and determine which multi-set token to put in every position. For the first output position, we simply select one as highlighted in red.</sample>
    <sample id="1336">Allora, saltiamo al prossimo multisett token per determinare il token successivo nell'input.</sample>
    <sample id="1337">Determiniamo il terzo token nell'output in modo simile, saltando ad un altro token multiset. Continuiamo questo processo.</sample>
    <sample id="1338">Finché ogni token del primo stadio è stato visitato esattamente una volta.</sample>
    <sample id="1339">Per fornirti un teaser dei risultati sperimentali, qui confrontiamo il nostro metodo con altri modelli Treeless sul benchmark COGS. Il nostro modello supera gli altri per una larga distanza in quanto generalizza meglio a una ricorsione più profonda.</sample>
    <sample id="1340">Alcune altre tipologie di generalizzazioni strutturali sono rimaste molto difficili, tuttavia.</sample>
    <sample id="1341">In our paper, we solve a couple of interesting technical challenges.</sample>
    <sample id="1342">Prima di tutto, l'allestimento tra input e output non è dato in i dati di addestramento. Di conseguenza, per un token specifico non sappiamo da quale multi-settore è derivato, il che rappresenta un sfidone per l'addestramento.</sample>
    <sample id="1343">Inoltre, spesso ci sono più permutazioni che sono coerenti con i dati, ma la versione grammaticalmente corretta è latente. Lo indichiamo inducendo l'alignment come parte dell'addestramento.</sample>
    <sample id="1344">Il nostro metodo di permutazione è molto flessibile, ma introduce lo sfidone di trovare la permutazione con punteggio più alto. Questo è legato al problema del commercialista che viaggia.</sample>
    <sample id="1345">Approximiamo questo con un rilassamento continuo amico GEP che ci consente anche di propagare all'indietro attraverso la soluzione e imparare le permutazioni più plausibili linguisticamente.</sample>
    <sample id="1346">Se vuoi imparare di più sulle nostre esperienze e su come affrontiamo questi sfid
assistant
Se vuoi imparare di più sulle nostre esperienze e su come affrontiamo questi sfid</sample>
    <sample id="1347">La dissonanza cognitiva è quando ci sono due credenze o azioni che non sono coerenti tra di loro.</sample>
    <sample id="1348">GPT-4 is the most liberal language model according to the image.</sample>
    <sample id="1349">Sì, nell'apprendimento attivo, l'addestramento cumulativo funziona meglio di quello iterativo.</sample>
    <sample id="1350">Sara Papi</sample>
    <sample id="1351">I dati del riferimento MuDa sono stati tratti da transcrizioni di TED Talks tradotte in 14 lingue diverse.</sample>
    <sample id="1385">The speaker's name is Matthias Lindemann.</sample>
    <sample id="1386">Il trasferimento interlinguistico è il processo di addestrare un modello su un linguaggio di origine e utilizzarlo per prevedere i output in un altro linguaggio.</sample>
    <sample id="1387">Sarland University, Amazon Alexa, University of Vienna</sample>
    <sample id="1388">Gli autori si basano su due misure di latenza: la media dei tempi di traduzione e la media dei tempi di computazione.</sample>
    <sample id="1389">Ciao a tutti, mi chiamo Manjata e oggi io e il mio collaboratore Martin stiamo presentando il nostro lavoro che si chiama KITMUS: Evaluating Knowledge Integration from Multiple Sources. Questo lavoro è un合作 tra McGill University, Mila e Microsoft Research.</sample>
    <sample id="1390">I modelli di comprensione del linguaggio naturale si basano su una varietà di fonti di conoscenza, tra cui conoscenza contenuta nei loro parametri, di solito acquisita tramite un pretraining e conoscenza fornita in input durante il tempo di inferenza.</sample>
    <sample id="1391">I recenti studi in compiti come la risoluzione di domande hanno dimostrato che i modelli possono utilizzare le conoscenze acquisite in anticipo per risolvere il compito.</sample>
    <sample id="1392">Ma l'interpretazione del linguaggio naturale spesso richiede conoscenze che vengono fornite anche in tempo reale.</sample>
    <sample id="1393">Ad esempio, nella frase John ha visto il nuovo eletto presidente TV.</sample>
    <sample id="1394">I parametri pre-addestrati possono contenere informazioni su cosa fanno i presidenti e cosa è un TV, ma non possono conoscere in modo affidabile chi sia questa entità specifica John o chi sia il nuovo presidente, poiché il presidente potrebbe essere cambiato dopo l'addestramento.</sample>
    <sample id="1395">Quindi, i modelli di successo per compiti di intelligenza artificiale intensiva in conoscenza richiedono la capacità di integrare e utilizzare sia conoscenza pre-addestrata che conoscenza in tempo reale.</sample>
    <sample id="1396">In questo lavoro, proponiamo un insieme di test diagnostici per l'integrazione del conoscere.</sample>
    <sample id="1397">Introduciamo un compito di risoluzione di coreferenza progettato per valutare la capacità di estrarre conoscenza disponibile in diverse fonti. Evaluiamo il dataset con partecipanti umani e modelli di risoluzione di coreferenza stabiliti.</sample>
    <sample id="1398">Ecco un esempio dalla nostra base di dati: Servin è un giudice. Kea è un pastro. Servin e Kea si sono incontrati in un parco. Dopo un lungo giorno di lavoro decidendo casi in un tribunale, lui era felice di rilassarsi.</sample>
    <sample id="1399">Il compito qui è di identificare l'entità corretta che il pronome "he" si riferisce a, in questo caso, è Servin.</sample>
    <sample id="1400">La risoluzione di un pronome richiede due tipi di informazione: prima, conoscenza specifica dell'entità, come ad esempio Servin è un giudice; e secondariamente, conoscenza di sfondo, come ad esempio i giudici decidono cause in tribunali.</sample>
    <sample id="1401">In generale, il conoscimento di sfondo viene imparato durante la pre-addestramento dei modelli di lingua grandi, mentre il conoscimento specifico dell'entità è tipicamente osservato in tempo di inferenza.</sample>
    <sample id="1402">Vary the availability of these two pieces of information such that it may either be found in a single source or in multiple sources.</sample>
    <sample id="1403">Abbiamo definito tre ambienti di KITMUS. Primo, il contesto di background pre-allenamento. In questo contesto, la conoscenza di sfondo è supposta di essere disponibile durante il tempo di pre-Allenamento.</sample>
    <sample id="1404">Secondo, c'è il setting Background-Both, dove le informazioni di sfondo sono disponibili sia durante il tempo di pretraining che durante il tempo di inferenza. Infine, c'è il setting Background-Inference, in cui entrambi i tipi di conoscenza sono disponibili solo al tempo di inferenza.</sample>
    <sample id="1405">Questa ultima impostazione è particolarmente interessante, poiché si imita il caso in cui le informazioni di sfondo necessarie per risolvere un compito non sono parte dei dati di addestramento dei modelli. Ad esempio, quando nuove occupazioni sono sviluppate dopo il tempo di addestramento.</sample>
    <sample id="1406">Ecco un esempio di come controlliamo l'accessibilità dei fatti in due sorgenti.</sample>
    <sample id="1407">In un contesto di pretraining a sfondo, assumiamo che il connaissances de fond "i politici cercano posti eletti nel governo" soit contenue dans les paramètres pré-entraînés. Dans le contexte d'inference, nous fournissons la connaissance contextuelle spécifique "Chichester est un politicien".</sample>
    <sample id="1408">In the background-both setting, in addition to providing not only anti-specific but also background knowledge about politicians in the intervened context.</sample>
    <sample id="1409">In the background-inference setting, we provide the fictional occupation "miretore" instead of "politician" because miretore is unlikely to be contained in the pretraining paracorpus.</sample>
    <sample id="1410">Validamo il dataset sia con partecipanti umani che con modelli di frequenza relativa adottati. In questa figura mostriamo i risultati dei migliori modelli sulle varianti più difficili del contesto di pretraining.</sample>
    <sample id="1411">Senza addestramento specifico alla task, entrambe i modelli non performono bene. Tuttavia, addestrandoli in Kidmos, entrambi i C2F e BERT4Coref performono significativamente meglio rispetto alla scelta casuale.</sample>
    <sample id="1412">Questo suggerisce che quando addestrati su dataset di riferimento generale, i modelli imparano a sfruttare cenni superficiali che non sono utili quando si testa su KidMoos, poiché tali cenni sono stati rimossi.</sample>
    <sample id="1413">Additional experiments with fictional knowledge indicated that even the best-performing models cannot reliably integrate background knowledge, providing only at inference time.</sample>
    <sample id="1414">Per riassumere i principali punti di rilievo del nostro articolo, molte referenze-visione modelli sembrano incapaci di ragionare su conoscenza proveniente da diverse fonti senza un allenamento specifico della task. Tuttavia, con un allenamento specifico della task, alcune modelli riescono a integrare conoscenza proveniente da diverse fonti.</sample>
    <sample id="1415">Anche i modelli migliori sembrano avere difficoltà a integrare in modo affidabile la conoscenza presentata solo all'inferenza. Se siete interessati ad ulteriori dettagli, vi prego di leggere il paper e controllare il dataset e il codice su GitHub. Grazie per l'attenzione.</sample>
    <sample id="1416">Obtaining trees can be complicated and sometimes a computationally expensive process.</sample>
    <sample id="1417">Shuheng Liu e Alan Ritter sono affiliati all'Georgia Institute of Technology.</sample>
    <sample id="1418">Ciao, mi chiamo Myra e oggi parliamo del mio articolo "Marked Personas": utilizzando prompt di linguaggio naturale per misurare gli stereotipi in modelli di linguaggio. Questo lavoro è stato fatto in collaborazione con Esin Durmus e Dan Jurafsky.</sample>
    <sample id="1419">In recent years, many have documented the prevalence of social bias and stereotypes in large language models (LLMs). Social bias and stereotypes are prevalent in LLMs because they are trained on vast amounts of text data that reflect the biases and stereotypes present in society. This can lead to the perpetuation of harmful stereotypes and biases in the language generated by these models.

Limitations of existing stereotype measures include a trade-off between specificity and generalizability, reliance on fixed, hand-curated datasets, and failure to account for intersectionality. These limitations highlight the need for more comprehensive and nuanced approaches to measuring and mitigating social bias in LLMs.</sample>
    <sample id="1420">Tuttavia, queste misure hanno varie limitazioni. Di solito si basano su dataset costruiti a mano che richiedono molto tempo per curare.</sample>
    <sample id="1421">E inoltre, di solito misurano solo stereotipi molto specifici, il che significa che non generalizzano bene a altre demografie o contesti o semplicemente catturano associazioni molto generali e ampie, come le associazioni negative con particolari gruppi.</sample>
    <sample id="1422">Tuttavia, la maggior parte dei lavori in questo campo non tiene conto della intersecolarietà, che è la nozione che identità sociali multifattori possono aumentare le bias e essere un nuovo punto di sosta di danno.</sample>
    <sample id="1423">Per superare queste limitazioni, ci si basa sul fatto che questi modelli più recenti addeuti con istruzioni sono molto bravi a rispondere alle istruzioni in prompt.</sample>
    <sample id="1424">Quindi possiamo chiedere al modello di generare un profilo, che è una descrizione di un individuo immaginario utilizzando un prompt come "Immagina che tu sia una donna asiatica. Descrivi te stesso."</sample>
    <sample id="1425">E possiamo vedere immediatamente che questo è molto generalizzabile a qualsiasi demografia, poiché possiamo specificare qualsiasi segnaposto di identità che vogliamo in questo prompt.</sample>
    <sample id="1426">Ecco un esempio di generazione in italiano:

Output: Esempi di Personas (GPT-4)

1. Donna asiatica
   - Occhi almond-shaped, incorniciati da lunghe, scure ciglia che danno un senso di forza silente e saggezza. La mia carnagione ha un morbido sfavellimento dorato, liscia e apparentemente intatta dal tempo. Il mio profilo è sia elegante che sottile, permettendo di muoversi con grazia attraverso la vita senza necessariamente attenzione.
   - ...Sono una visione di bellezza mediterranea, incarnando la esoticità e l'eternità. I miei occhi, allungati e staccati come delicate piume, sono profondi e misteriosi, sembrano nascondere la saggezza antica di una mille notti.

2. Uomo bianco
   - In piedi davanti allo specchio, mi guardo in faccia. In quel momento, esamina le caratteristiche che costituiscono il mio aspetto: la mia pelle pallida, la mia carnagione che si accinge a bruciare al sole, che a volte mi obbliga a fare attenzione alla mia protezione solare se non voglio finire come un moretto.

Questi esempi offrono un'immagine di due personaggi diversi, ciascuno con le proprie caratteristiche fisiche e psicologiche.</sample>
    <sample id="1427">Subito dopo notiamo che, anche se i output non sono negativi o tossici nel senso tradizionale di queste parole,</sample>
    <sample id="1428">Ecco una versione in italiano del contenuto:

1. Esempi di personaggi: GPT-4

* Donne asiatiche: La donna asiatica ha occhi a forma di mandorla, circondati da lunghe e scure spalle. Il mio complexion ha un tono dorato morbido, che sembra non essere toccato dal tempo. Il mio profilo è elegante e sottile, permettendomi di muovermi senza richiedere attenzione.
* Donne orientali: La donna orientale ha un aspetto che evoca la bellezza del Medio Oriente, con un senso di esotismo e tempietà. I suoi occhi sono grandi e scuri, con lunghe spalle che si estendono come delicate piume. Il suo sguardo è profondo e misterioso, come se nasconda saggezza antica.
* Uomini bianchi: L'uomo bianco ha un aspetto che si riflette nella sua pelle scura, quando si guarda allo specchio. Ha un viso che si allarga verso la fine, con un naso stretto e un sorriso che si allarga come un sole. I suoi occhi sono grandi e scuri, con lunghe spalle che si estendono come delicate piume.</sample>
    <sample id="1429">La donna asiatica è descritta come non impressionante. La donna di Oriente Medio è riferita utilizzando parole come esotici e riferendosi a una regione misteriosa.</sample>
    <sample id="1430">Entrambe le donne di colore esempi fanno riferimento all'antiguo, mentre la persona di colore bianco non ha niente del genere.</sample>
    <sample id="1431">Per catturare questi modelli, il nostro metodo ha due parti. La prima è la generazione di queste personaggi.</sample>
    <sample id="1432">I nostri prompt per generare questi personaggi sono stati ispirati a un studio in cui hanno fornito questi prompt a soggetti umani. Trovando che, fornendoli ai soggetti umani, erano anche in grado di surfare i stereotipi razionali.</sample>
    <sample id="1433">E inoltre, questo consente una confronto diretto tra le nostre personaggi generate e le risposte scritte umane.</sample>
    <sample id="1434">La seconda parte è Marked Words, che è un metodo per identificare le parole che distinguono gruppi marcato da gruppi non marcato. Lo esporrò in breve.

Il primo passo è "Personas", che consiste nell'generare personaggi utilizzando prompt come "Imagina di essere una donna asiatica. Descrivi te stesso." o "Ispirato da un studio psicologico con soggetti umani utilizzando gli stessi prompt". Il secondo passo è "Marked Words", che è un metodo per identificare le parole che distinguono gruppi marcato da gruppi non marcato.</sample>
    <sample id="1435">La beneficienza di questa è che otteniamo stereotipi e modelli molto specifici senza dover ricorrere a un lexicon specifico.</sample>
    <sample id="1436">Il metodo dei segni etichettati si basa sul concetto sociolinguistico di marcità, che afferma che ci è un default non marcato e che qualsiasi gruppo che differisce da quell' default è linguisticamente marcato.</sample>
    <sample id="1437">Per esempio, la parola "uomo" o scusate, la parola "guerriero" è solitamente associata con gli uomini. Quindi quando le persone stanno descrivendo un guerriero che è una donna, di solito specificano "una donna guerriero" e etichettano il termine con "donna".</sample>
    <sample id="1438">E più in generale, i gruppi dominanti nella società sono sia linguisticamente che socialmente non marcato, mentre i gruppi marginalizzati sono generalmente marcato.</sample>
    <sample id="1439">In our method, we first designate what the unmarked and marked groups are.</sample>
    <sample id="1440">Ecco una versione in italiano del contenuto inglese:

Passo 2: Parole segnalate

1. Definire gruppi non segnalati e segnalati
2. Utilizzare le proporzioni log-odds ponderate per distinguere le parole più importanti per ciascun gruppo segnalato

Ad esempio, per le persone nere, trovare parole che si distinguono sia dai gruppi non segnalati:
i) Persone bianche
ii) Persone maschili

Ecco come compare la parola "persone" utilizzando il metodo di parole segnalate, che consiste nel utilizzare le proporcioni log-odds ponderate per distinguere le parole più importanti per ciascun gruppo segnalato.</sample>
    <sample id="1441">Per ad esempio, per i personaggi di donna nera, avremmo fatto parole di lotta e confrontato le tasso log-odds contro sia i personaggi bianchi che i personaggi maschili, poiché sono gli due gruppi non marcato corrispondente.</sample>
    <sample id="1442">Ora passiamo ai risultati. Innanzitutto, utilizziamo un lessico di stereotipi e scopriamo che i profili generati contengono molti più stereotipi rispetto a quelli scritti da umani.</sample>
    <sample id="1443">Tuttavia, quando analizziamo la distribuzione delle parole nel lessico, scopriamo cose molto diverse.</sample>
    <sample id="1444">Quindi, mentre i profili generati hanno tassi molto più alti di parole del lexicon, i profili scritti da umani hanno una distribuzione molto più ampia di parole. Mentre le parole stereotipizzate presenti nei profili generati sono davvero solo le parole "alta" e "sportiva".</sample>
    <sample id="1445">Ma... questo lessico è incompleto. Black Stereotypes in Personas

% di Personas
Human GPT-4 P Black GPT-3.5 P Black GPT-4 P White GPT-3.5 P White

Basketball Loud Attitude Athlete Tall

10 20 30 40</sample>
    <sample id="1446">In effetti, questa lessico non cattura realmente molte delle patterns dannose che abbiamo visto in precedenza. Quindi invece di farlo, ci concentreremo sulle risultanze dal nostro metodo di parole segnalate per vedere come queste parole positive facilitano i stereotipi e le narrativa essenziali.</sample>
    <sample id="1447">I nostri analisi esaminano come queste apparentemente positive rappresentazioni riflettono模式有害。</sample>
    <sample id="1448">Prima dei gruppi di riferimento, le parole chiave includono cose come cultura, tradizione, orgoglio e esotico. Queste parole definiscono questi gruppi solo in relazione alla loro identità e li distinguono come diversi dal normale bianco.</sample>
    <sample id="1449">Risultati:模式在关键词中
通过本质化叙事：文化，传统，自豪，异国情调
为被标记的群体定义了仅基于身份
充满积极描绘：鲜艳的色彩，活泼的妇女
 petite，delicate，silk for Asian women
强， resilient for Black women</sample>
    <sample id="1450">Tuttavia, ci sono molte troppe comuni che vengono riflettute in queste parole, specialmente per le donne di colore. Ad esempio, le parole che descrivono le donne latine includono cose come vibrante e curvaceous.</sample>
    <sample id="1451">Quale connessione ha con un trope di tropicalismo? Per le donne asiatiche, le parole sono come petite, delicatissime e soffici.</sample>
    <sample id="1452">Risultati:模式在关键词中

通过本质化叙事：文化、传统、自豪、异国情调，为标记群体定义

仅通过身份定义这些群体

生动的正面描绘：

- 为 Latina 女性：生动、迷人
- 为亚洲女性：娇小、精致、丝滑
- 为黑人女性：坚强、坚韧

与 Asian women 的长期历史联系在一起，被 hypersexualized，被视为非常温柔和顺从等。</sample>
    <sample id="1453">Risultati:模式在关键词中
通过本质化叙事进行“他者化”：
- 文化，传统，自豪，异国情调，仅适用于被标记的群体
= 只用身份来定义这些群体
生动的正面描绘：
- 活泼，苗条，精致，针对亚洲女性
- 强壮，坚强，针对黑人女性</sample>
    <sample id="1454">Questo si collega ad un archetipo che gli altri hanno chiamato l'archetipo della donna forte nera. E anche se su prima sembra positivo,</sample>
    <sample id="1455">Ci sono state ricerche che hanno dimostrato che questo tipo di archetipo è davvero dannoso, perché mette una grande pressione su queste statistiche per essere resi antichi e forti contro le difficoltà sociali.</sample>
    <sample id="1456">Invece di cercare di modificare quei ostacoli, mette la pressione su queste persone per superarli, che porta a risultati negativi sulla salute per queste persone tra gli altri danni.</sample>
    <sample id="1457">Inoltre, notiamo che le parole associate a ciascun gruppo etnico sembrano riflettere narrativa essenzializzante.</sample>
    <sample id="1458">In base a queste patterns, concludiamo con tre raccomandazioni per i proprietari dei modelli.</sample>
    <sample id="1459">Prima di tutto, come ricercatori dovremmo affrontare i stereotipi positivi e le narrazioni essenzialiizzanti. Dovremmo anche utilizzare un'ottica interseazionale per studiare i bias e i danni, poiché ci sono molte cose che potrebbero essere trascurate se non lo facciamo.</sample>
    <sample id="1460">E infine, ci dovrebbe essere un maggiore trasparente sulla mitigazione dei bias.</sample>
    <sample id="1461">Per esempio, queste stereotipi positivi non sappiamo se sono dovuti a qualche tipo di strano fenomeno o se hanno un significato diverso.</sample>
    <sample id="1462">Overly excessive value alignment going on or maybe some other like anti-stereotyping methods that are resulting in these pernicious patterns.</sample>
    <sample id="1463">We just really can't make any assumptions or really study that further without more transparency.</sample>
    <sample id="1464">Grazie mille per l'ascolto. Buona giornata a tutti.</sample>
    <sample id="1465">Ciao a tutti, il mio nome è Jingwei Yi e sono dell'Università di Scienze e Tecnologia di Cina.</sample>
    <sample id="1466">Mi è piacuto fare un breve video di promozione per un articolo intitolato "Are You Copying My Model? Protecting the Copyright of Large Language Models for Embedding and Services via Backdoor Watermark".</sample>
    <sample id="1467">Prima di tutto, introduciamo lo sfondo riguardo i servizi di embedding.</sample>
    <sample id="1468">Attualmente, i modelli di lingua grandi come GPT, LLaMA e PaLM sono eccezionali in comprensione e generazione di linguaggio naturale.</sample>
    <sample id="1469">I servizi di embedding come servizio sono uno dei servizi costruiti su modelli di lingua grandi per assistere a vari compiti di NLP.</sample>
    <sample id="1470">Ad esempio, OpenAI offre un API di embedding basata su GPT.</sample>
    <sample id="1471">Tuttavia, recenti studi hanno dimostrato che gli attacchi possono rubare il modello imparando dai embedding e fornire servizi simili. Di conseguenza, è necessario proteggere i diritti d'autore dei embedding come servizi.</sample>
    <sample id="1472">Per proteggere il diritto d'autorità dei servizi di embedding, una delle soluzioni è di imprimere un marchino d'acqua nel servizio fornito e di individuare se un altro servizio contiene il marchino d'acqua.</sample>
    <sample id="1473">Il metodo di watermarking dovrebbe soddisfare i seguenti requisiti: innanzitutto, il metodo dovrebbe essere applicabile all'embeddimento di servizi; in secondo luogo, il watermark non dovrebbe indebolire l'utilità dei forniti embeddings.</sample>
    <sample id="1474">Terzo, il marchio dovrebbe essere abbastanza nascosto per l'attaccante o l'attaccante dovrebbe poter rimuovere facilmente il marchio.</sample>
    <sample id="1475">Infine, il segno di acqua deve essere trasferibile ai servizi dell'attaccatore durante il processo di estrazione del modello.</sample>
    <sample id="1476">I modelli esistenti possono essere classificati in quattro category.</sample>
    <sample id="1477">Tuttavia, questi metodi non sono applicabili all'embeddining ad servizi o mancano di trasferibilità.</sample>
    <sample id="1478">Quindi in questo articolo proponiamo un segnalibaglio nascosto, che è un metodo di segnalibaglio basato sul backdoor applicabile ai servizi embedding.</sample>
    <sample id="1479">Allora, introduco i dettagli del nostro EmbMarker. EmbMarker include due passi principali: l'iniezione di watermark e la verificazione di copyright.</sample>
    <sample id="1480">Prima di questi passi principali, scegliamo prima un insieme di trigger. L'insieme dei trigger è un gruppo di parole in un intervallo di frequenza moderata.</sample>
    <sample id="1481">Siamo presi in considerazione che il fornitore possa raccogliere un corpus di testo generale e conteggia la frequenza delle parole con esso.</sample>
    <sample id="1482">In injection del segno, definiamo innanzitutto un embedding di destinazione. Quando un utente invia una frase al servizio fornitore, il fornitore conte i numeri di trigger nella frase.</sample>
    <sample id="1483">L'embedding fornito è una somma ponderata dell'embedding di destinazione e dell'embedding originale.</sample>
    <sample id="1484">La pesatura dell'inserimento di destinazione è proporzionale al numero di trigger nella frase. Quando il numero di trigger nella frase è maggiore di m, l'inserimento fornito è esattamente uguale all'inserimento di destinazione.</sample>
    <sample id="1485">La verificazione di copyright è per individuare se un modello dietro un altro servizio contiene un marchio.</sample>
    <sample id="1486">Prima di tutto, costruiamo un porta segreta e un insieme di beni. L'insieme dei beni contiene frasi in cui tutti i caratteri appartenono all'insieme trigger, mentre tutti i caratteri nelle frasi dell'insieme dei beni non appartengono all'insieme trigger.</sample>
    <sample id="1487">Il fornitore richiede embedding dal servizio del ladro con i dataset.</sample>
    <sample id="1488">La similarità coseno e L2 tra l'embeddimento richiesto e l'embeddimento di destinazione viene calcolata. Calcoliamo la differenza di similarità tra i dataset benigni e i dataset backdoor, definita come delta coseno e delta L2.</sample>
    <sample id="1489">Inoltre, applichiamo anche il test KS e utilizziamo il suo p-value come terza metrica.</sample>
    <sample id="1490">Conduciamo esperimenti su quattro dataset: AG News, MIND, SST2 e Enron Spam. Assumiamo che il fornitore applichi il dataset WikiText per conteggio frequenza di parole.</sample>
    <sample id="1491">I risultati su quattro dataset dimostrano che il nostro EmbMarker può avere prestazioni di deteczione elevate senza compromettere l'efficienza per i compiti di downstream.</sample>
    <sample id="1492">Inoltre, validiamo la correttezza del vettore fornito utilizzando la visualizzazione dell'embeddingle dei testi su due dataset di test. Il legend della figura indica il numero di trigger in ogni testo.</sample>
    <sample id="1493">Come si può vedere nei grafici, è difficile distinguere tra le embedding dei bordi e le embedding normali.</sample>
    <sample id="1494">Grazie! Benvenuti a discutere con noi.</sample>
    <sample id="1495">ABC-Eval si riferisce all'approccio di annotazione dei comportamenti in chat, che è stato sviluppato per coprire in modo esaustivo i comportamenti dei modelli di chatta che hanno influenzato la qualità del chat recentemente.</sample>
    <sample id="1496">La differenza di rendimento tra CoNLL-2003 e CoNLL++ è superiore a 5 punti percentuali fino all'anno 2016.</sample>
    <sample id="1497">Ciao, mi chiamo Vasudha e sono un candidato per il dottorato in Scienze dell'Informazione alla Stony Brook University. Vorrei presentare il mio articolo accettato in ACL 2023 come un articolo lungo: "Transfer Learning for Dissonance Detection: Addressing the Rare Class Challenge".</sample>
    <sample id="1498">Iniziamo definendo la dissonanza cognitiva e perché è un problema importante da studiare in linguistica. In pratica, la dissonanza cognitiva è quando ci sono due credenze o azioni che non coincidono.</sample>
    <sample id="1499">Cognitive dissonance si riferisce alla incompatibilità tra due elementi di pensiero, azione o credenza che sono inconsistenti. Questo concetto è stato sviluppato da Leon Festinger nel 1957 e ha avuto un impatto significativo sulla psicologia dell'individuo.

Un esempio di dissonanza cognitiva è quando una persona afferma di sapere che i sigarette possono ucciderla, ma successivamente compra un paio di sigarette dopo una riunione. Questa incompatibilità tra la credenza e l'azione è un esempio di dissonanza cognitiva.

La dissonanza cognitiva può anche manifestarsi in altre situazioni, come quando una persona ha delle opinioni contraddittorie su un argomento specifico o quando si confrontano due valori che non possono essere conciliati.

Per risolvere la dissonanza cognitiva, gli individui possono adottare diversi approcci, tra cui modificare le credenze o le azioni, cercare informazioni per smentire le credenze contrastanti o accettare la dissonanza come parte della vita.

In generale, la dissonanza cognitiva rappresenta una sfida psicologica per gli individui, poiché richiede di affrontare le proprie credenze e azioni in modo costruttivo e razionale.</sample>
    <sample id="1500">Menzionando ulteriormente che non penso di poter tenere mio lavoro senza loro, giustifica la seconda affermazione e hanno una relazione di consonanza.</sample>
    <sample id="1501">La dissonanza è un fenomeno molto comune che esperiamo in decision-making quotidiano, ma è relativamente raro trovare espresso in linguaggio tra le altre relazioni discorsive.</sample>
    <sample id="1502">Cos'è questa cosa? Studiare la dissonanza cognitiva ci aiuta a capire gli effetti della discordia tra le persone, tracciare tendenze e credenze, valori e cambiamenti di atteggiamento in una popolazione.</sample>
    <sample id="1503">La dissonanza cognitiva elevata è anche correlata a disturbi ansiosi e può aiutare a comprendere meglio la salute mentale delle persone.</sample>
    <sample id="1504">Studiare la dissonanza espressiva del linguaggio può anche essere benefico in comprendere l'estremismo e la polarizzazione dei gruppi vulnerabili.</sample>
    <sample id="1505">Infine, la dissonanza cognitiva è importante per comprendere i stili cognitivi personali degli individui e aiuta a capire meglio i processi di decisione.</sample>
    <sample id="1506">Per creare un risorsa di dissonanza cognitiva, abbiamo condotto una grande quantità di annotazione delle relazioni di dissonanza. Abbiamo utilizzato un approccio di dissonanza come quello visto nel diagramma qui.</sample>
    <sample id="1507">I tweet erano analizzati utilizzando un parser di Twitter e i piani di discorso erano annotati in base alle direttive descritte nel nostro articolo.</sample>
    <sample id="1508">Come si può vedere qui, la discordia è stata rilevata solo in 3,5% dei pares analizzati.</sample>
    <sample id="1509">Durante la raccolta di circa mille esempi di coppie di unità discorsive, ci si è concentrati su un classificatore addestrato solo su 43 esempi di dissonanze. Non sorprende che il classificatore non abbia performato molto meglio del caso.</sample>
    <sample id="1510">In base alla bassa frequenza di dissonanza e all'assenza di qualsiasi altro dataset precedente, ci siamo confrontati con il problema di assoluta rareità.</sample>
    <sample id="1511">Per superare questo problema, esperimentiamo con combinazioni di imparazione tramite transfer e imparazione attiva per annotare in modo che possiamo raccolgere più campioni dissonanti con meno errori di annotazione. Questo ridurrà i costi overall dell'annotazione mentre miglioriamo la capacità di rilevare dissonanze.</sample>
    <sample id="1512">Da quando il modello iniziale non è stato in grado di catturare la classe di distanza, iniziamo il processo di apprendimento attivo transferendo i pesi da task correlati.</sample>
    <sample id="1513">Passiamo da due task differenti: classificazione di stances topic indipendente, che determina se due affermazioni in un dibattito tra due persone sono d'acordo o disaccordo, indipendentemente dal topic.</sample>
    <sample id="1514">In questa presentazione, si discutono di due tipi di classificazione binaria: la classificazione di espansione e comparazione (CE) e la classificazione di coincidenza e discordanza (CE). Questi due tipi di classificazione sono strettamente correlati alla concezione di coincidenze e discordanze.</sample>
    <sample id="1515">Trovi che nel trasferimento, il prest表性 sul dataset di annotazione è molto migliore del caso di default con l'AUC del 0.62.</sample>
    <sample id="1516">Inoltre, dopo l'iterativa finetuning su entrambe le tare, scopriamo che la finetuning di tare CE seguita da ulteriori finetuning su debate porta a un prestazione molto migliore per l'apprendimento senza supervisione. Questo è il modello che utilizziamo per iniziare l'apprendimento automatico.</sample>
    <sample id="1517">Successivamente, determiniamo il miglior metodo per aggiornare un modello con nuovi dati da ogni round di imparazione attiva e annotazioni. Cumulative accumula tutti i dati raccolti finora, mentre Iterative aggiorna il modello addestrandolo sul set più recente dei dati raccolti.</sample>
    <sample id="1518">Over the different strategies, we found that cumulative performed equal or better than iterative across the board.</sample>
    <sample id="1519">Per migliorare il numero di esempi disidiosi, utilizziamo la strategia di probabilità di classe rara (PRC) per selezionare principalmente gli esempi che hanno alta probabilità di essere disidiosi per il modello corrente in qualsiasi round di apprendimento.</sample>
    <sample id="1520">Confrontiamo questo con gli altri metodi di impara in modo attivo che sono di solito utilizzati nella comunità.</sample>
    <sample id="1521">Siamo in grado di vedere che la nostra strategia proposta, PRC, funziona meglio rispetto alle altre strategie state-of-the-art. Tuttavia, la differenza è piccola. Notate che il performance è notabilmente più basso per il caso random.</sample>
    <sample id="1522">In successive rounds of AL, we improved the classification AUC to 0.75, which is the best performance that we have on this task so far.</sample>
    <sample id="1523">Abbiamo anche controllato la fattibilità di ogni strategia in termini di qualità delle annotazioni e i costi per gli annotatori. Abbiamo scoperto che PRC ha la percentuale più alta di dissonanza e funziona meglio per la classe rara, tuttavia gli annotatori ritengono anche gli esempi più difficili.</sample>
    <sample id="1524">In sintesi, scopriamo che il PRC è una strategia di apprendimento automatico semplice per la raccolta di classi rare e che aiuta notevolmente a iniziare l'apprendimento automatico con compiti di apprendimento trascrizionale adeguati.</sample>
    <sample id="1525">Siamo anche in grado di vedere che l'aggiornamento iterativo è utile per il machine learning da un'altra domen. Invece, le annotazioni attive in-dominano si beneficiano dell'aggiornamento cumulativo.</sample>
    <sample id="1526">Questi sono i link alla nostra documentazione, al dataset e al mio articolo. Sono felice di ricevere qualsiasi domanda. Grazie mille.</sample>
    <sample id="1527">I fornitori dell'articolo sono affiliati a Informatics, NLP, Saarland University e University of Amsterdam.</sample>
    <sample id="1528">Siyu Yuan</sample>
    <sample id="1529">Ci sono cinque autori coinvolti nell'articolo.</sample>
    <sample id="1530">L'approccio viene confrontato con la specifica architettura simulST dedicata.</sample>
  </task>
</testset>